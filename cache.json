{
  "sources": [
    {
      "title": "ML in Production",
      "feedUrl": "https://mlinproduction.com/feed",
      "siteUrl": "https://mlinproduction.com",
      "articles": []
    },
    {
      "title": "Blog – Machine Learning Mastery",
      "feedUrl": "http://machinelearningmastery.com/blog/feed",
      "siteUrl": "https://machinelearningmastery.com",
      "articles": [
        {
          "id": "https://machinelearningmastery.com/?p=13176",
          "author": "Adrian Tam",
          "description": "In all programming exercises, it is difficult to go far and deep without a handy debugger. The built-in debugger, pdb, […]\nThe post Python debugging tools appeared first on Machine Learning Mastery.",
          "link": "https://machinelearningmastery.com/python-debugging-tools/",
          "publishedOn": "2022-01-06T21:00:14.000Z",
          "wordCount": 4721,
          "title": "Python debugging tools"
        },
        {
          "id": "https://machinelearningmastery.com/?p=13146",
          "author": "Muhammad Asad Iqbal Khan",
          "description": "Anomaly detection is to find data points that deviate from the norm. In other words, those are the points that […]\nThe post Anomaly Detection with Isolation Forest and Kernel Density Estimation appeared first on Machine Learning Mastery.",
          "link": "https://machinelearningmastery.com/anomaly-detection-with-isolation-forest-and-kernel-density-estimation/",
          "publishedOn": "2021-12-31T20:20:10.000Z",
          "wordCount": 2204,
          "title": "Anomaly Detection with Isolation Forest and Kernel Density Estimation"
        },
        {
          "id": "https://machinelearningmastery.com/?p=13157",
          "author": "Muhammad Asad Iqbal Khan",
          "description": "PyTorch is an open-source deep learning framework based on Python language. It allows you to build, train, and deploy deep […]\nThe post One-Dimensional Tensors in Pytorch appeared first on Machine Learning Mastery.",
          "link": "https://machinelearningmastery.com/one-dimensional-tensors-in-pytorch/",
          "publishedOn": "2021-12-29T21:30:13.000Z",
          "wordCount": 2819,
          "title": "One-Dimensional Tensors in Pytorch"
        },
        {
          "id": "https://machinelearningmastery.com/?p=13153",
          "author": "Stefania Cristina",
          "description": "Running your Python scripts is an important step in the development process, because it is in this manner that you’ll […]\nThe post Running and Passing Information to a Python Script appeared first on Machine Learning Mastery.",
          "link": "https://machinelearningmastery.com/running-and-passing-information-to-a-python-script/",
          "publishedOn": "2021-12-29T04:10:32.000Z",
          "wordCount": 2915,
          "title": "Running and Passing Information to a Python Script"
        },
        {
          "id": "https://machinelearningmastery.com/?p=13148",
          "author": "Adrian Tam",
          "description": "When an exception occurs in a Python program, often a traceback will be printed. Knowing how to read the traceback […]\nThe post Understanding Traceback in Python appeared first on Machine Learning Mastery.",
          "link": "https://machinelearningmastery.com/understanding-traceback-in-python/",
          "publishedOn": "2021-12-23T23:56:02.000Z",
          "wordCount": 4034,
          "title": "Understanding Traceback in Python"
        },
        {
          "id": "https://machinelearningmastery.com/?p=13128",
          "author": "Mehreen Saeed",
          "description": "Python is a fantastic programming language. It is likely to be your first choice for developing a machine learning or […]\nThe post Functional Programming In Python appeared first on Machine Learning Mastery.",
          "link": "https://machinelearningmastery.com/functional-programming-in-python/",
          "publishedOn": "2021-12-19T02:26:12.000Z",
          "wordCount": 3943,
          "title": "Functional Programming In Python"
        }
      ]
    },
    {
      "title": "Machine Learning – Uber Engineering Blog",
      "feedUrl": "https://eng.uber.com/tag/machine-learning/feed",
      "siteUrl": "https://eng.uber.com",
      "articles": []
    },
    {
      "title": "AWS Machine Learning Blog",
      "feedUrl": "https://aws.amazon.com/blogs/machine-learning/feed",
      "siteUrl": "https://aws.amazon.com/blogs/machine-learning/",
      "articles": [
        {
          "id": "710cac6401dc2f94577ce3b24f5fc276f08cb97d",
          "author": "Phi Nguyen",
          "description": "In deep learning, batch processing refers to feeding multiple inputs into a model. Although it’s essential during training, it can be very helpful to manage the cost and optimize throughput during inference time as well. Hardware accelerators are optimized for parallelism, and batching helps saturate the compute capacity and often leads to higher throughput. Batching […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/optimize-your-inference-jobs-using-dynamic-batch-inference-with-torchserve-on-amazon-sagemaker/",
          "publishedOn": "2022-01-12T23:37:41.000Z",
          "wordCount": 2435,
          "title": "Optimize your inference jobs using dynamic batch inference with TorchServe on Amazon SageMaker"
        },
        {
          "id": "18e28255b8a479496e28e432ae565a43c98e7a10",
          "author": "Yanwei Cui",
          "description": "Recommendation systems are one of the most widely adopted machine learning (ML) technologies in real-world applications, ranging from social networks to ecommerce platforms. Users of many online systems rely on recommendation systems to make new friendships, discover new music according to suggested music lists, or even make ecommerce purchase decisions based on the recommended products. […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/graph-based-recommendation-system-with-neptune-ml-an-illustration-on-social-network-link-prediction-challenges/",
          "publishedOn": "2022-01-12T22:50:19.000Z",
          "wordCount": 3906,
          "title": "Graph-based recommendation system with Neptune ML: An illustration on social network link prediction challenges"
        },
        {
          "id": "19c85b480bb3317ac9d52279ce6b1d42f4878163",
          "author": "Jerome Bachelet",
          "description": "Cloud security at AWS is the highest priority. Amazon SageMaker Studio offers various mechanisms to protect your data and code using integration with AWS security services like AWS Identity and Access Management (IAM), AWS Key Management Service (AWS KMS), or network isolation with Amazon Virtual Private Cloud (Amazon VPC). Customers in highly regulated industries, like […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/secure-access-to-amazon-sagemaker-studio-with-aws-sso-and-a-saml-application/",
          "publishedOn": "2022-01-12T21:02:00.000Z",
          "wordCount": 2807,
          "title": "Secure access to Amazon SageMaker Studio with AWS SSO and a SAML application"
        },
        {
          "id": "1a768c0bafbac9e76c5fc2995b99cee8349b1ad7",
          "author": "Divya Bhargavi",
          "description": "This is the first in a two-part blog series on how Tyson Foods, Inc., is utilizing Amazon SageMaker and AWS Panorama to automate industrial processes at their meat packing plants by bringing the benefits of artificial intelligence applications at the edge. In part one, we discuss an inventory counting application for packaging lines. In part […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/industrial-automation-at-tyson-with-computer-vision-aws-panorama-and-amazon-sagemaker/",
          "publishedOn": "2022-01-11T20:05:52.000Z",
          "wordCount": 2727,
          "title": "Industrial automation at Tyson with computer vision, AWS Panorama, and Amazon SageMaker"
        },
        {
          "id": "07f0b128b6669e586da68c420996cf8b153a6edc",
          "author": "Jihye Park",
          "description": "This is a guest post by Jihye Park, a Data Scientist at MUSINSA.  MUSINSA is one of the largest online fashion platforms in South Korea, serving 8.4M customers and selling 6,000 fashion brands. Our monthly user traffic reaches 4M, and over 90% of our demographics consist of teens and young adults who are sensitive to […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/develop-an-automatic-review-image-inspection-service-with-amazon-sagemaker/",
          "publishedOn": "2022-01-10T22:15:42.000Z",
          "wordCount": 2498,
          "title": "Develop an automatic review image inspection service with Amazon SageMaker"
        },
        {
          "id": "a1d79d40649b9bdabe57f885de165529cfc800c9",
          "author": "Daniel Burke",
          "description": "Cybersecurity continues to be a top concern for enterprises. Yet the constantly evolving threat landscape that they face makes it harder than ever to be confident in their cybersecurity protections. To address this, ReliaQuest built GreyMatter, an Open XDR-as-a-Service platform that brings together telemetry from any security and business solution, whether on-premises or in one or multiple clouds, to unify detection, investigation, response, and resilience. In 2021, ReliaQuest turned to AWS to help it enhance its artificial intelligence (AI) capabilities and build new features faster.",
          "link": "https://aws.amazon.com/blogs/machine-learning/how-reliaquest-uses-amazon-sagemaker-to-accelerate-its-ai-innovation-by-35x/",
          "publishedOn": "2022-01-10T17:59:10.000Z",
          "wordCount": 1366,
          "title": "How ReliaQuest uses Amazon SageMaker to accelerate its AI innovation by 35x"
        },
        {
          "id": "c4eceecbda37afb0e026c0c0181b6322fda8d5ed",
          "author": "Anastasia Pachni Tsitiridou",
          "description": "With the advent of artificial intelligence (AI) and machine learning (ML), customers and the general public have become increasingly aware of their privacy, as well as the value that it holds in today’s data-driven world. Enterprises are actively seeking out and marketing privacy-first solutions, especially in the Computer Vision (CV) domain. They need to reassure […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/blur-faces-in-videos-automatically-with-amazon-rekognition-video/",
          "publishedOn": "2022-01-07T19:45:55.000Z",
          "wordCount": 2283,
          "title": "Blur faces in videos automatically with Amazon Rekognition Video"
        },
        {
          "id": "adf2202cf0c47a7020764a4818e842b90c266b2b",
          "author": "Assaf Elovic",
          "description": "With over 200 million users worldwide, Wix is a leading cloud-based development platform for building fully personalized, high-quality websites. Wix makes it easy for anyone to create a beautiful and professional web presence. When Wix started, it was easy to understand user sentiment and identify product improvement opportunities because the user base was small. Such […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/how-wix-empowers-customer-care-with-ai-capabilities-using-amazon-transcribe/",
          "publishedOn": "2022-01-05T21:21:03.000Z",
          "wordCount": 1997,
          "title": "How Wix empowers customer care with AI capabilities using Amazon Transcribe"
        },
        {
          "id": "5d353acdae2f243b026ada3aa895ab14d5fa6849",
          "author": "Nancy Clarke",
          "description": "In parts one and two of our guide to conversation design with Amazon Lex, we discussed how to gather requirements for your conversational AI application and draft conversational flows. In this post, we help you bring all the pieces together. You’ll learn how draft an interaction model to deliver natural conversational experiences, and how to […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/part-3-how-to-approach-conversation-design-with-amazon-lex-building-and-testing/",
          "publishedOn": "2022-01-05T20:21:59.000Z",
          "wordCount": 3177,
          "title": "How to approach conversation design with Amazon Lex: Building and testing (Part 3)"
        },
        {
          "id": "6215bf2285e9eb14f54e8dbcb388d71380ff4a0f",
          "author": "Ram Vegiraju",
          "description": "Amazon SageMaker Serverless Inference (Preview) was recently announced at re:Invent 2021 as a new model hosting feature that lets customers serve model predictions without having to explicitly provision compute instances or configure scaling policies to handle traffic variations. Serverless Inference is a new deployment capability that complements SageMaker’s existing options for deployment that include: SageMaker […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/deploying-ml-models-using-sagemaker-serverless-inference-preview/",
          "publishedOn": "2022-01-05T20:03:57.000Z",
          "wordCount": 2153,
          "title": "Deploying ML models using SageMaker Serverless Inference (Preview)"
        },
        {
          "id": "e56da2bbbc39a5d38180fb8551687ecd6f8a781c",
          "author": "Ahmed Saef Zamzam",
          "description": "We’re living in a world of everything-as-an-online-service. Service providers from almost every industry are in the race to feature the best user experience for their online channels like web portals and mobile applications. This raises a new challenge. How do we stop illegal and fraudulent behaviors without impacting typical legitimate interactions? This challenge is even […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/build-and-visualize-a-real-time-fraud-prevention-system-using-amazon-fraud-detector/",
          "publishedOn": "2022-01-05T19:58:41.000Z",
          "wordCount": 3538,
          "title": "Build and visualize a real-time fraud prevention system using Amazon Fraud Detector"
        },
        {
          "id": "f4ffa85e5d7f589e9c9df6e2fe62e7dddb6842c3",
          "author": "Raghu Ramesha",
          "description": "Deployment guardrails in Amazon SageMaker provide a new set of deployment capabilities allowing you to implement advanced deployment strategies that minimize risk when deploying new model versions on SageMaker hosting. Depending on your use case, you can use a variety of deployment strategies to release new model versions. Each of these strategies relies on a […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/take-advantage-of-advanced-deployment-strategies-using-amazon-sagemaker-deployment-guardrails/",
          "publishedOn": "2022-01-04T20:43:55.000Z",
          "wordCount": 3310,
          "title": "Take advantage of advanced deployment strategies using Amazon SageMaker deployment guardrails"
        },
        {
          "id": "a821bb05fde90c047b723f7476b9e6ee8a5f4267",
          "author": "Zichen Wang",
          "description": "There are over 180,000 unique proteins with 3D structures determined, with tens of thousands new structures resolved every year. This is only a small fraction of the 200 million known proteins with distinctive sequences. Recent deep learning algorithms such as AlphaFold can accurately predict 3D structures of proteins using their sequences, which help scale the […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/train-graph-neural-nets-for-millions-of-proteins-on-amazon-sagemaker-and-amazon-documentdb-with-mongodb-compatibility/",
          "publishedOn": "2022-01-04T18:48:58.000Z",
          "wordCount": 3295,
          "title": "Train graph neural nets for millions of proteins on Amazon SageMaker and Amazon DocumentDB (with MongoDB compatibility)"
        },
        {
          "id": "a39bc768d7da4b8131f197a938afd616a8a7ee5e",
          "author": "Nate Bachmeier",
          "description": "In-person user identity verification is slow to scale, costly, and high friction for users. Machine learning (ML) powered facial recognition technology can enable online user identity verification. Amazon Rekognition offers pre-trained facial recognition capabilities that you can quickly add to your user onboarding and authentication workflows to verify opted-in users’ identities online. No ML expertise […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/identity-verification-using-amazon-rekognition/",
          "publishedOn": "2022-01-03T22:22:34.000Z",
          "wordCount": 2673,
          "title": "Identity verification using Amazon Rekognition"
        },
        {
          "id": "211e19b2ca986a29acad038c2a6fce1c14edcab4",
          "author": "Alak Eswaradass",
          "description": "Gartner predicts that by the end of 2024, 75% of enterprises will shift from piloting to operationalizing artificial intelligence (AI), and the vast majority of workloads will end up in the cloud in the long run. For some enterprises that plan to migrate to the cloud, the complexity, magnitude, and length of migrations may be […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/introducing-hybrid-machine-learning/",
          "publishedOn": "2021-12-23T20:13:09.000Z",
          "wordCount": 2361,
          "title": "Introducing hybrid machine learning"
        },
        {
          "id": "a1ba3dac7c896a0731f566fca4d13bf7c3d9a00c",
          "author": "Patrick Sard",
          "description": "Until recently, customers who wanted to use a deep learning (DL) framework with Amazon SageMaker Processing faced increased complexity compared to those using scikit-learn or Apache Spark. This post shows you how SageMaker Processing has simplified running machine learning (ML) preprocessing and postprocessing tasks with popular frameworks such as PyTorch, TensorFlow, Hugging Face, MXNet, and […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/use-deep-learning-frameworks-natively-in-amazon-sagemaker-processing/",
          "publishedOn": "2021-12-23T19:57:34.000Z",
          "wordCount": 2193,
          "title": "Use deep learning frameworks natively in Amazon SageMaker Processing"
        },
        {
          "id": "4ad6604866aa6f367ad543ba773b887ee5278e55",
          "author": "Bob Strahan",
          "description": "Your contact center connects your business to your community, enabling customers to order products, callers to request support, clients to make appointments, and much more. When calls go well, callers retain a positive image of your brand, and are likely to return and recommend you to others. And the converse, of course, is also true. […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/live-call-analytics-for-your-contact-center-with-amazon-language-ai-services/",
          "publishedOn": "2021-12-18T00:47:33.000Z",
          "wordCount": 4155,
          "title": "Live call analytics for your contact center with Amazon language AI services"
        },
        {
          "id": "ce2f56854ba560eea06ff25c4fb2f22d80b1505d",
          "author": "Andrew Kane",
          "description": "Your contact center connects your business to your community, enabling customers to order products, callers to request support, clients to make appointments, and much more. Each conversation with a caller is an opportunity to learn more about that caller’s needs, and how well those needs were addressed during the call. You can uncover insights from […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/post-call-analytics-for-your-contact-center-with-amazon-language-ai-services/",
          "publishedOn": "2021-12-18T00:38:22.000Z",
          "wordCount": 4014,
          "title": "Post call analytics for your contact center with Amazon language AI services"
        },
        {
          "id": "da9379ec44cc5e0492af56eff2007df098992e38",
          "author": "Jonathan Chung",
          "description": "In many industries, including financial services, banking, healthcare, legal, and real estate, automating document handling is an essential part of the business and customer service. In addition, strict compliance regulations make it necessary for businesses to handle sensitive documents, especially customer data, properly. Documents can come in a variety of formats, including digital forms or […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/build-custom-amazon-sagemaker-pytorch-models-for-real-time-handwriting-text-recognition/",
          "publishedOn": "2021-12-16T20:02:18.000Z",
          "wordCount": 3639,
          "title": "Build custom Amazon SageMaker PyTorch models for real-time handwriting text recognition"
        },
        {
          "id": "eb7a81155a6bd57d09b8fff92c593ab73c284a36",
          "author": "Yu Liu",
          "description": "Natural language processing (NLP) has been a hot topic in the AI field for some time. As current NLP models get larger and larger, data scientists and developers struggle to set up the infrastructure for such growth of model size. For faster training time, distributed training across multiple machines is a natural choice for developers. […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/achieve-35-faster-training-with-hugging-face-deep-learning-containers-on-amazon-sagemaker/",
          "publishedOn": "2021-12-15T22:46:01.000Z",
          "wordCount": 2398,
          "title": "Achieve 35% faster training with Hugging Face Deep Learning Containers on Amazon SageMaker"
        },
        {
          "id": "cc2fe3b5037e1b1ecc49be195de0c301bbf5f531",
          "author": "Raul Diaz Garcia",
          "description": "Building accurate computer vision models to detect objects in images requires deep knowledge of each step in the process—from labeling, processing, and preparing the training and validation data, to making the right model choice and tuning the model’s hyperparameters adequately to achieve the maximum accuracy. Fortunately, these complex steps are simplified by Amazon Rekognition Custom […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/build-a-computer-vision-model-using-amazon-rekognition-custom-labels-and-compare-the-results-with-a-custom-trained-tensorflow-model/",
          "publishedOn": "2021-12-15T20:14:55.000Z",
          "wordCount": 2593,
          "title": "Build a computer vision model using Amazon Rekognition Custom Labels and compare the results with a custom trained TensorFlow model"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2201.04543",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pastur_L/0/1/0/all/0/1\">Leonid Pastur</a>",
          "description": "The paper deals with the distribution of singular values of the input-output\nJacobian of deep untrained neural networks in the limit of their infinite\nwidth. The Jacobian is the product of random matrices where the independent\nrectangular weight matrices alternate with diagonal matrices whose entries\ndepend on the corresponding column of the nearest neighbor weight matrix. The\nproblem was considered in \\cite{Pe-Co:18} for the Gaussian weights and biases\nand also for the weights that are Haar distributed orthogonal matrices and\nGaussian biases. Basing on a free probability argument, it was claimed that in\nthese cases the singular value distribution of the Jacobian in the limit of\ninfinite width (matrix size) coincides with that of the analog of the Jacobian\nwith special random but weight independent diagonal matrices, the case well\nknown in random matrix theory. The claim was rigorously proved in\n\\cite{Pa-Sl:21} for a quite general class of weights and biases with i.i.d.\n(including Gaussian) entries by using a version of the techniques of random\nmatrix theory. In this paper we use another version of the techniques to\njustify the claim for random Haar distributed weight matrices and Gaussian\nbiases.",
          "link": "http://arxiv.org/abs/2201.04543",
          "publishedOn": "2022-01-14T00:38:51.494Z",
          "wordCount": 627,
          "title": "Eigenvalue Distribution of Large Random Matrices Arising in Deep Neural Networks: Orthogonal Case. (arXiv:2201.04543v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2112.07891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xingjian Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bilei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zejun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-kirkpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubnov_S/0/1/0/all/0/1\">Shlomo Dubnov</a>",
          "description": "Deep learning techniques for separating audio into different sound sources\nface several challenges. Standard architectures require training separate\nmodels for different types of audio sources. Although some universal separators\nemploy a single model to target multiple sources, they have difficulty\ngeneralizing to unseen sources. In this paper, we propose a three-component\npipeline to train a universal audio source separator from a large, but\nweakly-labeled dataset: AudioSet. First, we propose a transformer-based sound\nevent detection system for processing weakly-labeled training data. Second, we\ndevise a query-based audio separation model that leverages this data for model\ntraining. Third, we design a latent embedding processor to encode queries that\nspecify audio targets for separation, allowing for zero-shot generalization.\nOur approach uses a single model for source separation of multiple sound types,\nand relies solely on weakly-labeled data for training. In addition, the\nproposed audio separator can be used in a zero-shot setting, learning to\nseparate types of audio sources that were never seen in training. To evaluate\nthe separation performance, we test our model on MUSDB18, while training on the\ndisjoint AudioSet. We further verify the zero-shot performance by conducting\nanother experiment on audio source types that are held-out from training. The\nmodel achieves comparable Source-to-Distortion Ratio (SDR) performance to\ncurrent supervised models in both cases.",
          "link": "http://arxiv.org/abs/2112.07891",
          "publishedOn": "2022-01-14T00:38:51.476Z",
          "wordCount": 712,
          "title": "Zero-shot Audio Source Separation through Query-based Learning from Weakly-labeled Data. (arXiv:2112.07891v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.10325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongquan Yang</a>",
          "description": "One-step abductive multi-target learning (OSAMTL) is an approach proposed to\nhandle complex noisy labels. However, OSAMTL is not suitable for the situation\nwhere diverse noisy samples (DNS) are provided for a learning task. In this\npaper, giving definition of DNS, we propose one-step abductive multi-target\nlearning with DNS (OSAMTL-DNS) to expand the original OSAMTL to a wider range\nof tasks that handle complex noisy labels. Applying OSAMTL-DNS to tumour\nsegmentation for breast cancer in medical histopathology whole slide image\nanalysis, we show that OSAMTL-DNS is able to enable various state-of-the-art\napproaches for learning from noisy labels to achieve significantly more\nrational predictions.",
          "link": "http://arxiv.org/abs/2110.10325",
          "publishedOn": "2022-01-14T00:38:51.386Z",
          "wordCount": null,
          "title": "One-Step Abductive Multi-Target Learning with Diverse Noisy Samples: An Application to Tumour Segmentation for Breast Cancer. (arXiv:2110.10325v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.04487",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Grandits_T/0/1/0/all/0/1\">Thomas Grandits</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pezzuto_S/0/1/0/all/0/1\">Simone Pezzuto</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Plank_G/0/1/0/all/0/1\">Gernot Plank</a>",
          "description": "The field of cardiac electrophysiology tries to abstract, describe and\nfinally model the electrical characteristics of a heartbeat. With recent\nadvances in cardiac electrophysiology, models have become more powerful and\ndescriptive as ever. However, to advance to the field of inverse\nelectrophysiological modeling, i.e. creating models from electrical\nmeasurements such as the ECG, the less investigated field of smoothness of the\nsimulated ECGs w.r.t. model parameters need to be further explored. The present\npaper discusses smoothness in terms of the whole pipeline which describes how\nfrom physiological parameters, we arrive at the simulated ECG. Employing such a\npipeline, we create a test-bench of a simplified idealized left ventricle model\nand demonstrate the most important factors for efficient inverse modeling\nthrough smooth cost functionals. Such knowledge will be important for designing\nand creating inverse models in future optimization and machine learning\nmethods.",
          "link": "http://arxiv.org/abs/2201.04487",
          "publishedOn": "2022-01-14T00:38:51.385Z",
          "wordCount": null,
          "title": "Smoothness and continuity of cost functionals for ECG mismatch computation. (arXiv:2201.04487v1 [physics.med-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2112.11161",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Kumar_A/0/1/0/all/0/1\">Akshat Kumar</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Sarovar_M/0/1/0/all/0/1\">Mohan Sarovar</a>",
          "description": "We introduce an algorithm for computing geodesics on sampled manifolds that\nrelies on simulation of quantum dynamics on a graph embedding of the sampled\ndata. Our approach exploits classic results in semiclassical analysis and the\nquantum-classical correspondence, and forms a basis for techniques to learn the\nmanifold from which a dataset is sampled, and subsequently for nonlinear\ndimensionality reduction of high-dimensional datasets. We illustrate the new\nalgorithm with data sampled from model manifolds and also by a clustering\ndemonstration based on COVID-19 mobility data. Finally, our method reveals\ninteresting connections between the discretization provided by data sampling\nand quantization.",
          "link": "http://arxiv.org/abs/2112.11161",
          "publishedOn": "2022-01-14T00:38:51.385Z",
          "wordCount": null,
          "title": "Manifold learning via quantum dynamics. (arXiv:2112.11161v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.04227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohtaj_S/0/1/0/all/0/1\">Salar Mohtaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_V/0/1/0/all/0/1\">Vera Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_S/0/1/0/all/0/1\">Sebastian M&#xf6;ller</a>",
          "description": "The detection of hate speech online has become an important task, as\noffensive language such as hurtful, obscene and insulting content can harm\nmarginalized people or groups. This paper presents TU Berlin team experiments\nand results on the task 1A and 1B of the shared task on hate speech and\noffensive content identification in Indo-European languages 2021. The success\nof different Natural Language Processing models is evaluated for the respective\nsubtasks throughout the competition. We tested different models based on\nrecurrent neural networks in word and character levels and transfer learning\napproaches based on Bert on the provided dataset by the competition. Among the\ntested models that have been used for the experiments, the transfer\nlearning-based models achieved the best results in both subtasks.",
          "link": "http://arxiv.org/abs/2201.04227",
          "publishedOn": "2022-01-14T00:38:51.382Z",
          "wordCount": 564,
          "title": "A Feature Extraction based Model for Hate Speech Identification. (arXiv:2201.04227v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Putzel_P/0/1/0/all/0/1\">Preston Putzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Scott Lee</a>",
          "description": "Applying standard machine learning approaches for classification can produce\nunequal results across different demographic groups. When then used in\nreal-world settings, these inequities can have negative societal impacts. This\nhas motivated the development of various approaches to fair classification with\nmachine learning models in recent years. In this paper, we consider the problem\nof modifying the predictions of a blackbox machine learning classifier in order\nto achieve fairness in a multiclass setting. To accomplish this, we extend the\n'post-processing' approach in Hardt et al. 2016, which focuses on fairness for\nbinary classification, to the setting of fair multiclass classification. We\nexplore when our approach produces both fair and accurate predictions through\nsystematic synthetic experiments and also evaluate discrimination-fairness\ntradeoffs on several publicly available real-world application datasets. We\nfind that overall, our approach produces minor drops in accuracy and enforces\nfairness when the number of individuals in the dataset is high relative to the\nnumber of classes and protected groups.",
          "link": "http://arxiv.org/abs/2201.04461",
          "publishedOn": "2022-01-14T00:38:51.361Z",
          "wordCount": 574,
          "title": "Blackbox Post-Processing for Multiclass Fairness. (arXiv:2201.04461v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.11369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Allerbo_O/0/1/0/all/0/1\">Oskar Allerbo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jornsten_R/0/1/0/all/0/1\">Rebecka J&#xf6;rnsten</a>",
          "description": "Non-parametric, additive models are able to capture complex data dependencies\nin a flexible, yet interpretable way. However, choosing the format of the\nadditive components often requires non-trivial data exploration. Here, as an\nalternative, we propose PrAda-net, a one-hidden-layer neural network, trained\nwith proximal gradient descent and adaptive lasso. PrAda-net automatically\nadjusts the size and architecture of the neural network to reflect the\ncomplexity and structure of the data. The compact network obtained by PrAda-net\ncan be translated to additive model components, making it suitable for\nnon-parametric statistical modelling with automatic model selection. We\ndemonstrate PrAda-net on simulated data, where wecompare the test error\nperformance, variable importance and variable subset identification properties\nof PrAda-net to other lasso-based regularization approaches for neural\nnetworks. We also apply PrAda-net to the massive U.K. black smoke data set, to\ndemonstrate how PrAda-net can be used to model complex and heterogeneous data\nwith spatial and temporal components. In contrast to classical, statistical\nnon-parametric approaches, PrAda-net requires no preliminary modeling to select\nthe functional forms of the additive components, yet still results in an\ninterpretable model representation.",
          "link": "http://arxiv.org/abs/2012.11369",
          "publishedOn": "2022-01-14T00:38:51.355Z",
          "wordCount": 631,
          "title": "Flexible, Non-parametric Modeling Using Regularized Neural Networks. (arXiv:2012.11369v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.04455",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bjorklund_A/0/1/0/all/0/1\">Anton Bj&#xf6;rklund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makela_J/0/1/0/all/0/1\">Jarmo M&#xe4;kel&#xe4;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puolamaki_K/0/1/0/all/0/1\">Kai Puolam&#xe4;ki</a>",
          "description": "Existing explanation methods for black-box supervised learning models\ngenerally work by building local models that explain the models behaviour for a\nparticular data item. It is possible to make global explanations, but the\nexplanations may have low fidelity for complex models. Most of the prior work\non explainable models has been focused on classification problems, with less\nattention on regression.\n\nWe propose a new manifold visualization method, SLISEMAP, that at the same\ntime finds local explanations for all of the data items and builds a\ntwo-dimensional visualization of model space such that the data items explained\nby the same model are projected nearby. We provide an open source\nimplementation of our methods, implemented by using GPU-optimized PyTorch\nlibrary. SLISEMAP works both on classification and regression models.\n\nWe compare SLISEMAP to most popular dimensionality reduction methods and some\nlocal explanation methods. We provide mathematical derivation of our problem\nand show that SLISEMAP provides fast and stable visualizations that can be used\nto explain and understand black box regression and classification models.",
          "link": "http://arxiv.org/abs/2201.04455",
          "publishedOn": "2022-01-14T00:38:51.347Z",
          "wordCount": null,
          "title": "SLISEMAP: Explainable Dimensionality Reduction. (arXiv:2201.04455v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04545",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Imaizumi_M/0/1/0/all/0/1\">Masaaki Imaizumi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schmidt_Hieber_J/0/1/0/all/0/1\">Johannes Schmidt-Hieber</a>",
          "description": "The classical statistical learning theory says that fitting too many\nparameters leads to overfitting and poor performance. That modern deep neural\nnetworks generalize well despite a large number of parameters contradicts this\nfinding and constitutes a major unsolved problem towards explaining the success\nof deep learning. The implicit regularization induced by stochastic gradient\ndescent (SGD) has been regarded to be important, but its specific principle is\nstill unknown. In this work, we study how the local geometry of the energy\nlandscape around local minima affects the statistical properties of SGD with\nGaussian gradient noise. We argue that under reasonable assumptions, the local\ngeometry forces SGD to stay close to a low dimensional subspace and that this\ninduces implicit regularization and results in tighter bounds on the\ngeneralization error for deep neural networks. To derive generalization error\nbounds for neural networks, we first introduce a notion of stagnation sets\naround the local minima and impose a local essential convexity property of the\npopulation risk. Under these conditions, lower bounds for SGD to remain in\nthese stagnation sets are derived. If stagnation occurs, we derive a bound on\nthe generalization error of deep neural networks involving the spectral norms\nof the weight matrices but not the number of network parameters. Technically,\nour proofs are based on controlling the change of parameter values in the SGD\niterates and local uniform convergence of the empirical loss functions based on\nthe entropy of suitable neighborhoods around local minima. Our work attempts to\nbetter connect non-convex optimization and generalization analysis with uniform\nconvergence.",
          "link": "http://arxiv.org/abs/2201.04545",
          "publishedOn": "2022-01-14T00:38:51.347Z",
          "wordCount": null,
          "title": "On generalization bounds for deep networks based on loss surface implicit regularization. (arXiv:2201.04545v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02990",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kamulete_V/0/1/0/all/0/1\">Vathy M. Kamulete</a>",
          "description": "Statistical tests for dataset shift are susceptible to false alarms: they are\nsensitive to minor differences when there is in fact adequate sample coverage\nand predictive performance. We propose instead a framework to detect adverse\ndataset shifts based on outlier scores, $\\texttt{D-SOS}$ for short.\n$\\texttt{D-SOS}$ holds that the new (test) sample is not substantively worse\nthan the reference (training) sample, and not that the two are equal. The key\nidea is to reduce observations to outlier scores and compare contamination\nrates at varying weighted thresholds. Users can define what $\\it{worse}$ means\nin terms of relevant notions of outlyingness, including proxies for predictive\nperformance. Compared to tests of equal distribution, our approach is uniquely\ntailored to serve as a robust metric for model monitoring and data validation.\nWe show how versatile and practical $\\texttt{D-SOS}$ is on a wide range of real\nand simulated data.",
          "link": "http://arxiv.org/abs/2107.02990",
          "publishedOn": "2022-01-14T00:38:51.330Z",
          "wordCount": 594,
          "title": "Test for non-negligible adverse shifts. (arXiv:2107.02990v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.00801",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Landeros_A/0/1/0/all/0/1\">Alfonso Landeros</a>, <a href=\"http://arxiv.org/find/math/1/au:+Padilla_O/0/1/0/all/0/1\">Oscar Hernan Madrid Padilla</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhou_H/0/1/0/all/0/1\">Hua Zhou</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lange_K/0/1/0/all/0/1\">Kenneth Lange</a>",
          "description": "The current paper studies the problem of minimizing a loss\n$f(\\boldsymbol{x})$ subject to constraints of the form\n$\\boldsymbol{D}\\boldsymbol{x} \\in S$, where $S$ is a closed set, convex or not,\nand $\\boldsymbol{D}$ is a matrix that fuses parameters. Fusion constraints can\ncapture smoothness, sparsity, or more general constraint patterns. To tackle\nthis generic class of problems, we combine the Beltrami-Courant penalty method\nwith the proximal distance principle. The latter is driven by minimization of\npenalized objectives\n$f(\\boldsymbol{x})+\\frac{\\rho}{2}\\text{dist}(\\boldsymbol{D}\\boldsymbol{x},S)^2$\ninvolving large tuning constants $\\rho$ and the squared Euclidean distance of\n$\\boldsymbol{D}\\boldsymbol{x}$ from $S$. The next iterate\n$\\boldsymbol{x}_{n+1}$ of the corresponding proximal distance algorithm is\nconstructed from the current iterate $\\boldsymbol{x}_n$ by minimizing the\nmajorizing surrogate function\n$f(\\boldsymbol{x})+\\frac{\\rho}{2}\\|\\boldsymbol{D}\\boldsymbol{x}-\\mathcal{P}_{S}(\\boldsymbol{D}\\boldsymbol{x}_n)\\|^2$.\nFor fixed $\\rho$ and a subanalytic loss $f(\\boldsymbol{x})$ and a subanalytic\nconstraint set $S$, we prove convergence to a stationary point. Under stronger\nassumptions, we provide convergence rates and demonstrate linear local\nconvergence. We also construct a steepest descent (SD) variant to avoid costly\nlinear system solves. To benchmark our algorithms, we compare against the\nalternating direction method of multipliers (ADMM). Our extensive numerical\ntests include problems on metric projection, convex regression, convex\nclustering, total variation image denoising, and projection of a matrix to a\ngood condition number. These experiments demonstrate the superior speed and\nacceptable accuracy of our steepest variant on high-dimensional problems.",
          "link": "http://arxiv.org/abs/2009.00801",
          "publishedOn": "2022-01-14T00:38:51.320Z",
          "wordCount": 675,
          "title": "Extensions to the Proximal Distance Method of Constrained Optimization. (arXiv:2009.00801v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.04424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Melaragno_A/0/1/0/all/0/1\">Anthony Melaragno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casey_W/0/1/0/all/0/1\">William Casey</a>",
          "description": "Ransomware has been an ongoing issue since the early 1990s. In recent times\nransomware has spread from traditional computational resources to\ncyber-physical systems and industrial controls. We devised a series of\nexperiments in which virtual instances are infected with ransomware. We\ninstrumented the instances and collected resource utilization data across a\nvariety of metrics (CPU, Memory, Disk Utility). We design a change point\ndetection and learning method for identifying ransomware execution. Finally we\nevaluate and demonstrate its ability to detect ransomware efficiently in a\ntimely manner when trained on a minimal set of samples. Our results represent a\nstep forward for defense, and we conclude with further remarks for the path\nforward.",
          "link": "http://arxiv.org/abs/2201.04424",
          "publishedOn": "2022-01-14T00:38:51.293Z",
          "wordCount": 533,
          "title": "Detecting Ransomware Execution in a Timely Manner. (arXiv:2201.04424v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04370",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yelu Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Huang Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lian Zhang</a>",
          "description": "Alzheimer's disease (AD) is an irreversible neurode generative disease of the\nbrain.The disease may causes memory loss, difficulty communicating and\ndisorientation. For the diagnosis of Alzheimer's disease, a series of scales\nare often needed to evaluate the diagnosis clinically, which not only increases\nthe workload of doctors, but also makes the results of diagnosis highly\nsubjective. Therefore, for Alzheimer's disease, imaging means to find early\ndiagnostic markers has become a top priority.\n\nIn this paper, we propose a novel 3DMgNet architecture which is a unified\nframework of multigrid and convolutional neural network to diagnose Alzheimer's\ndisease (AD). The model is trained using an open dataset (ADNI dataset) and\nthen test with a smaller dataset of ours. Finally, the model achieved 92.133%\naccuracy for AD vs NC classification and significantly reduced the model\nparameters.",
          "link": "http://arxiv.org/abs/2201.04370",
          "publishedOn": "2022-01-14T00:38:51.287Z",
          "wordCount": 560,
          "title": "Predicting Alzheimer's Disease Using 3DMgNet. (arXiv:2201.04370v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.01510",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1\">Enrico Fini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1\">St&#xe9;phane Lathuili&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1\">Enver Sangineto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>",
          "description": "Continual Learning (CL) aims to develop agents emulating the human ability to\nsequentially learn new tasks while being able to retain knowledge obtained from\npast experiences. In this paper, we introduce the novel problem of\nMemory-Constrained Online Continual Learning (MC-OCL) which imposes strict\nconstraints on the memory overhead that a possible algorithm can use to avoid\ncatastrophic forgetting. As most, if not all, previous CL methods violate these\nconstraints, we propose an algorithmic solution to MC-OCL: Batch-level\nDistillation (BLD), a regularization-based CL approach, which effectively\nbalances stability and plasticity in order to learn from data streams, while\npreserving the ability to solve old tasks through distillation. Our extensive\nexperimental evaluation, conducted on three publicly available benchmarks,\nempirically demonstrates that our approach successfully addresses the MC-OCL\nproblem and achieves comparable accuracy to prior distillation methods\nrequiring higher memory overhead.",
          "link": "http://arxiv.org/abs/2008.01510",
          "publishedOn": "2022-01-14T00:38:51.272Z",
          "wordCount": 606,
          "title": "Online Continual Learning under Extreme Memory Constraints. (arXiv:2008.01510v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.04429",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chenbo Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emadikhiav_M/0/1/0/all/0/1\">Mohsen Emadikhiav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_L/0/1/0/all/0/1\">Leonardo Lozano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergman_D/0/1/0/all/0/1\">David Bergman</a>",
          "description": "There is a recent proliferation of research on the integration of machine\nlearning and optimization. One expansive area within this research stream is\npredictive-model embedded optimization, which uses pre-trained predictive\nmodels for the objective function of an optimization problem, so that features\nof the predictive models become decision variables in the optimization problem.\nDespite a recent surge in publications in this area, one aspect of this\ndecision-making pipeline that has been largely overlooked is training\nrelevance, i.e., ensuring that solutions to the optimization problem should be\nsimilar to the data used to train the predictive models. In this paper, we\npropose constraints designed to enforce training relevance, and show through a\ncollection of experimental results that adding the suggested constraints\nsignificantly improves the quality of solutions obtained.",
          "link": "http://arxiv.org/abs/2201.04429",
          "publishedOn": "2022-01-14T00:38:51.254Z",
          "wordCount": 549,
          "title": "Careful! Training Relevance is Real. (arXiv:2201.04429v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhmoginov_A/0/1/0/all/0/1\">Andrey Zhmoginov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandler_M/0/1/0/all/0/1\">Mark Sandler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vladymyrov_M/0/1/0/all/0/1\">Max Vladymyrov</a>",
          "description": "In this work we propose a HyperTransformer, a transformer-based model for\nfew-shot learning that generates weights of a convolutional neural network\n(CNN) directly from support samples. Since the dependence of a small generated\nCNN model on a specific task is encoded by a high-capacity transformer model,\nwe effectively decouple the complexity of the large task space from the\ncomplexity of individual tasks. Our method is particularly effective for small\ntarget CNN architectures where learning a fixed universal task-independent\nembedding is not optimal and better performance is attained when the\ninformation about the task can modulate all model parameters. For larger models\nwe discover that generating the last layer alone allows us to produce\ncompetitive or better results than those obtained with state-of-the-art methods\nwhile being end-to-end differentiable. Finally, we extend our approach to a\nsemi-supervised regime utilizing unlabeled samples in the support set and\nfurther improving few-shot performance.",
          "link": "http://arxiv.org/abs/2201.04182",
          "publishedOn": "2022-01-14T00:38:51.248Z",
          "wordCount": 571,
          "title": "HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning. (arXiv:2201.04182v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mason_I/0/1/0/all/0/1\">Ian Mason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Starke_S/0/1/0/all/0/1\">Sebastian Starke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1\">Taku Komura</a>",
          "description": "Controlling the manner in which a character moves in a real-time animation\nsystem is a challenging task with useful applications. Existing style transfer\nsystems require access to a reference content motion clip, however, in\nreal-time systems the future motion content is unknown and liable to change\nwith user input. In this work we present a style modelling system that uses an\nanimation synthesis network to model motion content based on local motion\nphases. An additional style modulation network uses feature-wise\ntransformations to modulate style in real-time. To evaluate our method, we\ncreate and release a new style modelling dataset, 100STYLE, containing over 4\nmillion frames of stylised locomotion data in 100 different styles that present\na number of challenges for existing systems. To model these styles, we extend\nthe local phase calculation with a contact-free formulation. In comparison to\nother methods for real-time style modelling, we show our system is more robust\nand efficient in its style representation while improving motion quality.",
          "link": "http://arxiv.org/abs/2201.04439",
          "publishedOn": "2022-01-14T00:38:51.241Z",
          "wordCount": 600,
          "title": "Real-Time Style Modelling of Human Locomotion via Feature-Wise Transformations and Local Motion Phases. (arXiv:2201.04439v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ovaisi_Z/0/1/0/all/0/1\">Zohreh Ovaisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinecke_S/0/1/0/all/0/1\">Shelby Heinecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheleva_E/0/1/0/all/0/1\">Elena Zheleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>",
          "description": "Robust machine learning is an increasingly important topic that focuses on\ndeveloping models resilient to various forms of imperfect data. Due to the\npervasiveness of recommender systems in online technologies, researchers have\ncarried out several robustness studies focusing on data sparsity and profile\ninjection attacks. Instead, we propose a more holistic view of robustness for\nrecommender systems that encompasses multiple dimensions - robustness with\nrespect to sub-populations, transformations, distributional disparity, attack,\nand data sparsity. While there are several libraries that allow users to\ncompare different recommender system models, there is no software library for\ncomprehensive robustness evaluation of recommender system models under\ndifferent scenarios. As our main contribution, we present a robustness\nevaluation toolkit, Robustness Gym for RecSys (RGRecSys --\nhttps://www.github.com/salesforce/RGRecSys), that allows us to quickly and\nuniformly evaluate the robustness of recommender system models.",
          "link": "http://arxiv.org/abs/2201.04399",
          "publishedOn": "2022-01-14T00:38:51.227Z",
          "wordCount": 593,
          "title": "RGRecSys: A Toolkit for Robustness Evaluation of Recommender Systems. (arXiv:2201.04399v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luque_Cerpa_A/0/1/0/all/0/1\">Alejandro Luque-Cerpa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Naranjo_M/0/1/0/all/0/1\">Miguel A. Guti&#xe9;rrez-Naranjo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardenas_Montes_M/0/1/0/all/0/1\">Miguel C&#xe1;rdenas-Montes</a>",
          "description": "The improvement of air-quality in urban areas is one of the main concerns of\npublic government bodies. This concern emerges from the evidence between the\nair quality and the public health. Major efforts from government bodies in this\narea include monitoring and forecasting systems, banning more pollutant motor\nvehicles, and traffic limitations during the periods of low-quality air. In\nthis work, a proposal for dynamic prices in regulated parking services is\npresented. The dynamic prices in parking service must discourage motor vehicles\nparking when low-quality episodes are predicted. For this purpose, diverse deep\nlearning strategies are evaluated. They have in common the use of collective\nair-quality measurements for forecasting labels about air quality in the city.\nThe proposal is evaluated by using economic parameters and deep learning\nquality criteria at Madrid (Spain).",
          "link": "http://arxiv.org/abs/2201.04188",
          "publishedOn": "2022-01-14T00:38:51.199Z",
          "wordCount": null,
          "title": "Dynamic Price of Parking Service based on Deep Learning. (arXiv:2201.04188v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04200",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Brynjolfsson_E/0/1/0/all/0/1\">Erik Brynjolfsson</a>",
          "description": "In 1950, Alan Turing proposed an imitation game as the ultimate test of\nwhether a machine was intelligent: could a machine imitate a human so well that\nits answers to questions indistinguishable from a human. Ever since, creating\nintelligence that matches human intelligence has implicitly or explicitly been\nthe goal of thousands of researchers, engineers, and entrepreneurs. The\nbenefits of human-like artificial intelligence (HLAI) include soaring\nproductivity, increased leisure, and perhaps most profoundly, a better\nunderstanding of our own minds.\n\nBut not all types of AI are human-like. In fact, many of the most powerful\nsystems are very different from humans. So an excessive focus on developing and\ndeploying HLAI can lead us into a trap. As machines become better substitutes\nfor human labor, workers lose economic and political bargaining power and\nbecome increasingly dependent on those who control the technology. In contrast,\nwhen AI is focused on augmenting humans rather than mimicking them, then humans\nretain the power to insist on a share of the value created. Furthermore,\naugmentation creates new capabilities and new products and services, ultimately\ngenerating far more value than merely human-like AI. While both types of AI can\nbe enormously beneficial, there are currently excess incentives for automation\nrather than augmentation among technologists, business executives, and\npolicymakers.",
          "link": "http://arxiv.org/abs/2201.04200",
          "publishedOn": "2022-01-14T00:38:51.198Z",
          "wordCount": null,
          "title": "The Turing Trap: The Promise & Peril of Human-Like Artificial Intelligence. (arXiv:2201.04200v1 [econ.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingjing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1\">Osvaldo Simeone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>",
          "description": "Wall-clock convergence time and communication load are key performance\nmetrics for the distributed implementation of stochastic gradient descent (SGD)\nin parameter server settings. Communication-adaptive distributed Adam (CADA)\nhas been recently proposed as a way to reduce communication load via the\nadaptive selection of workers. CADA is subject to performance degradation in\nterms of wall-clock convergence time in the presence of stragglers. This paper\nproposes a novel scheme named grouping-based CADA (G-CADA) that retains the\nadvantages of CADA in reducing the communication load, while increasing the\nrobustness to stragglers at the cost of additional storage at the workers.\nG-CADA partitions the workers into groups of workers that are assigned the same\ndata shards. Groups are scheduled adaptively at each iteration, and the server\nonly waits for the fastest worker in each selected group. We provide analysis\nand experimental results to elaborate the significant gains on the wall-clock\ntime, as well as communication load and computation load, of G-CADA over other\nbenchmark schemes.",
          "link": "http://arxiv.org/abs/2201.04301",
          "publishedOn": "2022-01-14T00:38:51.197Z",
          "wordCount": null,
          "title": "Adaptive Worker Grouping For Communication-Efficient and Straggler-Tolerant Distributed SGD. (arXiv:2201.04301v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04286",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianxing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_B/0/1/0/all/0/1\">Bingsheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>",
          "description": "Evolutionary Algorithms (EAs) and Deep Reinforcement Learning (DRL) have\nrecently been combined to integrate the advantages of the two solutions for\nbetter policy learning. However, in existing hybrid methods, EA is used to\ndirectly train the policy network, which will lead to sample inefficiency and\nunpredictable impact on the policy performance. To better integrate these two\napproaches and avoid the drawbacks caused by the introduction of EA, we devote\nourselves to devising a more efficient and reasonable method of combining EA\nand DRL. In this paper, we propose Evolutionary Action Selection-Twin Delayed\nDeep Deterministic Policy Gradient (EAS-TD3), a novel combination of EA and\nDRL. In EAS, we focus on optimizing the action chosen by the policy network and\nattempt to obtain high-quality actions to guide policy learning through an\nevolutionary algorithm. We conduct several experiments on challenging\ncontinuous control tasks. The result shows that EAS-TD3 shows superior\nperformance over other state-of-art methods.",
          "link": "http://arxiv.org/abs/2201.04286",
          "publishedOn": "2022-01-14T00:38:51.190Z",
          "wordCount": 579,
          "title": "Evolutionary Action Selection for Gradient-based Policy Learning. (arXiv:2201.04286v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04243",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Duan_C/0/1/0/all/0/1\">Chenru Duan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chu_D/0/1/0/all/0/1\">Daniel B. K. Chu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nandy_A/0/1/0/all/0/1\">Aditya Nandy</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kulik_H/0/1/0/all/0/1\">Heather J. Kulik</a>",
          "description": "Appropriately identifying and treating molecules and materials with\nsignificant multi-reference (MR) character is crucial for achieving high data\nfidelity in virtual high throughput screening (VHTS). Nevertheless, most VHTS\nis carried out with approximate density functional theory (DFT) using a single\nfunctional. Despite development of numerous MR diagnostics, the extent to which\na single value of such a diagnostic indicates MR effect on chemical property\nprediction is not well established. We evaluate MR diagnostics of over 10,000\ntransition metal complexes (TMCs) and compare to those in organic molecules. We\nreveal that only some MR diagnostics are transferable across these materials\nspaces. By studying the influence of MR character on chemical properties (i.e.,\nMR effect) that involves multiple potential energy surfaces (i.e., adiabatic\nspin splitting, $\\Delta E_\\mathrm{H-L}$, and ionization potential, IP), we\nobserve that cancellation in MR effect outweighs accumulation. Differences in\nMR character are more important than the total degree of MR character in\npredicting MR effect in property prediction. Motivated by this observation, we\nbuild transfer learning models to directly predict CCSD(T)-level adiabatic\n$\\Delta E_\\mathrm{H-L}$ and IP from lower levels of theory. By combining these\nmodels with uncertainty quantification and multi-level modeling, we introduce a\nmulti-pronged strategy that accelerates data acquisition by at least a factor\nof three while achieving chemical accuracy (i.e., 1 kcal/mol) for robust VHTS.",
          "link": "http://arxiv.org/abs/2201.04243",
          "publishedOn": "2022-01-14T00:38:51.184Z",
          "wordCount": 661,
          "title": "Two Wrongs Can Make a Right: A Transfer Learning Approach for Chemical Discovery with Chemical Accuracy. (arXiv:2201.04243v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Otovic_E/0/1/0/all/0/1\">Erik Otovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Njirjak_M/0/1/0/all/0/1\">Marko Njirjak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jozinovic_D/0/1/0/all/0/1\">Dario Jozinovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausa_G/0/1/0/all/0/1\">Goran Mau&#x161;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michelini_A/0/1/0/all/0/1\">Alberto Michelini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stajduhar_I/0/1/0/all/0/1\">Ivan &#x160;tajduhar</a>",
          "description": "In practice, it is very demanding and sometimes impossible to collect\ndatasets of tagged data large enough to successfully train a machine learning\nmodel, and one possible solution to this problem is transfer learning. This\nstudy aims to assess how transferable are the features between different\ndomains of time series data and under which conditions. The effects of transfer\nlearning are observed in terms of predictive performance of the models and\ntheir convergence rate during training. In our experiment, we use reduced data\nsets of 1,500 and 9,000 data instances to mimic real world conditions. Using\nthe same scaled-down datasets, we trained two sets of machine learning models:\nthose that were trained with transfer learning and those that were trained from\nscratch. Four machine learning models were used for the experiment. Transfer of\nknowledge was performed within the same domain of application (seismology), as\nwell as between mutually different domains of application (seismology, speech,\nmedicine, finance). We observe the predictive performance of the models and the\nconvergence rate during the training. In order to confirm the validity of the\nobtained results, we repeated the experiments seven times and applied\nstatistical tests to confirm the significance of the results. The general\nconclusion of our study is that transfer learning is very likely to either\nincrease or not negatively affect the predictive performance of the model or\nits convergence rate. The collected data is analysed in more details to\ndetermine which source and target domains are compatible for transfer of\nknowledge. We also analyse the effect of target dataset size and the selection\nof model and its hyperparameters on the effects of transfer learning.",
          "link": "http://arxiv.org/abs/2201.04449",
          "publishedOn": "2022-01-14T00:38:51.174Z",
          "wordCount": 718,
          "title": "Intra-domain and cross-domain transfer learning for time series data -- How transferable are the features?. (arXiv:2201.04449v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chunheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedapati_T/0/1/0/all/0/1\">Tejaswini Pedapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianxi Gao</a>",
          "description": "Efficient model selection for identifying a suitable pre-trained neural\nnetwork to a downstream task is a fundamental yet challenging task in deep\nlearning. Current practice requires expensive computational costs in model\ntraining for performance prediction. In this paper, we propose a novel\nframework for neural network selection by analyzing the governing dynamics over\nsynaptic connections (edges) during training. Our framework is built on the\nfact that back-propagation during neural network training is equivalent to the\ndynamical evolution of synaptic connections. Therefore, a converged neural\nnetwork is associated with an equilibrium state of a networked system composed\nof those edges. To this end, we construct a network mapping $\\phi$, converting\na neural network $G_A$ to a directed line graph $G_B$ that is defined on those\nedges in $G_A$. Next, we derive a neural capacitance metric $\\beta_{\\rm eff}$\nas a predictive measure universally capturing the generalization capability of\n$G_A$ on the downstream task using only a handful of early training results. We\ncarried out extensive experiments using 17 popular pre-trained ImageNet models\nand five benchmark datasets, including CIFAR10, CIFAR100, SVHN, Fashion MNIST\nand Birds, to evaluate the fine-tuning performance of our framework. Our neural\ncapacitance metric is shown to be a powerful indicator for model selection\nbased only on early training results and is more efficient than\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2201.04194",
          "publishedOn": "2022-01-14T00:38:51.154Z",
          "wordCount": 662,
          "title": "Neural Capacitance: A New Perspective of Neural Network Selection via Edge Dynamics. (arXiv:2201.04194v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2111.05177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zhengyang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin-Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Shaojie Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>",
          "description": "This paper focuses on training implicit models of infinite layers.\nSpecifically, previous works employ implicit differentiation and solve the\nexact gradient for the backward propagation. However, is it necessary to\ncompute such an exact but expensive gradient for training? In this work, we\npropose a novel gradient estimate for implicit models, named phantom gradient,\nthat 1) forgoes the costly computation of the exact gradient; and 2) provides\nan update direction empirically preferable to the implicit model training. We\ntheoretically analyze the condition under which an ascent direction of the loss\nlandscape could be found, and provide two specific instantiations of the\nphantom gradient based on the damped unrolling and Neumann series. Experiments\non large-scale tasks demonstrate that these lightweight phantom gradients\nsignificantly accelerate the backward passes in training implicit models by\nroughly 1.7 times, and even boost the performance over approaches based on the\nexact gradient on ImageNet.",
          "link": "http://arxiv.org/abs/2111.05177",
          "publishedOn": "2022-01-14T00:38:51.148Z",
          "wordCount": 620,
          "title": "On Training Implicit Models. (arXiv:2111.05177v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.11067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1\">Claudiu Musat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>",
          "description": "Using personalized explanations to support recommendations has been shown to\nincrease trust and perceived quality. However, to actually obtain better\nrecommendations, there needs to be a means for users to modify the\nrecommendation criteria by interacting with the explanation. We present a novel\ntechnique using aspect markers that learns to generate personalized\nexplanations of recommendations from review texts, and we show that human users\nsignificantly prefer these explanations over those produced by state-of-the-art\ntechniques. Our work's most important innovation is that it allows users to\nreact to a recommendation by critiquing the textual explanation: removing\n(symmetrically adding) certain aspects they dislike or that are no longer\nrelevant (symmetrically that are of interest). The system updates its user\nmodel and the resulting recommendations according to the critique. This is\nbased on a novel unsupervised critiquing method for single- and multi-step\ncritiquing with textual explanations. Experiments on two real-world datasets\nshow that our system is the first to achieve good performance in adapting to\nthe preferences expressed in multi-step critiquing.",
          "link": "http://arxiv.org/abs/2005.11067",
          "publishedOn": "2022-01-14T00:38:51.141Z",
          "wordCount": 645,
          "title": "Interacting with Explanations through Critiquing. (arXiv:2005.11067v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.04279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Younes_A/0/1/0/all/0/1\">Abdelrahman Younes</a>",
          "description": "Recent work on audio-visual navigation targets a single static sound in\nnoise-free audio environments and struggles to generalize to unheard sounds. We\nintroduce the novel dynamic audio-visual navigation benchmark in which an\nembodied AI agent must catch a moving sound source in an unmapped environment\nin the presence of distractors and noisy sounds. We propose an end-to-end\nreinforcement learning approach that relies on a multi-modal architecture that\nfuses the spatial audio-visual information from a binaural audio signal and\nspatial occupancy maps to encode the features needed to learn a robust\nnavigation policy for our new complex task settings. We demonstrate that our\napproach outperforms the current state-of-the-art with better generalization to\nunheard sounds and better robustness to noisy scenarios on the two challenging\n3D scanned real-world datasets Replica and Matterport3D, for the static and\ndynamic audio-visual navigation benchmarks. Our novel benchmark will be made\navailable at this http URL",
          "link": "http://arxiv.org/abs/2201.04279",
          "publishedOn": "2022-01-14T00:38:51.134Z",
          "wordCount": 591,
          "title": "Dynamical Audio-Visual Navigation: Catching Unheard Moving Sound Sources in Unmapped 3D Environments. (arXiv:2201.04279v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krieg_S/0/1/0/all/0/1\">Steven J. Krieg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_C/0/1/0/all/0/1\">Christian W. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_R/0/1/0/all/0/1\">Rusha Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chawla_N/0/1/0/all/0/1\">Nitesh V. Chawla</a>",
          "description": "Dozens of terrorist attacks are perpetrated in the United States every year,\noften causing fatalities and other significant damage. Toward the end of better\nunderstanding and mitigating these attacks, we present a set of machine\nlearning models that learn from localized news data in order to predict whether\na terrorist attack will occur on a given calendar date and in a given state.\nThe best model--a Random Forest that learns from a novel variable-length moving\naverage representation of the feature space--achieves area under the receiver\noperating characteristic scores $> .667$ on four of the five states that were\nimpacted most by terrorism between 2015 and 2018. Our key findings include that\nmodeling terrorism as a set of independent events, rather than as a continuous\nprocess, is a fruitful approach--especially when the events are sparse and\ndissimilar. Additionally, our results highlight the need for localized models\nthat account for differences between locations. From a machine learning\nperspective, we found that the Random Forest model outperformed several deep\nmodels on our multimodal, noisy, and imbalanced data set, thus demonstrating\nthe efficacy of our novel feature representation method in such a context. We\nalso show that its predictions are relatively robust to time gaps between\nattacks and observed characteristics of the attacks. Finally, we analyze\nfactors that limit model performance, which include a noisy feature space and\nsmall amount of available data. These contributions provide an important\nfoundation for the use of machine learning in efforts against terrorism in the\nUnited States and beyond.",
          "link": "http://arxiv.org/abs/2201.04292",
          "publishedOn": "2022-01-14T00:38:51.110Z",
          "wordCount": 677,
          "title": "Predicting Terrorist Attacks in the United States using Localized News Data. (arXiv:2201.04292v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04207",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jensen_R/0/1/0/all/0/1\">Rasmus Jensen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>",
          "description": "Money laundering is a profound, global problem. Nonetheless, there is little\nstatistical and machine learning research on the topic. In this paper, we focus\non anti-money laundering in banks. To help organize existing research in the\nfield, we propose a unifying terminology and provide a review of the\nliterature. This is structured around two central tasks: (i) client risk\nprofiling and (ii) suspicious behavior flagging. We find that client risk\nprofiling is characterized by diagnostics, i.e., efforts to find and explain\nrisk factors. Suspicious behavior flagging, on the other hand, is characterized\nby non-disclosed features and hand-crafted risk indices. Finally, we discuss\ndirections for future research. One major challenge is the lack of public data\nsets. This may, potentially, be addressed by synthetic data generation. Other\npossible research directions include semi-supervised and deep learning,\ninterpretability and fairness of the results.",
          "link": "http://arxiv.org/abs/2201.04207",
          "publishedOn": "2022-01-14T00:38:51.103Z",
          "wordCount": 563,
          "title": "Fighting Money-Laundering with Statistics and Machine Learning: An Introduction and Review. (arXiv:2201.04207v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hecht_G/0/1/0/all/0/1\">Grant Hecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+KrisshnaKumar_P/0/1/0/all/0/1\">Prajit KrisshnaKumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Raj K. Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Souma Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botta_E/0/1/0/all/0/1\">Eleonora M. Botta</a>",
          "description": "Tether-net launched from a chaser spacecraft provides a promising method to\ncapture and dispose of large space debris in orbit. This tether-net system is\nsubject to several sources of uncertainty in sensing and actuation that affect\nthe performance of its net launch and closing control. Earlier\nreliability-based optimization approaches to design control actions however\nremain challenging and computationally prohibitive to generalize over varying\nlaunch scenarios and target (debris) state relative to the chaser. To search\nfor a general and reliable control policy, this paper presents a reinforcement\nlearning framework that integrates a proximal policy optimization (PPO2)\napproach with net dynamics simulations. The latter allows evaluating the\nepisodes of net-based target capture, and estimate the capture quality index\nthat serves as the reward feedback to PPO2. Here, the learned policy is\ndesigned to model the timing of the net closing action based on the state of\nthe moving net and the target, under any given launch scenario. A stochastic\nstate transition model is considered in order to incorporate synthetic\nuncertainties in state estimation and launch actuation. Along with notable\nreward improvement during training, the trained policy demonstrates capture\nperformance (over a wide range of launch/target scenarios) that is close to\nthat obtained with reliability-based optimization run over an individual\nscenario.",
          "link": "http://arxiv.org/abs/2201.04180",
          "publishedOn": "2022-01-14T00:38:51.097Z",
          "wordCount": 687,
          "title": "Learning Robust Policies for Generalized Debris Capture with an Automated Tether-Net System. (arXiv:2201.04180v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chuang_C/0/1/0/all/0/1\">Ching-Yao Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hjelm_R/0/1/0/all/0/1\">R Devon Hjelm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1\">Vibhav Vineet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Neel Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1\">Stefanie Jegelka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yale Song</a>",
          "description": "Contrastive learning relies on an assumption that positive pairs contain\nrelated views, e.g., patches of an image or co-occurring multimodal signals of\na video, that share certain underlying information about an instance. But what\nif this assumption is violated? The literature suggests that contrastive\nlearning produces suboptimal representations in the presence of noisy views,\ne.g., false positive pairs with no apparent shared information. In this work,\nwe propose a new contrastive loss function that is robust against noisy views.\nWe provide rigorous theoretical justifications by showing connections to robust\nsymmetric losses for noisy binary classification and by establishing a new\ncontrastive bound for mutual information maximization based on the Wasserstein\ndistance measure. The proposed loss is completely modality-agnostic and a\nsimple drop-in replacement for the InfoNCE loss, which makes it easy to apply\nto existing contrastive frameworks. We show that our approach provides\nconsistent improvements over the state-of-the-art on image, video, and graph\ncontrastive learning benchmarks that exhibit a variety of real-world noise\npatterns.",
          "link": "http://arxiv.org/abs/2201.04309",
          "publishedOn": "2022-01-14T00:38:51.090Z",
          "wordCount": 596,
          "title": "Robust Contrastive Learning against Noisy Views. (arXiv:2201.04309v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.00693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pengyu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_M/0/1/0/all/0/1\">Martin Renqiang Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinghan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malon_C/0/1/0/all/0/1\">Christopher Malon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>",
          "description": "Learning disentangled representations of natural language is essential for\nmany NLP tasks, e.g., conditional text generation, style transfer, personalized\ndialogue systems, etc. Similar problems have been studied extensively for other\nforms of data, such as images and videos. However, the discrete nature of\nnatural language makes the disentangling of textual representations more\nchallenging (e.g., the manipulation over the data space cannot be easily\nachieved). Inspired by information theory, we propose a novel method that\neffectively manifests disentangled representations of text, without any\nsupervision on semantics. A new mutual information upper bound is derived and\nleveraged to measure dependence between style and content. By minimizing this\nupper bound, the proposed method induces style and content embeddings into two\nindependent low-dimensional spaces. Experiments on both conditional text\ngeneration and text-style transfer demonstrate the high quality of our\ndisentangled representation in terms of content and style preservation.",
          "link": "http://arxiv.org/abs/2006.00693",
          "publishedOn": "2022-01-14T00:38:51.083Z",
          "wordCount": 628,
          "title": "Improving Disentangled Text Representation Learning with Information-Theoretic Guidance. (arXiv:2006.00693v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.04469",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kato_M/0/1/0/all/0/1\">Masahiro Kato</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ariu_K/0/1/0/all/0/1\">Kaito Ariu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Imaizumi_M/0/1/0/all/0/1\">Masaaki Imaizumi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Uehara_M/0/1/0/all/0/1\">Masatoshi Uehara</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nomura_M/0/1/0/all/0/1\">Masahiro Nomura</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Qin_a/0/1/0/all/0/1\">and Chao Qin</a>",
          "description": "We consider the fixed-budget best arm identification problem in two-armed\nGaussian bandits with unknown variances. The tightest lower bound on the\ncomplexity and an algorithm whose performance guarantee matches the lower bound\nhave long been open problems when the variances are unknown and when the\nalgorithm is agnostic to the optimal proportion of the arm draws. In this\npaper, we propose a strategy comprising a sampling rule with randomized\nsampling (RS) following the estimated target allocation probabilities of arm\ndraws and a recommendation rule using the augmented inverse probability\nweighting (AIPW) estimator, which is often used in the causal inference\nliterature. We refer to our strategy as the RS-AIPW strategy. In the\ntheoretical analysis, we first derive a large deviation principle for\nmartingales, which can be used when the second moment converges in mean, and\napply it to our proposed strategy. Then, we show that the proposed strategy is\nasymptotically optimal in the sense that the probability of misidentification\nachieves the lower bound by Kaufmann et al. (2016) when the sample size becomes\ninfinitely large and the gap between the two arms goes to zero.",
          "link": "http://arxiv.org/abs/2201.04469",
          "publishedOn": "2022-01-14T00:38:51.063Z",
          "wordCount": 641,
          "title": "Optimal Fixed-Budget Best Arm Identification using the Augmented Inverse Probability Estimator in Two-Armed Gaussian Bandits with Unknown Variances. (arXiv:2201.04469v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xue Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yali Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ru_B/0/1/0/all/0/1\">Binxin Ru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haifeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>",
          "description": "The Elo rating system is widely adopted to evaluate the skills of (chess)\ngame and sports players. Recently it has been also integrated into machine\nlearning algorithms in evaluating the performance of computerised AI agents.\nHowever, an accurate estimation of the Elo rating (for the top players) often\nrequires many rounds of competitions, which can be expensive to carry out. In\nthis paper, to improve the sample efficiency of the Elo evaluation (for top\nplayers), we propose an efficient online match scheduling algorithm.\nSpecifically, we identify and match the top players through a dueling bandits\nframework and tailor the bandit algorithm to the gradient-based update of Elo.\nWe show that it reduces the per-step memory and time complexity to constant,\ncompared to the traditional likelihood maximization approaches requiring $O(t)$\ntime. Our algorithm has a regret guarantee of $\\tilde{O}(\\sqrt{T})$, sublinear\nin the number of competition rounds and has been extended to the\nmultidimensional Elo ratings for handling intransitive games. We empirically\ndemonstrate that our method achieves superior convergence speed and time\nefficiency on a variety of gaming tasks.",
          "link": "http://arxiv.org/abs/2201.04480",
          "publishedOn": "2022-01-14T00:38:51.056Z",
          "wordCount": 605,
          "title": "Learning to Identify Top Elo Ratings: A Dueling Bandits Approach. (arXiv:2201.04480v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04234",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Saurabh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_S/0/1/0/all/0/1\">Sivaraman Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1\">Behnam Neyshabur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedghi_H/0/1/0/all/0/1\">Hanie Sedghi</a>",
          "description": "Real-world machine learning deployments are characterized by mismatches\nbetween the source (training) and target (test) distributions that may cause\nperformance drops. In this work, we investigate methods for predicting the\ntarget domain accuracy using only labeled source data and unlabeled target\ndata. We propose Average Thresholded Confidence (ATC), a practical method that\nlearns a threshold on the model's confidence, predicting accuracy as the\nfraction of unlabeled examples for which model confidence exceeds that\nthreshold. ATC outperforms previous methods across several model architectures,\ntypes of distribution shifts (e.g., due to synthetic corruptions, dataset\nreproduction, or novel subpopulations), and datasets (Wilds, ImageNet, Breeds,\nCIFAR, and MNIST). In our experiments, ATC estimates target performance\n$2$-$4\\times$ more accurately than prior methods. We also explore the\ntheoretical foundations of the problem, proving that, in general, identifying\nthe accuracy is just as hard as identifying the optimal predictor and thus, the\nefficacy of any method rests upon (perhaps unstated) assumptions on the nature\nof the shift. Finally, analyzing our method on some toy distributions, we\nprovide insights concerning when it works.",
          "link": "http://arxiv.org/abs/2201.04234",
          "publishedOn": "2022-01-14T00:38:51.047Z",
          "wordCount": 597,
          "title": "Leveraging Unlabeled Data to Predict Out-of-Distribution Performance. (arXiv:2201.04234v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaoyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhongwang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhi-Qin John Xu</a>",
          "description": "Understanding the structure of loss landscape of deep neural networks\n(DNNs)is obviously important. In this work, we prove an embedding principle\nthat the loss landscape of a DNN \"contains\" all the critical points of all the\nnarrower DNNs. More precisely, we propose a critical embedding such that any\ncritical point, e.g., local or global minima, of a narrower DNN can be embedded\nto a critical point/hyperplane of the target DNN with higher degeneracy and\npreserving the DNN output function. The embedding structure of critical points\nis independent of loss function and training data, showing a stark difference\nfrom other nonconvex problems such as protein-folding. Empirically, we find\nthat a wide DNN is often attracted by highly-degenerate critical points that\nare embedded from narrow DNNs. The embedding principle provides an explanation\nfor the general easy optimization of wide DNNs and unravels a potential\nimplicit low-complexity regularization during the training. Overall, our work\nprovides a skeleton for the study of loss landscape of DNNs and its\nimplication, by which a more exact and comprehensive understanding can be\nanticipated in the near",
          "link": "http://arxiv.org/abs/2105.14573",
          "publishedOn": "2022-01-14T00:38:51.040Z",
          "wordCount": 642,
          "title": "Embedding Principle of Loss Landscape of Deep Neural Networks. (arXiv:2105.14573v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.04416",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Das_S/0/1/0/all/0/1\">Sauman Das</a>",
          "description": "Glioblastoma Multiforme (GBM) is a malignant brain cancer forming around 48%\nof al brain and Central Nervous System (CNS) cancers. It is estimated that\nannually over 13,000 deaths occur in the US due to GBM, making it crucial to\nhave early diagnosis systems that can lead to predictable and effective\ntreatment. The most common treatment after GBM diagnosis is chemotherapy, which\nworks by sending rapidly dividing cells to apoptosis. However, this form of\ntreatment is not effective when the MGMT promoter sequence is methylated, and\ninstead leads to severe side effects decreasing patient survivability.\nTherefore, it is important to be able to identify the MGMT promoter methylation\nstatus through non-invasive magnetic resonance imaging (MRI) based machine\nlearning (ML) models. This is accomplished using the Brain Tumor Segmentation\n(BraTS) 2021 dataset, which was recently used for an international Kaggle\ncompetition. We developed four primary models - two radiomic models and two CNN\nmodels - each solving the binary classification task with progressive\nimprovements. We built a novel ML model termed as the Intermediate State\nGenerator which was used to normalize the slice thicknesses of all MRI scans.\nWith further improvements, our best model was able to achieve performance\nsignificantly ($p < 0.05$) better than the best performing Kaggle model with a\n6% increase in average cross-validation accuracy. This improvement could\npotentially lead to a more informed choice of chemotherapy as a treatment\noption, prolonging lives of thousands of patients with GBM each year.",
          "link": "http://arxiv.org/abs/2201.04416",
          "publishedOn": "2022-01-14T00:38:51.033Z",
          "wordCount": 679,
          "title": "Optimizing Prediction of MGMT Promoter Methylation from MRI Scans using Adversarial Learning. (arXiv:2201.04416v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04604",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_X/0/1/0/all/0/1\">Xiaobing Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Haoxi Zhan</a>",
          "description": "Multi-view subspace clustering has conventionally focused on integrating\nheterogeneous feature descriptions to capture higher-dimensional information.\nOne popular strategy is to generate a common subspace from different views and\nthen apply graph-based approaches to deal with clustering. However, the\nperformance of these methods is still subject to two limitations, namely the\nmultiple views fusion pattern and the connection between the fusion process and\nclustering tasks. To address these problems, we propose a novel multi-view\nsubspace clustering framework via fine-grained graph learning, which can tell\nthe consistency of local structures between different views and integrate all\nviews more delicately than previous weight regularizations. Different from\nother models in the literature, the point-level graph regularization and the\nreformulation of spectral clustering are introduced to perform graphs fusion\nand learn the shared cluster structure together. Extensive experiments on five\nreal-world datasets show that the proposed framework has comparable performance\nto the SOTA algorithms.",
          "link": "http://arxiv.org/abs/2201.04604",
          "publishedOn": "2022-01-14T00:38:51.004Z",
          "wordCount": 562,
          "title": "Fine-grained Graph Learning for Multi-view Subspace Clustering. (arXiv:2201.04604v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Faycal_T/0/1/0/all/0/1\">Tarek Faycal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zito_C/0/1/0/all/0/1\">Claudio Zito</a>",
          "description": "In this work we present a preliminary investigation of a novel algorithm\ncalled Dyna-T. In reinforcement learning (RL) a planning agent has its own\nrepresentation of the environment as a model. To discover an optimal policy to\ninteract with the environment, the agent collects experience in a trial and\nerror fashion. Experience can be used for learning a better model or improve\ndirectly the value function and policy. Typically separated, Dyna-Q is an\nhybrid approach which, at each iteration, exploits the real experience to\nupdate the model as well as the value function, while planning its action using\nsimulated data from its model. However, the planning process is computationally\nexpensive and strongly depends on the dimensionality of the state-action space.\nWe propose to build a Upper Confidence Tree (UCT) on the simulated experience\nand search for the best action to be selected during the on-line learning\nprocess. We prove the effectiveness of our proposed method on a set of\npreliminary tests on three testbed environments from Open AI. In contrast to\nDyna-Q, Dyna-T outperforms state-of-the-art RL agents in the stochastic\nenvironments by choosing a more robust action selection strategy.",
          "link": "http://arxiv.org/abs/2201.04502",
          "publishedOn": "2022-01-14T00:38:50.987Z",
          "wordCount": 607,
          "title": "Dyna-T: Dyna-Q and Upper Confidence Bounds Applied to Trees. (arXiv:2201.04502v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yingjie Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaning Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shaoliang Peng</a>",
          "description": "Self-supervised representation learning (SSL) on biomedical networks provides\nnew opportunities for drug discovery which is lack of available biological or\nclinic phenotype. However, how to effectively combine multiple SSL models is\nchallenging and rarely explored. Therefore, we propose multi-task joint\nstrategies of self-supervised representation learning on biomedical networks\nfor drug discovery, named MSSL2drug. We design six basic SSL tasks that are\ninspired by various modality features including structures, semantics, and\nattributes in biomedical heterogeneous networks. In addition, fifteen\ncombinations of multiple tasks are evaluated by a graph attention-based\nadversarial multi-task learning framework in two drug discovery scenarios. The\nresults suggest two important findings. (1) The combinations of multimodal\ntasks achieve the best performance compared to other multi-task joint\nstrategies. (2) The joint training of local and global SSL tasks yields higher\nperformance than random task combinations. Therefore, we conjecture that the\nmultimodal and local-global combination strategies can be regarded as a\nguideline for multi-task SSL to drug discovery.",
          "link": "http://arxiv.org/abs/2201.04437",
          "publishedOn": "2022-01-14T00:38:50.973Z",
          "wordCount": 599,
          "title": "Multi-task Joint Strategies of Self-supervised Representation Learning on Biomedical Networks for Drug Discovery. (arXiv:2201.04437v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04288",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shen Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_X/0/1/0/all/0/1\">Xuehan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhichao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>",
          "description": "Video understanding requires reasoning at multiple spatiotemporal resolutions\n-- from short fine-grained motions to events taking place over longer\ndurations. Although transformer architectures have recently advanced the\nstate-of-the-art, they have not explicitly modelled different spatiotemporal\nresolutions. To this end, we present Multiview Transformers for Video\nRecognition (MTV). Our model consists of separate encoders to represent\ndifferent views of the input video with lateral connections to fuse information\nacross views. We present thorough ablation studies of our model and show that\nMTV consistently performs better than single-view counterparts in terms of\naccuracy and computational cost across a range of model sizes. Furthermore, we\nachieve state-of-the-art results on five standard datasets, and improve even\nfurther with large-scale pretraining. We will release code and pretrained\ncheckpoints.",
          "link": "http://arxiv.org/abs/2201.04288",
          "publishedOn": "2022-01-14T00:38:49.760Z",
          "wordCount": 552,
          "title": "Multiview Transformers for Video Recognition. (arXiv:2201.04288v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baena_R/0/1/0/all/0/1\">Raphael Baena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drumetz_L/0/1/0/all/0/1\">Lucas Drumetz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gripon_V/0/1/0/all/0/1\">Vincent Gripon</a>",
          "description": "Mixup is a data-dependent regularization technique that consists in linearly\ninterpolating input samples and associated outputs. It has been shown to\nimprove accuracy when used to train on standard machine learning datasets.\nHowever, authors have pointed out that Mixup can produce out-of-distribution\nvirtual samples and even contradictions in the augmented training set,\npotentially resulting in adversarial effects. In this paper, we introduce Local\nMixup in which distant input samples are weighted down when computing the loss.\nIn constrained settings we demonstrate that Local Mixup can create a trade-off\nbetween bias and variance, with the extreme cases reducing to vanilla training\nand classical Mixup. Using standardized computer vision benchmarks , we also\nshow that Local Mixup can improve test accuracy.",
          "link": "http://arxiv.org/abs/2201.04368",
          "publishedOn": "2022-01-14T00:38:49.754Z",
          "wordCount": 532,
          "title": "Preventing Manifold Intrusion with Locality: Local Mixup. (arXiv:2201.04368v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04397",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yan_H/0/1/0/all/0/1\">Hanshu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfeng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y. F. Tan</a>",
          "description": "This work systematically investigates the adversarial robustness of deep\nimage denoisers (DIDs), i.e, how well DIDs can recover the ground truth from\nnoisy observations degraded by adversarial perturbations. Firstly, to evaluate\nDIDs' robustness, we propose a novel adversarial attack, namely\nObservation-based Zero-mean Attack ({\\sc ObsAtk}), to craft adversarial\nzero-mean perturbations on given noisy images. We find that existing DIDs are\nvulnerable to the adversarial noise generated by {\\sc ObsAtk}. Secondly, to\nrobustify DIDs, we propose an adversarial training strategy, hybrid adversarial\ntraining ({\\sc HAT}), that jointly trains DIDs with adversarial and\nnon-adversarial noisy data to ensure that the reconstruction quality is high\nand the denoisers around non-adversarial data are locally smooth. The resultant\nDIDs can effectively remove various types of synthetic and adversarial noise.\nWe also uncover that the robustness of DIDs benefits their generalization\ncapability on unseen real-world noise. Indeed, {\\sc HAT}-trained DIDs can\nrecover high-quality clean images from real-world noise even without training\non real noisy data. Extensive experiments on benchmark datasets, including\nSet68, PolyU, and SIDD, corroborate the effectiveness of {\\sc ObsAtk} and {\\sc\nHAT}.",
          "link": "http://arxiv.org/abs/2201.04397",
          "publishedOn": "2022-01-14T00:38:49.748Z",
          "wordCount": 613,
          "title": "Towards Adversarially Robust Deep Image Denoising. (arXiv:2201.04397v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04315",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Axelrod_B/0/1/0/all/0/1\">Brian Axelrod</a>, <a href=\"http://arxiv.org/find/math/1/au:+Garg_S/0/1/0/all/0/1\">Shivam Garg</a>, <a href=\"http://arxiv.org/find/math/1/au:+Han_Y/0/1/0/all/0/1\">Yanjun Han</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sharan_V/0/1/0/all/0/1\">Vatsal Sharan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Valiant_G/0/1/0/all/0/1\">Gregory Valiant</a>",
          "description": "Given $n$ i.i.d. samples drawn from an unknown distribution $P$, when is it\npossible to produce a larger set of $n+m$ samples which cannot be distinguished\nfrom $n+m$ i.i.d. samples drawn from $P$? (Axelrod et al. 2019) formalized this\nquestion as the sample amplification problem, and gave optimal amplification\nprocedures for discrete distributions and Gaussian location models. However,\nthese procedures and associated lower bounds are tailored to the specific\ndistribution classes, and a general statistical understanding of sample\namplification is still largely missing. In this work, we place the sample\namplification problem on a firm statistical foundation by deriving generally\napplicable amplification procedures, lower bound techniques and connections to\nexisting statistical notions. Our techniques apply to a large class of\ndistributions including the exponential family, and establish a rigorous\nconnection between sample amplification and distribution learning.",
          "link": "http://arxiv.org/abs/2201.04315",
          "publishedOn": "2022-01-14T00:38:49.740Z",
          "wordCount": 571,
          "title": "On the Statistical Complexity of Sample Amplification. (arXiv:2201.04315v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04229",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Essa_A/0/1/0/all/0/1\">Almabrok Essa</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kotte_H/0/1/0/all/0/1\">Hari Kotte</a>",
          "description": "Brain signals constitute the information that are processed by millions of\nbrain neurons (nerve cells and brain cells). These brain signals can be\nrecorded and analyzed using various of non-invasive techniques such as the\nElectroencephalograph (EEG), Magneto-encephalograph (MEG) as well as\nbrain-imaging techniques such as Magnetic Resonance Imaging (MRI), Computed\nTomography (CT) and others, which will be discussed briefly in this paper. This\npaper discusses about the currently emerging techniques such as the usage of\ndifferent Deep Learning (DL) algorithms for the analysis of these brain signals\nand how these algorithms will be helpful in determining the neurological status\nof a person by applying the signal decoding strategy.",
          "link": "http://arxiv.org/abs/2201.04229",
          "publishedOn": "2022-01-14T00:38:49.732Z",
          "wordCount": 557,
          "title": "Brain Signals Analysis Based Deep Learning Methods: Recent advances in the study of non-invasive brain signals. (arXiv:2201.04229v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04584",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Asad_M/0/1/0/all/0/1\">Muhammad Asad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fidon_L/0/1/0/all/0/1\">Lucas Fidon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>",
          "description": "Automatic segmentation of lung lesions associated with COVID-19 in CT images\nrequires large amount of annotated volumes. Annotations mandate expert\nknowledge and are time-intensive to obtain through fully manual segmentation\nmethods. Additionally, lung lesions have large inter-patient variations, with\nsome pathologies having similar visual appearance as healthy lung tissues. This\nposes a challenge when applying existing semi-automatic interactive\nsegmentation techniques for data labelling. To address these challenges, we\npropose an efficient convolutional neural networks (CNNs) that can be learned\nonline while the annotator provides scribble-based interaction. To accelerate\nlearning from only the samples labelled through user-interactions, a\npatch-based approach is used for training the network. Moreover, we use\nweighted cross-entropy loss to address the class imbalance that may result from\nuser-interactions. During online inference, the learned network is applied to\nthe whole input volume using a fully convolutional approach. We compare our\nproposed method with state-of-the-art and show that it outperforms existing\nmethods on the task of annotating lung lesions associated with COVID-19,\nachieving 16% higher Dice score while reducing execution time by 3$\\times$ and\nrequiring 9000 lesser scribbles-based labelled voxels. Due to the online\nlearning aspect, our approach adapts quickly to user input, resulting in high\nquality segmentation labels. Source code will be made available upon\nacceptance.",
          "link": "http://arxiv.org/abs/2201.04584",
          "publishedOn": "2022-01-14T00:38:49.725Z",
          "wordCount": 695,
          "title": "ECONet: Efficient Convolutional Online Likelihood Network for Scribble-based Interactive Segmentation. (arXiv:2201.04584v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Callegaro_D/0/1/0/all/0/1\">Davide Callegaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Restuccia_F/0/1/0/all/0/1\">Francesco Restuccia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levorato_M/0/1/0/all/0/1\">Marco Levorato</a>",
          "description": "Mobile devices increasingly rely on object detection (OD) through deep neural\nnetworks (DNNs) to perform critical tasks. Due to their high complexity, the\nexecution of these DNNs requires excessive time and energy. Low-complexity\nobject tracking (OT) can be used with OD, where the latter is periodically\napplied to generate \"fresh\" references for tracking. However, the frames\nprocessed with OD incur large delays, which may make the reference outdated and\ndegrade tracking quality. Herein, we propose to use edge computing in this\ncontext, and establish parallel OT (at the mobile device) and OD (at the edge\nserver) processes that are resilient to large OD latency. We propose Katch-Up,\na novel tracking mechanism that improves the system resilience to excessive OD\ndelay. However, while Katch-Up significantly improves performance, it also\nincreases the computing load of the mobile device. Hence, we design SmartDet, a\nlow-complexity controller based on deep reinforcement learning (DRL) that\nlearns controlling the trade-off between resource utilization and OD\nperformance. SmartDet takes as input context-related information related to the\ncurrent video content and the current network conditions to optimize frequency\nand type of OD offloading, as well as Katch-Up utilization. We extensively\nevaluate SmartDet on a real-world testbed composed of a JetSon Nano as mobile\ndevice and a GTX 980 Ti as edge server, connected through a Wi-Fi link.\nExperimental results show that SmartDet achieves an optimal balance between\ntracking performance - mean Average Recall (mAR) and resource usage. With\nrespect to a baseline with full Katch-Upusage and maximum channel usage, we\nstill increase mAR by 4% while using 50% less of the channel and 30% power\nresources associated with Katch-Up. With respect to a fixed strategy using\nminimal resources, we increase mAR by 20% while using Katch-Up on 1/3 of the\nframes.",
          "link": "http://arxiv.org/abs/2201.04235",
          "publishedOn": "2022-01-14T00:38:49.683Z",
          "wordCount": 739,
          "title": "SmartDet: Context-Aware Dynamic Control of Edge Task Offloading for Mobile Object Detection. (arXiv:2201.04235v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04506",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moshkov_M/0/1/0/all/0/1\">Mikhail Moshkov</a>",
          "description": "In this paper, based on results of exact learning and test theory, we study\narbitrary infinite binary information systems each of which consists of an\ninfinite set of elements and an infinite set of two-valued functions\n(attributes) defined on the set of elements. We consider the notion of a\nproblem over information system, which is described by a finite number of\nattributes: for a given element, we should recognize values of these\nattributes. As algorithms for problem solving, we consider decision trees of\ntwo types: (i) using only proper hypotheses (an analog of proper equivalence\nqueries from exact learning), and (ii) using both attributes and proper\nhypotheses. As time complexity, we study the depth of decision trees. In the\nworst case, with the growth of the number of attributes in the problem\ndescription, the minimum depth of decision trees of both types either is\nbounded from above by a constant or grows as a logarithm, or linearly. Based on\nthese results and results obtained earlier for attributes and arbitrary\nhypotheses, we divide the set of all infinite binary information systems into\nseven complexity classes.",
          "link": "http://arxiv.org/abs/2201.04506",
          "publishedOn": "2022-01-14T00:38:49.677Z",
          "wordCount": 600,
          "title": "Exact learning and test theory. (arXiv:2201.04506v1 [cs.CC])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04302",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+He_D/0/1/0/all/0/1\">Da He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1\">Jiasheng Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shang_X/0/1/0/all/0/1\">Xiaoyu Shang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_J/0/1/0/all/0/1\">Jiajia Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1\">Sung-Liang Chen</a>",
          "description": "As a hybrid imaging technology, photoacoustic microscopy (PAM) imaging\nsuffers from noise due to the maximum permissible exposure of laser intensity,\nattenuation of ultrasound in the tissue, and the inherent noise of the\ntransducer. De-noising is a post-processing method to reduce noise, and PAM\nimage quality can be recovered. However, previous de-noising techniques usually\nheavily rely on mathematical priors as well as manually selected parameters,\nresulting in unsatisfactory and slow de-noising performance for different noisy\nimages, which greatly hinders practical and clinical applications. In this\nwork, we propose a deep learning-based method to remove complex noise from PAM\nimages without mathematical priors and manual selection of settings for\ndifferent input images. An attention enhanced generative adversarial network is\nused to extract image features and remove various noises. The proposed method\nis demonstrated on both synthetic and real datasets, including phantom (leaf\nveins) and in vivo (mouse ear blood vessels and zebrafish pigment) experiments.\nThe results show that compared with previous PAM de-noising methods, our method\nexhibits good performance in recovering images qualitatively and\nquantitatively. In addition, the de-noising speed of 0.016 s is achieved for an\nimage with $256\\times256$ pixels. Our approach is effective and practical for\nthe de-noising of PAM images.",
          "link": "http://arxiv.org/abs/2201.04302",
          "publishedOn": "2022-01-14T00:38:49.670Z",
          "wordCount": 637,
          "title": "De-Noising of Photoacoustic Microscopy Images by Deep Learning. (arXiv:2201.04302v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+dAscoli_S/0/1/0/all/0/1\">St&#xe9;phane d&#x27;Ascoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamienny_P/0/1/0/all/0/1\">Pierre-Alexandre Kamienny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lample_G/0/1/0/all/0/1\">Guillaume Lample</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charton_F/0/1/0/all/0/1\">Fran&#xe7;ois Charton</a>",
          "description": "Symbolic regression, i.e. predicting a function from the observation of its\nvalues, is well-known to be a challenging task. In this paper, we train\nTransformers to infer the function or recurrence relation underlying sequences\nof integers or floats, a typical task in human IQ tests which has hardly been\ntackled in the machine learning literature. We evaluate our integer model on a\nsubset of OEIS sequences, and show that it outperforms built-in Mathematica\nfunctions for recurrence prediction. We also demonstrate that our float model\nis able to yield informative approximations of out-of-vocabulary functions and\nconstants, e.g. $\\operatorname{bessel0}(x)\\approx\n\\frac{\\sin(x)+\\cos(x)}{\\sqrt{\\pi x}}$ and $1.644934\\approx \\pi^2/6$. An\ninteractive demonstration of our models is provided at https://bit.ly/3niE5FS.",
          "link": "http://arxiv.org/abs/2201.04600",
          "publishedOn": "2022-01-14T00:38:49.646Z",
          "wordCount": 527,
          "title": "Deep Symbolic Regression for Recurrent Sequences. (arXiv:2201.04600v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shuyin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiaochuan Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giem_E/0/1/0/all/0/1\">Elisabeth Giem</a>",
          "description": "Granular-ball computing is an efficient, robust, and scalable learning method\nfor granular computing. The basis of granular-ball computing is the\ngranular-ball generation method. This paper proposes a method for accelerating\nthe granular-ball generation using the division to replace $k$-means. It can\ngreatly improve the efficiency of granular-ball generation while ensuring the\naccuracy similar to the existing method. Besides, a new adaptive method for the\ngranular-ball generation is proposed by considering granular-ball's overlap\neliminating and some other factors. This makes the granular-ball generation\nprocess of parameter-free and completely adaptive in the true sense. In\naddition, this paper first provides the mathematical models for the\ngranular-ball covering. The experimental results on some real data sets\ndemonstrate that the proposed two granular-ball generation methods have similar\naccuracies with the existing method while adaptiveness or acceleration is\nrealized.",
          "link": "http://arxiv.org/abs/2201.04343",
          "publishedOn": "2022-01-14T00:38:49.638Z",
          "wordCount": 557,
          "title": "An Efficient and Adaptive Granular-ball Generation Method in Classification Problem. (arXiv:2201.04343v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.02334",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yaofeng Desmond Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_B/0/1/0/all/0/1\">Biswadip Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Amit Chakraborty</a>",
          "description": "The last few years have witnessed an increased interest in incorporating\nphysics-informed inductive bias in deep learning frameworks. In particular, a\ngrowing volume of literature has been exploring ways to enforce energy\nconservation while using neural networks for learning dynamics from observed\ntime-series data. In this work, we survey ten recently proposed\nenergy-conserving neural network models, including HNN, LNN, DeLaN, SymODEN,\nCHNN, CLNN and their variants. We provide a compact derivation of the theory\nbehind these models and explain their similarities and differences. Their\nperformance are compared in 4 physical systems. We point out the possibility of\nleveraging some of these energy-conserving models to design energy-based\ncontrollers.",
          "link": "http://arxiv.org/abs/2012.02334",
          "publishedOn": "2022-01-14T00:38:46.883Z",
          "wordCount": 596,
          "title": "Benchmarking Energy-Conserving Neural Networks for Learning Dynamics from Data. (arXiv:2012.02334v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11171",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kang_M/0/1/0/all/0/1\">Minsu Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Sungjae Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_I/0/1/0/all/0/1\">Injung Kim</a>",
          "description": "We propose a novel high-fidelity expressive speech synthesis model, UniTTS,\nthat learns and controls overlapping style attributes avoiding interference.\nUniTTS represents multiple style attributes in a single unified embedding space\nby the residuals between the phoneme embeddings before and after applying the\nattributes. The proposed method is especially effective in controlling multiple\nattributes that are difficult to separate cleanly, such as speaker ID and\nemotion, because it minimizes redundancy when adding variance in speaker ID and\nemotion, and additionally, predicts duration, pitch, and energy based on the\nspeaker ID and emotion. In experiments, the visualization results exhibit that\nthe proposed methods learned multiple attributes harmoniously in a manner that\ncan be easily separated again. As well, UniTTS synthesized high-fidelity speech\nsignals controlling multiple style attributes. The synthesized speech samples\nare presented at https://jackson-kang.github.io/paper_works/UniTTS/demos.",
          "link": "http://arxiv.org/abs/2106.11171",
          "publishedOn": "2022-01-14T00:38:46.876Z",
          "wordCount": 601,
          "title": "UniTTS: Residual Learning of Unified Embedding Space for Speech Style Control. (arXiv:2106.11171v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yixin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Austin S. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Undersander_E/0/1/0/all/0/1\">Eric Undersander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rai_A/0/1/0/all/0/1\">Akshara Rai</a>",
          "description": "Manipulation tasks, like loading a dishwasher, can be seen as a sequence of\nspatial constraints and relationships between different objects. We aim to\ndiscover these rules from demonstrations by posing manipulation as a\nclassification problem over a graph, whose nodes represent task-relevant\nentities like objects and goals, and present a graph neural network (GNN)\npolicy architecture for solving this problem from demonstrations. In our\nexperiments, a single GNN policy trained using imitation learning (IL) on 20\nexpert demos can solve blockstacking, rearrangement, and dishwasher loading\ntasks; once the policy has learned the spatial structure, it can generalize to\na larger number of objects, goal configurations, and from simulation to the\nreal world. These experiments show that graphical IL can solve complex\nlong-horizon manipulation problems without requiring detailed task\ndescriptions. Videos can be found at: https://youtu.be/POxaTDAj7aY.",
          "link": "http://arxiv.org/abs/2102.13177",
          "publishedOn": "2022-01-14T00:38:46.869Z",
          "wordCount": 604,
          "title": "Efficient and Interpretable Robot Manipulation with Graph Neural Networks. (arXiv:2102.13177v4 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06712",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Bilkis_M/0/1/0/all/0/1\">M. Bilkis</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Cerezo_M/0/1/0/all/0/1\">M. Cerezo</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Verdon_G/0/1/0/all/0/1\">Guillaume Verdon</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Coles_P/0/1/0/all/0/1\">Patrick J. Coles</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Cincio_L/0/1/0/all/0/1\">Lukasz Cincio</a>",
          "description": "Quantum machine learning (QML) offers a powerful, flexible paradigm for\nprogramming near-term quantum computers, with applications in chemistry,\nmetrology, materials science, data science, and mathematics. Here, one trains\nan ansatz, in the form of a parameterized quantum circuit, to accomplish a task\nof interest. However, challenges have recently emerged suggesting that deep\nansatzes are difficult to train, due to flat training landscapes caused by\nrandomness or by hardware noise. This motivates our work, where we present a\nvariable structure approach to build ansatzes for QML. Our approach, called\nVAns (Variable Ansatz), applies a set of rules to both grow and (crucially)\nremove quantum gates in an informed manner during the optimization.\nConsequently, VAns is ideally suited to mitigate trainability and noise-related\nissues by keeping the ansatz shallow. We employ VAns in the variational quantum\neigensolver for condensed matter and quantum chemistry applications and also in\nthe quantum autoencoder for data compression, showing successful results in all\ncases.",
          "link": "http://arxiv.org/abs/2103.06712",
          "publishedOn": "2022-01-14T00:38:46.861Z",
          "wordCount": 615,
          "title": "A semi-agnostic ansatz with variable structure for quantum machine learning. (arXiv:2103.06712v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.04684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wapnick_S/0/1/0/all/0/1\">Stefan Wapnick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manderson_T/0/1/0/all/0/1\">Travis Manderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meger_D/0/1/0/all/0/1\">David Meger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1\">Gregory Dudek</a>",
          "description": "We present a reward-predictive, model-based deep learning method featuring\ntrajectory-constrained visual attention for use in mapless, local visual\nnavigation tasks. Our method learns to place visual attention at locations in\nlatent image space which follow trajectories caused by vehicle control actions\nto enhance predictive accuracy during planning. The attention model is jointly\noptimized by the task-specific loss and an additional trajectory-constraint\nloss, allowing adaptability yet encouraging a regularized structure for\nimproved generalization and reliability. Importantly, visual attention is\napplied in latent feature map space instead of raw image space to promote\nefficient planning. We validated our model in visual navigation tasks of\nplanning low turbulence, collision-free trajectories in off-road settings and\nhill climbing with locking differentials in the presence of slippery terrain.\nExperiments involved randomized procedural generated simulation and real-world\nenvironments. We found our method improved generalization and learning\nefficiency when compared to no-attention and self-attention alternatives.",
          "link": "http://arxiv.org/abs/2112.04684",
          "publishedOn": "2022-01-14T00:38:46.838Z",
          "wordCount": 637,
          "title": "Trajectory-Constrained Deep Latent Visual Attention for Improved Local Planning in Presence of Heterogeneous Terrain. (arXiv:2112.04684v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aounon Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>",
          "description": "The study of provable adversarial robustness has mostly been limited to\nclassification tasks and models with one-dimensional real-valued outputs. We\nextend the scope of certifiable robustness to problems with more general and\nstructured outputs like sets, images, language, etc. We model the output space\nas a metric space under a distance/similarity function, such as\nintersection-over-union, perceptual similarity, total variation distance, etc.\nSuch models are used in many machine learning problems like image segmentation,\nobject detection, generative models, image/audio-to-text systems, etc. Based on\na robustness technique called randomized smoothing, our $\\textit{center\nsmoothing}$ procedure can produce models with the guarantee that the change in\nthe output, as measured by the distance metric, remains small for any\nnorm-bounded adversarial perturbation of the input. We apply our method to\ncreate certifiably robust models with disparate output spaces - from sets to\nimages - and show that it yields meaningful certificates without significantly\ndegrading the performance of the base model. Code for our experiments is\navailable at: https://github.com/aounon/center-smoothing.",
          "link": "http://arxiv.org/abs/2102.09701",
          "publishedOn": "2022-01-14T00:38:46.831Z",
          "wordCount": 624,
          "title": "Center Smoothing: Certified Robustness for Networks with Structured Outputs. (arXiv:2102.09701v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi-Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>",
          "description": "Generalizable person Re-Identification (ReID) has attracted growing attention\nin recent computer vision community. In this work, we construct a structural\ncausal model among identity labels, identity-specific factors (clothes/shoes\ncolor etc), and domain-specific factors (background, viewpoints etc). According\nto the causal analysis, we propose a novel Domain Invariant Representation\nLearning for generalizable person Re-Identification (DIR-ReID) framework.\nSpecifically, we first propose to disentangle the identity-specific and\ndomain-specific feature spaces, based on which we propose an effective\nalgorithmic implementation for backdoor adjustment, essentially serving as a\ncausal intervention towards the SCM. Extensive experiments have been conducted,\nshowing that DIR-ReID outperforms state-of-the-art methods on large-scale\ndomain generalization ReID benchmarks.",
          "link": "http://arxiv.org/abs/2103.15890",
          "publishedOn": "2022-01-14T00:38:46.822Z",
          "wordCount": 575,
          "title": "Learning Domain Invariant Representations for Generalizable Person Re-Identification. (arXiv:2103.15890v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.10189",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Bialas_P/0/1/0/all/0/1\">Piotr Bia&#x142;as</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Korcyl_P/0/1/0/all/0/1\">Piotr Korcyl</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Stebel_T/0/1/0/all/0/1\">Tomasz Stebel</a>",
          "description": "We provide a deepened study of autocorrelations in Neural Markov Chain Monte\nCarlo simulations, a version of the traditional Metropolis algorithm which\nemploys neural networks to provide independent proposals. We illustrate our\nideas using the two-dimensional Ising model. We propose several estimates of\nautocorrelation times, some inspired by analytical results derived for the\nMetropolized Independent Sampler, which we compare and study as a function of\ninverse temperature $\\beta$. Based on that we propose an alternative loss\nfunction and study its impact on the autocorelation times. Furthermore, we\ninvestigate the impact of imposing system symmetries ($Z_2$ and/or\ntranslational) in the neural network training process on the autocorrelation\ntimes. Eventually, we propose a scheme which incorporates partial heat-bath\nupdates. The impact of the above enhancements is discussed for a $16 \\times 16$\nspin system. The summary of our findings may serve as a guide to the\nimplementation of Neural Markov Chain Monte Carlo simulations of more\ncomplicated models.",
          "link": "http://arxiv.org/abs/2111.10189",
          "publishedOn": "2022-01-14T00:38:46.815Z",
          "wordCount": 617,
          "title": "Analysis of autocorrelation times in Neural Markov Chain Monte Carlo simulations. (arXiv:2111.10189v2 [cond-mat.stat-mech] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10395",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wald_Y/0/1/0/all/0/1\">Yoav Wald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1\">Amir Feder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenfeld_D/0/1/0/all/0/1\">Daniel Greenfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalit_U/0/1/0/all/0/1\">Uri Shalit</a>",
          "description": "Out-of-domain (OOD) generalization is a significant challenge for machine\nlearning models. Many techniques have been proposed to overcome this challenge,\noften focused on learning models with certain invariance properties. In this\nwork, we draw a link between OOD performance and model calibration, arguing\nthat calibration across multiple domains can be viewed as a special case of an\ninvariant representation leading to better OOD generalization. Specifically, we\nshow that under certain conditions, models which achieve \\emph{multi-domain\ncalibration} are provably free of spurious correlations. This leads us to\npropose multi-domain calibration as a measurable and trainable surrogate for\nthe OOD performance of a classifier. We therefore introduce methods that are\neasy to apply and allow practitioners to improve multi-domain calibration by\ntraining or modifying an existing model, leading to better performance on\nunseen domains. Using four datasets from the recently proposed WILDS OOD\nbenchmark, as well as the Colored MNIST dataset, we demonstrate that training\nor tuning models so they are calibrated across multiple domains leads to\nsignificantly improved performance on unseen test domains. We believe this\nintriguing connection between calibration and OOD generalization is promising\nfrom both a practical and theoretical point of view.",
          "link": "http://arxiv.org/abs/2102.10395",
          "publishedOn": "2022-01-14T00:38:46.809Z",
          "wordCount": 717,
          "title": "On Calibration and Out-of-domain Generalization. (arXiv:2102.10395v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.15829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bertsimas_D/0/1/0/all/0/1\">Dimitris Bertsimas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boussioux_L/0/1/0/all/0/1\">L&#xe9;onard Boussioux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carballo_K/0/1/0/all/0/1\">Kimberly Villalobos Carballo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Michael Lingzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paskov_A/0/1/0/all/0/1\">Alex Paskov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paskov_I/0/1/0/all/0/1\">Ivan Paskov</a>",
          "description": "There is much interest in deep learning to solve challenges that arise in\napplying neural network models in real-world environments. In particular, three\nareas have received considerable attention: adversarial robustness, parameter\nsparsity, and output stability. Despite numerous attempts on solving these\nproblems independently, there is very little work addressing the challenges\nsimultaneously. In this paper, we address this problem of constructing holistic\ndeep learning models by proposing a novel formulation that solves these issues\nin combination. Real-world experiments on both tabular and MNIST dataset show\nthat our formulation is able to simultaneously improve the accuracy,\nrobustness, stability, and sparsity over traditional deep learning models among\nmany others.",
          "link": "http://arxiv.org/abs/2110.15829",
          "publishedOn": "2022-01-14T00:38:46.786Z",
          "wordCount": 559,
          "title": "Holistic Deep Learning. (arXiv:2110.15829v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.09559",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Polese_M/0/1/0/all/0/1\">Michele Polese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonati_L/0/1/0/all/0/1\">Leonardo Bonati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DOro_S/0/1/0/all/0/1\">Salvatore D&#x27;Oro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basagni_S/0/1/0/all/0/1\">Stefano Basagni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melodia_T/0/1/0/all/0/1\">Tommaso Melodia</a>",
          "description": "In spite of the new opportunities brought about by the Open RAN, advances in\nML-based network automation have been slow, mainly because of the\nunavailability of large-scale datasets and experimental testing infrastructure.\nThis slows down the development and widespread adoption of Deep Reinforcement\nLearning (DRL) agents on real networks, delaying progress in intelligent and\nautonomous RAN control. In this paper, we address these challenges by proposing\npractical solutions and software pipelines for the design, training, testing,\nand experimental evaluation of DRL-based closed-loop control in the Open RAN.\nWe introduce ColO-RAN, the first publicly-available large-scale O-RAN testing\nframework with software-defined radios-in-the-loop. Building on the scale and\ncomputational capabilities of the Colosseum wireless network emulator, ColO-RAN\nenables ML research at scale using O-RAN components, programmable base\nstations, and a \"wireless data factory\". Specifically, we design and develop\nthree exemplary xApps for DRL-based control of RAN slicing, scheduling and\nonline model training, and evaluate their performance on a cellular network\nwith 7 softwarized base stations and 42 users. Finally, we showcase the\nportability of ColO-RAN to different platforms by deploying it on Arena, an\nindoor programmable testbed. Extensive results from our first-of-its-kind\nlarge-scale evaluation highlight the benefits and challenges of DRL-based\nadaptive control. They also provide insights on the development of wireless DRL\npipelines, from data analysis to the design of DRL agents, and on the tradeoffs\nassociated to training on a live RAN. ColO-RAN and the collected large-scale\ndataset will be made publicly available to the research community.",
          "link": "http://arxiv.org/abs/2112.09559",
          "publishedOn": "2022-01-14T00:38:46.778Z",
          "wordCount": 726,
          "title": "ColO-RAN: Developing Machine Learning-based xApps for Open RAN Closed-loop Control on Programmable Experimental Platforms. (arXiv:2112.09559v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perisetla_K/0/1/0/all/0/1\">Kartik Perisetla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anthony Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_N/0/1/0/all/0/1\">Nikhil Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DuBois_C/0/1/0/all/0/1\">Chris DuBois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>",
          "description": "Knowledge-dependent tasks typically use two sources of knowledge: parametric,\nlearned at training time, and contextual, given as a passage at inference time.\nTo understand how models use these sources together, we formalize the problem\nof knowledge conflicts, where the contextual information contradicts the\nlearned information. Analyzing the behaviour of popular models, we measure\ntheir over-reliance on memorized information (the cause of hallucinations), and\nuncover important factors that exacerbate this behaviour. Lastly, we propose a\nsimple method to mitigate over-reliance on parametric knowledge, which\nminimizes hallucination, and improves out-of-distribution generalization by\n4%-7%. Our findings demonstrate the importance for practitioners to evaluate\nmodel tendency to hallucinate rather than read, and show that our mitigation\nstrategy encourages generalization to evolving information (i.e.,\ntime-dependent queries). To encourage these practices, we have released our\nframework for generating knowledge conflicts.",
          "link": "http://arxiv.org/abs/2109.05052",
          "publishedOn": "2022-01-14T00:38:46.771Z",
          "wordCount": 595,
          "title": "Entity-Based Knowledge Conflicts in Question Answering. (arXiv:2109.05052v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00374",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengliu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_J/0/1/0/all/0/1\">Jeremy Kawahara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abhishek_K/0/1/0/all/0/1\">Kumar Abhishek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamanian_S/0/1/0/all/0/1\">Sajjad Shamanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1\">Ghassan Hamarneh</a>",
          "description": "We present an automated approach to detect and longitudinally track skin\nlesions on 3D total-body skin surface scans. The acquired 3D mesh of the\nsubject is unwrapped to a 2D texture image, where a trained objected detection\nmodel, Faster R-CNN, localizes the lesions within the 2D domain. These detected\nskin lesions are mapped back to the 3D surface of the subject and, for subjects\nimaged multiple times, we construct a graph-based matching procedure to\nlongitudinally track lesions that considers the anatomical correspondences\namong pairs of meshes and the geodesic proximity of corresponding lesions and\nthe inter-lesion geodesic distances.\n\nWe evaluated the proposed approach using 3DBodyTex, a publicly available\ndataset composed of 3D scans imaging the coloured skin (textured meshes) of 200\nhuman subjects. We manually annotated locations that appeared to the human eye\nto contain a pigmented skin lesion as well as tracked a subset of lesions\noccurring on the same subject imaged in different poses. Our results, when\ncompared to three human annotators, suggest that the trained Faster R-CNN\ndetects lesions at a similar performance level as the human annotators. Our\nlesion tracking algorithm achieves an average matching accuracy of 88% on a set\nof detected corresponding pairs of prominent lesions of subjects imaged in\ndifferent poses, and an average longitudinal accuracy of 71% when encompassing\nadditional errors due to lesion detection. As there currently is no other\nlarge-scale publicly available dataset of 3D total-body skin lesions, we\npublicly release over 25,000 3DBodyTex manual annotations, which we hope will\nfurther research on total-body skin lesion analysis.",
          "link": "http://arxiv.org/abs/2105.00374",
          "publishedOn": "2022-01-14T00:38:46.764Z",
          "wordCount": 744,
          "title": "Skin3D: Detection and Longitudinal Tracking of Pigmented Skin Lesions in 3D Total-Body Textured Meshes. (arXiv:2105.00374v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.02056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minghuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanye Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jian Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Li Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Offline reinforcement learning (RL) tasks require the agent to learn from a\npre-collected dataset with no further interactions with the environment.\nDespite the potential to surpass the behavioral policies, RL-based methods are\ngenerally impractical due to the training instability and bootstrapping the\nextrapolation errors, which always require careful hyperparameter tuning via\nonline evaluation. In contrast, offline imitation learning (IL) has no such\nissues since it learns the policy directly without estimating the value\nfunction by bootstrapping. However, IL is usually limited in the capability of\nthe behavioral policy and tends to learn a mediocre behavior from the dataset\ncollected by the mixture of policies. In this paper, we aim to take advantage\nof IL but mitigate such a drawback. Observing that behavior cloning is able to\nimitate neighboring policies with less data, we propose \\textit{Curriculum\nOffline Imitation Learning (COIL)}, which utilizes an experience picking\nstrategy for imitating from adaptive neighboring policies with a higher return,\nand improves the current policy along curriculum stages. On continuous control\nbenchmarks, we compare COIL against both imitation-based and RL-based methods,\nshowing that it not only avoids just learning a mediocre behavior on mixed\ndatasets but is also even competitive with state-of-the-art offline RL methods.",
          "link": "http://arxiv.org/abs/2111.02056",
          "publishedOn": "2022-01-14T00:38:46.756Z",
          "wordCount": 668,
          "title": "Curriculum Offline Imitation Learning. (arXiv:2111.02056v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.07011",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Feinstein_A/0/1/0/all/0/1\">Adina D. Feinstein</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Seligman_D/0/1/0/all/0/1\">Darryl Z. Seligman</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Gunther_M/0/1/0/all/0/1\">Maximilian N. G&#xfc;nther</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Adams_F/0/1/0/all/0/1\">Fred C. Adams</a>",
          "description": "Self-organized criticality describes a class of dynamical systems that\nmaintain themselves in an attractor state with no intrinsic length or time\nscale. Fundamentally, this theoretical construct requires a mechanism for\ninstability that may trigger additional instabilities locally via dissipative\nprocesses. This concept has been invoked to explain nonlinear dynamical\nphenomena such as featureless energy spectra that have been observed\nempirically for earthquakes, avalanches, and solar flares. If this\ninterpretation proves correct, it implies that the solar coronal magnetic field\nmaintains itself in a critical state via a delicate balance between the\ndynamo-driven injection of magnetic energy and the release of that energy via\nflaring events. All-sky high-cadence surveys like the Transiting Exoplanet\nSurvey Satellite (TESS) provide the necessary data to compare the energy\ndistribution of flaring events in stars of different spectral types to that\nobserved in the Sun. We identified $\\sim 10^6$ flaring events on $\\sim 10^5$\nstars observed by TESS at 2-minute cadence. By fitting the flare frequency\ndistribution for different mass bins, we find that all main sequence stars\nexhibit distributions of flaring events similar to that observed in the Sun,\nindependent of their mass or age. This may suggest that stars universally\nmaintain a critical state in their coronal topologies via magnetic reconnection\nevents. If this interpretation proves correct, we may be able to infer\nproperties of magnetic fields, interior structure, and dynamo mechanisms for\nstars that are otherwise unresolved point sources.",
          "link": "http://arxiv.org/abs/2109.07011",
          "publishedOn": "2022-01-14T00:38:46.749Z",
          "wordCount": 710,
          "title": "Testing Self-Organized Criticality Across the Main Sequence using Stellar Flares from TESS. (arXiv:2109.07011v2 [astro-ph.SR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.06166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shorya Sharma</a>",
          "description": "Adversarial attacks can generate adversarial inputs by applying small but\nintentionally worst-case perturbations to samples from the dataset, which leads\nto even state-of-the-art deep neural networks outputting incorrect answers with\nhigh confidence. Hence, some adversarial defense techniques are developed to\nimprove the security and robustness of the models and avoid them being\nattacked. Gradually, a game-like competition between attackers and defenders\nformed, in which both players would attempt to play their best strategies\nagainst each other while maximizing their own payoffs. To solve the game, each\nplayer would choose an optimal strategy against the opponent based on the\nprediction of the opponent's strategy choice. In this work, we are on the\ndefensive side to apply game-theoretic approaches on defending against attacks.\nWe use two randomization methods, random initialization and stochastic\nactivation pruning, to create diversity of networks. Furthermore, we use one\ndenoising technique, super resolution, to improve models' robustness by\npreprocessing images before attacks. Our experimental results indicate that\nthose three methods can effectively improve the robustness of deep-learning\nneural networks.",
          "link": "http://arxiv.org/abs/2110.06166",
          "publishedOn": "2022-01-14T00:38:46.723Z",
          "wordCount": 668,
          "title": "Game Theory for Adversarial Attacks and Defenses. (arXiv:2110.06166v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yanchao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Ruijie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yongyuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Furong Huang</a>",
          "description": "Evaluating the worst-case performance of a reinforcement learning (RL) agent\nunder the strongest/optimal adversarial perturbations on state observations\n(within some constraints) is crucial for understanding the robustness of RL\nagents. However, finding the optimal adversary is challenging, in terms of both\nwhether we can find the optimal attack and how efficiently we can find it.\nExisting works on adversarial RL either use heuristics-based methods that may\nnot find the strongest adversary, or directly train an RL-based adversary by\ntreating the agent as a part of the environment, which can find the optimal\nadversary but may become intractable in a large state space. This paper\nintroduces a novel attacking method to find the optimal attacks through\ncollaboration between a designed function named \"actor\" and an RL-based learner\nnamed \"director\". The actor crafts state perturbations for a given policy\nperturbation direction, and the director learns to propose the best policy\nperturbation directions. Our proposed algorithm, PA-AD, is theoretically\noptimal and significantly more efficient than prior RL-based works in\nenvironments with large state spaces. Empirical results show that our proposed\nPA-AD universally outperforms state-of-the-art attacking methods in various\nAtari and MuJoCo environments. By applying PA-AD to adversarial training, we\nachieve state-of-the-art empirical robustness in multiple tasks under strong\nadversaries.",
          "link": "http://arxiv.org/abs/2106.05087",
          "publishedOn": "2022-01-14T00:38:46.717Z",
          "wordCount": 676,
          "title": "Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL. (arXiv:2106.05087v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.04609",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Ghorbani_M/0/1/0/all/0/1\">Mahdi Ghorbani</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Prasad_S/0/1/0/all/0/1\">Samarjeet Prasad</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Klauda_J/0/1/0/all/0/1\">Jeffery B. Klauda</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Brooks_B/0/1/0/all/0/1\">Bernard R. Brooks</a>",
          "description": "Finding low dimensional representation of data from long-timescale\ntrajectories of biomolecular processes such as protein-folding or\nligand-receptor binding is of fundamental importance and kinetic models such as\nMarkov modeling have proven useful in describing the kinetics of these systems.\nRecently, an unsupervised machine learning technique called VAMPNet was\nintroduced to learn the low dimensional representation and linear dynamical\nmodel in an end-to-end manner. VAMPNet is based on variational approach to\nMarkov processes (VAMP) and relies on neural networks to learn the\ncoarse-grained dynamics. In this contribution, we combine VAMPNet and graph\nneural networks to generate an end-to-end framework to efficiently learn\nhigh-level dynamics and metastable states from the long-timescale molecular\ndynamics trajectories. This method bears the advantages of graph representation\nlearning and uses graph message passing operations to generate an embedding for\neach datapoint which is used in the VAMPNet to generate a coarse-grained\nrepresentation. This type of molecular representation results in a higher\nresolution and more interpretable Markov model than the standard VAMPNet\nenabling a more detailed kinetic study of the biomolecular processes. Our\nGraphVAMPNet approach is also enhanced with an attention mechanism to find the\nimportant residues for classification into different metastable states.",
          "link": "http://arxiv.org/abs/2201.04609",
          "publishedOn": "2022-01-14T00:38:45.353Z",
          "wordCount": 637,
          "title": "GraphVAMPNet, using graph neural networks and variational approach to markov processes for dynamical modeling of biomolecules. (arXiv:2201.04609v1 [physics.comp-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04612",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Baicen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramasubramanian_B/0/1/0/all/0/1\">Bhaskar Ramasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poovendran_R/0/1/0/all/0/1\">Radha Poovendran</a>",
          "description": "This paper considers multi-agent reinforcement learning (MARL) tasks where\nagents receive a shared global reward at the end of an episode. The delayed\nnature of this reward affects the ability of the agents to assess the quality\nof their actions at intermediate time-steps. This paper focuses on developing\nmethods to learn a temporal redistribution of the episodic reward to obtain a\ndense reward signal. Solving such MARL problems requires addressing two\nchallenges: identifying (1) relative importance of states along the length of\nan episode (along time), and (2) relative importance of individual agents'\nstates at any single time-step (among agents). In this paper, we introduce\nAgent-Temporal Attention for Reward Redistribution in Episodic Multi-Agent\nReinforcement Learning (AREL) to address these two challenges. AREL uses\nattention mechanisms to characterize the influence of actions on state\ntransitions along trajectories (temporal attention), and how each agent is\naffected by other agents at each time-step (agent attention). The redistributed\nrewards predicted by AREL are dense, and can be integrated with any given MARL\nalgorithm. We evaluate AREL on challenging tasks from the Particle World\nenvironment and the StarCraft Multi-Agent Challenge. AREL results in higher\nrewards in Particle World, and improved win rates in StarCraft compared to\nthree state-of-the-art reward redistribution methods. Our code is available at\nhttps://github.com/baicenxiao/AREL.",
          "link": "http://arxiv.org/abs/2201.04612",
          "publishedOn": "2022-01-14T00:38:45.346Z",
          "wordCount": 661,
          "title": "Agent-Temporal Attention for Reward Redistribution in Episodic Multi-Agent Reinforcement Learning. (arXiv:2201.04612v1 [cs.MA])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rakshit_S/0/1/0/all/0/1\">Sayan Rakshit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_A/0/1/0/all/0/1\">Anwesh Mohanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chavhan_R/0/1/0/all/0/1\">Ruchika Chavhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1\">Biplab Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roig_G/0/1/0/all/0/1\">Gemma Roig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Subhasis Chaudhuri</a>",
          "description": "We tackle the novel problem of incremental unsupervised domain adaptation\n(IDA) in this paper. We assume that a labeled source domain and different\nunlabeled target domains are incrementally observed with the constraint that\ndata corresponding to the current domain is only available at a time. The goal\nis to preserve the accuracies for all the past domains while generalizing well\nfor the current domain. The IDA setup suffers due to the abrupt differences\namong the domains and the unavailability of past data including the source\ndomain. Inspired by the notion of generative feature replay, we propose a novel\nframework called Feature Replay based Incremental Domain Adaptation (FRIDA)\nwhich leverages a new incremental generative adversarial network (GAN) called\ndomain-generic auxiliary classification GAN (DGAC-GAN) for producing\ndomain-specific feature representations seamlessly. For domain alignment, we\npropose a simple extension of the popular domain adversarial neural network\n(DANN) called DANN-IB which encourages discriminative domain-invariant and\ntask-relevant feature learning. Experimental results on Office-Home,\nOffice-CalTech, and DomainNet datasets confirm that FRIDA maintains superior\nstability-plasticity trade-off than the literature.",
          "link": "http://arxiv.org/abs/2112.14316",
          "publishedOn": "2022-01-13T00:40:19.958Z",
          "wordCount": 639,
          "title": "FRIDA -- Generative Feature Replay for Incremental Domain Adaptation. (arXiv:2112.14316v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.11305",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yin_S/0/1/0/all/0/1\">Shanzhi Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bao_Y/0/1/0/all/0/1\">Youneng Bao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_Y/0/1/0/all/0/1\">Yongsheng Liang</a>",
          "description": "Recently, Learning-based image compression has reached comparable performance\nwith traditional image codecs(such as JPEG, BPG, WebP). However, computational\ncomplexity and rate flexibility are still two major challenges for its\npractical deployment. To tackle these problems, this paper proposes two\nuniversal modules named Energy-based Channel Gating(ECG) and Bit-rate\nModulator(BM), which can be directly embedded into existing end-to-end image\ncompression models. ECG uses dynamic pruning to reduce FLOPs for more than 50\\%\nin convolution layers, and a BM pair can modulate the latent representation to\ncontrol the bit-rate in a channel-wise manner. By implementing these two\nmodules, existing learning-based image codecs can obtain ability to output\narbitrary bit-rate with a single model and reduced computation.",
          "link": "http://arxiv.org/abs/2111.11305",
          "publishedOn": "2022-01-13T00:40:19.950Z",
          "wordCount": 578,
          "title": "Universal Efficient Variable-rate Neural Image Compression. (arXiv:2111.11305v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.12650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1\">Andrei-Marius Avram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catrina_D/0/1/0/all/0/1\">Darius Catrina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cercel_D/0/1/0/all/0/1\">Dumitru-Clementin Cercel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dascalu_M/0/1/0/all/0/1\">Mihai Dasc&#x103;lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rebedea_T/0/1/0/all/0/1\">Traian Rebedea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pais_V/0/1/0/all/0/1\">Vasile P&#x103;i&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tufis_D/0/1/0/all/0/1\">Dan Tufi&#x15f;</a>",
          "description": "Running large-scale pre-trained language models in computationally\nconstrained environments remains a challenging problem yet to be addressed,\nwhile transfer learning from these models has become prevalent in Natural\nLanguage Processing tasks. Several solutions, including knowledge distillation,\nnetwork quantization, or network pruning have been previously proposed;\nhowever, these approaches focus mostly on the English language, thus widening\nthe gap when considering low-resource languages. In this work, we introduce\nthree light and fast versions of distilled BERT models for the Romanian\nlanguage: Distil-BERT-base-ro, Distil-RoBERT-base, and\nDistilMulti-BERT-base-ro. The first two models resulted from the individual\ndistillation of knowledge from two base versions of Romanian BERTs available in\nliterature, while the last one was obtained by distilling their ensemble. To\nour knowledge, this is the first attempt to create publicly available Romanian\ndistilled BERT models, which were thoroughly evaluated on five tasks:\npart-of-speech tagging, named entity recognition, sentiment analysis, semantic\ntextual similarity, and dialect identification. Our experimental results argue\nthat the three distilled models maintain most performance in terms of accuracy\nwith their teachers, while being twice as fast on a GPU and ~35% smaller. In\naddition, we further test the similarity between the predictions of our\nstudents versus their teachers by measuring their label and probability\nloyalty, together with regression loyalty - a new metric introduced in this\nwork.",
          "link": "http://arxiv.org/abs/2112.12650",
          "publishedOn": "2022-01-13T00:40:19.943Z",
          "wordCount": 673,
          "title": "Distilling the Knowledge of Romanian BERTs Using Multiple Teachers. (arXiv:2112.12650v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.06977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bolya_D/0/1/0/all/0/1\">Daniel Bolya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittapalli_R/0/1/0/all/0/1\">Rohit Mittapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1\">Judy Hoffman</a>",
          "description": "With the preponderance of pretrained deep learning models available\noff-the-shelf from model banks today, finding the best weights to fine-tune to\nyour use-case can be a daunting task. Several methods have recently been\nproposed to find good models for transfer learning, but they either don't scale\nwell to large model banks or don't perform well on the diversity of\noff-the-shelf models. Ideally the question we want to answer is, \"given some\ndata and a source model, can you quickly predict the model's accuracy after\nfine-tuning?\" In this paper, we formalize this setting as \"Scalable Diverse\nModel Selection\" and propose several benchmarks for evaluating on this task. We\nfind that existing model selection and transferability estimation methods\nperform poorly here and analyze why this is the case. We then introduce simple\ntechniques to improve the performance and speed of these algorithms. Finally,\nwe iterate on existing methods to create PARC, which outperforms all other\nmethods on diverse model selection. We have released the benchmarks and method\ncode in hope to inspire future work in model selection for accessible transfer\nlearning.",
          "link": "http://arxiv.org/abs/2111.06977",
          "publishedOn": "2022-01-13T00:40:19.905Z",
          "wordCount": 655,
          "title": "Scalable Diverse Model Selection for Accessible Transfer Learning. (arXiv:2111.06977v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.06890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alexander_S/0/1/0/all/0/1\">Samuel Allen Alexander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castaneda_M/0/1/0/all/0/1\">Michael Castaneda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Compher_K/0/1/0/all/0/1\">Kevin Compher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_O/0/1/0/all/0/1\">Oscar Martinez</a>",
          "description": "We consider an extended notion of reinforcement learning in which the\nenvironment can simulate the agent and base its outputs on the agent's\nhypothetical behavior. Since good performance usually requires paying attention\nto whatever things the environment's outputs are based on, we argue that for an\nagent to achieve on-average good performance across many such extended\nenvironments, it is necessary for the agent to self-reflect. Thus, an agent's\nself-reflection ability can be numerically estimated by running the agent\nthrough a battery of extended environments. We are simultaneously releasing an\nopen-source library of extended environments to serve as proof-of-concept of\nthis technique. As the library is first-of-kind, we have avoided the difficult\nproblem of optimizing it. Instead we have chosen environments with interesting\nproperties. Some seem paradoxical, some lead to interesting thought\nexperiments, some are even suggestive of how self-reflection might have evolved\nin nature. We give examples and introduce a simple transformation which\nexperimentally seems to increase self-reflection.",
          "link": "http://arxiv.org/abs/2110.06890",
          "publishedOn": "2022-01-13T00:40:19.897Z",
          "wordCount": 607,
          "title": "Extending Environments To Measure Self-Reflection In Reinforcement Learning. (arXiv:2110.06890v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.04426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensch_A/0/1/0/all/0/1\">Arthur Mensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_J/0/1/0/all/0/1\">Jordan Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Trevor Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rutherford_E/0/1/0/all/0/1\">Eliza Rutherford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millican_K/0/1/0/all/0/1\">Katie Millican</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driessche_G/0/1/0/all/0/1\">George van den Driessche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lespiau_J/0/1/0/all/0/1\">Jean-Baptiste Lespiau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damoc_B/0/1/0/all/0/1\">Bogdan Damoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_A/0/1/0/all/0/1\">Aidan Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casas_D/0/1/0/all/0/1\">Diego de Las Casas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guy_A/0/1/0/all/0/1\">Aurelia Guy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ring_R/0/1/0/all/0/1\">Roman Ring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennigan_T/0/1/0/all/0/1\">Tom Hennigan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Saffron Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maggiore_L/0/1/0/all/0/1\">Loren Maggiore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_C/0/1/0/all/0/1\">Chris Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassirer_A/0/1/0/all/0/1\">Albin Cassirer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1\">Andy Brock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paganini_M/0/1/0/all/0/1\">Michela Paganini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irving_G/0/1/0/all/0/1\">Geoffrey Irving</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osindero_S/0/1/0/all/0/1\">Simon Osindero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonyan_K/0/1/0/all/0/1\">Karen Simonyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rae_J/0/1/0/all/0/1\">Jack W. Rae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsen_E/0/1/0/all/0/1\">Erich Elsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifre_L/0/1/0/all/0/1\">Laurent Sifre</a>",
          "description": "We enhance auto-regressive language models by conditioning on document chunks\nretrieved from a large corpus, based on local similarity with preceding tokens.\nWith a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO)\nobtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite\nusing 25$\\times$ fewer parameters. After fine-tuning, RETRO performance\ntranslates to downstream knowledge-intensive tasks such as question answering.\nRETRO combines a frozen Bert retriever, a differentiable encoder and a chunked\ncross-attention mechanism to predict tokens based on an order of magnitude more\ndata than what is typically consumed during training. We typically train RETRO\nfrom scratch, yet can also rapidly RETROfit pre-trained transformers with\nretrieval and still achieve good performance. Our work opens up new avenues for\nimproving language models through explicit memory at unprecedented scale.",
          "link": "http://arxiv.org/abs/2112.04426",
          "publishedOn": "2022-01-13T00:40:19.882Z",
          "wordCount": 651,
          "title": "Improving language models by retrieving from trillions of tokens. (arXiv:2112.04426v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.04718",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klicpera_J/0/1/0/all/0/1\">Johannes Klicpera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeshwanth_C/0/1/0/all/0/1\">Chandan Yeshwanth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>",
          "description": "Graph neural networks that leverage coordinates via directional message\npassing have recently set the state of the art on multiple molecular property\nprediction tasks. However, they rely on atom position information that is often\nunavailable, and obtaining it is usually prohibitively expensive or even\nimpossible. In this paper we propose synthetic coordinates that enable the use\nof advanced GNNs without requiring the true molecular configuration. We propose\ntwo distances as synthetic coordinates: Distance bounds that specify the rough\nrange of molecular configurations, and graph-based distances using a symmetric\nvariant of personalized PageRank. To leverage both distance and angular\ninformation we propose a method of transforming normal graph neural networks\ninto directional MPNNs. We show that with this transformation we can reduce the\nerror of a normal graph neural network by 55% on the ZINC benchmark. We\nfurthermore set the state of the art on ZINC and coordinate-free QM9 by\nincorporating synthetic coordinates in the SMP and DimeNet++ models. Our\nimplementation is available online.",
          "link": "http://arxiv.org/abs/2111.04718",
          "publishedOn": "2022-01-13T00:40:19.859Z",
          "wordCount": 630,
          "title": "Directional Message Passing on Molecular Graphs via Synthetic Coordinates. (arXiv:2111.04718v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.03994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ihorn_S/0/1/0/all/0/1\">Shasta Ihorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siu_Y/0/1/0/all/0/1\">Yue-Ting Siu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodi_A/0/1/0/all/0/1\">Aditya Bodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narins_L/0/1/0/all/0/1\">Lothar Narins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castanon_J/0/1/0/all/0/1\">Jose M. Castanon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kant_Y/0/1/0/all/0/1\">Yash Kant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Abhishek Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_I/0/1/0/all/0/1\">Ilmi Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazli_P/0/1/0/all/0/1\">Pooyan Fazli</a>",
          "description": "Video accessibility is crucial for blind and low vision users for equitable\nengagements in education, employment, and entertainment. Despite the\navailability of professional and amateur services and tools, most\nhuman-generated descriptions are expensive and time consuming. Moreover, the\nrate of human-generated descriptions cannot match the speed of video\nproduction. To overcome the increasing gaps in video accessibility, we\ndeveloped a hybrid system of two tools to 1) automatically generate\ndescriptions for videos and 2) provide answers or additional descriptions in\nresponse to user queries on a video. Results from a mixed-methods study with 26\nblind and low vision individuals show that our system significantly improved\nuser comprehension and enjoyment of selected videos when both tools were used\nin tandem. In addition, participants reported no significant difference in\ntheir ability to understand videos when presented with autogenerated\ndescriptions versus human-revised autogenerated descriptions. Our results\ndemonstrate user enthusiasm about the developed system and its promise for\nproviding customized access to videos. We discuss the limitations of the\ncurrent work and provide recommendations for the future development of\nautomated video description tools.",
          "link": "http://arxiv.org/abs/2111.03994",
          "publishedOn": "2022-01-13T00:40:19.852Z",
          "wordCount": 673,
          "title": "NarrationBot and InfoBot: A Hybrid System for Automated Video Description. (arXiv:2111.03994v2 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.13320",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mehta_S/0/1/0/all/0/1\">Shivam Mehta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szekely_E/0/1/0/all/0/1\">&#xc9;va Sz&#xe9;kely</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beskow_J/0/1/0/all/0/1\">Jonas Beskow</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Henter_G/0/1/0/all/0/1\">Gustav Eje Henter</a>",
          "description": "Neural sequence-to-sequence TTS has achieved significantly better output\nquality than statistical speech synthesis using HMMs. However, neural TTS is\ngenerally not probabilistic and the use of non-monotonic attention both\nincreases training time and introduces \"babbling\" failure modes that are\nunacceptable in production. This paper demonstrates that the old and new\nparadigms can be combined to obtain the advantages of both worlds. In\nparticular, we replace the attention in Tacotron 2 with an autoregressive\nleft-right no-skip hidden Markov model defined by a neural network. This leads\nto an HMM-based neural TTS model with monotonic alignment, trained to maximise\nthe full sequence likelihood without approximations. We discuss how to combine\ninnovations from both classical and contemporary TTS for best results. The\nfinal system is smaller and simpler than Tacotron 2, and learns to speak with\nfewer iterations and less data, whilst achieving the same naturalness prior to\nthe post-net. Our approach also allows easy control over speaking rate. Audio\nexamples and code are available at https://shivammehta007.github.io/Neural-HMM/",
          "link": "http://arxiv.org/abs/2108.13320",
          "publishedOn": "2022-01-13T00:40:19.845Z",
          "wordCount": 677,
          "title": "Neural HMMs are all you need (for high-quality attention-free TTS). (arXiv:2108.13320v5 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.10211",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Phan_H/0/1/0/all/0/1\">Huy Phan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mertins_A/0/1/0/all/0/1\">Alfred Mertins</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baumert_M/0/1/0/all/0/1\">Mathias Baumert</a>",
          "description": "Despite the tremendous progress recently made towards automatic sleep staging\nin adults, it is currently unknown if the most advanced algorithms generalize\nto the pediatric population, which displays distinctive characteristics in\novernight polysomnography (PSG). To answer the question, in this work, we\nconduct a large-scale comparative study on the state-of-the-art deep learning\nmethods for pediatric automatic sleep staging. A selection of six different\ndeep neural networks with diverging features are adopted to evaluate a sample\nof more than 1,200 children across a wide spectrum of obstructive sleep apnea\n(OSA) severity. Our experimental results show that the individual performance\nof automated pediatric sleep stagers when evaluated on new subjects is\nequivalent to the expert-level one reported on adults. Combining the six\nstagers into ensemble models further boosts the staging accuracy, reaching an\noverall accuracy of 87.7%, a Cohen's kappa of 0.837, and a macro F1-score of\n84.2% in case of single-channel EEG when evaluated on new subjects. The\nperformance is further improved when dual-channel EEG$\\cdot$EOG are used,\nreaching an accuracy of 88.8%, a Cohen's kappa of 0.852, and a macro F1-score\nof 85.8%. At the same time, the ensemble models lead to reduced predictive\nuncertainty. The results also show that the studied algorithms and their\nensembles are robust to concept drift when the training and test data were\nrecorded 7-months apart and after clinical intervention. Detailed analyses\nfurther demonstrate \"almost perfect\" agreement between the automatic stagers to\none another and their similar patterns on the staging errors.",
          "link": "http://arxiv.org/abs/2108.10211",
          "publishedOn": "2022-01-13T00:40:19.837Z",
          "wordCount": 702,
          "title": "Pediatric Automatic Sleep Staging: Deep Learning Ensemble Improves Accuracy and Reduces Predictive Uncertainty. (arXiv:2108.10211v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuangbin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1\">Wenwei Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yuxin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>",
          "description": "Logs have been an imperative resource to ensure the reliability and\ncontinuity of many software systems, especially large-scale distributed\nsystems. They faithfully record runtime information to facilitate system\ntroubleshooting and behavior understanding. Due to the large scale and\ncomplexity of modern software systems, the volume of logs has reached an\nunprecedented level. Consequently, for log-based anomaly detection,\nconventional manual inspection methods or even traditional machine\nlearning-based methods become impractical, which serve as a catalyst for the\nrapid development of deep learning-based solutions. However, there is currently\na lack of rigorous comparison among the representative log-based anomaly\ndetectors that resort to neural networks. Moreover, the re-implementation\nprocess demands non-trivial efforts, and bias can be easily introduced. To\nbetter understand the characteristics of different anomaly detectors, in this\npaper, we provide a comprehensive review and evaluation of five popular neural\nnetworks used by six state-of-the-art methods. Particularly, four of the\nselected methods are unsupervised, and the remaining two are supervised. These\nmethods are evaluated with two publicly available log datasets, which contain\nnearly 16 million log messages and 0.4 million anomaly instances in total. We\nbelieve our work can serve as a basis in this field and contribute to future\nacademic research and industrial applications.",
          "link": "http://arxiv.org/abs/2107.05908",
          "publishedOn": "2022-01-13T00:40:19.800Z",
          "wordCount": 659,
          "title": "Experience Report: Deep Learning-based System Log Analysis for Anomaly Detection. (arXiv:2107.05908v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muller_A/0/1/0/all/0/1\">Arthur M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangras_V/0/1/0/all/0/1\">Vishal Rangras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnittker_G/0/1/0/all/0/1\">Georg Schnittker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waldmann_M/0/1/0/all/0/1\">Michael Waldmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friesen_M/0/1/0/all/0/1\">Maxim Friesen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferfers_T/0/1/0/all/0/1\">Tobias Ferfers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schreckenberg_L/0/1/0/all/0/1\">Lukas Schreckenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hufen_F/0/1/0/all/0/1\">Florian Hufen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jasperneite_J/0/1/0/all/0/1\">J&#xfc;rgen Jasperneite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiering_M/0/1/0/all/0/1\">Marco Wiering</a>",
          "description": "Sub-optimal control policies in intersection traffic signal controllers (TSC)\ncontribute to congestion and lead to negative effects on human health and the\nenvironment. Reinforcement learning (RL) for traffic signal control is a\npromising approach to design better control policies and has attracted\nconsiderable research interest in recent years. However, most work done in this\narea used simplified simulation environments of traffic scenarios to train\nRL-based TSC. To deploy RL in real-world traffic systems, the gap between\nsimplified simulation environments and real-world applications has to be\nclosed. Therefore, we propose LemgoRL, a benchmark tool to train RL agents as\nTSC in a realistic simulation environment of Lemgo, a medium-sized town in\nGermany. In addition to the realistic simulation model, LemgoRL encompasses a\ntraffic signal logic unit that ensures compliance with all regulatory and\nsafety requirements. LemgoRL offers the same interface as the wellknown OpenAI\ngym toolkit to enable easy deployment in existing research work. To demonstrate\nthe functionality and applicability of LemgoRL, we train a state-of-the-art\nDeep RL algorithm on a CPU cluster utilizing a framework for distributed and\nparallel RL and compare its performance with other methods. Our benchmark tool\ndrives the development of RL algorithms towards real-world applications.",
          "link": "http://arxiv.org/abs/2103.16223",
          "publishedOn": "2022-01-13T00:40:19.793Z",
          "wordCount": 698,
          "title": "Towards Real-World Deployment of Reinforcement Learning for Traffic Signal Control. (arXiv:2103.16223v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chee_K/0/1/0/all/0/1\">Kong Yao Chee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiahao_T/0/1/0/all/0/1\">Tom Z. Jiahao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_M/0/1/0/all/0/1\">M. Ani Hsieh</a>",
          "description": "In this work, we consider the problem of deriving and incorporating accurate\ndynamic models for model predictive control (MPC) with an application to\nquadrotor control. MPC relies on precise dynamic models to achieve the desired\nclosed-loop performance. However, the presence of uncertainties in complex\nsystems and the environments they operate in poses a challenge in obtaining\nsufficiently accurate representations of the system dynamics. In this work, we\nmake use of a deep learning tool, knowledge-based neural ordinary differential\nequations (KNODE), to augment a model obtained from first principles. The\nresulting hybrid model encompasses both a nominal first-principle model and a\nneural network learnt from simulated or real-world experimental data. Using a\nquadrotor, we benchmark our hybrid model against a state-of-the-art Gaussian\nProcess (GP) model and show that the hybrid model provides more accurate\npredictions of the quadrotor dynamics and is able to generalize beyond the\ntraining data. To improve closed-loop performance, the hybrid model is\nintegrated into a novel MPC framework, known as KNODE-MPC. Results show that\nthe integrated framework achieves 60.2% improvement in simulations and more\nthan 21% in physical experiments, in terms of trajectory tracking performance.",
          "link": "http://arxiv.org/abs/2109.04821",
          "publishedOn": "2022-01-13T00:40:19.786Z",
          "wordCount": 716,
          "title": "KNODE-MPC: A Knowledge-based Data-driven Predictive Control Framework for Aerial Robots. (arXiv:2109.04821v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08903",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Klicpera_J/0/1/0/all/0/1\">Johannes Klicpera</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Becker_F/0/1/0/all/0/1\">Florian Becker</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>",
          "description": "Effectively predicting molecular interactions has the potential to accelerate\nmolecular dynamics by multiple orders of magnitude and thus revolutionize\nchemical simulations. Graph neural networks (GNNs) have recently shown great\nsuccesses for this task, overtaking classical methods based on fixed molecular\nkernels. However, they still appear very limited from a theoretical\nperspective, since regular GNNs cannot distinguish certain types of graphs. In\nthis work we close this gap between theory and practice. We show that GNNs with\ndirected edge embeddings and two-hop message passing are indeed universal\napproximators for predictions that are invariant to translation, and\nequivariant to permutation and rotation. We then leverage these insights and\nmultiple structural improvements to propose the geometric message passing\nneural network (GemNet). We demonstrate the benefits of the proposed changes in\nmultiple ablation studies. GemNet outperforms previous models on the COLL,\nMD17, and OC20 datasets by 34%, 41%, and 20%, respectively, and performs\nespecially well on the most challenging molecules. Our implementation is\navailable online.",
          "link": "http://arxiv.org/abs/2106.08903",
          "publishedOn": "2022-01-13T00:40:19.779Z",
          "wordCount": 656,
          "title": "GemNet: Universal Directional Graph Neural Networks for Molecules. (arXiv:2106.08903v6 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rothfuss_J/0/1/0/all/0/1\">Jonas Rothfuss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heyn_D/0/1/0/all/0/1\">Dominique Heyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinfan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1\">Andreas Krause</a>",
          "description": "When data are scarce meta-learning can improve a learner's accuracy by\nharnessing previous experience from related learning tasks. However, existing\nmethods have unreliable uncertainty estimates which are often overconfident.\nAddressing these shortcomings, we introduce a novel meta-learning framework,\ncalled F-PACOH, that treats meta-learned priors as stochastic processes and\nperforms meta-level regularization directly in the function space. This allows\nus to directly steer the probabilistic predictions of the meta-learner towards\nhigh epistemic uncertainty in regions of insufficient meta-training data and,\nthus, obtain well-calibrated uncertainty estimates. Finally, we showcase how\nour approach can be integrated with sequential decision making, where reliable\nuncertainty quantification is imperative. In our benchmark study on\nmeta-learning for Bayesian Optimization (BO), F-PACOH significantly outperforms\nall other meta-learners and standard baselines.",
          "link": "http://arxiv.org/abs/2106.03195",
          "publishedOn": "2022-01-13T00:40:19.771Z",
          "wordCount": 582,
          "title": "Meta-Learning Reliable Priors in the Function Space. (arXiv:2106.03195v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.02593",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Duc_T/0/1/0/all/0/1\">Thanh Nguyen-Duc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">He Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>",
          "description": "Deep learning methods usually require a large amount of training data and\nlack interpretability. In this paper, we propose a novel knowledge distillation\nand model interpretation framework for medical image classification that\njointly solves the above two issues. Specifically, to address the data-hungry\nissue, a small student model is learned with less data by distilling knowledge\nfrom a cumbersome pretrained teacher model. To interpret the teacher model and\nassist the learning of the student, an explainer module is introduced to\nhighlight the regions of an input that are important for the predictions of the\nteacher model. Furthermore, the joint framework is trained by a principled way\nderived from the information-theoretic perspective. Our framework outperforms\non the knowledge distillation and model interpretation tasks compared to\nstate-of-the-art methods on a fundus dataset.",
          "link": "http://arxiv.org/abs/2008.02593",
          "publishedOn": "2022-01-13T00:40:19.749Z",
          "wordCount": 611,
          "title": "MED-TEX: Transferring and Explaining Knowledge with Less Data from Pretrained Medical Imaging Models. (arXiv:2008.02593v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.03996",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Fan_J/0/1/0/all/0/1\">Jianqing Fan</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Masini_R/0/1/0/all/0/1\">Ricardo P. Masini</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Medeiros_M/0/1/0/all/0/1\">Marcelo C. Medeiros</a>",
          "description": "Optimal pricing, i.e., determining the price level that maximizes profit or\nrevenue of a given product, is a vital task for the retail industry. To select\nsuch a quantity, one needs first to estimate the price elasticity from the\nproduct demand. Regression methods usually fail to recover such elasticities\ndue to confounding effects and price endogeneity. Therefore, randomized\nexperiments are typically required. However, elasticities can be highly\nheterogeneous depending on the location of stores, for example. As the\nrandomization frequently occurs at the municipal level, standard\ndifference-in-differences methods may also fail. Possible solutions are based\non methodologies to measure the effects of treatments on a single (or just a\nfew) treated unit(s) based on counterfactuals constructed from artificial\ncontrols. For example, for each city in the treatment group, a counterfactual\nmay be constructed from the untreated locations. In this paper, we apply a\nnovel high-dimensional statistical method to measure the effects of price\nchanges on daily sales from a major retailer in Brazil. The proposed\nmethodology combines principal components (factors) and sparse regressions,\nresulting in a method called Factor-Adjusted Regularized Method for Treatment\nevaluation (\\texttt{FarmTreat}). The data consist of daily sales and prices of\nfive different products over more than 400 municipalities. The products\nconsidered belong to the \\emph{sweet and candies} category and experiments have\nbeen conducted over the years of 2016 and 2017. Our results confirm the\nhypothesis of a high degree of heterogeneity yielding very different pricing\nstrategies over distinct municipalities.",
          "link": "http://arxiv.org/abs/2011.03996",
          "publishedOn": "2022-01-13T00:40:19.742Z",
          "wordCount": 713,
          "title": "Do We Exploit all Information for Counterfactual Analysis? Benefits of Factor Models and Idiosyncratic Correction. (arXiv:2011.03996v3 [econ.EM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Horvath_S/0/1/0/all/0/1\">Samuel Horvath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskaridis_S/0/1/0/all/0/1\">Stefanos Laskaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almeida_M/0/1/0/all/0/1\">Mario Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leontiadis_I/0/1/0/all/0/1\">Ilias Leontiadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1\">Stylianos I. Venieris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas D. Lane</a>",
          "description": "Federated Learning (FL) has been gaining significant traction across\ndifferent ML tasks, ranging from vision to keyboard predictions. In large-scale\ndeployments, client heterogeneity is a fact and constitutes a primary problem\nfor fairness, training performance and accuracy. Although significant efforts\nhave been made into tackling statistical data heterogeneity, the diversity in\nthe processing capabilities and network bandwidth of clients, termed as system\nheterogeneity, has remained largely unexplored. Current solutions either\ndisregard a large portion of available devices or set a uniform limit on the\nmodel's capacity, restricted by the least capable participants. In this work,\nwe introduce Ordered Dropout, a mechanism that achieves an ordered, nested\nrepresentation of knowledge in deep neural networks (DNNs) and enables the\nextraction of lower footprint submodels without the need of retraining. We\nfurther show that for linear maps our Ordered Dropout is equivalent to SVD. We\nemploy this technique, along with a self-distillation methodology, in the realm\nof FL in a framework called FjORD. FjORD alleviates the problem of client\nsystem heterogeneity by tailoring the model width to the client's capabilities.\nExtensive evaluation on both CNNs and RNNs across diverse modalities shows that\nFjORD consistently leads to significant performance gains over state-of-the-art\nbaselines, while maintaining its nested structure.",
          "link": "http://arxiv.org/abs/2102.13451",
          "publishedOn": "2022-01-13T00:40:19.734Z",
          "wordCount": 714,
          "title": "FjORD: Fair and Accurate Federated Learning under heterogeneous targets with Ordered Dropout. (arXiv:2102.13451v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1\">Pablo Samuel Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kastner_T/0/1/0/all/0/1\">Tyler Kastner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panangaden_P/0/1/0/all/0/1\">Prakash Panangaden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rowland_M/0/1/0/all/0/1\">Mark Rowland</a>",
          "description": "We present a new behavioural distance over the state space of a Markov\ndecision process, and demonstrate the use of this distance as an effective\nmeans of shaping the learnt representations of deep reinforcement learning\nagents. While existing notions of state similarity are typically difficult to\nlearn at scale due to high computational cost and lack of sample-based\nalgorithms, our newly-proposed distance addresses both of these issues. In\naddition to providing detailed theoretical analysis, we provide empirical\nevidence that learning this distance alongside the value function yields\nstructured and informative representations, including strong results on the\nArcade Learning Environment benchmark.",
          "link": "http://arxiv.org/abs/2106.08229",
          "publishedOn": "2022-01-13T00:40:19.727Z",
          "wordCount": 572,
          "title": "MICo: Improved representations via sampling-based state similarity for Markov decision processes. (arXiv:2106.08229v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.01174",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Choi_Y/0/1/0/all/0/1\">Yeunju Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jung_Y/0/1/0/all/0/1\">Youngmoon Jung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suh_Y/0/1/0/all/0/1\">Youngjoo Suh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Hoirin Kim</a>",
          "description": "Although recent neural text-to-speech (TTS) systems have achieved\nhigh-quality speech synthesis, there are cases where a TTS system generates\nlow-quality speech, mainly caused by limited training data or information loss\nduring knowledge distillation. Therefore, we propose a novel method to improve\nspeech quality by training a TTS model under the supervision of perceptual\nloss, which measures the distance between the maximum possible speech quality\nscore and the predicted one. We first pre-train a mean opinion score (MOS)\nprediction model and then train a TTS model to maximize the MOS of synthesized\nspeech using the pre-trained MOS prediction model. The proposed method can be\napplied universally (i.e., regardless of the TTS model architecture or the\ncause of speech quality degradation) and efficiently (i.e., without increasing\nthe inference time or model complexity). The evaluation results for the MOS and\nphone error rate demonstrate that our proposed approach improves previous\nmodels in terms of both naturalness and intelligibility.",
          "link": "http://arxiv.org/abs/2011.01174",
          "publishedOn": "2022-01-13T00:40:19.720Z",
          "wordCount": 657,
          "title": "Learning to Maximize Speech Quality Directly Using MOS Prediction for Neural Text-to-Speech. (arXiv:2011.01174v4 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiahao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhigang Zeng</a>",
          "description": "Multimodal sentiment analysis (MSA) is a fundamental complex research problem\ndue to the heterogeneity gap between different modalities and the ambiguity of\nhuman emotional expression. Although there have been many successful attempts\nto construct multimodal representations for MSA, there are still two challenges\nto be addressed: 1) A more robust multimodal representation needs to be\nconstructed to bridge the heterogeneity gap and cope with the complex\nmultimodal interactions, and 2) the contextual dynamics must be modeled\neffectively throughout the information flow. In this work, we propose a\nmultimodal representation model based on Mutual information Maximization and\nMinimization and Identity Embedding (MMMIE). We combine mutual information\nmaximization between modal pairs, and mutual information minimization between\ninput data and corresponding features to mine the modal-invariant and\ntask-related information. Furthermore, Identity Embedding is proposed to prompt\nthe downstream network to perceive the contextual information. Experimental\nresults on two public datasets demonstrate the effectiveness of the proposed\nmodel.",
          "link": "http://arxiv.org/abs/2201.03969",
          "publishedOn": "2022-01-13T00:40:19.696Z",
          "wordCount": 601,
          "title": "Multimodal Representations Learning Based on Mutual Information Maximization and Minimization and Identity Embedding for Multimodal Sentiment Analysis. (arXiv:2201.03969v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2111.06750",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lou_G/0/1/0/all/0/1\">Guannan Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tiehua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xi Zheng</a>",
          "description": "We present a spatial-temporal federated learning framework for graph neural\nnetworks, namely STFL. The framework explores the underlying correlation of the\ninput spatial-temporal data and transform it to both node features and\nadjacency matrix. The federated learning setting in the framework ensures data\nprivacy while achieving a good model generalization. Experiments results on the\nsleep stage dataset, ISRUC_S3, illustrate the effectiveness of STFL on graph\nprediction tasks.",
          "link": "http://arxiv.org/abs/2111.06750",
          "publishedOn": "2022-01-13T00:40:19.689Z",
          "wordCount": 519,
          "title": "STFL: A Temporal-Spatial Federated Learning Framework for Graph Neural Networks. (arXiv:2111.06750v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.03588",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zheng-Hua Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Achintya kr. Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehak_N/0/1/0/all/0/1\">Najim Dehak</a>",
          "description": "This paper presents an unsupervised segment-based method for robust voice\nactivity detection (rVAD). The method consists of two passes of denoising\nfollowed by a voice activity detection (VAD) stage. In the first pass,\nhigh-energy segments in a speech signal are detected by using a posteriori\nsignal-to-noise ratio (SNR) weighted energy difference and if no pitch is\ndetected within a segment, the segment is considered as a high-energy noise\nsegment and set to zero. In the second pass, the speech signal is denoised by a\nspeech enhancement method, for which several methods are explored. Next,\nneighbouring frames with pitch are grouped together to form pitch segments, and\nbased on speech statistics, the pitch segments are further extended from both\nends in order to include both voiced and unvoiced sounds and likely non-speech\nparts as well. In the end, a posteriori SNR weighted energy difference is\napplied to the extended pitch segments of the denoised speech signal for\ndetecting voice activity. We evaluate the VAD performance of the proposed\nmethod using two databases, RATS and Aurora-2, which contain a large variety of\nnoise conditions. The rVAD method is further evaluated, in terms of speaker\nverification performance, on the RedDots 2016 challenge database and its\nnoise-corrupted versions. Experiment results show that rVAD is compared\nfavourably with a number of existing methods. In addition, we present a\nmodified version of rVAD where computationally intensive pitch extraction is\nreplaced by computationally efficient spectral flatness calculation. The\nmodified version significantly reduces the computational complexity at the cost\nof moderately inferior VAD performance, which is an advantage when processing a\nlarge amount of data and running on low resource devices. The source code of\nrVAD is made publicly available.",
          "link": "http://arxiv.org/abs/1906.03588",
          "publishedOn": "2022-01-13T00:40:19.682Z",
          "wordCount": 757,
          "title": "rVAD: An Unsupervised Segment-Based Robust Voice Activity Detection Method. (arXiv:1906.03588v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vaswani_S/0/1/0/all/0/1\">Sharan Vaswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachem_O/0/1/0/all/0/1\">Olivier Bachem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Totaro_S/0/1/0/all/0/1\">Simone Totaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_R/0/1/0/all/0/1\">Robert Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Shivam Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1\">Matthieu Geist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_M/0/1/0/all/0/1\">Marlos C. Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1\">Pablo Samuel Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_N/0/1/0/all/0/1\">Nicolas Le Roux</a>",
          "description": "Common policy gradient methods rely on the maximization of a sequence of\nsurrogate functions. In recent years, many such surrogate functions have been\nproposed, most without strong theoretical guarantees, leading to algorithms\nsuch as TRPO, PPO or MPO. Rather than design yet another surrogate function, we\ninstead propose a general framework (FMA-PG) based on functional mirror ascent\nthat gives rise to an entire family of surrogate functions. We construct\nsurrogate functions that enable policy improvement guarantees, a property not\nshared by most existing surrogate functions. Crucially, these guarantees hold\nregardless of the choice of policy parameterization. Moreover, a particular\ninstantiation of FMA-PG recovers important implementation heuristics (e.g.,\nusing forward vs reverse KL divergence) resulting in a variant of TRPO with\nadditional desirable properties. Via experiments on simple bandit problems, we\nevaluate the algorithms instantiated by FMA-PG. The proposed framework also\nsuggests an improved variant of PPO, whose robustness and efficiency we\nempirically demonstrate on the MuJoCo suite.",
          "link": "http://arxiv.org/abs/2108.05828",
          "publishedOn": "2022-01-13T00:40:19.673Z",
          "wordCount": 663,
          "title": "A general class of surrogate functions for stable and efficient reinforcement learning. (arXiv:2108.05828v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangmeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1\">Wenwen Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Lingyu Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Changwen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>",
          "description": "Recent works explore learning graph representations in a self-supervised\nmanner. In graph contrastive learning, benchmark methods apply various graph\naugmentation approaches. However, most of the augmentation methods are\nnon-learnable, which causes the issue of generating unbeneficial augmented\ngraphs. Such augmentation may degenerate the representation ability of graph\ncontrastive learning methods. Therefore, we motivate our method to generate\naugmented graph by a learnable graph augmenter, called MEta Graph Augmentation\n(MEGA). We then clarify that a \"good\" graph augmentation must have uniformity\nat the instance-level and informativeness at the feature-level. To this end, we\npropose a novel approach to learning a graph augmenter that can generate an\naugmentation with uniformity and informativeness. The objective of the graph\naugmenter is to promote our feature extraction network to learn a more\ndiscriminative feature representation, which motivates us to propose a\nmeta-learning paradigm. Empirically, the experiments across multiple benchmark\ndatasets demonstrate that MEGA outperforms the state-of-the-art methods in\ngraph self-supervised learning tasks. Further experimental studies prove the\neffectiveness of different terms of MEGA.",
          "link": "http://arxiv.org/abs/2201.03812",
          "publishedOn": "2022-01-13T00:40:19.666Z",
          "wordCount": 594,
          "title": "Bootstrapping Informative Graph Augmentation via A Meta Learning Approach. (arXiv:2201.03812v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_R/0/1/0/all/0/1\">Ramin Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naeim_A/0/1/0/all/0/1\">Arash Naeim</a>",
          "description": "A data science task can be deemed as making sense of the data or testing a\nhypothesis about it. The conclusions inferred from data can greatly guide us to\nmake informative decisions. Big data has enabled us to carry out countless\nprediction tasks in conjunction with machine learning, such as identifying high\nrisk patients suffering from a certain disease and taking preventable measures.\nHowever, healthcare practitioners are not content with mere predictions - they\nare also interested in the cause-effect relation between input features and\nclinical outcomes. Understanding such relations will help doctors treat\npatients and reduce the risk effectively. Causality is typically identified by\nrandomized controlled trials. Often such trials are not feasible when\nscientists and researchers turn to observational studies and attempt to draw\ninferences. However, observational studies may also be affected by selection\nand/or confounding biases that can result in wrong causal conclusions. In this\nchapter, we will try to highlight some of the drawbacks that may arise in\ntraditional machine learning and statistical approaches to analyze the\nobservational data, particularly in the healthcare data analytics domain. We\nwill discuss causal inference and ways to discover the cause-effect from\nobservational studies in healthcare domain. Moreover, we will demonstrate the\napplications of causal inference in tackling some common machine learning\nissues such as missing data and model transportability. Finally, we will\ndiscuss the possibility of integrating reinforcement learning with causality as\na way to counter confounding bias.",
          "link": "http://arxiv.org/abs/2105.04655",
          "publishedOn": "2022-01-13T00:40:19.644Z",
          "wordCount": 723,
          "title": "Causal Inference in medicine and in health policy, a summary. (arXiv:2105.04655v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.08488",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Matsubara_T/0/1/0/all/0/1\">Takuo Matsubara</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Oates_C/0/1/0/all/0/1\">Chris J. Oates</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Briol_F/0/1/0/all/0/1\">Fran&#xe7;ois-Xavier Briol</a>",
          "description": "Bayesian neural networks attempt to combine the strong predictive performance\nof neural networks with formal quantification of uncertainty associated with\nthe predictive output in the Bayesian framework. However, it remains unclear\nhow to endow the parameters of the network with a prior distribution that is\nmeaningful when lifted into the output space of the network. A possible\nsolution is proposed that enables the user to posit an appropriate Gaussian\nprocess covariance function for the task at hand. Our approach constructs a\nprior distribution for the parameters of the network, called a ridgelet prior,\nthat approximates the posited Gaussian process in the output space of the\nnetwork. In contrast to existing work on the connection between neural networks\nand Gaussian processes, our analysis is non-asymptotic, with finite sample-size\nerror bounds provided. This establishes the universality property that a\nBayesian neural network can approximate any Gaussian process whose covariance\nfunction is sufficiently regular. Our experimental assessment is limited to a\nproof-of-concept, where we demonstrate that the ridgelet prior can out-perform\nan unstructured prior on regression problems for which a suitable Gaussian\nprocess prior can be provided.",
          "link": "http://arxiv.org/abs/2010.08488",
          "publishedOn": "2022-01-13T00:40:19.637Z",
          "wordCount": 650,
          "title": "The Ridgelet Prior: A Covariance Function Approach to Prior Specification for Bayesian Neural Networks. (arXiv:2010.08488v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weeraprameshwara_G/0/1/0/all/0/1\">Gihan Weeraprameshwara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayawickrama_V/0/1/0/all/0/1\">Vihanga Jayawickrama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijeratne_Y/0/1/0/all/0/1\">Yudhanjaya Wijeratne</a>",
          "description": "The relationship between Facebook posts and the corresponding reaction\nfeature is an interesting subject to explore and understand. To archive this\nend, we test state-of-the-art Sinhala sentiment analysis models against a data\nset containing a decade worth of Sinhala posts with millions of reactions. For\nthe purpose of establishing benchmarks and with the goal of identifying the\nbest model for Sinhala sentiment analysis, we also test, on the same data set\nconfiguration, other deep learning models catered for sentiment analysis. In\nthis study we report that the 3 layer Bidirectional LSTM model achieves an F1\nscore of 84.58% for Sinhala sentiment analysis, surpassing the current\nstate-of-the-art model; Capsule B, which only manages to get an F1 score of\n82.04%. Further, since all the deep learning models show F1 scores above 75% we\nconclude that it is safe to claim that Facebook reactions are suitable to\npredict the sentiment of a text.",
          "link": "http://arxiv.org/abs/2201.03941",
          "publishedOn": "2022-01-13T00:40:19.630Z",
          "wordCount": 596,
          "title": "Sentiment Analysis with Deep Learning Models: A Comparative Study on a Decade of Sinhala Language Facebook Data. (arXiv:2201.03941v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2006.01169",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Paraschiakos_S/0/1/0/all/0/1\">Stylianos Paraschiakos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sa_C/0/1/0/all/0/1\">Cl&#xe1;udio Rebelo de S&#xe1;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Okai_J/0/1/0/all/0/1\">Jeremiah Okai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Slagboom_E/0/1/0/all/0/1\">Eline P. Slagboom</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beekman_M/0/1/0/all/0/1\">Marian Beekman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Knobbe_A/0/1/0/all/0/1\">Arno Knobbe</a>",
          "description": "Through the quantification of physical activity energy expenditure (PAEE),\nhealth care monitoring has the potential to stimulate vital and healthy ageing,\ninducing behavioural changes in older people and linking these to personal\nhealth gains. To be able to measure PAEE in a monitoring environment, methods\nfrom wearable accelerometers have been developed, however, mainly targeted\ntowards younger people. Since elderly subjects differ in energy requirements\nand range of physical activities, the current models may not be suitable for\nestimating PAEE among the elderly. Because past activities influence present\nPAEE, we propose a modeling approach known for its ability to model sequential\ndata, the Recurrent Neural Network (RNN). To train the RNN for an elderly\npopulation, we used the GOTOV dataset with 34 healthy participants of 60 years\nand older (mean 65 years old), performing 16 different activities. We used\naccelerometers placed on wrist and ankle, and measurements of energy counts by\nmeans of indirect calorimetry. After optimization, we propose an architecture\nconsisting of an RNN with 3 GRU layers and a feedforward network combining both\naccelerometer and participant-level data. In this paper, we describe our\nefforts to go beyond the standard facilities of a GRU-based RNN, with the aim\nof achieving accuracy surpassing the state of the art. These efforts include\nswitching aggregation function from mean to dispersion measures (SD, IQR, ...),\ncombining temporal and static data (person-specific details such as age,\nweight, BMI) and adding symbolic activity data as predicted by a previously\ntrained ML model. The resulting architecture manages to increase its\nperformance by approximatelly 10% while decreasing training input by a factor\nof 10. It can thus be employed to investigate associations of PAEE with\nvitality parameters related to metabolic and cognitive health and mental\nwell-being.",
          "link": "http://arxiv.org/abs/2006.01169",
          "publishedOn": "2022-01-13T00:40:19.624Z",
          "wordCount": 801,
          "title": "RNNs on Monitoring Physical Activity Energy Expenditure in Older People. (arXiv:2006.01169v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03796",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Ruidong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Rui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_B/0/1/0/all/0/1\">Bin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diange Yang</a>",
          "description": "Deep deterministic policy gradient (DDPG)-based car-following strategy can\nbreak through the constraints of the differential equation model due to the\nability of exploration on complex environments. However, the car-following\nperformance of DDPG is usually degraded by unreasonable reward function design,\ninsufficient training, and low sampling efficiency. In order to solve this kind\nof problem, a hybrid car-following strategy based on DDPG and cooperative\nadaptive cruise control (CACC) is proposed. First, the car-following process is\nmodeled as the Markov decision process to calculate CACC and DDPG\nsimultaneously at each frame. Given a current state, two actions are obtained\nfrom CACC and DDPG, respectively. Then, an optimal action, corresponding to the\none offering a larger reward, is chosen as the output of the hybrid strategy.\nMeanwhile, a rule is designed to ensure that the change rate of acceleration is\nsmaller than the desired value. Therefore, the proposed strategy not only\nguarantees the basic performance of car-following through CACC but also makes\nfull use of the advantages of exploration on complex environments via DDPG.\nFinally, simulation results show that the car-following performance of the\nproposed strategy is improved compared with that of DDPG and CACC.",
          "link": "http://arxiv.org/abs/2103.03796",
          "publishedOn": "2022-01-13T00:40:19.617Z",
          "wordCount": 675,
          "title": "Hybrid Car-Following Strategy based on Deep Deterministic Policy Gradient and Cooperative Adaptive Cruise Control. (arXiv:2103.03796v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.04093",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Garcia_D/0/1/0/all/0/1\">David Peral Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Cruz_Benito_J/0/1/0/all/0/1\">Juan Cruz-Benito</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Garcia_Penalvo_F/0/1/0/all/0/1\">Francisco Jos&#xe9; Garc&#xed;a-Pe&#xf1;alvo</a>",
          "description": "Quantum computing is the process of performing calculations using quantum\nmechanics. This field studies the quantum behavior of certain subatomic\nparticles for subsequent use in performing calculations, as well as for\nlarge-scale information processing. These capabilities can give quantum\ncomputers an advantage in terms of computational time and cost over classical\ncomputers. Nowadays, there are scientific challenges that are impossible to\nperform by classical computation due to computational complexity or the time\nthe calculation would take, and quantum computation is one of the possible\nanswers. However, current quantum devices have not yet the necessary qubits and\nare not fault-tolerant enough to achieve these goals. Nonetheless, there are\nother fields like machine learning or chemistry where quantum computation could\nbe useful with current quantum devices. This manuscript aims to present a\nSystematic Literature Review of the papers published between 2017 and 2021 to\nidentify, analyze and classify the different algorithms used in quantum machine\nlearning and their applications. Consequently, this study identified 52\narticles that used quantum machine learning techniques and algorithms. The main\ntypes of found algorithms are quantum implementations of classical machine\nlearning algorithms, such as support vector machines or the k-nearest neighbor\nmodel, and classical deep learning algorithms, like quantum neural networks.\nMany articles try to solve problems currently answered by classical machine\nlearning but using quantum devices and algorithms. Even though results are\npromising, quantum machine learning is far from achieving its full potential.\nAn improvement in the quantum hardware is required since the existing quantum\ncomputers lack enough quality, speed, and scale to allow quantum computing to\nachieve its full potential.",
          "link": "http://arxiv.org/abs/2201.04093",
          "publishedOn": "2022-01-13T00:40:19.598Z",
          "wordCount": 692,
          "title": "Systematic Literature Review: Quantum Machine Learning and its applications. (arXiv:2201.04093v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2008.04548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haonan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hailin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiaodong Lin</a>",
          "description": "Capturing the composition patterns of relations is a vital task in knowledge\ngraph completion. It also serves as a fundamental step towards multi-hop\nreasoning over learned knowledge. Previously, several rotation-based\ntranslational methods have been developed to model composite relations using\nthe product of a series of complex-valued diagonal matrices. However, these\nmethods tend to make several oversimplified assumptions on the composite\nrelations, e.g., forcing them to be commutative, independent from entities and\nlacking semantic hierarchy. To systematically tackle these problems, we have\ndeveloped a novel knowledge graph embedding method, named DensE, to provide an\nimproved modeling scheme for the complex composition patterns of relations. In\nparticular, our method decomposes each relation into an SO(3) group-based\nrotation operator and a scaling operator in the three dimensional (3-D)\nEuclidean space. This design principle leads to several advantages of our\nmethod: (1) For composite relations, the corresponding diagonal relation\nmatrices can be non-commutative, reflecting a predominant scenario in real\nworld applications; (2) Our model preserves the natural interaction between\nrelational operations and entity embeddings; (3) The scaling operation provides\nthe modeling power for the intrinsic semantic hierarchical structure of\nentities; (4) The enhanced expressiveness of DensE is achieved with high\ncomputational efficiency in terms of both parameter size and training time; and\n(5) Modeling entities in Euclidean space instead of quaternion space keeps the\ndirect geometrical interpretations of relational patterns. Experimental results\non multiple benchmark knowledge graphs show that DensE outperforms the current\nstate-of-the-art models for missing link prediction, especially on composite\nrelations.",
          "link": "http://arxiv.org/abs/2008.04548",
          "publishedOn": "2022-01-13T00:40:19.572Z",
          "wordCount": 723,
          "title": "DensE: An Enhanced Non-commutative Representation for Knowledge Graph Embedding with Adaptive Semantic Hierarchy. (arXiv:2008.04548v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raza_A/0/1/0/all/0/1\">Ali Raza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Kim Phuc Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehl_L/0/1/0/all/0/1\">Ludovic Koehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shujun Li</a>",
          "description": "Deep learning play a vital role in classifying different arrhythmias using\nthe electrocardiography (ECG) data. Nevertheless, training deep learning models\nnormally requires a large amount of data and it can lead to privacy concerns.\nUnfortunately, a large amount of healthcare data cannot be easily collected\nfrom a single silo. Additionally, deep learning models are like black-box, with\nno explainability of the predicted results, which is often required in clinical\nhealthcare. This limits the application of deep learning in real-world health\nsystems. In this paper, we design a new explainable artificial intelligence\n(XAI) based deep learning framework in a federated setting for ECG-based\nhealthcare applications. The federated setting is used to solve issues such as\ndata availability and privacy concerns. Furthermore, the proposed framework\nsetting effectively classifies arrhythmia's using an autoencoder and a\nclassifier, both based on a convolutional neural network (CNN). Additionally,\nwe propose an XAI-based module on top of the proposed classifier to explain the\nclassification results, which help clinical practitioners make quick and\nreliable decisions. The proposed framework was trained and tested using the\nMIT-BIH Arrhythmia database. The classifier achieved accuracy up to 94% and 98%\nfor arrhythmia detection using noisy and clean data, respectively, with\nfive-fold cross-validation.",
          "link": "http://arxiv.org/abs/2105.12497",
          "publishedOn": "2022-01-13T00:40:19.565Z",
          "wordCount": 677,
          "title": "Designing ECG Monitoring Healthcare System with Federated Transfer Learning and Explainable AI. (arXiv:2105.12497v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sisman_B/0/1/0/all/0/1\">Berrak Sisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_R/0/1/0/all/0/1\">Rajib Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>",
          "description": "Emotional voice conversion (EVC) seeks to convert the emotional state of an\nutterance while preserving the linguistic content and speaker identity. In EVC,\nemotions are usually treated as discrete categories overlooking the fact that\nspeech also conveys emotions with various intensity levels that the listener\ncan perceive. In this paper, we aim to explicitly characterize and control the\nintensity of emotion. We propose to disentangle the speaker style from\nlinguistic content and encode the speaker style into a style embedding in a\ncontinuous space that forms the prototype of emotion embedding. We further\nlearn the actual emotion encoder from an emotion-labelled database and study\nthe use of relative attributes to represent fine-grained emotion intensity. To\nensure emotional intelligibility, we incorporate emotion classification loss\nand emotion embedding similarity loss into the training of the EVC network. As\ndesired, the proposed network controls the fine-grained emotion intensity in\nthe output speech. Through both objective and subjective evaluations, we\nvalidate the effectiveness of the proposed network for emotional expressiveness\nand emotion intensity control.",
          "link": "http://arxiv.org/abs/2201.03967",
          "publishedOn": "2022-01-13T00:40:19.557Z",
          "wordCount": 613,
          "title": "Emotion Intensity and its Control for Emotional Voice Conversion. (arXiv:2201.03967v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2104.00409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atchade_Adelomou_P/0/1/0/all/0/1\">Parfait Atchade-Adelomou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casado_Fauli_D/0/1/0/all/0/1\">Daniel Casado-Fauli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golobardes_Ribe_E/0/1/0/all/0/1\">Elisabet Golobardes-Ribe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vilasis_Cardona_X/0/1/0/all/0/1\">Xavier Vilasis-Cardona</a>",
          "description": "Case-Based Reasoning (CBR) is an artificial intelligence approach to\nproblem-solving with a good record of success. This article proposes using\nQuantum Computing to improve some of the key processes of CBR, such that a\nQuantum Case-Based Reasoning (qCBR) paradigm can be defined. The focus is set\non designing and implementing a qCBR based on the variational principle that\nimproves its classical counterpart in terms of average accuracy, scalability\nand tolerance to overlapping. A comparative study of the proposed qCBR with a\nclassic CBR is performed for the case of the Social Workers' Problem as a\nsample of a combinatorial optimization problem with overlapping. The\nalgorithm's quantum feasibility is modelled with docplex and tested on IBMQ\ncomputers, and experimented on the Qibo framework.",
          "link": "http://arxiv.org/abs/2104.00409",
          "publishedOn": "2022-01-13T00:40:19.550Z",
          "wordCount": 571,
          "title": "quantum Case-Based Reasoning (qCBR). (arXiv:2104.00409v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.12908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1\">Joshua T. Vogelstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_J/0/1/0/all/0/1\">Jayanta Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helm_H/0/1/0/all/0/1\">Hayden S. Helm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1\">Will LeVine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1\">Ronak D. Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geisa_A/0/1/0/all/0/1\">Ali Geisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoyin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ven_G/0/1/0/all/0/1\">Gido M. van de Ven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Emily Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chenyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Weiwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tower_B/0/1/0/all/0/1\">Bryan Tower</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larson_J/0/1/0/all/0/1\">Jonathan Larson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1\">Christopher M. White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1\">Carey E. Priebe</a>",
          "description": "In biological learning, data are used to improve performance not only on the\ncurrent task, but also on previously encountered, and as yet unencountered\ntasks. In contrast, classical machine learning starts from a blank slate, or\ntabula rasa, using data only for the single task at hand. While typical\ntransfer learning algorithms can improve performance on future tasks, their\nperformance on prior tasks degrades upon learning new tasks (called\ncatastrophic forgetting). Many recent approaches for continual or lifelong\nlearning have attempted to maintain performance given new tasks. But striving\nto avoid forgetting sets the goal unnecessarily low: the goal of lifelong\nlearning, whether biological or artificial, should be to improve performance on\nboth past and future tasks with any new data. Our key insight is that we can\nensemble representations learned independently across tasks to transfer\nomnidirectionally, that is, jointly improve performance on both future\nunforeseen tasks (forward transfer) and past tasks (backward transfer).\nSpecifically, we propose two lifelong learners: one ensembling trees and the\nother ensembling networks. In both cases, the algorithms are able to learn\nsynergistically, improving performance on both past and future tasks in a\nvariety of simulated and real data scenarios, including tabular data, image\ndata, spoken data, and adversarial tasks. Moreover, they can do so with\nquasilinear space and time complexity, a requirement for any bona fide lifelong\nlearning system.",
          "link": "http://arxiv.org/abs/2004.12908",
          "publishedOn": "2022-01-13T00:40:19.533Z",
          "wordCount": 806,
          "title": "Representation Ensembling for Synergistic Lifelong Learning with Quasilinear Complexity. (arXiv:2004.12908v12 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.04088",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sana_J/0/1/0/all/0/1\">Joydeb Kumar Sana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abedin_M/0/1/0/all/0/1\">Mohammad Zoynul Abedin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">M. Sohel Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">M. Saifur Rahman</a>",
          "description": "Data transformation (DT) is a process that transfers the original data into a\nform which supports a particular classification algorithm and helps to analyze\nthe data for a special purpose. To improve the prediction performance we\ninvestigated various data transform methods. This study is conducted in a\ncustomer churn prediction (CCP) context in the telecommunication industry\n(TCI), where customer attrition is a common phenomenon. We have proposed a\nnovel approach of combining data transformation methods with the machine\nlearning models for the CCP problem. We conducted our experiments on publicly\navailable TCI datasets and assessed the performance in terms of the widely used\nevaluation measures (e.g. AUC, precision, recall, and F-measure). In this\nstudy, we presented comprehensive comparisons to affirm the effect of the\ntransformation methods. The comparison results and statistical test proved that\nmost of the proposed data transformation based optimized models improve the\nperformance of CCP significantly. Overall, an efficient and optimized CCP model\nfor the telecommunication industry has been presented through this manuscript.",
          "link": "http://arxiv.org/abs/2201.04088",
          "publishedOn": "2022-01-13T00:40:19.524Z",
          "wordCount": 598,
          "title": "Data transformation based optimized customer churn prediction model for the telecommunication industry. (arXiv:2201.04088v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04125",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shrestha_R/0/1/0/all/0/1\">Raju Shrestha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Romero_D/0/1/0/all/0/1\">Daniel Romero</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chepuri_S/0/1/0/all/0/1\">Sundeep Prabhakar Chepuri</a>",
          "description": "Radio maps find numerous applications in wireless communications and mobile\nrobotics tasks, including resource allocation, interference coordination, and\nmission planning. Although numerous techniques have been proposed to construct\nradio maps from spatially distributed measurements, the locations of such\nmeasurements are assumed predetermined beforehand. In contrast, this paper\nproposes spectrum surveying, where a mobile robot such as an unmanned aerial\nvehicle (UAV) collects measurements at a set of locations that are actively\nselected to obtain high-quality map estimates in a short surveying time. This\nis performed in two steps. First, two novel algorithms, a model-based online\nBayesian estimator and a data-driven deep learning algorithm, are devised for\nupdating a map estimate and an uncertainty metric that indicates the\ninformativeness of measurements at each possible location. These algorithms\noffer complementary benefits and feature constant complexity per measurement.\nSecond, the uncertainty metric is used to plan the trajectory of the UAV to\ngather measurements at the most informative locations. To overcome the\ncombinatorial complexity of this problem, a dynamic programming approach is\nproposed to obtain lists of waypoints through areas of large uncertainty in\nlinear time. Numerical experiments conducted on a realistic dataset confirm\nthat the proposed scheme constructs accurate radio maps quickly.",
          "link": "http://arxiv.org/abs/2201.04125",
          "publishedOn": "2022-01-13T00:40:19.516Z",
          "wordCount": 640,
          "title": "Spectrum Surveying: Active Radio Map Estimation with Autonomous UAVs. (arXiv:2201.04125v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdeljawad_A/0/1/0/all/0/1\">Ahmed Abdeljawad</a>",
          "description": "In this work, we explore the approximation capability of deep Rectified\nQuadratic Unit neural networks for H\\\"older-regular functions, with respect to\nthe uniform norm. We find that theoretical approximation heavily depends on the\nselected activation function in the neural network.",
          "link": "http://arxiv.org/abs/2201.03747",
          "publishedOn": "2022-01-13T00:40:19.509Z",
          "wordCount": 484,
          "title": "Deep Neural Network Approximation For H\\\"older Functions. (arXiv:2201.03747v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2004.01646",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Bo Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhiyun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1\">Srinivasan Parthasarathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1\">Xia Ning</a>",
          "description": "Next-basket recommendation considers the problem of recommending a set of\nitems into the next basket that users will purchase as a whole. In this paper,\nwe develop a novel mixed model with preferences, popularities and transitions\n(M2) for the next-basket recommendation. This method models three important\nfactors in next-basket generation process: 1) users' general preferences, 2)\nitems' global popularities and 3) transition patterns among items. Unlike\nexisting recurrent neural network-based approaches, M2 does not use the\ncomplicated networks to model the transitions among items, or generate\nembeddings for users. Instead, it has a simple encoder-decoder based approach\n(ed-Trans) to better model the transition patterns among items. We compared M2\nwith different combinations of the factors with 5 state-of-the-art next-basket\nrecommendation methods on 4 public benchmark datasets in recommending the\nfirst, second and third next basket. Our experimental results demonstrate that\nM2 significantly outperforms the state-of-the-art methods on all the datasets\nin all the tasks, with an improvement of up to 22.1%. In addition, our ablation\nstudy demonstrates that the ed-Trans is more effective than recurrent neural\nnetworks in terms of the recommendation performance. We also have a thorough\ndiscussion on various experimental protocols and evaluation metrics for\nnext-basket recommendation evaluation.",
          "link": "http://arxiv.org/abs/2004.01646",
          "publishedOn": "2022-01-13T00:40:19.502Z",
          "wordCount": 681,
          "title": "M2: Mixed Models with Preferences, Popularities and Transitions for Next-Basket Recommendation. (arXiv:2004.01646v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Almeida_M/0/1/0/all/0/1\">Mario Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskaridis_S/0/1/0/all/0/1\">Stefanos Laskaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1\">Stylianos I. Venieris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leontiadis_I/0/1/0/all/0/1\">Ilias Leontiadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas D. Lane</a>",
          "description": "Recently, there has been an explosive growth of mobile and embedded\napplications using convolutional neural networks(CNNs). To alleviate their\nexcessive computational demands, developers have traditionally resorted to\ncloud offloading, inducing high infrastructure costs and a strong dependence on\nnetworking conditions. On the other end, the emergence of powerful SoCs is\ngradually enabling on-device execution. Nonetheless, low- and mid-tier\nplatforms still struggle to run state-of-the-art CNNs sufficiently. In this\npaper, we present DynO, a distributed inference framework that combines the\nbest of both worlds to address several challenges, such as device\nheterogeneity, varying bandwidth and multi-objective requirements. Key\ncomponents that enable this are its novel CNN-specific data packing method,\nwhich exploits the variability of precision needs in different parts of the CNN\nwhen onloading computation, and its novel scheduler that jointly tunes the\npartition point and transferred data precision at run time to adapt inference\nto its execution environment. Quantitative evaluation shows that DynO\noutperforms the current state-of-the-art, improving throughput by over an order\nof magnitude over device-only execution and up to 7.9x over competing CNN\noffloading systems, with up to 60x less data transferred.",
          "link": "http://arxiv.org/abs/2104.09949",
          "publishedOn": "2022-01-13T00:40:19.482Z",
          "wordCount": 685,
          "title": "DynO: Dynamic Onloading of Deep Neural Networks from Cloud to Device. (arXiv:2104.09949v2 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.04100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baechler_G/0/1/0/all/0/1\">Gilles Baechler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tragut_M/0/1/0/all/0/1\">Manuel Tragut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>",
          "description": "The layout of a mobile screen is a critical data source for UI designresearch\nand semantic understanding of the screen. However, UIlayouts in existing\ndatasets are often noisy, have mismatches withtheir visual representation, or\nconsists of generic or app-specifictypes that are difficult to analyze and\nmodel. In this paper, wepropose the CLAY pipeline that uses a deep learning\napproach fordenoising UI layouts, allowing us to automatically improve\nexistingmobile UI layout datasets at scale. Our pipeline takes both\nthescreenshot and the raw UI layout, and annotates the raw layout byremoving\nincorrect nodes and assigning a semantically meaningfultype to each node. To\nexperiment with our data-cleaning pipeline,we create the CLAY dataset of 59,555\nhuman-annotated screenlayouts, based on screenshots and raw layouts from Rico,\na publicmobile UI corpus. Our deep models achieve high accuracy withF1 scores\nof 82.7% for detecting layout objects that do not have avalid visual\nrepresentation and 85.9% for recognizing object types,which significantly\noutperforms a heuristic baseline. Our work laysa foundation for creating\nlarge-scale high quality UI layout datasetsfor data-driven mobile UI research\nand reduces the need of manuallabeling efforts that are prohibitively\nexpensive.",
          "link": "http://arxiv.org/abs/2201.04100",
          "publishedOn": "2022-01-13T00:40:19.475Z",
          "wordCount": 624,
          "title": "Learning to Denoise Raw Mobile UI Layouts for ImprovingDatasets at Scale. (arXiv:2201.04100v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1\">Jack Parker-Holder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_R/0/1/0/all/0/1\">Raghu Rajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyou Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biedenkapp_A/0/1/0/all/0/1\">Andr&#xe9; Biedenkapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yingjie Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eimer_T/0/1/0/all/0/1\">Theresa Eimer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baohe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Vu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calandra_R/0/1/0/all/0/1\">Roberto Calandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1\">Aleksandra Faust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1\">Marius Lindauer</a>",
          "description": "The combination of Reinforcement Learning (RL) with deep learning has led to\na series of impressive feats, with many believing (deep) RL provides a path\ntowards generally capable agents. However, the success of RL agents is often\nhighly sensitive to design choices in the training process, which may require\ntedious and error-prone manual tuning. This makes it challenging to use RL for\nnew problems, while also limits its full potential. In many other areas of\nmachine learning, AutoML has shown it is possible to automate such design\nchoices and has also yielded promising initial results when applied to RL.\nHowever, Automated Reinforcement Learning (AutoRL) involves not only standard\napplications of AutoML but also includes additional challenges unique to RL,\nthat naturally produce a different set of methods. As such, AutoRL has been\nemerging as an important area of research in RL, providing promise in a variety\nof applications from RNA design to playing games such as Go. Given the\ndiversity of methods and environments considered in RL, much of the research\nhas been conducted in distinct subfields, ranging from meta-learning to\nevolution. In this survey we seek to unify the field of AutoRL, we provide a\ncommon taxonomy, discuss each area in detail and pose open problems which would\nbe of interest to researchers going forward.",
          "link": "http://arxiv.org/abs/2201.03916",
          "publishedOn": "2022-01-13T00:40:19.457Z",
          "wordCount": 651,
          "title": "Automated Reinforcement Learning (AutoRL): A Survey and Open Problems. (arXiv:2201.03916v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1907.07330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Finocchiaro_J/0/1/0/all/0/1\">Jessie Finocchiaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frongillo_R/0/1/0/all/0/1\">Rafael Frongillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waggoner_B/0/1/0/all/0/1\">Bo Waggoner</a>",
          "description": "We formalize and study the natural approach of designing convex surrogate\nloss functions via embeddings, for problems such as classification, ranking, or\nstructured prediction. In this approach, one embeds each of the finitely many\npredictions (e.g.\\ rankings) as a point in $\\mathbb{R}^d$, assigns the original\nloss values to these points, and \"convexifies\" the loss in some way to obtain a\nsurrogate. We establish a strong connection between this approach and\npolyhedral (piecewise-linear convex) surrogate losses. Given any polyhedral\nloss $L$, we give a construction of a link function through which $L$ is a\nconsistent surrogate for the loss it embeds. Conversely, we show how to\nconstruct a consistent polyhedral surrogate for any given discrete loss. Our\nframework yields succinct proofs of consistency or inconsistency of various\npolyhedral surrogates in the literature, and for inconsistent surrogates, it\nfurther reveals the discrete losses for which these surrogates are consistent.\nWe show some additional structure of embeddings, such as the equivalence of\nembedding and matching Bayes risks, and the equivalence of various notions of\nnon-redudancy. Using these results, we establish that indirect elicitation, a\nnecessary condition for consistency, is also sufficient when working with\npolyhedral surrogates.",
          "link": "http://arxiv.org/abs/1907.07330",
          "publishedOn": "2022-01-13T00:40:19.449Z",
          "wordCount": 646,
          "title": "An Embedding Framework for Consistent Polyhedral Surrogates. (arXiv:1907.07330v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.04065",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Ya-Lin Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsieh_C/0/1/0/all/0/1\">Chia-Ying Hsieh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1\">Jian-Xue Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_C/0/1/0/all/0/1\">Chun-Shu Wei</a>",
          "description": "We have developed a graphic user interface (GUI), ExBrainable, dedicated to\nconvolutional neural networks (CNN) model training and visualization in\nelectroencephalography (EEG) decoding. Available functions include model\ntraining, evaluation, and parameter visualization in terms of temporal and\nspatial representations. We demonstrate these functions using a well-studied\npublic dataset of motor-imagery EEG and compare the results with existing\nknowledge of neuroscience. The primary objective of ExBrainable is to provide a\nfast, simplified, and user-friendly solution of EEG decoding for investigators\nacross disciplines to leverage cutting-edge methods in brain/neuroscience\nresearch.",
          "link": "http://arxiv.org/abs/2201.04065",
          "publishedOn": "2022-01-13T00:40:19.431Z",
          "wordCount": 524,
          "title": "ExBrainable: An Open-Source GUI for CNN-based EEG Decoding and Model Interpretation. (arXiv:2201.04065v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03965",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sikarwar_A/0/1/0/all/0/1\">Ankur Sikarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreiman_G/0/1/0/all/0/1\">Gabriel Kreiman</a>",
          "description": "In recent years, multi-modal transformers have shown significant progress in\nVision-Language tasks, such as Visual Question Answering (VQA), outperforming\nprevious architectures by a considerable margin. This improvement in VQA is\noften attributed to the rich interactions between vision and language streams.\nIn this work, we investigate the efficacy of co-attention transformer layers in\nhelping the network focus on relevant regions while answering the question. We\ngenerate visual attention maps using the question-conditioned image attention\nscores in these co-attention layers. We evaluate the effect of the following\ncritical components on visual attention of a state-of-the-art VQA model: (i)\nnumber of object region proposals, (ii) question part of speech (POS) tags,\n(iii) question semantics, (iv) number of co-attention layers, and (v) answer\naccuracy. We compare the neural network attention maps against human attention\nmaps both qualitatively and quantitatively. Our findings indicate that\nco-attention transformer modules are crucial in attending to relevant regions\nof the image given a question. Importantly, we observe that the semantic\nmeaning of the question is not what drives visual attention, but specific\nkeywords in the question do. Our work sheds light on the function and\ninterpretation of co-attention transformer layers, highlights gaps in current\nnetworks, and can guide the development of future VQA models and networks that\nsimultaneously process visual and language streams.",
          "link": "http://arxiv.org/abs/2201.03965",
          "publishedOn": "2022-01-13T00:40:19.424Z",
          "wordCount": 643,
          "title": "On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering. (arXiv:2201.03965v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kurin_V/0/1/0/all/0/1\">Vitaly Kurin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palma_A/0/1/0/all/0/1\">Alessandro De Palma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostrikov_I/0/1/0/all/0/1\">Ilya Kostrikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1\">Shimon Whiteson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">M. Pawan Kumar</a>",
          "description": "Recent multi-task learning research argues against unitary scalarization,\nwhere training simply minimizes the sum of the task losses. Several ad-hoc\nmulti-task optimization algorithms have instead been proposed, inspired by\nvarious hypotheses about what makes multi-task settings difficult. The majority\nof these optimizers require per-task gradients, and introduce significant\nmemory, runtime, and implementation overhead. We present a theoretical analysis\nsuggesting that many specialized multi-task optimizers can be interpreted as\nforms of regularization. Moreover, we show that, when coupled with standard\nregularization and stabilization techniques from single-task learning, unitary\nscalarization matches or improves upon the performance of complex multi-task\noptimizers in both supervised and reinforcement learning settings. We believe\nour results call for a critical reevaluation of recent research in the area.",
          "link": "http://arxiv.org/abs/2201.04122",
          "publishedOn": "2022-01-13T00:40:19.417Z",
          "wordCount": 556,
          "title": "In Defense of the Unitary Scalarization for Deep Multi-Task Learning. (arXiv:2201.04122v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04086",
          "author": "<a href=\"http://arxiv.org/find/gr-qc/1/au:+Dahal_D/0/1/0/all/0/1\">Damodar Dahal</a>",
          "description": "Common Spatial Patterns (CSP) is a feature extraction algorithm widely used\nin Brain-Computer Interface (BCI) Systems for detecting Event-Related\nPotentials (ERPs) in multi-channel magneto/electroencephalography (MEG/EEG)\ntime series data. In this article, we develop and apply a CSP algorithm to the\nproblem of identifying whether a given epoch of multi-detector Gravitational\nWave (GW) strains contains coalescenses. Paired with Signal Processing\ntechniques and a Logistic Regression classifier, we find that our pipeline is\ncorrectly able to detect 76 out of 82 confident events from Gravitational Wave\nTransient Catalog, using H1 and L1 strains, with a classification score of\n$93.72 \\pm 0.04\\%$ using $10 \\times 5$ cross validation. The false negative\nevents were: GW170817-v3, GW191219 163120-v1, GW200115 042309-v2, GW200210\n092254-v1, GW200220 061928-v1, and GW200322 091133-v1.",
          "link": "http://arxiv.org/abs/2201.04086",
          "publishedOn": "2022-01-13T00:40:19.409Z",
          "wordCount": 551,
          "title": "Application of Common Spatial Patterns in Gravitational Waves Detection. (arXiv:2201.04086v1 [gr-qc])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04070",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jones_M/0/1/0/all/0/1\">Ms Yola Jones</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deligianni_D/0/1/0/all/0/1\">Dr Fani Deligianni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalton_D/0/1/0/all/0/1\">Dr Jeff Dalton</a>",
          "description": "Cardiovascular disease is a large worldwide healthcare issue; symptoms often\npresent suddenly with minimal warning. The electrocardiogram (ECG) is a fast,\nsimple and reliable method of evaluating the health of the heart, by measuring\nelectrical activity recorded through electrodes placed on the skin. ECGs often\nneed to be analyzed by a cardiologist, taking time which could be spent on\nimproving patient care and outcomes. Because of this, automatic ECG\nclassification systems using machine learning have been proposed, which can\nlearn complex interactions between ECG features and use this to detect\nabnormalities. However, algorithms built for this purpose often fail to\ngeneralize well to unseen data, reporting initially impressive results which\ndrop dramatically when applied to new environments. Additionally, machine\nlearning algorithms suffer a \"black-box\" issue, in which it is difficult to\ndetermine how a decision has been made. This is vital for applications in\nhealthcare, as clinicians need to be able to verify the process of evaluation\nin order to trust the algorithm. This paper proposes a method for visualizing\nmodel decisions across each class in the MIT-BIH arrhythmia dataset, using\nadapted saliency maps averaged across complete classes to determine what\npatterns are being learned. We do this by building two algorithms based on\nstate-of-the-art models. This paper highlights how these maps can be used to\nfind problems in the model which could be affecting generalizability and model\nperformance. Comparing saliency maps across complete classes gives an overall\nimpression of confounding variables or other biases in the model, unlike what\nwould be highlighted when comparing saliency maps on an ECG-by-ECG basis.",
          "link": "http://arxiv.org/abs/2201.04070",
          "publishedOn": "2022-01-13T00:40:19.374Z",
          "wordCount": 685,
          "title": "Improving ECG Classification Interpretability using Saliency Maps. (arXiv:2201.04070v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03992",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kaczmar_Michalska_J/0/1/0/all/0/1\">J. Kaczmar-Michalska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hajizadeh_N/0/1/0/all/0/1\">N.R. Hajizadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rzepiela_A/0/1/0/all/0/1\">A.J. Rzepiela</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Norrelykke_S/0/1/0/all/0/1\">S.F. N&#xf8;rrelykke</a>",
          "description": "Image quality is a nebulous concept with different meanings to different\npeople. To quantify image quality a relative difference is typically calculated\nbetween a corrupted image and a ground truth image. But what metric should we\nuse for measuring this difference? Ideally, the metric should perform well for\nboth natural and scientific images. The structural similarity index (SSIM) is a\ngood measure for how humans perceive image similarities, but is not sensitive\nto differences that are scientifically meaningful in microscopy. In electron\nand super-resolution microscopy, the Fourier Ring Correlation (FRC) is often\nused, but is little known outside of these fields. Here we show that the FRC\ncan equally well be applied to natural images, e.g. the Google Open Images\ndataset. We then define a loss function based on the FRC, show that it is\nanalytically differentiable, and use it to train a U-net for denoising of\nimages. This FRC-based loss function allows the network to train faster and\nachieve similar or better results than when using L1- or L2- based losses. We\nalso investigate the properties and limitations of neural network denoising\nwith the FRC analysis.",
          "link": "http://arxiv.org/abs/2201.03992",
          "publishedOn": "2022-01-13T00:40:19.367Z",
          "wordCount": 624,
          "title": "Image quality measurements and denoising using Fourier Ring Correlations. (arXiv:2201.03992v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04068",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+OLuing_M/0/1/0/all/0/1\">Mervyn O&#x27;Luing</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Prestwich_S/0/1/0/all/0/1\">Steven Prestwich</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tarim_S/0/1/0/all/0/1\">S. Armagan Tarim</a>",
          "description": "In this study we propose a hybrid estimation of distribution algorithm (HEDA)\nto solve the joint stratification and sample allocation problem. This is a\ncomplex problem in which each the quality of each stratification from the set\nof all possible stratifications is measured its optimal sample allocation. EDAs\nare stochastic black-box optimization algorithms which can be used to estimate,\nbuild and sample probability models in the search for an optimal\nstratification. In this paper we enhance the exploitation properties of the EDA\nby adding a simulated annealing algorithm to make it a hybrid EDA. Results of\nempirical comparisons for atomic and continuous strata show that the HEDA\nattains the bests results found so far when compared to benchmark tests on the\nsame data using a grouping genetic algorithm, simulated annealing algorithm or\nhill-climbing algorithm. However, the execution times and total execution are,\nin general, higher for the HEDA.",
          "link": "http://arxiv.org/abs/2201.04068",
          "publishedOn": "2022-01-13T00:40:19.360Z",
          "wordCount": 583,
          "title": "A hybrid estimation of distribution algorithm for joint stratification and sample allocation. (arXiv:2201.04068v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Coupette_C/0/1/0/all/0/1\">Corinna Coupette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalleiger_S/0/1/0/all/0/1\">Sebastian Dalleiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vreeken_J/0/1/0/all/0/1\">Jilles Vreeken</a>",
          "description": "How does neural connectivity in autistic children differ from neural\nconnectivity in healthy children or autistic youths? What patterns in global\ntrade networks are shared across classes of goods, and how do these patterns\nchange over time? Answering questions like these requires us to differentially\ndescribe groups of graphs: Given a set of graphs and a partition of these\ngraphs into groups, discover what graphs in one group have in common, how they\nsystematically differ from graphs in other groups, and how multiple groups of\ngraphs are related. We refer to this task as graph group analysis, which seeks\nto describe similarities and differences between graph groups by means of\nstatistically significant subgraphs. To perform graph group analysis, we\nintroduce Gragra, which uses maximum entropy modeling to identify a\nnon-redundant set of subgraphs with statistically significant associations to\none or more graph groups. Through an extensive set of experiments on a wide\nrange of synthetic and real-world graph groups, we confirm that Gragra works\nwell in practice.",
          "link": "http://arxiv.org/abs/2201.04064",
          "publishedOn": "2022-01-13T00:40:19.343Z",
          "wordCount": 600,
          "title": "Differentially Describing Groups of Graphs. (arXiv:2201.04064v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04069",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Martinez_I/0/1/0/all/0/1\">I&#xf1;igo Martinez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Otamendi_U/0/1/0/all/0/1\">Urtzi Otamendi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Olaizola_I/0/1/0/all/0/1\">Igor G. Olaizola</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Solsona_R/0/1/0/all/0/1\">Roger Solsona</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maiza_M/0/1/0/all/0/1\">Mikel Maiza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Viles_E/0/1/0/all/0/1\">Elisabeth Viles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fernandez_A/0/1/0/all/0/1\">Arturo Fernandez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arzua_I/0/1/0/all/0/1\">Ignacio Arzua</a>",
          "description": "Accurate temperature measurements are essential for the proper monitoring and\ncontrol of industrial furnaces. However, measurement uncertainty is a risk for\nsuch a critical parameter. Certain instrumental and environmental errors must\nbe considered when using spectral-band radiation thermometry techniques, such\nas the uncertainty in the emissivity of the target surface, reflected radiation\nfrom surrounding objects, or atmospheric absorption and emission, to name a\nfew. Undesired contributions to measured radiation can be isolated using\nmeasurement models, also known as error-correction models. This paper presents\na methodology for budgeting significant sources of error and uncertainty during\ntemperature measurements in a petrochemical furnace scenario. A continuous\nmonitoring system is also presented, aided by a deep-learning-based measurement\ncorrection model, to allow domain experts to analyze the furnace's operation in\nreal-time. To validate the proposed system's functionality, a real-world\napplication case in a petrochemical plant is presented. The proposed solution\ndemonstrates the viability of precise industrial furnace monitoring, thereby\nincreasing operational security and improving the efficiency of such\nenergy-intensive systems.",
          "link": "http://arxiv.org/abs/2201.04069",
          "publishedOn": "2022-01-13T00:40:19.336Z",
          "wordCount": 640,
          "title": "A novel method for error analysis in radiation thermometry with application to industrial furnaces. (arXiv:2201.04069v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/1910.04858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mi_L/0/1/0/all/0/1\">Lu Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonglong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shavit_N/0/1/0/all/0/1\">Nir Shavit</a>",
          "description": "Uncertainty estimation is an essential step in the evaluation of the\nrobustness for deep learning models in computer vision, especially when applied\nin risk-sensitive areas. However, most state-of-the-art deep learning models\neither fail to obtain uncertainty estimation or need significant modification\n(e.g., formulating a proper Bayesian treatment) to obtain it. Most previous\nmethods are not able to take an arbitrary model off the shelf and generate\nuncertainty estimation without retraining or redesigning it. To address this\ngap, we perform a systematic exploration into training-free uncertainty\nestimation for dense regression, an unrecognized yet important problem, and\nprovide a theoretical construction justifying such estimations. We propose\nthree simple and scalable methods to analyze the variance of outputs from a\ntrained network under tolerable perturbations: infer-transformation,\ninfer-noise, and infer-dropout. They operate solely during the inference,\nwithout the need to re-train, re-design, or fine-tune the models, as typically\nrequired by state-of-the-art uncertainty estimation methods. Surprisingly, even\nwithout involving such perturbations in training, our methods produce\ncomparable or even better uncertainty estimation when compared to\ntraining-required state-of-the-art methods.",
          "link": "http://arxiv.org/abs/1910.04858",
          "publishedOn": "2022-01-13T00:40:19.330Z",
          "wordCount": 654,
          "title": "Training-Free Uncertainty Estimation for Dense Regression: Sensitivity as a Surrogate. (arXiv:1910.04858v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vodisch_N/0/1/0/all/0/1\">Niclas V&#xf6;disch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_O/0/1/0/all/0/1\">Ozan Unal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>",
          "description": "Existing learning methods for LiDAR-based applications use 3D points scanned\nunder a pre-determined beam configuration, e.g., the elevation angles of beams\nare often evenly distributed. Those fixed configurations are task-agnostic, so\nsimply using them can lead to sub-optimal performance. In this work, we take a\nnew route to learn to optimize the LiDAR beam configuration for a given\napplication. Specifically, we propose a reinforcement learning-based\nlearning-to-optimize (RL-L2O) framework to automatically optimize the beam\nconfiguration in an end-to-end manner for different LiDAR-based applications.\nThe optimization is guided by the final performance of the target task and thus\nour method can be integrated easily with any LiDAR-based application as a\nsimple drop-in module. The method is especially useful when a low-resolution\n(low-cost) LiDAR is needed, for instance, for system deployment at a massive\nscale. We use our method to search for the beam configuration of a\nlow-resolution LiDAR for two important tasks: 3D object detection and\nlocalization. Experiments show that the proposed RL-L2O method improves the\nperformance in both tasks significantly compared to the baseline methods. We\nbelieve that a combination of our method with the recent advances of\nprogrammable LiDARs can start a new research direction for LiDAR-based active\nperception. The code is publicly available at\nhttps://github.com/vniclas/lidar_beam_selection",
          "link": "http://arxiv.org/abs/2201.03860",
          "publishedOn": "2022-01-13T00:40:19.282Z",
          "wordCount": 648,
          "title": "End-To-End Optimization of LiDAR Beam Configuration for 3D Object Detection and Localization. (arXiv:2201.03860v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1\">Jesus Bujalance Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1\">Fabien Moutarde</a>",
          "description": "During recent years, deep reinforcement learning (DRL) has made successful\nincursions into complex decision-making applications such as robotics,\nautonomous driving or video games. In the search for more sample-efficient\nalgorithms, a promising direction is to leverage as much external off-policy\ndata as possible. One staple of this data-driven approach is to learn from\nexpert demonstrations. In the past, multiple ideas have been proposed to make\ngood use of the demonstrations added to the replay buffer, such as pretraining\non demonstrations only or minimizing additional cost functions. We present a\nnew method, able to leverage demonstrations and episodes collected online in\nany sparse-reward environment with any off-policy algorithm. Our method is\nbased on a reward bonus given to demonstrations and successful episodes,\nencouraging expert imitation and self-imitation. First, we give a reward bonus\nto the transitions coming from demonstrations to encourage the agent to match\nthe demonstrated behaviour. Then, upon collecting a successful episode, we\nrelabel its transitions with the same bonus before adding them to the replay\nbuffer, encouraging the agent to also match its previous successes. Our\nexperiments focus on manipulation robotics, specifically on three tasks for a 6\ndegrees-of-freedom robotic arm in simulation. We show that our method based on\nreward relabeling improves the performance of the base algorithm (SAC and DDPG)\non these tasks, even in the absence of demonstrations. Furthermore, integrating\ninto our method two improvements from previous works allows our approach to\noutperform all baselines.",
          "link": "http://arxiv.org/abs/2201.03834",
          "publishedOn": "2022-01-13T00:40:19.274Z",
          "wordCount": 672,
          "title": "Reward Relabelling for combined Reinforcement and Imitation Learning on sparse-reward tasks. (arXiv:2201.03834v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04038",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wendi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yingce Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>",
          "description": "In many real-world scenarios, we often deal with streaming data that is\nsequentially collected over time. Due to the non-stationary nature of the\nenvironment, the streaming data distribution may change in unpredictable ways,\nwhich is known as concept drift. To handle concept drift, previous methods\nfirst detect when/where the concept drift happens and then adapt models to fit\nthe distribution of the latest data. However, there are still many cases that\nsome underlying factors of environment evolution are predictable, making it\npossible to model the future concept drift trend of the streaming data, while\nsuch cases are not fully explored in previous work.\n\nIn this paper, we propose a novel method DDG-DA, that can effectively\nforecast the evolution of data distribution and improve the performance of\nmodels. Specifically, we first train a predictor to estimate the future data\ndistribution, then leverage it to generate training samples, and finally train\nmodels on the generated data. We conduct experiments on three real-world tasks\n(forecasting on stock price trend, electricity load and solar irradiance) and\nobtain significant improvement on multiple widely-used models.",
          "link": "http://arxiv.org/abs/2201.04038",
          "publishedOn": "2022-01-13T00:40:19.267Z",
          "wordCount": 616,
          "title": "DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation. (arXiv:2201.04038v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kundacina_O/0/1/0/all/0/1\">Ognjen Kundacina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosovic_M/0/1/0/all/0/1\">Mirsad Cosovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vukobratovic_D/0/1/0/all/0/1\">Dejan Vukobratovic</a>",
          "description": "The goal of the state estimation (SE) algorithm is to estimate complex bus\nvoltages as state variables based on the available set of measurements in the\npower system. Because phasor measurement units (PMUs) are increasingly being\nused in transmission power systems, there is a need for a fast SE solver that\ncan take advantage of PMU high sampling rates. This paper proposes training a\ngraph neural network (GNN) to learn the estimates given the PMU voltage and\ncurrent measurements as inputs, with the intent of obtaining fast and accurate\npredictions during the evaluation phase. GNN is trained using synthetic\ndatasets, created by randomly sampling sets of measurements in the power system\nand labelling them with a solution obtained using a linear SE with PMUs solver.\nThe presented results display the accuracy of GNN predictions in various test\nscenarios and tackle the sensitivity of the predictions to the missing input\ndata.",
          "link": "http://arxiv.org/abs/2201.04056",
          "publishedOn": "2022-01-13T00:40:19.249Z",
          "wordCount": 589,
          "title": "State Estimation in Electric Power Systems Leveraging Graph Neural Networks. (arXiv:2201.04056v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Michelucci_U/0/1/0/all/0/1\">Umberto Michelucci</a>",
          "description": "In this article, we will look at autoencoders. This article covers the\nmathematics and the fundamental concepts of autoencoders. We will discuss what\nthey are, what the limitations are, the typical use cases, and we will look at\nsome examples. We will start with a general introduction to autoencoders, and\nwe will discuss the role of the activation function in the output layer and the\nloss function. We will then discuss what the reconstruction error is. Finally,\nwe will look at typical applications as dimensionality reduction,\nclassification, denoising, and anomaly detection. This paper contains the notes\nof a PhD-level lecture on autoencoders given in 2021.",
          "link": "http://arxiv.org/abs/2201.03898",
          "publishedOn": "2022-01-13T00:40:19.242Z",
          "wordCount": 518,
          "title": "An Introduction to Autoencoders. (arXiv:2201.03898v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03698",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bacci_E/0/1/0/all/0/1\">Edoardo Bacci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parker_D/0/1/0/all/0/1\">David Parker</a>",
          "description": "Deep reinforcement learning is an increasingly popular technique for\nsynthesising policies to control an agent's interaction with its environment.\nThere is also growing interest in formally verifying that such policies are\ncorrect and execute safely. Progress has been made in this area by building on\nexisting work for verification of deep neural networks and of continuous-state\ndynamical systems. In this paper, we tackle the problem of verifying\nprobabilistic policies for deep reinforcement learning, which are used to, for\nexample, tackle adversarial environments, break symmetries and manage\ntrade-offs. We propose an abstraction approach, based on interval Markov\ndecision processes, that yields probabilistic guarantees on a policy's\nexecution, and present techniques to build and solve these models using\nabstract interpretation, mixed-integer linear programming, entropy-based\nrefinement and probabilistic model checking. We implement our approach and\nillustrate its effectiveness on a selection of reinforcement learning\nbenchmarks.",
          "link": "http://arxiv.org/abs/2201.03698",
          "publishedOn": "2022-01-13T00:40:19.235Z",
          "wordCount": 563,
          "title": "Verified Probabilistic Policies for Deep Reinforcement Learning. (arXiv:2201.03698v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03822",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Meuwly_M/0/1/0/all/0/1\">M. Meuwly</a>",
          "description": "Atomistic simulations using accurate energy functions can provide\nmolecular-level insight into functional motions of molecules in the gas- and in\nthe condensed phase. Together with recently developed and currently pursued\nefforts in integrating and combining this with machine learning techniques\nprovides a unique opportunity to bring such dynamics simulations closer to\nreality. This perspective delineates the present status of the field from\nefforts of others in the field and some of your own work and discusses open\nquestions and future prospects.",
          "link": "http://arxiv.org/abs/2201.03822",
          "publishedOn": "2022-01-13T00:40:19.221Z",
          "wordCount": 509,
          "title": "Atomistic Simulations for Reactions and Spectroscopy in the Era of Machine Learning -- Quo Vadis?. (arXiv:2201.03822v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_C/0/1/0/all/0/1\">Chhavi Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhe_A/0/1/0/all/0/1\">Ankur Gandhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulyko_I/0/1/0/all/0/1\">Ivan Bulyko</a>",
          "description": "End-to-end (E2E) automatic speech recognition models like Recurrent Neural\nNetworks Transducer (RNN-T) are becoming a popular choice for streaming ASR\napplications like voice assistants. While E2E models are very effective at\nlearning representation of the training data they are trained on, their\naccuracy on unseen domains remains a challenging problem. Additionally, these\nmodels require paired audio and text training data, are computationally\nexpensive and are difficult to adapt towards the fast evolving nature of\nconversational speech. In this work, we explore a contextual biasing approach\nusing likelihood-ratio that leverages text data sources to adapt RNN-T model to\nnew domains and entities. We show that this method is effective in improving\nrare words recognition, and results in a relative improvement of 10% in 1-best\nword error rate (WER) and 10% in n-best Oracle WER (n=8) on multiple\nout-of-domain datasets without any degradation on a general dataset. We also\nshow that complementing the contextual biasing adaptation with adaptation of a\nsecond-pass rescoring model gives additive WER improvements.",
          "link": "http://arxiv.org/abs/2201.03655",
          "publishedOn": "2022-01-13T00:40:19.214Z",
          "wordCount": 597,
          "title": "A Likelihood Ratio based Domain Adaptation Method for E2E Models. (arXiv:2201.03655v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ryabinin_M/0/1/0/all/0/1\">Max Ryabinin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorbunov_E/0/1/0/all/0/1\">Eduard Gorbunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plokhotnyuk_V/0/1/0/all/0/1\">Vsevolod Plokhotnyuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pekhimenko_G/0/1/0/all/0/1\">Gennady Pekhimenko</a>",
          "description": "Training deep neural networks on large datasets can often be accelerated by\nusing multiple compute nodes. This approach, known as distributed training, can\nutilize hundreds of computers via specialized message-passing protocols such as\nRing All-Reduce. However, running these protocols at scale requires reliable\nhigh-speed networking that is only available in dedicated clusters. In\ncontrast, many real-world applications, such as federated learning and\ncloud-based distributed training, operate on unreliable devices with unstable\nnetwork bandwidth. As a result, these applications are restricted to using\nparameter servers or gossip-based averaging protocols. In this work, we lift\nthat restriction by proposing Moshpit All-Reduce - an iterative averaging\nprotocol that exponentially converges to the global average. We demonstrate the\nefficiency of our protocol for distributed optimization with strong theoretical\nguarantees. The experiments show 1.3x speedup for ResNet-50 training on\nImageNet compared to competitive gossip-based strategies and 1.5x speedup when\ntraining ALBERT-large from scratch using preemptible compute nodes.",
          "link": "http://arxiv.org/abs/2103.03239",
          "publishedOn": "2022-01-13T00:40:17.933Z",
          "wordCount": 644,
          "title": "Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices. (arXiv:2103.03239v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03968",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fallah_A/0/1/0/all/0/1\">Alireza Fallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makhdoumi_A/0/1/0/all/0/1\">Ali Makhdoumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malekian_A/0/1/0/all/0/1\">Azarakhsh Malekian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozdaglar_A/0/1/0/all/0/1\">Asuman Ozdaglar</a>",
          "description": "We consider a platform's problem of collecting data from privacy sensitive\nusers to estimate an underlying parameter of interest. We formulate this\nquestion as a Bayesian-optimal mechanism design problem, in which an individual\ncan share her (verifiable) data in exchange for a monetary reward or services,\nbut at the same time has a (private) heterogeneous privacy cost which we\nquantify using differential privacy. We consider two popular differential\nprivacy settings for providing privacy guarantees for the users: central and\nlocal. In both settings, we establish minimax lower bounds for the estimation\nerror and derive (near) optimal estimators for given heterogeneous privacy loss\nlevels for users. Building on this characterization, we pose the mechanism\ndesign problem as the optimal selection of an estimator and payments that will\nelicit truthful reporting of users' privacy sensitivities. Under a regularity\ncondition on the distribution of privacy sensitivities we develop efficient\nalgorithmic mechanisms to solve this problem in both privacy settings. Our\nmechanism in the central setting can be implemented in time $\\mathcal{O}(n \\log\nn)$ where $n$ is the number of users and our mechanism in the local setting\nadmits a Polynomial Time Approximation Scheme (PTAS).",
          "link": "http://arxiv.org/abs/2201.03968",
          "publishedOn": "2022-01-13T00:40:17.926Z",
          "wordCount": 627,
          "title": "Optimal and Differentially Private Data Acquisition: Central and Local Mechanisms. (arXiv:2201.03968v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Loveland_D/0/1/0/all/0/1\">Donald Loveland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jiayi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhathena_A/0/1/0/all/0/1\">Aaresh Farrokh Bhathena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yiyang Lu</a>",
          "description": "Graph Neural Networks (GNNs) have proven to excel in predictive modeling\ntasks where the underlying data is a graph. However, as GNNs are extensively\nused in human-centered applications, the issue of fairness has arisen. While\nedge deletion is a common method used to promote fairness in GNNs, it fails to\nconsider when data is inherently missing fair connections. In this work we\nconsider the unexplored method of edge addition, accompanied by deletion, to\npromote fairness. We propose two model-agnostic algorithms to perform edge\nediting: a brute force approach and a continuous approximation approach,\nFairEdit. FairEdit performs efficient edge editing by leveraging gradient\ninformation of a fairness loss to find edges that improve fairness. We find\nthat FairEdit outperforms standard training for many data sets and GNN methods,\nwhile performing comparably to many state-of-the-art methods, demonstrating\nFairEdit's ability to improve fairness across many domains and models.",
          "link": "http://arxiv.org/abs/2201.03681",
          "publishedOn": "2022-01-13T00:40:17.919Z",
          "wordCount": 568,
          "title": "FairEdit: Preserving Fairness in Graph Neural Networks through Greedy Graph Editing. (arXiv:2201.03681v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chase_Z/0/1/0/all/0/1\">Zachary Chase</a>",
          "description": "Resolving a conjecture of Littlestone and Warmuth, we show that any concept\nclass of VC-dimension $d$ has a sample compression scheme of size $d$.",
          "link": "http://arxiv.org/abs/2201.04131",
          "publishedOn": "2022-01-13T00:40:17.912Z",
          "wordCount": 435,
          "title": "Optimally compressing VC classes. (arXiv:2201.04131v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chmielinski_K/0/1/0/all/0/1\">Kasia S. Chmielinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newman_S/0/1/0/all/0/1\">Sarah Newman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1\">Matt Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_J/0/1/0/all/0/1\">Josh Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_K/0/1/0/all/0/1\">Kemi Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yurkofsky_J/0/1/0/all/0/1\">Jessica Yurkofsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yue Chelsea Qiu</a>",
          "description": "As the production of and reliance on datasets to produce automated\ndecision-making systems (ADS) increases, so does the need for processes for\nevaluating and interrogating the underlying data. After launching the Dataset\nNutrition Label in 2018, the Data Nutrition Project has made significant\nupdates to the design and purpose of the Label, and is launching an updated\nLabel in late 2020, which is previewed in this paper. The new Label includes\ncontext-specific Use Cases &Alerts presented through an updated design and user\ninterface targeted towards the data scientist profile. This paper discusses the\nharm and bias from underlying training data that the Label is intended to\nmitigate, the current state of the work including new datasets being labeled,\nnew and existing challenges, and further directions of the work, as well as\nFigures previewing the new label.",
          "link": "http://arxiv.org/abs/2201.03954",
          "publishedOn": "2022-01-13T00:40:17.894Z",
          "wordCount": 576,
          "title": "The Dataset Nutrition Label (2nd Gen): Leveraging Context to Mitigate Harms in Artificial Intelligence. (arXiv:2201.03954v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_C/0/1/0/all/0/1\">Chenyang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhaoci Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenzhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huijia Li</a>",
          "description": "Community detection is a fundamental and important issue in network science,\nbut there are only a few community detection algorithms based on graph neural\nnetworks, among which unsupervised algorithms are almost blank. By fusing the\nhigh-order modularity information with network features, this paper proposes a\nVariational Graph AutoEncoder Reconstruction based community detection VGAER\nfor the first time, and gives its non-probabilistic version. They do not need\nany prior information. We have carefully designed corresponding input features,\ndecoder, and downstream tasks based on the community detection task and these\ndesigns are concise, natural, and perform well (NMI values under our design are\nimproved by 59.1% - 565.9%). Based on a series of experiments with wide range\nof datasets and advanced methods, VGAER has achieved superior performance and\nshows strong competitiveness and potential with a simpler design. Finally, we\nreport the results of algorithm convergence analysis and t-SNE visualization,\nwhich clearly depicted the stable performance and powerful network modularity\nability of VGAER. Our codes are available at https://github.com/qcydm/VGAER.",
          "link": "http://arxiv.org/abs/2201.04066",
          "publishedOn": "2022-01-13T00:40:17.887Z",
          "wordCount": 605,
          "title": "VGAER: graph neural network reconstruction based community detection. (arXiv:2201.04066v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Strazzera_L/0/1/0/all/0/1\">Luca Strazzera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gori_V/0/1/0/all/0/1\">Valentina Gori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veneri_G/0/1/0/all/0/1\">Giacomo Veneri</a>",
          "description": "We propose an adversarial learning method to tackle a Domain Adaptation (DA)\ntime series regression task (DANNTe). The regression aims at building a virtual\ncopy of a sensor installed on a gas turbine, to be used in place of the\nphysical sensor which can be missing in certain situations. Our DA approach is\nto search for a domain-invariant representation of the features. The learner\nhas access to both a labelled source dataset and an unlabeled target dataset\n(unsupervised DA) and is trained on both, exploiting the minmax game between a\ntask regressor and a domain classifier Neural Networks. Both models share the\nsame feature representation, learnt by a feature extractor. This work is based\non the results published by Ganin et al. arXiv:1505.07818; indeed, we present\nan extension suitable to time series applications. We report a significant\nimprovement in regression performance, compared to the baseline model trained\non the source domain only.",
          "link": "http://arxiv.org/abs/2201.03850",
          "publishedOn": "2022-01-13T00:40:17.875Z",
          "wordCount": 582,
          "title": "DANNTe: a case study of a turbo-machinery sensor virtualization under domain shift. (arXiv:2201.03850v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavao_A/0/1/0/all/0/1\">Adrien Pavao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1\">Fabio Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guyon_I/0/1/0/all/0/1\">Isabelle Guyon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Sirui Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_J/0/1/0/all/0/1\">Julio C. S. Jacques Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1\">Marius Lindauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madadi_M/0/1/0/all/0/1\">Meysam Madadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nierhoff_T/0/1/0/all/0/1\">Thomas Nierhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_K/0/1/0/all/0/1\">Kangning Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chunguang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoll_D/0/1/0/all/0/1\">Danny Stoll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treguer_S/0/1/0/all/0/1\">Sebastien Treguer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenglin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Youcheng Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zela_A/0/1/0/all/0/1\">Arbe r Zela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>",
          "description": "This paper reports the results and post-challenge analyses of ChaLearn's\nAutoDL challenge series, which helped sorting out a profusion of AutoML\nsolutions for Deep Learning (DL) that had been introduced in a variety of\nsettings, but lacked fair comparisons. All input data modalities (time series,\nimages, videos, text, tabular) were formatted as tensors and all tasks were\nmulti-label classification problems. Code submissions were executed on hidden\ntasks, with limited time and computational resources, pushing solutions that\nget results quickly. In this setting, DL methods dominated, though popular\nNeural Architecture Search (NAS) was impractical. Solutions relied on\nfine-tuned pre-trained networks, with architectures matching data modality.\nPost-challenge tests did not reveal improvements beyond the imposed time limit.\nWhile no component is particularly original or novel, a high level modular\norganization emerged featuring a \"meta-learner\", \"data ingestor\", \"model\nselector\", \"model/learner\", and \"evaluator\". This modularity enabled ablation\nstudies, which revealed the importance of (off-platform) meta-learning,\nensembling, and efficient data management. Experiments on heterogeneous module\ncombinations further confirm the (local) optimality of the winning solutions.\nOur challenge legacy includes an ever-lasting benchmark\n(this http URL), the open-sourced code of the winners, and a free\n\"AutoDL self-service\".",
          "link": "http://arxiv.org/abs/2201.03801",
          "publishedOn": "2022-01-13T00:40:17.868Z",
          "wordCount": 689,
          "title": "Winning solutions and post-challenge analyses of the ChaLearn AutoDL challenge 2019. (arXiv:2201.03801v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaohui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1\">Abhinav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriharsha_R/0/1/0/all/0/1\">Ram Sriharsha</a>",
          "description": "Changepoints are abrupt variations in the underlying distribution of data.\nDetecting changes in a data stream is an important problem with many\napplications. In this paper, we are interested in changepoint detection\nalgorithms which operate in an online setting in the sense that both its\nstorage requirements and worst-case computational complexity per observation\nare independent of the number of previous observations. We propose an online\nchangepoint detection algorithm for both univariate and multivariate data which\ncompares favorably with offline changepoint detection algorithms while also\noperating in a strictly more constrained computational model. In addition, we\npresent a simple online hyperparameter auto tuning technique for these\nalgorithms.",
          "link": "http://arxiv.org/abs/2201.03710",
          "publishedOn": "2022-01-13T00:40:14.932Z",
          "wordCount": 518,
          "title": "Online Changepoint Detection on a Budget. (arXiv:2201.03710v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongjie Zhang</a>",
          "description": "In this study, we propose a feature extraction framework based on contrastive\nlearning with adaptive positive and negative samples (CL-FEFA) that is suitable\nfor unsupervised, supervised, and semi-supervised single-view feature\nextraction. CL-FEFA constructs adaptively the positive and negative samples\nfrom the results of feature extraction, which makes it more appropriate and\naccurate. Thereafter, the discriminative features are re extracted to according\nto InfoNCE loss based on previous positive and negative samples, which will\nmake the intra-class samples more compact and the inter-class samples more\ndispersed. At the same time, using the potential structure information of\nsubspace samples to dynamically construct positive and negative samples can\nmake our framework more robust to noisy data. Furthermore, CL-FEFA considers\nthe mutual information between positive samples, that is, similar samples in\npotential structures, which provides theoretical support for its advantages in\nfeature extraction. The final numerical experiments prove that the proposed\nframework has a strong advantage over the traditional feature extraction\nmethods and contrastive learning methods.",
          "link": "http://arxiv.org/abs/2201.03942",
          "publishedOn": "2022-01-13T00:40:14.909Z",
          "wordCount": 594,
          "title": "Feature Extraction Framework based on Contrastive Learning with Adaptive Positive and Negative Samples. (arXiv:2201.03942v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04040",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Goujaud_B/0/1/0/all/0/1\">Baptiste Goujaud</a>, <a href=\"http://arxiv.org/find/math/1/au:+Moucer_C/0/1/0/all/0/1\">C&#xe9;line Moucer</a>, <a href=\"http://arxiv.org/find/math/1/au:+Glineur_F/0/1/0/all/0/1\">Fran&#xe7;ois Glineur</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hendrickx_J/0/1/0/all/0/1\">Julien Hendrickx</a>, <a href=\"http://arxiv.org/find/math/1/au:+Taylor_A/0/1/0/all/0/1\">Adrien Taylor</a>, <a href=\"http://arxiv.org/find/math/1/au:+Dieuleveut_A/0/1/0/all/0/1\">Aymeric Dieuleveut</a>",
          "description": "PEPit is a Python package aiming at simplifying the access to worst-case\nanalyses of a large family of first-order optimization methods possibly\ninvolving gradient, projection, proximal, or linear optimization oracles, along\nwith their approximate, or Bregman variants.\n\nIn short, PEPit is a package enabling computer-assisted worst-case analyses\nof first-order optimization methods. The key underlying idea is to cast the\nproblem of performing a worst-case analysis, often referred to as a performance\nestimation problem (PEP), as a semidefinite program (SDP) which can be solved\nnumerically. For doing that, the package users are only required to write\nfirst-order methods nearly as they would have implemented them. The package\nthen takes care of the SDP modelling parts, and the worst-case analysis is\nperformed numerically via a standard solver.",
          "link": "http://arxiv.org/abs/2201.04040",
          "publishedOn": "2022-01-13T00:40:14.902Z",
          "wordCount": 579,
          "title": "PEPit: computer-assisted worst-case analyses of first-order optimization methods in Python. (arXiv:2201.04040v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03949",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Keriven_N/0/1/0/all/0/1\">Nicolas Keriven</a>",
          "description": "In graph analysis, a classic task consists in computing similarity measures\nbetween (groups of) nodes. In latent space random graphs, nodes are associated\nto unknown latent variables. One may then seek to compute distances directly in\nthe latent space, using only the graph structure. In this paper, we show that\nit is possible to consistently estimate entropic-regularized Optimal Transport\n(OT) distances between groups of nodes in the latent space. We provide a\ngeneral stability result for entropic OT with respect to perturbations of the\ncost matrix. We then apply it to several examples of random graphs, such as\ngraphons or $\\epsilon$-graphs on manifolds. Along the way, we prove new\nconcentration results for the so-called Universal Singular Value Thresholding\nestimator, and for the estimation of geodesic distances on a manifold.",
          "link": "http://arxiv.org/abs/2201.03949",
          "publishedOn": "2022-01-13T00:40:14.895Z",
          "wordCount": 544,
          "title": "Entropic Optimal Transport in Random Graphs. (arXiv:2201.03949v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sunwoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_A/0/1/0/all/0/1\">Anit Kumar Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chaoyang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1\">Salman Avestimehr</a>",
          "description": "Local Stochastic Gradient Descent (SGD) with periodic model averaging\n(FedAvg) is a foundational algorithm in Federated Learning. The algorithm\nindependently runs SGD on multiple workers and periodically averages the model\nacross all the workers. When local SGD runs with many workers, however, the\nperiodic averaging causes a significant model discrepancy across the workers\nmaking the global loss converge slowly. While recent advanced optimization\nmethods tackle the issue focused on non-IID settings, there still exists the\nmodel discrepancy issue due to the underlying periodic model averaging. We\npropose a partial model averaging framework that mitigates the model\ndiscrepancy issue in Federated Learning. The partial averaging encourages the\nlocal models to stay close to each other on parameter space, and it enables to\nmore effectively minimize the global loss. Given a fixed number of iterations\nand a large number of workers (128), the partial averaging achieves up to 2.2%\nhigher validation accuracy than the periodic full averaging.",
          "link": "http://arxiv.org/abs/2201.03789",
          "publishedOn": "2022-01-13T00:40:14.888Z",
          "wordCount": 581,
          "title": "Partial Model Averaging in Federated Learning: Performance Guarantees and Benefits. (arXiv:2201.03789v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hohlfeld_P/0/1/0/all/0/1\">Philipp Hohlfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostermeier_T/0/1/0/all/0/1\">Tobias Ostermeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandl_D/0/1/0/all/0/1\">Dominik Brandl</a>",
          "description": "Classification problems are common in Computer Vision. Despite this, there is\nno dedicated work for the classification of beer bottles. As part of the\nchallenge of the master course Deep Learning, a dataset of 5207 beer bottle\nimages and brand labels was created. An image contains exactly one beer bottle.\nIn this paper we present a deep learning model which classifies pictures of\nbeer bottles in a two step approach. As the first step, a Faster-R-CNN detects\nimage sections relevant for classification independently of the brand. In the\nsecond step, the relevant image sections are classified by a ResNet-18. The\nimage section with the highest confidence is returned as class label. We\npropose a model, with which we surpass the classic one step transfer learning\napproach and reached an accuracy of 99.86% during the challenge on the final\ntest dataset. We were able to achieve 100% accuracy after the challenge ended",
          "link": "http://arxiv.org/abs/2201.03791",
          "publishedOn": "2022-01-13T00:40:14.881Z",
          "wordCount": 579,
          "title": "Classification of Beer Bottles using Object Detection and Transfer Learning. (arXiv:2201.03791v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aktas_O/0/1/0/all/0/1\">&#xd6;zlem Akta&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coskuner_B/0/1/0/all/0/1\">Berkay Co&#x15f;kuner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soner_I/0/1/0/all/0/1\">&#x130;lker Soner</a>",
          "description": "Satisfaction measurement, which emerges in every sector today, is a very\nimportant factor for many companies. In this study, it is aimed to reach the\nhighest accuracy rate with various machine learning algorithms by using the\ndata on Yemek Sepeti and variations of this data. The accuracy values of each\nalgorithm were calculated together with the various natural language processing\nmethods used. While calculating these accuracy values, the parameters of the\nalgorithms used were tried to be optimized. The models trained in this study on\nlabeled data can be used on unlabeled data and can give companies an idea in\nmeasuring customer satisfaction. It was observed that 3 different natural\nlanguage processing methods applied resulted in approximately 5% accuracy\nincrease in most of the developed models.",
          "link": "http://arxiv.org/abs/2201.03848",
          "publishedOn": "2022-01-13T00:40:14.863Z",
          "wordCount": 557,
          "title": "Turkish Sentiment Analysis Using Machine Learning Methods: Application on Online Food Order Site Reviews. (arXiv:2201.03848v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03624",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Panousis_K/0/1/0/all/0/1\">Konstantinos P. Panousis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antoniadis_A/0/1/0/all/0/1\">Anastasios Antoniadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzis_S/0/1/0/all/0/1\">Sotirios Chatzis</a>",
          "description": "This work aims to address the long-established problem of learning\ndiversified representations. To this end, we combine information-theoretic\narguments with stochastic competition-based activations, namely Stochastic\nLocal Winner-Takes-All (LWTA) units. In this context, we ditch the conventional\ndeep architectures commonly used in Representation Learning, that rely on\nnon-linear activations; instead, we replace them with sets of locally and\nstochastically competing linear units. In this setting, each network layer\nyields sparse outputs, determined by the outcome of the competition between\nunits that are organized into blocks of competitors. We adopt stochastic\narguments for the competition mechanism, which perform posterior sampling to\ndetermine the winner of each block. We further endow the considered networks\nwith the ability to infer the sub-part of the network that is essential for\nmodeling the data at hand; we impose appropriate stick-breaking priors to this\nend. To further enrich the information of the emerging representations, we\nresort to information-theoretic principles, namely the Information Competing\nProcess (ICP). Then, all the components are tied together under the stochastic\nVariational Bayes framework for inference. We perform a thorough experimental\ninvestigation for our approach using benchmark datasets on image\nclassification. As we experimentally show, the resulting networks yield\nsignificant discriminative representation learning abilities. In addition, the\nintroduced paradigm allows for a principled investigation mechanism of the\nemerging intermediate network representations.",
          "link": "http://arxiv.org/abs/2201.03624",
          "publishedOn": "2022-01-13T00:40:14.851Z",
          "wordCount": 650,
          "title": "Competing Mutual Information Constraints with Stochastic Competition-based Activations for Learning Diversified Representations. (arXiv:2201.03624v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian-wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "The imbalanced classification problem turns out to be one of the important\nand challenging problems in data mining and machine learning. The performances\nof traditional classifiers will be severely affected by many data problems,\nsuch as class imbalanced problem, class overlap and noise. The Tomek-Link\nalgorithm was only used to clean data when it was proposed. In recent years,\nthere have been reports of combining Tomek-Link algorithm with sampling\ntechnique. The Tomek-Link sampling algorithm can effectively reduce the class\noverlap on data, remove the majority instances that are difficult to\ndistinguish, and improve the algorithm classification accuracy. However, the\nTomek-Links under-sampling algorithm only considers the boundary instances that\nare the nearest neighbors to each other globally and ignores the potential\nlocal overlapping instances. When the number of minority instances is small,\nthe under-sampling effect is not satisfactory, and the performance improvement\nof the classification model is not obvious. Therefore, on the basis of\nTomek-Link, a multi-granularity relabeled under-sampling algorithm (MGRU) is\nproposed. This algorithm fully considers the local information of the data set\nin the local granularity subspace, and detects the local potential overlapping\ninstances in the data set. Then, the overlapped majority instances are\neliminated according to the global relabeled index value, which effectively\nexpands the detection range of Tomek-Links. The simulation results show that\nwhen we select the optimal global relabeled index value for under-sampling, the\nclassification accuracy and generalization performance of the proposed\nunder-sampling algorithm are significantly better than other baseline\nalgorithms.",
          "link": "http://arxiv.org/abs/2201.03957",
          "publishedOn": "2022-01-13T00:40:14.844Z",
          "wordCount": 660,
          "title": "Multi-granularity Relabeled Under-sampling Algorithm for Imbalanced Data. (arXiv:2201.03957v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Mauro Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pajola_L/0/1/0/all/0/1\">Luca Pajola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tricomi_P/0/1/0/all/0/1\">Pier Paolo Tricomi</a>",
          "description": "Nowadays, people generate and share massive content on online platforms\n(e.g., social networks, blogs). In 2021, the 1.9 billion daily active Facebook\nusers posted around 150 thousand photos every minute. Content moderators\nconstantly monitor these online platforms to prevent the spreading of\ninappropriate content (e.g., hate speech, nudity images). Based on deep\nlearning (DL) advances, Automatic Content Moderators (ACM) help human\nmoderators handle high data volume. Despite their advantages, attackers can\nexploit weaknesses of DL components (e.g., preprocessing, model) to affect\ntheir performance. Therefore, an attacker can leverage such techniques to\nspread inappropriate content by evading ACM.\n\nIn this work, we propose CAPtcha Attack (CAPA), an adversarial technique that\nallows users to spread inappropriate text online by evading ACM controls. CAPA,\nby generating custom textual CAPTCHAs, exploits ACM's careless design\nimplementations and internal procedures vulnerabilities. We test our attack on\nreal-world ACM, and the results confirm the ferocity of our simple yet\neffective attack, reaching up to a 100% evasion success in most cases. At the\nsame time, we demonstrate the difficulties in designing CAPA mitigations,\nopening new challenges in CAPTCHAs research area.",
          "link": "http://arxiv.org/abs/2201.04014",
          "publishedOn": "2022-01-13T00:40:14.805Z",
          "wordCount": 609,
          "title": "Captcha Attack:Turning Captchas Against Humanity. (arXiv:2201.04014v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03549",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Zhang_T/0/1/0/all/0/1\">Tianhan Zhang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yi_Y/0/1/0/all/0/1\">Yuxiao Yi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Xu_Y/0/1/0/all/0/1\">Yifan Xu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi X. Chen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaoyu Zhang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+E_W/0/1/0/all/0/1\">Weinan E</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Xu_Z/0/1/0/all/0/1\">Zhi-Qin John Xu</a>",
          "description": "Machine learning has long been considered as a black box for predicting\ncombustion chemical kinetics due to the extremely large number of parameters\nand the lack of evaluation standards and reproducibility. The current work aims\nto understand two basic questions regarding the deep neural network (DNN)\nmethod: what data the DNN needs and how general the DNN method can be. Sampling\nand preprocessing determine the DNN training dataset, further affect DNN\nprediction ability. The current work proposes using Box-Cox transformation\n(BCT) to preprocess the combustion data. In addition, this work compares\ndifferent sampling methods with or without preprocessing, including the Monte\nCarlo method, manifold sampling, generative neural network method (cycle-GAN),\nand newly-proposed multi-scale sampling. Our results reveal that the DNN\ntrained by the manifold data can capture the chemical kinetics in limited\nconfigurations but cannot remain robust toward perturbation, which is\ninevitable for the DNN coupled with the flow field. The Monte Carlo and\ncycle-GAN samplings can cover a wider phase space but fail to capture\nsmall-scale intermediate species, producing poor prediction results. A\nthree-hidden-layer DNN, based on the multi-scale method without specific flame\nsimulation data, allows predicting chemical kinetics in various scenarios and\nbeing stable during the temporal evolutions. This single DNN is readily\nimplemented with several CFD codes and validated in various combustors,\nincluding (1). zero-dimensional autoignition, (2). one-dimensional freely\npropagating flame, (3). two-dimensional jet flame with triple-flame structure,\nand (4). three-dimensional turbulent lifted flames. The results demonstrate the\nsatisfying accuracy and generalization ability of the pre-trained DNN. The\nFortran and Python versions of DNN and example code are attached in the\nsupplementary for reproducibility.",
          "link": "http://arxiv.org/abs/2201.03549",
          "publishedOn": "2022-01-13T00:40:14.640Z",
          "wordCount": 725,
          "title": "A multi-scale sampling method for accurate and robust deep neural network to predict combustion chemical kinetics. (arXiv:2201.03549v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Ruocheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_M/0/1/0/all/0/1\">Mengting Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Longqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aidong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jundong Li</a>",
          "description": "Fair machine learning aims to mitigate the biases of model predictions\nagainst certain subpopulations regarding sensitive attributes such as race and\ngender. Among the many existing fairness notions, counterfactual fairness\nmeasures the model fairness from a causal perspective by comparing the\npredictions of each individual from the original data and the counterfactuals.\nIn counterfactuals, the sensitive attribute values of this individual had been\nmodified. Recently, a few works extend counterfactual fairness to graph data,\nbut most of them neglect the following facts that can lead to biases: 1) the\nsensitive attributes of each node's neighbors may causally affect the\nprediction w.r.t. this node; 2) the sensitive attributes may causally affect\nother features and the graph structure. To tackle these issues, in this paper,\nwe propose a novel fairness notion - graph counterfactual fairness, which\nconsiders the biases led by the above facts. To learn node representations\ntowards graph counterfactual fairness, we propose a novel framework based on\ncounterfactual data augmentation. In this framework, we generate\ncounterfactuals corresponding to perturbations on each node's and their\nneighbors' sensitive attributes. Then we enforce fairness by minimizing the\ndiscrepancy between the representations learned from the original graph and the\ncounterfactuals for each node. Experiments on both synthetic and real-world\ngraphs show that our framework outperforms the state-of-the-art baselines in\ngraph counterfactual fairness, and also achieves comparable prediction\nperformance.",
          "link": "http://arxiv.org/abs/2201.03662",
          "publishedOn": "2022-01-13T00:40:14.633Z",
          "wordCount": 656,
          "title": "Learning Fair Node Representations with Graph Counterfactual Fairness. (arXiv:2201.03662v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03668",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lokhande_V/0/1/0/all/0/1\">Vishnu Suresh Lokhande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kihyuk Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jinsung Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udell_M/0/1/0/all/0/1\">Madeleine Udell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>",
          "description": "Learning invariant representations is an important requirement when training\nmachine learning models that are driven by spurious correlations in the\ndatasets. These spurious correlations, between input samples and the target\nlabels, wrongly direct the neural network predictions resulting in poor\nperformance on certain groups, especially the minority groups. Robust training\nagainst these spurious correlations requires the knowledge of group membership\nfor every sample. Such a requirement is impractical in situations where the\ndata labeling efforts for minority or rare groups are significantly laborious\nor where the individuals comprising the dataset choose to conceal sensitive\ninformation. On the other hand, the presence of such data collection efforts\nresults in datasets that contain partially labeled group information. Recent\nworks have tackled the fully unsupervised scenario where no labels for groups\nare available. Thus, we aim to fill the missing gap in the literature by\ntackling a more realistic setting that can leverage partially available\nsensitive or group information during training. First, we construct a\nconstraint set and derive a high probability bound for the group assignment to\nbelong to the set. Second, we propose an algorithm that optimizes for the\nworst-off group assignments from the constraint set. Through experiments on\nimage and tabular datasets, we show improvements in the minority group's\nperformance while preserving overall aggregate accuracy across groups.",
          "link": "http://arxiv.org/abs/2201.03668",
          "publishedOn": "2022-01-13T00:40:14.626Z",
          "wordCount": 659,
          "title": "Towards Group Robustness in the presence of Partial Group Labels. (arXiv:2201.03668v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gazak_J/0/1/0/all/0/1\">J. Zachary Gazak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McQuaid_I/0/1/0/all/0/1\">Ian McQuaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swindle_R/0/1/0/all/0/1\">Ryan Swindle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phelps_M/0/1/0/all/0/1\">Matthew Phelps</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fletcher_J/0/1/0/all/0/1\">Justin Fletcher</a>",
          "description": "Effective space traffic management requires positive identification of\nartificial satellites. Current methods for extracting object identification\nfrom observed data require spatially resolved imagery which limits\nidentification to objects in low earth orbits. Most artificial satellites,\nhowever, operate in geostationary orbits at distances which prohibit ground\nbased observatories from resolving spatial information. This paper demonstrates\nan object identification solution leveraging modified residual convolutional\nneural networks to map distance-invariant spectroscopic data to object\nidentity. We report classification accuracies exceeding 80% for a simulated\n64-class satellite problem--even in the case of satellites undergoing constant,\nrandom re-orientation. An astronomical observing campaign driven by these\nresults returned accuracies of 72% for a nine-class problem with an average of\n100 examples per class, performing as expected from simulation. We demonstrate\nthe application of variational Bayesian inference by dropout, stochastic weight\naveraging (SWA), and SWA-focused deep ensembling to measure classification\nuncertainties--critical components in space traffic management where routine\ndecisions risk expensive space assets and carry geopolitical consequences.",
          "link": "http://arxiv.org/abs/2201.03614",
          "publishedOn": "2022-01-13T00:40:14.618Z",
          "wordCount": 619,
          "title": "SpectraNet: Learned Recognition of Artificial Satellites From High Contrast Spectroscopic Imagery. (arXiv:2201.03614v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_R/0/1/0/all/0/1\">Robi Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_G/0/1/0/all/0/1\">Gaurav Mahajan</a>",
          "description": "We consider a lifelong learning scenario in which a learner faces a\nneverending and arbitrary stream of facts and has to decide which ones to\nretain in its limited memory. We introduce a mathematical model based on the\nonline learning framework, in which the learner measures itself against a\ncollection of experts that are also memory-constrained and that reflect\ndifferent policies for what to remember. Interspersed with the stream of facts\nare occasional questions, and on each of these the learner incurs a loss if it\nhas not remembered the corresponding fact. Its goal is to do almost as well as\nthe best expert in hindsight, while using roughly the same amount of memory. We\nidentify difficulties with using the multiplicative weights update algorithm in\nthis memory-constrained scenario, and design an alternative scheme whose regret\nguarantees are close to the best possible.",
          "link": "http://arxiv.org/abs/2201.03806",
          "publishedOn": "2022-01-13T00:40:14.413Z",
          "wordCount": 553,
          "title": "Learning what to remember. (arXiv:2201.03806v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Harry Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Stone Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_H/0/1/0/all/0/1\">Hisham Mohammad</a>",
          "description": "This work aims to reproduce results from the CVPR 2020 paper by Gidaris et\nal. Self-supervised learning (SSL) is used to learn feature representations of\nan image using an unlabeled dataset. This work proposes to use bag-of-words\n(BoW) deep feature descriptors as a self-supervised learning target to learn\nrobust, deep representations. BowNet is trained to reconstruct the histogram of\nvisual words (ie. the deep BoW descriptor) of a reference image when presented\na perturbed version of the image as input. Thus, this method aims to learn\nperturbation-invariant and context-aware image features that can be useful for\nfew-shot tasks or supervised downstream tasks. In the paper, the author\ndescribes BowNet as a network consisting of a convolutional feature extractor\n$\\Phi(\\cdot)$ and a Dense-softmax layer $\\Omega(\\cdot)$ trained to predict BoW\nfeatures from images. After BoW training, the features of $\\Phi$ are used in\ndownstream tasks. For this challenge we were trying to build and train a\nnetwork that could reproduce the CIFAR-100 accuracy improvements reported in\nthe original paper. However, we were unsuccessful in reproducing an accuracy\nimprovement comparable to what the authors mentioned.",
          "link": "http://arxiv.org/abs/2201.03556",
          "publishedOn": "2022-01-13T00:40:14.098Z",
          "wordCount": 633,
          "title": "Reproducing BowNet: Learning Representations by Predicting Bags of Visual Words. (arXiv:2201.03556v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Irofti_P/0/1/0/all/0/1\">Paul Irofti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusu_C/0/1/0/all/0/1\">Cristian Rusu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patrascu_A/0/1/0/all/0/1\">Andrei P&#x103;tra&#x15f;cu</a>",
          "description": "Many applications like audio and image processing show that sparse\nrepresentations are a powerful and efficient signal modeling technique. Finding\nan optimal dictionary that generates at the same time the sparsest\nrepresentations of data and the smallest approximation error is a hard problem\napproached by dictionary learning (DL). We study how DL performs in detecting\nabnormal samples in a dataset of signals. In this paper we use a particular DL\nformulation that seeks uniform sparse representations model to detect the\nunderlying subspace of the majority of samples in a dataset, using a K-SVD-type\nalgorithm. Numerical simulations show that one can efficiently use this\nresulted subspace to discriminate the anomalies over the regular data points.",
          "link": "http://arxiv.org/abs/2201.03869",
          "publishedOn": "2022-01-13T00:40:14.091Z",
          "wordCount": 542,
          "title": "Dictionary Learning with Uniform Sparse Representations for Anomaly Detection. (arXiv:2201.03869v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reichhuber_S/0/1/0/all/0/1\">Simon Reichhuber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomforde_S/0/1/0/all/0/1\">Sven Tomforde</a>",
          "description": "Intelligent systems have the ability to improve their behaviour over time\ntaking observations, experiences or explicit feedback into account. Traditional\napproaches separate the learning problem and make isolated use of techniques\nfrom different field of machine learning such as reinforcement learning, active\nlearning, anomaly detection or transfer learning, for instance. In this\ncontext, the fundamental reinforcement learning approaches come with several\ndrawbacks that hinder their application to real-world systems: trial-and-error,\npurely reactive behaviour or isolated problem handling. The idea of this\narticle is to present a concept for alleviating these drawbacks by setting up a\nresearch agenda towards what we call \"active reinforcement learning\" in\nintelligent systems.",
          "link": "http://arxiv.org/abs/2201.03947",
          "publishedOn": "2022-01-13T00:40:14.084Z",
          "wordCount": 534,
          "title": "Active Reinforcement Learning -- A Roadmap Towards Curious Classifier Systems for Self-Adaptation. (arXiv:2201.03947v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03560",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Dawood_P/0/1/0/all/0/1\">Peter Dawood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blaimer_M/0/1/0/all/0/1\">Martin Blaimer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breuer_F/0/1/0/all/0/1\">Felix Breuer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burd_P/0/1/0/all/0/1\">Paul R. Burd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Homolya_I/0/1/0/all/0/1\">Istv&#xe1;n Homolya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jakob_P/0/1/0/all/0/1\">Peter M. Jakob</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oberberger_J/0/1/0/all/0/1\">Johannes Oberberger</a>",
          "description": "MRI scan time reduction is commonly achieved by Parallel Imaging methods,\ntypically based on uniform undersampling of the inverse image space (a.k.a.\nk-space) and simultaneous signal reception with multiple receiver coils. The\nGRAPPA method interpolates missing k-space signals by linear combination of\nadjacent, acquired signals across all coils, and can be described by a\nconvolution in k-space. Recently, a more generalized method called RAKI was\nintroduced. RAKI is a deep-learning method that generalizes GRAPPA with\nadditional convolution layers, on which a non-linear activation function is\napplied. This enables non-linear estimation of missing signals by convolutional\nneural networks. In analogy to GRAPPA, the convolution kernels in RAKI are\ntrained using scan-specific training samples obtained from\nauto-calibration-signals (ACS). RAKI provides superior reconstruction quality\ncompared to GRAPPA, however, often requires much more ACS due to its increased\nnumber of unknown parameters. In order to overcome this limitation, this study\ninvestigates the influence of training data on the reconstruction quality for\nstandard 2D imaging, with particular focus on its amount and contrast\ninformation. Furthermore, an iterative k-space interpolation approach (iRAKI)\nis evaluated, which includes training data augmentation via an initial GRAPPA\nreconstruction, and refinement of convolution filters by iterative training.\nUsing only 18, 20 and 25 ACS lines (8%), iRAKI outperforms RAKI by suppressing\nresidual artefacts occurring at accelerations factors R=4 and R=5, and yields\nstrong noise suppression in comparison to GRAPPA, underlined by quantitative\nquality metrics. Combination with a phase-constraint yields further\nimprovement. Additionally, iRAKI shows better performance than GRAPPA and RAKI\nin case of pre-scan calibration and strongly varying contrast between training-\nand undersampled data.",
          "link": "http://arxiv.org/abs/2201.03560",
          "publishedOn": "2022-01-13T00:40:14.067Z",
          "wordCount": 730,
          "title": "Iterative RAKI with Complex-Valued Convolution for Improved Image Reconstruction with Limited Scan-Specific Training Samples. (arXiv:2201.03560v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03604",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stein_S/0/1/0/all/0/1\">Sebastian Stein</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_J/0/1/0/all/0/1\">John H. Williamson</a> (1) ((1) School of Computing Science, University of Glasgow, Scotland, United Kingdom)",
          "description": "Probabilistic models inform an increasingly broad range of business and\npolicy decisions ultimately made by people. Recent algorithmic, computational,\nand software framework development progress facilitate the proliferation of\nBayesian probabilistic models, which characterise unobserved parameters by\ntheir joint distribution instead of point estimates. While they can empower\ndecision makers to explore complex queries and to perform what-if-style\nconditioning in theory, suitable visualisations and interactive tools are\nneeded to maximise users' comprehension and rational decision making under\nuncertainty. In this paper, propose a protocol for quantitative evaluation of\nBayesian model visualisations and introduce a software framework implementing\nthis protocol to support standardisation in evaluation practice and facilitate\nreproducibility. We illustrate the evaluation and analysis workflow on a user\nstudy that explores whether making Boxplots and Hypothetical Outcome Plots\ninteractive can increase comprehension or rationality and conclude with design\nguidelines for researchers looking to conduct similar studies in the future.",
          "link": "http://arxiv.org/abs/2201.03604",
          "publishedOn": "2022-01-13T00:40:14.059Z",
          "wordCount": 574,
          "title": "Evaluating Bayesian Model Visualisations. (arXiv:2201.03604v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marx_S/0/1/0/all/0/1\">Swann Marx</a> (LS2N), <a href=\"http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1\">Edouard Pauwels</a> (IRIT)",
          "description": "We consider flows of ordinary differential equations (ODEs) driven by path\ndifferentiable vector fields. Path differentiable functions constitute a proper\nsubclass of Lipschitz functions which admit conservative gradients, a notion of\ngeneralized derivative compatible with basic calculus rules. Our main result\nstates that such flows inherit the path differentiability property of the\ndriving vector field. We show indeed that forward propagation of derivatives\ngiven by the sensitivity differential inclusions provide a conservative\nJacobian for the flow. This allows to propose a nonsmooth version of the\nadjoint method, which can be applied to integral costs under an ODE constraint.\nThis result constitutes a theoretical ground to the application of small step\nfirst order methods to solve a broad class of nonsmooth optimization problems\nwith parametrized ODE constraints. This is illustrated with the convergence of\nsmall step first order methods based on the proposed nonsmooth adjoint.",
          "link": "http://arxiv.org/abs/2201.03819",
          "publishedOn": "2022-01-13T00:40:14.051Z",
          "wordCount": 563,
          "title": "Path differentiability of ODE flows. (arXiv:2201.03819v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03550",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Konstantinova_T/0/1/0/all/0/1\">Tatiana Konstantinova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maffettone_P/0/1/0/all/0/1\">Phillip M. Maffettone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravel_B/0/1/0/all/0/1\">Bruce Ravel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_S/0/1/0/all/0/1\">Stuart I. Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbour_A/0/1/0/all/0/1\">Andi M. Barbour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olds_D/0/1/0/all/0/1\">Daniel Olds</a>",
          "description": "Imaging, scattering, and spectroscopy are fundamental in understanding and\ndiscovering new functional materials. Contemporary innovations in automation\nand experimental techniques have led to these measurements being performed much\nfaster and with higher resolution, thus producing vast amounts of data for\nanalysis. These innovations are particularly pronounced at user facilities and\nsynchrotron light sources. Machine learning (ML) methods are regularly\ndeveloped to process and interpret large datasets in real-time with\nmeasurements. However, there remain conceptual barriers to entry for the\nfacility general user community, whom often lack expertise in ML, and technical\nbarriers for deploying ML models. Herein, we demonstrate a variety of\narchetypal ML models for on-the-fly analysis at multiple beamlines at the\nNational Synchrotron Light Source II (NSLS-II). We describe these examples\ninstructively, with a focus on integrating the models into existing\nexperimental workflows, such that the reader can easily include their own ML\ntechniques into experiments at NSLS-II or facilities with a common\ninfrastructure. The framework presented here shows how with little effort,\ndiverse ML models operate in conjunction with feedback loops via integration\ninto the existing Bluesky Suite for experimental orchestration and data\nmanagement.",
          "link": "http://arxiv.org/abs/2201.03550",
          "publishedOn": "2022-01-13T00:40:14.045Z",
          "wordCount": 626,
          "title": "Machine learning enabling high-throughput and remote operations at large-scale user facilities. (arXiv:2201.03550v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03617",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Momenifar_M/0/1/0/all/0/1\">Mohammadreza Momenifar</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Diao_E/0/1/0/all/0/1\">Enmao Diao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tarokh_V/0/1/0/all/0/1\">Vahid Tarokh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bragg_A/0/1/0/all/0/1\">Andrew D. Bragg</a>",
          "description": "Analyzing large-scale data from simulations of turbulent flows is memory\nintensive, requiring significant resources. This major challenge highlights the\nneed for data compression techniques. In this study, we apply a\nphysics-informed Deep Learning technique based on vector quantization to\ngenerate a discrete, low-dimensional representation of data from simulations of\nthree-dimensional turbulent flows. The deep learning framework is composed of\nconvolutional layers and incorporates physical constraints on the flow, such as\npreserving incompressibility and global statistical characteristics of the\nvelocity gradients. The accuracy of the model is assessed using statistical,\ncomparison-based similarity and physics-based metrics. The training data set is\nproduced from Direct Numerical Simulation of an incompressible, statistically\nstationary, isotropic turbulent flow. The performance of this lossy data\ncompression scheme is evaluated not only with unseen data from the stationary,\nisotropic turbulent flow, but also with data from decaying isotropic\nturbulence, and a Taylor-Green vortex flow. Defining the compression ratio (CR)\nas the ratio of original data size to the compressed one, the results show that\nour model based on vector quantization can offer CR $=85$ with a mean square\nerror (MSE) of $O(10^{-3})$, and predictions that faithfully reproduce the\nstatistics of the flow, except at the very smallest scales where there is some\nloss. Compared to the recent study based on a conventional autoencoder where\ncompression is performed in a continuous space, our model improves the CR by\nmore than $30$ percent, and reduces the MSE by an order of magnitude. Our\ncompression model is an attractive solution for situations where fast, high\nquality and low-overhead encoding and decoding of large data are required.",
          "link": "http://arxiv.org/abs/2201.03617",
          "publishedOn": "2022-01-13T00:40:14.037Z",
          "wordCount": 702,
          "title": "A Physics-Informed Vector Quantized Autoencoder for Data Compression of Turbulent Flow. (arXiv:2201.03617v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fanchao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orr_M/0/1/0/all/0/1\">Mark Orr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swarup_S/0/1/0/all/0/1\">Samarth Swarup</a>",
          "description": "In classic graph signal processing, given a real-valued graph signal, its\ngraph Fourier transform is typically defined as the series of inner products\nbetween the signal and each eigenvector of the graph Laplacian. Unfortunately,\nthis definition is not mathematically valid in the cases of vector-valued graph\nsignals which however are typical operands in the state-of-the-art graph\nlearning modeling and analyses. Seeking a generalized transformation decoding\nthe magnitudes of eigencomponents from vector-valued signals is thus the main\nobjective of this paper. Several attempts are explored, and also it is found\nthat performing the transformation at hierarchical levels of adjacency help\nprofile the spectral characteristics of signals more insightfully. The proposed\nmethods are introduced as a new tool assisting on diagnosing and profiling\nbehaviors of graph learning models.",
          "link": "http://arxiv.org/abs/2201.03696",
          "publishedOn": "2022-01-13T00:40:14.011Z",
          "wordCount": 531,
          "title": "Stratified Graph Spectra. (arXiv:2201.03696v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Ming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Leyuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Muxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bob Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghamisi_P/0/1/0/all/0/1\">Pedram Ghamisi</a>",
          "description": "The use of deep learning for water extraction requires precise pixel-level\nlabels. However, it is very difficult to label high-resolution remote sensing\nimages at the pixel level. Therefore, we study how to utilize point labels to\nextract water bodies and propose a novel method called the neighbor feature\naggregation network (NFANet). Compared with pixellevel labels, point labels are\nmuch easier to obtain, but they will lose much information. In this paper, we\ntake advantage of the similarity between the adjacent pixels of a local\nwater-body, and propose a neighbor sampler to resample remote sensing images.\nThen, the sampled images are sent to the network for feature aggregation. In\naddition, we use an improved recursive training algorithm to further improve\nthe extraction accuracy, making the water boundary more natural. Furthermore,\nour method utilizes neighboring features instead of global or local features to\nlearn more representative features. The experimental results show that the\nproposed NFANet method not only outperforms other studied weakly supervised\napproaches, but also obtains similar results as the state-of-the-art ones.",
          "link": "http://arxiv.org/abs/2201.03686",
          "publishedOn": "2022-01-13T00:40:14.004Z",
          "wordCount": 616,
          "title": "NFANet: A Novel Method for Weakly Supervised Water Extraction from High-Resolution Remote Sensing Imagery. (arXiv:2201.03686v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Suyun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zhigang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xizhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_P/0/1/0/all/0/1\">Peng Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hengheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cuiping Li</a>",
          "description": "Rule-based classifier, that extract a subset of induced rules to efficiently\nlearn/mine while preserving the discernibility information, plays a crucial\nrole in human-explainable artificial intelligence. However, in this era of big\ndata, rule induction on the whole datasets is computationally intensive. So\nfar, to the best of our knowledge, no known method focusing on accelerating\nrule induction has been reported. This is first study to consider the\nacceleration technique to reduce the scale of computation in rule induction. We\npropose an accelerator for rule induction based on fuzzy rough theory; the\naccelerator can avoid redundant computation and accelerate the building of a\nrule classifier. First, a rule induction method based on consistence degree,\ncalled Consistence-based Value Reduction (CVR), is proposed and used as basis\nto accelerate. Second, we introduce a compacted search space termed Key Set,\nwhich only contains the key instances required to update the induced rule, to\nconduct value reduction. The monotonicity of Key Set ensures the feasibility of\nour accelerator. Third, a rule-induction accelerator is designed based on Key\nSet, and it is theoretically guaranteed to display the same results as the\nunaccelerated version. Specifically, the rank preservation property of Key Set\nensures consistency between the rule induction achieved by the accelerator and\nthe unaccelerated method. Finally, extensive experiments demonstrate that the\nproposed accelerator can perform remarkably faster than the unaccelerated\nrule-based classifier methods, especially on datasets with numerous instances.",
          "link": "http://arxiv.org/abs/2201.03649",
          "publishedOn": "2022-01-13T00:40:13.989Z",
          "wordCount": 666,
          "title": "An Accelerator for Rule Induction in Fuzzy Rough Theory. (arXiv:2201.03649v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03559",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gautam_S/0/1/0/all/0/1\">Srishti Gautam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hohne_M/0/1/0/all/0/1\">Marina M.-C. H&#xf6;hne</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hansen_S/0/1/0/all/0/1\">Stine Hansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jenssen_R/0/1/0/all/0/1\">Robert Jenssen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kampffmeyer_M/0/1/0/all/0/1\">Michael Kampffmeyer</a>",
          "description": "The recent trend of integrating multi-source Chest X-Ray datasets to improve\nautomated diagnostics raises concerns that models learn to exploit\nsource-specific correlations to improve performance by recognizing the source\ndomain of an image rather than the medical pathology. We hypothesize that this\neffect is enforced by and leverages label-imbalance across the source domains,\ni.e, prevalence of a disease corresponding to a source. Therefore, in this\nwork, we perform a thorough study of the effect of label-imbalance in\nmulti-source training for the task of pneumonia detection on the widely used\nChestX-ray14 and CheXpert datasets. The results highlight and stress the\nimportance of using more faithful and transparent self-explaining models for\nautomated diagnosis, thus enabling the inherent detection of spurious learning.\nThey further illustrate that this undesirable effect of learning spurious\ncorrelations can be reduced considerably when ensuring label-balanced source\ndomain datasets.",
          "link": "http://arxiv.org/abs/2201.03559",
          "publishedOn": "2022-01-13T00:40:13.974Z",
          "wordCount": 599,
          "title": "Demonstrating The Risk of Imbalanced Datasets in Chest X-ray Image-based Diagnostics by Prototypical Relevance Propagation. (arXiv:2201.03559v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Butcher_A/0/1/0/all/0/1\">Andrew Butcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johanson_M/0/1/0/all/0/1\">Michael Bradley Johanson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davoodi_E/0/1/0/all/0/1\">Elnaz Davoodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brenneis_D/0/1/0/all/0/1\">Dylan J. A. Brenneis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acker_L/0/1/0/all/0/1\">Leslie Acker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parker_A/0/1/0/all/0/1\">Adam S. R. Parker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1\">Adam White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modayil_J/0/1/0/all/0/1\">Joseph Modayil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilarski_P/0/1/0/all/0/1\">Patrick M. Pilarski</a>",
          "description": "In this paper, we contribute a multi-faceted study into Pavlovian signalling\n-- a process by which learned, temporally extended predictions made by one\nagent inform decision-making by another agent. Signalling is intimately\nconnected to time and timing. In service of generating and receiving signals,\nhumans and other animals are known to represent time, determine time since past\nevents, predict the time until a future stimulus, and both recognize and\ngenerate patterns that unfold in time. We investigate how different temporal\nprocesses impact coordination and signalling between learning agents by\nintroducing a partially observable decision-making domain we call the Frost\nHollow. In this domain, a prediction learning agent and a reinforcement\nlearning agent are coupled into a two-part decision-making system that works to\nacquire sparse reward while avoiding time-conditional hazards. We evaluate two\ndomain variations: machine agents interacting in a seven-state linear walk, and\nhuman-machine interaction in a virtual-reality environment. Our results\nshowcase the speed of learning for Pavlovian signalling, the impact that\ndifferent temporal representations do (and do not) have on agent-agent\ncoordination, and how temporal aliasing impacts agent-agent and human-agent\ninteractions differently. As a main contribution, we establish Pavlovian\nsignalling as a natural bridge between fixed signalling paradigms and fully\nadaptive communication learning between two agents. We further show how to\ncomputationally build this adaptive signalling process out of a fixed\nsignalling process, characterized by fast continual prediction learning and\nminimal constraints on the nature of the agent receiving signals. Our results\ntherefore suggest an actionable, constructivist path towards communication\nlearning between reinforcement learning agents.",
          "link": "http://arxiv.org/abs/2201.03709",
          "publishedOn": "2022-01-13T00:40:13.965Z",
          "wordCount": 708,
          "title": "Pavlovian Signalling with General Value Functions in Agent-Agent Temporal Decision Making. (arXiv:2201.03709v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2101.06549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingkang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pun_A/0/1/0/all/0/1\">Ava Pun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1\">James Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manivasagam_S/0/1/0/all/0/1\">Sivabalan Manivasagam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadat_A/0/1/0/all/0/1\">Abbas Sadat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casas_S/0/1/0/all/0/1\">Sergio Casas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Mengye Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1\">Raquel Urtasun</a>",
          "description": "As self-driving systems become better, simulating scenarios where the\nautonomy stack may fail becomes more important. Traditionally, those scenarios\nare generated for a few scenes with respect to the planning module that takes\nground-truth actor states as input. This does not scale and cannot identify all\npossible autonomy failures, such as perception failures due to occlusion. In\nthis paper, we propose AdvSim, an adversarial framework to generate\nsafety-critical scenarios for any LiDAR-based autonomy system. Given an initial\ntraffic scenario, AdvSim modifies the actors' trajectories in a physically\nplausible manner and updates the LiDAR sensor data to match the perturbed\nworld. Importantly, by simulating directly from sensor data, we obtain\nadversarial scenarios that are safety-critical for the full autonomy stack. Our\nexperiments show that our approach is general and can identify thousands of\nsemantically meaningful safety-critical scenarios for a wide range of modern\nself-driving systems. Furthermore, we show that the robustness and safety of\nthese systems can be further improved by training them with scenarios generated\nby AdvSim.",
          "link": "http://arxiv.org/abs/2101.06549",
          "publishedOn": "2022-01-12T00:38:46.625Z",
          "wordCount": 649,
          "title": "AdvSim: Generating Safety-Critical Scenarios for Self-Driving Vehicles. (arXiv:2101.06549v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.02649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Chih-Hong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tobias Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burton_S/0/1/0/all/0/1\">Simon Burton</a>",
          "description": "We investigate the issues of achieving sufficient rigor in the arguments for\nthe safety of machine learning functions. By considering the known weaknesses\nof DNN-based 2D bounding box detection algorithms, we sharpen the metric of\nimprecise pedestrian localization by associating it with the safety goal. The\nsharpening leads to introducing a conservative post-processor after the\nstandard non-max-suppression as a counter-measure. We then propose a\nsemi-formal assurance case for arguing the effectiveness of the post-processor,\nwhich is further translated into formal proof obligations for demonstrating the\nsoundness of the arguments. Applying theorem proving not only discovers the\nneed to introduce missing claims and mathematical concepts but also reveals the\nlimitation of Dempster-Shafer's rules used in semi-formal argumentation.",
          "link": "http://arxiv.org/abs/2111.02649",
          "publishedOn": "2022-01-12T00:38:46.579Z",
          "wordCount": null,
          "title": "Logically Sound Arguments for the Effectiveness of ML Safety Measures. (arXiv:2111.02649v2 [cs.LO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03340",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1\">Minjae Park</a>",
          "description": "Heterogeneous graph neural networks can represent information of\nheterogeneous graphs with excellent ability. Recently, self-supervised learning\nmanner is researched which learns the unique expression of a graph through a\ncontrastive learning method. In the absence of labels, this learning methods\nshow great potential. However, contrastive learning relies heavily on positive\nand negative pairs, and generating high-quality pairs from heterogeneous graphs\nis difficult. In this paper, in line with recent innovations in self-supervised\nlearning, we introduce a that can generate good representations without\ngenerating large number of pairs. In addition, paying attention to the fact\nthat heterogeneous graphs can be viewed from two perspectives in this process,\nhigh-level expressions in the graphs are captured and expressed. The proposed\nmodel showed state-of-the-art performance than other methods in various real\nworld datasets.",
          "link": "http://arxiv.org/abs/2201.03340",
          "publishedOn": "2022-01-12T00:38:46.573Z",
          "wordCount": 548,
          "title": "Cross-view Self-Supervised Learning on Heterogeneous Graph Neural Network via Bootstrapping. (arXiv:2201.03340v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gawlikowski_J/0/1/0/all/0/1\">Jakob Gawlikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tassi_C/0/1/0/all/0/1\">Cedrique Rovile Njieutcheu Tassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mohsin Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jongseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Humt_M/0/1/0/all/0/1\">Matthias Humt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianxiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruspe_A/0/1/0/all/0/1\">Anna Kruspe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triebel_R/0/1/0/all/0/1\">Rudolph Triebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_P/0/1/0/all/0/1\">Peter Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1\">Ribana Roscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahzad_M/0/1/0/all/0/1\">Muhammad Shahzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamler_R/0/1/0/all/0/1\">Richard Bamler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>",
          "description": "Due to their increasing spread, confidence in neural network predictions\nbecame more and more important. However, basic neural networks do not deliver\ncertainty estimates or suffer from over or under confidence. Many researchers\nhave been working on understanding and quantifying uncertainty in a neural\nnetwork's prediction. As a result, different types and sources of uncertainty\nhave been identified and a variety of approaches to measure and quantify\nuncertainty in neural networks have been proposed. This work gives a\ncomprehensive overview of uncertainty estimation in neural networks, reviews\nrecent advances in the field, highlights current challenges, and identifies\npotential research opportunities. It is intended to give anyone interested in\nuncertainty estimation in neural networks a broad overview and introduction,\nwithout presupposing prior knowledge in this field. A comprehensive\nintroduction to the most crucial sources of uncertainty is given and their\nseparation into reducible model uncertainty and not reducible data uncertainty\nis presented. The modeling of these uncertainties based on deterministic neural\nnetworks, Bayesian neural networks, ensemble of neural networks, and test-time\ndata augmentation approaches is introduced and different branches of these\nfields as well as the latest developments are discussed. For a practical\napplication, we discuss different measures of uncertainty, approaches for the\ncalibration of neural networks and give an overview of existing baselines and\nimplementations. Different examples from the wide spectrum of challenges in\ndifferent fields give an idea of the needs and challenges regarding\nuncertainties in practical applications. Additionally, the practical\nlimitations of current methods for mission- and safety-critical real world\napplications are discussed and an outlook on the next steps towards a broader\nusage of such methods is given.",
          "link": "http://arxiv.org/abs/2107.03342",
          "publishedOn": "2022-01-12T00:38:46.541Z",
          "wordCount": null,
          "title": "A Survey of Uncertainty in Deep Neural Networks. (arXiv:2107.03342v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.00269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1\">David Heckerman</a>",
          "description": "A Bayesian network is a graphical model that encodes probabilistic\nrelationships among variables of interest. When used in conjunction with\nstatistical techniques, the graphical model has several advantages for data\nanalysis. One, because the model encodes dependencies among all variables, it\nreadily handles situations where some data entries are missing. Two, a Bayesian\nnetwork can be used to learn causal relationships, and hence can be used to\ngain understanding about a problem domain and to predict the consequences of\nintervention. Three, because the model has both a causal and probabilistic\nsemantics, it is an ideal representation for combining prior knowledge (which\noften comes in causal form) and data. Four, Bayesian statistical methods in\nconjunction with Bayesian networks offer an efficient and principled approach\nfor avoiding the overfitting of data. In this paper, we discuss methods for\nconstructing Bayesian networks from prior knowledge and summarize Bayesian\nstatistical methods for using data to improve these models. With regard to the\nlatter task, we describe methods for learning both the parameters and structure\nof a Bayesian network, including techniques for learning with incomplete data.\nIn addition, we relate Bayesian-network methods for learning to techniques for\nsupervised and unsupervised learning. We illustrate the graphical-modeling\napproach using a real-world case study.",
          "link": "http://arxiv.org/abs/2002.00269",
          "publishedOn": "2022-01-12T00:38:46.537Z",
          "wordCount": null,
          "title": "A Tutorial on Learning With Bayesian Networks. (arXiv:2002.00269v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.13366",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Podusenko_A/0/1/0/all/0/1\">Albert Podusenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Erp_B/0/1/0/all/0/1\">Bart van Erp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koudahl_M/0/1/0/all/0/1\">Magnus Koudahl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vries_B/0/1/0/all/0/1\">Bert de Vries</a>",
          "description": "In this paper we present AIDA, which is an active inference-based agent that\niteratively designs a personalized audio processing algorithm through situated\ninteractions with a human client. The target application of AIDA is to propose\non-the-spot the most interesting alternative values for the tuning parameters\nof a hearing aid (HA) algorithm, whenever a HA client is not satisfied with\ntheir HA performance. AIDA interprets searching for the \"most interesting\nalternative\" as an issue of optimal (acoustic) context-aware Bayesian trial\ndesign. In computational terms, AIDA is realized as an active inference-based\nagent with an Expected Free Energy criterion for trial design. This type of\narchitecture is inspired by neuro-economic models on efficient (Bayesian) trial\ndesign in brains and implies that AIDA comprises generative probabilistic\nmodels for acoustic signals and user responses. We propose a novel generative\nmodel for acoustic signals as a sum of time-varying auto-regressive filters and\na user response model based on a Gaussian Process Classifier. The full AIDA\nagent has been implemented in a factor graph for the generative model and all\ntasks (parameter learning, acoustic context classification, trial design, etc.)\nare realized by variational message passing on the factor graph. All\nverification and validation experiments and demonstrations are freely\naccessible at our GitHub repository.",
          "link": "http://arxiv.org/abs/2112.13366",
          "publishedOn": "2022-01-12T00:38:46.524Z",
          "wordCount": null,
          "title": "AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms. (arXiv:2112.13366v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.13827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhenghao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Quanyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1\">Ka Ming Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>",
          "description": "Self-Driven Particles (SDP) describe a category of multi-agent systems common\nin everyday life, such as flocking birds and traffic flows. In a SDP system,\neach agent pursues its own goal and constantly changes its cooperative or\ncompetitive behaviors with its nearby agents. Manually designing the\ncontrollers for such SDP system is time-consuming, while the resulting emergent\nbehaviors are often not realistic nor generalizable. Thus the realistic\nsimulation of SDP systems remains challenging. Reinforcement learning provides\nan appealing alternative for automating the development of the controller for\nSDP. However, previous multi-agent reinforcement learning (MARL) methods define\nthe agents to be teammates or enemies before hand, which fail to capture the\nessence of SDP where the role of each agent varies to be cooperative or\ncompetitive even within one episode. To simulate SDP with MARL, a key challenge\nis to coordinate agents' behaviors while still maximizing individual\nobjectives. Taking traffic simulation as the testing bed, in this work we\ndevelop a novel MARL method called Coordinated Policy Optimization (CoPO),\nwhich incorporates social psychology principle to learn neural controller for\nSDP. Experiments show that the proposed method can achieve superior performance\ncompared to MARL baselines in various metrics. Noticeably the trained vehicles\nexhibit complex and diverse social behaviors that improve performance and\nsafety of the population as a whole. Demo video and source code are available\nat: https://decisionforce.github.io/CoPO/",
          "link": "http://arxiv.org/abs/2110.13827",
          "publishedOn": "2022-01-12T00:38:46.517Z",
          "wordCount": null,
          "title": "Learning to Simulate Self-Driven Particles System with Coordinated Policy Optimization. (arXiv:2110.13827v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_K/0/1/0/all/0/1\">Kai Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Richeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Chau-Wai Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Huaiyu Dai</a>",
          "description": "Federated learning can enable remote workers to collaboratively train a\nshared machine learning model while allowing training data to be kept locally.\nIn the use case of wireless mobile devices, the communication overhead is a\ncritical bottleneck due to limited power and bandwidth. Prior work has utilized\nvarious data compression tools such as quantization and sparsification to\nreduce the overhead. In this paper, we propose a predictive coding based\ncompression scheme for federated learning. The scheme has shared prediction\nfunctions among all devices and allows each worker to transmit a compressed\nresidual vector derived from the reference. In each communication round, we\nselect the predictor and quantizer based on the rate-distortion cost, and\nfurther reduce the redundancy with entropy coding. Extensive simulations reveal\nthat the communication cost can be reduced up to 99% with even better learning\nperformance when compared with other baseline methods.",
          "link": "http://arxiv.org/abs/2108.00918",
          "publishedOn": "2022-01-12T00:38:46.515Z",
          "wordCount": null,
          "title": "Communication-Efficient Federated Learning via Predictive Coding. (arXiv:2108.00918v2 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.13164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Luxuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Ting Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yubin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jinqiao Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tao Liu</a>",
          "description": "With the fast development of modern deep learning techniques, the study of\ndynamic systems and neural networks is increasingly benefiting each other in a\nlot of different ways. Since uncertainties often arise in real world\nobservations, SDEs (stochastic differential equations) come to play an\nimportant role. To be more specific, in this paper, we use a collection of SDEs\nequipped with neural networks to predict long-term trend of noisy time series\nwhich has big jump properties and high probability distribution shift. Our\ncontributions are, first, we explored SDEs driven by $\\alpha$-stable L\\'evy\nmotion to model the time series data and solved the problem through neural\nnetwork approximation. Second, we theoretically proved the convergence of the\nmodel and obtained the convergence rate. Finally, we illustrated our method by\napplying it to stock marketing time series prediction and found the convergence\norder of error.",
          "link": "http://arxiv.org/abs/2111.13164",
          "publishedOn": "2022-01-12T00:38:46.514Z",
          "wordCount": null,
          "title": "L\\'evy Induced Stochastic Differential Equation Equipped with Neural Network for Time Series Forecasting. (arXiv:2111.13164v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.03892",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hu_Y/0/1/0/all/0/1\">Yihao Hu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhao_T/0/1/0/all/0/1\">Tong Zhao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Xu_S/0/1/0/all/0/1\">Shixin Xu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiliang Xu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lin_L/0/1/0/all/0/1\">Lizhen Lin</a>",
          "description": "Partial differential equations (PDEs) play a crucial role in studying a vast\nnumber of problems in science and engineering. Numerically solving nonlinear\nand/or high-dimensional PDEs is often a challenging task. Inspired by the\ntraditional finite difference and finite elements methods and emerging\nadvancements in machine learning, we propose a sequence deep learning framework\ncalled Neural-PDE, which allows to automatically learn governing rules of any\ntime-dependent PDE system from existing data by using a bidirectional LSTM\nencoder, and predict the next n time steps data. One critical feature of our\nproposed framework is that the Neural-PDE is able to simultaneously learn and\nsimulate the multiscale variables.We test the Neural-PDE by a range of examples\nfrom one-dimensional PDEs to a high-dimensional and nonlinear complex fluids\nmodel. The results show that the Neural-PDE is capable of learning the initial\nconditions, boundary conditions and differential operators without the\nknowledge of the specific form of a PDE system.In our experiments the\nNeural-PDE can efficiently extract the dynamics within 20 epochs training, and\nproduces accurate predictions. Furthermore, unlike the traditional machine\nlearning approaches in learning PDE such as CNN and MLP which require vast\nparameters for model precision, Neural-PDE shares parameters across all time\nsteps, thus considerably reduces the computational complexity and leads to a\nfast learning algorithm.",
          "link": "http://arxiv.org/abs/2009.03892",
          "publishedOn": "2022-01-12T00:38:46.513Z",
          "wordCount": null,
          "title": "Neural-PDE: A RNN based neural network for solving time dependent PDEs. (arXiv:2009.03892v3 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03480",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Li_H/0/1/0/all/0/1\">Haoya Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ying_L/0/1/0/all/0/1\">Lexing Ying</a>",
          "description": "In this paper, we propose a semigroup method for solving high-dimensional\nelliptic partial differential equations (PDEs) and the associated eigenvalue\nproblems based on neural networks. For the PDE problems, we reformulate the\noriginal equations as variational problems with the help of semigroup operators\nand then solve the variational problems with neural network (NN)\nparameterization. The main advantages are that no mixed second-order derivative\ncomputation is needed during the stochastic gradient descent training and that\nthe boundary conditions are taken into account automatically by the semigroup\noperator. Unlike popular methods like PINN \\cite{raissi2019physics} and Deep\nRitz \\cite{weinan2018deep} where the Dirichlet boundary condition is enforced\nsolely through penalty functions and thus changes the true solution, the\nproposed method is able to address the boundary conditions without penalty\nfunctions and it gives the correct true solution even when penalty functions\nare added, thanks to the semigroup operator. For eigenvalue problems, a\nprimal-dual method is proposed, efficiently resolving the constraint with a\nsimple scalar dual variable and resulting in a faster algorithm compared with\nthe BSDE solver \\cite{han2020solving} in certain problems such as the\neigenvalue problem associated with the linear Schr\\\"odinger operator. Numerical\nresults are provided to demonstrate the performance of the proposed methods.",
          "link": "http://arxiv.org/abs/2105.03480",
          "publishedOn": "2022-01-12T00:38:46.512Z",
          "wordCount": null,
          "title": "A semigroup method for high dimensional elliptic PDEs and eigenvalue problems based on neural networks. (arXiv:2105.03480v3 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gorbunov_E/0/1/0/all/0/1\">Eduard Gorbunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burlachenko_K/0/1/0/all/0/1\">Konstantin Burlachenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhize Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1\">Peter Richt&#xe1;rik</a>",
          "description": "We develop and analyze MARINA: a new communication efficient method for\nnon-convex distributed learning over heterogeneous datasets. MARINA employs a\nnovel communication compression strategy based on the compression of gradient\ndifferences that is reminiscent of but different from the strategy employed in\nthe DIANA method of Mishchenko et al. (2019). Unlike virtually all competing\ndistributed first-order methods, including DIANA, ours is based on a carefully\ndesigned biased gradient estimator, which is the key to its superior\ntheoretical and practical performance. The communication complexity bounds we\nprove for MARINA are evidently better than those of all previous first-order\nmethods. Further, we develop and analyze two variants of MARINA: VR-MARINA and\nPP-MARINA. The first method is designed for the case when the local loss\nfunctions owned by clients are either of a finite sum or of an expectation\nform, and the second method allows for a partial participation of clients -- a\nfeature important in federated learning. All our methods are superior to\nprevious state-of-the-art methods in terms of oracle/communication complexity.\nFinally, we provide a convergence analysis of all methods for problems\nsatisfying the Polyak-Lojasiewicz condition.",
          "link": "http://arxiv.org/abs/2102.07845",
          "publishedOn": "2022-01-12T00:38:46.502Z",
          "wordCount": null,
          "title": "MARINA: Faster Non-Convex Distributed Learning with Compression. (arXiv:2102.07845v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03230",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1\">Jiahao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1\">Yingying Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yinzhe Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Huanjun Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1\">Zhifan Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ser_J/0/1/0/all/0/1\">Javier Del Ser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_J/0/1/0/all/0/1\">Jun Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>",
          "description": "Magnetic resonance imaging (MRI) is an important non-invasive clinical tool\nthat can produce high-resolution and reproducible images. However, a long\nscanning time is required for high-quality MR images, which leads to exhaustion\nand discomfort of patients, inducing more artefacts due to voluntary movements\nof the patients and involuntary physiological movements. To accelerate the\nscanning process, methods by k-space undersampling and deep learning based\nreconstruction have been popularised. This work introduced SwinMR, a novel Swin\ntransformer based method for fast MRI reconstruction. The whole network\nconsisted of an input module (IM), a feature extraction module (FEM) and an\noutput module (OM). The IM and OM were 2D convolutional layers and the FEM was\ncomposed of a cascaded of residual Swin transformer blocks (RSTBs) and 2D\nconvolutional layers. The RSTB consisted of a series of Swin transformer layers\n(STLs). The shifted windows multi-head self-attention (W-MSA/SW-MSA) of STL was\nperformed in shifted windows rather than the multi-head self-attention (MSA) of\nthe original transformer in the whole image space. A novel multi-channel loss\nwas proposed by using the sensitivity maps, which was proved to reserve more\ntextures and details. We performed a series of comparative studies and ablation\nstudies in the Calgary-Campinas public brain MR dataset and conducted a\ndownstream segmentation experiment in the Multi-modal Brain Tumour Segmentation\nChallenge 2017 dataset. The results demonstrate our SwinMR achieved\nhigh-quality reconstruction compared with other benchmark methods, and it shows\ngreat robustness with different undersampling masks, under noise interruption\nand on different datasets. The code is publicly available at\nhttps://github.com/ayanglab/SwinMR.",
          "link": "http://arxiv.org/abs/2201.03230",
          "publishedOn": "2022-01-12T00:38:46.498Z",
          "wordCount": null,
          "title": "Swin Transformer for Fast MRI. (arXiv:2201.03230v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_T/0/1/0/all/0/1\">Tao Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhuo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagduyu_Y/0/1/0/all/0/1\">Yalin Sagduyu</a>",
          "description": "With the proliferation of IoT devices, researchers have developed a variety\nof IoT device identification methods with the assistance of machine learning.\nNevertheless, the security of these identification methods mostly depends on\ncollected training data. In this research, we propose a novel attack strategy\nnamed IoTGAN to manipulate an IoT device's traffic such that it can evade\nmachine learning based IoT device identification. In the development of IoTGAN,\nwe have two major technical challenges: (i) How to obtain the discriminative\nmodel in a black-box setting, and (ii) How to add perturbations to IoT traffic\nthrough the manipulative model, so as to evade the identification while not\ninfluencing the functionality of IoT devices. To address these challenges, a\nneural network based substitute model is used to fit the target model in\nblack-box settings, it works as a discriminative model in IoTGAN. A\nmanipulative model is trained to add adversarial perturbations into the IoT\ndevice's traffic to evade the substitute model. Experimental results show that\nIoTGAN can successfully achieve the attack goals. We also develop efficient\ncountermeasures to protect machine learning based IoT device identification\nfrom been undermined by IoTGAN.",
          "link": "http://arxiv.org/abs/2201.03281",
          "publishedOn": "2022-01-12T00:38:46.495Z",
          "wordCount": null,
          "title": "IoTGAN: GAN Powered Camouflage Against Machine Learning Based IoT Device Identification. (arXiv:2201.03281v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2004.01123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Funkner_A/0/1/0/all/0/1\">Anastasia A. Funkner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yakovlev_A/0/1/0/all/0/1\">Aleksey N. Yakovlev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovalchuk_S/0/1/0/all/0/1\">Sergey V. Kovalchuk</a>",
          "description": "The paper proposes and investigates an approach for surrogate-assisted\nperformance prediction of data-driven knowledge discovery algorithms. The\napproach is based on the identification of surrogate models for prediction of\nthe target algorithm's quality and performance. The proposed approach was\nimplemented and investigated as applied to an evolutionary algorithm for\ndiscovering clusters of interpretable clinical pathways in electronic health\nrecords of patients with acute coronary syndrome. Several clustering metrics\nand execution time were used as the target quality and performance metrics\nrespectively. An analytical software prototype based on the proposed approach\nfor the prediction of algorithm characteristics and feature analysis was\ndeveloped to provide a more interpretable prediction of the target algorithm's\nperformance and quality that can be further used for parameter tuning.",
          "link": "http://arxiv.org/abs/2004.01123",
          "publishedOn": "2022-01-12T00:38:46.493Z",
          "wordCount": null,
          "title": "Surrogate-assisted performance prediction for data-driven knowledge discovery algorithms: application to evolutionary modeling of clinical pathways. (arXiv:2004.01123v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yue Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1\">Wei Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1\">Trevor Cohen</a>",
          "description": "Health literacy has emerged as a crucial factor in making appropriate health\ndecisions and ensuring treatment outcomes. However, medical jargon and the\ncomplex structure of professional language in this domain make health\ninformation especially hard to interpret. Thus, there is an urgent unmet need\nfor automated methods to enhance the accessibility of the biomedical literature\nto the general population. This problem can be framed as a type of translation\nproblem between the language of healthcare professionals, and that of the\ngeneral public. In this paper, we introduce the novel task of automated\ngeneration of lay language summaries of biomedical scientific reviews, and\nconstruct a dataset to support the development and evaluation of automated\nmethods through which to enhance the accessibility of the biomedical\nliterature. We conduct analyses of the various challenges in solving this task,\nincluding not only summarization of the key points but also explanation of\nbackground knowledge and simplification of professional language. We experiment\nwith state-of-the-art summarization models as well as several data augmentation\ntechniques, and evaluate their performance using both automated metrics and\nhuman assessment. Results indicate that automatically generated summaries\nproduced using contemporary neural architectures can achieve promising quality\nand readability as compared with reference summaries developed for the lay\npublic by experts (best ROUGE-L of 50.24 and Flesch-Kincaid readability score\nof 13.30). We also discuss the limitations of the current attempt, providing\ninsights and directions for future work.",
          "link": "http://arxiv.org/abs/2012.12573",
          "publishedOn": "2022-01-12T00:38:46.462Z",
          "wordCount": null,
          "title": "Automated Lay Language Summarization of Biomedical Scientific Reviews. (arXiv:2012.12573v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.10675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jingyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giardina_F/0/1/0/all/0/1\">Fabio Giardina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosendo_A/0/1/0/all/0/1\">Andre Rosendo</a>",
          "description": "Deep Reinforcement Learning (DRL) experiments are commonly performed in\nsimulated environments due to the tremendous training sample demands from deep\nneural networks. In contrast, model-based Bayesian Learning allows a robot to\nlearn good policies within a few trials in the real world. Although it takes\nfewer iterations, Bayesian methods pay a relatively higher computational cost\nper trial, and the advantage of such methods is strongly tied to dimensionality\nand noise. In here, we compare a Deep Bayesian Learning algorithm with a\nmodel-free DRL algorithm while analyzing our results collected from both\nsimulations and real-world experiments. While considering Sim and Real\nlearning, our experiments show that the sample-efficient Deep Bayesian RL\nperformance is better than DRL even when computation time (as opposed to number\nof iterations) is taken in consideration. Additionally, the difference in\ncomputation time between Deep Bayesian RL performed in simulation and in\nexperiments point to a viable path to traverse the reality gap. We also show\nthat a mix between Sim and Real does not outperform a purely Real approach,\npointing to the possibility that reality can provide the best prior knowledge\nto a Bayesian Learning. Roboticists design and build robots every day, and our\nresults show that a higher learning efficiency in the real-world will shorten\nthe time between design and deployment by skipping simulations.",
          "link": "http://arxiv.org/abs/2007.10675",
          "publishedOn": "2022-01-12T00:38:46.371Z",
          "wordCount": null,
          "title": "Trade-off on Sim2Real Learning: Real-world Learning Faster than Simulations. (arXiv:2007.10675v4 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.10053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiangyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mei Li</a>",
          "description": "Scoring systems, as a type of predictive model, have significant advantages\nin interpretability and transparency and facilitate quick decision-making. As\nsuch, scoring systems have been extensively used in a wide variety of\nindustries such as healthcare and criminal justice. However, the fairness\nissues in these models have long been criticized, and the use of big data and\nmachine learning algorithms in the construction of scoring systems heightens\nthis concern. In this paper, we propose a general framework to create\nfairness-aware, data-driven scoring systems. First, we develop a social welfare\nfunction that incorporates both efficiency and group fairness. Then, we\ntransform the social welfare maximization problem into the risk minimization\ntask in machine learning, and derive a fairness-aware scoring system with the\nhelp of mixed integer programming. Lastly, several theoretical bounds are\nderived for providing parameter selection suggestions. Our proposed framework\nprovides a suitable solution to address group fairness concerns in the\ndevelopment of scoring systems. It enables policymakers to set and customize\ntheir desired fairness requirements as well as other application-specific\nconstraints. We test the proposed algorithm with several empirical data sets.\nExperimental evidence supports the effectiveness of the proposed scoring system\nin achieving the optimal welfare of stakeholders and in balancing the needs for\ninterpretability, fairness, and efficiency.",
          "link": "http://arxiv.org/abs/2109.10053",
          "publishedOn": "2022-01-12T00:38:46.345Z",
          "wordCount": null,
          "title": "Toward a Fairness-Aware Scoring System for Algorithmic Decision-Making. (arXiv:2109.10053v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.13655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bertoli_G/0/1/0/all/0/1\">Gustavo de Carvalho Bertoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_L/0/1/0/all/0/1\">Louren&#xe7;o Alves Pereira Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verri_F/0/1/0/all/0/1\">Filipe Alves Neto Verri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_A/0/1/0/all/0/1\">Aldri Luiz dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saotome_O/0/1/0/all/0/1\">Osamu Saotome</a>",
          "description": "Most research using machine learning (ML) for network intrusion detection\nsystems (NIDS) uses well-established datasets such as KDD-CUP99, NSL-KDD,\nUNSW-NB15, and CICIDS-2017. In this context, the possibilities of machine\nlearning techniques are explored, aiming for metrics improvements compared to\nthe published baselines (model-centric approach). However, those datasets\npresent some limitations as aging that make it unfeasible to transpose those\nML-based solutions to real-world applications. This paper presents a systematic\ndata-centric approach to address the current limitations of NIDS research,\nspecifically the datasets. This approach generates NIDS datasets composed of\nthe most recent network traffic and attacks, with the labeling process\nintegrated by design.",
          "link": "http://arxiv.org/abs/2110.13655",
          "publishedOn": "2022-01-12T00:38:46.345Z",
          "wordCount": null,
          "title": "Bridging the gap to real-world for network intrusion detection systems with data-centric approach. (arXiv:2110.13655v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shuyin_X/0/1/0/all/0/1\">Xia Shuyin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+GuoYing_W/0/1/0/all/0/1\">Wang GuoYing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+XinBo_G/0/1/0/all/0/1\">Gao XinBo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giem_E/0/1/0/all/0/1\">Elisabeth Giem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+JianHang_Y/0/1/0/all/0/1\">Yu JianHang</a>",
          "description": "Pawlak rough set and neighborhood rough set are the two most common rough set\ntheoretical models. Pawlawk can use equivalence classes to represent knowledge,\nbut it cannot process continuous data; neighborhood rough sets can process\ncontinuous data, but it loses the ability of using equivalence classes to\nrepresent knowledge. To this end, this paper presents a granular-ball rough set\nbased on the granlar-ball computing. The granular-ball rough set can\nsimultaneously represent Pawlak rough sets, and the neighborhood rough set, so\nas to realize the unified representation of the two. This makes the\ngranular-ball rough set not only can deal with continuous data, but also can\nuse equivalence classes for knowledge representation. In addition, we propose\nan implementation algorithms of granular-ball rough sets. The experimental\nresuts on benchmark datasets demonstrate that, due to the combination of the\nrobustness and adaptability of the granular-ball computing, the learning\naccuracy of the granular-ball rough set has been greatly improved compared with\nthe Pawlak rough set and the traditional neighborhood rough set. The\ngranular-ball rough set also outperforms nine popular or the state-of-the-art\nfeature selection methods.",
          "link": "http://arxiv.org/abs/2201.03349",
          "publishedOn": "2022-01-12T00:38:46.340Z",
          "wordCount": null,
          "title": "GBRS: An Unified Model of Pawlak Rough Set and Neighborhood Rough Set. (arXiv:2201.03349v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2111.15318",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Strecke_M/0/1/0/all/0/1\">Michael Strecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stueckler_J/0/1/0/all/0/1\">Joerg Stueckler</a>",
          "description": "Differentiable physics is a powerful tool in computer vision and robotics for\nscene understanding and reasoning about interactions. Existing approaches have\nfrequently been limited to objects with simple shape or shapes that are known\nin advance. In this paper, we propose a novel approach to differentiable\nphysics with frictional contacts which represents object shapes implicitly\nusing signed distance fields (SDFs). Our simulation supports contact point\ncalculation even when the involved shapes are nonconvex. Moreover, we propose\nways for differentiating the dynamics for the object shape to facilitate shape\noptimization using gradient-based methods. In our experiments, we demonstrate\nthat our approach allows for model-based inference of physical parameters such\nas friction coefficients, mass, forces or shape parameters from trajectory and\ndepth image observations in several challenging synthetic scenarios and a real\nimage sequence.",
          "link": "http://arxiv.org/abs/2111.15318",
          "publishedOn": "2022-01-12T00:38:46.322Z",
          "wordCount": null,
          "title": "DiffSDFSim: Differentiable Rigid-Body Dynamics With Implicit Shapes. (arXiv:2111.15318v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08717",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Goscinski_A/0/1/0/all/0/1\">Alexander Goscinski</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Musil_F/0/1/0/all/0/1\">F&#xe9;lix Musil</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pozdnyakov_S/0/1/0/all/0/1\">Sergey Pozdnyakov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ceriotti_M/0/1/0/all/0/1\">Michele Ceriotti</a>",
          "description": "The input of almost every machine learning algorithm targeting the properties\nof matter at the atomic scale involves a transformation of the list of\nCartesian atomic coordinates into a more symmetric representation. Many of the\nmost popular representations can be seen as an expansion of the symmetrized\ncorrelations of the atom density, and differ mainly by the choice of basis.\nConsiderable effort has been dedicated to the optimization of the basis set,\ntypically driven by heuristic considerations on the behavior of the regression\ntarget. Here we take a different, unsupervised viewpoint, aiming to determine\nthe basis that encodes in the most compact way possible the structural\ninformation that is relevant for the dataset at hand. For each training dataset\nand number of basis functions, one can determine a unique basis that is optimal\nin this sense, and can be computed at no additional cost with respect to the\nprimitive basis by approximating it with splines. We demonstrate that this\nconstruction yields representations that are accurate and computationally\nefficient, particularly when constructing representations that correspond to\nhigh-body order correlations. We present examples that involve both molecular\nand condensed-phase machine-learning models.",
          "link": "http://arxiv.org/abs/2105.08717",
          "publishedOn": "2022-01-12T00:38:46.313Z",
          "wordCount": null,
          "title": "Optimal radial basis for density-based atomic representations. (arXiv:2105.08717v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.03183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1\">Prateek Verma</a>",
          "description": "This paper presents a way of doing large scale audio understanding without\ntraditional state of the art neural architectures. Ever since the introduction\nof deep learning for understanding audio signals in the past decade,\nconvolutional architectures have been able to achieve state of the art results\nsurpassing traditional hand-crafted features. In the recent past, there has\nbeen a similar shift away from traditional convolutional and recurrent neural\nnetworks towards purely end-to-end Transformer architectures. We, in this work,\nexplore an approach, based on Bag-of-Words model. Our approach does not have\nany convolutions, recurrence, attention, transformers or other approaches such\nas BERT. We utilize micro and macro level clustered vanilla embeddings, and use\na MLP head for classification. We only use feed-forward encoder-decoder models\nto get the bottlenecks of spectral envelops, spectral patches and slices as\nwell as multi-resolution spectra. A classification head (a feed-forward layer),\nsimilar to the approach in SimCLR is trained on a learned representation. Using\nsimple codes learned on latent representations, we show how we surpass\ntraditional convolutional neural network architectures, and come strikingly\nclose to outperforming powerful Transformer architectures. This work hopefully\nwould pave way for exciting advancements in the field of representation\nlearning without massive, end-to-end neural architectures.",
          "link": "http://arxiv.org/abs/2110.03183",
          "publishedOn": "2022-01-12T00:38:46.283Z",
          "wordCount": null,
          "title": "Attention is All You Need? Good Embeddings with Statistics are enough:Large Scale Audio Understanding without Transformers/ Convolutions/ BERTs/ Mixers/ Attention/ RNNs or ..... (arXiv:2110.03183v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.03473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Villar_Corrales_A/0/1/0/all/0/1\">Angel Villar-Corrales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>",
          "description": "The ability to decompose scenes into their object components is a desired\nproperty for autonomous agents, allowing them to reason and act in their\nsurroundings. Recently, different methods have been proposed to learn\nobject-centric representations from data in an unsupervised manner. These\nmethods often rely on latent representations learned by deep neural networks,\nhence requiring high computational costs and large amounts of curated data.\nSuch models are also difficult to interpret. To address these challenges, we\npropose the Phase-Correlation Decomposition Network (PCDNet), a novel model\nthat decomposes a scene into its object components, which are represented as\ntransformed versions of a set of learned object prototypes. The core building\nblock in PCDNet is the Phase-Correlation Cell (PC Cell), which exploits the\nfrequency-domain representation of the images in order to estimate the\ntransformation between an object prototype and its transformed version in the\nimage. In our experiments, we show how PCDNet outperforms state-of-the-art\nmethods for unsupervised object discovery and segmentation on simple benchmark\ndatasets and on more challenging data, while using a small number of learnable\nparameters and being fully interpretable.",
          "link": "http://arxiv.org/abs/2110.03473",
          "publishedOn": "2022-01-12T00:38:46.280Z",
          "wordCount": null,
          "title": "Unsupervised Image Decomposition with Phase-Correlation Networks. (arXiv:2110.03473v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yujun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>",
          "description": "Continual learning tackles the setting of learning different tasks\nsequentially. Despite the lots of previous solutions, most of them still suffer\nsignificant forgetting or expensive memory cost. In this work, targeted at\nthese problems, we first study the continual learning process through the lens\nof information theory and observe that forgetting of a model stems from the\nloss of \\emph{information gain} on its parameters from the previous tasks when\nlearning a new task. From this viewpoint, we then propose a novel continual\nlearning approach called Bit-Level Information Preserving (BLIP) that preserves\nthe information gain on model parameters through updating the parameters at the\nbit level, which can be conveniently implemented with parameter quantization.\nMore specifically, BLIP first trains a neural network with weight quantization\non the new incoming task and then estimates information gain on each parameter\nprovided by the task data to determine the bits to be frozen to prevent\nforgetting. We conduct extensive experiments ranging from classification tasks\nto reinforcement learning tasks, and the results show that our method produces\nbetter or on par results comparing to previous state-of-the-arts. Indeed, BLIP\nachieves close to zero forgetting while only requiring constant memory\noverheads throughout continual learning.",
          "link": "http://arxiv.org/abs/2105.04444",
          "publishedOn": "2022-01-12T00:38:46.275Z",
          "wordCount": null,
          "title": "Continual Learning via Bit-Level Information Preserving. (arXiv:2105.04444v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1\">Frances Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardt_M/0/1/0/all/0/1\">Moritz Hardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1\">John Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>",
          "description": "Although the fairness community has recognized the importance of data,\nresearchers in the area primarily rely on UCI Adult when it comes to tabular\ndata. Derived from a 1994 US Census survey, this dataset has appeared in\nhundreds of research papers where it served as the basis for the development\nand comparison of many algorithmic fairness interventions. We reconstruct a\nsuperset of the UCI Adult data from available US Census sources and reveal\nidiosyncrasies of the UCI Adult dataset that limit its external validity. Our\nprimary contribution is a suite of new datasets derived from US Census surveys\nthat extend the existing data ecosystem for research on fair machine learning.\nWe create prediction tasks relating to income, employment, health,\ntransportation, and housing. The data span multiple years and all states of the\nUnited States, allowing researchers to study temporal shift and geographic\nvariation. We highlight a broad initial sweep of new empirical insights\nrelating to trade-offs between fairness criteria, performance of algorithmic\ninterventions, and the role of distribution shift based on our new datasets.\nOur findings inform ongoing debates, challenge some existing narratives, and\npoint to future research directions. Our datasets are available at\nhttps://github.com/zykls/folktables.",
          "link": "http://arxiv.org/abs/2108.04884",
          "publishedOn": "2022-01-12T00:38:46.274Z",
          "wordCount": null,
          "title": "Retiring Adult: New Datasets for Fair Machine Learning. (arXiv:2108.04884v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.00364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Carole-Jean Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavendra_R/0/1/0/all/0/1\">Ramya Raghavendra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_U/0/1/0/all/0/1\">Udit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acun_B/0/1/0/all/0/1\">Bilge Acun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardalani_N/0/1/0/all/0/1\">Newsha Ardalani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maeng_K/0/1/0/all/0/1\">Kiwan Maeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_G/0/1/0/all/0/1\">Gloria Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behram_F/0/1/0/all/0/1\">Fiona Aga Behram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">James Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1\">Charles Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gschwind_M/0/1/0/all/0/1\">Michael Gschwind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anurag Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_M/0/1/0/all/0/1\">Myle Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melnikov_A/0/1/0/all/0/1\">Anastasia Melnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Candido_S/0/1/0/all/0/1\">Salvatore Candido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brooks_D/0/1/0/all/0/1\">David Brooks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_G/0/1/0/all/0/1\">Geeta Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Benjamin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsien-Hsin S. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akyildiz_B/0/1/0/all/0/1\">Bugra Akyildiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balandat_M/0/1/0/all/0/1\">Maximilian Balandat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spisak_J/0/1/0/all/0/1\">Joe Spisak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Ravi Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1\">Mike Rabbat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazelwood_K/0/1/0/all/0/1\">Kim Hazelwood</a>",
          "description": "This paper explores the environmental impact of the super-linear growth\ntrends for AI from a holistic perspective, spanning Data, Algorithms, and\nSystem Hardware. We characterize the carbon footprint of AI computing by\nexamining the model development cycle across industry-scale machine learning\nuse cases and, at the same time, considering the life cycle of system hardware.\nTaking a step further, we capture the operational and manufacturing carbon\nfootprint of AI computing and present an end-to-end analysis for what and how\nhardware-software design and at-scale optimization can help reduce the overall\ncarbon footprint of AI. Based on the industry experience and lessons learned,\nwe share the key challenges and chart out important development directions\nacross the many dimensions of AI. We hope the key messages and insights\npresented in this paper can inspire the community to advance the field of AI in\nan environmentally-responsible manner.",
          "link": "http://arxiv.org/abs/2111.00364",
          "publishedOn": "2022-01-12T00:38:46.274Z",
          "wordCount": null,
          "title": "Sustainable AI: Environmental Implications, Challenges and Opportunities. (arXiv:2111.00364v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srishankar_N/0/1/0/all/0/1\">Nishan Srishankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_S/0/1/0/all/0/1\">Sujitha Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1\">Masayoshi Tomizuka</a>",
          "description": "Explainability is essential for autonomous vehicles and other robotics\nsystems interacting with humans and other objects during operation. Humans need\nto understand and anticipate the actions taken by the machines for trustful and\nsafe cooperation. In this work, we aim to develop an explainable model that\ngenerates explanations consistent with both human domain knowledge and the\nmodel's inherent causal relation. In particular, we focus on an essential\nbuilding block of autonomous driving, multi-agent interaction modeling. We\npropose Grounded Relational Inference (GRI). It models an interactive system's\nunderlying dynamics by inferring an interaction graph representing the agents'\nrelations. We ensure a semantically meaningful interaction graph by grounding\nthe relational latent space into semantic interactive behaviors defined with\nexpert domain knowledge. We demonstrate that it can model interactive traffic\nscenarios under both simulation and real-world settings, and generate semantic\ngraphs explaining the vehicle's behavior by their interactions.",
          "link": "http://arxiv.org/abs/2102.11905",
          "publishedOn": "2022-01-12T00:38:46.272Z",
          "wordCount": null,
          "title": "Grounded Relational Inference: Domain Knowledge Driven Explainable Autonomous Driving. (arXiv:2102.11905v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09468",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Tang_H/0/1/0/all/0/1\">Hewei Tang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Fu_P/0/1/0/all/0/1\">Pengcheng Fu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sherman_C/0/1/0/all/0/1\">Christopher S. Sherman</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_J/0/1/0/all/0/1\">Jize Zhang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ju_X/0/1/0/all/0/1\">Xin Ju</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hamon_F/0/1/0/all/0/1\">Fran&#xe7;ois Hamon</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Azzolina_N/0/1/0/all/0/1\">Nicholas A. Azzolina</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Burton_Kelly_M/0/1/0/all/0/1\">Matthew Burton-Kelly</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Morris_J/0/1/0/all/0/1\">Joseph P. Morris</a>",
          "description": "Fast assimilation of monitoring data to update forecasts of pressure buildup\nand carbon dioxide (CO2) plume migration under geologic uncertainties is a\nchallenging problem in geologic carbon storage. The high computational cost of\ndata assimilation with a high-dimensional parameter space impedes fast\ndecision-making for commercial-scale reservoir management. We propose to\nleverage physical understandings of porous medium flow behavior with deep\nlearning techniques to develop a fast history matching-reservoir response\nforecasting workflow. Applying an Ensemble Smoother Multiple Data Assimilation\nframework, the workflow updates geologic properties and predicts reservoir\nperformance with quantified uncertainty from pressure history and CO2 plumes\ninterpreted through seismic inversion. As the most computationally expensive\ncomponent in such a workflow is reservoir simulation, we developed surrogate\nmodels to predict dynamic pressure and CO2 plume extents under multi-well\ninjection. The surrogate models employ deep convolutional neural networks,\nspecifically, a wide residual network and a residual U-Net. The workflow is\nvalidated against a flat three-dimensional reservoir model representative of a\nclastic shelf depositional environment. Intelligent treatments are applied to\nbridge between quantities in a true-3D reservoir model and those in a\nsingle-layer reservoir model. The workflow can complete history matching and\nreservoir forecasting with uncertainty quantification in less than one hour on\na mainstream personal workstation.",
          "link": "http://arxiv.org/abs/2105.09468",
          "publishedOn": "2022-01-12T00:38:46.270Z",
          "wordCount": null,
          "title": "A Deep Learning-Accelerated Data Assimilation and Forecasting Workflow for Commercial-Scale Geologic Carbon Storage. (arXiv:2105.09468v2 [physics.geo-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03319",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wulff_D/0/1/0/all/0/1\">Daniel Wulff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hagenah_J/0/1/0/all/0/1\">Jannis Hagenah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ernst_F/0/1/0/all/0/1\">Floris Ernst</a>",
          "description": "3D ultrasound (3DUS) becomes more interesting for target tracking in\nradiation therapy due to its capability to provide volumetric images in\nreal-time without using ionizing radiation. It is potentially usable for\ntracking without using fiducials. For this, a method for learning meaningful\nrepresentations would be useful to recognize anatomical structures in different\ntime frames in representation space (r-space). In this study, 3DUS patches are\nreduced into a 128-dimensional r-space using conventional autoencoder,\nvariational autoencoder and sliced-wasserstein autoencoder. In the r-space, the\ncapability of separating different ultrasound patches as well as recognizing\nsimilar patches is investigated and compared based on a dataset of liver\nimages. Two metrics to evaluate the tracking capability in the r-space are\nproposed. It is shown that ultrasound patches with different anatomical\nstructures can be distinguished and sets of similar patches can be clustered in\nr-space. The results indicate that the investigated autoencoders have different\nlevels of usability for target tracking in 3DUS.",
          "link": "http://arxiv.org/abs/2201.03319",
          "publishedOn": "2022-01-12T00:38:46.267Z",
          "wordCount": null,
          "title": "Comparison of Representation Learning Techniques for Tracking in time resolved 3D Ultrasound. (arXiv:2201.03319v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.11193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feldman_V/0/1/0/all/0/1\">Vitaly Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zrnic_T/0/1/0/all/0/1\">Tijana Zrnic</a>",
          "description": "We consider a sequential setting in which a single dataset of individuals is\nused to perform adaptively-chosen analyses, while ensuring that the\ndifferential privacy loss of each participant does not exceed a pre-specified\nprivacy budget. The standard approach to this problem relies on bounding a\nworst-case estimate of the privacy loss over all individuals and all possible\nvalues of their data, for every single analysis. Yet, in many scenarios this\napproach is overly conservative, especially for \"typical\" data points which\nincur little privacy loss by participation in most of the analyses. In this\nwork, we give a method for tighter privacy loss accounting based on the value\nof a personalized privacy loss estimate for each individual in each analysis.\nTo implement the accounting method we design a filter for R\\'enyi differential\nprivacy. A filter is a tool that ensures that the privacy parameter of a\ncomposed sequence of algorithms with adaptively-chosen privacy parameters does\nnot exceed a pre-specified budget. Our filter is simpler and tighter than the\nknown filter for $(\\epsilon,\\delta)$-differential privacy by Rogers et al. We\napply our results to the analysis of noisy gradient descent and show that\npersonalized accounting can be practical, easy to implement, and can only make\nthe privacy-utility tradeoff tighter.",
          "link": "http://arxiv.org/abs/2008.11193",
          "publishedOn": "2022-01-12T00:38:46.265Z",
          "wordCount": null,
          "title": "Individual Privacy Accounting via a Renyi Filter. (arXiv:2008.11193v4 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Santos_C/0/1/0/all/0/1\">Claudio Filipi Gon&#xe7;alves dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1\">Diego de Souza Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passos_L/0/1/0/all/0/1\">Leandro A. Passos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pires_R/0/1/0/all/0/1\">Rafael Gon&#xe7;alves Pires</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_D/0/1/0/all/0/1\">Daniel Felipe Silva Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valem_L/0/1/0/all/0/1\">Lucas Pascotti Valem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_T/0/1/0/all/0/1\">Thierry P. Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santana_M/0/1/0/all/0/1\">Marcos Cleison S. Santana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roder_M/0/1/0/all/0/1\">Mateus Roder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Papa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombo_D/0/1/0/all/0/1\">Danilo Colombo</a>",
          "description": "In general, biometry-based control systems may not rely on individual\nexpected behavior or cooperation to operate appropriately. Instead, such\nsystems should be aware of malicious procedures for unauthorized access\nattempts. Some works available in the literature suggest addressing the problem\nthrough gait recognition approaches. Such methods aim at identifying human\nbeings through intrinsic perceptible features, despite dressed clothes or\naccessories. Although the issue denotes a relatively long-time challenge, most\nof the techniques developed to handle the problem present several drawbacks\nrelated to feature extraction and low classification rates, among other issues.\nHowever, deep learning-based approaches recently emerged as a robust set of\ntools to deal with virtually any image and computer-vision related problem,\nproviding paramount results for gait recognition as well. Therefore, this work\nprovides a surveyed compilation of recent works regarding biometric detection\nthrough gait recognition with a focus on deep learning approaches, emphasizing\ntheir benefits, and exposing their weaknesses. Besides, it also presents\ncategorized and characterized descriptions of the datasets, approaches, and\narchitectures employed to tackle associated constraints.",
          "link": "http://arxiv.org/abs/2201.03323",
          "publishedOn": "2022-01-12T00:38:46.264Z",
          "wordCount": null,
          "title": "Gait Recognition Based on Deep Learning: A Survey. (arXiv:2201.03323v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2111.15655",
          "author": "<a href=\"http://arxiv.org/find/hep-ph/1/au:+Biro_G/0/1/0/all/0/1\">G&#xe1;bor B&#xed;r&#xf3;</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Tanko_Bartalis_B/0/1/0/all/0/1\">Bence Tank&#xf3;-Bartalis</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Barnafoldi_G/0/1/0/all/0/1\">Gergely G&#xe1;bor Barnaf&#xf6;ldi</a>",
          "description": "Hadronization is a non-perturbative process, which theoretical description\ncan not be deduced from first principles. Modeling hadron formation requires\nseveral assumptions and various phenomenological approaches. Utilizing\nstate-of-the-art Computer Vision and Deep Learning algorithms, it is eventually\npossible to train neural networks to learn non-linear and non-perturbative\nfeatures of the physical processes. In this study, results of two ResNet\nnetworks are presented by investigating global and kinematical quantities,\nindeed jet- and event-shape variables. The widely used Lund string\nfragmentation model is applied as a baseline in $\\sqrt{s}= 7$ TeV proton-proton\ncollisions to predict the most relevant observables at further LHC energies.",
          "link": "http://arxiv.org/abs/2111.15655",
          "publishedOn": "2022-01-12T00:38:46.264Z",
          "wordCount": null,
          "title": "Studying Hadronization by Machine Learning Techniques. (arXiv:2111.15655v2 [hep-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gerald_T/0/1/0/all/0/1\">Thomas Gerald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soulier_L/0/1/0/all/0/1\">Laure Soulier</a>",
          "description": "In information retrieval (IR) systems, trends and users' interests may change\nover time, altering either the distribution of requests or contents to be\nrecommended. Since neural ranking approaches heavily depend on the training\ndata, it is crucial to understand the transfer capacity of recent IR approaches\nto address new domains in the long term. In this paper, we first propose a\ndataset based upon the MSMarco corpus aiming at modeling a long stream of\ntopics as well as IR property-driven controlled settings. We then in-depth\nanalyze the ability of recent neural IR models while continually learning those\nstreams. Our empirical study highlights in which particular cases catastrophic\nforgetting occurs (e.g., level of similarity between tasks, peculiarities on\ntext length, and ways of learning models) to provide future directions in terms\nof model design.",
          "link": "http://arxiv.org/abs/2201.03356",
          "publishedOn": "2022-01-12T00:38:46.258Z",
          "wordCount": null,
          "title": "Continual Learning of Long Topic Sequences in Neural Information Retrieval. (arXiv:2201.03356v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08208",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "Adaptive gradient methods have shown excellent performances for solving many\nmachine learning problems. Although multiple adaptive gradient methods were\nrecently studied, they mainly focus on either empirical or theoretical aspects\nand also only work for specific problems by using some specific adaptive\nlearning rates. Thus, it is desired to design a universal framework for\npractical algorithms of adaptive gradients with theoretical guarantee to solve\ngeneral problems. To fill this gap, we propose a faster and universal framework\nof adaptive gradients (i.e., SUPER-ADAM) by introducing a universal adaptive\nmatrix that includes most existing adaptive gradient forms. Moreover, our\nframework can flexibly integrate the momentum and variance reduced techniques.\nIn particular, our novel framework provides the convergence analysis support\nfor adaptive gradient methods under the nonconvex setting. In theoretical\nanalysis, we prove that our SUPER-ADAM algorithm can achieve the best known\ngradient (i.e., stochastic first-order oracle (SFO)) complexity of\n$\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point of\nnonconvex optimization, which matches the lower bound for stochastic smooth\nnonconvex optimization. In numerical experiments, we employ various deep\nlearning tasks to validate that our algorithm consistently outperforms the\nexisting adaptive algorithms. Code is available at\nhttps://github.com/LIJUNYI95/SuperAdam",
          "link": "http://arxiv.org/abs/2106.08208",
          "publishedOn": "2022-01-12T00:38:46.257Z",
          "wordCount": null,
          "title": "SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v8 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zeyu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyu Zhang</a>",
          "description": "Factorization machine (FM) is a prevalent approach to modeling pairwise\n(second-order) feature interactions when dealing with high-dimensional sparse\ndata. However, on the one hand, FM fails to capture higher-order feature\ninteractions suffering from combinatorial expansion, on the other hand, taking\ninto account interaction between every pair of features may introduce noise and\ndegrade prediction accuracy. To solve the problems, we propose a novel approach\nGraph Factorization Machine (GraphFM) by naturally representing features in the\ngraph structure. In particular, a novel mechanism is designed to select the\nbeneficial feature interactions and formulate them as edges between features.\nThen our proposed model which integrates the interaction function of FM into\nthe feature aggregation strategy of Graph Neural Network (GNN), can model\narbitrary-order feature interactions on the graph-structured features by\nstacking layers. Experimental results on several real-world datasets has\ndemonstrated the rationality and effectiveness of our proposed approach.",
          "link": "http://arxiv.org/abs/2105.11866",
          "publishedOn": "2022-01-12T00:38:46.256Z",
          "wordCount": null,
          "title": "GraphFM: Graph Factorization Machines for Feature Interaction Modeling. (arXiv:2105.11866v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jiajun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Changnan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yue Huang</a>",
          "description": "Deep Q Network (DQN) firstly kicked the door of deep reinforcement learning\n(DRL) via combining deep learning (DL) with reinforcement learning (RL), which\nhas noticed that the distribution of the acquired data would change during the\ntraining process. DQN found this property might cause instability for training,\nso it proposed effective methods to handle the downside of the property.\nInstead of focusing on the unfavourable aspects, we find it critical for RL to\nease the gap between the estimated data distribution and the ground truth data\ndistribution while supervised learning (SL) fails to do so. From this new\nperspective, we extend the basic paradigm of RL called the Generalized Policy\nIteration (GPI) into a more generalized version, which is called the\nGeneralized Data Distribution Iteration (GDI). We see massive RL algorithms and\ntechniques can be unified into the GDI paradigm, which can be considered as one\nof the special cases of GDI. We provide theoretical proof of why GDI is better\nthan GPI and how it works. Several practical algorithms based on GDI have been\nproposed to verify the effectiveness and extensiveness of it. Empirical\nexperiments prove our state-of-the-art (SOTA) performance on Arcade Learning\nEnvironment (ALE), wherein our algorithm has achieved 9620.98% mean human\nnormalized score (HNS), 1146.39% median HNS and 22 human world record\nbreakthroughs (HWRB) using only 200M training frames. Our work aims to lead the\nRL research to step into the journey of conquering the human world records and\nseek real superhuman agents on both performance and efficiency.",
          "link": "http://arxiv.org/abs/2106.06232",
          "publishedOn": "2022-01-12T00:38:46.254Z",
          "wordCount": null,
          "title": "GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning. (arXiv:2106.06232v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mizutani_T/0/1/0/all/0/1\">Tomohiko Mizutani</a>",
          "description": "Hottopixx, proposed by Bittorf et al. at NIPS 2012, is an algorithm for\nsolving nonnegative matrix factorization (NMF) problems under the separability\nassumption. Separable NMFs have important applications, such as topic\nextraction from documents and unmixing of hyperspectral images. In such\napplications, the robustness of the algorithm to noise is the key to the\nsuccess. Hottopixx has been shown to be robust to noise, and its robustness can\nbe further enhanced through postprocessing. However, there is a drawback.\nHottopixx and its postprocessing require us to estimate the noise level\ninvolved in the matrix we want to factorize before running, since they use it\nas part of the input data. The noise-level estimation is not an easy task. In\nthis paper, we overcome this drawback. We present a refinement of Hottopixx and\nits postprocessing that runs without prior knowledge of the noise level. We\nshow that the refinement has almost the same robustness to noise as the\noriginal algorithm.",
          "link": "http://arxiv.org/abs/2109.02863",
          "publishedOn": "2022-01-12T00:38:46.252Z",
          "wordCount": null,
          "title": "Refinement of Hottopixx Method for Nonnegative Matrix Factorization Under Noisy Separability. (arXiv:2109.02863v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.01745",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Carnevale_G/0/1/0/all/0/1\">Guido Carnevale</a>, <a href=\"http://arxiv.org/find/math/1/au:+Farina_F/0/1/0/all/0/1\">Francesco Farina</a>, <a href=\"http://arxiv.org/find/math/1/au:+Notarnicola_I/0/1/0/all/0/1\">Ivano Notarnicola</a>, <a href=\"http://arxiv.org/find/math/1/au:+Notarstefano_G/0/1/0/all/0/1\">Giuseppe Notarstefano</a>",
          "description": "This paper deals with a network of computing agents aiming to solve an online\noptimization problem in a distributed fashion, i.e., by means of local\ncomputation and communication, without any central coordinator. We propose the\ngradient tracking with adaptive momentum estimation (GTAdam) distributed\nalgorithm, which combines a gradient tracking mechanism with first and second\norder momentum estimates of the gradient. The algorithm is analyzed in the\nonline setting for strongly convex cost functions with Lipschitz continuous\ngradients. We provide an upper bound for the dynamic regret given by a term\nrelated to the initial conditions, and another term related to the temporal\nvariations of the objective functions. Moreover, a linear convergence rate is\nguaranteed in the static set-up. The algorithm is tested on a time-varying\nclassification problem, on a (moving) target localization problem and in a\nstochastic optimization setup from image classification. In these numerical\nexperiments from multi-agent learning, GTAdam outperforms state-of-the-art\ndistributed optimization methods.",
          "link": "http://arxiv.org/abs/2009.01745",
          "publishedOn": "2022-01-12T00:38:46.249Z",
          "wordCount": null,
          "title": "GTAdam: Gradient Tracking with Adaptive Momentum for Distributed Online Optimization. (arXiv:2009.01745v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.07471",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chowdhury_A/0/1/0/all/0/1\">Arindam Chowdhury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gama_F/0/1/0/all/0/1\">Fernando Gama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Segarra_S/0/1/0/all/0/1\">Santiago Segarra</a>",
          "description": "Power allocation is one of the fundamental problems in wireless networks and\na wide variety of algorithms address this problem from different perspectives.\nA common element among these algorithms is that they rely on an estimation of\nthe channel state, which may be inaccurate on account of hardware defects,\nnoisy feedback systems, and environmental and adversarial disturbances.\nTherefore, it is essential that the output power allocation of these algorithms\nis stable with respect to input perturbations, to the extent that the\nvariations in the output are bounded for bounded variations in the input. In\nthis paper, we focus on UWMMSE -- a modern algorithm leveraging graph neural\nnetworks --, and illustrate its stability to additive input perturbations of\nbounded energy through both theoretical analysis and empirical validation.",
          "link": "http://arxiv.org/abs/2110.07471",
          "publishedOn": "2022-01-12T00:38:46.246Z",
          "wordCount": null,
          "title": "Stability Analysis of Unfolded WMMSE for Power Allocation. (arXiv:2110.07471v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1\">Nan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marschall_M/0/1/0/all/0/1\">Max Marschall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burry_J/0/1/0/all/0/1\">Jane Burry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watkins_S/0/1/0/all/0/1\">Simon Watkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1\">Flora D. Salim</a>",
          "description": "We conducted a field study at a K-12 private school in the suburbs of\nMelbourne, Australia. The data capture contained two elements: First, a 5-month\nlongitudinal field study In-Gauge using two outdoor weather stations, as well\nas indoor weather stations in 17 classrooms and temperature sensors on the\nvents of occupant-controlled room air-conditioners; these were collated into\nindividual datasets for each classroom at a 5-minute logging frequency,\nincluding additional data on occupant presence. The dataset was used to derive\npredictive models of how occupants operate room air-conditioning units. Second,\nwe tracked 23 students and 6 teachers in a 4-week cross-sectional study\nEn-Gage, using wearable sensors to log physiological data, as well as daily\nsurveys to query the occupants' thermal comfort, learning engagement, emotions\nand seating behaviours. Overall, the combined dataset could be used to analyse\nthe relationships between indoor/outdoor climates and students'\nbehaviours/mental states on campus, which provide opportunities for the future\ndesign of intelligent feedback systems to benefit both students and staff.",
          "link": "http://arxiv.org/abs/2105.06637",
          "publishedOn": "2022-01-12T00:38:46.245Z",
          "wordCount": null,
          "title": "Understanding occupants' behaviour, engagement, emotion, and comfort indoors with heterogeneous sensors and wearables. (arXiv:2105.06637v2 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.07649",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shome_D/0/1/0/all/0/1\">Debaditya Shome</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Waqar_O/0/1/0/all/0/1\">Omer Waqar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_W/0/1/0/all/0/1\">Wali Ullah Khan</a>",
          "description": "In order to meet the extremely heterogeneous requirements of the next\ngeneration wireless communication networks, research community is increasingly\ndependent on using machine learning solutions for real-time decision-making and\nradio resource management. Traditional machine learning employs fully\ncentralized architecture in which the entire training data is collected at one\nnode e.g., cloud server, that significantly increases the communication\noverheads and also raises severe privacy concerns. Towards this end, a\ndistributed machine learning paradigm termed as Federated learning (FL) has\nbeen proposed recently. In FL, each participating edge device trains its local\nmodel by using its own training data. Then, via the wireless channels the\nweights or parameters of the locally trained models are sent to the central PS,\nthat aggregates them and updates the global model. On one hand, FL plays an\nimportant role for optimizing the resources of wireless communication networks,\non the other hand, wireless communications is crucial for FL. Thus, a\n`bidirectional' relationship exists between FL and wireless communications.\nAlthough FL is an emerging concept, many publications have already been\npublished in the domain of FL and its applications for next generation wireless\nnetworks. Nevertheless, we noticed that none of the works have highlighted the\nbidirectional relationship between FL and wireless communications. Therefore,\nthe purpose of this survey paper is to bridge this gap in literature by\nproviding a timely and comprehensive discussion on the interdependency between\nFL and wireless communications.",
          "link": "http://arxiv.org/abs/2110.07649",
          "publishedOn": "2022-01-12T00:38:46.245Z",
          "wordCount": null,
          "title": "Federated learning and next generation wireless communications: A survey on bidirectional relationship. (arXiv:2110.07649v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.02901",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mernyei_P/0/1/0/all/0/1\">P&#xe9;ter Mernyei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cangea_C/0/1/0/all/0/1\">C&#x103;t&#x103;lina Cangea</a>",
          "description": "We present Wiki-CS, a novel dataset derived from Wikipedia for benchmarking\nGraph Neural Networks. The dataset consists of nodes corresponding to Computer\nScience articles, with edges based on hyperlinks and 10 classes representing\ndifferent branches of the field. We use the dataset to evaluate semi-supervised\nnode classification and single-relation link prediction models. Our experiments\nshow that these methods perform well on a new domain, with structural\nproperties different from earlier benchmarks. The dataset is publicly\navailable, along with the implementation of the data pipeline and the benchmark\nexperiments, at https://github.com/pmernyei/wiki-cs-dataset .",
          "link": "http://arxiv.org/abs/2007.02901",
          "publishedOn": "2022-01-12T00:38:46.240Z",
          "wordCount": null,
          "title": "Wiki-CS: A Wikipedia-Based Benchmark for Graph Neural Networks. (arXiv:2007.02901v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.14051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1\">Sahidul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jieren Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shanglin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Caiwen Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Mimi Xie</a>",
          "description": "Energy harvesting (EH) IoT devices that operate intermittently without\nbatteries, coupled with advances in deep neural networks (DNNs), have opened up\nnew opportunities for enabling sustainable smart applications. Nevertheless,\nimplementing those computation and memory-intensive intelligent algorithms on\nEH devices is extremely difficult due to the challenges of limited resources\nand intermittent power supply that causes frequent failures. To address those\nchallenges, this paper proposes a methodology that enables fast deep learning\nwith low-energy accelerators for tiny energy harvesting devices. We first\npropose $RAD$, a resource-aware structured DNN training framework, which\nemploys block circulant matrix and structured pruning to achieve high\ncompression for leveraging the advantage of various vector operation\naccelerators. A DNN implementation method, $ACE$, is then proposed that employs\nlow-energy accelerators to profit maximum performance with small energy\nconsumption. Finally, we further design $FLEX$, the system support for\nintermittent computation in energy harvesting situations. Experimental results\nfrom three different DNN models demonstrate that $RAD$, $ACE$, and $FLEX$ can\nenable fast and correct inference on energy harvesting devices with up to 4.26X\nruntime reduction, up to 7.7X energy reduction with higher accuracy over the\nstate-of-the-art.",
          "link": "http://arxiv.org/abs/2111.14051",
          "publishedOn": "2022-01-12T00:38:46.240Z",
          "wordCount": null,
          "title": "Enabling Fast Deep Learning on Tiny Energy-Harvesting IoT Devices. (arXiv:2111.14051v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00363",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guoyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinbao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Guo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yaochu Jin</a>",
          "description": "Due to limited computational cost and energy consumption, most neural network\nmodels deployed in mobile devices are tiny. However, tiny neural networks are\ncommonly very vulnerable to attacks. Current research has proved that larger\nmodel size can improve robustness, but little research focuses on how to\nenhance the robustness of tiny neural networks. Our work focuses on how to\nimprove the robustness of tiny neural networks without seriously deteriorating\nof clean accuracy under mobile-level resources. To this end, we propose a\nmulti-objective oneshot network architecture search (NAS) algorithm to obtain\nthe best trade-off networks in terms of the adversarial accuracy, the clean\naccuracy and the model size. Specifically, we design a novel search space based\non new tiny blocks and channels to balance model size and adversarial\nperformance. Moreover, since the supernet significantly affects the performance\nof subnets in our NAS algorithm, we reveal the insights into how the supernet\nhelps to obtain the best subnet under white-box adversarial attacks.\nConcretely, we explore a new adversarial training paradigm by analyzing the\nadversarial transferability, the width of the supernet and the difference\nbetween training the subnets from scratch and fine-tuning. Finally, we make a\nstatistical analysis for the layer-wise combination of certain blocks and\nchannels on the first non-dominated front, which can serve as a guideline to\ndesign tiny neural network architectures for the resilience of adversarial\nperturbations.",
          "link": "http://arxiv.org/abs/2103.00363",
          "publishedOn": "2022-01-12T00:38:46.222Z",
          "wordCount": null,
          "title": "Tiny Adversarial Mulit-Objective Oneshot Neural Architecture Search. (arXiv:2103.00363v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Messina_P/0/1/0/all/0/1\">Pablo Messina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_P/0/1/0/all/0/1\">Pablo Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parra_D/0/1/0/all/0/1\">Denis Parra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1\">Alvaro Soto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besa_C/0/1/0/all/0/1\">Cecilia Besa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uribe_S/0/1/0/all/0/1\">Sergio Uribe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+andia_M/0/1/0/all/0/1\">Marcelo and&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tejos_C/0/1/0/all/0/1\">Cristian Tejos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prieto_C/0/1/0/all/0/1\">Claudia Prieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capurro_D/0/1/0/all/0/1\">Daniel Capurro</a>",
          "description": "Every year physicians face an increasing demand of image-based diagnosis from\npatients, a problem that can be addressed with recent artificial intelligence\nmethods. In this context, we survey works in the area of automatic report\ngeneration from medical images, with emphasis on methods using deep neural\nnetworks, with respect to: (1) Datasets, (2) Architecture Design, (3)\nExplainability and (4) Evaluation Metrics. Our survey identifies interesting\ndevelopments, but also remaining challenges. Among them, the current evaluation\nof generated reports is especially weak, since it mostly relies on traditional\nNatural Language Processing (NLP) metrics, which do not accurately capture\nmedical correctness.",
          "link": "http://arxiv.org/abs/2010.10563",
          "publishedOn": "2022-01-12T00:38:46.221Z",
          "wordCount": null,
          "title": "A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images. (arXiv:2010.10563v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.01184",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Polo_F/0/1/0/all/0/1\">Felipe Maia Polo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vicente_R/0/1/0/all/0/1\">Renato Vicente</a>",
          "description": "In supervised learning, training and test datasets are often sampled from\ndistinct distributions. Domain adaptation techniques are thus required.\nCovariate shift adaptation yields good generalization performance when domains\ndiffer only by the marginal distribution of features. Covariate shift\nadaptation is usually implemented using importance weighting, which may fail,\naccording to common wisdom, due to small effective sample sizes (ESS). Previous\nresearch argues this scenario is more common in high-dimensional settings.\nHowever, how effective sample size, dimensionality, and model\nperformance/generalization are formally related in supervised learning,\nconsidering the context of covariate shift adaptation, is still somewhat\nobscure in the literature. Thus, a main challenge is presenting a unified\ntheory connecting those points. Hence, in this paper, we focus on building a\nunified view connecting the ESS, data dimensionality, and generalization in the\ncontext of covariate shift adaptation. Moreover, we also demonstrate how\ndimensionality reduction or feature selection can increase the ESS, and argue\nthat our results support dimensionality reduction before covariate shift\nadaptation as a good practice.",
          "link": "http://arxiv.org/abs/2010.01184",
          "publishedOn": "2022-01-12T00:38:46.220Z",
          "wordCount": null,
          "title": "Effective Sample Size, Dimensionality, and Generalization in Covariate Shift Adaptation. (arXiv:2010.01184v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boukhers_Z/0/1/0/all/0/1\">Zeyd Boukhers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_T/0/1/0/all/0/1\">Timo Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurjens_J/0/1/0/all/0/1\">Jan J&#xfc;rjens</a>",
          "description": "Due to the significant advancement of Natural Language Processing and\nComputer Vision-based models, Visual Question Answering (VQA) systems are\nbecoming more intelligent and advanced. However, they are still error-prone\nwhen dealing with relatively complex questions. Therefore, it is important to\nunderstand the behaviour of the VQA models before adopting their results. In\nthis paper, we introduce an interpretability approach for VQA models by\ngenerating counterfactual images. Specifically, the generated image is supposed\nto have the minimal possible change to the original image and leads the VQA\nmodel to give a different answer. In addition, our approach ensures that the\ngenerated image is realistic. Since quantitative metrics cannot be employed to\nevaluate the interpretability of the model, we carried out a user study to\nassess different aspects of our approach. In addition to interpreting the\nresult of VQA models on single images, the obtained results and the discussion\nprovides an extensive explanation of VQA models' behaviour.",
          "link": "http://arxiv.org/abs/2201.03342",
          "publishedOn": "2022-01-12T00:38:46.217Z",
          "wordCount": null,
          "title": "COIN: Counterfactual Image Generation for VQA Interpretation. (arXiv:2201.03342v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.14600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verma_M/0/1/0/all/0/1\">Miki E. Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iannacone_M/0/1/0/all/0/1\">Michael D. Iannacone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bridges_R/0/1/0/all/0/1\">Robert A. Bridges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollifield_S/0/1/0/all/0/1\">Samuel C. Hollifield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moriano_P/0/1/0/all/0/1\">Pablo Moriano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kay_B/0/1/0/all/0/1\">Bill Kay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Combs_F/0/1/0/all/0/1\">Frank L. Combs</a>",
          "description": "Although ubiquitous in modern vehicles, Controller Area Networks (CANs) lack\nbasic security properties and are easily exploitable. A rapidly growing field\nof CAN security research has emerged that seeks to detect intrusions on CANs.\nProducing vehicular CAN data with a variety of intrusions is out of reach for\nmost researchers as it requires expensive assets and expertise. To assist\nresearchers, we present the first comprehensive guide to the existing open CAN\nintrusion datasets, including a quality analysis of each dataset and an\nenumeration of each's benefits, drawbacks, and suggested use case. Current\npublic CAN IDS datasets are limited to real fabrication (simple message\ninjection) attacks and simulated attacks often in synthetic data, which lack\nfidelity. In general, the physical effects of attacks on the vehicle are not\nverified in the available datasets. Only one dataset provides signal-translated\ndata but not a corresponding raw binary version. Overall, the available data\npigeon-holes CAN IDS works into testing on limited, often inappropriate data\n(usually with attacks that are too easily detectable to truly test the method),\nand this lack data has stymied comparability and reproducibility of results. As\nour primary contribution, we present the ROAD (Real ORNL Automotive\nDynamometer) CAN Intrusion Dataset, consisting of over 3.5 hours of one\nvehicle's CAN data. ROAD contains ambient data recorded during a diverse set of\nactivities, and attacks of increasing stealth with multiple variants and\ninstances of real fuzzing, fabrication, and unique advanced attacks, as well as\nsimulated masquerade attacks. To facilitate benchmarking CAN IDS methods that\nrequire signal-translated inputs, we also provide the signal time series format\nfor many of the CAN captures. Our contributions aim to facilitate appropriate\nbenchmarking and needed comparability in the CAN IDS field.",
          "link": "http://arxiv.org/abs/2012.14600",
          "publishedOn": "2022-01-12T00:38:46.209Z",
          "wordCount": null,
          "title": "Addressing the Lack of Comparability & Testing in CAN Intrusion Detection Research: A Comprehensive Guide to CAN IDS Data & Introduction of the ROAD Dataset. (arXiv:2012.14600v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10716",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ponomarchuk_A/0/1/0/all/0/1\">Alexander Ponomarchuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burenko_I/0/1/0/all/0/1\">Ilya Burenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malkin_E/0/1/0/all/0/1\">Elian Malkin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nazarov_I/0/1/0/all/0/1\">Ivan Nazarov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kokh_V/0/1/0/all/0/1\">Vladimir Kokh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Avetisian_M/0/1/0/all/0/1\">Manvel Avetisian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhukov_L/0/1/0/all/0/1\">Leonid Zhukov</a>",
          "description": "The COVID-19 pandemic created a significant interest and demand for infection\ndetection and monitoring solutions. In this paper we propose a machine learning\nmethod to quickly triage COVID-19 using recordings made on consumer devices.\nThe approach combines signal processing methods with fine-tuned deep learning\nnetworks and provides methods for signal denoising, cough detection and\nclassification. We have also developed and deployed a mobile application that\nuses symptoms checker together with voice, breath and cough signals to detect\nCOVID-19 infection. The application showed robust performance on both open\nsourced datasets and on the noisy data collected during beta testing by the end\nusers.",
          "link": "http://arxiv.org/abs/2107.10716",
          "publishedOn": "2022-01-12T00:38:46.209Z",
          "wordCount": null,
          "title": "Project Achoo: A Practical Model and Application for COVID-19 Detection from Recordings of Breath, Voice, and Cough. (arXiv:2107.10716v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.01406",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Helou_M/0/1/0/all/0/1\">Majed El Helou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susstrunk_S/0/1/0/all/0/1\">Sabine S&#xfc;sstrunk</a>",
          "description": "Classic image-restoration algorithms use a variety of priors, either\nimplicitly or explicitly. Their priors are hand-designed and their\ncorresponding weights are heuristically assigned. Hence, deep learning methods\noften produce superior image restoration quality. Deep networks are, however,\ncapable of inducing strong and hardly predictable hallucinations. Networks\nimplicitly learn to be jointly faithful to the observed data while learning an\nimage prior; and the separation of original data and hallucinated data\ndownstream is then not possible. This limits their wide-spread adoption in\nimage restoration. Furthermore, it is often the hallucinated part that is\nvictim to degradation-model overfitting.\n\nWe present an approach with decoupled network-prior based hallucination and\ndata fidelity terms. We refer to our framework as the Bayesian Integration of a\nGenerative Prior (BIGPrior). Our method is rooted in a Bayesian framework and\ntightly connected to classic restoration methods. In fact, it can be viewed as\na generalization of a large family of classic restoration algorithms. We use\nnetwork inversion to extract image prior information from a generative network.\nWe show that, on image colorization, inpainting and denoising, our framework\nconsistently improves the inversion results. Our method, though partly reliant\non the quality of the generative network inversion, is competitive with\nstate-of-the-art supervised and task-specific restoration methods. It also\nprovides an additional metric that sets forth the degree of prior reliance per\npixel relative to data fidelity.",
          "link": "http://arxiv.org/abs/2011.01406",
          "publishedOn": "2022-01-12T00:38:46.203Z",
          "wordCount": null,
          "title": "BIGPrior: Towards Decoupling Learned Prior Hallucination and Data Fidelity in Image Restoration. (arXiv:2011.01406v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.05673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heaton_J/0/1/0/all/0/1\">Jeff Heaton</a>",
          "description": "Deep learning is a group of exciting new technologies for neural networks.\nThrough a combination of advanced training techniques and neural network\narchitectural components, it is now possible to create neural networks that can\nhandle tabular data, images, text, and audio as both input and output. Deep\nlearning allows a neural network to learn hierarchies of information in a way\nthat is like the function of the human brain. This course will introduce the\nstudent to classic neural network structures, Convolution Neural Networks\n(CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU),\nGeneral Adversarial Networks (GAN), and reinforcement learning. Application of\nthese architectures to computer vision, time series, security, natural language\nprocessing (NLP), and data generation will be covered. High-Performance\nComputing (HPC) aspects will demonstrate how deep learning can be leveraged\nboth on graphical processing units (GPUs), as well as grids. Focus is primarily\nupon the application of deep learning to problems, with some introduction to\nmathematical foundations. Readers will use the Python programming language to\nimplement deep learning using Google TensorFlow and Keras. It is not necessary\nto know Python prior to this book; however, familiarity with at least one\nprogramming language is assumed.",
          "link": "http://arxiv.org/abs/2009.05673",
          "publishedOn": "2022-01-12T00:38:46.201Z",
          "wordCount": null,
          "title": "Applications of Deep Neural Networks. (arXiv:2009.05673v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03243",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ajaz_A/0/1/0/all/0/1\">Aleena Ajaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salar_A/0/1/0/all/0/1\">Ayesha Salar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamal_T/0/1/0/all/0/1\">Tauseef Jamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Asif Ullah Khan</a>",
          "description": "Now a days, UAVs such as drones are greatly used for various purposes like\nthat of capturing and target detection from ariel imagery etc. Easy access of\nthese small ariel vehicles to public can cause serious security threats. For\ninstance, critical places may be monitored by spies blended in public using\ndrones. Study in hand proposes an improved and efficient Deep Learning based\nautonomous system which can detect and track very small drones with great\nprecision. The proposed system consists of a custom deep learning model Tiny\nYOLOv3, one of the flavors of very fast object detection model You Look Only\nOnce (YOLO) is built and used for detection. The object detection algorithm\nwill efficiently the detect the drones. The proposed architecture has shown\nsignificantly better performance as compared to the previous YOLO version. The\nimprovement is observed in the terms of resource usage and time complexity. The\nperformance is measured using the metrics of recall and precision that are 93%\nand 91% respectively.",
          "link": "http://arxiv.org/abs/2201.03243",
          "publishedOn": "2022-01-12T00:38:46.197Z",
          "wordCount": null,
          "title": "Small Object Detection using Deep Learning. (arXiv:2201.03243v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11397",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Ji_W/0/1/0/all/0/1\">Weiqi Ji</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Richter_F/0/1/0/all/0/1\">Franz Richter</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gollner_M/0/1/0/all/0/1\">Michael J. Gollner</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Deng_S/0/1/0/all/0/1\">Sili Deng</a>",
          "description": "Modeling the burning processes of biomass such as wood, grass, and crops is\ncrucial for the modeling and prediction of wildland and urban fire behavior.\nDespite its importance, the burning of solid fuels remains poorly understood,\nwhich can be partly attributed to the unknown chemical kinetics of most solid\nfuels. Most available kinetic models were built upon expert knowledge, which\nrequires chemical insights and years of experience. This work presents a\nframework for autonomously discovering biomass pyrolysis kinetic models from\nthermogravimetric analyzer (TGA) experimental data using the recently developed\nchemical reaction neural networks (CRNN). The approach incorporated the CRNN\nmodel into the framework of neural ordinary differential equations to predict\nthe residual mass in TGA data. In addition to the flexibility of\nneural-network-based models, the learned CRNN model is interpretable, by\nincorporating the fundamental physics laws, such as the law of mass action and\nArrhenius law, into the neural network structure. The learned CRNN model can\nthen be translated into the classical forms of biomass chemical kinetic models,\nwhich facilitates the extraction of chemical insights and the integration of\nthe kinetic model into large-scale fire simulations. We demonstrated the\neffectiveness of the framework in predicting the pyrolysis and oxidation of\ncellulose. This successful demonstration opens the possibility of rapid and\nautonomous chemical kinetic modeling of solid fuels, such as wildfire fuels and\nindustrial polymers.",
          "link": "http://arxiv.org/abs/2105.11397",
          "publishedOn": "2022-01-12T00:38:46.197Z",
          "wordCount": null,
          "title": "Autonomous Kinetic Modeling of Biomass Pyrolysis using Chemical Reaction Neural Networks. (arXiv:2105.11397v2 [physics.chem-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.09382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_T/0/1/0/all/0/1\">Tomoki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yen-Ju Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>",
          "description": "Deep learning based models have significantly improved the performance of\nspeech separation with input mixtures like the cocktail party. Prominent\nmethods (e.g., frequency-domain and time-domain speech separation) usually\nbuild regression models to predict the ground-truth speech from the mixture,\nusing the masking-based design and the signal-level loss criterion (e.g., MSE\nor SI-SNR). This study demonstrates, for the first time, that the\nsynthesis-based approach can also perform well on this problem, with great\nflexibility and strong potential. Specifically, we propose a novel speech\nseparation/enhancement model based on the recognition of discrete symbols, and\nconvert the paradigm of the speech separation/enhancement related tasks from\nregression to classification. By utilizing the synthesis model with the input\nof discrete symbols, after the prediction of discrete symbol sequence, each\ntarget speech could be re-synthesized. Evaluation results based on the\nWSJ0-2mix and VCTK-noisy corpora in various settings show that our proposed\nmethod can steadily synthesize the separated speech with high speech quality\nand without any interference, which is difficult to avoid in regression-based\nmethods. In addition, with negligible loss of listening quality, the speaker\nconversion of enhanced/separated speech could be easily realized through our\nmethod.",
          "link": "http://arxiv.org/abs/2112.09382",
          "publishedOn": "2022-01-12T00:38:46.197Z",
          "wordCount": null,
          "title": "Discretization and Re-synthesis: an alternative method to solve the Cocktail Party Problem. (arXiv:2112.09382v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bentsen_L/0/1/0/all/0/1\">Lars &#xd8;degaard Bentsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warakagoda_N/0/1/0/all/0/1\">Narada Dilp Warakagoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenbro_R/0/1/0/all/0/1\">Roy Stenbro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelstad_P/0/1/0/all/0/1\">Paal Engelstad</a>",
          "description": "With the increased penetration of wind energy into the power grid, it has\nbecome increasingly important to be able to predict the expected power\nproduction for larger wind farms. Deep learning (DL) models can learn complex\npatterns in the data and have found wide success in predicting wake losses and\nexpected power production. This paper proposes a modular framework for\nattention-based graph neural networks (GNN), where attention can be applied to\nany desired component of a graph block. The results show that the model\nsignificantly outperforms a multilayer perceptron (MLP) and a bidirectional\nLSTM (BLSTM) model, while delivering performance on-par with a vanilla GNN\nmodel. Moreover, we argue that the proposed graph attention architecture can\neasily adapt to different applications by offering flexibility into the desired\nattention operations to be used, which might depend on the specific\napplication. Through analysis of the attention weights, it was showed that\nemploying attention-based GNNs can provide insights into what the models learn.\nIn particular, the attention networks seemed to realise turbine dependencies\nthat aligned with some physical intuition about wake losses.",
          "link": "http://arxiv.org/abs/2201.03229",
          "publishedOn": "2022-01-12T00:38:46.195Z",
          "wordCount": null,
          "title": "Wind Park Power Prediction: Attention-Based Graph Networks and Deep Learning to Capture Wake Losses. (arXiv:2201.03229v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2111.13834",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Repetto_M/0/1/0/all/0/1\">Marco Repetto</a>, <a href=\"http://arxiv.org/find/math/1/au:+Torre_D/0/1/0/all/0/1\">Davide La Torre</a>, <a href=\"http://arxiv.org/find/math/1/au:+Tariq_M/0/1/0/all/0/1\">Muhammad Tariq</a>",
          "description": "Large-scale data analysis is growing at an exponential rate as data\nproliferates in our societies. This abundance of data has the advantage of\nallowing the decision-maker to implement complex models in scenarios that were\nprohibitive before. At the same time, such an amount of data requires a\ndistributed thinking approach. In fact, Deep Learning models require plenty of\nresources, and distributed training is needed. This paper presents a\nMulticriteria approach for distributed learning. Our approach uses the Weighted\nGoal Programming approach in its Chebyshev formulation to build an ensemble of\ndecision rules that optimize aprioristically defined performance metrics. Such\na formulation is beneficial because it is both model and metric agnostic and\nprovides an interpretable output for the decision-maker. We test our approach\nby showing a practical application in electricity demand forecasting. Our\nresults suggest that when we allow for dataset split overlapping, the\nperformances of our methodology are consistently above the baseline model\ntrained on the whole dataset.",
          "link": "http://arxiv.org/abs/2111.13834",
          "publishedOn": "2022-01-12T00:38:46.194Z",
          "wordCount": null,
          "title": "Federated Deep Learning in Electricity Forecasting: An MCDM Approach. (arXiv:2111.13834v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.01575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poole_B/0/1/0/all/0/1\">Benjamin Poole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minwoo Lee</a>",
          "description": "Reinforcement learning (RL) and brain-computer interfaces (BCI) are two\nfields that have been growing over the past decade. Until recently, these\nfields have operated independently of one another. With the rising interest in\nhuman-in-the-loop (HITL) applications, RL algorithms have been adapted to\naccount for human guidance giving rise to the sub-field of interactive\nreinforcement learning (IRL). Adjacently, BCI applications have been long\ninterested in extracting intrinsic feedback from neural activity during\nhuman-computer interactions. These two ideas have set RL and BCI on a collision\ncourse for one another through the integration of BCI into the IRL framework\nwhere intrinsic feedback can be utilized to help train an agent. This\nintersection has created a new and emerging paradigm denoted as intrinsic IRL.\nTo further help facilitate deeper ingratiation of BCI and IRL, we provide a\ntutorial and review of intrinsic IRL so far with an emphasis on its parent\nfield of feedback-driven IRL along with discussions concerning validity,\nchallenges, and open problems.",
          "link": "http://arxiv.org/abs/2112.01575",
          "publishedOn": "2022-01-12T00:38:46.185Z",
          "wordCount": null,
          "title": "Towards Intrinsic Interactive Reinforcement Learning. (arXiv:2112.01575v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02980",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shifat_E_Rabbi_M/0/1/0/all/0/1\">Mohammad Shifat-E-Rabbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubaiyat_A/0/1/0/all/0/1\">Abu Hasnat Mohammad Rubaiyat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xuwang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohde_G/0/1/0/all/0/1\">Gustavo K. Rohde</a>",
          "description": "Deep convolutional neural networks (CNNs) are broadly considered to be\nstate-of-the-art generic end-to-end image classification systems. However, they\nare known to underperform when training data are limited and thus require data\naugmentation strategies that render the method computationally expensive and\nnot always effective. Rather than using a data augmentation strategy to encode\ninvariances as typically done in machine learning, here we propose to\nmathematically augment a nearest subspace classification model in\nsliced-Wasserstein space by exploiting certain mathematical properties of the\nRadon Cumulative Distribution Transform (R-CDT), a recently introduced image\ntransform. We demonstrate that for a particular type of learning problem, our\nmathematical solution has advantages over data augmentation with deep CNNs in\nterms of classification accuracy and computational complexity, and is\nparticularly effective under a limited training data setting. The method is\nsimple, effective, computationally efficient, non-iterative, and requires no\nparameters to be tuned. Python code implementing our method is available at\nhttps://github.com/rohdelab/mathematical_augmentation. Our method is integrated\nas a part of the software package PyTransKit, which is available at\nhttps://github.com/rohdelab/PyTransKit.",
          "link": "http://arxiv.org/abs/2201.02980",
          "publishedOn": "2022-01-12T00:38:46.178Z",
          "wordCount": null,
          "title": "Invariance encoding in sliced-Wasserstein space for image classification with limited training data. (arXiv:2201.02980v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03028",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daneshi_M/0/1/0/all/0/1\">Maryam Daneshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fard_R/0/1/0/all/0/1\">Reza Taghavi Fard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zomorodian_Z/0/1/0/all/0/1\">Zahra Sadat Zomorodian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tahsildoost_M/0/1/0/all/0/1\">Mohammad Tahsildoost</a>",
          "description": "Solar shading design should be done for the desired Indoor Environmental\nQuality (IEQ) in the early design stages. This field can be very challenging\nand time-consuming also requires experts, sophisticated software, and a large\namount of money. The primary purpose of this research is to design a simple\ntool to study various models of solar shadings and make decisions easier and\nfaster in the early stages. Database generation methods, artificial\nintelligence, and optimization have been used to achieve this goal. This tool\nincludes two main parts of 1. predicting the performance of the user-selected\nmodel along with proposing effective parameters and 2. proposing optimal\npre-prepared models to the user. In this regard, initially, a side-lit shoebox\nmodel with variable parameters was modeled parametrically, and five common\nsolar shading models with their variables were applied to the space. For each\nsolar shadings and the state without shading, metrics related to daylight and\nglare, view, and initial costs were simulated. The database generated in this\nresearch includes 87912 alternatives and six calculated metrics introduced to\noptimized machine learning models, including neural network, random Forrest,\nsupport vector regression, and k nearest neighbor. According to the results,\nthe most accurate and fastest estimation model was Random Forrest, with an\nr2_score of 0.967 to 1. Then, sensitivity analysis was performed to identify\nthe most influential parameters for each shading model and the state without\nit. This analysis distinguished the most effective parameters, including window\norientation, WWR, room width, length, and shading depth. Finally, by optimizing\nthe estimation function of machine learning models with the NSGA II algorithm,\nabout 7300 optimal models were identified. The developed tool can evaluate\nvarious design alternatives in less than a few seconds for each.",
          "link": "http://arxiv.org/abs/2201.03028",
          "publishedOn": "2022-01-12T00:38:46.165Z",
          "wordCount": null,
          "title": "Development of a hybrid machine-learning and optimization tool for performance-based solar shading design. (arXiv:2201.03028v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.00054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Samarth Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phoo_C/0/1/0/all/0/1\">Cheng Perng Phoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun-Fu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1\">Leonid Karlinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saligrama_V/0/1/0/all/0/1\">Venkatesh Saligrama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio S. Feris</a>",
          "description": "Pre-training models on Imagenet or other massive datasets of real images has\nled to major advances in computer vision, albeit accompanied with shortcomings\nrelated to curation cost, privacy, usage rights, and ethical issues. In this\npaper, for the first time, we study the transferability of pre-trained models\nbased on synthetic data generated by graphics simulators to downstream tasks\nfrom very different domains. In using such synthetic data for pre-training, we\nfind that downstream performance on different tasks are favored by different\nconfigurations of simulation parameters (e.g. lighting, object pose,\nbackgrounds, etc.), and that there is no one-size-fits-all solution. It is thus\nbetter to tailor synthetic pre-training data to a specific downstream task, for\nbest performance. We introduce Task2Sim, a unified model mapping downstream\ntask representations to optimal simulation parameters to generate synthetic\npre-training data for them. Task2Sim learns this mapping by training to find\nthe set of best parameters on a set of \"seen\" tasks. Once trained, it can then\nbe used to predict best simulation parameters for novel \"unseen\" tasks in one\nshot, without requiring additional training. Given a budget in number of images\nper class, our extensive experiments with 20 diverse downstream tasks show\nTask2Sim's task-adaptive pre-training data results in significantly better\ndownstream performance than non-adaptively choosing simulation parameters on\nboth seen and unseen tasks. It is even competitive with pre-training on real\nimages from Imagenet.",
          "link": "http://arxiv.org/abs/2112.00054",
          "publishedOn": "2022-01-12T00:38:46.165Z",
          "wordCount": null,
          "title": "Task2Sim : Towards Effective Pre-training and Transfer from Synthetic Data. (arXiv:2112.00054v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03050",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Oda_M/0/1/0/all/0/1\">Masahiro Oda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hayashi_Y/0/1/0/all/0/1\">Yuichiro Hayashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Otake_Y/0/1/0/all/0/1\">Yoshito Otake</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hashimoto_M/0/1/0/all/0/1\">Masahiro Hashimoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akashi_T/0/1/0/all/0/1\">Toshiaki Akashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mori_K/0/1/0/all/0/1\">Kensaku Mori</a>",
          "description": "This paper proposes an automated segmentation method of infection and normal\nregions in the lung from CT volumes of COVID-19 patients. From December 2019,\nnovel coronavirus disease 2019 (COVID-19) spreads over the world and giving\nsignificant impacts to our economic activities and daily lives. To diagnose the\nlarge number of infected patients, diagnosis assistance by computers is needed.\nChest CT is effective for diagnosis of viral pneumonia including COVID-19. A\nquantitative analysis method of condition of the lung from CT volumes by\ncomputers is required for diagnosis assistance of COVID-19. This paper proposes\nan automated segmentation method of infection and normal regions in the lung\nfrom CT volumes using a COVID-19 segmentation fully convolutional network\n(FCN). In diagnosis of lung diseases including COVID-19, analysis of conditions\nof normal and infection regions in the lung is important. Our method recognizes\nand segments lung normal and infection regions in CT volumes. To segment\ninfection regions that have various shapes and sizes, we introduced dense\npooling connections and dilated convolutions in our FCN. We applied the\nproposed method to CT volumes of COVID-19 cases. From mild to severe cases of\nCOVID-19, the proposed method correctly segmented normal and infection regions\nin the lung. Dice scores of normal and infection regions were 0.911 and 0.753,\nrespectively.",
          "link": "http://arxiv.org/abs/2201.03050",
          "publishedOn": "2022-01-12T00:38:46.142Z",
          "wordCount": null,
          "title": "Lung infection and normal region segmentation from CT volumes of COVID-19 cases. (arXiv:2201.03050v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14624",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grohe_M/0/1/0/all/0/1\">Martin Grohe</a>",
          "description": "Graph neural networks (GNNs) are deep learning architectures for machine\nlearning problems on graphs. It has recently been shown that the expressiveness\nof GNNs can be characterised precisely by the combinatorial Weisfeiler-Leman\nalgorithms and by finite variable counting logics. The correspondence has even\nled to new, higher-order GNNs corresponding to the WL algorithm in higher\ndimensions.\n\nThe purpose of this paper is to explain these descriptive characterisations\nof GNNs.",
          "link": "http://arxiv.org/abs/2104.14624",
          "publishedOn": "2022-01-12T00:38:46.113Z",
          "wordCount": null,
          "title": "The Logic of Graph Neural Networks. (arXiv:2104.14624v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yangyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_X/0/1/0/all/0/1\">Xiaoye Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenjia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianwei Yin</a>",
          "description": "Data imputation has been extensively explored to solve the missing data\nproblem. The dramatically increasing volume of incomplete data makes the\nimputation models computationally infeasible in many real-life applications. In\nthis paper, we propose an effective scalable imputation system named SCIS to\nsignificantly speed up the training of the differentiable generative\nadversarial imputation models under accuracy-guarantees for large-scale\nincomplete data. SCIS consists of two modules, differentiable imputation\nmodeling (DIM) and sample size estimation (SSE). DIM leverages a new masking\nSinkhorn divergence function to make an arbitrary generative adversarial\nimputation model differentiable, while for such a differentiable imputation\nmodel, SSE can estimate an appropriate sample size to ensure the user-specified\nimputation accuracy of the final model. Extensive experiments upon several\nreal-life large-scale datasets demonstrate that, our proposed system can\naccelerate the generative adversarial model training by 7.1x. Using around 7.6%\nsamples, SCIS yields competitive accuracy with the state-of-the-art imputation\nmethods in a much shorter computation time.",
          "link": "http://arxiv.org/abs/2201.03202",
          "publishedOn": "2022-01-12T00:38:46.102Z",
          "wordCount": null,
          "title": "Differentiable and Scalable Generative Adversarial Models for Data Imputation. (arXiv:2201.03202v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03263",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gerber_P/0/1/0/all/0/1\">Pascal Gerber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jockel_L/0/1/0/all/0/1\">Lisa J&#xf6;ckel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klas_M/0/1/0/all/0/1\">Michael Kl&#xe4;s</a>",
          "description": "Outcomes of data-driven AI models cannot be assumed to be always correct. To\nestimate the uncertainty in these outcomes, the uncertainty wrapper framework\nhas been proposed, which considers uncertainties related to model fit, input\nquality, and scope compliance. Uncertainty wrappers use a decision tree\napproach to cluster input quality related uncertainties, assigning inputs\nstrictly to distinct uncertainty clusters. Hence, a slight variation in only\none feature may lead to a cluster assignment with a significantly different\nuncertainty. Our objective is to replace this with an approach that mitigates\nhard decision boundaries of these assignments while preserving\ninterpretability, runtime complexity, and prediction performance. Five\napproaches were selected as candidates and integrated into the uncertainty\nwrapper framework. For the evaluation based on the Brier score, datasets for a\npedestrian detection use case were generated using the CARLA simulator and\nYOLOv3. All integrated approaches achieved a softening, i.e., smoothing, of\nuncertainty estimation. Yet, compared to decision trees, they are not so easy\nto interpret and have higher runtime complexity. Moreover, some components of\nthe Brier score impaired while others improved. Most promising regarding the\nBrier score were random forests. In conclusion, softening hard decision tree\nboundaries appears to be a trade-off decision.",
          "link": "http://arxiv.org/abs/2201.03263",
          "publishedOn": "2022-01-12T00:38:46.101Z",
          "wordCount": null,
          "title": "A Study on Mitigating Hard Boundaries of Decision-Tree-based Uncertainty Estimates for AI Models. (arXiv:2201.03263v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2110.13511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Egele_R/0/1/0/all/0/1\">Romain Egele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maulik_R/0/1/0/all/0/1\">Romit Maulik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_K/0/1/0/all/0/1\">Krishnan Raghavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lusch_B/0/1/0/all/0/1\">Bethany Lusch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guyon_I/0/1/0/all/0/1\">Isabelle Guyon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaprakash_P/0/1/0/all/0/1\">Prasanna Balaprakash</a>",
          "description": "Deep neural networks are powerful predictors for a variety of tasks. However,\nthey do not capture uncertainty directly. Using neural network ensembles to\nquantify uncertainty is competitive with approaches based on Bayesian neural\nnetworks while benefiting from better computational scalability. However,\nbuilding ensembles of neural networks is a challenging task because, in\naddition to choosing the right neural architecture or hyperparameters for each\nmember of the ensemble, there is an added cost of training each model. We\npropose AutoDEUQ, an automated approach for generating an ensemble of deep\nneural networks. Our approach leverages joint neural architecture and\nhyperparameter search to generate ensembles. We use the law of total variance\nto decompose the predictive variance of deep ensembles into aleatoric (data)\nand epistemic (model) uncertainties. We show that AutoDEUQ outperforms\nprobabilistic backpropagation, Monte Carlo dropout, deep ensemble,\ndistribution-free ensembles, and hyper ensemble methods on a number of\nregression benchmarks.",
          "link": "http://arxiv.org/abs/2110.13511",
          "publishedOn": "2022-01-12T00:38:46.095Z",
          "wordCount": null,
          "title": "AutoDEUQ: Automated Deep Ensemble with Uncertainty Quantification. (arXiv:2110.13511v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03182",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Xu_L/0/1/0/all/0/1\">Lihu Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yao_F/0/1/0/all/0/1\">Fang Yao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yao_Q/0/1/0/all/0/1\">Qiuran Yao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1\">Huiming Zhang</a>",
          "description": "There has been a surge of interest in developing robust estimators for models\nwith heavy-tailed data in statistics and machine learning. This paper proposes\na log-truncated M-estimator for a large family of statistical regressions and\nestablishes its excess risk bound under the condition that the data have\n$(1+\\varepsilon)$-th moment with $\\varepsilon \\in (0,1]$. With an additional\nassumption on the associated risk function, we obtain an $\\ell_2$-error bound\nfor the estimation. Our theorems are applied to establish robust M-estimators\nfor concrete regressions. Besides convex regressions such as quantile\nregression and generalized linear models, many non-convex regressions can also\nbe fit into our theorems, we focus on robust deep neural network regressions,\nwhich can be solved by the stochastic gradient descent algorithms. Simulations\nand real data analysis demonstrate the superiority of log-truncated estimations\nover standard estimations.",
          "link": "http://arxiv.org/abs/2201.03182",
          "publishedOn": "2022-01-12T00:38:46.086Z",
          "wordCount": null,
          "title": "Non-Asymptotic Guarantees for Robust Statistical Learning under $(1+\\varepsilon)$-th Moment Assumption. (arXiv:2201.03182v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03128",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Morais_M/0/1/0/all/0/1\">Michael J. Morais</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pillow_J/0/1/0/all/0/1\">Jonathan W. Pillow</a>",
          "description": "Approximate Bayesian inference methods provide a powerful suite of tools for\nfinding approximations to intractable posterior distributions. However, machine\nlearning applications typically involve selecting actions, which -- in a\nBayesian setting -- depend on the posterior distribution only via its\ncontribution to expected utility. A growing body of work on loss-calibrated\napproximate inference methods has therefore sought to develop posterior\napproximations sensitive to the influence of the utility function. Here we\nintroduce loss-calibrated expectation propagation (Loss-EP), a loss-calibrated\nvariant of expectation propagation. This method resembles standard EP with an\nadditional factor that \"tilts\" the posterior towards higher-utility decisions.\nWe show applications to Gaussian process classification under binary utility\nfunctions with asymmetric penalties on False Negative and False Positive\nerrors, and show how this asymmetry can have dramatic consequences on what\ninformation is \"useful\" to capture in an approximation.",
          "link": "http://arxiv.org/abs/2201.03128",
          "publishedOn": "2022-01-12T00:38:46.085Z",
          "wordCount": null,
          "title": "Loss-calibrated expectation propagation for approximate Bayesian decision-making. (arXiv:2201.03128v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2111.02840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chejian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Large-scale pre-trained language models have achieved tremendous success\nacross a wide range of natural language understanding (NLU) tasks, even\nsurpassing human performance. However, recent studies reveal that the\nrobustness of these models can be challenged by carefully crafted textual\nadversarial examples. While several individual datasets have been proposed to\nevaluate model robustness, a principled and comprehensive benchmark is still\nmissing. In this paper, we present Adversarial GLUE (AdvGLUE), a new multi-task\nbenchmark to quantitatively and thoroughly explore and evaluate the\nvulnerabilities of modern large-scale language models under various types of\nadversarial attacks. In particular, we systematically apply 14 textual\nadversarial attack methods to GLUE tasks to construct AdvGLUE, which is further\nvalidated by humans for reliable annotations. Our findings are summarized as\nfollows. (i) Most existing adversarial attack algorithms are prone to\ngenerating invalid or ambiguous adversarial examples, with around 90% of them\neither changing the original semantic meanings or misleading human annotators\nas well. Therefore, we perform a careful filtering process to curate a\nhigh-quality benchmark. (ii) All the language models and robust training\nmethods we tested perform poorly on AdvGLUE, with scores lagging far behind the\nbenign accuracy. We hope our work will motivate the development of new\nadversarial attacks that are more stealthy and semantic-preserving, as well as\nnew robust language models against sophisticated adversarial attacks. AdvGLUE\nis available at https://adversarialglue.github.io.",
          "link": "http://arxiv.org/abs/2111.02840",
          "publishedOn": "2022-01-12T00:38:46.074Z",
          "wordCount": null,
          "title": "Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models. (arXiv:2111.02840v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazzia_V/0/1/0/all/0/1\">Vittorio Mazzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angarano_S/0/1/0/all/0/1\">Simone Angarano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvetti_F/0/1/0/all/0/1\">Francesco Salvetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelini_F/0/1/0/all/0/1\">Federico Angelini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiaberge_M/0/1/0/all/0/1\">Marcello Chiaberge</a>",
          "description": "Deep neural networks based purely on attention have been successful across\nseveral domains, relying on minimal architectural priors from the designer. In\nHuman Action Recognition (HAR), attention mechanisms have been primarily\nadopted on top of standard convolutional or recurrent layers, improving the\noverall generalization capability. In this work, we introduce Action\nTransformer (AcT), a simple, fully self-attentional architecture that\nconsistently outperforms more elaborated networks that mix convolutional,\nrecurrent and attentive layers. In order to limit computational and energy\nrequests, building on previous human action recognition research, the proposed\napproach exploits 2D pose representations over small temporal windows,\nproviding a low latency solution for accurate and effective real-time\nperformance. Moreover, we open-source MPOSE2021, a new large-scale dataset, as\nan attempt to build a formal training and evaluation benchmark for real-time,\nshort-time HAR. The proposed methodology was extensively tested on MPOSE2021\nand compared to several state-of-the-art architectures, proving the\neffectiveness of the AcT model and laying the foundations for future work on\nHAR.",
          "link": "http://arxiv.org/abs/2107.00606",
          "publishedOn": "2022-01-12T00:38:46.073Z",
          "wordCount": null,
          "title": "Action Transformer: A Self-Attention Model for Short-Time Pose-Based Human Action Recognition. (arXiv:2107.00606v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.09046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhenan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Huang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zirui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedlander_M/0/1/0/all/0/1\">Michael P. Friedlander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Changxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>",
          "description": "Federated learning is an emerging decentralized machine learning scheme that\nallows multiple data owners to work collaboratively while ensuring data\nprivacy. The success of federated learning depends largely on the participation\nof data owners. To sustain and encourage data owners' participation, it is\ncrucial to fairly evaluate the quality of the data provided by the data owners\nand reward them correspondingly. Federated Shapley value, recently proposed by\nWang et al. [Federated Learning, 2020], is a measure for data value under the\nframework of federated learning that satisfies many desired properties for data\nvaluation. However, there are still factors of potential unfairness in the\ndesign of federated Shapley value because two data owners with the same local\ndata may not receive the same evaluation. We propose a new measure called\ncompleted federated Shapley value to improve the fairness of federated Shapley\nvalue. The design depends on completing a matrix consisting of all the possible\ncontributions by different subsets of the data owners. It is shown under mild\nconditions that this matrix is approximately low-rank by leveraging concepts\nand tools from optimization. Both theoretical analysis and empirical evaluation\nverify that the proposed measure does improve fairness in many circumstances.",
          "link": "http://arxiv.org/abs/2109.09046",
          "publishedOn": "2022-01-12T00:38:46.057Z",
          "wordCount": null,
          "title": "Improving Fairness for Data Valuation in Federated Learning. (arXiv:2109.09046v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_J/0/1/0/all/0/1\">Jash Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bafna_S/0/1/0/all/0/1\">Sanket Bafna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagaria_D/0/1/0/all/0/1\">Devansh Bagaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virnodkar_S/0/1/0/all/0/1\">Shyamal Virnodkar</a>",
          "description": "Face Recognition has proven to be one of the most successful technology and\nhas impacted heterogeneous domains. Deep learning has proven to be the most\nsuccessful at computer vision tasks because of its convolution-based\narchitecture. Since the advent of deep learning, face recognition technology\nhas had a substantial increase in its accuracy. In this paper, some of the most\nimpactful face recognition systems were surveyed. Firstly, the paper gives an\noverview of a general face recognition system. Secondly, the survey covers\nvarious network architectures and training losses that have had a substantial\nimpact. Finally, the paper talks about various databases that are used to\nevaluate the capabilities of a face recognition system.",
          "link": "http://arxiv.org/abs/2201.02991",
          "publishedOn": "2022-01-12T00:38:46.046Z",
          "wordCount": null,
          "title": "A Survey on Face Recognition Systems. (arXiv:2201.02991v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2110.13972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goldbraikh_A/0/1/0/all/0/1\">Adam Goldbraikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAngelo_A/0/1/0/all/0/1\">Anne-Lise D&#x27;Angelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pugh_C/0/1/0/all/0/1\">Carla M. Pugh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laufer_S/0/1/0/all/0/1\">Shlomi Laufer</a>",
          "description": "The goal of this study was to develop new reliable open surgery suturing\nsimulation system for training medical students in situation where resources\nare limited or in the domestic setup. Namely, we developed an algorithm for\ntools and hands localization as well as identifying the interactions between\nthem based on simple webcam video data, calculating motion metrics for\nassessment of surgical skill. Twenty-five participants performed multiple\nsuturing tasks using our simulator. The YOLO network has been modified to a\nmulti-task network, for the purpose of tool localization and tool-hand\ninteraction detection. This was accomplished by splitting the YOLO detection\nheads so that they supported both tasks with minimal addition to computer\nrun-time. Furthermore, based on the outcome of the system, motion metrics were\ncalculated. These metrics included traditional metrics such as time and path\nlength as well as new metrics assessing the technique participants use for\nholding the tools. The dual-task network performance was similar to that of two\nnetworks, while computational load was only slightly bigger than one network.\nIn addition, the motion metrics showed significant differences between experts\nand novices. While video capture is an essential part of minimally invasive\nsurgery, it is not an integral component of open surgery. Thus, new algorithms,\nfocusing on the unique challenges open surgery videos present, are required. In\nthis study, a dual-task network was developed to solve both a localization task\nand a hand-tool interaction task. The dual network may be easily expanded to a\nmulti-task network, which may be useful for images with multiple layers and for\nevaluating the interaction between these different layers.",
          "link": "http://arxiv.org/abs/2110.13972",
          "publishedOn": "2022-01-12T00:38:46.033Z",
          "wordCount": null,
          "title": "Video-based fully automatic assessment of open surgery suturing skills. (arXiv:2110.13972v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weipeng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kuangen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_C/0/1/0/all/0/1\">Clarence W. de Silva</a>",
          "description": "Recent advances in unsupervised domain adaptation have shown that mitigating\nthe domain divergence by extracting the domain-invariant representation could\nsignificantly improve the generalization of a model to an unlabeled data\ndomain. Nevertheless, the existing methods fail to effectively preserve the\nrepresentation that is private to the label-missing domain, which could\nadversely affect the generalization. In this paper, we propose an approach to\npreserve such representation so that the latent distribution of the unlabeled\ndomain could represent both the domain-invariant features and the individual\ncharacteristics that are private to the unlabeled domain. In particular, we\ndemonstrate that maximizing the mutual information between the unlabeled domain\nand its latent space while mitigating the domain divergence can achieve such\npreservation. We also theoretically and empirically validate that preserving\nthe representation that is private to the unlabeled domain is important and of\nnecessity for the cross-domain generalization. Our approach outperforms\nstate-of-the-art methods on several public datasets.",
          "link": "http://arxiv.org/abs/2201.03102",
          "publishedOn": "2022-01-12T00:38:46.032Z",
          "wordCount": null,
          "title": "Preserving Domain Private Representation via Mutual Information Maximization. (arXiv:2201.03102v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03200",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Shashwat Bhattacharya</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Verma_M/0/1/0/all/0/1\">Mahendra K Verma</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Arnab Bhattacharya</a>",
          "description": "In this paper, we develop a multivariate regression model and a neural\nnetwork model to predict the Reynolds number (Re) and Nusselt number in\nturbulent thermal convection. We compare their predictions with those of\nearlier models of convection: Grossmann-Lohse~[Phys. Rev. Lett. \\textbf{86},\n3316 (2001)], revised Grossmann-Lohse~[Phys. Fluids \\textbf{33}, 015113\n(2021)], and Pandey-Verma [Phys. Rev. E \\textbf{94}, 053106 (2016)] models. We\nobserve that although the predictions of all the models are quite close to each\nother, the machine learning models developed in this work provide the best\nmatch with the experimental and numerical results.",
          "link": "http://arxiv.org/abs/2201.03200",
          "publishedOn": "2022-01-12T00:38:46.029Z",
          "wordCount": null,
          "title": "Predictions of Reynolds and Nusselt numbers in turbulent convection using machine-learning models. (arXiv:2201.03200v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rohanian_O/0/1/0/all/0/1\">Omid Rohanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouchaki_S/0/1/0/all/0/1\">Samaneh Kouchaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltan_A/0/1/0/all/0/1\">Andrew Soltan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jenny Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohanian_M/0/1/0/all/0/1\">Morteza Rohanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1\">David Clifton</a>",
          "description": "Early detection of COVID-19 is an ongoing area of research that can help with\ntriage, monitoring and general health assessment of potential patients and may\nreduce operational strain on hospitals that cope with the coronavirus pandemic.\nDifferent machine learning techniques have been used in the literature to\ndetect coronavirus using routine clinical data (blood tests, and vital signs).\nData breaches and information leakage when using these models can bring\nreputational damage and cause legal issues for hospitals. In spite of this,\nprotecting healthcare models against leakage of potentially sensitive\ninformation is an understudied research area. In this work, we examine two\nmachine learning approaches, intended to predict a patient's COVID-19 status\nusing routinely collected and readily available clinical data. We employ\nadversarial training to explore robust deep learning architectures that protect\nattributes related to demographic information about the patients. The two\nmodels we examine in this work are intended to preserve sensitive information\nagainst adversarial attacks and information leakage. In a series of experiments\nusing datasets from the Oxford University Hospitals, Bedfordshire Hospitals NHS\nFoundation Trust, University Hospitals Birmingham NHS Foundation Trust, and\nPortsmouth Hospitals University NHS Trust we train and test two neural networks\nthat predict PCR test results using information from basic laboratory blood\ntests, and vital signs performed on a patients' arrival to hospital. We assess\nthe level of privacy each one of the models can provide and show the efficacy\nand robustness of our proposed architectures against a comparable baseline. One\nof our main contributions is that we specifically target the development of\neffective COVID-19 detection models with built-in mechanisms in order to\nselectively protect sensitive attributes against adversarial attacks.",
          "link": "http://arxiv.org/abs/2201.03004",
          "publishedOn": "2022-01-12T00:38:46.016Z",
          "wordCount": null,
          "title": "Privacy-aware Early Detection of COVID-19 through Adversarial Training. (arXiv:2201.03004v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.05804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murdock_C/0/1/0/all/0/1\">Calvin Murdock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cazenavette_G/0/1/0/all/0/1\">George Cazenavette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1\">Simon Lucey</a>",
          "description": "In comparison to classical shallow representation learning techniques, deep\nneural networks have achieved superior performance in nearly every application\nbenchmark. But despite their clear empirical advantages, it is still not well\nunderstood what makes them so effective. To approach this question, we\nintroduce deep frame approximation: a unifying framework for constrained\nrepresentation learning with structured overcomplete frames. While exact\ninference requires iterative optimization, it may be approximated by the\noperations of a feed-forward deep neural network. We indirectly analyze how\nmodel capacity relates to frame structures induced by architectural\nhyperparameters such as depth, width, and skip connections. We quantify these\nstructural differences with the deep frame potential, a data-independent\nmeasure of coherence linked to representation uniqueness and stability. As a\ncriterion for model selection, we show correlation with generalization error on\na variety of common deep network architectures and datasets. We also\ndemonstrate how recurrent networks implementing iterative optimization\nalgorithms can achieve performance comparable to their feed-forward\napproximations while improving adversarial robustness. This connection to the\nestablished theory of overcomplete representations suggests promising new\ndirections for principled deep network architecture design with less reliance\non ad-hoc engineering.",
          "link": "http://arxiv.org/abs/2103.05804",
          "publishedOn": "2022-01-12T00:38:46.015Z",
          "wordCount": null,
          "title": "Reframing Neural Networks: Deep Structure in Overcomplete Representations. (arXiv:2103.05804v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.12988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shenao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>",
          "description": "In multi-agent reinforcement learning, the behaviors that agents learn in a\nsingle Markov Game (MG) are typically confined to the given agent number (i.e.,\npopulation size). Every single MG induced by varying population sizes may\npossess distinct optimal joint strategies and game-specific knowledge, which\nare modeled independently in modern multi-agent algorithms. In this work, we\nfocus on creating agents that generalize across population-varying MGs. Instead\nof learning a unimodal policy, each agent learns a policy set that is formed by\neffective strategies across a variety of games. We propose Meta Representations\nfor Agents (MRA) that explicitly models the game-common and game-specific\nstrategic knowledge. By representing the policy sets with multi-modal latent\npolicies, the common strategic knowledge and diverse strategic modes are\ndiscovered with an iterative optimization procedure. We prove that as an\napproximation to a constrained mutual information maximization objective, the\nlearned policies can reach Nash Equilibrium in every evaluation MG under the\nassumption of Lipschitz game on a sufficiently large latent space. When\ndeploying it at practical latent models with limited size, fast adaptation can\nbe achieved by leveraging the first-order gradient information. Extensive\nexperiments show the effectiveness of MRA on both training performance and\ngeneralization ability in hard and unseen games.",
          "link": "http://arxiv.org/abs/2108.12988",
          "publishedOn": "2022-01-12T00:38:46.015Z",
          "wordCount": null,
          "title": "Learning Meta Representations for Agents in Multi-Agent Reinforcement Learning. (arXiv:2108.12988v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Inan_M/0/1/0/all/0/1\">Muhammad Sakib Khan Inan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_I/0/1/0/all/0/1\">Istiakur Rahman</a>",
          "description": "Landslides have been a regular occurrence and an alarming threat to human\nlife and property in the era of anthropogenic global warming. An early\nprediction of landslide susceptibility using a data-driven approach is a demand\nof time. In this study, we explored the eloquent features that best describe\nlandslide susceptibility with state-of-the-art machine learning methods. In our\nstudy, we employed state-of-the-art machine learning algorithms including\nXgBoost, LR, KNN, SVM, Adaboost for landslide susceptibility prediction. To\nfind the best hyperparameters of each individual classifier for optimized\nperformance, we have incorporated the Grid Search method, with 10 Fold\nCross-Validation. In this context, the optimized version of XgBoost\noutperformed all other classifiers with a Cross-validation Weighted F1 score of\n94.62%. Followed by this empirical evidence, we explored the XgBoost classifier\nby incorporating TreeSHAP and identified eloquent features such as SLOPE,\nELEVATION, TWI that complement the performance of the XGBoost classifier mostly\nand features such as LANDUSE, NDVI, SPI which has less effect on models\nperformance. According to the TreeSHAP explanation of features, we selected the\n9 most significant landslide causal factors out of 15. Evidently, an optimized\nversion of XgBoost along with feature reduction by 40%, has outperformed all\nother classifiers in terms of popular evaluation metrics with a\nCross-Validation Weighted F1 score of 95.01% on the training and AUC score of\n97%.",
          "link": "http://arxiv.org/abs/2201.03225",
          "publishedOn": "2022-01-12T00:38:46.001Z",
          "wordCount": null,
          "title": "Integration of Explainable Artificial Intelligence to Identify Significant Landslide Causal Factors for Extreme Gradient Boosting based Landslide Susceptibility Mapping with Improved Feature Selection. (arXiv:2201.03225v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Binici_K/0/1/0/all/0/1\">Kuluhan Binici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_S/0/1/0/all/0/1\">Shivam Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1\">Nam Trung Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leman_K/0/1/0/all/0/1\">Karianto Leman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_T/0/1/0/all/0/1\">Tulika Mitra</a>",
          "description": "Data-Free Knowledge Distillation (KD) allows knowledge transfer from a\ntrained neural network (teacher) to a more compact one (student) in the absence\nof original training data. Existing works use a validation set to monitor the\naccuracy of the student over real data and report the highest performance\nthroughout the entire process. However, validation data may not be available at\ndistillation time either, making it infeasible to record the student snapshot\nthat achieved the peak accuracy. Therefore, a practical data-free KD method\nshould be robust and ideally provide monotonically increasing student accuracy\nduring distillation. This is challenging because the student experiences\nknowledge degradation due to the distribution shift of the synthetic data. A\nstraightforward approach to overcome this issue is to store and rehearse the\ngenerated samples periodically, which increases the memory footprint and\ncreates privacy concerns. We propose to model the distribution of the\npreviously observed synthetic samples with a generative network. In particular,\nwe design a Variational Autoencoder (VAE) with a training objective that is\ncustomized to learn the synthetic data representations optimally. The student\nis rehearsed by the generative pseudo replay technique, with samples produced\nby the VAE. Hence knowledge degradation can be prevented without storing any\nsamples. Experiments on image classification benchmarks show that our method\noptimizes the expected value of the distilled model accuracy while eliminating\nthe large memory overhead incurred by the sample-storing methods.",
          "link": "http://arxiv.org/abs/2201.03019",
          "publishedOn": "2022-01-12T00:38:46.000Z",
          "wordCount": null,
          "title": "Robust and Resource-Efficient Data-Free Knowledge Distillation by Generative Pseudo Replay. (arXiv:2201.03019v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.06940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shand_C/0/1/0/all/0/1\">Cameron Shand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allmendinger_R/0/1/0/all/0/1\">Richard Allmendinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Handl_J/0/1/0/all/0/1\">Julia Handl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webb_A/0/1/0/all/0/1\">Andrew Webb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keane_J/0/1/0/all/0/1\">John Keane</a>",
          "description": "Comprehensive benchmarking of clustering algorithms is rendered difficult by\ntwo key factors: (i)~the elusiveness of a unique mathematical definition of\nthis unsupervised learning approach and (ii)~dependencies between the\ngenerating models or clustering criteria adopted by some clustering algorithms\nand indices for internal cluster validation. Consequently, there is no\nconsensus regarding the best practice for rigorous benchmarking, and whether\nthis is possible at all outside the context of a given application. Here, we\nargue that synthetic datasets must continue to play an important role in the\nevaluation of clustering algorithms, but that this necessitates constructing\nbenchmarks that appropriately cover the diverse set of properties that impact\nclustering algorithm performance. Through our framework, HAWKS, we demonstrate\nthe important role evolutionary algorithms play to support flexible generation\nof such benchmarks, allowing simple modification and extension. We illustrate\ntwo possible uses of our framework: (i)~the evolution of benchmark data\nconsistent with a set of hand-derived properties and (ii)~the generation of\ndatasets that tease out performance differences between a given pair of\nalgorithms. Our work has implications for the design of clustering benchmarks\nthat sufficiently challenge a broad range of algorithms, and for furthering\ninsight into the strengths and weaknesses of specific approaches.",
          "link": "http://arxiv.org/abs/2102.06940",
          "publishedOn": "2022-01-12T00:38:45.999Z",
          "wordCount": null,
          "title": "HAWKS: Evolving Challenging Benchmark Sets for Cluster Analysis. (arXiv:2102.06940v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03211",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Grooby_E/0/1/0/all/0/1\">Ethan Grooby</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sitaula_C/0/1/0/all/0/1\">Chiranjibi Sitaula</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fattahi_D/0/1/0/all/0/1\">Davood Fattahi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sameni_R/0/1/0/all/0/1\">Reza Sameni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_K/0/1/0/all/0/1\">Kenneth Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_L/0/1/0/all/0/1\">Lindsay Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+King_A/0/1/0/all/0/1\">Arrabella King</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramanathan_A/0/1/0/all/0/1\">Ashwin Ramanathan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malhotra_A/0/1/0/all/0/1\">Atul Malhotra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dumont_G/0/1/0/all/0/1\">Guy A. Dumont</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marzbanrad_F/0/1/0/all/0/1\">Faezeh Marzbanrad</a>",
          "description": "Stethoscope-recorded chest sounds provide the opportunity for remote\ncardio-respiratory health monitoring of neonates. However, reliable monitoring\nrequires high-quality heart and lung sounds. This paper presents novel\nNon-negative Matrix Factorisation (NMF) and Non-negative Matrix\nCo-Factorisation (NMCF) methods for neonatal chest sound separation. To assess\nthese methods and compare with existing single-source separation methods, an\nartificial mixture dataset was generated comprising of heart, lung and noise\nsounds. Signal-to-noise ratios were then calculated for these artificial\nmixtures. These methods were also tested on real-world noisy neonatal chest\nsounds and assessed based on vital sign estimation error and a signal quality\nscore of 1-5 developed in our previous works. Additionally, the computational\ncost of all methods was assessed to determine the applicability for real-time\nprocessing. Overall, both the proposed NMF and NMCF methods outperform the next\nbest existing method by 2.7dB to 11.6dB for the artificial dataset and 0.40 to\n1.12 signal quality improvement for the real-world dataset. The median\nprocessing time for the sound separation of a 10s recording was found to be\n28.3s for NMCF and 342ms for NMF. Because of stable and robust performance, we\nbelieve that our proposed methods are useful to denoise neonatal heart and lung\nsound in a real-world environment. Codes for proposed and existing methods can\nbe found at: https://github.com/egrooby-monash/Heart-and-Lung-Sound-Separation.",
          "link": "http://arxiv.org/abs/2201.03211",
          "publishedOn": "2022-01-12T00:38:45.988Z",
          "wordCount": null,
          "title": "Noisy Neonatal Chest Sound Separation for High-Quality Heart and Lung Sounds. (arXiv:2201.03211v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05836",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mavridis_C/0/1/0/all/0/1\">Christos Mavridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baras_J/0/1/0/all/0/1\">John Baras</a>",
          "description": "Inherent in virtually every iterative machine learning algorithm is the\nproblem of hyper-parameter tuning, which includes three major design\nparameters: (a) the complexity of the model, e.g., the number of neurons in a\nneural network, (b) the initial conditions, which heavily affect the behavior\nof the algorithm, and (c) the dissimilarity measure used to quantify its\nperformance. We introduce an online prototype-based learning algorithm that can\nbe viewed as a progressively growing competitive-learning neural network\narchitecture for classification and clustering. The learning rule of the\nproposed approach is formulated as an online gradient-free stochastic\napproximation algorithm that solves a sequence of appropriately defined\noptimization problems, simulating an annealing process. The annealing nature of\nthe algorithm contributes to avoiding poor local minima, offers robustness with\nrespect to the initial conditions, and provides a means to progressively\nincrease the complexity of the learning model, through an intuitive bifurcation\nphenomenon. The proposed approach is interpretable, requires minimal\nhyper-parameter tuning, and allows online control over the\nperformance-complexity trade-off. Finally, we show that Bregman divergences\nappear naturally as a family of dissimilarity measures that play a central role\nin both the performance and the computational complexity of the learning\nalgorithm.",
          "link": "http://arxiv.org/abs/2102.05836",
          "publishedOn": "2022-01-12T00:38:45.981Z",
          "wordCount": null,
          "title": "Online Deterministic Annealing for Classification and Clustering. (arXiv:2102.05836v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03121",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Seonguk Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joon-Young Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bohyung Han</a>",
          "description": "We propose an information-theoretic bias measurement technique through a\ncausal interpretation of spurious correlation, which is effective to identify\nthe feature-level algorithmic bias by taking advantage of conditional mutual\ninformation. Although several bias measurement methods have been proposed and\nwidely investigated to achieve algorithmic fairness in various tasks such as\nface recognition, their accuracy- or logit-based metrics are susceptible to\nleading to trivial prediction score adjustment rather than fundamental bias\nreduction. Hence, we design a novel debiasing framework against the algorithmic\nbias, which incorporates a bias regularization loss derived by the proposed\ninformation-theoretic bias measurement approach. In addition, we present a\nsimple yet effective unsupervised debiasing technique based on stochastic label\nnoise, which does not require the explicit supervision of bias information. The\nproposed bias measurement and debiasing approaches are validated in diverse\nrealistic scenarios through extensive experiments on multiple standard\nbenchmarks.",
          "link": "http://arxiv.org/abs/2201.03121",
          "publishedOn": "2022-01-12T00:38:45.971Z",
          "wordCount": null,
          "title": "Information-Theoretic Bias Reduction via Causal View of Spurious Correlation. (arXiv:2201.03121v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Buffelli_D/0/1/0/all/0/1\">Davide Buffelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandin_F/0/1/0/all/0/1\">Fabio Vandin</a>",
          "description": "Graph Neural Networks (GNNs) have become the state-of-the-art method for many\napplications on graph structured data. GNNs are a framework for graph\nrepresentation learning, where a model learns to generate low dimensional node\nembeddings that encapsulate structural and feature-related information. GNNs\nare usually trained in an end-to-end fashion, leading to highly specialized\nnode embeddings. While this approach achieves great results in the single-task\nsetting, generating node embeddings that can be used to perform multiple tasks\n(with performance comparable to single-task models) is still an open problem.\nWe propose a novel training strategy for graph representation learning, based\non meta-learning, which allows the training of a GNN model capable of producing\nmulti-task node embeddings. Our method avoids the difficulties arising when\nlearning to perform multiple tasks concurrently by, instead, learning to\nquickly (i.e. with a few steps of gradient descent) adapt to multiple tasks\nsingularly. We show that the embeddings produced by a model trained with our\nmethod can be used to perform multiple tasks with comparable or, surprisingly,\neven higher performance than both single-task and multi-task end-to-end models.",
          "link": "http://arxiv.org/abs/2201.03326",
          "publishedOn": "2022-01-12T00:38:45.963Z",
          "wordCount": null,
          "title": "Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach. (arXiv:2201.03326v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.02002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+K_P/0/1/0/all/0/1\">Priyadarshini K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Subhasis Chaudhuri</a>",
          "description": "Discriminative features are crucial for several learning applications, such\nas object detection and classification. Neural networks are extensively used\nfor extracting discriminative features of images and speech signals. However,\nthe lack of large datasets in the haptics domain often limits the applicability\nof such techniques. This paper presents a general framework for the analysis of\nthe discriminative properties of haptic signals. We demonstrate the\neffectiveness of spectral features and a boosted embedding technique in\nenhancing the distinguishability of haptic signals. Experiments indicate our\nframework needs less training data, generalizes well for different predictors,\nand outperforms the related state-of-the-art.",
          "link": "http://arxiv.org/abs/2010.02002",
          "publishedOn": "2022-01-12T00:38:45.941Z",
          "wordCount": null,
          "title": "Enhancing Haptic Distinguishability of Surface Materials with Boosting Technique. (arXiv:2010.02002v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Keller_T/0/1/0/all/0/1\">T. Anderson Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1\">Max Welling</a>",
          "description": "In this work we seek to bridge the concepts of topographic organization and\nequivariance in neural networks. To accomplish this, we introduce the\nTopographic VAE: a novel method for efficiently training deep generative models\nwith topographically organized latent variables. We show that such a model\nindeed learns to organize its activations according to salient characteristics\nsuch as digit class, width, and style on MNIST. Furthermore, through\ntopographic organization over time (i.e. temporal coherence), we demonstrate\nhow predefined latent space transformation operators can be encouraged for\nobserved transformed input sequences -- a primitive form of unsupervised\nlearned equivariance. We demonstrate that this model successfully learns sets\nof approximately equivariant features (i.e. \"capsules\") directly from sequences\nand achieves higher likelihood on correspondingly transforming test sequences.\nEquivariance is verified quantitatively by measuring the approximate\ncommutativity of the inference network and the sequence transformations.\nFinally, we demonstrate approximate equivariance to complex transformations,\nexpanding upon the capabilities of existing group equivariant neural networks.",
          "link": "http://arxiv.org/abs/2109.01394",
          "publishedOn": "2022-01-12T00:38:45.935Z",
          "wordCount": null,
          "title": "Topographic VAEs learn Equivariant Capsules. (arXiv:2109.01394v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.03972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dandi_Y/0/1/0/all/0/1\">Yatin Dandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacot_A/0/1/0/all/0/1\">Arthur Jacot</a>",
          "description": "Spectral analysis is a powerful tool, decomposing any function into simpler\nparts. In machine learning, Mercer's theorem generalizes this idea, providing\nfor any kernel and input distribution a natural basis of functions of\nincreasing frequency. More recently, several works have extended this analysis\nto deep neural networks through the framework of Neural Tangent Kernel. In this\nwork, we analyze the layer-wise spectral bias of Deep Neural Networks and\nrelate it to the contributions of different layers in the reduction of\ngeneralization error for a given target function. We utilize the properties of\nHermite polynomials and Spherical Harmonics to prove that initial layers\nexhibit a larger bias towards high-frequency functions defined on the unit\nsphere. We further provide empirical results validating our theory in high\ndimensional datasets for Deep Neural Networks.",
          "link": "http://arxiv.org/abs/2111.03972",
          "publishedOn": "2022-01-12T00:38:45.935Z",
          "wordCount": null,
          "title": "Understanding Layer-wise Contributions in Deep Neural Networks through Spectral Analysis. (arXiv:2111.03972v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.01592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tu_E/0/1/0/all/0/1\">Enmei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guanghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shangbo Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rachmawati_L/0/1/0/all/0/1\">Lily Rachmawati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guang-Bin Huang</a>",
          "description": "The prosperity of artificial intelligence has aroused intensive interests in\nintelligent/autonomous navigation, in which path prediction is a key\nfunctionality for decision supports, e.g. route planning, collision warning,\nand traffic regulation. For maritime intelligence, Automatic Identification\nSystem (AIS) plays an important role because it recently has been made\ncompulsory for large international commercial vessels and is able to provide\nnearly real-time information of the vessel. Therefore AIS data based vessel\npath prediction is a promising way in future maritime intelligence. However,\nreal-world AIS data collected online are just highly irregular trajectory\nsegments (AIS message sequences) from different types of vessels and\ngeographical regions, with possibly very low data quality. So even there are\nsome works studying how to build a path prediction model using historical AIS\ndata, but still, it is a very challenging problem. In this paper, we propose a\ncomprehensive framework to model massive historical AIS trajectory segments for\naccurate vessel path prediction. Experimental comparisons with existing popular\nmethods are made to validate the proposed approach and results show that our\napproach could outperform the baseline methods by a wide margin.",
          "link": "http://arxiv.org/abs/2001.01592",
          "publishedOn": "2022-01-12T00:38:45.929Z",
          "wordCount": null,
          "title": "Modeling Historical AIS Data For Vessel Path Prediction: A Comprehensive Treatment. (arXiv:2001.01592v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03156",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Xie_H/0/1/0/all/0/1\">Hao Xie</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zhang_L/0/1/0/all/0/1\">Linfeng Zhang</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>",
          "description": "The quasiparticle effective mass $m^\\ast$ of interacting electrons is a\nfundamental quantity in the Fermi liquid theory. However, the precise value of\nthe effective mass of uniform electron gas is still elusive after decades of\nresearch. The newly developed neural canonical transformation approach\narXiv:2105.08644 offers a principled way to extract the effective mass of\nelectron gas by directly calculating the thermal entropy at low temperature.\nThe approach models a variational many-electron density matrix using two\ngenerative neural networks: an autoregressive model for momentum occupation and\na normalizing flow for electron coordinates. Our calculation reveals a\nsuppression of effective mass in the two-dimensional spin-polarized electron\ngas, which is more pronounced than previous reports in the low-density\nstrong-coupling region. This prediction calls for verification in\ntwo-dimensional electron gas experiments.",
          "link": "http://arxiv.org/abs/2201.03156",
          "publishedOn": "2022-01-12T00:38:45.926Z",
          "wordCount": null,
          "title": "$m^\\ast$ of two-dimensional electron gas: a neural canonical transformation study. (arXiv:2201.03156v1 [cond-mat.stat-mech])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiabao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Peng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xuemin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenjie Zhang</a>",
          "description": "With the development of traffic prediction technology, spatiotemporal\nprediction models have attracted more and more attention from academia\ncommunities and industry. However, most existing researches focus on reducing\nmodel's prediction error but ignore the error caused by the uneven distribution\nof spatial events within a region. In this paper, we study a region\npartitioning problem, namely optimal grid size selection problem (OGSS), which\naims to minimize the real error of spatiotemporal prediction models by\nselecting the optimal grid size. In order to solve OGSS, we analyze the upper\nbound of real error of spatiotemporal prediction models and minimize the real\nerror by minimizing its upper bound. Through in-depth analysis, we find that\nthe upper bound of real error will decrease then increase when the number of\nmodel grids increase from 1 to the maximum allowed value. Then, we propose two\nalgorithms, namely Ternary Search and Iterative Method, to automatically find\nthe optimal grid size. Finally, the experiments verify that the error of\nprediction has the same trend as its upper bound, and the change trend of the\nupper bound of real error with respect to the increase of the number of model\ngrids will decrease then increase. Meanwhile, in a case study, by selecting the\noptimal grid size, the order dispatching results of a state-of-the-art\nprediction-based algorithm can be improved up to 13.6%, which shows the\neffectiveness of our methods on tuning the region partition for spatiotemporal\nprediction models.",
          "link": "http://arxiv.org/abs/2201.03244",
          "publishedOn": "2022-01-12T00:38:45.926Z",
          "wordCount": null,
          "title": "GridTuner: Reinvestigate Grid Size Selection for Spatiotemporal Prediction Models [Technical Report]. (arXiv:2201.03244v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_N/0/1/0/all/0/1\">Nasrullah Sheikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reinwald_B/0/1/0/all/0/1\">Berthold Reinwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1\">Chuan Lei</a>",
          "description": "Developing scalable solutions for training Graph Neural Networks (GNNs) for\nlink prediction tasks is challenging due to the high data dependencies which\nentail high computational cost and huge memory footprint. We propose a new\nmethod for scaling training of knowledge graph embedding models for link\nprediction to address these challenges. Towards this end, we propose the\nfollowing algorithmic strategies: self-sufficient partitions, constraint-based\nnegative sampling, and edge mini-batch training. Both, partitioning strategy\nand constraint-based negative sampling, avoid cross partition data transfer\nduring training. In our experimental evaluation, we show that our scaling\nsolution for GNN-based knowledge graph embedding models achieves a 16x speed up\non benchmark datasets while maintaining a comparable model performance as\nnon-distributed methods on standard metrics.",
          "link": "http://arxiv.org/abs/2201.02791",
          "publishedOn": "2022-01-12T00:38:45.925Z",
          "wordCount": null,
          "title": "Scaling Knowledge Graph Embedding Models. (arXiv:2201.02791v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.11109",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yunhui Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rimanic_L/0/1/0/all/0/1\">Luka Rimanic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Recent success of deep neural networks (DNNs) hinges on the availability of\nlarge-scale dataset; however, training on such dataset often poses privacy\nrisks for sensitive training information. In this paper, we aim to explore the\npower of generative models and gradient sparsity, and propose a scalable\nprivacy-preserving generative model DATALENS. Comparing with the standard PATE\nprivacy-preserving framework which allows teachers to vote on one-dimensional\npredictions, voting on the high dimensional gradient vectors is challenging in\nterms of privacy preservation. As dimension reduction techniques are required,\nwe need to navigate a delicate tradeoff space between (1) the improvement of\nprivacy preservation and (2) the slowdown of SGD convergence. To tackle this,\nwe take advantage of communication efficient learning and propose a novel noise\ncompression and aggregation approach TOPAGG by combining top-k compression for\ndimension reduction with a corresponding noise injection mechanism. We\ntheoretically prove that the DATALENS framework guarantees differential privacy\nfor its generated data, and provide analysis on its convergence. To demonstrate\nthe practical usage of DATALENS, we conduct extensive experiments on diverse\ndatasets including MNIST, Fashion-MNIST, and high dimensional CelebA, and we\nshow that, DATALENS significantly outperforms other baseline DP generative\nmodels. In addition, we adapt the proposed TOPAGG approach, which is one of the\nkey building blocks in DATALENS, to DP SGD training, and show that it is able\nto achieve higher utility than the state-of-the-art DP SGD approach in most\ncases. Our code is publicly available at https://github.com/AI-secure/DataLens.",
          "link": "http://arxiv.org/abs/2103.11109",
          "publishedOn": "2022-01-12T00:38:45.924Z",
          "wordCount": null,
          "title": "DataLens: Scalable Privacy Preserving Training via Gradient Compression and Aggregation. (arXiv:2103.11109v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1\">Feiyang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jian Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qiaoxi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1\">Haiyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenwu Wang</a>",
          "description": "Automated audio captioning (AAC) aims to describe audio data with captions\nusing natural language. Most existing AAC methods adopt an encoder-decoder\nstructure, where the attention based mechanism is a popular choice in the\ndecoder (e.g., Transformer decoder) for predicting captions from audio\nfeatures. Such attention based decoders can capture the global information from\nthe audio features, however, their ability in extracting local information can\nbe limited, which may lead to degraded quality in the generated captions. In\nthis paper, we present an AAC method with an attention-free decoder, where an\nencoder based on PANNs is employed for audio feature extraction, and the\nattention-free decoder is designed to introduce local information. The proposed\nmethod enables the effective use of both global and local information from\naudio signals. Experiments show that our method outperforms the\nstate-of-the-art methods with the standard attention based decoder in Task 6 of\nthe DCASE 2021 Challenge.",
          "link": "http://arxiv.org/abs/2201.03217",
          "publishedOn": "2022-01-12T00:38:45.922Z",
          "wordCount": null,
          "title": "Local Information Assisted Attention-free Decoder for Audio Captioning. (arXiv:2201.03217v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03116",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Hua Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_W/0/1/0/all/0/1\">Wei Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1\">Keqi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>",
          "description": "Driven by the key challenges of cell therapy manufacturing, including high\ncomplexity, high uncertainty, and very limited process data, we propose a\nstochastic optimization framework named \"hybrid-RL\" to efficiently guide\nprocess development and control. We first create the bioprocess probabilistic\nknowledge graph that is a hybrid model characterizing the understanding of\nbiomanufacturing process mechanisms and quantifying inherent stochasticity,\nsuch as batch-to-batch variation and bioprocess noise. It can capture the key\nfeatures, including nonlinear reactions, time-varying kinetics, and partially\nobserved bioprocess state. This hybrid model can leverage on existing\nmechanistic models and facilitate the learning from process data. Given limited\nprocess data, a computational sampling approach is used to generate posterior\nsamples quantifying the model estimation uncertainty. Then, we introduce hybrid\nmodel-based Bayesian reinforcement learning (RL), accounting for both inherent\nstochasticity and model uncertainty, to guide optimal, robust, and\ninterpretable decision making, which can overcome the key challenges of cell\ntherapy manufacturing. In the empirical study, cell therapy manufacturing\nexamples are used to demonstrate that the proposed hybrid-RL framework can\noutperform the classical deterministic mechanistic model assisted process\noptimization.",
          "link": "http://arxiv.org/abs/2201.03116",
          "publishedOn": "2022-01-12T00:38:45.919Z",
          "wordCount": null,
          "title": "Opportunities of Hybrid Model-based Reinforcement Learning for Cell Therapy Manufacturing Process Development and Control. (arXiv:2201.03116v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05185",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1\">Andrew Slavin Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1\">Finale Doshi-Velez</a>",
          "description": "In representation learning, there has been recent interest in developing\nalgorithms to disentangle the ground-truth generative factors behind a dataset,\nand metrics to quantify how fully this occurs. However, these algorithms and\nmetrics often assume that both representations and ground-truth factors are\nflat, continuous, and factorized, whereas many real-world generative processes\ninvolve rich hierarchical structure, mixtures of discrete and continuous\nvariables with dependence between them, and even varying intrinsic\ndimensionality. In this work, we develop benchmarks, algorithms, and metrics\nfor learning such hierarchical representations.",
          "link": "http://arxiv.org/abs/2102.05185",
          "publishedOn": "2022-01-12T00:38:45.919Z",
          "wordCount": null,
          "title": "Benchmarks, Algorithms, and Metrics for Hierarchical Disentanglement. (arXiv:2102.05185v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1\">Lianghao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huance Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiguo Zhang</a>",
          "description": "As the deep learning techniques have expanded to real-world recommendation\ntasks, many deep neural network based Collaborative Filtering (CF) models have\nbeen developed to project user-item interactions into latent feature space,\nbased on various neural architectures, such as multi-layer perceptron,\nauto-encoder and graph neural networks. However, the majority of existing\ncollaborative filtering systems are not well designed to handle missing data.\nParticularly, in order to inject the negative signals in the training phase,\nthese solutions largely rely on negative sampling from unobserved user-item\ninteractions and simply treating them as negative instances, which brings the\nrecommendation performance degradation. To address the issues, we develop a\nCollaborative Reflection-Augmented Autoencoder Network (CRANet), that is\ncapable of exploring transferable knowledge from observed and unobserved\nuser-item interactions. The network architecture of CRANet is formed of an\nintegrative structure with a reflective receptor network and an information\nfusion autoencoder module, which endows our recommendation framework with the\nability of encoding implicit user's pairwise preference on both interacted and\nnon-interacted items. Additionally, a parametric regularization-based\ntied-weight scheme is designed to perform robust joint training of the\ntwo-stage CRANet model. We finally experimentally validate CRANet on four\ndiverse benchmark datasets corresponding to two recommendation tasks, to show\nthat debiasing the negative signals of user-item interactions improves the\nperformance as compared to various state-of-the-art recommendation techniques.\nOur source code is available at https://github.com/akaxlh/CRANet.",
          "link": "http://arxiv.org/abs/2201.03158",
          "publishedOn": "2022-01-12T00:38:45.918Z",
          "wordCount": null,
          "title": "Collaborative Reflection-Augmented Autoencoder Network for Recommender Systems. (arXiv:2201.03158v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.07452",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Kara_A/0/1/0/all/0/1\">Ali Devran Kara</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yuksel_S/0/1/0/all/0/1\">Serdar Yuksel</a>",
          "description": "In the theory of Partially Observed Markov Decision Processes (POMDPs),\nexistence of optimal policies have in general been established via converting\nthe original partially observed stochastic control problem to a fully observed\none on the belief space, leading to a belief-MDP. However, computing an optimal\npolicy for this fully observed model, and so for the original POMDP, using\nclassical dynamic or linear programming methods is challenging even if the\noriginal system has finite state and action spaces, since the state space of\nthe fully observed belief-MDP model is always uncountable. Furthermore, there\nexist very few rigorous value function approximation and optimal policy\napproximation results, as regularity conditions needed often require a tedious\nstudy involving the spaces of probability measures leading to properties such\nas Feller continuity. In this paper, we study a planning problem for POMDPs\nwhere the system dynamics and measurement channel model are assumed to be\nknown. We construct an approximate belief model by discretizing the belief\nspace using only finite window information variables. We then find optimal\npolicies for the approximate model and we rigorously establish near optimality\nof the constructed finite window control policies in POMDPs under mild\nnon-linear filter stability conditions and the assumption that the measurement\nand action sets are finite (and the state space is real vector valued). We also\nestablish a rate of convergence result which relates the finite window memory\nsize and the approximation error bound, where the rate of convergence is\nexponential under explicit and testable exponential filter stability\nconditions. While there exist many experimental results and few rigorous\nasymptotic convergence results, an explicit rate of convergence result is new\nin the literature, to our knowledge.",
          "link": "http://arxiv.org/abs/2010.07452",
          "publishedOn": "2022-01-12T00:38:45.915Z",
          "wordCount": null,
          "title": "Near Optimality of Finite Memory Feedback Policies in Partially Observed Markov Decision Processes. (arXiv:2010.07452v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.09428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gangan_M/0/1/0/all/0/1\">Manjary P Gangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_A/0/1/0/all/0/1\">Anoop K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+L_L/0/1/0/all/0/1\">Lajish V L</a>",
          "description": "The problem of distinguishing natural images from photo-realistic\ncomputer-generated ones either addresses natural images versus computer\ngraphics or natural images versus GAN images, at a time. But in a real-world\nimage forensic scenario, it is highly essential to consider all categories of\nimage generation, since in most cases image generation is unknown. We, for the\nfirst time, to our best knowledge, approach the problem of distinguishing\nnatural images from photo-realistic computer-generated images as a three-class\nclassification task classifying natural, computer graphics, and GAN images. For\nthe task, we propose a Multi-Colorspace fused EfficientNet model by parallelly\nfusing three EfficientNet networks that follow transfer learning methodology\nwhere each network operates in different colorspaces, RGB, LCH, and HSV, chosen\nafter analyzing the efficacy of various colorspace transformations in this\nimage forensics problem. Our model outperforms the baselines in terms of\naccuracy, robustness towards post-processing, and generalizability towards\nother datasets. We conduct psychophysics experiments to understand how\naccurately humans can distinguish natural, computer graphics, and GAN images\nwhere we could observe that humans find difficulty in classifying these images,\nparticularly the computer-generated images, indicating the necessity of\ncomputational algorithms for the task. We also analyze the behavior of our\nmodel through visual explanations to understand salient regions that contribute\nto the model's decision making and compare with manual explanations provided by\nhuman participants in the form of region markings, where we could observe\nsimilarities in both the explanations indicating the powerful nature of our\nmodel to take the decisions meaningfully.",
          "link": "http://arxiv.org/abs/2110.09428",
          "publishedOn": "2022-01-12T00:38:45.899Z",
          "wordCount": null,
          "title": "Distinguishing Natural and Computer-Generated Images using Multi-Colorspace fused EfficientNet. (arXiv:2110.09428v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kevin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1\">Aayush Bansal</a>",
          "description": "We study video-specific autoencoders that allow a human user to explore,\nedit, and efficiently transmit videos. Prior work has independently looked at\nthese problems (and sub-problems) and proposed different formulations. In this\nwork, we train a simple autoencoder (from scratch) on multiple frames of a\nspecific video. We observe: (1) latent codes learned by a video-specific\nautoencoder capture spatial and temporal properties of that video; and (2)\nautoencoders can project out-of-sample inputs onto the video-specific manifold.\nThese two properties allow us to explore, edit, and efficiently transmit a\nvideo using one learned representation. For e.g., linear operations on latent\ncodes allow users to visualize the contents of a video. Associating latent\ncodes of a video and manifold projection enables users to make desired edits.\nInterpolating latent codes and manifold projection allows the transmission of\nsparse low-res frames over a network.",
          "link": "http://arxiv.org/abs/2103.17261",
          "publishedOn": "2022-01-12T00:38:45.896Z",
          "wordCount": null,
          "title": "Video-Specific Autoencoders for Exploring, Editing and Transmitting Videos. (arXiv:2103.17261v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hayes_J/0/1/0/all/0/1\">Jamie Hayes</a>",
          "description": "Historically, machine learning methods have not been designed with security\nin mind. In turn, this has given rise to adversarial examples, carefully\nperturbed input samples aimed to mislead detection at test time, which have\nbeen applied to attack spam and malware classification, and more recently to\nattack image classification. Consequently, an abundance of research has been\ndevoted to designing machine learning methods that are robust to adversarial\nexamples. Unfortunately, there are desiderata besides robustness that a secure\nand safe machine learning model must satisfy, such as fairness and privacy.\nRecent work by Song et al. (2019) has shown, empirically, that there exists a\ntrade-off between robust and private machine learning models. Models designed\nto be robust to adversarial examples often overfit on training data to a larger\nextent than standard (non-robust) models. If a dataset contains private\ninformation, then any statistical test that separates training and test data by\nobserving a model's outputs can represent a privacy breach, and if a model\noverfits on training data, these statistical tests become easier.\n\nIn this work, we identify settings where standard models will overfit to a\nlarger extent in comparison to robust models, and as empirically observed in\nprevious works, settings where the opposite behavior occurs. Thus, it is not\nnecessarily the case that privacy must be sacrificed to achieve robustness. The\ndegree of overfitting naturally depends on the amount of data available for\ntraining. We go on to characterize how the training set size factors into the\nprivacy risks exposed by training a robust model on a simple Gaussian data\ntask, and show empirically that our findings hold on image classification\nbenchmark datasets, such as CIFAR-10 and CIFAR-100.",
          "link": "http://arxiv.org/abs/2006.04622",
          "publishedOn": "2022-01-12T00:38:45.863Z",
          "wordCount": null,
          "title": "Trade-offs between membership privacy & adversarially robust learning. (arXiv:2006.04622v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03299",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Santos_C/0/1/0/all/0/1\">Claudio Filipi Gon&#xe7;alves dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Papa</a>",
          "description": "Several image processing tasks, such as image classification and object\ndetection, have been significantly improved using Convolutional Neural Networks\n(CNN). Like ResNet and EfficientNet, many architectures have achieved\noutstanding results in at least one dataset by the time of their creation. A\ncritical factor in training concerns the network's regularization, which\nprevents the structure from overfitting. This work analyzes several\nregularization methods developed in the last few years, showing significant\nimprovements for different CNN models. The works are classified into three main\nareas: the first one is called \"data augmentation\", where all the techniques\nfocus on performing changes in the input data. The second, named \"internal\nchanges\", which aims to describe procedures to modify the feature maps\ngenerated by the neural network or the kernels. The last one, called \"label\",\nconcerns transforming the labels of a given input. This work presents two main\ndifferences comparing to other available surveys about regularization: (i) the\nfirst concerns the papers gathered in the manuscript, which are not older than\nfive years, and (ii) the second distinction is about reproducibility, i.e., all\nworks refered here have their code available in public repositories or they\nhave been directly implemented in some framework, such as TensorFlow or Torch.",
          "link": "http://arxiv.org/abs/2201.03299",
          "publishedOn": "2022-01-12T00:38:45.861Z",
          "wordCount": null,
          "title": "Avoiding Overfitting: A Survey on Regularization Methods for Convolutional Neural Networks. (arXiv:2201.03299v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grubl_T/0/1/0/all/0/1\">Thomas Grubl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lallie_H/0/1/0/all/0/1\">Harjinder Singh Lallie</a>",
          "description": "The precise age estimation of child sexual abuse and exploitation (CSAE)\nvictims is one of the most significant digital forensic challenges.\nInvestigators often need to determine the age of victims by looking at images\nand interpreting the sexual development stages and other human characteristics.\nThe main priority - safeguarding children -- is often negatively impacted by a\nhuge forensic backlog, cognitive bias and the immense psychological stress that\nthis work can entail. This paper evaluates existing facial image datasets and\nproposes a new dataset tailored to the needs of similar digital forensic\nresearch contributions. This small, diverse dataset of 0 to 20-year-old\nindividuals contains 245 images and is merged with 82 unique images from the\nFG-NET dataset, thus achieving a total of 327 images with high image diversity\nand low age range density. The new dataset is tested on the Deep EXpectation\n(DEX) algorithm pre-trained on the IMDB-WIKI dataset. The overall results for\nyoung adolescents aged 10 to 15 and older adolescents/adults aged 16 to 20 are\nvery encouraging -- achieving MAEs as low as 1.79, but also suggest that the\naccuracy for children aged 0 to 10 needs further work. In order to determine\nthe efficacy of the prototype, valuable input of four digital forensic experts,\nincluding two forensic investigators, has been taken into account to improve\nage estimation results. Further research is required to extend datasets both\nconcerning image density and the equal distribution of factors such as gender\nand racial diversity.",
          "link": "http://arxiv.org/abs/2201.03045",
          "publishedOn": "2022-01-12T00:38:45.847Z",
          "wordCount": null,
          "title": "Applying Artificial Intelligence for Age Estimation in Digital Forensic Investigations. (arXiv:2201.03045v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.01606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ensinger_K/0/1/0/all/0/1\">Katharina Ensinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solowjow_F/0/1/0/all/0/1\">Friedrich Solowjow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziesche_S/0/1/0/all/0/1\">Sebastian Ziesche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiemann_M/0/1/0/all/0/1\">Michael Tiemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trimpe_S/0/1/0/all/0/1\">Sebastian Trimpe</a>",
          "description": "Most physical processes posses structural properties such as constant\nenergies, volumes, and other invariants over time. When learning models of such\ndynamical systems, it is critical to respect these invariants to ensure\naccurate predictions and physically meaningful behavior. Strikingly,\nstate-of-the-art methods in Gaussian process (GP) dynamics model learning are\nnot addressing this issue. On the other hand, classical numerical integrators\nare specifically designed to preserve these crucial properties through time. We\npropose to combine the advantages of GPs as function approximators with\nstructure preserving numerical integrators for dynamical systems, such as\nRunge-Kutta methods. These integrators assume access to the ground truth\ndynamics and require evaluations of intermediate and future time steps that are\nunknown in a learning-based scenario. This makes direct inference of the GP\ndynamics, with embedded numerical scheme, intractable. Our key technical\ncontribution is the evaluation of the implicitly defined Runge-Kutta transition\nprobability. In a nutshell, we introduce an implicit layer for GP regression,\nwhich is embedded into a variational inference-based model learning scheme.",
          "link": "http://arxiv.org/abs/2102.01606",
          "publishedOn": "2022-01-12T00:38:45.846Z",
          "wordCount": null,
          "title": "Structure-preserving Gaussian Process Dynamics. (arXiv:2102.01606v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Siyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1\">Lixin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiding Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wenwen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Suqi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuaiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hechang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dawei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yi Chang</a>",
          "description": "Post-click conversion, as a strong signal indicating the user preference, is\nsalutary for building recommender systems. However, accurately estimating the\npost-click conversion rate (CVR) is challenging due to the selection bias,\ni.e., the observed clicked events usually happen on users' preferred items.\nCurrently, most existing methods utilize counterfactual learning to debias\nrecommender systems. Among them, the doubly robust (DR) estimator has achieved\ncompetitive performance by combining the error imputation based (EIB) estimator\nand the inverse propensity score (IPS) estimator in a doubly robust way.\nHowever, inaccurate error imputation may result in its higher variance than the\nIPS estimator. Worse still, existing methods typically use simple\nmodel-agnostic methods to estimate the imputation error, which are not\nsufficient to approximate the dynamically changing model-correlated target\n(i.e., the gradient direction of the prediction model). To solve these\nproblems, we first derive the bias and variance of the DR estimator. Based on\nit, a more robust doubly robust (MRDR) estimator has been proposed to further\nreduce its variance while retaining its double robustness. Moreover, we propose\na novel double learning approach for the MRDR estimator, which can convert the\nerror imputation into the general CVR estimation. Besides, we empirically\nverify that the proposed learning scheme can further eliminate the high\nvariance problem of the imputation learning. To evaluate its effectiveness,\nextensive experiments are conducted on a semi-synthetic dataset and two\nreal-world datasets. The results demonstrate the superiority of the proposed\napproach over the state-of-the-art methods. The code is available at\nhttps://github.com/guosyjlu/MRDR-DL.",
          "link": "http://arxiv.org/abs/2105.13623",
          "publishedOn": "2022-01-12T00:38:45.844Z",
          "wordCount": null,
          "title": "Enhanced Doubly Robust Learning for Debiasing Post-click Conversion Rate Estimation. (arXiv:2105.13623v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.07207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiaoyun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>",
          "description": "We propose improving the cross-target and cross-scene generalization of\nvisual navigation through learning an agent that is guided by conceiving the\nnext observations it expects to see. This is achieved by learning a variational\nBayesian model, called NeoNav, which generates the next expected observations\n(NEO) conditioned on the current observations of the agent and the target view.\nOur generative model is learned through optimizing a variational objective\nencompassing two key designs. First, the latent distribution is conditioned on\ncurrent observations and the target view, leading to a model-based,\ntarget-driven navigation. Second, the latent space is modeled with a Mixture of\nGaussians conditioned on the current observation and the next best action. Our\nuse of mixture-of-posteriors prior effectively alleviates the issue of\nover-regularized latent space, thus significantly boosting the model\ngeneralization for new targets and in novel scenes. Moreover, the NEO\ngeneration models the forward dynamics of agent-environment interaction, which\nimproves the quality of approximate inference and hence benefits data\nefficiency. We have conducted extensive evaluations on both real-world and\nsynthetic benchmarks, and show that our model consistently outperforms the\nstate-of-the-art models in terms of success rate, data efficiency, and\ngeneralization.",
          "link": "http://arxiv.org/abs/1906.07207",
          "publishedOn": "2022-01-12T00:38:45.839Z",
          "wordCount": null,
          "title": "NeoNav: Improving the Generalization of Visual Navigation via Generating Next Expected Observations. (arXiv:1906.07207v4 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Izutsu_J/0/1/0/all/0/1\">Jun Izutsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komiya_K/0/1/0/all/0/1\">Kanako Komiya</a>",
          "description": "This study proposes a method to develop neural models of the morphological\nanalyzer for Japanese Hiragana sentences using the Bi-LSTM CRF model.\nMorphological analysis is a technique that divides text data into words and\nassigns information such as parts of speech. This technique plays an essential\nrole in downstream applications in Japanese natural language processing systems\nbecause the Japanese language does not have word delimiters between words.\nHiragana is a type of Japanese phonogramic characters, which is used for texts\nfor children or people who cannot read Chinese characters. Morphological\nanalysis of Hiragana sentences is more difficult than that of ordinary Japanese\nsentences because there is less information for dividing. For morphological\nanalysis of Hiragana sentences, we demonstrated the effectiveness of\nfine-tuning using a model based on ordinary Japanese text and examined the\ninfluence of training data on texts of various genres.",
          "link": "http://arxiv.org/abs/2201.03366",
          "publishedOn": "2022-01-12T00:38:45.826Z",
          "wordCount": null,
          "title": "Morphological Analysis of Japanese Hiragana Sentences using the BI-LSTM CRF Model. (arXiv:2201.03366v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiasen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mohammadreza Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1\">Aditya Kusupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>",
          "description": "As humans, we navigate the world through all our senses, using perceptual\ninput from each one to correct the others. We introduce MERLOT Reserve, a model\nthat represents videos jointly over time -- through a new training objective\nthat learns from audio, subtitles, and video frames. Given a video, we replace\nsnippets of text and audio with a MASK token; the model learns by choosing the\ncorrect masked-out snippet. Our objective learns faster than alternatives, and\nperforms well at scale: we pretrain on 20 million YouTube videos.\n\nEmpirical results show that MERLOT Reserve learns strong representations\nabout videos through all constituent modalities. When finetuned, it sets a new\nstate-of-the-art on both VCR and TVQA, outperforming prior work by 5% and 7%\nrespectively. Ablations show that both tasks benefit from audio pretraining --\neven VCR, a QA task centered around images (without sound). Moreover, our\nobjective enables out-of-the-box prediction, revealing strong multimodal\ncommonsense understanding. In a fully zero-shot setting, our model obtains\ncompetitive results on four video understanding tasks, even outperforming\nsupervised approaches on the recently proposed Situated Reasoning (STAR)\nbenchmark.\n\nWe analyze why incorporating audio leads to better vision-language\nrepresentations, suggesting significant opportunities for future research. We\nconclude by discussing ethical and societal implications of multimodal\npretraining.",
          "link": "http://arxiv.org/abs/2201.02639",
          "publishedOn": "2022-01-12T00:38:45.824Z",
          "wordCount": null,
          "title": "MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound. (arXiv:2201.02639v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huanyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_Y/0/1/0/all/0/1\">Yang Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Zhenhua Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>",
          "description": "Few-sample compression aims to compress a big redundant model into a small\ncompact one with only few samples. If we fine-tune models with these limited\nfew samples directly, models will be vulnerable to overfit and learn almost\nnothing. Hence, previous methods optimize the compressed model layer-by-layer\nand try to make every layer have the same outputs as the corresponding layer in\nthe teacher model, which is cumbersome. In this paper, we propose a new\nframework named Mimicking then Replacing (MiR) for few-sample compression,\nwhich firstly urges the pruned model to output the same features as the\nteacher's in the penultimate layer, and then replaces teacher's layers before\npenultimate with a well-tuned compact one. Unlike previous layer-wise\nreconstruction methods, our MiR optimizes the entire network holistically,\nwhich is not only simple and effective, but also unsupervised and general. MiR\noutperforms previous methods with large margins. Codes will be available soon.",
          "link": "http://arxiv.org/abs/2201.02620",
          "publishedOn": "2022-01-12T00:38:45.808Z",
          "wordCount": null,
          "title": "Compressing Models with Few Samples: Mimicking then Replacing. (arXiv:2201.02620v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2111.08498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lim_Y/0/1/0/all/0/1\">Yi Heng Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasim_M/0/1/0/all/0/1\">Muhammad Firmansyah Kasim</a>",
          "description": "Deep-learning-based models are increasingly used to emulate scientific\nsimulations to accelerate scientific research. However, accurate, supervised\ndeep learning models require huge amount of labelled data, and that often\nbecomes the bottleneck in employing neural networks. In this work, we leveraged\nan active learning approach called core-set selection to actively select data,\nper a pre-defined budget, to be labelled for training. To further improve the\nmodel performance and reduce the training costs, we also warm started the\ntraining using a shrink-and-perturb trick. We tested on two case studies in\ndifferent fields, namely galaxy halo occupation distribution modelling in\nastrophysics and x-ray emission spectroscopy in plasma physics, and the results\nare promising: we achieved competitive overall performance compared to using a\nrandom sampling baseline, and more importantly, successfully reduced the larger\nabsolute losses, i.e. the long tail in the loss distribution, at virtually no\noverhead costs.",
          "link": "http://arxiv.org/abs/2111.08498",
          "publishedOn": "2022-01-12T00:38:45.785Z",
          "wordCount": null,
          "title": "Reducing the Long Tail Losses in Scientific Emulations with Active Learning. (arXiv:2111.08498v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kien Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yingli Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1\">Arun Ross</a>",
          "description": "The rapid emergence of airborne platforms and imaging sensors are enabling\nnew forms of aerial surveillance due to their unprecedented advantages in\nscale, mobility, deployment and covert observation capabilities. This paper\nprovides a comprehensive overview of human-centric aerial surveillance tasks\nfrom a computer vision and pattern recognition perspective. It aims to provide\nreaders with an in-depth systematic review and technical analysis of the\ncurrent state of aerial surveillance tasks using drones, UAVs and other\nairborne platforms. The main object of interest is humans, where single or\nmultiple subjects are to be detected, identified, tracked, re-identified and\nhave their behavior analyzed. More specifically, for each of these four tasks,\nwe first discuss unique challenges in performing these tasks in an aerial\nsetting compared to a ground-based setting. We then review and analyze the\naerial datasets publicly available for each task, and delve deep into the\napproaches in the aerial literature and investigate how they presently address\nthe aerial challenges. We conclude the paper with discussion on the missing\ngaps and open research questions to inform future research avenues.",
          "link": "http://arxiv.org/abs/2201.03080",
          "publishedOn": "2022-01-12T00:38:45.772Z",
          "wordCount": null,
          "title": "The State of Aerial Surveillance: A Survey. (arXiv:2201.03080v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1\">Yilin Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_M/0/1/0/all/0/1\">Marcus Eng Hock Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1\">Feng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_B/0/1/0/all/0/1\">Bibhas Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ting_D/0/1/0/all/0/1\">Daniel Shu Wei Ting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nan Liu</a>",
          "description": "Risk scores are widely used for clinical decision making and commonly\ngenerated from logistic regression models. Machine-learning-based methods may\nwork well for identifying important predictors, but such 'black box' variable\nselection limits interpretability, and variable importance evaluated from a\nsingle model can be biased. We propose a robust and interpretable variable\nselection approach using the recently developed Shapley variable importance\ncloud (ShapleyVIC) that accounts for variability across models. Our approach\nevaluates and visualizes overall variable contributions for in-depth inference\nand transparent variable selection, and filters out non-significant\ncontributors to simplify model building steps. We derive an ensemble variable\nranking from variable contributions, which is easily integrated with an\nautomated and modularized risk score generator, AutoScore, for convenient\nimplementation. In a study of early death or unplanned readmission, ShapleyVIC\nselected 6 of 41 candidate variables to create a well-performing model, which\nhad similar performance to a 16-variable model from machine-learning-based\nranking.",
          "link": "http://arxiv.org/abs/2201.03291",
          "publishedOn": "2022-01-12T00:38:45.762Z",
          "wordCount": null,
          "title": "A novel interpretable machine learning system to generate clinical risk scores: An application for predicting early mortality or unplanned readmission in a retrospective cohort study. (arXiv:2201.03291v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.14722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohla_S/0/1/0/all/0/1\">Satyam Mohla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasery_A/0/1/0/all/0/1\">Anshul Nasery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1\">Biplab Banerjee</a>",
          "description": "Recent experiments in computer vision demonstrate texture bias as the primary\nreason for supreme results in models employing Convolutional Neural Networks\n(CNNs), conflicting with early works claiming that these networks identify\nobjects using shape. It is believed that the cost function forces the CNN to\ntake a greedy approach and develop a proclivity for local information like\ntexture to increase accuracy, thus failing to explore any global statistics. We\npropose CognitiveCNN, a new intuitive architecture, inspired from feature\nintegration theory in psychology to utilise human interpretable feature like\nshape, texture, edges etc. to reconstruct, and classify the image. We define\nnovel metrics to quantify the \"relevance\" of \"abstract information\" present in\nthese modalities using attention maps. We further introduce a regularisation\nmethod which ensures that each modality like shape, texture etc. gets\nproportionate influence in a given task, as it does for reconstruction; and\nperform experiments to show the resulting boost in accuracy and robustness,\nbesides imparting explainability to these CNNs for achieving superior\nperformance in object recognition.",
          "link": "http://arxiv.org/abs/2006.14722",
          "publishedOn": "2022-01-12T00:38:45.761Z",
          "wordCount": null,
          "title": "Teaching CNNs to mimic Human Visual Cognitive Process & regularise Texture-Shape bias. (arXiv:2006.14722v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Run-kun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian-wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuan-fang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Hao-jie Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xin Zuo</a>",
          "description": "Multi-view learning is a learning problem that utilizes the various\nrepresentations of an object to mine valuable knowledge and improve the\nperformance of learning algorithm, and one of the significant directions of\nmulti-view learning is sub-space learning. As we known, auto-encoder is a\nmethod of deep learning, which can learn the latent feature of raw data by\nreconstructing the input, and based on this, we propose a novel algorithm\ncalled Auto-encoder based Co-training Multi-View Learning (ACMVL), which\nutilizes both complementarity and consistency and finds a joint latent feature\nrepresentation of multiple views. The algorithm has two stages, the first is to\ntrain auto-encoder of each view, and the second stage is to train a supervised\nnetwork. Interestingly, the two stages share the weights partly and assist each\nother by co-training process. According to the experimental result, we can\nlearn a well performed latent feature representation, and auto-encoder of each\nview has more powerful reconstruction ability than traditional auto-encoder.",
          "link": "http://arxiv.org/abs/2201.02978",
          "publishedOn": "2022-01-12T00:38:45.758Z",
          "wordCount": null,
          "title": "Auto-Encoder based Co-Training Multi-View Representation Learning. (arXiv:2201.02978v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.05883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hanwen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quader_N/0/1/0/all/0/1\">Niamul Quader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zhixiang Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1\">Peng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Juwei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>",
          "description": "Recent self-supervised video representation learning methods have found\nsignificant success by exploring essential properties of videos, e.g. speed,\ntemporal order, etc. This work exploits an essential yet under-explored\nproperty of videos, the video continuity, to obtain supervision signals for\nself-supervised representation learning. Specifically, we formulate three novel\ncontinuity-related pretext tasks, i.e. continuity justification, discontinuity\nlocalization, and missing section approximation, that jointly supervise a\nshared backbone for video representation learning. This self-supervision\napproach, termed as Continuity Perception Network (CPNet), solves the three\ntasks altogether and encourages the backbone network to learn local and\nlong-ranged motion and context representations. It outperforms prior arts on\nmultiple downstream tasks, such as action recognition, video retrieval, and\naction localization. Additionally, the video continuity can be complementary to\nother coarse-grained video properties for representation learning, and\nintegrating the proposed pretext task to prior arts can yield much performance\ngains.",
          "link": "http://arxiv.org/abs/2112.05883",
          "publishedOn": "2022-01-12T00:38:45.758Z",
          "wordCount": null,
          "title": "Self-supervised Spatiotemporal Representation Learning by Exploiting Video Continuity. (arXiv:2112.05883v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03215",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hung Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oka_H/0/1/0/all/0/1\">Haruki Oka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishioka_T/0/1/0/all/0/1\">Tsunenori Ishioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakagawa_M/0/1/0/all/0/1\">Masaki Nakagawa</a>",
          "description": "This paper presents an experiment of automatically scoring handwritten\ndescriptive answers in the trial tests for the new Japanese university entrance\nexamination, which were made for about 120,000 examinees in 2017 and 2018.\nThere are about 400,000 answers with more than 20 million characters. Although\nall answers have been scored by human examiners, handwritten characters are not\nlabelled. We present our attempt to adapt deep neural network-based handwriting\nrecognizers trained on a labelled handwriting dataset into this unlabeled\nanswer set. Our proposed method combines different training strategies,\nensembles multiple recognizers, and uses a language model built from a large\ngeneral corpus to avoid overfitting into specific data. In our experiment, the\nproposed method records character accuracy of over 97% using about 2,000\nverified labelled answers that account for less than 0.5% of the dataset. Then,\nthe recognized answers are fed into a pre-trained automatic scoring system\nbased on the BERT model without correcting misrecognized characters and\nproviding rubric annotations. The automatic scoring system achieves from 0.84\nto 0.98 of Quadratic Weighted Kappa (QWK). As QWK is over 0.8, it represents\nacceptable similarity of scoring between the automatic scoring system and the\nhuman examiners. These results are promising for further research on end-to-end\nautomatic scoring of descriptive answers.",
          "link": "http://arxiv.org/abs/2201.03215",
          "publishedOn": "2022-01-12T00:38:45.739Z",
          "wordCount": null,
          "title": "Fully automatic scoring of handwritten descriptive answers in Japanese language tests. (arXiv:2201.03215v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2110.07077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chun-Hung Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Di-Chun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gau_R/0/1/0/all/0/1\">Rung-Hung Gau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Lu Wei</a>",
          "description": "Federated learning (FL) is a promising distributed learning technique\nparticularly suitable for wireless learning scenarios since it can accomplish a\nlearning task without raw data transportation so as to preserve data privacy\nand lower network resource consumption. However, current works on FL over\nwireless networks do not profoundly study the fundamental performance of FL\nover wireless networks that suffers from communication outage due to channel\nimpairment and network interference. To accurately exploit the performance of\nFL over wireless networks, this paper proposes a novel intermittent FL model\nover a cellular-connected unmanned aerial vehicle (UAV) network, which\ncharacterizes communication outage from UAV (clients) to their server and data\nheterogeneity among the datasets at UAVs. We propose an analytically tractable\nframework to derive the uplink outage probability and use it to devise a\nsimulation-based approach so as to evaluate the performance of the proposed\nintermittent FL model. Our findings reveal how the intermittent FL model is\nimpacted by uplink communication outage and UAV deployment. Extensive numerical\nsimulations are provided to show the consistency between the simulated and\nanalytical performances of the proposed intermittent FL model.",
          "link": "http://arxiv.org/abs/2110.07077",
          "publishedOn": "2022-01-12T00:38:45.736Z",
          "wordCount": null,
          "title": "Modeling and Analysis of Intermittent Federated Learning Over Cellular-Connected UAV Networks. (arXiv:2110.07077v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kamo_S/0/1/0/all/0/1\">Satoshi Kamo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1\">Yiqiang Sheng</a>",
          "description": "Identifying anomaly multimedia traffic in cyberspace is a big challenge in\ndistributed service systems, multiple generation networks and future internet\nof everything. This letter explores meta-generalization for a multiparty\nprivacy learning model in graynet to improve the performance of anomaly\nmultimedia traffic identification. The multiparty privacy learning model in\ngraynet is a globally shared model that is partitioned, distributed and trained\nby exchanging multiparty parameters updates with preserving private data. The\nmeta-generalization refers to discovering the inherent attributes of a learning\nmodel to reduce its generalization error. In experiments, three\nmeta-generalization principles are tested as follows. The generalization error\nof the multiparty privacy learning model in graynet is reduced by changing the\ndimension of byte-level imbedding. Following that, the error is reduced by\nadapting the depth for extracting packet-level features. Finally, the error is\nreduced by adjusting the size of support set for preprocessing traffic-level\ndata. Experimental results demonstrate that the proposal outperforms the\nstate-of-the-art learning models for identifying anomaly multimedia traffic.",
          "link": "http://arxiv.org/abs/2201.03027",
          "publishedOn": "2022-01-12T00:38:45.727Z",
          "wordCount": null,
          "title": "Meta-Generalization for Multiparty Privacy Learning to Identify Anomaly Multimedia Traffic in Graynet. (arXiv:2201.03027v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_T/0/1/0/all/0/1\">Tian Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Song Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Han Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jialiang Lu</a>",
          "description": "Learning-based Network Intrusion Detection Systems (NIDSs) are widely\ndeployed for defending various cyberattacks. Existing learning-based NIDS\nmainly uses Neural Network (NN) as a classifier that relies on the quality and\nquantity of cyberattack data. Such NN-based approaches are also hard to\ninterpret for improving efficiency and scalability. In this paper, we design a\nnew local-global computation paradigm, FEDFOREST, a novel learning-based NIDS\nby combining the interpretable Gradient Boosting Decision Tree (GBDT) and\nFederated Learning (FL) framework. Specifically, FEDFOREST is composed of\nmultiple clients that extract local cyberattack data features for the server to\ntrain models and detect intrusions. A privacy-enhanced technology is also\nproposed in FEDFOREST to further defeat the privacy of the FL systems.\nExtensive experiments on 4 cyberattack datasets of different tasks demonstrate\nthat FEDFOREST is effective, efficient, interpretable, and extendable.\nFEDFOREST ranks first in the collaborative learning and cybersecurity\ncompetition 2021 for Chinese college students.",
          "link": "http://arxiv.org/abs/2201.03134",
          "publishedOn": "2022-01-12T00:38:45.725Z",
          "wordCount": null,
          "title": "An Interpretable Federated Learning-based Network Intrusion Detection Framework. (arXiv:2201.03134v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiyang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Beibei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tian Lu</a>",
          "description": "We develop a structural econometric model to capture the decision dynamics of\nhuman evaluators on an online micro-lending platform, and estimate the model\nparameters using a real-world dataset. We find two types of biases in gender,\npreference-based bias and belief-based bias, are present in human evaluators'\ndecisions. Both types of biases are in favor of female applicants. Through\ncounterfactual simulations, we quantify the effect of gender bias on loan\ngranting outcomes and the welfare of the company and the borrowers. Our results\nimply that both the existence of the preference-based bias and that of the\nbelief-based bias reduce the company's profits. When the preference-based bias\nis removed, the company earns more profits. When the belief-based bias is\nremoved, the company's profits also increase. Both increases result from\nraising the approval probability for borrowers, especially male borrowers, who\neventually pay back loans. For borrowers, the elimination of either bias\ndecreases the gender gap of the true positive rates in the credit risk\nevaluation. We also train machine learning algorithms on both the real-world\ndata and the data from the counterfactual simulations. We compare the decisions\nmade by those algorithms to see how evaluators' biases are inherited by the\nalgorithms and reflected in machine-based decisions. We find that machine\nlearning algorithms can mitigate both the preference-based bias and the\nbelief-based bias.",
          "link": "http://arxiv.org/abs/2201.03092",
          "publishedOn": "2022-01-12T00:38:45.722Z",
          "wordCount": null,
          "title": "Uncovering the Source of Machine Bias. (arXiv:2201.03092v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03172",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Geeho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinkyu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bohyung Han</a>",
          "description": "Federated learning often suffers from unstable and slow convergence due to\nheterogeneous characteristics of participating clients. Such tendency is\naggravated when the client participation ratio is low since the information\ncollected from the clients at each round is prone to be more inconsistent. To\ntackle the challenge, we propose a novel federated learning framework, which\nimproves the stability of the server-side aggregation step, which is achieved\nby sending the clients an accelerated model estimated with the global gradient\nto guide the local gradient updates. Our algorithm naturally aggregates and\nconveys the global update information to participants with no additional\ncommunication cost and does not require to store the past models in the\nclients. We also regularize local update to further reduce the bias and improve\nthe stability of local updates. We perform comprehensive empirical studies on\nreal data under various settings and demonstrate the remarkable performance of\nthe proposed method in terms of accuracy and communication-efficiency compared\nto the state-of-the-art methods, especially with low client participation\nrates. Our code is available at https://github.com/ ninigapa0/FedAGM",
          "link": "http://arxiv.org/abs/2201.03172",
          "publishedOn": "2022-01-12T00:38:45.714Z",
          "wordCount": null,
          "title": "Communication-Efficient Federated Learning with Acceleration of Global Momentum. (arXiv:2201.03172v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinhui Xu</a>",
          "description": "We study the problem of Differentially Private Stochastic Convex Optimization\n(DP-SCO) with heavy-tailed data. Specifically, we focus on the $\\ell_1$-norm\nlinear regression in the $\\epsilon$-DP model. While most of the previous work\nfocuses on the case where the loss function is Lipschitz, here we only need to\nassume the variates has bounded moments. Firstly, we study the case where the\n$\\ell_2$ norm of data has bounded second order moment. We propose an algorithm\nwhich is based on the exponential mechanism and show that it is possible to\nachieve an upper bound of $\\tilde{O}(\\sqrt{\\frac{d}{n\\epsilon}})$ (with high\nprobability). Next, we relax the assumption to bounded $\\theta$-th order moment\nwith some $\\theta\\in (1, 2)$ and show that it is possible to achieve an upper\nbound of $\\tilde{O}(({\\frac{d}{n\\epsilon}})^\\frac{\\theta-1}{\\theta})$. Our\nalgorithms can also be extended to more relaxed cases where only each\ncoordinate of the data has bounded moments, and we can get an upper bound of\n$\\tilde{O}({\\frac{d}{\\sqrt{n\\epsilon}}})$ and\n$\\tilde{O}({\\frac{d}{({n\\epsilon})^\\frac{\\theta-1}{\\theta}}})$ in the second\nand $\\theta$-th moment case respectively.",
          "link": "http://arxiv.org/abs/2201.03204",
          "publishedOn": "2022-01-12T00:38:45.643Z",
          "wordCount": null,
          "title": "Differentially Private $\\ell_1$-norm Linear Regression with Heavy-tailed Data. (arXiv:2201.03204v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunnan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongzhi Wang</a>",
          "description": "Trajectory Prediction (TP) is an important research topic in computer vision\nand robotics fields. Recently, many stochastic TP models have been proposed to\ndeal with this problem and have achieved better performance than the\ntraditional models with deterministic trajectory outputs. However, these\nstochastic models can generate a number of future trajectories with different\nqualities. They are lack of self-evaluation ability, that is, to examine the\nrationality of their prediction results, thus failing to guide users to\nidentify high-quality ones from their candidate results. This hinders them from\nplaying their best in real applications. In this paper, we make up for this\ndefect and propose TPAD, a novel TP evaluation method based on the trajectory\nAnomaly Detection (AD) technique. In TPAD, we firstly combine the Automated\nMachine Learning (AutoML) technique and the experience in the AD and TP field\nto automatically design an effective trajectory AD model. Then, we utilize the\nlearned trajectory AD model to examine the rationality of the predicted\ntrajectories, and screen out good TP results for users. Extensive experimental\nresults demonstrate that TPAD can effectively identify near-optimal prediction\nresults, improving stochastic TP models' practical application effect.",
          "link": "http://arxiv.org/abs/2201.02941",
          "publishedOn": "2022-01-12T00:38:45.642Z",
          "wordCount": null,
          "title": "TPAD: Identifying Effective Trajectory Predictions Under the Guidance of Trajectory Anomaly Detection Model. (arXiv:2201.02941v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2110.00115",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Choe_Y/0/1/0/all/0/1\">Yo Joong Choe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1\">Aaditya Ramdas</a>",
          "description": "Consider two or more forecasters, each making a sequence of predictions for\ndifferent events over time. We ask a relatively basic question: how might we\ncompare these forecasters, either online or post-hoc, while avoiding\nunverifiable assumptions on how the forecasts or outcomes were generated? This\nwork presents a novel and rigorous answer to this question. We design a\nsequential inference procedure for estimating the time-varying difference in\nforecast quality as measured by any scoring rule. The resulting confidence\nintervals are nonasymptotically valid and can be continuously monitored to\nyield statistically valid comparisons at arbitrary data-dependent stopping\ntimes (\"anytime-valid\"); this is enabled by adapting variance-adaptive\nsupermartingales, confidence sequences, and e-processes to our setting.\nMotivated by Shafer and Vovk's game-theoretic probability, our coverage\nguarantees are also distribution-free, in the sense that they make no\ndistributional assumptions on the forecasts or outcomes. In contrast to a\nrecent work by Henzi and Ziegel, our tools can sequentially test a weak null\nhypothesis about whether one forecaster outperforms another on average over\ntime. We demonstrate their effectiveness by comparing probability forecasts on\nMajor League Baseball (MLB) games and statistical postprocessing methods for\nensemble weather forecasts.",
          "link": "http://arxiv.org/abs/2110.00115",
          "publishedOn": "2022-01-12T00:38:45.640Z",
          "wordCount": null,
          "title": "Comparing Sequential Forecasters. (arXiv:2110.00115v3 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.08527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ng_I/0/1/0/all/0/1\">Ignavier Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shengyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhuangyan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhitang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>",
          "description": "This paper studies the problem of learning causal structures from\nobservational data. We reformulate the Structural Equation Model (SEM) with\nadditive noises in a form parameterized by binary graph adjacency matrix and\nshow that, if the original SEM is identifiable, then the binary adjacency\nmatrix can be identified up to super-graphs of the true causal graph under mild\nconditions. We then utilize the reformulated SEM to develop a causal structure\nlearning method that can be efficiently trained using gradient-based\noptimization, by leveraging a smooth characterization on acyclicity and the\nGumbel-Softmax approach to approximate the binary adjacency matrix. It is found\nthat the obtained entries are typically near zero or one and can be easily\nthresholded to identify the edges. We conduct experiments on synthetic and real\ndatasets to validate the effectiveness of the proposed method, and show that it\nreadily includes different smooth model functions and achieves a much improved\nperformance on most datasets considered.",
          "link": "http://arxiv.org/abs/1910.08527",
          "publishedOn": "2022-01-12T00:38:45.576Z",
          "wordCount": null,
          "title": "Masked Gradient-Based Causal Structure Learning. (arXiv:1910.08527v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Arindam Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tiancong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingxue Zhou</a>",
          "description": "We study generalization bounds for noisy stochastic mini-batch iterative\nalgorithms based on the notion of stability. Recent years have seen key\nadvances in data-dependent generalization bounds for noisy iterative learning\nalgorithms such as stochastic gradient Langevin dynamics (SGLD) based on\nstability (Mou et al., 2018; Li et al., 2020) and information theoretic\napproaches (Xu and Raginsky, 2017; Negrea et al., 2019; Steinke and\nZakynthinou, 2020; Haghifam et al., 2020). In this paper, we unify and\nsubstantially generalize stability based generalization bounds and make three\ntechnical advances. First, we bound the generalization error of general noisy\nstochastic iterative algorithms (not necessarily gradient descent) in terms of\nexpected (not uniform) stability. The expected stability can in turn be bounded\nby a Le Cam Style Divergence. Such bounds have a O(1/n) sample dependence\nunlike many existing bounds with O(1/\\sqrt{n}) dependence. Second, we introduce\nExponential Family Langevin Dynamics(EFLD) which is a substantial\ngeneralization of SGLD and which allows exponential family noise to be used\nwith stochastic gradient descent (SGD). We establish data-dependent expected\nstability based generalization bounds for general EFLD algorithms. Third, we\nconsider an important special case of EFLD: noisy sign-SGD, which extends\nsign-SGD using Bernoulli noise over {-1,+1}. Generalization bounds for noisy\nsign-SGD are implied by that of EFLD and we also establish optimization\nguarantees for the algorithm. Further, we present empirical results on\nbenchmark datasets to illustrate that our bounds are non-vacuous and\nquantitatively much sharper than existing bounds.",
          "link": "http://arxiv.org/abs/2201.03064",
          "publishedOn": "2022-01-12T00:38:45.540Z",
          "wordCount": 662,
          "title": "Stability Based Generalization Bounds for Exponential Family Langevin Dynamics. (arXiv:2201.03064v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02942",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Yang_B/0/1/0/all/0/1\">Bin Yang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Feng_J/0/1/0/all/0/1\">Jinglang Feng</a>, <a href=\"http://arxiv.org/find/math/1/au:+Vasile_M/0/1/0/all/0/1\">Massimiliano Vasile</a>",
          "description": "This paper presents a novel and fast solver for the J2-perturbed Lambert\nproblem. The solver consists of an intelligent initial guess generator combined\nwith a differential correction procedure. The intelligent initial guess\ngenerator is a deep neural network that is trained to correct the initial\nvelocity vector coming from the solution of the unperturbed Lambert problem.\nThe differential correction module takes the initial guess and uses a forward\nshooting procedure to further update the initial velocity and exactly meet the\nterminal conditions. Eight sample forms are analyzed and compared to find the\noptimum form to train the neural network on the J2-perturbed Lambert problem.\nThe accuracy and performance of this novel approach will be demonstrated on a\nrepresentative test case: the solution of a multi-revolution J2-perturbed\nLambert problem in the Jupiter system. We will compare the performance of the\nproposed approach against a classical standard shooting method and a\nhomotopy-based perturbed Lambert algorithm. It will be shown that, for a\ncomparable level of accuracy, the proposed method is significantly faster than\nthe other two.",
          "link": "http://arxiv.org/abs/2201.02942",
          "publishedOn": "2022-01-12T00:38:45.530Z",
          "wordCount": 612,
          "title": "Fast solver for J2-perturbed Lambert problem using deep neural network. (arXiv:2201.02942v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soni_H/0/1/0/all/0/1\">Himanshu Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raunak Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laban_R/0/1/0/all/0/1\">Ronald Melwin Laban</a>",
          "description": "A lot of prognostication methodologies have been formulated for early\ndetection of Polycystic Ovary Syndrome also known as PCOS using Machine\nLearning. PCOS is a binary classification problem. Dimensionality Reduction\nmethods impact the performance of Machine Learning to a greater extent and\nusing a Supervised Dimensionality Reduction method can give us a new edge to\ntackle this problem. In this paper we present Discriminant Analysis in\ndifferent dimensions with Linear and Quadratic form for binary classification\nalong with metrics. We were able to achieve good accuracy and less variation\nwith Discriminant Analysis as compared to many commonly used classification\nalgorithms with training accuracy reaching 97.37% and testing accuracy of\n95.92% using Quadratic Discriminant Analysis. Paper also gives the analysis of\ndata with visualizations for deeper understanding of problem.",
          "link": "http://arxiv.org/abs/2201.03029",
          "publishedOn": "2022-01-12T00:38:45.523Z",
          "wordCount": 558,
          "title": "Discriminant Analysis in Contrasting Dimensions for Polycystic Ovary Syndrome Prognostication. (arXiv:2201.03029v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2111.12602",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bourached_A/0/1/0/all/0/1\">Anthony Bourached</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_R/0/1/0/all/0/1\">Robert Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_R/0/1/0/all/0/1\">Ryan-Rhys Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Ashwani Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachev_P/0/1/0/all/0/1\">Parashkev Nachev</a>",
          "description": "Models of human motion commonly focus either on trajectory prediction or\naction classification but rarely both. The marked heterogeneity and intricate\ncompositionality of human motion render each task vulnerable to the data\ndegradation and distributional shift common to real-world scenarios. A\nsufficiently expressive generative model of action could in theory enable data\nconditioning and distributional resilience within a unified framework\napplicable to both tasks. Here we propose a novel architecture based on\nhierarchical variational autoencoders and deep graph convolutional neural\nnetworks for generating a holistic model of action over multiple time-scales.\nWe show this Hierarchical Graph-convolutional Variational Autoencoder (HG-VAE)\nto be capable of generating coherent actions, detecting out-of-distribution\ndata, and imputing missing data by gradient ascent on the model's posterior.\nTrained and evaluated on H3.6M and the largest collection of open source human\nmotion data, AMASS, we show HG-VAE can facilitate downstream discriminative\nlearning better than baseline models.",
          "link": "http://arxiv.org/abs/2111.12602",
          "publishedOn": "2022-01-12T00:38:45.516Z",
          "wordCount": 630,
          "title": "Hierarchical Graph-Convolutional Variational AutoEncoding for Generative Modelling of Human Motion. (arXiv:2111.12602v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03131",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Dhar_S/0/1/0/all/0/1\">Sanchari Dhar</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Shamir_L/0/1/0/all/0/1\">Lior Shamir</a>",
          "description": "Deep convolutional neural networks (DCNNs) have become the most common\nsolution for automatic image annotation due to their non-parametric nature,\ngood performance, and their accessibility through libraries such as TensorFlow.\nAmong other fields, DCNNs are also a common approach to the annotation of large\nastronomical image databases acquired by digital sky surveys. One of the main\ndownsides of DCNNs is the complex non-intuitive rules that make DCNNs act as a\n``black box\", providing annotations in a manner that is unclear to the user.\nTherefore, the user is often not able to know what information is used by the\nDCNNs for the classification. Here we demonstrate that the training of a DCNN\nis sensitive to the context of the training data such as the location of the\nobjects in the sky. We show that for basic classification of elliptical and\nspiral galaxies, the sky location of the galaxies used for training affects the\nbehavior of the algorithm, and leads to a small but consistent and\nstatistically significant bias. That bias exhibits itself in the form of\ncosmological-scale anisotropy in the distribution of basic galaxy morphology.\nTherefore, while DCNNs are powerful tools for annotating images of extended\nsources, the construction of training sets for galaxy morphology should take\ninto consideration more aspects than the visual appearance of the object. In\nany case, catalogs created with deep neural networks that exhibit signs of\ncosmological anisotropy should be interpreted with the possibility of\nconsistent bias.",
          "link": "http://arxiv.org/abs/2201.03131",
          "publishedOn": "2022-01-12T00:38:45.488Z",
          "wordCount": 690,
          "title": "Systematic biases when using deep neural networks for annotating large catalogs of astronomical images. (arXiv:2201.03131v1 [astro-ph.GA])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03053",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Oda_M/0/1/0/all/0/1\">Masahiro Oda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_T/0/1/0/all/0/1\">Tong Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hayashi_Y/0/1/0/all/0/1\">Yuichiro Hayashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Otake_Y/0/1/0/all/0/1\">Yoshito Otake</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hashimoto_M/0/1/0/all/0/1\">Masahiro Hashimoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akashi_T/0/1/0/all/0/1\">Toshiaki Akashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aoki_S/0/1/0/all/0/1\">Shigeki Aoki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mori_K/0/1/0/all/0/1\">Kensaku Mori</a>",
          "description": "This paper proposes a segmentation method of infection regions in the lung\nfrom CT volumes of COVID-19 patients. COVID-19 spread worldwide, causing many\ninfected patients and deaths. CT image-based diagnosis of COVID-19 can provide\nquick and accurate diagnosis results. An automated segmentation method of\ninfection regions in the lung provides a quantitative criterion for diagnosis.\nPrevious methods employ whole 2D image or 3D volume-based processes. Infection\nregions have a considerable variation in their sizes. Such processes easily\nmiss small infection regions. Patch-based process is effective for segmenting\nsmall targets. However, selecting the appropriate patch size is difficult in\ninfection region segmentation. We utilize the scale uncertainty among various\nreceptive field sizes of a segmentation FCN to obtain infection regions. The\nreceptive field sizes can be defined as the patch size and the resolution of\nvolumes where patches are clipped from. This paper proposes an infection\nsegmentation network (ISNet) that performs patch-based segmentation and a scale\nuncertainty-aware prediction aggregation method that refines the segmentation\nresult. We design ISNet to segment infection regions that have various\nintensity values. ISNet has multiple encoding paths to process patch volumes\nnormalized by multiple intensity ranges. We collect prediction results\ngenerated by ISNets having various receptive field sizes. Scale uncertainty\namong the prediction results is extracted by the prediction aggregation method.\nWe use an aggregation FCN to generate a refined segmentation result considering\nscale uncertainty among the predictions. In our experiments using 199 chest CT\nvolumes of COVID-19 cases, the prediction aggregation method improved the dice\nsimilarity score from 47.6% to 62.1%.",
          "link": "http://arxiv.org/abs/2201.03053",
          "publishedOn": "2022-01-12T00:38:45.478Z",
          "wordCount": 784,
          "title": "COVID-19 Infection Segmentation from Chest CT Images Based on Scale Uncertainty. (arXiv:2201.03053v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_S/0/1/0/all/0/1\">Sen-ching Samson Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuah_C/0/1/0/all/0/1\">Chen-Nee Chuah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozonoff_S/0/1/0/all/0/1\">Sally Ozonoff</a>",
          "description": "To protect sensitive data in training a Generative Adversarial Network (GAN),\nthe standard approach is to use differentially private (DP) stochastic gradient\ndescent method in which controlled noise is added to the gradients. The quality\nof the output synthetic samples can be adversely affected and the training of\nthe network may not even converge in the presence of these noises. We propose\nDifferentially Private Model Inversion (DPMI) method where the private data is\nfirst mapped to the latent space via a public generator, followed by a\nlower-dimensional DP-GAN with better convergent properties. Experimental\nresults on standard datasets CIFAR10 and SVHN as well as on a facial landmark\ndataset for Autism screening show that our approach outperforms the standard\nDP-GAN method based on Inception Score, Fr\\'echet Inception Distance, and\nclassification accuracy under the same privacy guarantee.",
          "link": "http://arxiv.org/abs/2201.03139",
          "publishedOn": "2022-01-12T00:38:45.466Z",
          "wordCount": 576,
          "title": "Differentially Private Generative Adversarial Networks with Model Inversion. (arXiv:2201.03139v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02665",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moriano_P/0/1/0/all/0/1\">Pablo Moriano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bridges_R/0/1/0/all/0/1\">Robert A. Bridges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iannacone_M/0/1/0/all/0/1\">Michael D. Iannacone</a>",
          "description": "Vehicular Controller Area Networks (CANs) are susceptible to cyber attacks of\ndifferent levels of sophistication. Fabrication attacks are the easiest to\nadminister -- an adversary simply sends (extra) frames on a CAN -- but also the\neasiest to detect because they disrupt frame frequency. To overcome time-based\ndetection methods, adversaries must administer masquerade attacks by sending\nframes in lieu of (and therefore at the expected time of) benign frames but\nwith malicious payloads. Research efforts have proven that CAN attacks, and\nmasquerade attacks in particular, can affect vehicle functionality. Examples\ninclude causing unintended acceleration, deactivation of vehicle's brakes, as\nwell as steering the vehicle. We hypothesize that masquerade attacks modify the\nnuanced correlations of CAN signal time series and how they cluster together.\nTherefore, changes in cluster assignments should indicate anomalous behavior.\nWe confirm this hypothesis by leveraging our previously developed capability\nfor reverse engineering CAN signals (i.e., CAN-D [Controller Area Network\nDecoder]) and focus on advancing the state of the art for detecting masquerade\nattacks by analyzing time series extracted from raw CAN frames. Specifically,\nwe demonstrate that masquerade attacks can be detected by computing time series\nclustering similarity using hierarchical clustering on the vehicle's CAN\nsignals (time series) and comparing the clustering similarity across CAN\ncaptures with and without attacks. We test our approach in a previously\ncollected CAN dataset with masquerade attacks (i.e., the ROAD dataset) and\ndevelop a forensic tool as a proof of concept to demonstrate the potential of\nthe proposed approach for detecting CAN masquerade attacks.",
          "link": "http://arxiv.org/abs/2201.02665",
          "publishedOn": "2022-01-12T00:38:45.458Z",
          "wordCount": 685,
          "title": "Detecting CAN Masquerade Attacks with Signal Clustering Similarity. (arXiv:2201.02665v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Needell_C/0/1/0/all/0/1\">Coen D. Needell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bainbridge_W/0/1/0/all/0/1\">Wilma A. Bainbridge</a>",
          "description": "Various work has suggested that the memorability of an image is consistent\nacross people, and thus can be treated as an intrinsic property of an image.\nUsing computer vision models, we can make specific predictions about what\npeople will remember or forget. While older work has used now-outdated deep\nlearning architectures to predict image memorability, innovations in the field\nhave given us new techniques to apply to this problem. Here, we propose and\nevaluate five alternative deep learning models which exploit developments in\nthe field from the last five years, largely the introduction of residual neural\nnetworks, which are intended to allow the model to use semantic information in\nthe memorability estimation process. These new models were tested against the\nprior state of the art with a combined dataset built to optimize both\nwithin-category and across-category predictions. Our findings suggest that the\nkey prior memorability network had overstated its generalizability and was\noverfit on its training set. Our new models outperform this prior model,\nleading us to conclude that Residual Networks outperform simpler convolutional\nneural networks in memorability regression. We make our new state-of-the-art\nmodel readily available to the research community, allowing memory researchers\nto make predictions about memorability on a wider range of images.",
          "link": "http://arxiv.org/abs/2105.10598",
          "publishedOn": "2022-01-12T00:38:45.447Z",
          "wordCount": 689,
          "title": "Embracing New Techniques in Deep Learning for Estimating Image Memorability. (arXiv:2105.10598v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02967",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Houdouin_P/0/1/0/all/0/1\">Pierre Houdouin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pascal_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Pascal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jonckheere_M/0/1/0/all/0/1\">Matthieu Jonckheere</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_A/0/1/0/all/0/1\">Andrew Wang</a>",
          "description": "Linear and Quadratic Discriminant Analysis are well-known classical methods\nbut can heavily suffer from non-Gaussian distributions and/or contaminated\ndatasets, mainly because of the underlying Gaussian assumption that is not\nrobust. To fill this gap, this paper presents a new robust discriminant\nanalysis where each data point is drawn by its own arbitrary Elliptically\nSymmetrical (ES) distribution and its own arbitrary scale parameter. Such a\nmodel allows for possibly very heterogeneous, independent but non-identically\ndistributed samples. After deriving a new decision rule, it is shown that\nmaximum-likelihood parameter estimation and classification are very simple,\nfast and robust compared to state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2201.02967",
          "publishedOn": "2022-01-12T00:38:45.418Z",
          "wordCount": 532,
          "title": "Robust classification with flexible discriminant analysis in heterogeneous data. (arXiv:2201.02967v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11627",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cai_Z/0/1/0/all/0/1\">Zhiqiang Cai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chen_J/0/1/0/all/0/1\">Jingshuang Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_M/0/1/0/all/0/1\">Min Liu</a>",
          "description": "We introduced the least-squares ReLU neural network (LSNN) method for solving\nthe linear advection-reaction problem with discontinuous solution and showed\nthat the method outperforms mesh-based numerical methods in terms of the number\nof degrees of freedom. This paper studies the LSNN method for scalar nonlinear\nhyperbolic conservation law. The method is a discretization of an equivalent\nleast-squares (LS) formulation in the set of neural network functions with the\nReLU activation function. Evaluation of the LS functional is done by using\nnumerical integration and conservative finite volume scheme. Numerical results\nof some test problems show that the method is capable of approximating the\ndiscontinuous interface of the underlying problem automatically through the\nfree breaking lines of the ReLU neural network. Moreover, the method does not\nexhibit the common Gibbs phenomena along the discontinuous interface.",
          "link": "http://arxiv.org/abs/2105.11627",
          "publishedOn": "2022-01-12T00:38:45.410Z",
          "wordCount": 595,
          "title": "Least-Squares ReLU Neural Network (LSNN) Method For Scalar Nonlinear Hyperbolic Conservation Law. (arXiv:2105.11627v3 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Siddhant_A/0/1/0/all/0/1\">Aditya Siddhant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mia Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caswell_I/0/1/0/all/0/1\">Isaac Caswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>",
          "description": "Achieving universal translation between all human language pairs is the\nholy-grail of machine translation (MT) research. While recent progress in\nmassively multilingual MT is one step closer to reaching this goal, it is\nbecoming evident that extending a multilingual MT system simply by training on\nmore parallel data is unscalable, since the availability of labeled data for\nlow-resource and non-English-centric language pairs is forbiddingly limited. To\nthis end, we present a pragmatic approach towards building a multilingual MT\nmodel that covers hundreds of languages, using a mixture of supervised and\nself-supervised objectives, depending on the data availability for different\nlanguage pairs. We demonstrate that the synergy between these two training\nparadigms enables the model to produce high-quality translations in the\nzero-resource setting, even surpassing supervised translation quality for low-\nand mid-resource languages. We conduct a wide array of experiments to\nunderstand the effect of the degree of multilingual supervision, domain\nmismatches and amounts of parallel and monolingual data on the quality of our\nself-supervised multilingual models. To demonstrate the scalability of the\napproach, we train models with over 200 languages and demonstrate high\nperformance on zero-resource translation on several previously under-studied\nlanguages. We hope our findings will serve as a stepping stone towards enabling\ntranslation for the next thousand languages.",
          "link": "http://arxiv.org/abs/2201.03110",
          "publishedOn": "2022-01-12T00:38:45.403Z",
          "wordCount": 658,
          "title": "Towards the Next 1000 Languages in Multilingual Machine Translation: Exploring the Synergy Between Supervised and Self-Supervised Learning. (arXiv:2201.03110v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03035",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poellabauer_C/0/1/0/all/0/1\">Christian Poellabauer</a>",
          "description": "Medication errors most commonly occur at the ordering or prescribing stage,\npotentially leading to medical complications and poor health outcomes. While it\nis possible to catch these errors using different techniques; the focus of this\nwork is on textual and contextual analysis of prescription information to\ndetect and prevent potential medication errors. In this paper, we demonstrate\nhow to use BERT-based contextual language models to detect anomalies in written\nor spoken text based on a data set extracted from real-world medical data of\nthousands of patient records. The proposed models are able to learn patterns of\ntext dependency and predict erroneous output based on contextual information\nsuch as patient data. The experimental results yield accuracy up to 96.63% for\ntext input and up to 79.55% for speech input, which is satisfactory for most\nreal-world applications.",
          "link": "http://arxiv.org/abs/2201.03035",
          "publishedOn": "2022-01-12T00:38:45.394Z",
          "wordCount": 555,
          "title": "Medication Error Detection Using Contextual Language Models. (arXiv:2201.03035v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02627",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Teh_E/0/1/0/all/0/1\">Eu Wern Teh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taylor_G/0/1/0/all/0/1\">Graham W. Taylor</a>",
          "description": "A critical challenge of training deep learning models in the Digital\nPathology (DP) domain is the high annotation cost by medical experts. One way\nto tackle this issue is via transfer learning from the natural image domain\n(NI), where the annotation cost is considerably cheaper. Cross-domain transfer\nlearning from NI to DP is shown to be successful via class\nlabels~\\cite{teh2020learning}. One potential weakness of relying on class\nlabels is the lack of spatial information, which can be obtained from spatial\nlabels such as full pixel-wise segmentation labels and scribble labels. We\ndemonstrate that scribble labels from NI domain can boost the performance of DP\nmodels on two cancer classification datasets (Patch Camelyon Breast Cancer and\nColorectal Cancer dataset). Furthermore, we show that models trained with\nscribble labels yield the same performance boost as full pixel-wise\nsegmentation labels despite being significantly easier and faster to collect.",
          "link": "http://arxiv.org/abs/2201.02627",
          "publishedOn": "2022-01-12T00:38:45.369Z",
          "wordCount": 602,
          "title": "Learning with less labels in Digital Pathology via Scribble Supervision from natural images. (arXiv:2201.02627v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2111.08922",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shaojie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaughan_J/0/1/0/all/0/1\">Joel Vaughan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudjianto_A/0/1/0/all/0/1\">Agus Sudjianto</a>",
          "description": "Although neural networks (NNs) with ReLU activation functions have found\nsuccess in a wide range of applications, their adoption in risk-sensitive\nsettings has been limited by the concerns on robustness and interpretability.\nPrevious works to examine robustness and to improve interpretability partially\nexploited the piecewise linear function form of ReLU NNs. In this paper, we\nexplore the unique topological structure that ReLU NNs create in the input\nspace, identifying the adjacency among the partitioned local polytopes and\ndeveloping a traversing algorithm based on this adjacency. Our polytope\ntraversing algorithm can be adapted to verify a wide range of network\nproperties related to robustness and interpretability, providing an unified\napproach to examine the network behavior. As the traversing algorithm\nexplicitly visits all local polytopes, it returns a clear and full picture of\nthe network behavior within the traversed region. The time and space complexity\nof the traversing algorithm is determined by the number of a ReLU NN's\npartitioning hyperplanes passing through the traversing region.",
          "link": "http://arxiv.org/abs/2111.08922",
          "publishedOn": "2022-01-12T00:38:45.361Z",
          "wordCount": 634,
          "title": "Traversing the Local Polytopes of ReLU Neural Networks: A Unified Approach for Network Verification. (arXiv:2111.08922v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.06910",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_F/0/1/0/all/0/1\">Fanghui Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Suykens_J/0/1/0/all/0/1\">Johan A.K. Suykens</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cevher_V/0/1/0/all/0/1\">Volkan Cevher</a>",
          "description": "This paper studies generalization properties of random features (RF)\nregression in high dimensions optimized by stochastic gradient descent (SGD).\nIn this regime, we derive precise non-asymptotic error bounds of RF regression\nunder both constant and adaptive step-size SGD setting, and observe the double\ndescent phenomenon both theoretically and empirically. Our analysis shows how\nto cope with multiple randomness sources of initialization, label noise, and\ndata sampling (as well as stochastic gradients) with no closed-form solution,\nand also goes beyond the commonly-used Gaussian/spherical data assumption. Our\ntheoretical results demonstrate that, with SGD training, RF regression still\ngeneralizes well for interpolation learning, and is able to characterize the\ndouble descent behavior by the unimodality of variance and monotonic decrease\nof bias. Besides, we also prove that the constant step-size SGD setting incurs\nno loss in convergence rate when compared to the exact minimal-norm\ninterpolator, as a theoretical justification of using SGD in practice.",
          "link": "http://arxiv.org/abs/2110.06910",
          "publishedOn": "2022-01-12T00:38:45.352Z",
          "wordCount": 629,
          "title": "On the Double Descent of Random Features Models Trained with SGD. (arXiv:2110.06910v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lingfeng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>",
          "description": "Recently, it has been shown that natural language processing (NLP) models are\nvulnerable to a kind of security threat called the Backdoor Attack, which\nutilizes a `backdoor trigger' paradigm to mislead the models. The most\nthreatening backdoor attack is the stealthy backdoor, which defines the\ntriggers as text style or syntactic. Although they have achieved an incredible\nhigh attack success rate (ASR), we find that the principal factor contributing\nto their ASR is not the `backdoor trigger' paradigm. Thus the capacity of these\nstealthy backdoor attacks is overestimated when categorized as backdoor\nattacks. Therefore, to evaluate the real attack power of backdoor attacks, we\npropose a new metric called attack successful rate difference (ASRD), which\nmeasures the ASR difference between clean state and poison state models.\nBesides, since the defenses against stealthy backdoor attacks are absent, we\npropose Trigger Breaker, consisting of two too simple tricks that can defend\nagainst stealthy backdoor attacks effectively. Experiments on text\nclassification tasks show that our method achieves significantly better\nperformance than state-of-the-art defense methods against stealthy backdoor\nattacks.",
          "link": "http://arxiv.org/abs/2201.02993",
          "publishedOn": "2022-01-12T00:38:45.344Z",
          "wordCount": 602,
          "title": "Rethink Stealthy Backdoor Attacks in Natural Language Processing. (arXiv:2201.02993v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>",
          "description": "We present CSWin Transformer, an efficient and effective Transformer-based\nbackbone for general-purpose vision tasks. A challenging issue in Transformer\ndesign is that global self-attention is very expensive to compute whereas local\nself-attention often limits the field of interactions of each token. To address\nthis issue, we develop the Cross-Shaped Window self-attention mechanism for\ncomputing self-attention in the horizontal and vertical stripes in parallel\nthat form a cross-shaped window, with each stripe obtained by splitting the\ninput feature into stripes of equal width. We provide a mathematical analysis\nof the effect of the stripe width and vary the stripe width for different\nlayers of the Transformer network which achieves strong modeling capability\nwhile limiting the computation cost. We also introduce Locally-enhanced\nPositional Encoding (LePE), which handles the local positional information\nbetter than existing encoding schemes. LePE naturally supports arbitrary input\nresolutions, and is thus especially effective and friendly for downstream\ntasks. Incorporated with these designs and a hierarchical structure, CSWin\nTransformer demonstrates competitive performance on common vision tasks.\nSpecifically, it achieves 85.4\\% Top-1 accuracy on ImageNet-1K without any\nextra training data or label, 53.9 box AP and 46.4 mask AP on the COCO\ndetection task, and 52.2 mIOU on the ADE20K semantic segmentation task,\nsurpassing previous state-of-the-art Swin Transformer backbone by +1.2, +2.0,\n+1.4, and +2.0 respectively under the similar FLOPs setting. By further\npretraining on the larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy\non ImageNet-1K and high segmentation performance on ADE20K with 55.7 mIoU. The\ncode and models are available at\nhttps://github.com/microsoft/CSWin-Transformer.",
          "link": "http://arxiv.org/abs/2107.00652",
          "publishedOn": "2022-01-12T00:38:45.337Z",
          "wordCount": 734,
          "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. (arXiv:2107.00652v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04831",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pfitzinger_J/0/1/0/all/0/1\">Johann Pfitzinger</a>",
          "description": "This paper proposes a novel graph-based regularized regression estimator -\nthe hierarchical feature regression (HFR) -, which mobilizes insights from the\ndomains of machine learning and graph theory to estimate robust parameters for\na linear regression. The estimator constructs a supervised feature graph that\ndecomposes parameters along its edges, adjusting first for common variation and\nsuccessively incorporating idiosyncratic patterns into the fitting process. The\ngraph structure has the effect of shrinking parameters towards group targets,\nwhere the extent of shrinkage is governed by a hyperparamter, and group\ncompositions as well as shrinkage targets are determined endogenously. The\nmethod offers rich resources for the visual exploration of the latent effect\nstructure in the data, and demonstrates good predictive accuracy and\nversatility when compared to a panel of commonly used regularization techniques\nacross a range of empirical and simulated regression tasks.",
          "link": "http://arxiv.org/abs/2107.04831",
          "publishedOn": "2022-01-12T00:38:45.328Z",
          "wordCount": 576,
          "title": "Cluster Regularization via a Hierarchical Feature Regression. (arXiv:2107.04831v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_G/0/1/0/all/0/1\">Guangdong Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Q/0/1/0/all/0/1\">Qin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_N/0/1/0/all/0/1\">Nikhil R. Pal</a>",
          "description": "A major limitation of fuzzy or neuro-fuzzy systems is their failure to deal\nwith high-dimensional datasets. This happens primarily due to the use of\nT-norm, particularly, product or minimum (or a softer version of it). Thus,\nthere are hardly any work dealing with datasets with dimensions more than\nhundred or so. Here, we propose a neuro-fuzzy framework that can handle\ndatasets with dimensions even more than 7000! In this context, we propose an\nadaptive softmin (Ada-softmin) which effectively overcomes the drawbacks of\n``numeric underflow\" and ``fake minimum\" that arise for existing fuzzy systems\nwhile dealing with high-dimensional problems. We call it an Adaptive\nTakagi-Sugeno-Kang (AdaTSK) fuzzy system. We then equip the AdaTSK system to\nperform feature selection and rule extraction in an integrated manner. In this\ncontext, a novel gate function is introduced and embedded only in the\nconsequent parts, which can determine the useful features and rules, in two\nsuccessive phases of learning. Unlike conventional fuzzy rule bases, we design\nan enhanced fuzzy rule base (En-FRB), which maintains adequate rules but does\nnot grow the number of rules exponentially with dimension that typically\nhappens for fuzzy neural networks. The integrated Feature Selection and Rule\nExtraction AdaTSK (FSRE-AdaTSK) system consists of three sequential phases: (i)\nfeature selection, (ii) rule extraction, and (iii) fine tuning. The\neffectiveness of the FSRE-AdaTSK is demonstrated on 19 datasets of which five\nare in more than 2000 dimension including two with dimension greater than 7000.\nThis may be the first time fuzzy systems are realized for classification\ninvolving more than 7000 input features.",
          "link": "http://arxiv.org/abs/2201.03187",
          "publishedOn": "2022-01-12T00:38:45.277Z",
          "wordCount": 696,
          "title": "An Adaptive Neuro-Fuzzy System with Integrated Feature Selection and Rule Extraction for High-Dimensional Classification Problems. (arXiv:2201.03187v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jamgochian_A/0/1/0/all/0/1\">Arec Jamgochian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menda_K/0/1/0/all/0/1\">Kunal Menda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Soyeon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1\">Mykel J. Kochenderfer</a>",
          "description": "Some real-world decision-making problems require making probabilistic\nforecasts over multiple steps at once. However, methods for probabilistic\nforecasting may fail to capture correlations in the underlying time-series that\nexist over long time horizons as errors accumulate. One such application is\nwith resource scheduling under uncertainty in a grid environment, which\nrequires forecasting electricity demand that is inherently noisy, but often\ncyclic. In this paper, we introduce the conditional approximate normalizing\nflow (CANF) to make probabilistic multi-step time-series forecasts when\ncorrelations are present over long time horizons. We first demonstrate our\nmethod's efficacy on estimating the density of a toy distribution, finding that\nCANF improves the KL divergence by one-third compared to that of a Gaussian\nmixture model while still being amenable to explicit conditioning. We then use\na publicly available household electricity consumption dataset to showcase the\neffectiveness of CANF on joint probabilistic multi-step forecasting. Empirical\nresults show that conditional approximate normalizing flows outperform other\nmethods in terms of multi-step forecast accuracy and lead to up to 10x better\nscheduling decisions. Our implementation is available at\nhttps://github.com/sisl/JointDemandForecasting.",
          "link": "http://arxiv.org/abs/2201.02753",
          "publishedOn": "2022-01-12T00:38:45.268Z",
          "wordCount": 605,
          "title": "Conditional Approximate Normalizing Flows for Joint Multi-Step Probabilistic Electricity Demand Forecasting. (arXiv:2201.02753v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03169",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyuan Zhang</a>",
          "description": "Applying knowledge distillation to personalized cross-silo federated learning\ncan well alleviate the problem of user heterogeneity. This approach, however,\nrequires a proxy dataset, which is difficult to obtain in the real world.\nMoreover, the global model based on parameter averaging will lead to the\nleakage of user privacy. We introduce a distributed three-player GAN to\nimplement datafree co-distillation between clients. This technique mitigates\nthe user heterogeneity problem and better protects user privacy. We confirmed\nthat thefake samples generated by GAN can make federated distillation more\nefficient and robust, and the co-distillation can achieve good performance for\nindividual clients on the basis of obtaining global knowledge. Our extensive\nexperiments on benchmark datasets demonstrate the superior generalization\nperformance of the proposed methods, compared with the state-of-the-art.",
          "link": "http://arxiv.org/abs/2201.03169",
          "publishedOn": "2022-01-12T00:38:45.261Z",
          "wordCount": 536,
          "title": "FedDTG:Federated Data-Free Knowledge Distillation via Three-Player Generative Adversarial Networks. (arXiv:2201.03169v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kelchtermans_K/0/1/0/all/0/1\">Klaas Kelchtermans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1\">Tinne Tuytelaars</a>",
          "description": "The gap between simulation and the real-world restrains many machine learning\nbreakthroughs in computer vision and reinforcement learning from being\napplicable in the real world. In this work, we tackle this gap for the specific\ncase of camera-based navigation, formulating it as following a visual cue in\nthe foreground with arbitrary backgrounds. The visual cue in the foreground can\noften be simulated realistically, such as a line, gate or cone. The challenge\nthen lies in coping with the unknown backgrounds and integrating both. As such,\nthe goal is to train a visual agent on data captured in an empty simulated\nenvironment except for this foreground cue and test this model directly in a\nvisually diverse real world. In order to bridge this big gap, we show it's\ncrucial to combine following techniques namely: Randomized augmentation of the\nfore- and background, regularization with both deep supervision and triplet\nloss and finally abstraction of the dynamics by using waypoints rather than\ndirect velocity commands. The various techniques are ablated in our\nexperimental results both qualitatively and quantitatively finally\ndemonstrating a successful transfer from simulation to the real world.",
          "link": "http://arxiv.org/abs/2201.02798",
          "publishedOn": "2022-01-12T00:38:45.201Z",
          "wordCount": 623,
          "title": "RARA: Zero-shot Sim2Real Visual Navigation with Following Foreground Cues. (arXiv:2201.02798v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2112.04216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Celik_O/0/1/0/all/0/1\">Onur Celik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongzhuoran Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becker_P/0/1/0/all/0/1\">Philipp Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1\">Gerhard Neumann</a>",
          "description": "A long-cherished vision in robotics is to equip robots with skills that match\nthe versatility and precision of humans. For example, when playing table\ntennis, a robot should be capable of returning the ball in various ways while\nprecisely placing it at the desired location. A common approach to model such\nversatile behavior is to use a Mixture of Experts (MoE) model, where each\nexpert is a contextual motion primitive. However, learning such MoEs is\nchallenging as most objectives force the model to cover the entire context\nspace, which prevents specialization of the primitives resulting in rather\nlow-quality components. Starting from maximum entropy reinforcement learning\n(RL), we decompose the objective into optimizing an individual lower bound per\nmixture component. Further, we introduce a curriculum by allowing the\ncomponents to focus on a local context region, enabling the model to learn\nhighly accurate skill representations. To this end, we use local context\ndistributions that are adapted jointly with the expert primitives. Our lower\nbound advocates an iterative addition of new components, where new components\nwill concentrate on local context regions not covered by the current MoE. This\nlocal and incremental learning results in a modular MoE model of high accuracy\nand versatility, where both properties can be scaled by adding more components\non the fly. We demonstrate this by an extensive ablation and on two challenging\nsimulated robot skill learning tasks. We compare our achieved performance to\nLaDiPS and HiREPS, a known hierarchical policy search method for learning\ndiverse skills.",
          "link": "http://arxiv.org/abs/2112.04216",
          "publishedOn": "2022-01-12T00:38:45.190Z",
          "wordCount": 707,
          "title": "Specializing Versatile Skill Libraries using Local Mixture of Experts. (arXiv:2112.04216v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wen-Chi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raskin_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Raskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raedt_L/0/1/0/all/0/1\">Luc De Raedt</a>",
          "description": "Probabilistic model checking has been developed for verifying systems that\nhave stochastic and nondeterministic behavior. Given a probabilistic system, a\nprobabilistic model checker takes a property and checks whether or not the\nproperty holds in that system. For this reason, probabilistic model checking\nprovide rigorous guarantees. So far, however, probabilistic model checking has\nfocused on propositional models where a state is represented by a symbol. On\nthe other hand, it is commonly required to make relational abstractions in\nplanning and reinforcement learning. Various frameworks handle relational\ndomains, for instance, STRIPS planning and relational Markov Decision\nProcesses. Using propositional model checking in relational settings requires\none to ground the model, which leads to the well known state explosion problem\nand intractability. We present pCTL-REBEL, a lifted model checking approach for\nverifying pCTL properties of relational MDPs. It extends REBEL, a relational\nmodel-based reinforcement learning technique, toward relational pCTL model\nchecking. PCTL-REBEL is lifted, which means that rather than grounding, the\nmodel exploits symmetries to reason about a group of objects as a whole at the\nrelational level. Theoretically, we show that pCTL model checking is decidable\nfor relational MDPs that have a possibly infinite domain, provided that the\nstates have a bounded size. Practically, we contribute algorithms and an\nimplementation of lifted relational model checking, and we show that the lifted\napproach improves the scalability of the model checking approach.",
          "link": "http://arxiv.org/abs/2106.11735",
          "publishedOn": "2022-01-12T00:38:45.094Z",
          "wordCount": null,
          "title": "Lifted Model Checking for Relational MDPs. (arXiv:2106.11735v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duddu_V/0/1/0/all/0/1\">Vasisht Duddu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutet_A/0/1/0/all/0/1\">Antoine Boutet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shejwalkar_V/0/1/0/all/0/1\">Virat Shejwalkar</a>",
          "description": "Embedded systems demand on-device processing of data using Neural Networks\n(NNs) while conforming to the memory, power and computation constraints,\nleading to an efficiency and accuracy tradeoff. To bring NNs to edge devices,\nseveral optimizations such as model compression through pruning, quantization,\nand off-the-shelf architectures with efficient design have been extensively\nadopted. These algorithms when deployed to real world sensitive applications,\nrequires to resist inference attacks to protect privacy of users training data.\nHowever, resistance against inference attacks is not accounted for designing NN\nmodels for IoT. In this work, we analyse the three-dimensional\nprivacy-accuracy-efficiency tradeoff in NNs for IoT devices and propose Gecko\ntraining methodology where we explicitly add resistance to private inferences\nas a design objective. We optimize the inference-time memory, computation, and\npower constraints of embedded devices as a criterion for designing NN\narchitecture while also preserving privacy. We choose quantization as design\nchoice for highly efficient and private models. This choice is driven by the\nobservation that compressed models leak more information compared to baseline\nmodels while off-the-shelf efficient architectures indicate poor efficiency and\nprivacy tradeoff. We show that models trained using Gecko methodology are\ncomparable to prior defences against black-box membership attacks in terms of\naccuracy and privacy while providing efficiency.",
          "link": "http://arxiv.org/abs/2010.00912",
          "publishedOn": "2022-01-12T00:38:45.085Z",
          "wordCount": null,
          "title": "GECKO: Reconciling Privacy, Accuracy and Efficiency in Embedded Deep Learning. (arXiv:2010.00912v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.07272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Slivkins_A/0/1/0/all/0/1\">Aleksandrs Slivkins</a>",
          "description": "Multi-armed bandits a simple but very powerful framework for algorithms that\nmake decisions over time under uncertainty. An enormous body of work has\naccumulated over the years, covered in several books and surveys. This book\nprovides a more introductory, textbook-like treatment of the subject. Each\nchapter tackles a particular line of work, providing a self-contained,\nteachable technical introduction and a brief review of the further\ndevelopments; many of the chapters conclude with exercises.\n\nThe book is structured as follows. The first four chapters are on IID\nrewards, from the basic model to impossibility results to Bayesian priors to\nLipschitz rewards. The next three chapters cover adversarial rewards, from the\nfull-feedback version to adversarial bandits to extensions with linear rewards\nand combinatorially structured actions. Chapter 8 is on contextual bandits, a\nmiddle ground between IID and adversarial bandits in which the change in reward\ndistributions is completely explained by observable contexts. The last three\nchapters cover connections to economics, from learning in repeated games to\nbandits with supply/budget constraints to exploration in the presence of\nincentives. The appendix provides sufficient background on concentration and\nKL-divergence.\n\nThe chapters on \"bandits with similarity information\", \"bandits with\nknapsacks\" and \"bandits and agents\" can also be consumed as standalone surveys\non the respective topics.",
          "link": "http://arxiv.org/abs/1904.07272",
          "publishedOn": "2022-01-12T00:38:44.992Z",
          "wordCount": 744,
          "title": "Introduction to Multi-Armed Bandits. (arXiv:1904.07272v7 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yulin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_K/0/1/0/all/0/1\">Kangchen Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haojun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenhui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_P/0/1/0/all/0/1\">Pengfei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>",
          "description": "Spatial redundancy widely exists in visual recognition tasks, i.e.,\ndiscriminative features in an image or video frame usually correspond to only a\nsubset of pixels, while the remaining regions are irrelevant to the task at\nhand. Therefore, static models which process all the pixels with an equal\namount of computation result in considerable redundancy in terms of time and\nspace consumption. In this paper, we formulate the image recognition problem as\na sequential coarse-to-fine feature learning process, mimicking the human\nvisual system. Specifically, the proposed Glance and Focus Network (GFNet)\nfirst extracts a quick global representation of the input image at a low\nresolution scale, and then strategically attends to a series of salient (small)\nregions to learn finer features. The sequential process naturally facilitates\nadaptive inference at test time, as it can be terminated once the model is\nsufficiently confident about its prediction, avoiding further redundant\ncomputation. It is worth noting that the problem of locating discriminant\nregions in our model is formulated as a reinforcement learning task, thus\nrequiring no additional manual annotations other than classification labels.\nGFNet is general and flexible as it is compatible with any off-the-shelf\nbackbone models (such as MobileNets, EfficientNets and TSM), which can be\nconveniently deployed as the feature extractor. Extensive experiments on a\nvariety of image classification and video recognition tasks and with various\nbackbone models demonstrate the remarkable efficiency of our method. For\nexample, it reduces the average latency of the highly efficient MobileNet-V3 on\nan iPhone XS Max by 1.3x without sacrificing accuracy. Code and pre-trained\nmodels are available at https://github.com/blackfeather-wang/GFNet-Pytorch.",
          "link": "http://arxiv.org/abs/2201.03014",
          "publishedOn": "2022-01-12T00:38:44.984Z",
          "wordCount": 712,
          "title": "Glance and Focus Networks for Dynamic Visual Recognition. (arXiv:2201.03014v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2112.08366",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Feng_R/0/1/0/all/0/1\">Ruiwei Feng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xie_Y/0/1/0/all/0/1\">Yufeng Xie</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lai_M/0/1/0/all/0/1\">Minshan Lai</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_D/0/1/0/all/0/1\">Danny Z. Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cao_J/0/1/0/all/0/1\">Ji Cao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>",
          "description": "Accurate drug response prediction (DRP) is a crucial yet challenging task in\nprecision medicine. This paper presents a novel Attention-Guided Multi-omics\nIntegration (AGMI) approach for DRP, which first constructs a Multi-edge Graph\n(MeG) for each cell line, and then aggregates multi-omics features to predict\ndrug response using a novel structure, called Graph edge-aware Network (GeNet).\nFor the first time, our AGMI approach explores gene constraint based\nmulti-omics integration for DRP with the whole-genome using GNNs. Empirical\nexperiments on the CCLE and GDSC datasets show that our AGMI largely\noutperforms state-of-the-art DRP methods by 8.3%--34.2% on four metrics. Our\ndata and code are available at https://github.com/yivan-WYYGDSG/AGMI.",
          "link": "http://arxiv.org/abs/2112.08366",
          "publishedOn": "2022-01-12T00:38:44.946Z",
          "wordCount": 561,
          "title": "AGMI: Attention-Guided Multi-omics Integration for Drug Response Prediction with Graph Neural Networks. (arXiv:2112.08366v2 [q-bio.GN] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhenan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Huang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zirui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedlander_M/0/1/0/all/0/1\">Michael P. Friedlander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>",
          "description": "Federated learning is a popular technology for training machine learning\nmodels on distributed data sources without sharing data. Vertical federated\nlearning or feature-based federated learning applies to the cases that\ndifferent data sources share the same sample ID space but differ in feature\nspace. To ensure the data owners' long-term engagement, it is critical to\nobjectively assess the contribution from each data source and recompense them\naccordingly. The Shapley value (SV) is a provably fair contribution valuation\nmetric originated from cooperative game theory. However, computing the SV\nrequires extensively retraining the model on each subset of data sources, which\ncauses prohibitively high communication costs in federated learning. We propose\na contribution valuation metric called vertical federated Shapley value\n(VerFedSV) based on SV. We show that VerFedSV not only satisfies many desirable\nproperties for fairness but is also efficient to compute, and can be adapted to\nboth synchronous and asynchronous vertical federated learning algorithms. Both\ntheoretical analysis and extensive experimental results verify the fairness,\nefficiency, and adaptability of VerFedSV.",
          "link": "http://arxiv.org/abs/2201.02658",
          "publishedOn": "2022-01-12T00:38:44.378Z",
          "wordCount": 591,
          "title": "Fair and efficient contribution valuation for vertical federated learning. (arXiv:2201.02658v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gunel_B/0/1/0/all/0/1\">Beliz Gunel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potti_N/0/1/0/all/0/1\">Navneet Potti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tata_S/0/1/0/all/0/1\">Sandeep Tata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wendt_J/0/1/0/all/0/1\">James B. Wendt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najork_M/0/1/0/all/0/1\">Marc Najork</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jing Xie</a>",
          "description": "Automating information extraction from form-like documents at scale is a\npressing need due to its potential impact on automating business workflows\nacross many industries like financial services, insurance, and healthcare. The\nkey challenge is that form-like documents in these business workflows can be\nlaid out in virtually infinitely many ways; hence, a good solution to this\nproblem should generalize to documents with unseen layouts and languages. A\nsolution to this problem requires a holistic understanding of both the textual\nsegments and the visual cues within a document, which is non-trivial. While the\nnatural language processing and computer vision communities are starting to\ntackle this problem, there has not been much focus on (1) data-efficiency, and\n(2) ability to generalize across different document types and languages.\n\nIn this paper, we show that when we have only a small number of labeled\ndocuments for training (~50), a straightforward transfer learning approach from\na considerably structurally-different larger labeled corpus yields up to a 27\nF1 point improvement over simply training on the small corpus in the target\ndomain. We improve on this with a simple multi-domain transfer learning\napproach, that is currently in production use, and show that this yields up to\na further 8 F1 point improvement. We make the case that data efficiency is\ncritical to enable information extraction systems to scale to handle hundreds\nof different document-types, and learning good representations is critical to\naccomplishing this.",
          "link": "http://arxiv.org/abs/2201.02647",
          "publishedOn": "2022-01-12T00:38:44.365Z",
          "wordCount": 667,
          "title": "Data-Efficient Information Extraction from Form-Like Documents. (arXiv:2201.02647v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02812",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Peng_C/0/1/0/all/0/1\">Chong Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yongyong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xinxin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_A/0/1/0/all/0/1\">Andrew Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chenglizhao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_Q/0/1/0/all/0/1\">Qiang Cheng</a>",
          "description": "In this paper, we propose a novel nonconvex approach to robust principal\ncomponent analysis for HSI denoising, which focuses on simultaneously\ndeveloping more accurate approximations to both rank and column-wise sparsity\nfor the low-rank and sparse components, respectively. In particular, the new\nmethod adopts the log-determinant rank approximation and a novel\n$\\ell_{2,\\log}$ norm, to restrict the local low-rank or column-wisely sparse\nproperties for the component matrices, respectively. For the\n$\\ell_{2,\\log}$-regularized shrinkage problem, we develop an efficient,\nclosed-form solution, which is named $\\ell_{2,\\log}$-shrinkage operator. The\nnew regularization and the corresponding operator can be generally used in\nother problems that require column-wise sparsity. Moreover, we impose the\nspatial-spectral total variation regularization in the log-based nonconvex RPCA\nmodel, which enhances the global piece-wise smoothness and spectral consistency\nfrom the spatial and spectral views in the recovered HSI. Extensive experiments\non both simulated and real HSIs demonstrate the effectiveness of the proposed\nmethod in denoising HSIs.",
          "link": "http://arxiv.org/abs/2201.02812",
          "publishedOn": "2022-01-12T00:38:44.317Z",
          "wordCount": 610,
          "title": "Hyperspectral Image Denoising Using Non-convex Local Low-rank and Sparse Separation with Spatial-Spectral Total Variation Regularization. (arXiv:2201.02812v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02936",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Goswami_M/0/1/0/all/0/1\">Mononito Goswami</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boecking_B/0/1/0/all/0/1\">Benedikt Boecking</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dubrawski_A/0/1/0/all/0/1\">Artur Dubrawski</a>",
          "description": "Analysing electrocardiograms (ECGs) is an inexpensive and non-invasive, yet\npowerful way to diagnose heart disease. ECG studies using Machine Learning to\nautomatically detect abnormal heartbeats so far depend on large, manually\nannotated datasets. While collecting vast amounts of unlabeled data can be\nstraightforward, the point-by-point annotation of abnormal heartbeats is\ntedious and expensive. We explore the use of multiple weak supervision sources\nto learn diagnostic models of abnormal heartbeats via human designed\nheuristics, without using ground truth labels on individual data points. Our\nwork is among the first to define weak supervision sources directly on time\nseries data. Results show that with as few as six intuitive time series\nheuristics, we are able to infer high quality probabilistic label estimates for\nover 100,000 heartbeats with little human effort, and use the estimated labels\nto train competitive classifiers evaluated on held out test data.",
          "link": "http://arxiv.org/abs/2201.02936",
          "publishedOn": "2022-01-12T00:38:44.306Z",
          "wordCount": 586,
          "title": "Weak Supervision for Affordable Modeling of Electrocardiogram Data. (arXiv:2201.02936v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Utkin_L/0/1/0/all/0/1\">Lev V. Utkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konstantinov_A/0/1/0/all/0/1\">Andrei V. Konstantinov</a>",
          "description": "A new approach called ABRF (the attention-based random forest) and its\nmodifications for applying the attention mechanism to the random forest (RF)\nfor regression and classification are proposed. The main idea behind the\nproposed ABRF models is to assign attention weights with trainable parameters\nto decision trees in a specific way. The weights depend on the distance between\nan instance, which falls into a corresponding leaf of a tree, and instances,\nwhich fall in the same leaf. This idea stems from representation of the\nNadaraya-Watson kernel regression in the form of a RF. Three modifications of\nthe general approach are proposed. The first one is based on applying the\nHuber's contamination model and on computing the attention weights by solving\nquadratic or linear optimization problems. The second and the third\nmodifications use the gradient-based algorithms for computing trainable\nparameters. Numerical experiments with various regression and classification\ndatasets illustrate the proposed method.",
          "link": "http://arxiv.org/abs/2201.02880",
          "publishedOn": "2022-01-12T00:38:44.282Z",
          "wordCount": 571,
          "title": "Attention-based Random Forest and Contamination Model. (arXiv:2201.02880v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02615",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gelaw_T/0/1/0/all/0/1\">Tariku Adane Gelaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagos_M/0/1/0/all/0/1\">Misgina Tsighe Hagos</a>",
          "description": "Poor sitting habits have been identified as a risk factor to musculoskeletal\ndisorders and lower back pain especially on the elderly, disabled people, and\noffice workers. In the current computerized world, even while involved in\nleisure or work activity, people tend to spend most of their days sitting at\ncomputer desks. This can result in spinal pain and related problems. Therefore,\na means to remind people about their sitting habits and provide recommendations\nto counterbalance, such as physical exercise, is important. Posture recognition\nfor seated postures have not received enough attention as most works focus on\nstanding postures. Wearable sensors, pressure or force sensors, videos and\nimages were used for posture recognition in the literature. The aim of this\nstudy is to build Machine Learning models for classifying sitting posture of a\nperson by analyzing data collected from a chair platted with two 32 by 32\npressure sensors at its seat and backrest. Models were built using five\nalgorithms: Random Forest (RF), Gaussian Na\\\"ive Bayes, Logistic Regression,\nSupport Vector Machine and Deep Neural Network (DNN). All the models are\nevaluated using KFold cross-validation technique. This paper presents\nexperiments conducted using the two separate datasets, controlled and\nrealistic, and discusses results achieved at classifying six sitting postures.\nAverage classification accuracies of 98% and 97% were achieved on the\ncontrolled and realistic datasets, respectively.",
          "link": "http://arxiv.org/abs/2201.02615",
          "publishedOn": "2022-01-12T00:38:44.274Z",
          "wordCount": 664,
          "title": "Posture Prediction for Healthy Sitting using a Smart Chair. (arXiv:2201.02615v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_L/0/1/0/all/0/1\">Lovedeep Singh</a>",
          "description": "Clustering Text has been an important problem in the domain of Natural\nLanguage Processing. While there are techniques to cluster text based on using\nconventional clustering techniques on top of contextual or non-contextual\nvector space representations, it still remains a prevalent area of research\npossible to various improvements in performance and implementation of these\ntechniques. This paper discusses a novel technique to cluster text using\nattention mechanisms. Attention Mechanisms have proven to be highly effective\nin various NLP tasks in recent times. This paper extends the idea of attention\nmechanism in clustering space and sheds some light on a whole new area of\nresearch",
          "link": "http://arxiv.org/abs/2201.02816",
          "publishedOn": "2022-01-12T00:38:44.265Z",
          "wordCount": 529,
          "title": "Clustering Text Using Attention. (arXiv:2201.02816v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haonan Chen</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Bo Yuan Chang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Naiel1_M/0/1/0/all/0/1\">Mohamed A. Naiel1</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Younes_G/0/1/0/all/0/1\">Georges Younes</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wardell_S/0/1/0/all/0/1\">Steven Wardell</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Kleinikkink_S/0/1/0/all/0/1\">Stan Kleinikkink</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Zelek_J/0/1/0/all/0/1\">John S. Zelek</a> (1) ((1) University of Waterloo, (2) ATS Automation)",
          "description": "Causal discovery between collections of time-series data can help diagnose\ncauses of symptoms and hopefully prevent faults before they occur. However,\nreliable causal discovery can be very challenging, especially when the data\nacquisition rate varies (i.e., non-uniform data sampling), or in the presence\nof missing data points (e.g., sparse data sampling). To address these issues,\nwe proposed a new system comprised of two parts, the first part fills missing\ndata with a Gaussian Process Regression, and the second part leverages an Echo\nState Network, which is a type of reservoir computer (i.e., used for chaotic\nsystem modeling) for Causal discovery. We evaluate the performance of our\nproposed system against three other off-the-shelf causal discovery algorithms,\nnamely, structural expectation-maximization, sub-sampled linear auto-regression\nabsolute coefficients, and multivariate Granger Causality with vector\nauto-regressive using the Tennessee Eastman chemical dataset; we report on\ntheir corresponding Matthews Correlation Coefficient(MCC) and Receiver\nOperating Characteristic curves (ROC) and show that the proposed system\noutperforms existing algorithms, demonstrating the viability of our approach to\ndiscover causal relationships in a complex system with missing entries.",
          "link": "http://arxiv.org/abs/2201.02933",
          "publishedOn": "2022-01-12T00:38:44.259Z",
          "wordCount": 624,
          "title": "Causal Discovery from Sparse Time-Series Data Using Echo State Network. (arXiv:2201.02933v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02834",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Peng_B/0/1/0/all/0/1\">Bile Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Termohlen_J/0/1/0/all/0/1\">Jan-Aike Term&#xf6;hlen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_C/0/1/0/all/0/1\">Cong Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_D/0/1/0/all/0/1\">Danping He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guan_K/0/1/0/all/0/1\">Ke Guan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fingscheidt_T/0/1/0/all/0/1\">Tim Fingscheidt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jorswieck_E/0/1/0/all/0/1\">Eduard A. Jorswieck</a>",
          "description": "Reconfigurable intelligent surface (RIS) is an emerging technology for future\nwireless communication systems. In this work, we consider downlink spatial\nmultiplexing enabled by the RIS for weighted sum-rate (WSR) maximization. In\nthe literature, most solutions use alternating gradient-based optimization,\nwhich has moderate performance, high complexity, and limited scalability. We\npropose to apply a fully convolutional network (FCN) to solve this problem,\nwhich was originally designed for semantic segmentation of images. The\nrectangular shape of the RIS and the spatial correlation of channels with\nadjacent RIS antennas due to the short distance between them encourage us to\napply it for the RIS configuration. We design a set of channel features that\nincludes both cascaded channels via the RIS and the direct channel. In the base\nstation (BS), the differentiable minimum mean squared error (MMSE) precoder is\nused for pretraining and the weighted minimum mean squared error (WMMSE)\nprecoder is then applied for fine-tuning, which is nondifferentiable, more\ncomplex, but achieves a better performance. Evaluation results show that the\nproposed solution has higher performance and allows for a faster evaluation\nthan the baselines. Hence it scales better to a large number of antennas,\nadvancing the RIS one step closer to practical deployment.",
          "link": "http://arxiv.org/abs/2201.02834",
          "publishedOn": "2022-01-12T00:38:44.252Z",
          "wordCount": 635,
          "title": "Reconfigurable Intelligent Surface Enabled Spatial Multiplexing with Fully Convolutional Network. (arXiv:2201.02834v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Abbas Raza Ali</a>",
          "description": "In this paper, the challenges of maintaining a healthy IT operational\nenvironment have been addressed by proactively analyzing IT Service Desk\ntickets, customer satisfaction surveys, and social media data. A Cognitive\nsolution goes beyond the traditional structured data analysis by deep analyses\nof both structured and unstructured text. The salient features of the proposed\nplatform include language identification, translation, hierarchical extraction\nof the most frequently occurring topics, entities and their relationships, text\nsummarization, sentiments, and knowledge extraction from the unstructured text\nusing Natural Language Processing techniques. Moreover, the insights from\nunstructured text combined with structured data allow the development of\nvarious classification, segmentation, and time-series forecasting use-cases on\nthe incident, problem, and change datasets. Further, the text and predictive\ninsights together with raw data are used for visualization and exploration of\nactionable insights on a rich and interactive dashboard. However, it is hard\nnot only to find these insights using traditional structured data analysis but\nit might also take a very long time to discover them, especially while dealing\nwith a massive amount of unstructured data. By taking action on these insights,\norganizations can benefit from a significant reduction of ticket volume,\nreduced operational costs, and increased customer satisfaction. In various\nexperiments, on average, upto 18-25% of yearly ticket volume has been reduced\nusing the proposed approach.",
          "link": "http://arxiv.org/abs/2201.02737",
          "publishedOn": "2022-01-12T00:38:44.225Z",
          "wordCount": 647,
          "title": "Cognitive Computing to Optimize IT Services. (arXiv:2201.02737v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abedin_A/0/1/0/all/0/1\">Afia Fairoose Abedin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamun_A/0/1/0/all/0/1\">Amirul Islam Al Mamun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowrin_R/0/1/0/all/0/1\">Rownak Jahan Nowrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_A/0/1/0/all/0/1\">Amitabha Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mostakim_M/0/1/0/all/0/1\">Moin Mostakim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naskar_S/0/1/0/all/0/1\">Sudip Kumar Naskar</a>",
          "description": "In recent times, a large number of people have been involved in establishing\ntheir own businesses. Unlike humans, chatbots can serve multiple customers at a\ntime, are available 24/7 and reply in less than a fraction of a second. Though\nchatbots perform well in task-oriented activities, in most cases they fail to\nunderstand personalized opinions, statements or even queries which later impact\nthe organization for poor service management. Lack of understanding\ncapabilities in bots disinterest humans to continue conversations with them.\nUsually, chatbots give absurd responses when they are unable to interpret a\nuser's text accurately. Extracting the client reviews from conversations by\nusing chatbots, organizations can reduce the major gap of understanding between\nthe users and the chatbot and improve their quality of products and\nservices.Thus, in our research we incorporated all the key elements that are\nnecessary for a chatbot to analyse and understand an input text precisely and\naccurately. We performed sentiment analysis, emotion detection, intent\nclassification and named-entity recognition using deep learning to develop\nchatbots with humanistic understanding and intelligence. The efficiency of our\napproach can be demonstrated accordingly by the detailed analysis.",
          "link": "http://arxiv.org/abs/2201.02735",
          "publishedOn": "2022-01-12T00:38:44.219Z",
          "wordCount": 625,
          "title": "A Deep Learning Approach to Integrate Human-Level Understanding in a Chatbot. (arXiv:2201.02735v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1\">Tuan Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_D/0/1/0/all/0/1\">Daewon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zhixu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Liang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kangwook Lee</a>",
          "description": "We study the GAN conditioning problem, whose goal is to convert a pretrained\nunconditional GAN into a conditional GAN using labeled data. We first identify\nand analyze three approaches to this problem -- conditional GAN training from\nscratch, fine-tuning, and input reprogramming. Our analysis reveals that when\nthe amount of labeled data is small, input reprogramming performs the best.\nMotivated by real-world scenarios with scarce labeled data, we focus on the\ninput reprogramming approach and carefully analyze the existing algorithm.\nAfter identifying a few critical issues of the previous input reprogramming\napproach, we propose a new algorithm called InRep+. Our algorithm InRep+\naddresses the existing issues with the novel uses of invertible neural networks\nand Positive-Unlabeled (PU) learning. Via extensive experiments, we show that\nInRep+ outperforms all existing methods, particularly when label information is\nscarce, noisy, and/or imbalanced. For instance, for the task of conditioning a\nCIFAR10 GAN with 1% labeled data, InRep+ achieves an average Intra-FID of\n82.13, whereas the second-best method achieves 114.51.",
          "link": "http://arxiv.org/abs/2201.02692",
          "publishedOn": "2022-01-12T00:38:44.212Z",
          "wordCount": 589,
          "title": "Improved Input Reprogramming for GAN Conditioning. (arXiv:2201.02692v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hongyi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badawi_D/0/1/0/all/0/1\">Diaa Badawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cetin_A/0/1/0/all/0/1\">Ahmet Enis Cetin</a>",
          "description": "Convolution has been the core operation of modern deep neural networks. It is\nwell-known that convolutions can be implemented in the Fourier Transform\ndomain. In this paper, we propose to use binary block Walsh-Hadamard transform\n(WHT) instead of the Fourier transform. We use WHT-based binary layers to\nreplace some of the regular convolution layers in deep neural networks. We\nutilize both one-dimensional (1-D) and two-dimensional (2-D) binary WHTs in\nthis paper. In both 1-D and 2-D layers, we compute the binary WHT of the input\nfeature map and denoise the WHT domain coefficients using a nonlinearity which\nis obtained by combining soft-thresholding with the tanh function. After\ndenoising, we compute the inverse WHT. We use 1D-WHT to replace the $1\\times 1$\nconvolutional layers, and 2D-WHT layers can replace the 3$\\times$3 convolution\nlayers and Squeeze-and-Excite layers. 2D-WHT layers with trainable weights can\nbe also inserted before the Global Average Pooling (GAP) layers to assist the\ndense layers. In this way, we can reduce the number of trainable parameters\nsignificantly with a slight decrease in trainable parameters. In this paper, we\nimplement the WHT layers into MobileNet-V2, MobileNet-V3-Large, and ResNet to\nreduce the number of parameters significantly with negligible accuracy loss.\nMoreover, according to our speed test, the 2D-FWHT layer runs about 24 times as\nfast as the regular $3\\times 3$ convolution with 19.51\\% less RAM usage in an\nNVIDIA Jetson Nano experiment.",
          "link": "http://arxiv.org/abs/2201.02711",
          "publishedOn": "2022-01-12T00:38:44.205Z",
          "wordCount": 678,
          "title": "Block Walsh-Hadamard Transform Based Binary Layers in Deep Neural Networks. (arXiv:2201.02711v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_T/0/1/0/all/0/1\">Tiago Gaspar Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1\">Arlindo L. Oliveira</a>",
          "description": "The model-based reinforcement learning paradigm, which uses planning\nalgorithms and neural network models, has recently achieved unprecedented\nresults in diverse applications, leading to what is now known as deep\nreinforcement learning. These agents are quite complex and involve multiple\ncomponents, factors that can create challenges for research. In this work, we\npropose a new modular software architecture suited for these types of agents,\nand a set of building blocks that can be easily reused and assembled to\nconstruct new model-based reinforcement learning agents. These building blocks\ninclude planning algorithms, policies, and loss functions.\n\nWe illustrate the use of this architecture by combining several of these\nbuilding blocks to implement and test agents that are optimized to three\ndifferent test environments: Cartpole, Minigrid, and Tictactoe. One particular\nplanning algorithm, made available in our implementation and not previously\nused in reinforcement learning, which we called averaged minimax, achieved good\nresults in the three tested environments.\n\nExperiments performed with this architecture have shown that the best\ncombination of planning algorithm, policy, and loss function is heavily problem\ndependent. This result provides evidence that the proposed architecture, which\nis modular and reusable, is useful for reinforcement learning researchers who\nwant to study new environments and techniques.",
          "link": "http://arxiv.org/abs/2201.02874",
          "publishedOn": "2022-01-12T00:38:44.081Z",
          "wordCount": 641,
          "title": "Assessing Policy, Loss and Planning Combinations in Reinforcement Learning using a New Modular Architecture. (arXiv:2201.02874v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02729",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Pavlyshenko_B/0/1/0/all/0/1\">Bohdan M. Pavlyshenko</a>",
          "description": "The paper studies the linear model for Bitcoin price which includes\nregression features based on Bitcoin currency statistics, mining processes,\nGoogle search trends, Wikipedia pages visits. The pattern of deviation of\nregression model prediction from real prices is simpler comparing to price time\nseries. It is assumed that this pattern can be predicted by an experienced\nexpert. In such a way, using the combination of the regression model and expert\ncorrection, one can receive better results than with either regression model or\nexpert opinion only. It is shown that Bayesian approach makes it possible to\nutilize the probabilistic approach using distributions with fat tails and take\ninto account the outliers in Bitcoin price time series.",
          "link": "http://arxiv.org/abs/2201.02729",
          "publishedOn": "2022-01-12T00:38:44.073Z",
          "wordCount": 529,
          "title": "Bitcoin Price Predictive Modeling Using Expert Correction. (arXiv:2201.02729v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02923",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_A/0/1/0/all/0/1\">Alexander Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klabjan_D/0/1/0/all/0/1\">Diego Klabjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuan Luo</a>",
          "description": "Open-set recognition generalizes a classification task by classifying test\nsamples as one of the known classes from training or \"unknown.\" As novel cancer\ndrug cocktails with improved treatment are continually discovered, predicting\ncancer treatments can naturally be formulated in terms of an open-set\nrecognition problem. Drawbacks, due to modeling unknown samples during\ntraining, arise from straightforward implementations of prior work in\nhealthcare open-set learning. Accordingly, we reframe the problem methodology\nand apply a recent existing Gaussian mixture variational autoencoder model,\nwhich achieves state-of-the-art results for image datasets, to breast cancer\npatient data. Not only do we obtain more accurate and robust classification\nresults, with a 24.5% average F1 increase compared to a recent method, but we\nalso reexamine open-set recognition in terms of deployability to a clinical\nsetting.",
          "link": "http://arxiv.org/abs/2201.02923",
          "publishedOn": "2022-01-12T00:38:43.884Z",
          "wordCount": 548,
          "title": "Open-Set Recognition of Breast Cancer Treatments. (arXiv:2201.02923v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shehnepoor_S/0/1/0/all/0/1\">Saeedreza Shehnepoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Togneri_R/0/1/0/all/0/1\">Roberto Togneri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>",
          "description": "Motivated by potential financial gain, companies may hire fraudster groups to\nwrite fake reviews to either demote competitors or promote their own\nbusinesses. Such groups are considerably more successful in misleading\ncustomers, as people are more likely to be influenced by the opinion of a large\ngroup. To detect such groups, a common model is to represent fraudster groups'\nstatic networks, consequently overlooking the longitudinal behavior of a\nreviewer thus the dynamics of co-review relations among reviewers in a group.\nHence, these approaches are incapable of excluding outlier reviewers, which are\nfraudsters intentionally camouflaging themselves in a group and genuine\nreviewers happen to co-review in fraudster groups. To address this issue, in\nthis work, we propose to first capitalize on the effectiveness of the HIN-RNN\nin both reviewers' representation learning while capturing the collaboration\nbetween reviewers, we first utilize the HIN-RNN to model the co-review\nrelations of reviewers in a group in a fixed time window of 28 days. We refer\nto this as spatial relation learning representation to signify the\ngeneralisability of this work to other networked scenarios. Then we use an RNN\non the spatial relations to predict the spatio-temporal relations of reviewers\nin the group. In the third step, a Graph Convolution Network (GCN) refines the\nreviewers' vector representations using these predicted relations. These\nrefined representations are then used to remove outlier reviewers. The average\nof the remaining reviewers' representation is then fed to a simple fully\nconnected layer to predict if the group is a fraudster group or not. Exhaustive\nexperiments of the proposed approach showed a 5% (4%), 12% (5%), 12% (5%)\nimprovement over three of the most recent approaches on precision, recall, and\nF1-value over the Yelp (Amazon) dataset, respectively.",
          "link": "http://arxiv.org/abs/2201.02621",
          "publishedOn": "2022-01-12T00:38:43.877Z",
          "wordCount": 712,
          "title": "Spatio-Temporal Graph Representation Learning for Fraudster Group Detection. (arXiv:2201.02621v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02824",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Stephanovitch_A/0/1/0/all/0/1\">Arthur St&#xe9;phanovitch</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tanielian_U/0/1/0/all/0/1\">Ugo Tanielian</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cadre_B/0/1/0/all/0/1\">Beno&#xee;t Cadre</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Klutchnikoff_N/0/1/0/all/0/1\">Nicolas Klutchnikoff</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Biau_G/0/1/0/all/0/1\">G&#xe9;rard Biau</a>",
          "description": "The mathematical forces at work behind Generative Adversarial Networks raise\nchallenging theoretical issues. Motivated by the important question of\ncharacterizing the geometrical properties of the generated distributions, we\nprovide a thorough analysis of Wasserstein GANs (WGANs) in both the finite\nsample and asymptotic regimes. We study the specific case where the latent\nspace is univariate and derive results valid regardless of the dimension of the\noutput space. We show in particular that for a fixed sample size, the optimal\nWGANs are closely linked with connected paths minimizing the sum of the squared\nEuclidean distances between the sample points. We also highlight the fact that\nWGANs are able to approach (for the 1-Wasserstein distance) the target\ndistribution as the sample size tends to infinity, at a given convergence rate\nand provided the family of generative Lipschitz functions grows appropriately.\nWe derive in passing new results on optimal transport theory in the\nsemi-discrete setting.",
          "link": "http://arxiv.org/abs/2201.02824",
          "publishedOn": "2022-01-12T00:38:43.332Z",
          "wordCount": 575,
          "title": "Optimal 1-Wasserstein Distance for WGANs. (arXiv:2201.02824v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_Q/0/1/0/all/0/1\">Qi Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuanyuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>",
          "description": "Vertical collaborative learning system also known as vertical federated\nlearning (VFL) system has recently become prominent as a concept to process\ndata distributed across many individual sources without the need to centralize\nit. Multiple participants collaboratively train models based on their local\ndata in a privacy-preserving manner. To date, VFL has become a de facto\nsolution to securely learn a model among organizations, allowing knowledge to\nbe shared without compromising privacy of any individual organizations.\n\nDespite the prosperous development of VFL systems, we find that certain\ninputs of a participant, named adversarial dominating inputs (ADIs), can\ndominate the joint inference towards the direction of the adversary's will and\nforce other (victim) participants to make negligible contributions, losing\nrewards that are usually offered regarding the importance of their\ncontributions in collaborative learning scenarios.\n\nWe conduct a systematic study on ADIs by first proving their existence in\ntypical VFL systems. We then propose gradient-based methods to synthesize ADIs\nof various formats and exploit common VFL systems. We further launch greybox\nfuzz testing, guided by the resiliency score of \"victim\" participants, to\nperturb adversary-controlled inputs and systematically explore the VFL attack\nsurface in a privacy-preserving manner. We conduct an in-depth study on the\ninfluence of critical parameters and settings in synthesizing ADIs. Our study\nreveals new VFL attack opportunities, promoting the identification of unknown\nthreats before breaches and building more secure VFL systems.",
          "link": "http://arxiv.org/abs/2201.02775",
          "publishedOn": "2022-01-12T00:38:43.266Z",
          "wordCount": 659,
          "title": "Attacking Vertical Collaborative Learning System Using Adversarial Dominating Inputs. (arXiv:2201.02775v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jaewoo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fangzhen Lin</a>",
          "description": "Standard deep learning algorithms are implemented using floating-point real\nnumbers. This presents an obstacle for implementing them on low-end devices\nwhich may not have dedicated floating-point units (FPUs). As a result,\nresearchers in TinyML have considered machine learning algorithms that can\ntrain and run a deep neural network (DNN) on a low-end device using integer\noperations only. In this paper we propose PocketNN, a light and self-contained\nproof-of-concept framework in pure C++ for the training and inference of DNNs\nusing only integers. Unlike other approaches, PocketNN directly operates on\nintegers without requiring any explicit quantization algorithms or customized\nfixed-point formats. This was made possible by pocket activations, which are a\nfamily of activation functions devised for integer-only DNNs, and an emerging\nDNN training algorithm called direct feedback alignment (DFA). Unlike the\nstandard backpropagation (BP), DFA trains each layer independently, thus\navoiding integer overflow which is a key problem when using BP with\ninteger-only operations. We used PocketNN to train some DNNs on two well-known\ndatasets, MNIST and Fashion-MNIST. Our experiments show that the DNNs trained\nwith our PocketNN achieved 96.98% and 87.7% accuracies on MNIST and\nFashion-MNIST datasets, respectively. The accuracies are very close to the\nequivalent DNNs trained using BP with floating-point real number operations,\nsuch that accuracy degradations were just 1.02%p and 2.09%p, respectively.\nFinally, our PocketNN has high compatibility and portability for low-end\ndevices as it is open source and implemented in pure C++ without any\ndependencies.",
          "link": "http://arxiv.org/abs/2201.02863",
          "publishedOn": "2022-01-12T00:38:43.258Z",
          "wordCount": 689,
          "title": "PocketNN: Integer-only Training and Inference of Neural Networks via Direct Feedback Alignment and Pocket Activations in Pure C++. (arXiv:2201.02863v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02664",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_N/0/1/0/all/0/1\">Nicole Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balle_J/0/1/0/all/0/1\">Johannes Ball&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charles_Z/0/1/0/all/0/1\">Zachary Charles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konecny_J/0/1/0/all/0/1\">Jakub Kone&#x10d;n&#xfd;</a>",
          "description": "A significant bottleneck in federated learning is the network communication\ncost of sending model updates from client devices to the central server. We\npropose a method to reduce this cost. Our method encodes quantized updates with\nan appropriate universal code, taking into account their empirical\ndistribution. Because quantization introduces error, we select quantization\nlevels by optimizing for the desired trade-off in average total bitrate and\ngradient distortion. We demonstrate empirically that in spite of the non-i.i.d.\nnature of federated learning, the rate-distortion frontier is consistent across\ndatasets, optimizers, clients and training rounds, and within each setting,\ndistortion reliably predicts model performance. This allows for a remarkably\nsimple compression scheme that is near-optimal in many use cases, and\noutperforms Top-K, DRIVE, 3LC and QSGD on the Stack Overflow next-word\nprediction benchmark.",
          "link": "http://arxiv.org/abs/2201.02664",
          "publishedOn": "2022-01-12T00:38:43.234Z",
          "wordCount": 569,
          "title": "Optimizing the Communication-Accuracy Trade-off in Federated Learning with Rate-Distortion Theory. (arXiv:2201.02664v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02867",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Donnat_C/0/1/0/all/0/1\">Claire Donnat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Levy_A/0/1/0/all/0/1\">Axel Levy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Poitevin_F/0/1/0/all/0/1\">Frederic Poitevin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miolane_N/0/1/0/all/0/1\">Nina Miolane</a>",
          "description": "Recent breakthroughs in high resolution imaging of biomolecules in solution\nwith cryo-electron microscopy (cryo-EM) have unlocked new doors for the\nreconstruction of molecular volumes, thereby promising further advances in\nbiology, chemistry, and pharmacological research amongst others. Despite\nsignificant headway, the immense challenges in cryo-EM data analysis remain\nlegion and intricately inter-disciplinary in nature, requiring insights from\nphysicists, structural biologists, computer scientists, statisticians, and\napplied mathematicians. Meanwhile, recent next-generation volume reconstruction\nalgorithms that combine generative modeling with end-to-end unsupervised deep\nlearning techniques have shown promising results on simulated data, but still\nface considerable hurdles when applied to experimental cryo-EM images. In light\nof the proliferation of such methods and given the interdisciplinary nature of\nthe task, we propose here a critical review of recent advances in the field of\ndeep generative modeling for high resolution cryo-EM volume reconstruction. The\npresent review aims to (i) compare and contrast these new methods, while (ii)\npresenting them from a perspective and using terminology familiar to scientists\nin each of the five aforementioned fields with no specific background in\ncryo-EM. The review begins with an introduction to the mathematical and\ncomputational challenges of deep generative models for cryo-EM volume\nreconstruction, along with an overview of the baseline methodology shared\nacross this class of algorithms. Having established the common thread weaving\nthrough these different models, we provide a practical comparison of these\nstate-of-the-art algorithms, highlighting their relative strengths and\nweaknesses, along with the assumptions that they rely on. This allows us to\nidentify bottlenecks in current methods and avenues for future research.",
          "link": "http://arxiv.org/abs/2201.02867",
          "publishedOn": "2022-01-12T00:38:43.226Z",
          "wordCount": 698,
          "title": "Deep Generative Modeling for Volume Reconstruction in Cryo-Electron Microscop. (arXiv:2201.02867v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hajimoradlou_A/0/1/0/all/0/1\">Ainaz Hajimoradlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazemi_M/0/1/0/all/0/1\">Mehran Kazemi</a>",
          "description": "Knowledge graphs (KGs) are typically incomplete and we often wish to infer\nnew facts given the existing ones. This can be thought of as a binary\nclassification problem; we aim to predict if new facts are true or false.\nUnfortunately, we generally only have positive examples (the known facts) but\nwe also need negative ones to train a classifier. To resolve this, it is usual\nto generate negative examples using a negative sampling strategy. However, this\ncan produce false negatives which may reduce performance, is computationally\nexpensive, and does not produce calibrated classification probabilities. In\nthis paper, we propose a training procedure that obviates the need for negative\nsampling by adding a novel regularization term to the loss function. Our\nresults for two relational embedding models (DistMult and SimplE) show the\nmerit of our proposal both in terms of performance and speed.",
          "link": "http://arxiv.org/abs/2201.02661",
          "publishedOn": "2022-01-12T00:38:43.218Z",
          "wordCount": 558,
          "title": "Stay Positive: Knowledge Graph Embedding Without Negative Sampling. (arXiv:2201.02661v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02733",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1\">Aftab Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nanduri_S/0/1/0/all/0/1\">Sai Durga Prasad Nanduri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seenuvasavarathan_S/0/1/0/all/0/1\">Sneha Seenuvasavarathan</a>",
          "description": "The growing prevalence of counterfeit stories on the internet has fostered\nsignificant interest towards fast and scalable detection of fake news in the\nmachine learning community. While several machine learning techniques for this\npurpose have emerged, we observe that there is a need to evaluate the impact of\nnoise on these techniques' performance, where noise constitutes news articles\nbeing mistakenly labeled as fake (or real). This work takes a step in that\ndirection, where we examine the impact of noise on a state-of-the-art,\nstructural model based on BiLSTM (Bidirectional Long-Short Term Model) for fake\nnews detection, Hierarchical Discourse-level Structure for Fake News Detection\nby Karimi and Tang (Reference no. 9).",
          "link": "http://arxiv.org/abs/2201.02733",
          "publishedOn": "2022-01-12T00:38:43.210Z",
          "wordCount": 534,
          "title": "Testing the Robustness of a BiLSTM-based Structural Story Classifier. (arXiv:2201.02733v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gangadhar_G/0/1/0/all/0/1\">Govind Krishnan Gangadhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Ashish Kulkarni</a>",
          "description": "E-commerce product pages on the web often present product specification data\nin structured tabular blocks. Extraction of these product attribute-value\nspecifications has benefited applications like product catalogue curation,\nsearch, question answering, and others. However, across different Websites,\nthere is a wide variety of HTML elements (like <table>, <ul>, <div>, <span>,\n<dl> etc.) typically used to render these blocks that makes their automatic\nextraction a challenge. Most of the current research has focused on extracting\nproduct specifications from tables and lists and, therefore, suffers from\nrecall when applied to a large-scale extraction setting. In this paper, we\npresent a product specification extraction approach that goes beyond tables or\nlists and generalizes across the diverse HTML elements used for rendering\nspecification blocks. Using a combination of hand-coded features and deep\nlearned spatial and token features, we first identify the specification blocks\non a product page. We then extract the product attribute-value pairs from these\nblocks following an approach inspired by wrapper induction. We created a\nlabeled dataset of product specifications extracted from 14,111 diverse\nspecification blocks taken from a range of different product websites. Our\nexperiments show the efficacy of our approach compared to the current\nspecification extraction models and support our claim about its application to\nlarge-scale product specification extraction.",
          "link": "http://arxiv.org/abs/2201.02896",
          "publishedOn": "2022-01-12T00:38:43.203Z",
          "wordCount": 652,
          "title": "Extraction of Product Specifications from the Web -- Going Beyond Tables and Lists. (arXiv:2201.02896v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02968",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Niu_T/0/1/0/all/0/1\">Tao Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1\">Yinglei Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_P/0/1/0/all/0/1\">Panpan Zou</a>",
          "description": "Recently, the applications of deep neural network (DNN) have been very\nprominent in many fields such as computer vision (CV) and natural language\nprocessing (NLP) due to its superior feature extraction performance. However,\nthe high-dimension parameter model and large-scale mathematical calculation\nrestrict the execution efficiency, especially for Internet of Things (IoT)\ndevices. Different from the previous cloud/edge-only pattern that brings huge\npressure for uplink communication and device-only fashion that undertakes\nunaffordable calculation strength, we highlight the collaborative computation\nbetween the device and edge for DNN models, which can achieve a good balance\nbetween the communication load and execution accuracy. Specifically, a\nsystematic on-demand co-inference framework is proposed to exploit the\nmulti-branch structure, in which the pre-trained Alexnet is right-sized through\n\\emph{early-exit} and partitioned at an intermediate DNN layer. The integer\nquantization is enforced to further compress transmission bits. As a result, we\nestablish a new Deep Reinforcement Learning (DRL) optimizer-Soft Actor Critic\nfor discrete (SAC-d), which generates the \\emph{exit point}, \\emph{partition\npoint}, and \\emph{compressing bits} by soft policy iterations. Based on the\nlatency and accuracy aware reward design, such an optimizer can well adapt to\nthe complex environment like dynamic wireless channel and arbitrary CPU\nprocessing, and is capable of supporting the 5G URLLC. Real-world experiment on\nRaspberry Pi 4 and PC shows the outperformance of the proposed solution.",
          "link": "http://arxiv.org/abs/2201.02968",
          "publishedOn": "2022-01-12T00:38:43.195Z",
          "wordCount": 648,
          "title": "An Adaptive Device-Edge Co-Inference Framework Based on Soft Actor-Critic. (arXiv:2201.02968v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02873",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Z/0/1/0/all/0/1\">Zhe Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shangqing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Bo Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhuo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yao Liu</a>",
          "description": "Federated learning (FL) provides a high efficient decentralized machine\nlearning framework, where the training data remains distributed at remote\nclients in a network. Though FL enables a privacy-preserving mobile edge\ncomputing framework using IoT devices, recent studies have shown that this\napproach is susceptible to poisoning attacks from the side of remote clients.\nTo address the poisoning attacks on FL, we provide a \\textit{two-phase} defense\nalgorithm called {Lo}cal {Ma}licious Facto{r} (LoMar). In phase I, LoMar scores\nmodel updates from each remote client by measuring the relative distribution\nover their neighbors using a kernel density estimation method. In phase II, an\noptimal threshold is approximated to distinguish malicious and clean updates\nfrom a statistical perspective. Comprehensive experiments on four real-world\ndatasets have been conducted, and the experimental results show that our\ndefense strategy can effectively protect the FL system. {Specifically, the\ndefense performance on Amazon dataset under a label-flipping attack indicates\nthat, compared with FG+Krum, LoMar increases the target label testing accuracy\nfrom $96.0\\%$ to $98.8\\%$, and the overall averaged testing accuracy from\n$90.1\\%$ to $97.0\\%$.",
          "link": "http://arxiv.org/abs/2201.02873",
          "publishedOn": "2022-01-12T00:38:43.167Z",
          "wordCount": 609,
          "title": "LoMar: A Local Defense Against Poisoning Attack on Federated Learning. (arXiv:2201.02873v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dachao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihua Zhang</a>",
          "description": "In this paper, we follow Eftekhari's work to give a non-local convergence\nanalysis of deep linear networks. Specifically, we consider optimizing deep\nlinear networks which have a layer with one neuron under quadratic loss. We\ndescribe the convergent point of trajectories with arbitrary starting point\nunder gradient flow, including the paths which converge to one of the saddle\npoints or the original point. We also show specific convergence rates of\ntrajectories that converge to the global minimizer by stages. To achieve these\nresults, this paper mainly extends the machinery in Eftekhari's work to\nprovably identify the rank-stable set and the global minimizer convergent set.\nWe also give specific examples to show the necessity of our definitions.\nCrucially, as far as we know, our results appear to be the first to give a\nnon-local global analysis of linear neural networks from arbitrary initialized\npoints, rather than the lazy training regime which has dominated the literature\nof neural networks, and restricted benign initialization in Eftekhari's work.\nWe also note that extending our results to general linear networks without one\nhidden neuron assumption remains a challenging open problem.",
          "link": "http://arxiv.org/abs/2201.02761",
          "publishedOn": "2022-01-12T00:38:43.159Z",
          "wordCount": 610,
          "title": "Global Convergence Analysis of Deep Linear Networks with A One-neuron Layer. (arXiv:2201.02761v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_Y/0/1/0/all/0/1\">Yoshitomo Matsubara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callegaro_D/0/1/0/all/0/1\">Davide Callegaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levorato_M/0/1/0/all/0/1\">Marco Levorato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Restuccia_F/0/1/0/all/0/1\">Francesco Restuccia</a>",
          "description": "Although mission-critical applications require the use of deep neural\nnetworks (DNNs), their continuous execution at mobile devices results in a\nsignificant increase in energy consumption. While edge offloading can decrease\nenergy consumption, erratic patterns in channel quality, network and edge\nserver load can lead to severe disruption of the system's key operations. An\nalternative approach, called split computing, generates compressed\nrepresentations within the model (called \"bottlenecks\"), to reduce bandwidth\nusage and energy consumption. Prior work has proposed approaches that introduce\nadditional layers, to the detriment of energy consumption and latency. For this\nreason, we propose a new framework called BottleFit, which, in addition to\ntargeted DNN architecture modifications, includes a novel training strategy to\nachieve high accuracy even with strong compression rates. We apply BottleFit on\ncutting-edge DNN models in image classification, and show that BottleFit\nachieves 77.1% data compression with up to 0.6% accuracy loss on ImageNet\ndataset, while state of the art such as SPINN loses up to 6% in accuracy. We\nexperimentally measure the power consumption and latency of an image\nclassification application running on an NVIDIA Jetson Nano board (GPU-based)\nand a Raspberry PI board (GPU-less). We show that BottleFit decreases power\nconsumption and latency respectively by up to 49% and 89% with respect to\n(w.r.t.) local computing and by 37% and 55% w.r.t. edge offloading. We also\ncompare BottleFit with state-of-the-art autoencoders-based approaches, and show\nthat (i) BottleFit reduces power consumption and execution time respectively by\nup to 54% and 44% on the Jetson and 40% and 62% on Raspberry PI; (ii) the size\nof the head model executed on the mobile device is 83 times smaller. The code\nrepository will be published for full reproducibility of the results.",
          "link": "http://arxiv.org/abs/2201.02693",
          "publishedOn": "2022-01-12T00:38:43.149Z",
          "wordCount": 735,
          "title": "BottleFit: Learning Compressed Representations in Deep Neural Networks for Effective and Efficient Split Computing. (arXiv:2201.02693v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_A/0/1/0/all/0/1\">Ashish Ranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahad_M/0/1/0/all/0/1\">Md Shah Fahad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deepak_A/0/1/0/all/0/1\">Akshay Deepak</a>",
          "description": "Attention-based deep networks have been successfully applied on textual data\nin the field of NLP. However, their application on protein sequences poses\nadditional challenges due to the weak semantics of the protein words, unlike\nthe plain text words. These unexplored challenges faced by the standard\nattention technique include (i) vanishing attention score problem and (ii) high\nvariations in the attention distribution. In this regard, we introduce a novel\n{\\lambda}-scaled attention technique for fast and efficient modeling of the\nprotein sequences that addresses both the above problems. This is used to\ndevelop the {\\lambda}-scaled attention network and is evaluated for the task of\nprotein function prediction implemented at the protein sub-sequence level.\nExperiments on the datasets for biological process (BP) and molecular function\n(MF) showed significant improvements in the F1 score values for the proposed\n{\\lambda}-scaled attention technique over its counterpart approach based on the\nstandard attention technique (+2.01% for BP and +4.67% for MF) and\nstate-of-the-art ProtVecGen-Plus approach (+2.61% for BP and +4.20% for MF).\nFurther, fast convergence (converging in half the number of epochs) and\nefficient learning (in terms of very low difference between the training and\nvalidation losses) were also observed during the training process.",
          "link": "http://arxiv.org/abs/2201.02912",
          "publishedOn": "2022-01-12T00:38:43.142Z",
          "wordCount": 625,
          "title": "{\\lambda}-Scaled-Attention: A Novel Fast Attention Mechanism for Efficient Modeling of Protein Sequences. (arXiv:2201.02912v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anderson_D/0/1/0/all/0/1\">Daron Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_G/0/1/0/all/0/1\">George Iosifidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leith_D/0/1/0/all/0/1\">Douglas J. Leith</a>",
          "description": "We consider the general problem of online convex optimization with\ntime-varying additive constraints in the presence of predictions for the next\ncost and constraint functions. A novel primal-dual algorithm is designed by\ncombining a Follow-The-Regularized-Leader iteration with prediction-adaptive\ndynamic steps. The algorithm achieves $\\mathcal O(T^{\\frac{3-\\beta}{4}})$\nregret and $\\mathcal O(T^{\\frac{1+\\beta}{2}})$ constraint violation bounds that\nare tunable via parameter $\\beta\\!\\in\\![1/2,1)$ and have constant factors that\nshrink with the predictions quality, achieving eventually $\\mathcal O(1)$\nregret for perfect predictions. Our work extends the FTRL framework for this\nconstrained OCO setting and outperforms the respective state-of-the-art\ngreedy-based solutions, without imposing conditions on the quality of\npredictions, the cost functions or the geometry of constraints, beyond\nconvexity.",
          "link": "http://arxiv.org/abs/2201.02890",
          "publishedOn": "2022-01-12T00:38:43.110Z",
          "wordCount": 537,
          "title": "Lazy Lagrangians with Predictions for Online Learning. (arXiv:2201.02890v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sai Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jieyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>",
          "description": "Federated learning (FL) is a training technique that enables client devices\nto jointly learn a shared model by aggregating locally-computed models without\nexposing their raw data. While most of the existing work focuses on improving\nthe FL model accuracy, in this paper, we focus on the improving the training\nefficiency, which is often a hurdle for adopting FL in real-world applications.\nSpecifically, we design an efficient FL framework which jointly optimizes model\naccuracy, processing latency and communication efficiency, all of which are\nprimary design considerations for real implementation of FL. Inspired by the\nrecent success of Multi-Agent Reinforcement Learning (MARL) in solving complex\ncontrol problems, we present \\textit{FedMarl}, an MARL-based FL framework which\nperforms efficient run-time client selection. Experiments show that FedMarl can\nsignificantly improve model accuracy with much lower processing latency and\ncommunication cost.",
          "link": "http://arxiv.org/abs/2201.02932",
          "publishedOn": "2022-01-12T00:38:43.041Z",
          "wordCount": 571,
          "title": "A Multi-agent Reinforcement Learning Approach for Efficient Client Selection in Federated Learning. (arXiv:2201.02932v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02702",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Chen_Y/0/1/0/all/0/1\">Yuyang Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bi_K/0/1/0/all/0/1\">Kaiming Bi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wu_C/0/1/0/all/0/1\">Chih-Hang J. Wu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ben_Arieh_D/0/1/0/all/0/1\">David Ben-Arieh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sinha_A/0/1/0/all/0/1\">Ashesh Sinha</a>",
          "description": "Sepsis is a life-threatening medical emergency, which is a major cause of\ndeath worldwide and the second highest cause of mortality in the United States.\nResearching the optimal control treatment or intervention strategy on the\ncomprehensive sepsis system is key in reducing mortality. For this purpose,\nfirst, this paper improves a complex nonlinear sepsis model proposed in our\nprevious work. Then, bifurcation analyses are conducted for each sepsis\nsubsystem to study the model behaviors under some system parameters. The\nbifurcation analysis results also further indicate the necessity of control\ntreatment and intervention therapy. If the sepsis system is without adding any\ncontrol under some parameter and initial system value settings, the system will\nperform persistent inflammation outcomes as time goes by. Therefore, we develop\nour complex improved nonlinear sepsis model into a sepsis optimal control\nmodel, and then use some effective biomarkers recommended in existing clinic\npractices as optimization objective function to measure the development of\nsepsis. Besides that, a Bayesian optimization algorithm by combining Recurrent\nneural network (RNN-BO algorithm) is introduced to predict the optimal control\nstrategy for the studied sepsis optimal control system. The difference between\nthe RNN-BO algorithm from other optimization algorithms is that once given any\nnew initial system value setting (initial value is associated with the initial\nconditions of patients), the RNN-BO algorithm is capable of quickly predicting\na corresponding time-series optimal control based on the historical optimal\ncontrol data for any new sepsis patient. To demonstrate the effectiveness and\nefficiency of the RNN-BO algorithm on solving the optimal control solution on\nthe complex nonlinear sepsis system, some numerical simulations are implemented\nby comparing with other optimization algorithms in this paper.",
          "link": "http://arxiv.org/abs/2201.02702",
          "publishedOn": "2022-01-12T00:38:43.028Z",
          "wordCount": 740,
          "title": "An Improved Mathematical Model of Sepsis: Modeling, Bifurcation Analysis, and Optimal Control Study for Complex Nonlinear Infectious Disease System. (arXiv:2201.02702v1 [math.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02696",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Matchev_K/0/1/0/all/0/1\">Konstantin T. Matchev</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Matcheva_K/0/1/0/all/0/1\">Katia Matcheva</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Roman_A/0/1/0/all/0/1\">Alexander Roman</a>",
          "description": "Transit spectroscopy is a powerful tool to decode the chemical composition of\nthe atmospheres of extrasolar planets. In this paper we focus on unsupervised\ntechniques for analyzing spectral data from transiting exoplanets. We\ndemonstrate methods for i) cleaning and validating the data, ii) initial\nexploratory data analysis based on summary statistics (estimates of location\nand variability), iii) exploring and quantifying the existing correlations in\nthe data, iv) pre-processing and linearly transforming the data to its\nprincipal components, v) dimensionality reduction and manifold learning, vi)\nclustering and anomaly detection, vii) visualization and interpretation of the\ndata. To illustrate the proposed unsupervised methodology, we use a well-known\npublic benchmark data set of synthetic transit spectra. We show that there is a\nhigh degree of correlation in the spectral data, which calls for appropriate\nlow-dimensional representations. We explore a number of different techniques\nfor such dimensionality reduction and identify several suitable options in\nterms of summary statistics, principal components, etc. We uncover interesting\nstructures in the principal component basis, namely, well-defined branches\ncorresponding to different chemical regimes of the underlying atmospheres. We\ndemonstrate that those branches can be successfully recovered with a K-means\nclustering algorithm in fully unsupervised fashion. We advocate for a\nthree-dimensional representation of the spectroscopic data in terms of the\nfirst three principal components, in order to reveal the existing structure in\nthe data and quickly characterize the chemical class of a planet.",
          "link": "http://arxiv.org/abs/2201.02696",
          "publishedOn": "2022-01-12T00:38:43.015Z",
          "wordCount": 681,
          "title": "Unsupervised Machine Learning for Exploratory Data Analysis of Exoplanet Transmission Spectra. (arXiv:2201.02696v1 [astro-ph.EP])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02771",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Guan_S/0/1/0/all/0/1\">Shuyue Guan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Loew_M/0/1/0/all/0/1\">Murray Loew</a>",
          "description": "Instead of using current deep-learning segmentation models (like the UNet and\nvariants), we approach the segmentation problem using trained Convolutional\nNeural Network (CNN) classifiers, which automatically extract important\nfeatures from classified targets for image classification. Those extracted\nfeatures can be visualized and formed heatmaps using Gradient-weighted Class\nActivation Mapping (Grad-CAM). This study tested whether the heatmaps could be\nused to segment the classified targets. We also proposed an evaluation method\nfor the heatmaps; that is, to re-train the CNN classifier using images filtered\nby heatmaps and examine its performance. We used the mean-Dice coefficient to\nevaluate segmentation results. Results from our experiments show that heatmaps\ncan locate and segment partial tumor areas. But only use of the heatmaps from\nCNN classifiers may not be an optimal approach for segmentation. In addition,\nwe have verified that the predictions of CNN classifiers mainly depend on tumor\nareas, and dark regions in Grad-CAM's heatmaps also contribute to\nclassification.",
          "link": "http://arxiv.org/abs/2201.02771",
          "publishedOn": "2022-01-12T00:38:43.002Z",
          "wordCount": 615,
          "title": "A Sneak Attack on Segmentation of Medical Images Using Deep Neural Network Classifiers. (arXiv:2201.02771v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02745",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_M/0/1/0/all/0/1\">Mostafa Rahmani</a>",
          "description": "This paper focuses on the Matrix Factorization based Clustering (MFC) method\nwhich is one of the few closed form algorithms for the subspace clustering\nproblem. Despite being simple, closed-form, and computation-efficient, MFC can\noutperform the other sophisticated subspace clustering methods in many\nchallenging scenarios. We reveal the connection between MFC and the Innovation\nPursuit (iPursuit) algorithm which was shown to be able to outperform the other\nspectral clustering based methods with a notable margin especially when the\nspan of clusters are close. A novel theoretical study is presented which sheds\nlight on the key performance factors of both algorithms (MFC/iPursuit) and it\nis shown that both algorithms can be robust to notable intersections between\nthe span of clusters. Importantly, in contrast to the theoretical guarantees of\nother algorithms which emphasized on the distance between the subspaces as the\nkey performance factor and without making the innovation assumption, it is\nshown that the performance of MFC/iPursuit mainly depends on the distance\nbetween the innovative components of the clusters.",
          "link": "http://arxiv.org/abs/2201.02745",
          "publishedOn": "2022-01-12T00:38:42.985Z",
          "wordCount": 596,
          "title": "Provable Clustering of a Union of Linear Manifolds Using Optimal Directions. (arXiv:2201.02745v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1\">Mubashir Imran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongzhi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kai Zheng</a>",
          "description": "Modeling heterogeneity by extraction and exploitation of high-order\ninformation from heterogeneous information networks (HINs) has been attracting\nimmense research attention in recent times. Such heterogeneous network\nembedding (HNE) methods effectively harness the heterogeneity of small-scale\nHINs. However, in the real world, the size of HINs grow exponentially with the\ncontinuous introduction of new nodes and different types of links, making it a\nbillion-scale network. Learning node embeddings on such HINs creates a\nperformance bottleneck for existing HNE methods that are commonly centralized,\ni.e., complete data and the model are both on a single machine. To address\nlarge-scale HNE tasks with strong efficiency and effectiveness guarantee, we\npresent \\textit{Decentralized Embedding Framework for Heterogeneous Information\nNetwork} (DeHIN) in this paper. In DeHIN, we generate a distributed parallel\npipeline that utilizes hypergraphs in order to infuse parallelization into the\nHNE task. DeHIN presents a context preserving partition mechanism that\ninnovatively formulates a large HIN as a hypergraph, whose hyperedges connect\nsemantically similar nodes. Our framework then adopts a decentralized strategy\nto efficiently partition HINs by adopting a tree-like pipeline. Then, each\nresulting subnetwork is assigned to a distributed worker, which employs the\ndeep information maximization theorem to locally learn node embeddings from the\npartition it receives. We further devise a novel embedding alignment scheme to\nprecisely project independently learned node embeddings from all subnetworks\nonto a common vector space, thus allowing for downstream tasks like link\nprediction and node classification.",
          "link": "http://arxiv.org/abs/2201.02757",
          "publishedOn": "2022-01-12T00:38:42.978Z",
          "wordCount": 668,
          "title": "DeHIN: A Decentralized Framework for Embedding Large-scale Heterogeneous Information Networks. (arXiv:2201.02757v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02626",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhiming Lin</a>",
          "description": "Graph embedding techniques have led to significant progress in recent years.\nHowever, present techniques are not effective enough to capture the patterns of\nnetworks. This paper propose neighbor2vec, a neighbor-based sampling strategy\nused algorithm to learn the neighborhood representations of node, a framework\nto gather the structure information by feature propagation between the node and\nits neighbors. We claim that neighbor2vec is a simple and effective approach to\nenhancing the scalability as well as equality of graph embedding, and it breaks\nthe limits of the existing state-of-the-art unsupervised techniques. We conduct\nexperiments on several node classification and link prediction tasks for\nnetworks such as ogbn-arxiv, ogbn-products, ogbn-proteins, ogbl-ppa,ogbl-collab\nand ogbl-citation2. The result shows that Neighbor2vec's representations\nprovide an average accuracy scores up to 6.8 percent higher than competing\nmethods in node classification tasks and 3.0 percent higher in link prediction\ntasks. The neighbor2vec's representations are able to outperform all baseline\nmethods and two classical GNN models in all six experiments.",
          "link": "http://arxiv.org/abs/2201.02626",
          "publishedOn": "2022-01-12T00:38:42.948Z",
          "wordCount": 581,
          "title": "Neighbor2vec: an efficient and effective method for Graph Embedding. (arXiv:2201.02626v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chunduru_R/0/1/0/all/0/1\">Raviteja Chunduru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>",
          "description": "Temporal abstraction in reinforcement learning is the ability of an agent to\nlearn and use high-level behaviors, called options. The option-critic\narchitecture provides a gradient-based end-to-end learning method to construct\noptions. We propose an attention-based extension to this framework, which\nenables the agent to learn to focus different options on different aspects of\nthe observation space. We show that this leads to behaviorally diverse options\nwhich are also capable of state abstraction, and prevents the degeneracy\nproblems of option domination and frequent option switching that occur in\noption-critic, while achieving a similar sample complexity. We also demonstrate\nthe more efficient, interpretable, and reusable nature of the learned options\nin comparison with option-critic, through different transfer learning tasks.\nExperimental results in a relatively simple four-rooms environment and the more\ncomplex ALE (Arcade Learning Environment) showcase the efficacy of our\napproach.",
          "link": "http://arxiv.org/abs/2201.02628",
          "publishedOn": "2022-01-12T00:38:42.942Z",
          "wordCount": 544,
          "title": "Attention Option-Critic. (arXiv:2201.02628v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahsan_M/0/1/0/all/0/1\">Md Manjurul Ahsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddique_Z/0/1/0/all/0/1\">Zahed Siddique</a>",
          "description": "Machine Learning (ML) has garnered considerable attention from researchers\nand practitioners as a new and adaptable tool for disease diagnosis. With the\nadvancement of ML and the proliferation of papers and research in this field, a\ncomplete examination of Machine Learning-Based Disease Diagnosis (MLBDD) is\nrequired. From a bibliometrics standpoint, this article comprehensively studies\nMLBDD papers from 2012 to 2021. Consequently, with particular keywords, 1710\npapers with associate information have been extracted from the Scopus and Web\nof Science (WOS) database and integrated into the excel datasheet for further\nanalysis. First, we examine the publication structures based on yearly\npublications and the most productive countries/regions, institutions, and\nauthors. Second, the co-citation networks of countries/regions, institutions,\nauthors, and articles are visualized using R-studio software. They are further\nexamined in terms of citation structure and the most influential ones. This\narticle gives an overview of MLBDD for researchers interested in the subject\nand conducts a thorough and complete study of MLBDD for those interested in\nconducting more research in this field.",
          "link": "http://arxiv.org/abs/2201.02755",
          "publishedOn": "2022-01-12T00:38:42.935Z",
          "wordCount": 583,
          "title": "Machine Learning-Based Disease Diagnosis:A Bibliometric Analysis. (arXiv:2201.02755v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haizhou Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xinwei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongbin Sun</a>",
          "description": "In a modern power system, real-time data on power generation/consumption and\nits relevant features are stored in various distributed parties, including\nhousehold meters, transformer stations and external organizations. To fully\nexploit the underlying patterns of these distributed data for accurate power\nprediction, federated learning is needed as a collaborative but\nprivacy-preserving training scheme. However, current federated learning\nframeworks are polarized towards addressing either the horizontal or vertical\nseparation of data, and tend to overlook the case where both are present.\nFurthermore, in mainstream horizontal federated learning frameworks, only\nartificial neural networks are employed to learn the data patterns, which are\nconsidered less accurate and interpretable compared to tree-based models on\ntabular datasets. To this end, we propose a hybrid federated learning framework\nbased on XGBoost, for distributed power prediction from real-time external\nfeatures. In addition to introducing boosted trees to improve accuracy and\ninterpretability, we combine horizontal and vertical federated learning, to\naddress the scenario where features are scattered in local heterogeneous\nparties and samples are scattered in various local districts. Moreover, we\ndesign a dynamic task allocation scheme such that each party gets a fair share\nof information, and the computing power of each party can be fully leveraged to\nboost training efficiency. A follow-up case study is presented to justify the\nnecessity of adopting the proposed framework. The advantages of the proposed\nframework in fairness, efficiency and accuracy performance are also confirmed.",
          "link": "http://arxiv.org/abs/2201.02783",
          "publishedOn": "2022-01-12T00:38:42.928Z",
          "wordCount": 685,
          "title": "A Fair and Efficient Hybrid Federated Learning Framework based on XGBoost for Distributed Power Prediction. (arXiv:2201.02783v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gunder_M/0/1/0/all/0/1\">Maurice G&#xfc;nder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamati_F/0/1/0/all/0/1\">Facundo R. Ispizua Yamati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kierdorf_J/0/1/0/all/0/1\">Jana Kierdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1\">Ribana Roscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahlein_A/0/1/0/all/0/1\">Anne-Katrin Mahlein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauckhage_C/0/1/0/all/0/1\">Christian Bauckhage</a>",
          "description": "UAV-based image retrieval in modern agriculture enables gathering large\namounts of spatially referenced crop image data. In large-scale experiments,\nhowever, UAV images suffer from containing a multitudinous amount of crops in a\ncomplex canopy architecture. Especially for the observation of temporal\neffects, this complicates the recognition of individual plants over several\nimages and the extraction of relevant information tremendously. In this work,\nwe present a hands-on workflow for the automatized temporal and spatial\nidentification and individualization of crop images from UAVs abbreviated as\n\"cataloging\" based on comprehensible computer vision methods. We evaluate the\nworkflow on two real-world datasets. One dataset is recorded for observation of\nCercospora leaf spot - a fungal disease - in sugar beet over an entire growing\ncycle. The other one deals with harvest prediction of cauliflower plants. The\nplant catalog is utilized for the extraction of single plant images seen over\nmultiple time points. This gathers large-scale spatio-temporal image dataset\nthat in turn can be applied to train further machine learning models including\nvarious data layers. The presented approach improves analysis and\ninterpretation of UAV data in agriculture significantly. By validation with\nsome reference data, our method shows an accuracy that is similar to more\ncomplex deep learning-based recognition techniques. Our workflow is able to\nautomatize plant cataloging and training image extraction, especially for large\ndatasets.",
          "link": "http://arxiv.org/abs/2201.02885",
          "publishedOn": "2022-01-12T00:38:42.901Z",
          "wordCount": 681,
          "title": "Agricultural Plant Cataloging and Establishment of a Data Framework from UAV-based Crop Images by Computer Vision. (arXiv:2201.02885v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruenin_P/0/1/0/all/0/1\">Pisol Ruenin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Techakaew_S/0/1/0/all/0/1\">Sarayut Techakaew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Towatrakool_P/0/1/0/all/0/1\">Patsakorn Towatrakool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chawachat_J/0/1/0/all/0/1\">Jakarin Chawachat</a>",
          "description": "Falling, especially in the elderly, is a critical issue to care for and\nsurveil. There have been many studies focusing on fall detection. However, from\nour survey, there is still no research indicating the prior-fall activities,\nwhich we believe that they have a strong correlation with the intensity of the\nfall. The purpose of this research is to develop a fall alert system that also\nidentifies prior-fall activities. First, we want to find a suitable location to\nattach a sensor to the body. We created multiple-spot on-body devices to\ncollect various activity data. We used that dataset to train 5 different\nclassification models. We selected the XGBoost classification model for\ndetecting a prior-fall activity and the chest location for use in fall\ndetection from a comparison of the detection accuracy. We then tested 3\nexisting fall detection threshold algorithms to detect fall and fall to their\nknees first, and selected the 3-phase threshold algorithm of Chaitep and\nChawachat [3] in our system. From the experiment, we found that the fall\ndetection accuracy is 88.91%, the fall to their knees first detection accuracy\nis 91.25%, and the average accuracy of detection of prior-fall activities is\n86.25%. Although we use an activity dataset of young to middle-aged adults\n(18-49 years), we are confident that this system can be developed to monitor\nactivities before the fall, especially in the elderly, so that caretakers can\nbetter manage the situation.",
          "link": "http://arxiv.org/abs/2201.02803",
          "publishedOn": "2022-01-12T00:38:42.827Z",
          "wordCount": 656,
          "title": "A fall alert system with prior-fall activity identification. (arXiv:2201.02803v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haixu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiehui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>",
          "description": "Extending the forecasting time is a critical demand for real applications,\nsuch as extreme weather early warning and long-term energy consumption\nplanning. This paper studies the long-term forecasting problem of time series.\nPrior Transformer-based models adopt various self-attention mechanisms to\ndiscover the long-range dependencies. However, intricate temporal patterns of\nthe long-term future prohibit the model from finding reliable dependencies.\nAlso, Transformers have to adopt the sparse versions of point-wise\nself-attentions for long series efficiency, resulting in the information\nutilization bottleneck. Going beyond Transformers, we design Autoformer as a\nnovel decomposition architecture with an Auto-Correlation mechanism. We break\nwith the pre-processing convention of series decomposition and renovate it as a\nbasic inner block of deep models. This design empowers Autoformer with\nprogressive decomposition capacities for complex time series. Further, inspired\nby the stochastic process theory, we design the Auto-Correlation mechanism\nbased on the series periodicity, which conducts the dependencies discovery and\nrepresentation aggregation at the sub-series level. Auto-Correlation\noutperforms self-attention in both efficiency and accuracy. In long-term\nforecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative\nimprovement on six benchmarks, covering five practical applications: energy,\ntraffic, economics, weather and disease. Code is available at this repository:\n\\url{https://github.com/thuml/Autoformer}.",
          "link": "http://arxiv.org/abs/2106.13008",
          "publishedOn": "2022-01-11T00:39:36.154Z",
          "wordCount": 674,
          "title": "Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting. (arXiv:2106.13008v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Cheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_P/0/1/0/all/0/1\">Pengwei Tian</a>",
          "description": "Recent advances in AIoT technologies have led to an increasing popularity of\nutilizing machine learning algorithms to detect operational failures for\ncyber-physical systems (CPS). In its basic form, an anomaly detection module\nmonitors the sensor measurements and actuator states from the physical plant,\nand detects anomalies in these measurements to identify abnormal operation\nstatus. Nevertheless, building effective anomaly detection models for CPS is\nrather challenging as the model has to accurately detect anomalies in presence\nof highly complicated system dynamics and unknown amount of sensor noise. In\nthis work, we propose a novel time series anomaly detection method called\nNeural System Identification and Bayesian Filtering (NSIBF) in which a\nspecially crafted neural network architecture is posed for system\nidentification, i.e., capturing the dynamics of CPS in a dynamical state-space\nmodel; then a Bayesian filtering algorithm is naturally applied on top of the\n\"identified\" state-space model for robust anomaly detection by tracking the\nuncertainty of the hidden state of the system recursively over time. We provide\nqualitative as well as quantitative experiments with the proposed method on a\nsynthetic and three real-world CPS datasets, showing that NSIBF compares\nfavorably to the state-of-the-art methods with considerable improvements on\nanomaly detection in CPS.",
          "link": "http://arxiv.org/abs/2106.07992",
          "publishedOn": "2022-01-11T00:39:36.147Z",
          "wordCount": 671,
          "title": "Time Series Anomaly Detection for Cyber-Physical Systems via Neural System Identification and Bayesian Filtering. (arXiv:2106.07992v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.03347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kabir_H/0/1/0/all/0/1\">H M Dipu Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdar_M/0/1/0/all/0/1\">Moloud Abdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalali_S/0/1/0/all/0/1\">Seyed Mohammad Jafar Jalali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atiya_A/0/1/0/all/0/1\">Amir F Atiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_D/0/1/0/all/0/1\">Dipti Srinivasan</a>",
          "description": "Deep neural networks (DNNs) have achieved the state of the art performance in\nnumerous fields. However, DNNs need high computation times, and people always\nexpect better performance in a lower computation. Therefore, we study the human\nsomatosensory system and design a neural network (SpinalNet) to achieve higher\naccuracy with fewer computations. Hidden layers in traditional NNs receive\ninputs in the previous layer, apply activation function, and then transfer the\noutcomes to the next layer. In the proposed SpinalNet, each layer is split into\nthree splits: 1) input split, 2) intermediate split, and 3) output split. Input\nsplit of each layer receives a part of the inputs. The intermediate split of\neach layer receives outputs of the intermediate split of the previous layer and\noutputs of the input split of the current layer. The number of incoming weights\nbecomes significantly lower than traditional DNNs. The SpinalNet can also be\nused as the fully connected or classification layer of DNN and supports both\ntraditional learning and transfer learning. We observe significant error\nreductions with lower computational costs in most of the DNNs. Traditional\nlearning on the VGG-5 network with SpinalNet classification layers provided the\nstate-of-the-art (SOTA) performance on QMNIST, Kuzushiji-MNIST, EMNIST\n(Letters, Digits, and Balanced) datasets. Traditional learning with ImageNet\npre-trained initial weights and SpinalNet classification layers provided the\nSOTA performance on STL-10, Fruits 360, Bird225, and Caltech-101 datasets. The\nscripts of the proposed SpinalNet are available at the following link:\nhttps://github.com/dipuk0506/SpinalNet",
          "link": "http://arxiv.org/abs/2007.03347",
          "publishedOn": "2022-01-11T00:39:36.131Z",
          "wordCount": 731,
          "title": "SpinalNet: Deep Neural Network with Gradual Input. (arXiv:2007.03347v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.08176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Strouse_D/0/1/0/all/0/1\">DJ Strouse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKee_K/0/1/0/all/0/1\">Kevin R. McKee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1\">Matt Botvinick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_E/0/1/0/all/0/1\">Edward Hughes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Everett_R/0/1/0/all/0/1\">Richard Everett</a>",
          "description": "Collaborating with humans requires rapidly adapting to their individual\nstrengths, weaknesses, and preferences. Unfortunately, most standard\nmulti-agent reinforcement learning techniques, such as self-play (SP) or\npopulation play (PP), produce agents that overfit to their training partners\nand do not generalize well to humans. Alternatively, researchers can collect\nhuman data, train a human model using behavioral cloning, and then use that\nmodel to train \"human-aware\" agents (\"behavioral cloning play\", or BCP). While\nsuch an approach can improve the generalization of agents to new human\nco-players, it involves the onerous and expensive step of collecting large\namounts of human data first. Here, we study the problem of how to train agents\nthat collaborate well with human partners without using human data. We argue\nthat the crux of the problem is to produce a diverse set of training partners.\nDrawing inspiration from successful multi-agent approaches in competitive\ndomains, we find that a surprisingly simple approach is highly effective. We\ntrain our agent partner as the best response to a population of self-play\nagents and their past checkpoints taken throughout training, a method we call\nFictitious Co-Play (FCP). Our experiments focus on a two-player collaborative\ncooking simulator that has recently been proposed as a challenge problem for\ncoordination with humans. We find that FCP agents score significantly higher\nthan SP, PP, and BCP when paired with novel agent and human partners.\nFurthermore, humans also report a strong subjective preference to partnering\nwith FCP agents over all baselines.",
          "link": "http://arxiv.org/abs/2110.08176",
          "publishedOn": "2022-01-11T00:39:36.117Z",
          "wordCount": 705,
          "title": "Collaborating with Humans without Human Data. (arXiv:2110.08176v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02535",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Morabit_M/0/1/0/all/0/1\">Mouad Morabit</a>, <a href=\"http://arxiv.org/find/math/1/au:+Desaulniers_G/0/1/0/all/0/1\">Guy Desaulniers</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lodi_A/0/1/0/all/0/1\">Andrea Lodi</a>",
          "description": "Column generation is an iterative method used to solve a variety of\noptimization problems. It decomposes the problem into two parts: a master\nproblem, and one or more pricing problems (PP). The total computing time taken\nby the method is divided between these two parts. In routing or scheduling\napplications, the problems are mostly defined on a network, and the PP is\nusually an NP-hard shortest path problem with resource constraints. In this\nwork, we propose a new heuristic pricing algorithm based on machine learning.\nBy taking advantage of the data collected during previous executions, the\nobjective is to reduce the size of the network and accelerate the PP, keeping\nonly the arcs that have a high chance to be part of the linear relaxation\nsolution. The method has been applied to two specific problems: the vehicle and\ncrew scheduling problem in public transit and the vehicle routing problem with\ntime windows. Reductions in computational time of up to 40% can be obtained.",
          "link": "http://arxiv.org/abs/2201.02535",
          "publishedOn": "2022-01-11T00:39:36.093Z",
          "wordCount": 589,
          "title": "Machine-learning-based arc selection for constrained shortest path problems in column generation. (arXiv:2201.02535v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2008.04790",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Avrachenkov_K/0/1/0/all/0/1\">Konstantin Avrachenkov</a>, <a href=\"http://arxiv.org/find/math/1/au:+Dreveton_M/0/1/0/all/0/1\">Maximilien Dreveton</a>, <a href=\"http://arxiv.org/find/math/1/au:+Leskela_L/0/1/0/all/0/1\">Lasse Leskel&#xe4;</a>",
          "description": "This article studies the estimation of community memberships from non-binary\npair interactions represented by an $N$-by-$N$ tensor whose values are elements\nof $\\mathcal S$, where $N$ is the number of nodes and $\\mathcal S$ is the space\nof the pairwise interactions between the nodes. As an information-theoretic\nbenchmark, we study data sets generated by a non-binary stochastic block model,\nand derive fundamental information criteria for the recovery of the community\nmemberships as $N \\to \\infty$. Examples of applications include weighted\nnetworks ($\\mathcal S = \\mathbb R$), link-labeled networks $(\\mathcal S = \\{0,\n1, \\dots, L\\}$), multiplex networks $(\\mathcal S = \\{0,1\\}^M$) and temporal\nnetworks ($\\mathcal S = \\{0,1\\}^T$).\n\nFor temporal interactions, we show that (i) even a small increase in $T$ may\nhave a big impact on the recovery of community memberships, (ii) consistent\nrecovery is possible even for very sparse data (e.g.\\ bounded average degree)\nwhen $T$ is large enough. We also present several estimation algorithms, both\noffline and online, which fully utilise the temporal nature of the observed\ndata. We analyse the accuracy of the proposed estimation algorithms under\nvarious assumptions on data sparsity and identifiability. Numerical experiments\nshow that even a poor initial estimate (e.g., blind random guess) of the\ncommunity assignment leads to high accuracy obtained by the online algorithm\nafter a small number of iterations, and remarkably so also in very sparse\nregimes.",
          "link": "http://arxiv.org/abs/2008.04790",
          "publishedOn": "2022-01-11T00:39:36.080Z",
          "wordCount": 681,
          "title": "Community recovery in non-binary and temporal stochastic block models. (arXiv:2008.04790v3 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.10329",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jeon_W/0/1/0/all/0/1\">Woojay Jeon</a>",
          "description": "This paper proposes a novel acoustic word embedding called Acoustic Neighbor\nEmbeddings where speech or text of arbitrary length are mapped to a vector\nspace of fixed, reduced dimensions by adapting stochastic neighbor embedding\n(SNE) to sequential inputs. The Euclidean distance between coordinates in the\nembedding space reflects the phonetic confusability between their corresponding\nsequences. Two encoder neural networks are trained: an acoustic encoder that\naccepts speech signals in the form of frame-wise subword posterior\nprobabilities obtained from an acoustic model and a text encoder that accepts\ntext in the form of subword transcriptions. Compared to a triplet loss\ncriterion, the proposed method is shown to have more effective gradients for\nneural network training. Experimentally, it also gives more accurate results\nwith low-dimensional embeddings when the two encoder networks are used in\ntandem in a word (name) recognition task, and when the text encoder network is\nused standalone in an approximate phonetic matching task. In particular, in an\nisolated name recognition task depending solely on Euclidean nearest-neighbor\nsearch between the proposed embedding vectors, the recognition accuracy is\nidentical to that of conventional finite state transducer(FST)-based decoding\nusing test data with up to 1 million names in the vocabulary and 40 dimensions\nin the embeddings.",
          "link": "http://arxiv.org/abs/2007.10329",
          "publishedOn": "2022-01-11T00:39:36.054Z",
          "wordCount": 677,
          "title": "Acoustic Neighbor Embeddings. (arXiv:2007.10329v5 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1\">Aske Plaat</a>",
          "description": "Deep reinforcement learning has gathered much attention recently. Impressive\nresults were achieved in activities as diverse as autonomous driving, game\nplaying, molecular recombination, and robotics. In all these fields, computer\nprograms have taught themselves to solve difficult problems. They have learned\nto fly model helicopters and perform aerobatic manoeuvers such as loops and\nrolls. In some applications they have even become better than the best humans,\nsuch as in Atari, Go, poker and StarCraft. The way in which deep reinforcement\nlearning explores complex environments reminds us of how children learn, by\nplayfully trying out things, getting feedback, and trying again. The computer\nseems to truly possess aspects of human learning; this goes to the heart of the\ndream of artificial intelligence. The successes in research have not gone\nunnoticed by educators, and universities have started to offer courses on the\nsubject. The aim of this book is to provide a comprehensive overview of the\nfield of deep reinforcement learning. The book is written for graduate students\nof artificial intelligence, and for researchers and practitioners who wish to\nbetter understand deep reinforcement learning methods and their challenges. We\nassume an undergraduate-level of understanding of computer science and\nartificial intelligence; the programming language of this book is Python. We\ndescribe the foundations, the algorithms and the applications of deep\nreinforcement learning. We cover the established model-free and model-based\nmethods that form the basis of the field. Developments go quickly, and we also\ncover advanced topics: deep multi-agent reinforcement learning, deep\nhierarchical reinforcement learning, and deep meta learning.",
          "link": "http://arxiv.org/abs/2201.02135",
          "publishedOn": "2022-01-11T00:39:36.049Z",
          "wordCount": 669,
          "title": "Deep Reinforcement Learning. (arXiv:2201.02135v1 [cs.AI] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.08078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baharisangari_N/0/1/0/all/0/1\">Nasim Baharisangari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirota_K/0/1/0/all/0/1\">Kazuma Hirota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Ruixuan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Julius_A/0/1/0/all/0/1\">Agung Julius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>",
          "description": "Extracting spatial-temporal knowledge from data is useful in many\napplications. It is important that the obtained knowledge is\nhuman-interpretable and amenable to formal analysis. In this paper, we propose\na method that trains neural networks to learn spatial-temporal properties in\nthe form of weighted graph-based signal temporal logic (wGSTL) formulas. For\nlearning wGSTL formulas, we introduce a flexible wGSTL formula structure in\nwhich the user's preference can be applied in the inferred wGSTL formulas. In\nthe proposed framework, each neuron of the neural networks corresponds to a\nsubformula in a flexible wGSTL formula structure. We initially train a neural\nnetwork to learn the wGSTL operators and then train a second neural network to\nlearn the parameters in a flexible wGSTL formula structure. We use a COVID-19\ndataset and a rain prediction dataset to evaluate the performance of the\nproposed framework and algorithms. We compare the performance of the proposed\nframework with three baseline classification methods including K-nearest\nneighbors, decision trees, support vector machine, and artificial neural\nnetworks. The classification accuracy obtained by the proposed framework is\ncomparable with the baseline classification methods.",
          "link": "http://arxiv.org/abs/2109.08078",
          "publishedOn": "2022-01-11T00:39:36.042Z",
          "wordCount": 686,
          "title": "Weighted Graph-Based Signal Temporal Logic Inference Using Neural Networks. (arXiv:2109.08078v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02547",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Carson_W/0/1/0/all/0/1\">William E. Carson IV</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Talbot_A/0/1/0/all/0/1\">Austin Talbot</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carlson_D/0/1/0/all/0/1\">David Carlson</a>",
          "description": "Deep autoencoders are often extended with a supervised or adversarial loss to\nlearn latent representations with desirable properties, such as greater\npredictivity of labels and outcomes or fairness with respects to a sensitive\nvariable. Despite the ubiquity of supervised and adversarial deep latent factor\nmodels, these methods should demonstrate improvement over simpler linear\napproaches to be preferred in practice. This necessitates a reproducible linear\nanalog that still adheres to an augmenting supervised or adversarial objective.\nWe address this methodological gap by presenting methods that augment the\nprincipal component analysis (PCA) objective with either a supervised or an\nadversarial objective and provide analytic and reproducible solutions. We\nimplement these methods in an open-source Python package, AugmentedPCA, that\ncan produce excellent real-world baselines. We demonstrate the utility of these\nfactor models on an open-source, RNA-seq cancer gene expression dataset,\nshowing that augmenting with a supervised objective results in improved\ndownstream classification performance, produces principal components with\ngreater class fidelity, and facilitates identification of genes aligned with\nthe principal axes of data variance with implications to development of\nspecific types of cancer.",
          "link": "http://arxiv.org/abs/2201.02547",
          "publishedOn": "2022-01-11T00:39:36.020Z",
          "wordCount": 619,
          "title": "AugmentedPCA: A Python Package of Supervised and Adversarial Linear Factor Models. (arXiv:2201.02547v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1909.04226",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Sarma_A/0/1/0/all/0/1\">Abhijat Sarma</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chatterjee_R/0/1/0/all/0/1\">Rupak Chatterjee</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Gili_K/0/1/0/all/0/1\">Kaitlin Gili</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yu_T/0/1/0/all/0/1\">Ting Yu</a>",
          "description": "Machine learning algorithms perform well on identifying patterns in many\ndifferent datasets due to their versatility. However, as one increases the size\nof the dataset, the computation time for training and using these statistical\nmodels grows quickly. Quantum computing offers a new paradigm which may have\nthe ability to overcome these computational difficulties. Here, we propose a\nquantum analogue to K-means clustering, implement it on simulated\nsuperconducting qubits, and compare it to a previously developed quantum\nsupport vector machine. We find the algorithm's accuracy comparable to the\nclassical K-means algorithm for clustering and classification problems, and\nfind that it has asymptotic complexity $O(N^{3/2}K^{1/2}\\log{P})$, where $N$ is\nthe number of data points, $K$ is the number of clusters, and $P$ is the\ndimension of the data points, giving a significant speedup over the classical\nanalogue.",
          "link": "http://arxiv.org/abs/1909.04226",
          "publishedOn": "2022-01-11T00:39:35.984Z",
          "wordCount": 586,
          "title": "Quantum Unsupervised and Supervised Learning on Superconducting Processors. (arXiv:1909.04226v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_W/0/1/0/all/0/1\">Wai Weng Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Layeghy_S/0/1/0/all/0/1\">Siamak Layeghy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarhan_M/0/1/0/all/0/1\">Mohanad Sarhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallagher_M/0/1/0/all/0/1\">Marcus Gallagher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portmann_M/0/1/0/all/0/1\">Marius Portmann</a>",
          "description": "This paper presents a new Network Intrusion Detection System (NIDS) based on\nGraph Neural Networks (GNNs). GNNs are a relatively new sub-field of deep\nneural networks, which can leverage the inherent structure of graph-based data.\nTraining and evaluation data for NIDSs are typically represented as flow\nrecords, which can naturally be represented in a graph format. In this paper,\nwe propose E-GraphSAGE, a GNN approach that allows capturing both the edge\nfeatures of a graph as well as the topological information for network\nintrusion detection in IoT networks. To the best of our knowledge, our proposal\nis the first successful, practical, and extensively evaluated approach of\napplying GNNs on the problem of network intrusion detection for IoT using\nflow-based data. Our extensive experimental evaluation on four recent NIDS\nbenchmark datasets shows that our approach outperforms the state-of-the-art in\nterms of key classification metrics, which demonstrates the potential of GNNs\nin network intrusion detection, and provides motivation for further research.",
          "link": "http://arxiv.org/abs/2103.16329",
          "publishedOn": "2022-01-11T00:39:35.969Z",
          "wordCount": 686,
          "title": "E-GraphSAGE: A Graph Neural Network based Intrusion Detection System for IoT. (arXiv:2103.16329v7 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.08338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Mengjiao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sane_S/0/1/0/all/0/1\">Sudhanshu Sane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_C/0/1/0/all/0/1\">Chris R. Johnson</a>",
          "description": "Time-varying vector fields produced by computational fluid dynamics\nsimulations are often prohibitively large and pose challenges for accurate\ninteractive analysis and exploration. To address these challenges, reduced\nLagrangian representations have been increasingly researched as a means to\nimprove scientific time-varying vector field exploration capabilities. This\npaper presents a novel deep neural network-based particle tracing method to\nexplore time-varying vector fields represented by Lagrangian flow maps. In our\nworkflow, in situ processing is first utilized to extract Lagrangian flow maps,\nand deep neural networks then use the extracted data to learn flow field\nbehavior. Using a trained model to predict new particle trajectories offers a\nfixed small memory footprint and fast inference. To demonstrate and evaluate\nthe proposed method, we perform an in-depth study of performance using a\nwell-known analytical data set, the Double Gyre. Our study considers two flow\nmap extraction strategies as well as the impact of the number of training\nsamples and integration durations on efficacy, evaluates multiple sampling\noptions for training and testing and informs hyperparameter settings. Overall,\nwe find our method requires a fixed memory footprint of 10.5 MB to encode a\nLagrangian representation of a time-varying vector field while maintaining\naccuracy. For post hoc analysis, loading the trained model costs only two\nseconds, significantly reducing the burden of I/O when reading data for\nvisualization. Moreover, our parallel implementation can infer one hundred\nlocations for each of two thousand new pathlines across the entire temporal\nresolution in 1.3 seconds using one NVIDIA Titan RTX GPU.",
          "link": "http://arxiv.org/abs/2110.08338",
          "publishedOn": "2022-01-11T00:39:35.956Z",
          "wordCount": 710,
          "title": "Exploratory Lagrangian-Based Particle Tracing Using Deep Learning. (arXiv:2110.08338v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02432",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Llorente_F/0/1/0/all/0/1\">Fernando Llorente</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Martino_L/0/1/0/all/0/1\">Luca Martino</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Read_J/0/1/0/all/0/1\">Jesse Read</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Delgado_Gomez_D/0/1/0/all/0/1\">David Delgado-G&#xf3;mez</a>",
          "description": "In this work, we analyze the noisy importance sampling (IS), i.e., IS working\nwith noisy evaluations of the target density. We present the general framework\nand derive optimal proposal densities for noisy IS estimators. The optimal\nproposals incorporate the information of the variance of the noisy\nrealizations, proposing points in regions where the noise power is higher. We\nalso compare the use of the optimal proposals with previous optimality\napproaches considered in a noisy IS framework.",
          "link": "http://arxiv.org/abs/2201.02432",
          "publishedOn": "2022-01-11T00:39:35.900Z",
          "wordCount": 497,
          "title": "Optimality in Noisy Importance Sampling. (arXiv:2201.02432v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2010.10029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1\">Guillaume Rabusseau</a>",
          "description": "In this paper, we present connections between three models used in different\nresearch fields: weighted finite automata~(WFA) from formal languages and\nlinguistics, recurrent neural networks used in machine learning, and tensor\nnetworks which encompasses a set of optimization techniques for high-order\ntensors used in quantum physics and numerical analysis. We first present an\nintrinsic relation between WFA and the tensor train decomposition, a particular\nform of tensor network. This relation allows us to exhibit a novel low rank\nstructure of the Hankel matrix of a function computed by a WFA and to design an\nefficient spectral learning algorithm leveraging this structure to scale the\nalgorithm up to very large Hankel matrices.We then unravel a fundamental\nconnection between WFA and second-orderrecurrent neural networks~(2-RNN): in\nthe case of sequences of discrete symbols, WFA and 2-RNN with linear\nactivationfunctions are expressively equivalent. Leveraging this equivalence\nresult combined with the classical spectral learning algorithm for weighted\nautomata, we introduce the first provable learning algorithm for linear 2-RNN\ndefined over sequences of continuous input vectors.This algorithm relies on\nestimating low rank sub-blocks of the Hankel tensor, from which the parameters\nof a linear 2-RNN can be provably recovered. The performances of the proposed\nlearning algorithm are assessed in a simulation study on both synthetic and\nreal-world data.",
          "link": "http://arxiv.org/abs/2010.10029",
          "publishedOn": "2022-01-11T00:39:35.878Z",
          "wordCount": 687,
          "title": "Connecting Weighted Automata, Tensor Networks and Recurrent Neural Networks through Spectral Learning. (arXiv:2010.10029v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03090",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bartolomaeus_W/0/1/0/all/0/1\">Wiebke Bartolomaeus</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Boutaib_Y/0/1/0/all/0/1\">Youness Boutaib</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nestler_S/0/1/0/all/0/1\">Sandra Nestler</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rauhut_H/0/1/0/all/0/1\">Holger Rauhut</a>",
          "description": "We investigate the functioning of a classifying biological neural network\nfrom the perspective of statistical learning theory, modelled, in a simplified\nsetting, as a continuous-time stochastic recurrent neural network (RNN) with\nidentity activation function. In the purely stochastic (robust) regime, we give\na generalisation error bound that holds with high probability, thus showing\nthat the empirical risk minimiser is the best-in-class hypothesis. We show that\nRNNs retain a partial signature of the paths they are fed as the unique\ninformation exploited for training and classification tasks. We argue that\nthese RNNs are easy to train and robust and back these observations with\nnumerical experiments on both synthetic and real data. We also exhibit a\ntrade-off phenomenon between accuracy and robustness.",
          "link": "http://arxiv.org/abs/2108.03090",
          "publishedOn": "2022-01-11T00:39:35.872Z",
          "wordCount": 574,
          "title": "Path classification by stochastic linear recurrent neural networks. (arXiv:2108.03090v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.13408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duffy_K/0/1/0/all/0/1\">Kate Duffy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandal_T/0/1/0/all/0/1\">Thomas Vandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weile Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemani_R/0/1/0/all/0/1\">Ramakrishna Nemani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1\">Auroop R. Ganguly</a>",
          "description": "Numerical models based on physics represent the state-of-the-art in earth\nsystem modeling and comprise our best tools for generating insights and\npredictions. Despite rapid growth in computational power, the perceived need\nfor higher model resolutions overwhelms the latest-generation computers,\nreducing the ability of modelers to generate simulations for understanding\nparameter sensitivities and characterizing variability and uncertainty. Thus,\nsurrogate models are often developed to capture the essential attributes of the\nfull-blown numerical models. Recent successes of machine learning methods,\nespecially deep learning, across many disciplines offer the possibility that\ncomplex nonlinear connectionist representations may be able to capture the\nunderlying complex structures and nonlinear processes in earth systems. A\ndifficult test for deep learning-based emulation, which refers to function\napproximation of numerical models, is to understand whether they can be\ncomparable to traditional forms of surrogate models in terms of computational\nefficiency while simultaneously reproducing model results in a credible manner.\nA deep learning emulation that passes this test may be expected to perform even\nbetter than simple models with respect to capturing complex processes and\nspatiotemporal dependencies. Here we examine, with a case study in\nsatellite-based remote sensing, the hypothesis that deep learning approaches\ncan credibly represent the simulations from a surrogate model with comparable\ncomputational efficiency. Our results are encouraging in that the deep learning\nemulation reproduces the results with acceptable accuracy and often even faster\nperformance. We discuss the broader implications of our results in light of the\npace of improvements in high-performance implementations of deep learning as\nwell as the growing desire for higher-resolution simulations in the earth\nsciences.",
          "link": "http://arxiv.org/abs/1910.13408",
          "publishedOn": "2022-01-11T00:39:35.865Z",
          "wordCount": 750,
          "title": "A framework for deep learning emulation of numerical models with a case study in satellite remote sensing. (arXiv:1910.13408v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09716",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "Identifying the status of individual network units is critical for\nunderstanding the mechanism of convolutional neural networks (CNNs). However,\nit is still challenging to reliably give a general indication of unit status,\nespecially for units in different network models. To this end, we propose a\nnovel method for quantitatively clarifying the status of single unit in CNN\nusing algebraic topological tools. Unit status is indicated via the calculation\nof a defined topological-based entropy, called feature entropy, which measures\nthe degree of chaos of the global spatial pattern hidden in the unit for a\ncategory. In this way, feature entropy could provide an accurate indication of\nstatus for units in different networks with diverse situations like\nweight-rescaling operation. Further, we show that feature entropy decreases as\nthe layer goes deeper and shares almost simultaneous trend with loss during\ntraining. We show that by investigating the feature entropy of units on only\ntraining data, it could give discrimination between networks with different\ngeneralization ability from the view of the effectiveness of feature\nrepresentations.",
          "link": "http://arxiv.org/abs/2103.09716",
          "publishedOn": "2022-01-11T00:39:35.859Z",
          "wordCount": 631,
          "title": "Quantitative Performance Assessment of CNN Units via Topological Entropy Calculation. (arXiv:2103.09716v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10845",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Wang_H/0/1/0/all/0/1\">Hanrui Wang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ding_Y/0/1/0/all/0/1\">Yongshan Ding</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Gu_J/0/1/0/all/0/1\">Jiaqi Gu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Li_Z/0/1/0/all/0/1\">Zirui Li</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Lin_Y/0/1/0/all/0/1\">Yujun Lin</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Pan_D/0/1/0/all/0/1\">David Z. Pan</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chong_F/0/1/0/all/0/1\">Frederic T. Chong</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>",
          "description": "Quantum noise is the key challenge in Noisy Intermediate-Scale Quantum (NISQ)\ncomputers. Previous work for mitigating noise has primarily focused on\ngate-level or pulse-level noise-adaptive compilation. However, limited research\nefforts have explored a higher level of optimization by making the quantum\ncircuits themselves resilient to noise.\n\nWe propose QuantumNAS, a comprehensive framework for noise-adaptive co-search\nof the variational circuit and qubit mapping. Variational quantum circuits are\na promising approach for constructing QML and quantum simulation. However,\nfinding the best variational circuit and its optimal parameters is challenging\ndue to the large design space and parameter training cost. We propose to\ndecouple the circuit search and parameter training by introducing a novel\nSuperCircuit. The SuperCircuit is constructed with multiple layers of\npre-defined parameterized gates and trained by iteratively sampling and\nupdating the parameter subsets (SubCircuits) of it. It provides an accurate\nestimation of SubCircuits performance trained from scratch. Then we perform an\nevolutionary co-search of SubCircuit and its qubit mapping. The SubCircuit\nperformance is estimated with parameters inherited from SuperCircuit and\nsimulated with real device noise models. Finally, we perform iterative gate\npruning and finetuning to remove redundant gates.\n\nExtensively evaluated with 12 QML and VQE benchmarks on 14 quantum computers,\nQuantumNAS significantly outperforms baselines. For QML, QuantumNAS is the\nfirst to demonstrate over 95% 2-class, 85% 4-class, and 32% 10-class\nclassification accuracy on real QC. It also achieves the lowest eigenvalue for\nVQE tasks on H2, H2O, LiH, CH4, BeH2 compared with UCCSD. We also open-source\nTorchQuantum (https://github.com/mit-han-lab/torchquantum) for fast training of\nparameterized quantum circuits to facilitate future research.",
          "link": "http://arxiv.org/abs/2107.10845",
          "publishedOn": "2022-01-11T00:39:35.713Z",
          "wordCount": 756,
          "title": "QuantumNAS: Noise-Adaptive Search for Robust Quantum Circuits. (arXiv:2107.10845v5 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02213",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Elmaghbub_A/0/1/0/all/0/1\">Abdurrahman Elmaghbub</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamdaoui_B/0/1/0/all/0/1\">Bechir Hamdaoui</a>",
          "description": "Deep learning-based RF fingerprinting has recently been recognized as a\npotential solution for enabling newly emerging wireless network applications,\nsuch as spectrum access policy enforcement, automated network device\nauthentication, and unauthorized network access monitoring and control. Real,\ncomprehensive RF datasets are now needed more than ever to enable the study,\nassessment, and validation of newly developed RF fingerprinting approaches. In\nthis paper, we present and release a large-scale RF fingerprinting dataset,\ncollected from 25 different LoRa-enabled IoT transmitting devices using USRP\nB210 receivers. Our dataset consists of a large number of SigMF-compliant\nbinary files representing the I/Q time-domain samples and their corresponding\nFFT-based files of LoRa transmissions. This dataset provides a comprehensive\nset of essential experimental scenarios, considering both indoor and outdoor\nenvironments and various network deployments and configurations, such as the\ndistance between the transmitters and the receiver, the configuration of the\nconsidered LoRa modulation, the physical location of the conducted experiment,\nand the receiver hardware used for training and testing the neural network\nmodels.",
          "link": "http://arxiv.org/abs/2201.02213",
          "publishedOn": "2022-01-11T00:39:35.696Z",
          "wordCount": 608,
          "title": "Comprehensive RF Dataset Collection and Release: A Deep Learning-Based Device Fingerprinting Use Case. (arXiv:2201.02213v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02582",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhuyan_N/0/1/0/all/0/1\">Neelkamal Bhuyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moharir_S/0/1/0/all/0/1\">Sharayu Moharir</a>",
          "description": "Federated learning is a form of distributed learning with the key challenge\nbeing the non-identically distributed nature of the data in the participating\nclients. In this paper, we extend federated learning to the setting where\nmultiple unrelated models are trained simultaneously. Specifically, every\nclient is able to train any one of M models at a time and the server maintains\na model for each of the M models which is typically a suitably averaged version\nof the model computed by the clients. We propose multiple policies for\nassigning learning tasks to clients over time. In the first policy, we extend\nthe widely studied FedAvg to multi-model learning by allotting models to\nclients in an i.i.d. stochastic manner. In addition, we propose two new\npolicies for client selection in a multi-model federated setting which make\ndecisions based on current local losses for each client-model pair. We compare\nthe performance of the policies on tasks involving synthetic and real-world\ndata and characterize the performance of the proposed policies. The key\ntake-away from our work is that the proposed multi-model policies perform\nbetter or at least as good as single model training using FedAvg.",
          "link": "http://arxiv.org/abs/2201.02582",
          "publishedOn": "2022-01-11T00:39:35.482Z",
          "wordCount": 600,
          "title": "Multi-Model Federated Learning. (arXiv:2201.02582v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02397",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kiermayer_M/0/1/0/all/0/1\">Mark Kiermayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_C/0/1/0/all/0/1\">Christian Wei&#xdf;</a>",
          "description": "Markov chains play a key role in a vast number of areas, including life\ninsurance mathematics. Standard actuarial quantities as the premium value can\nbe interpreted as compressed, lossy information about the underlying Markov\nprocess. We introduce a method to reconstruct the underlying Markov chain given\ncollective information of a portfolio of contracts. Our neural architecture\nexplainably characterizes the process by explicitly providing one-step\ntransition probabilities. Further, we provide an intrinsic, economic model\nvalidation to inspect the quality of the information decompression. Lastly, our\nmethodology is successfully tested for a realistic data set of German term life\ninsurance contracts.",
          "link": "http://arxiv.org/abs/2201.02397",
          "publishedOn": "2022-01-11T00:39:35.471Z",
          "wordCount": null,
          "title": "Neural calibration of hidden inhomogeneous Markov chains -- Information decompression in life insurance. (arXiv:2201.02397v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02445",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Belharbi_S/0/1/0/all/0/1\">Soufiane Belharbi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McCaffrey_L/0/1/0/all/0/1\">Luke McCaffrey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>",
          "description": "Using only global annotations such as the image class labels,\nweakly-supervised learning methods allow CNN classifiers to jointly classify an\nimage, and yield the regions of interest associated with the predicted class.\nHowever, without any guidance at the pixel level, such methods may yield\ninaccurate regions. This problem is known to be more challenging with histology\nimages than with natural ones, since objects are less salient, structures have\nmore variations, and foreground and background regions have stronger\nsimilarities. Therefore, methods in computer vision literature for visual\ninterpretation of CNNs may not directly apply. In this work, we propose a\nsimple yet efficient method based on a composite loss function that leverages\ninformation from the fully negative samples. Our new loss function contains two\ncomplementary terms: the first exploits positive evidence collected from the\nCNN classifier, while the second leverages the fully negative samples from the\ntraining dataset. In particular, we equip a pre-trained classifier with a\ndecoder that allows refining the regions of interest. The same classifier is\nexploited to collect both the positive and negative evidence at the pixel level\nto train the decoder. This enables to take advantages of the fully negative\nsamples that occurs naturally in the data, without any additional supervision\nsignals and using only the image class as supervision. Compared to several\nrecent related methods, over the public benchmark GlaS for colon cancer and a\nCamelyon16 patch-based benchmark for breast cancer using three different\nbackbones, we show the substantial improvements introduced by our method. Our\nresults shows the benefits of using both negative and positive evidence, ie,\nthe one obtained from a classifier and the one naturally available in datasets.\nWe provide an ablation study of both terms. Our code is publicly available.",
          "link": "http://arxiv.org/abs/2201.02445",
          "publishedOn": "2022-01-11T00:39:35.470Z",
          "wordCount": null,
          "title": "Negative Evidence Matters in Interpretable Histology Image Classification. (arXiv:2201.02445v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02534",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1\">Qiaoyu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ninghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Rui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Soo-Hyun Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>",
          "description": "We introduce a novel masked graph autoencoder (MGAE) framework to perform\neffective learning on graph structure data. Taking insights from\nself-supervised learning, we randomly mask a large proportion of edges and try\nto reconstruct these missing edges during training. MGAE has two core designs.\nFirst, we find that masking a high ratio of the input graph structure, e.g.,\n$70\\%$, yields a nontrivial and meaningful self-supervisory task that benefits\ndownstream applications. Second, we employ a graph neural network (GNN) as an\nencoder to perform message propagation on the partially-masked graph. To\nreconstruct the large number of masked edges, a tailored cross-correlation\ndecoder is proposed. It could capture the cross-correlation between the head\nand tail nodes of anchor edge in multi-granularity. Coupling these two designs\nenables MGAE to be trained efficiently and effectively. Extensive experiments\non multiple open datasets (Planetoid and OGB benchmarks) demonstrate that MGAE\ngenerally performs better than state-of-the-art unsupervised learning\ncompetitors on link prediction and node classification.",
          "link": "http://arxiv.org/abs/2201.02534",
          "publishedOn": "2022-01-11T00:39:35.464Z",
          "wordCount": 589,
          "title": "MGAE: Masked Autoencoders for Self-Supervised Learning on Graphs. (arXiv:2201.02534v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hertrich_C/0/1/0/all/0/1\">Christoph Hertrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1\">Amitabh Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Summa_M/0/1/0/all/0/1\">Marco Di Summa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skutella_M/0/1/0/all/0/1\">Martin Skutella</a>",
          "description": "We contribute to a better understanding of the class of functions that is\nrepresented by a neural network with ReLU activations and a given architecture.\nUsing techniques from mixed-integer optimization, polyhedral theory, and\ntropical geometry, we provide a mathematical counterbalance to the universal\napproximation theorems which suggest that a single hidden layer is sufficient\nfor learning tasks. In particular, we investigate whether the class of exactly\nrepresentable functions strictly increases by adding more layers (with no\nrestrictions on size). This problem has potential impact on algorithmic and\nstatistical aspects because of the insight it provides into the class of\nfunctions represented by neural hypothesis classes. However, to the best of our\nknowledge, this question has not been investigated in the neural network\nliterature. We also present upper bounds on the sizes of neural networks\nrequired to represent functions in these neural hypothesis classes.",
          "link": "http://arxiv.org/abs/2105.14835",
          "publishedOn": "2022-01-11T00:39:35.440Z",
          "wordCount": 631,
          "title": "Towards Lower Bounds on the Depth of ReLU Neural Networks. (arXiv:2105.14835v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shipeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>",
          "description": "This paper tackles the problem of few-shot learning, which aims to learn new\nvisual concepts from a few examples. A common problem setting in few-shot\nclassification assumes random sampling strategy in acquiring data labels, which\nis inefficient in practical applications. In this work, we introduce a new\nbudget-aware few-shot learning problem that not only aims to learn novel object\ncategories, but also needs to select informative examples to annotate in order\nto achieve data efficiency.\n\nWe develop a meta-learning strategy for our budget-aware few-shot learning\ntask, which jointly learns a novel data selection policy based on a Graph\nConvolutional Network (GCN) and an example-based few-shot classifier. Our\nselection policy computes a context-sensitive representation for each unlabeled\ndata by graph message passing, which is then used to predict an informativeness\nscore for sequential selection. We validate our method by extensive experiments\non the mini-ImageNet, tiered-ImageNet and Omniglot datasets. The results show\nour few-shot learning strategy outperforms baselines by a sizable margin, which\ndemonstrates the efficacy of our method.",
          "link": "http://arxiv.org/abs/2201.02304",
          "publishedOn": "2022-01-11T00:39:35.430Z",
          "wordCount": 589,
          "title": "Budget-aware Few-shot Learning via Graph Convolutional Network. (arXiv:2201.02304v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2112.03898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zadik_I/0/1/0/all/0/1\">Ilias Zadik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Min Jae Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wein_A/0/1/0/all/0/1\">Alexander S. Wein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>",
          "description": "Clustering is a fundamental primitive in unsupervised learning which gives\nrise to a rich class of computationally-challenging inference tasks. In this\nwork, we focus on the canonical task of clustering d-dimensional Gaussian\nmixtures with unknown (and possibly degenerate) covariance. Recent works (Ghosh\net al. '20; Mao, Wein '21; Davis, Diaz, Wang '21) have established lower bounds\nagainst the class of low-degree polynomial methods and the sum-of-squares (SoS)\nhierarchy for recovering certain hidden structures planted in Gaussian\nclustering instances. Prior work on many similar inference tasks portends that\nsuch lower bounds strongly suggest the presence of an inherent\nstatistical-to-computational gap for clustering, that is, a parameter regime\nwhere the clustering task is statistically possible but no polynomial-time\nalgorithm succeeds.\n\nOne special case of the clustering task we consider is equivalent to the\nproblem of finding a planted hypercube vector in an otherwise random subspace.\nWe show that, perhaps surprisingly, this particular clustering model does not\nexhibit a statistical-to-computational gap, even though the aforementioned\nlow-degree and SoS lower bounds continue to apply in this case. To achieve\nthis, we give a polynomial-time algorithm based on the Lenstra--Lenstra--Lovasz\nlattice basis reduction method which achieves the statistically-optimal sample\ncomplexity of d+1 samples. This result extends the class of problems whose\nconjectured statistical-to-computational gaps can be \"closed\" by \"brittle\"\npolynomial-time algorithms, highlighting the crucial but subtle role of noise\nin the onset of statistical-to-computational gaps.",
          "link": "http://arxiv.org/abs/2112.03898",
          "publishedOn": "2022-01-11T00:39:35.422Z",
          "wordCount": 709,
          "title": "Lattice-Based Methods Surpass Sum-of-Squares in Clustering. (arXiv:2112.03898v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.08693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malik_O/0/1/0/all/0/1\">Osman Asif Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ushijima_Mwesigwa_H/0/1/0/all/0/1\">Hayato Ushijima-Mwesigwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Arnab Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandal_A/0/1/0/all/0/1\">Avradip Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_I/0/1/0/all/0/1\">Indradeep Ghosh</a>",
          "description": "Many fundamental problems in data mining can be reduced to one or more\nNP-hard combinatorial optimization problems. Recent advances in novel\ntechnologies such as quantum and quantum-inspired hardware promise a\nsubstantial speedup for solving these problems compared to when using general\npurpose computers but often require the problem to be modeled in a special\nform, such as an Ising or quadratic unconstrained binary optimization (QUBO)\nmodel, in order to take advantage of these devices. In this work, we focus on\nthe important binary matrix factorization (BMF) problem which has many\napplications in data mining. We propose two QUBO formulations for BMF. We show\nhow clustering constraints can easily be incorporated into these formulations.\nThe special purpose hardware we consider is limited in the number of variables\nit can handle which presents a challenge when factorizing large matrices. We\npropose a sampling based approach to overcome this challenge, allowing us to\nfactorize large rectangular matrices. In addition to these methods, we also\npropose a simple baseline algorithm which outperforms our more sophisticated\nmethods in a few situations. We run experiments on the Fujitsu Digital\nAnnealer, a quantum-inspired complementary metal-oxide-semiconductor (CMOS)\nannealer, on both synthetic and real data, including gene expression data.\nThese experiments show that our approach is able to produce more accurate BMFs\nthan competing methods.",
          "link": "http://arxiv.org/abs/2010.08693",
          "publishedOn": "2022-01-11T00:39:35.376Z",
          "wordCount": 688,
          "title": "Binary matrix factorization on special purpose hardware. (arXiv:2010.08693v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02310",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Radha_S/0/1/0/all/0/1\">Santosh Kumar Radha</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Jao_C/0/1/0/all/0/1\">Casey Jao</a>",
          "description": "The similarity between objects is significant in a broad range of areas.\nWhile similarity can be measured using off-the-shelf distance functions, they\nmay fail to capture the inherent meaning of similarity, which tends to depend\non the underlying data and task. Moreover, conventional distance functions\nlimit the space of similarity measures to be symmetric and do not directly\nallow comparing objects from different spaces. We propose using quantum\nnetworks (GQSim) for learning task-dependent (a)symmetric similarity between\ndata that need not have the same dimensionality. We analyze the properties of\nsuch similarity function analytically (for a simple case) and numerically (for\na complex case) and showthat these similarity measures can extract salient\nfeatures of the data. We also demonstrate that the similarity measure derived\nusing this technique is $(\\epsilon,\\gamma,\\tau)$-good, resulting in\ntheoretically guaranteed performance. Finally, we conclude by applying this\ntechnique for three relevant applications - Classification, Graph Completion,\nGenerative modeling.",
          "link": "http://arxiv.org/abs/2201.02310",
          "publishedOn": "2022-01-11T00:39:35.364Z",
          "wordCount": 564,
          "title": "Generalized quantum similarity learning. (arXiv:2201.02310v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02441",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Akyildirim_E/0/1/0/all/0/1\">Erdinc Akyildirim</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Gambara_M/0/1/0/all/0/1\">Matteo Gambara</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Teichmann_J/0/1/0/all/0/1\">Josef Teichmann</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zhou_S/0/1/0/all/0/1\">Syang Zhou</a>",
          "description": "Anomaly detection is the process of identifying abnormal instances or events\nin data sets which deviate from the norm significantly. In this study, we\npropose a signatures based machine learning algorithm to detect rare or\nunexpected items in a given data set of time series type. We present\napplications of signature or randomized signature as feature extractors for\nanomaly detection algorithms; additionally we provide an easy, representation\ntheoretic justification for the construction of randomized signatures. Our\nfirst application is based on synthetic data and aims at distinguishing between\nreal and fake trajectories of stock prices, which are indistinguishable by\nvisual inspection. We also show a real life application by using transaction\ndata from the cryptocurrency market. In this case, we are able to identify pump\nand dump attempts organized on social networks with F1 scores up to 88% by\nmeans of our unsupervised learning algorithm, thus achieving results that are\nclose to the state-of-the-art in the field based on supervised learning.",
          "link": "http://arxiv.org/abs/2201.02441",
          "publishedOn": "2022-01-11T00:39:35.341Z",
          "wordCount": 591,
          "title": "Applications of Signature Methods to Market Anomaly Detection. (arXiv:2201.02441v1 [q-fin.CP])"
        },
        {
          "id": "http://arxiv.org/abs/2110.07430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Freguglia_V/0/1/0/all/0/1\">Victor Freguglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1\">Nancy Garcia</a>",
          "description": "Markov chains with variable length are useful parsimonious stochastic models\nable to generate most stationary sequence of discrete symbols. The idea is to\nidentify the suffixes of the past, called contexts, that are relevant to\npredict the future symbol. Sometimes a single state is a context, and looking\nat the past and finding this specific state makes the further past irrelevant.\nStates with such property are called renewal states and they can be used to\nsplit the chain into independent and identically distributed blocks. In order\nto identify renewal states for chains with variable length, we propose the use\nof Intrinsic Bayes Factor to evaluate the hypothesis that some particular state\nis a renewal state. In this case, the difficulty lies in integrating the\nmarginal posterior distribution for the random context trees for general prior\ndistribution on the space of context trees, with Dirichlet prior for the\ntransition probabilities, and Monte Carlo methods are applied. To show the\nstrength of our method, we analyzed artificial datasets generated from\ndifferent binary models models and one example coming from the field of\nLinguistics.",
          "link": "http://arxiv.org/abs/2110.07430",
          "publishedOn": "2022-01-11T00:39:35.334Z",
          "wordCount": 638,
          "title": "Detecting Renewal States in Chains of Variable Length via Intrinsic Bayes Factors. (arXiv:2110.07430v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02263",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chuah_W/0/1/0/all/0/1\">WeiQin Chuah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tennakoon_R/0/1/0/all/0/1\">Ruwan Tennakoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoseinnezhad_R/0/1/0/all/0/1\">Reza Hoseinnezhad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bab_Hadiashar_A/0/1/0/all/0/1\">Alireza Bab-Hadiashar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suter_D/0/1/0/all/0/1\">David Suter</a>",
          "description": "State-of-the-art stereo matching networks trained only on synthetic data\noften fail to generalize to more challenging real data domains. In this paper,\nwe attempt to unfold an important factor that hinders the networks from\ngeneralizing across domains: through the lens of shortcut learning. We\ndemonstrate that the learning of feature representations in stereo matching\nnetworks is heavily influenced by synthetic data artefacts (shortcut\nattributes). To mitigate this issue, we propose an Information-Theoretic\nShortcut Avoidance~(ITSA) approach to automatically restrict shortcut-related\ninformation from being encoded into the feature representations. As a result,\nour proposed method learns robust and shortcut-invariant features by minimizing\nthe sensitivity of latent features to input variations. To avoid the\nprohibitive computational cost of direct input sensitivity optimization, we\npropose an effective yet feasible algorithm to achieve robustness. We show that\nusing this method, state-of-the-art stereo matching networks that are trained\npurely on synthetic data can effectively generalize to challenging and\npreviously unseen real data scenarios. Importantly, the proposed method\nenhances the robustness of the synthetic trained networks to the point that\nthey outperform their fine-tuned counterparts (on real data) for challenging\nout-of-domain stereo datasets.",
          "link": "http://arxiv.org/abs/2201.02263",
          "publishedOn": "2022-01-11T00:39:35.328Z",
          "wordCount": 633,
          "title": "ITSA: An Information-Theoretic Approach to Automatic Shortcut Avoidance and Domain Generalization in Stereo Matching Networks. (arXiv:2201.02263v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02283",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhao_W/0/1/0/all/0/1\">Weijie Zhao</a>",
          "description": "We develop the \"generalized consistent weighted sampling\" (GCWS) for hashing\nthe \"powered-GMM\" (pGMM) kernel (with a tuning parameter $p$). It turns out\nthat GCWS provides a numerically stable scheme for applying power\ntransformation on the original data, regardless of the magnitude of $p$ and the\ndata. The power transformation is often effective for boosting the performance,\nin many cases considerably so. We feed the hashed data to neural networks on a\nvariety of public classification datasets and name our method ``GCWSNet''. Our\nextensive experiments show that GCWSNet often improves the classification\naccuracy. Furthermore, it is evident from the experiments that GCWSNet\nconverges substantially faster. In fact, GCWS often reaches a reasonable\naccuracy with merely (less than) one epoch of the training process. This\nproperty is much desired because many applications, such as advertisement\nclick-through rate (CTR) prediction models, or data streams (i.e., data seen\nonly once), often train just one epoch. Another beneficial side effect is that\nthe computations of the first layer of the neural networks become additions\ninstead of multiplications because the input data become binary (and highly\nsparse).\n\nEmpirical comparisons with (normalized) random Fourier features (NRFF) are\nprovided. We also propose to reduce the model size of GCWSNet by count-sketch\nand develop the theory for analyzing the impact of using count-sketch on the\naccuracy of GCWS. Our analysis shows that an ``8-bit'' strategy should work\nwell in that we can always apply an 8-bit count-sketch hashing on the output of\nGCWS hashing without hurting the accuracy much. There are many other ways to\ntake advantage of GCWS when training deep neural networks. For example, one can\napply GCWS on the outputs of the last layer to boost the accuracy of trained\ndeep neural networks.",
          "link": "http://arxiv.org/abs/2201.02283",
          "publishedOn": "2022-01-11T00:39:35.320Z",
          "wordCount": 714,
          "title": "GCWSNet: Generalized Consistent Weighted Sampling for Scalable and Accurate Training of Neural Networks. (arXiv:2201.02283v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02487",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pia_A/0/1/0/all/0/1\">Alberto Del Pia</a>",
          "description": "Sparse PCA is the optimization problem obtained from PCA by adding a sparsity\nconstraint on the principal components. Sparse PCA is NP-hard and hard to\napproximate even in the single-component case. In this paper we settle the\ncomputational complexity of sparse PCA with respect to the rank of the\ncovariance matrix. We show that, if the rank of the covariance matrix is a\nfixed value, then there is an algorithm that solves sparse PCA to global\noptimality, whose running time is polynomial in the number of features. We also\nprove a similar result for the version of sparse PCA which requires the\nprincipal components to have disjoint supports.",
          "link": "http://arxiv.org/abs/2201.02487",
          "publishedOn": "2022-01-11T00:39:35.314Z",
          "wordCount": 532,
          "title": "Sparse PCA on fixed-rank matrices. (arXiv:2201.02487v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.11724",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Cheng_L/0/1/0/all/0/1\">Lu Cheng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Guo_R/0/1/0/all/0/1\">Ruocheng Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>",
          "description": "An important problem in causal inference is to break down the total effect of\na treatment on an outcome into different causal pathways and to quantify the\ncausal effect in each pathway. For instance, in causal fairness, the total\neffect of being a male employee (i.e., treatment) constitutes its direct effect\non annual income (i.e., outcome) and the indirect effect via the employee's\noccupation (i.e., mediator). Causal mediation analysis (CMA) is a formal\nstatistical framework commonly used to reveal such underlying causal\nmechanisms. One major challenge of CMA in observational studies is handling\nconfounders, variables that cause spurious causal relationships among\ntreatment, mediator, and outcome. Conventional methods assume sequential\nignorability that implies all confounders can be measured, which is often\nunverifiable in practice. This work aims to circumvent the stringent sequential\nignorability assumptions and consider hidden confounders. Drawing upon proxy\nstrategies and recent advances in deep learning, we propose to simultaneously\nuncover the latent variables that characterize hidden confounders and estimate\nthe causal effects. Empirical evaluations using both synthetic and\nsemi-synthetic datasets validate the effectiveness of the proposed method. We\nfurther show the potentials of our approach for causal fairness analysis.",
          "link": "http://arxiv.org/abs/2102.11724",
          "publishedOn": "2022-01-11T00:39:35.292Z",
          "wordCount": 648,
          "title": "Causal Mediation Analysis with Hidden Confounders. (arXiv:2102.11724v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03531",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shaker_A/0/1/0/all/0/1\">Ammar Shaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shujian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onoro_Rubio_D/0/1/0/all/0/1\">Daniel O&#xf1;oro-Rubio</a>",
          "description": "The similarity of feature representations plays a pivotal role in the success\nof problems related to domain adaptation. Feature similarity includes both the\ninvariance of marginal distributions and the closeness of conditional\ndistributions given the desired response $y$ (e.g., class labels).\nUnfortunately, traditional methods always learn such features without fully\ntaking into consideration the information in $y$, which in turn may lead to a\nmismatch of the conditional distributions or the mix-up of discriminative\nstructures underlying data distributions. In this work, we introduce the\nrecently proposed von Neumann conditional divergence to improve the\ntransferability across multiple domains. We show that this new divergence is\ndifferentiable and eligible to easily quantify the functional dependence\nbetween features and $y$. Given multiple source tasks, we integrate this\ndivergence to capture discriminative information in $y$ and design novel\nlearning objectives assuming those source tasks are observed either\nsimultaneously or sequentially. In both scenarios, we obtain favorable\nperformance against state-of-the-art methods in terms of smaller generalization\nerror on new tasks and less catastrophic forgetting on source tasks (in the\nsequential setup).",
          "link": "http://arxiv.org/abs/2108.03531",
          "publishedOn": "2022-01-11T00:39:35.286Z",
          "wordCount": 627,
          "title": "Learning to Transfer with von Neumann Conditional Divergence. (arXiv:2108.03531v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Orang_O/0/1/0/all/0/1\">Omid Orang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_P/0/1/0/all/0/1\">Petr&#xf4;nio C&#xe2;ndido de Lima e Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadelha_F/0/1/0/all/0/1\">Frederico Guimar&#xe3;es Gadelha</a>",
          "description": "Among various soft computing approaches for time series forecasting, Fuzzy\nCognitive Maps (FCM) have shown remarkable results as a tool to model and\nanalyze the dynamics of complex systems. FCM have similarities to recurrent\nneural networks and can be classified as a neuro-fuzzy method. In other words,\nFCMs are a mixture of fuzzy logic, neural network, and expert system aspects,\nwhich act as a powerful tool for simulating and studying the dynamic behavior\nof complex systems. The most interesting features are knowledge\ninterpretability, dynamic characteristics and learning capability. The goal of\nthis survey paper is mainly to present an overview on the most relevant and\nrecent FCM-based time series forecasting models proposed in the literature. In\naddition, this article considers an introduction on the fundamentals of FCM\nmodel and learning methodologies. Also, this survey provides some ideas for\nfuture research to enhance the capabilities of FCM in order to cover some\nchallenges in the real-world experiments such as handling non-stationary data\nand scalability issues. Moreover, equipping FCMs with fast learning algorithms\nis one of the major concerns in this area.",
          "link": "http://arxiv.org/abs/2201.02297",
          "publishedOn": "2022-01-11T00:39:35.280Z",
          "wordCount": null,
          "title": "Time Series Forecasting Using Fuzzy Cognitive Maps: A Survey. (arXiv:2201.02297v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2112.03477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghavami_B/0/1/0/all/0/1\">Behnam Ghavami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadati_M/0/1/0/all/0/1\">Mani Sadati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahidzadeh_M/0/1/0/all/0/1\">Mohammad Shahidzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhenman Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shannon_L/0/1/0/all/0/1\">Lesley Shannon</a>",
          "description": "Adversarial bit-flip attack (BFA) on Neural Network weights can result in\ncatastrophic accuracy degradation by flipping a very small number of bits. A\nmajor drawback of prior bit flip attack techniques is their reliance on test\ndata. This is frequently not possible for applications that contain sensitive\nor proprietary data. In this paper, we propose Blind Data Adversarial Bit-flip\nAttack (BDFA), a novel technique to enable BFA without any access to the\ntraining or testing data. This is achieved by optimizing for a synthetic\ndataset, which is engineered to match the statistics of batch normalization\nacross different layers of the network and the targeted label. Experimental\nresults show that BDFA could decrease the accuracy of ResNet50 significantly\nfrom 75.96\\% to 13.94\\% with only 4 bits flips.",
          "link": "http://arxiv.org/abs/2112.03477",
          "publishedOn": "2022-01-11T00:39:35.279Z",
          "wordCount": 583,
          "title": "BDFA: A Blind Data Adversarial Bit-flip Attack on Deep Neural Networks. (arXiv:2112.03477v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Ching-Chun Chang</a>",
          "description": "Recent advances in deep learning have led to a paradigm shift in reversible\nsteganography. A fundamental pillar of reversible steganography is predictive\nmodelling which can be realised via deep neural networks. However, non-trivial\nerrors exist in inferences about some out-of-distribution and noisy data. In\nview of this issue, we propose to consider uncertainty in predictive models\nbased upon a theoretical framework of Bayesian deep learning. Bayesian neural\nnetworks can be regarded as self-aware machinery; that is, a machine that knows\nits own limitations. To quantify uncertainty, we approximate the posterior\npredictive distribution through Monte Carlo sampling with stochastic forward\npasses. We further show that predictive uncertainty can be disentangled into\naleatoric and epistemic uncertainties and these quantities can be learnt in an\nunsupervised manner. Experimental results demonstrate an improvement delivered\nby Bayesian uncertainty analysis upon steganographic capacity-distortion\nperformance.",
          "link": "http://arxiv.org/abs/2201.02478",
          "publishedOn": "2022-01-11T00:39:35.269Z",
          "wordCount": 554,
          "title": "Bayesian Neural Networks for Reversible Steganography. (arXiv:2201.02478v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bennett_M/0/1/0/all/0/1\">Michele Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_K/0/1/0/all/0/1\">Karin Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleczyk_E/0/1/0/all/0/1\">Ewa J. Kleczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1\">Rajesh Mehta</a>",
          "description": "Data scientists and statisticians are often at odds when determining the best\napproach, machine learning or statistical modeling, to solve an analytics\nchallenge. However, machine learning and statistical modeling are more cousins\nthan adversaries on different sides of an analysis battleground. Choosing\nbetween the two approaches or in some cases using both is based on the problem\nto be solved and outcomes required as well as the data available for use and\ncircumstances of the analysis. Machine learning and statistical modeling are\ncomplementary, based on similar mathematical principles, but simply using\ndifferent tools in an overall analytics knowledge base. Determining the\npredominant approach should be based on the problem to be solved as well as\nempirical evidence, such as size and completeness of the data, number of\nvariables, assumptions or lack thereof, and expected outcomes such as\npredictions or causality. Good analysts and data scientists should be well\nversed in both techniques and their proper application, thereby using the right\ntool for the right project to achieve the desired results.",
          "link": "http://arxiv.org/abs/2201.02469",
          "publishedOn": "2022-01-11T00:39:35.261Z",
          "wordCount": 608,
          "title": "Similarities and Differences between Machine Learning and Traditional Advanced Statistical Modeling in Healthcare Analytics. (arXiv:2201.02469v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cramer_E/0/1/0/all/0/1\">Eike Cramer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitsos_A/0/1/0/all/0/1\">Alexander Mitsos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tempone_R/0/1/0/all/0/1\">Raul Tempone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahmen_M/0/1/0/all/0/1\">Manuel Dahmen</a>",
          "description": "Neural networks-based learning of the distribution of non-dispatchable\nrenewable electricity generation from sources such as photovoltaics (PV) and\nwind as well as load demands has recently gained attention. Normalizing flow\ndensity models are particularly well suited for this task due to the training\nthrough direct log-likelihood maximization. However, research from the field of\nimage generation has shown that standard normalizing flows can only learn\nsmeared-out versions of manifold distributions. Previous works on normalizing\nflow-based scenario generation do not address this issue, and the smeared-out\ndistributions result in the sampling of noisy time series. In this paper, we\nexploit the isometry of the principal component analysis (PCA), which sets up\nthe normalizing flow in a lower-dimensional space while maintaining the direct\nand computationally efficient likelihood maximization. We train the resulting\nprincipal component flow (PCF) on data of PV and wind power generation as well\nas load demand in Germany in the years 2013 to 2015. The results of this\ninvestigation show that the PCF preserves critical features of the original\ndistributions, such as the probability density and frequency behavior of the\ntime series. The application of the PCF is, however, not limited to renewable\npower generation but rather extends to any data set, time series, or otherwise,\nwhich can be efficiently reduced using PCA.",
          "link": "http://arxiv.org/abs/2104.10410",
          "publishedOn": "2022-01-11T00:39:35.240Z",
          "wordCount": 673,
          "title": "Principal Component Density Estimation for Scenario Generation Using Normalizing Flows. (arXiv:2104.10410v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08191",
          "author": "The <a href=\"http://arxiv.org/find/stat/1/au:+Mai_T/0/1/0/all/0/1\">Tien Mai</a>",
          "description": "We study the problem of matrix completion in this paper. A spectral scaled\nStudent prior is exploited to favour the underlying low-rank structure of the\ndata matrix. We provide a thorough theoretical investigation for our approach\nthrough PAC-Bayesian bounds. More precisely, our PAC-Bayesian approach enjoys a\nminimax-optimal oracle inequality which guarantees that our method works well\nunder model misspecification and under general sampling distribution.\nInterestingly, we also provide efficient gradient-based sampling\nimplementations for our approach by using Langevin Monte Carlo. More\nspecifically, we show that our algorithms are significantly faster than Gibbs\nsampler in this problem. To illustrate the attractive features of our inference\nstrategy, some numerical simulations are conducted and an application to image\ninpainting is demonstrated.",
          "link": "http://arxiv.org/abs/2104.08191",
          "publishedOn": "2022-01-11T00:39:35.233Z",
          "wordCount": 554,
          "title": "PAC-Bayesian Matrix Completion with a Spectral Scaled Student Prior. (arXiv:2104.08191v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.13867",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Humenberger_M/0/1/0/all/0/1\">Martin Humenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabon_Y/0/1/0/all/0/1\">Yohann Cabon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_N/0/1/0/all/0/1\">Nicolas Guerin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morat_J/0/1/0/all/0/1\">Julien Morat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leroy_V/0/1/0/all/0/1\">Vincent Leroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revaud_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Revaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rerole_P/0/1/0/all/0/1\">Philippe Rerole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pion_N/0/1/0/all/0/1\">No&#xe9; Pion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Souza_C/0/1/0/all/0/1\">Cesar de Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Csurka_G/0/1/0/all/0/1\">Gabriela Csurka</a>",
          "description": "Visual localization tackles the challenge of estimating the camera pose from\nimages by using correspondence analysis between query images and a map. This\ntask is computation and data intensive which poses challenges on thorough\nevaluation of methods on various datasets. However, in order to further advance\nin the field, we claim that robust visual localization algorithms should be\nevaluated on multiple datasets covering a broad domain variety. To facilitate\nthis, we introduce kapture, a new, flexible, unified data format and toolbox\nfor visual localization and structure-from-motion (SFM). It enables easy usage\nof different datasets as well as efficient and reusable data processing. To\ndemonstrate this, we present a versatile pipeline for visual localization that\nfacilitates the use of different local and global features, 3D data (e.g. depth\nmaps), non-vision sensor data (e.g. IMU, GPS, WiFi), and various processing\nalgorithms. Using multiple configurations of the pipeline, we show the great\nversatility of kapture in our experiments. Furthermore, we evaluate our methods\non eight public datasets where they rank top on all and first on many of them.\nTo foster future research, we release code, models, and all datasets used in\nthis paper in the kapture format open source under a permissive BSD license.\ngithub.com/naver/kapture, github.com/naver/kapture-localization",
          "link": "http://arxiv.org/abs/2007.13867",
          "publishedOn": "2022-01-11T00:39:35.227Z",
          "wordCount": 688,
          "title": "Robust Image Retrieval-based Visual Localization using Kapture. (arXiv:2007.13867v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03814",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Autthasan_P/0/1/0/all/0/1\">Phairot Autthasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chaisaen_R/0/1/0/all/0/1\">Rattanaphon Chaisaen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sudhawiyangkul_T/0/1/0/all/0/1\">Thapanun Sudhawiyangkul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rangpong_P/0/1/0/all/0/1\">Phurin Rangpong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiatthaveephong_S/0/1/0/all/0/1\">Suktipol Kiatthaveephong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dilokthanakul_N/0/1/0/all/0/1\">Nat Dilokthanakul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhakdisongkhram_G/0/1/0/all/0/1\">Gun Bhakdisongkhram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phan_H/0/1/0/all/0/1\">Huy Phan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wilaiprasitporn_T/0/1/0/all/0/1\">Theerawit Wilaiprasitporn</a>",
          "description": "Advances in the motor imagery (MI)-based brain-computer interfaces (BCIs)\nallow control of several applications by decoding neurophysiological phenomena,\nwhich are usually recorded by electroencephalography (EEG) using a non-invasive\ntechnique. Despite great advances in MI-based BCI, EEG rhythms are specific to\na subject and various changes over time. These issues point to significant\nchallenges to enhance the classification performance, especially in a\nsubject-independent manner. To overcome these challenges, we propose MIN2Net, a\nnovel end-to-end multi-task learning to tackle this task. We integrate deep\nmetric learning into a multi-task autoencoder to learn a compact and\ndiscriminative latent representation from EEG and perform classification\nsimultaneously. This approach reduces the complexity in pre-processing, results\nin significant performance improvement on EEG classification. Experimental\nresults in a subject-independent manner show that MIN2Net outperforms the\nstate-of-the-art techniques, achieving an F1-score improvement of 6.72%, and\n2.23% on the SMR-BCI, and OpenBMI datasets, respectively. We demonstrate that\nMIN2Net improves discriminative information in the latent representation. This\nstudy indicates the possibility and practicality of using this model to develop\nMI-based BCI applications for new users without the need for calibration.",
          "link": "http://arxiv.org/abs/2102.03814",
          "publishedOn": "2022-01-11T00:39:35.217Z",
          "wordCount": 676,
          "title": "MIN2Net: End-to-End Multi-Task Learning for Subject-Independent Motor Imagery EEG Classification. (arXiv:2102.03814v4 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuezhou Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1\">Rong Pan</a>",
          "description": "Federated learning trains models across devices with distributed data, while\nprotecting the privacy and obtaining a model similar to that of centralized ML.\nA large number of workers with data and computing power are the foundation of\nfederal learning. However, the inevitable costs prevent self-interested workers\nfrom serving for free. Moreover, due to data isolation, task publishers lack\neffective methods to select, evaluate and pay reliable workers with\nhigh-quality data. Therefore, we design an auction-based incentive mechanism\nfor horizontal federated learning with reputation and contribution measurement.\nBy designing a reasonable method of measuring contribution, we establish the\nreputation of workers, which is easy to decline and difficult to improve.\nThrough reverse auctions, workers bid for tasks, and the task publisher selects\nworkers combining reputation and bid price. With the budget constraint, winning\nworkers are paid based on performance. We proved that our mechanism satisfies\nthe individual rationality of the honest worker, budget feasibility,\ntruthfulness, and computational efficiency.",
          "link": "http://arxiv.org/abs/2201.02410",
          "publishedOn": "2022-01-11T00:39:35.210Z",
          "wordCount": 592,
          "title": "Auction-Based Ex-Post-Payment Incentive Mechanism Design for Horizontal Federated Learning with Reputation and Contribution Measurement. (arXiv:2201.02410v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2110.07313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sangeeta Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tjandra_A/0/1/0/all/0/1\">Andros Tjandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Anurag Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kritika Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saraf_Y/0/1/0/all/0/1\">Yatharth Saraf</a>",
          "description": "Representation learning from unlabeled data has been of major interest in\nartificial intelligence research. While self-supervised speech representation\nlearning has been popular in the speech research community, very few works have\ncomprehensively analyzed audio representation learning for non-speech audio\ntasks. In this paper, we propose a self-supervised audio representation\nlearning method and apply it to a variety of downstream non-speech audio tasks.\nWe combine the well-known wav2vec 2.0 framework, which has shown success in\nself-supervised learning for speech tasks, with parameter-efficient conformer\narchitectures. Our self-supervised pre-training can reduce the need for labeled\ndata by two-thirds. On the AudioSet benchmark, we achieve a mean average\nprecision (mAP) score of 0.415, which is a new state-of-the-art on this dataset\nthrough audio-only self-supervised learning. Our fine-tuned conformers also\nsurpass or match the performance of previous systems pre-trained in a\nsupervised way on several downstream tasks. We further discuss the important\ndesign considerations for both pre-training and fine-tuning.",
          "link": "http://arxiv.org/abs/2110.07313",
          "publishedOn": "2022-01-11T00:39:35.189Z",
          "wordCount": 631,
          "title": "Conformer-Based Self-Supervised Learning for Non-Speech Audio Tasks. (arXiv:2110.07313v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.08438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weichao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenchao Li</a>",
          "description": "Reward design is a fundamental problem in reinforcement learning (RL). A\nmisspecified or poorly designed reward can result in low sample efficiency and\nundesired behaviors. In this paper, we propose the idea of programmatic reward\ndesign, i.e. using programs to specify the reward functions in RL environments.\nPrograms allow human engineers to express sub-goals and complex task scenarios\nin a structured and interpretable way. The challenge of programmatic reward\ndesign, however, is that while humans can provide the high-level structures,\nproperly setting the low-level details, such as the right amount of reward for\na specific sub-task, remains difficult. A major contribution of this paper is a\nprobabilistic framework that can infer the best candidate programmatic reward\nfunction from expert demonstrations. Inspired by recent generative-adversarial\napproaches, our framework searches for the most likely programmatic reward\nfunction under which the optimally generated trajectories cannot be\ndifferentiated from the demonstrated trajectories. Experimental results show\nthat programmatic reward functionslearned using this framework can\nsignificantly outperform those learned using existing reward learning\nalgo-rithms, and enable RL agents to achieve state-of-the-artperformance on\nhighly complex tasks.",
          "link": "http://arxiv.org/abs/2112.08438",
          "publishedOn": "2022-01-11T00:39:35.160Z",
          "wordCount": 620,
          "title": "Programmatic Reward Design by Example. (arXiv:2112.08438v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kunjir_M/0/1/0/all/0/1\">Mayuresh Kunjir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chawla_S/0/1/0/all/0/1\">Sanjay Chawla</a>",
          "description": "Traffic signal control is an important problem in urban mobility with a\nsignificant potential of economic and environmental impact. While there is a\ngrowing interest in Reinforcement Learning (RL) for traffic control, the work\nso far has focussed on learning through interactions which, in practice, is\ncostly. Instead, real experience data on traffic is available and could be\nexploited at minimal costs. Recent progress in offline or batch RL has enabled\njust that. Model-based offline RL methods, in particular, have been shown to\ngeneralize to the experience data much better than others. We build a\nmodel-based learning framework, A-DAC, which infers a Markov Decision Process\n(MDP) from dataset with pessimistic costs built in to deal with data\nuncertainties. The costs are modeled through an adaptive shaping of rewards in\nthe MDP which provides better regularization of data compared to the prior\nrelated work. A-DAC is evaluated on a complex signalized roundabout using\nmultiple datasets varying in size and in batch collection policy. The\nevaluation results show that it is possible to build high performance control\npolicies in a data efficient manner using simplistic batch collection policies.",
          "link": "http://arxiv.org/abs/2201.02381",
          "publishedOn": "2022-01-11T00:39:35.153Z",
          "wordCount": 606,
          "title": "Offline Reinforcement Learning for Road Traffic Control. (arXiv:2201.02381v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02596",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Liu_Y/0/1/0/all/0/1\">Yumin Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Duffy_K/0/1/0/all/0/1\">Kate Duffy</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dy_J/0/1/0/all/0/1\">Jennifer G. Dy</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ganguly_A/0/1/0/all/0/1\">Auroop R. Ganguly</a>",
          "description": "The El Nino Southern Oscillation (ENSO) is a semi-periodic fluctuation in sea\nsurface temperature (SST) over the tropical central and eastern Pacific Ocean\nthat influences interannual variability in regional hydrology across the world\nthrough long-range dependence or teleconnections. Recent research has\ndemonstrated the value of Deep Learning (DL) methods for improving ENSO\nprediction as well as Complex Networks (CN) for understanding teleconnections.\nHowever, gaps in predictive understanding of ENSO-driven river flows include\nthe black box nature of DL, the use of simple ENSO indices to describe a\ncomplex phenomenon and translating DL-based ENSO predictions to river flow\npredictions. Here we show that eXplainable DL (XDL) methods, based on saliency\nmaps, can extract interpretable predictive information contained in global SST\nand discover novel SST information regions and dependence structures relevant\nfor river flows which, in tandem with climate network constructions, enable\nimproved predictive understanding. Our results reveal additional information\ncontent in global SST beyond ENSO indices, develop new understanding of how\nSSTs influence river flows, and generate improved river flow predictions with\nuncertainties. Observations, reanalysis data, and earth system model\nsimulations are used to demonstrate the value of the XDL-CN based methods for\nfuture interannual and decadal scale climate projections.",
          "link": "http://arxiv.org/abs/2201.02596",
          "publishedOn": "2022-01-11T00:39:35.128Z",
          "wordCount": 631,
          "title": "Explainable deep learning for insights in El Nino and river flows. (arXiv:2201.02596v1 [physics.ao-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Browne_P/0/1/0/all/0/1\">Pierre Browne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lima_A/0/1/0/all/0/1\">Aranildo Lima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arcucci_R/0/1/0/all/0/1\">Rossella Arcucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quilodran_Casas_C/0/1/0/all/0/1\">C&#xe9;sar Quilodr&#xe1;n-Casas</a>",
          "description": "Starting from the Kaya identity, we used a Neural ODE model to predict the\nevolution of several indicators related to carbon emissions, on a\ncountry-level: population, GDP per capita, energy intensity of GDP, carbon\nintensity of energy. We compared the model with a baseline statistical model -\nVAR - and obtained good performances. We conclude that this machine-learning\napproach can be used to produce a wide range of results and give relevant\ninsight to policymakers",
          "link": "http://arxiv.org/abs/2201.02433",
          "publishedOn": "2022-01-11T00:39:35.121Z",
          "wordCount": 511,
          "title": "Forecasting emissions through Kaya identity using Neural Ordinary Differential Equations. (arXiv:2201.02433v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merchie_F/0/1/0/all/0/1\">Florian Merchie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_D/0/1/0/all/0/1\">Damien Ernst</a>",
          "description": "In business retention, churn prevention has always been a major concern. This\nwork contributes to this domain by formalizing the problem of churn prediction\nin the context of online gambling as a binary classification task. We also\npropose an algorithmic answer to this problem based on recurrent neural\nnetwork. This algorithm is tested with online gambling data that have the form\nof time series, which can be efficiently processed by recurrent neural\nnetworks. To evaluate the performances of the trained models, standard machine\nlearning metrics were used, such as accuracy, precision and recall. For this\nproblem in particular, the conducted experiments allowed to assess that the\nchoice of a specific architecture depends on the metric which is given the\ngreatest importance. Architectures using nBRC favour precision, those using\nLSTM give better recall, while GRU-based architectures allow a higher accuracy\nand balance two other metrics. Moreover, further experiments showed that using\nonly the more recent time-series histories to train the networks decreases the\nquality of the results. We also study the performances of models learned at a\nspecific instant $t$, at other times $t^{\\prime} > t$. The results show that\nthe performances of the models learned at time $t$ remain good at the following\ninstants $t^{\\prime} > t$, suggesting that there is no need to refresh the\nmodels at a high rate. However, the performances of the models were subject to\nnoticeable variance due to one-off events impacting the data.",
          "link": "http://arxiv.org/abs/2201.02463",
          "publishedOn": "2022-01-11T00:39:35.099Z",
          "wordCount": 659,
          "title": "Churn prediction in online gambling. (arXiv:2201.02463v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1808.09222",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Ayeni_B/0/1/0/all/0/1\">Babatunde M. Ayeni</a>",
          "description": "A set of introductory notes on the subject of data classification using a\nlinear classifier and least-squares cost function, and the negative effect of\nthe presence of outliers on the decision boundary of the linear discriminant.\nWe also show how a simple scaling could make the outlier less significant,\nthereby obtaining a much better decision boundary. We present some numerical\nresults.",
          "link": "http://arxiv.org/abs/1808.09222",
          "publishedOn": "2022-01-11T00:39:35.044Z",
          "wordCount": 511,
          "title": "Linear classifier, least-squares cost function, and outliers. (arXiv:1808.09222v2 [physics.data-an] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaur_R/0/1/0/all/0/1\">Ramneet Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Susmit Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Anirban Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sangdon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobriban_E/0/1/0/all/0/1\">Edgar Dobriban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolsky_O/0/1/0/all/0/1\">Oleg Sokolsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Insup Lee</a>",
          "description": "Machine learning methods such as deep neural networks (DNNs), despite their\nsuccess across different domains, are known to often generate incorrect\npredictions with high confidence on inputs outside their training distribution.\nThe deployment of DNNs in safety-critical domains requires detection of\nout-of-distribution (OOD) data so that DNNs can abstain from making predictions\non those. A number of methods have been recently developed for OOD detection,\nbut there is still room for improvement. We propose the new method iDECODe,\nleveraging in-distribution equivariance for conformal OOD detection. It relies\non a novel base non-conformity measure and a new aggregation method, used in\nthe inductive conformal anomaly detection framework, thereby guaranteeing a\nbounded false detection rate. We demonstrate the efficacy of iDECODe by\nexperiments on image and audio datasets, obtaining state-of-the-art results. We\nalso show that iDECODe can detect adversarial examples.",
          "link": "http://arxiv.org/abs/2201.02331",
          "publishedOn": "2022-01-11T00:39:35.016Z",
          "wordCount": 568,
          "title": "iDECODe: In-distribution Equivariance for Conformal Out-of-distribution Detection. (arXiv:2201.02331v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.00888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharp_N/0/1/0/all/0/1\">Nicholas Sharp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attaiki_S/0/1/0/all/0/1\">Souhaib Attaiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crane_K/0/1/0/all/0/1\">Keenan Crane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1\">Maks Ovsjanikov</a>",
          "description": "We introduce a new general-purpose approach to deep learning on 3D surfaces,\nbased on the insight that a simple diffusion layer is highly effective for\nspatial communication. The resulting networks are automatically robust to\nchanges in resolution and sampling of a surface -- a basic property which is\ncrucial for practical applications. Our networks can be discretized on various\ngeometric representations such as triangle meshes or point clouds, and can even\nbe trained on one representation then applied to another. We optimize the\nspatial support of diffusion as a continuous network parameter ranging from\npurely local to totally global, removing the burden of manually choosing\nneighborhood sizes. The only other ingredients in the method are a multi-layer\nperceptron applied independently at each point, and spatial gradient features\nto support directional filters. The resulting networks are simple, robust, and\nefficient. Here, we focus primarily on triangle mesh surfaces, and demonstrate\nstate-of-the-art results for a variety of tasks including surface\nclassification, segmentation, and non-rigid correspondence.",
          "link": "http://arxiv.org/abs/2012.00888",
          "publishedOn": "2022-01-11T00:39:34.422Z",
          "wordCount": 638,
          "title": "DiffusionNet: Discretization Agnostic Learning on Surfaces. (arXiv:2012.00888v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.02291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Linping Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shenghui Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsui_C/0/1/0/all/0/1\">Chi-Ying Tsui</a>",
          "description": "Federated learning (FL) is an emerging privacy-preserving distributed\nlearning scheme. Due to the large model size and frequent model aggregation, FL\nsuffers from critical communication bottleneck. Many techniques have been\nproposed to reduce the communication volume, including model compression and\nquantization. Existing adaptive quantization schemes use ascending-trend\nquantization where the quantizaion level increases with the training stages. In\nthis paper, we formulate the problem as optimizing the training convergence\nrate for a given communication volume. The result shows that the optimal\nquantizaiton level can be represented by two factors, i.e., the training loss\nand the range of model updates, and it is preferable to decrease the\nquantization level rather than increase. Then, we propose two descending\nquantization schemes based on the training loss and model range. Experimental\nresults show that proposed schemes not only reduce the communication volume but\nalso help FL converge faster, when compared with current ascending\nquantization.",
          "link": "http://arxiv.org/abs/2110.02291",
          "publishedOn": "2022-01-11T00:39:34.416Z",
          "wordCount": 599,
          "title": "FedDQ: Communication-Efficient Federated Learning with Descending Quantization. (arXiv:2110.02291v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_C/0/1/0/all/0/1\">Christian Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wengeler_S/0/1/0/all/0/1\">Simon Wengeler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loquercio_A/0/1/0/all/0/1\">Antonio Loquercio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>",
          "description": "Humans race drones faster than neural networks trained for end-to-end\nautonomous flight. This may be related to the ability of human pilots to select\ntask-relevant visual information effectively. This work investigates whether\nneural networks capable of imitating human eye gaze behavior and attention can\nimprove neural network performance for the challenging task of vision-based\nautonomous drone racing. We hypothesize that gaze-based attention prediction\ncan be an efficient mechanism for visual information selection and decision\nmaking in a simulator-based drone racing task. We test this hypothesis using\neye gaze and flight trajectory data from 18 human drone pilots to train a\nvisual attention prediction model. We then use this visual attention prediction\nmodel to train an end-to-end controller for vision-based autonomous drone\nracing using imitation learning. We compare the drone racing performance of the\nattention-prediction controller to those using raw image inputs and image-based\nabstractions (i.e., feature tracks). Our results show that attention-prediction\nbased controllers outperform the baselines and are able to complete a\nchallenging race track consistently with up to 88% success rate. Furthermore,\nvisual attention-prediction and feature-track based models showed better\ngeneralization performance than image-based models when evaluated on hold-out\nreference trajectories. Our results demonstrate that human visual attention\nprediction improves the performance of autonomous vision-based drone racing\nagents and provides an essential step towards vision-based, fast, and agile\nautonomous flight that eventually can reach and even exceed human performances.",
          "link": "http://arxiv.org/abs/2201.02569",
          "publishedOn": "2022-01-11T00:39:34.396Z",
          "wordCount": 663,
          "title": "Visual Attention Prediction Improves Performance of Autonomous Drone Racing Agents. (arXiv:2201.02569v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2008.08727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elangovan_A/0/1/0/all/0/1\">Aparna Elangovan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_M/0/1/0/all/0/1\">Melissa Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verspoor_K/0/1/0/all/0/1\">Karin Verspoor</a>",
          "description": "Motivation: Protein-protein interactions (PPI) are critical to the function\nof proteins in both normal and diseased cells, and many critical protein\nfunctions are mediated by interactions.Knowledge of the nature of these\ninteractions is important for the construction of networks to analyse\nbiological data. However, only a small percentage of PPIs captured in protein\ninteraction databases have annotations of function available, e.g. only 4% of\nPPI are functionally annotated in the IntAct database. Here, we aim to label\nthe function type of PPIs by extracting relationships described in PubMed\nabstracts.\n\nMethod: We create a weakly supervised dataset from the IntAct PPI database\ncontaining interacting protein pairs with annotated function and associated\nabstracts from the PubMed database. We apply a state-of-the-art deep learning\ntechnique for biomedical natural language processing tasks, BioBERT, to build a\nmodel - dubbed PPI-BioBERT - for identifying the function of PPIs. In order to\nextract high quality PPI functions at large scale, we use an ensemble of\nPPI-BioBERT models to improve uncertainty estimation and apply an interaction\ntype-specific threshold to counteract the effects of variations in the number\nof training samples per interaction type.\n\nResults: We scan 18 million PubMed abstracts to automatically identify 3253\nnew typed PPIs, including phosphorylation and acetylation interactions, with an\noverall precision of 46% (87% for acetylation) based on a human-reviewed\nsample. This work demonstrates that analysis of biomedical abstracts for PPI\nfunction extraction is a feasible approach to substantially increasing the\nnumber of interactions annotated with function captured in online databases.",
          "link": "http://arxiv.org/abs/2008.08727",
          "publishedOn": "2022-01-11T00:39:34.388Z",
          "wordCount": 732,
          "title": "Assigning function to protein-protein interactions: a weakly supervised BioBERT based approach using PubMed abstracts. (arXiv:2008.08727v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ivanov_D/0/1/0/all/0/1\">Dmitry Ivanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiselev_M/0/1/0/all/0/1\">Mikhail Kiselev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larionov_D/0/1/0/all/0/1\">Denis Larionov</a>",
          "description": "This article proposes a sparse computation-based method for optimizing neural\nnetworks for reinforcement learning (RL) tasks. This method combines two ideas:\nneural network pruning and taking into account input data correlations; it\nmakes it possible to update neuron states only when changes in them exceed a\ncertain threshold. It significantly reduces the number of multiplications when\nrunning neural networks. We tested different RL tasks and achieved 20-150x\nreduction in the number of multiplications. There were no substantial\nperformance losses; sometimes the performance even improved.",
          "link": "http://arxiv.org/abs/2201.02571",
          "publishedOn": "2022-01-11T00:39:34.382Z",
          "wordCount": 508,
          "title": "Neural Network Optimization for Reinforcement Learning Tasks Using Sparse Computations. (arXiv:2201.02571v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Java_A/0/1/0/all/0/1\">Abhinav Java</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1\">Surya Kant Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_A/0/1/0/all/0/1\">Arshad Shaikh</a>",
          "description": "Session-based recommendation systems suggest relevant items to users by\nmodeling user behavior and preferences using short-term anonymous sessions.\nExisting methods leverage Graph Neural Networks (GNNs) that propagate and\naggregate information from neighboring nodes i.e., local message passing. Such\ngraph-based architectures have representational limits, as a single sub-graph\nis susceptible to overfit the sequential dependencies instead of accounting for\ncomplex transitions between items in different sessions. We propose a new\ntechnique that leverages a Transformer in combination with a target attentive\nGNN. This allows richer representations to be learnt, which translates to\nempirical performance gains in comparison to a vanilla target attentive GNN.\nOur experimental results and ablation show that our proposed method is\ncompetitive with the existing methods on real-world benchmark datasets,\nimproving on graph-based hypotheses. Code is available at\nhttps://github.com/The-Learning-Machines/SBR",
          "link": "http://arxiv.org/abs/2107.01516",
          "publishedOn": "2022-01-11T00:39:34.375Z",
          "wordCount": 597,
          "title": "Introducing Self-Attention to Target Attentive Graph Neural Networks. (arXiv:2107.01516v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hayes_J/0/1/0/all/0/1\">Jamie Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balle_B/0/1/0/all/0/1\">Borja Balle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">M. Pawan Kumar</a>",
          "description": "We study the difficulties in learning that arise from robust and\ndifferentially private optimization. We first study convergence of gradient\ndescent based adversarial training with differential privacy, taking a simple\nbinary classification task on linearly separable data as an illustrative\nexample. We compare the gap between adversarial and nominal risk in both\nprivate and non-private settings, showing that the data dimensionality\ndependent term introduced by private optimization compounds the difficulties of\nlearning a robust model. After this, we discuss what parts of adversarial\ntraining and differential privacy hurt optimization, identifying that the size\nof adversarial perturbation and clipping norm in differential privacy both\nincrease the curvature of the loss landscape, implying poorer generalization\nperformance.",
          "link": "http://arxiv.org/abs/2201.02265",
          "publishedOn": "2022-01-11T00:39:34.369Z",
          "wordCount": 536,
          "title": "Learning to be adversarially robust and differentially private. (arXiv:2201.02265v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2110.03007",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koromilas_P/0/1/0/all/0/1\">Panagiotis Koromilas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannakopoulos_T/0/1/0/all/0/1\">Theodoros Giannakopoulos</a>",
          "description": "Multimodal Language Analysis is a demanding area of research, since it is\nassociated with two requirements: combining different modalities and capturing\ntemporal information. During the last years, several works have been proposed\nin the area, mostly centered around supervised learning in downstream tasks. In\nthis paper we propose extracting unsupervised Multimodal Language\nrepresentations that are universal and can be applied to different tasks.\nTowards this end, we map the word-level aligned multimodal sequences to 2-D\nmatrices and then use Convolutional Autoencoders to learn embeddings by\ncombining multiple datasets. Extensive experimentation on Sentiment Analysis\n(MOSEI) and Emotion Recognition (IEMOCAP) indicate that the learned\nrepresentations can achieve near-state-of-the-art performance with just the use\nof a Logistic Regression algorithm for downstream classification. It is also\nshown that our method is extremely lightweight and can be easily generalized to\nother tasks and unseen data with small performance drop and almost the same\nnumber of parameters. The proposed multimodal representation models are\nopen-sourced and will help grow the applicability of Multimodal Language.",
          "link": "http://arxiv.org/abs/2110.03007",
          "publishedOn": "2022-01-11T00:39:34.347Z",
          "wordCount": 616,
          "title": "Unsupervised Multimodal Language Representations using Convolutional Autoencoders. (arXiv:2110.03007v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.04621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shor_J/0/1/0/all/0/1\">Joel Shor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jansen_A/0/1/0/all/0/1\">Aren Jansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Daniel Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>",
          "description": "Many speech applications require understanding aspects beyond the words being\nspoken, such as recognizing emotion, detecting whether the speaker is wearing a\nmask, or distinguishing real from synthetic speech. In this work, we introduce\na new state-of-the-art paralinguistic representation derived from large-scale,\nfully self-supervised training of a 600M+ parameter Conformer-based\narchitecture. We benchmark on a diverse set of speech tasks and demonstrate\nthat simple linear classifiers trained on top of our time-averaged\nrepresentation outperform nearly all previous results, in some cases by large\nmargins. Our analyses of context-window size demonstrate that, surprisingly, 2\nsecond context-windows achieve 96\\% the performance of the Conformers that use\nthe full long-term context on 7 out of 9 tasks. Furthermore, while the best\nper-task representations are extracted internally in the network, stable\nperformance across several layers allows a single universal representation to\nreach near optimal performance on all tasks.",
          "link": "http://arxiv.org/abs/2110.04621",
          "publishedOn": "2022-01-11T00:39:34.340Z",
          "wordCount": 600,
          "title": "Universal Paralinguistic Speech Representations Using Self-Supervised Conformers. (arXiv:2110.04621v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuba_J/0/1/0/all/0/1\">Jakub Grudzien Kuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witt_C/0/1/0/all/0/1\">Christian Schroeder de Witt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1\">Jakob Foerster</a>",
          "description": "General policy improvement (GPI) and trust-region learning (TRL) are the\npredominant frameworks within contemporary reinforcement learning (RL), which\nserve as the core models for solving Markov decision processes (MDPs).\nUnfortunately, in their mathematical form, they are sensitive to modifications,\nand thus, the practical instantiations that implement them do not automatically\ninherit their improvement guarantees. As a result, the spectrum of available\nrigorous MDP-solvers is narrow. Indeed, many state-of-the-art (SOTA)\nalgorithms, such as TRPO and PPO, are not proven to converge. In this paper, we\npropose \\textsl{mirror learning} -- a general solution to the RL problem. We\nreveal GPI and TRL to be but small points within this far greater space of\nalgorithms which boasts the monotonic improvement property and converges to the\noptimal policy. We show that virtually all SOTA algorithms for RL are instances\nof mirror learning, and thus suggest that their empirical performance is a\nconsequence of their theoretical properties, rather than of approximate\nanalogies. Excitingly, we show that mirror learning opens up a whole new space\nof policy learning methods with convergence guarantees.",
          "link": "http://arxiv.org/abs/2201.02373",
          "publishedOn": "2022-01-11T00:39:34.333Z",
          "wordCount": 598,
          "title": "Mirror Learning: A Unifying Framework of Policy Optimisation. (arXiv:2201.02373v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02325",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yoshizawa_G/0/1/0/all/0/1\">Ginga Yoshizawa</a>",
          "description": "In time series data analysis, detecting change points on a real-time basis\n(online) is of great interest in many areas, such as finance, environmental\nmonitoring, and medicine. One promising means to achieve this is the Bayesian\nonline change point detection (BOCPD) algorithm, which has been successfully\nadopted in particular cases in which the time series of interest has a fixed\nbaseline. However, we have found that the algorithm struggles when the baseline\nirreversibly shifts from its initial state. This is because with the original\nBOCPD algorithm, the sensitivity with which a change point can be detected is\ndegraded if the data points are fluctuating at locations relatively far from\nthe original baseline. In this paper, we not only extend the original BOCPD\nalgorithm to be applicable to a time series whose baseline is constantly\nshifting toward unknown values but also visualize why the proposed extension\nworks. To demonstrate the efficacy of the proposed algorithm compared to the\noriginal one, we examine these algorithms on two real-world data sets and six\nsynthetic data sets.",
          "link": "http://arxiv.org/abs/2201.02325",
          "publishedOn": "2022-01-11T00:39:34.326Z",
          "wordCount": 609,
          "title": "Bayesian Online Change Point Detection for Baseline Shifts. (arXiv:2201.02325v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sohn_J/0/1/0/all/0/1\">Jy-yong Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Liang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongxu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jaekyun Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1\">Dimitris Papailiopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kangwook Lee</a>",
          "description": "Mixup is a data augmentation method that generates new data points by mixing\na pair of input data. While mixup generally improves the prediction\nperformance, it sometimes degrades the performance. In this paper, we first\nidentify the main causes of this phenomenon by theoretically and empirically\nanalyzing the mixup algorithm. To resolve this, we propose GenLabel, a simple\nyet effective relabeling algorithm designed for mixup. In particular, GenLabel\nhelps the mixup algorithm correctly label mixup samples by learning the\nclass-conditional data distribution using generative models. Via extensive\ntheoretical and empirical analysis, we show that mixup, when used together with\nGenLabel, can effectively resolve the aforementioned phenomenon, improving the\ngeneralization performance and the adversarial robustness.",
          "link": "http://arxiv.org/abs/2201.02354",
          "publishedOn": "2022-01-11T00:39:34.320Z",
          "wordCount": 531,
          "title": "GenLabel: Mixup Relabeling using Generative Models. (arXiv:2201.02354v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02198",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shao_D/0/1/0/all/0/1\">Di Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>",
          "description": "Intracranial aneurysms are common nowadays and how to detect them\nintelligently is of great significance in digital health. While most existing\ndeep learning research focused on medical images in a supervised way, we\nintroduce an unsupervised method for the detection of intracranial aneurysms\nbased on 3D point cloud data. In particular, our method consists of two stages:\nunsupervised pre-training and downstream tasks. As for the former, the main\nidea is to pair each point cloud with its jittered counterpart and maximise\ntheir correspondence. Then we design a dual-branch contrastive network with an\nencoder for each branch and a subsequent common projection head. As for the\nlatter, we design simple networks for supervised classification and\nsegmentation training. Experiments on the public dataset (IntrA) show that our\nunsupervised method achieves comparable or even better performance than some\nstate-of-the-art supervised techniques, and it is most prominent in the\ndetection of aneurysmal vessels. Experiments on the ModelNet40 also show that\nour method achieves the accuracy of 90.79\\% which outperforms existing\nstate-of-the-art unsupervised models.",
          "link": "http://arxiv.org/abs/2201.02198",
          "publishedOn": "2022-01-11T00:39:34.298Z",
          "wordCount": 617,
          "title": "3D Intracranial Aneurysm Classification and Segmentation via Unsupervised Dual-branch Learning. (arXiv:2201.02198v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pellizzoni_P/0/1/0/all/0/1\">Paolo Pellizzoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietracaprina_A/0/1/0/all/0/1\">Andrea Pietracaprina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pucci_G/0/1/0/all/0/1\">Geppino Pucci</a>",
          "description": "Metric $k$-center clustering is a fundamental unsupervised learning\nprimitive. Although widely used, this primitive is heavily affected by noise in\nthe data, so that a more sensible variant seeks for the best solution that\ndisregards a given number $z$ of points of the dataset, called outliers. We\nprovide efficient algorithms for this important variant in the streaming model\nunder the sliding window setting, where, at each time step, the dataset to be\nclustered is the window $W$ of the most recent data items. Our algorithms\nachieve $O(1)$ approximation and, remarkably, require a working memory linear\nin $k+z$ and only logarithmic in $|W|$. As a by-product, we show how to\nestimate the effective diameter of the window $W$, which is a measure of the\nspread of the window points, disregarding a given fraction of noisy distances.\nWe also provide experimental evidence of the practical viability of our\ntheoretical results.",
          "link": "http://arxiv.org/abs/2201.02448",
          "publishedOn": "2022-01-11T00:39:34.291Z",
          "wordCount": 566,
          "title": "k-Center Clustering with Outliers in Sliding Windows. (arXiv:2201.02448v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.10252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1\">Surya Kant Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamdar_J/0/1/0/all/0/1\">Juhi Kamdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_M/0/1/0/all/0/1\">Meet Gandhi</a>",
          "description": "Transformers have seen an unprecedented rise in Natural Language Processing\nand Computer Vision tasks. However, in audio tasks, they are either infeasible\nto train due to extremely large sequence length of audio waveforms or incur a\nperformance penalty when trained on Fourier-based features. In this work, we\nintroduce an architecture, Audiomer, where we combine 1D Residual Networks with\nPerformer Attention to achieve state-of-the-art performance in keyword spotting\nwith raw audio waveforms, outperforming all previous methods while being\ncomputationally cheaper and parameter-efficient. Additionally, our model has\npractical advantages for speech processing, such as inference on arbitrarily\nlong audio clips owing to the absence of positional encoding. The code is\navailable at https://github.com/The-Learning-Machines/Audiomer-PyTorch.",
          "link": "http://arxiv.org/abs/2109.10252",
          "publishedOn": "2022-01-11T00:39:34.241Z",
          "wordCount": 587,
          "title": "Audiomer: A Convolutional Transformer For Keyword Spotting. (arXiv:2109.10252v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1\">Lianghao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1\">Peng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1\">Liefeng Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianyi Chen</a>",
          "description": "Crime prediction is crucial for public safety and resource optimization, yet\nis very challenging due to two aspects: i) the dynamics of criminal patterns\nacross time and space, crime events are distributed unevenly on both spatial\nand temporal domains; ii) time-evolving dependencies between different types of\ncrimes (e.g., Theft, Robbery, Assault, Damage) which reveal fine-grained\nsemantics of crimes. To tackle these challenges, we propose Spatial-Temporal\nSequential Hypergraph Network (ST-SHN) to collectively encode complex crime\nspatial-temporal patterns as well as the underlying category-wise crime\nsemantic relationships. In specific, to handle spatial-temporal dynamics under\nthe long-range and global context, we design a graph-structured message passing\narchitecture with the integration of the hypergraph learning paradigm. To\ncapture category-wise crime heterogeneous relations in a dynamic environment,\nwe introduce a multi-channel routing mechanism to learn the time-evolving\nstructural dependency across crime types. We conduct extensive experiments on\ntwo real-world datasets, showing that our proposed ST-SHN framework can\nsignificantly improve the prediction performance as compared to various\nstate-of-the-art baselines. The source code is available at:\nhttps://github.com/akaxlh/ST-SHN.",
          "link": "http://arxiv.org/abs/2201.02435",
          "publishedOn": "2022-01-11T00:39:34.235Z",
          "wordCount": 602,
          "title": "Spatial-Temporal Sequential Hypergraph Network for Crime Prediction. (arXiv:2201.02435v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.06784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1\">James Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huichen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xinchen Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Mengye Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Ming Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitar_E/0/1/0/all/0/1\">Eilyan Bitar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yumer_E/0/1/0/all/0/1\">Ersin Yumer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1\">Raquel Urtasun</a>",
          "description": "Modern self-driving perception systems have been shown to improve upon\nprocessing complementary inputs such as LiDAR with images. In isolation, 2D\nimages have been found to be extremely vulnerable to adversarial attacks. Yet,\nthere have been limited studies on the adversarial robustness of multi-modal\nmodels that fuse LiDAR features with image features. Furthermore, existing\nworks do not consider physically realizable perturbations that are consistent\nacross the input modalities. In this paper, we showcase practical\nsusceptibilities of multi-sensor detection by placing an adversarial object on\ntop of a host vehicle. We focus on physically realizable and input-agnostic\nattacks as they are feasible to execute in practice, and show that a single\nuniversal adversary can hide different host vehicles from state-of-the-art\nmulti-modal detectors. Our experiments demonstrate that successful attacks are\nprimarily caused by easily corrupted image features. Furthermore, we find that\nin modern sensor fusion methods which project image features into 3D,\nadversarial attacks can exploit the projection process to generate false\npositives across distant regions in 3D. Towards more robust multi-modal\nperception systems, we show that adversarial training with feature denoising\ncan boost robustness to such attacks significantly. However, we find that\nstandard adversarial defenses still struggle to prevent false positives which\nare also caused by inaccurate associations between 3D LiDAR points and 2D\npixels.",
          "link": "http://arxiv.org/abs/2101.06784",
          "publishedOn": "2022-01-11T00:39:34.220Z",
          "wordCount": 694,
          "title": "Exploring Adversarial Robustness of Multi-Sensor Perception Systems in Self Driving. (arXiv:2101.06784v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elangovan_A/0/1/0/all/0/1\">Aparna Elangovan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pires_D/0/1/0/all/0/1\">Douglas E. V. Pires</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_M/0/1/0/all/0/1\">Melissa J. Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verspoor_K/0/1/0/all/0/1\">Karin Verspoor</a>",
          "description": "Protein-protein interactions (PPIs) are critical to normal cellular function\nand are related to many disease pathways. However, only 4% of PPIs are\nannotated with PTMs in biological knowledge databases such as IntAct, mainly\nperformed through manual curation, which is neither time nor cost-effective. We\nuse the IntAct PPI database to create a distant supervised dataset annotated\nwith interacting protein pairs, their corresponding PTM type, and associated\nabstracts from the PubMed database. We train an ensemble of BioBERT models -\ndubbed PPI-BioBERT-x10 to improve confidence calibration. We extend the use of\nensemble average confidence approach with confidence variation to counteract\nthe effects of class imbalance to extract high confidence predictions. The\nPPI-BioBERT-x10 model evaluated on the test set resulted in a modest F1-micro\n41.3 (P =5 8.1, R = 32.1). However, by combining high confidence and low\nvariation to identify high quality predictions, tuning the predictions for\nprecision, we retained 19% of the test predictions with 100% precision. We\nevaluated PPI-BioBERT-x10 on 18 million PubMed abstracts and extracted 1.6\nmillion (546507 unique PTM-PPI triplets) PTM-PPI predictions, and filter ~ 5700\n(4584 unique) high confidence predictions. Of the 5700, human evaluation on a\nsmall randomly sampled subset shows that the precision drops to 33.7% despite\nconfidence calibration and highlights the challenges of generalisability beyond\nthe test set even with confidence calibration. We circumvent the problem by\nonly including predictions associated with multiple papers, improving the\nprecision to 58.8%. In this work, we highlight the benefits and challenges of\ndeep learning-based text mining in practice, and the need for increased\nemphasis on confidence calibration to facilitate human curation efforts.",
          "link": "http://arxiv.org/abs/2201.02229",
          "publishedOn": "2022-01-11T00:39:34.213Z",
          "wordCount": 711,
          "title": "Large-scale protein-protein post-translational modification extraction with distant supervision and confidence calibrated BioBERT. (arXiv:2201.02229v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.04973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Paras Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ajay Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1\">Ion Stoica</a>",
          "description": "Recent work learns contextual representations of source code by\nreconstructing tokens from their context. For downstream semantic understanding\ntasks like summarizing code in English, these representations should ideally\ncapture program functionality. However, we show that the popular\nreconstruction-based BERT model is sensitive to source code edits, even when\nthe edits preserve semantics. We propose ContraCode: a contrastive pre-training\ntask that learns code functionality, not form. ContraCode pre-trains a neural\nnetwork to identify functionally similar variants of a program among many\nnon-equivalent distractors. We scalably generate these variants using an\nautomated source-to-source compiler as a form of data augmentation. Contrastive\npre-training improves JavaScript summarization and TypeScript type inference\naccuracy by 2% to 13%. We also propose a new zero-shot JavaScript code clone\ndetection dataset, showing that ContraCode is both more robust and semantically\nmeaningful. On it, we outperform RoBERTa by 39% AUROC in an adversarial setting\nand up to 5% on natural code.",
          "link": "http://arxiv.org/abs/2007.04973",
          "publishedOn": "2022-01-11T00:39:34.185Z",
          "wordCount": 652,
          "title": "Contrastive Code Representation Learning. (arXiv:2007.04973v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02313",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Wood_K/0/1/0/all/0/1\">Killian Wood</a>, <a href=\"http://arxiv.org/find/math/1/au:+DallAnese_E/0/1/0/all/0/1\">Emiliano Dall&#x27;Anese</a>",
          "description": "This paper focuses on stochastic saddle point problems with\ndecision-dependent distributions in both the static and time-varying settings.\nThese are problems whose objective is the expected value of a stochastic payoff\nfunction, where random variables are drawn from a distribution induced by a\ndistributional map. For general distributional maps, the problem of finding\nsaddle points is in general computationally burdensome, even if the\ndistribution is known. To enable a tractable solution approach, we introduce\nthe notion of equilibrium points -- which are saddle points for the stationary\nstochastic minimax problem that they induce -- and provide conditions for their\nexistence and uniqueness. We demonstrate that the distance between the two\nclasses of solutions is bounded provided that the objective has a\nstrongly-convex-strongly-concave payoff and Lipschitz continuous distributional\nmap. We develop deterministic and stochastic primal-dual algorithms and\ndemonstrate their convergence to the equilibrium point. In particular, by\nmodeling errors emerging from a stochastic gradient estimator as sub-Weibull\nrandom variables, we provide error bounds in expectation and in high\nprobability that hold for each iteration; moreover, we show convergence to a\nneighborhood in expectation and almost surely. Finally, we investigate a\ncondition on the distributional map -- which we call opposing mixture dominance\n-- that ensures the objective is strongly-convex-strongly-concave. Under this\nassumption, we show that primal-dual algorithms converge to the saddle points\nin a similar fashion.",
          "link": "http://arxiv.org/abs/2201.02313",
          "publishedOn": "2022-01-11T00:39:34.179Z",
          "wordCount": 650,
          "title": "Stochastic Saddle Point Problems with Decision-Dependent Distributions. (arXiv:2201.02313v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02300",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Miyaguchi_K/0/1/0/all/0/1\">Kohei Miyaguchi</a>",
          "description": "We are concerned with the problem of hyperparameter selection of offline\npolicy evaluation (OPE). OPE is a key component of offline reinforcement\nlearning, which is a core technology for data-driven decision optimization\nwithout environment simulators. However, the current state-of-the-art OPE\nmethods are not hyperparameter-free, which undermines their utility in\nreal-life applications. We address this issue by introducing a new approximate\nhyperparameter selection (AHS) framework for OPE, which defines a notion of\noptimality (called selection criteria) in a quantitative and interpretable\nmanner without hyperparameters. We then derive four AHS methods each of which\nhas different characteristics such as convergence rate and time complexity.\nFinally, we verify effectiveness and limitation of these methods with a\npreliminary experiment.",
          "link": "http://arxiv.org/abs/2201.02300",
          "publishedOn": "2022-01-11T00:39:34.171Z",
          "wordCount": 543,
          "title": "A Theoretical Framework of Almost Hyperparameter-free Hyperparameter Selection Methods for Offline Policy Evaluation. (arXiv:2201.02300v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1\">Huaiqian You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DElia_M/0/1/0/all/0/1\">Marta D&#x27;Elia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tian Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silling_S/0/1/0/all/0/1\">Stewart Silling</a>",
          "description": "Neural operators have recently become popular tools for designing solution\nmaps between function spaces in the form of neural networks. Differently from\nclassical scientific machine learning approaches that learn parameters of a\nknown partial differential equation (PDE) for a single instance of the input\nparameters at a fixed resolution, neural operators approximate the solution map\nof a family of PDEs. Despite their success, the uses of neural operators are so\nfar restricted to relatively shallow neural networks and confined to learning\nhidden governing laws. In this work, we propose a novel nonlocal neural\noperator, which we refer to as nonlocal kernel network (NKN), that is\nresolution independent, characterized by deep neural networks, and capable of\nhandling a variety of tasks such as learning governing equations and\nclassifying images. Our NKN stems from the interpretation of the neural network\nas a discrete nonlocal diffusion reaction equation that, in the limit of\ninfinite layers, is equivalent to a parabolic nonlocal equation, whose\nstability is analyzed via nonlocal vector calculus. The resemblance with\nintegral forms of neural operators allows NKNs to capture long-range\ndependencies in the feature space, while the continuous treatment of\nnode-to-node interactions makes NKNs resolution independent. The resemblance\nwith neural ODEs, reinterpreted in a nonlocal sense, and the stable network\ndynamics between layers allow for generalization of NKN's optimal parameters\nfrom shallow to deep networks. This fact enables the use of shallow-to-deep\ninitialization techniques. Our tests show that NKNs outperform baseline methods\nin both learning governing equations and image classification tasks and\ngeneralize well to different resolutions and depths.",
          "link": "http://arxiv.org/abs/2201.02217",
          "publishedOn": "2022-01-11T00:39:34.164Z",
          "wordCount": 688,
          "title": "Nonlocal Kernel Network (NKN): a Stable and Resolution-Independent Deep Neural Network. (arXiv:2201.02217v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02273",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Ali_S/0/1/0/all/0/1\">Sarwan Ali</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bello_B/0/1/0/all/0/1\">Babatunde Bello</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chourasia_P/0/1/0/all/0/1\">Prakash Chourasia</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Punathil_R/0/1/0/all/0/1\">Ria Thazhe Punathil</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhou_Y/0/1/0/all/0/1\">Yijing Zhou</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Patterson_M/0/1/0/all/0/1\">Murray Patterson</a>",
          "description": "COVID-19 pandemic, is still unknown and is an important open question. There\nare speculations that bats are a possible origin. Likewise, there are many\nclosely related (corona-) viruses, such as SARS, which was found to be\ntransmitted through civets. The study of the different hosts which can be\npotential carriers and transmitters of deadly viruses to humans is crucial to\nunderstanding, mitigating and preventing current and future pandemics. In\ncoronaviruses, the surface (S) protein, or spike protein, is an important part\nof determining host specificity since it is the point of contact between the\nvirus and the host cell membrane. In this paper, we classify the hosts of over\nfive thousand coronaviruses from their spike protein sequences, segregating\nthem into clusters of distinct hosts among avians, bats, camels, swines, humans\nand weasels, to name a few. We propose a feature embedding based on the\nwell-known position-weight matrix (PWM), which we call PWM2Vec, and use to\ngenerate feature vectors from the spike protein sequences of these\ncoronaviruses. While our embedding is inspired by the success of PWMs in\nbiological applications such as determining protein function, or identifying\ntranscription factor binding sites, we are the first (to the best of our\nknowledge) to use PWMs in the context of host classification from viral\nsequences to generate a fixed-length feature vector representation. The results\non the real world data show that in using PWM2Vec, we are able to perform\ncomparably well as compared to baseline models. We also measure the importance\nof different amino acids using information gain to show the amino acids which\nare important for predicting the host of a given coronavirus.",
          "link": "http://arxiv.org/abs/2201.02273",
          "publishedOn": "2022-01-11T00:39:34.143Z",
          "wordCount": 754,
          "title": "PWM2Vec: An Efficient Embedding Approach for Viral Host Specification from Coronavirus Spike Sequences. (arXiv:2201.02273v1 [q-bio.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2112.06116",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berger_Z/0/1/0/all/0/1\">Zachary Berger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1\">Parth Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tian Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alex Wong</a>",
          "description": "We study the effect of adversarial perturbations of images on deep stereo\nmatching networks for the disparity estimation task. We present a method to\ncraft a single set of perturbations that, when added to any stereo image pair\nin a dataset, can fool a stereo network to significantly alter the perceived\nscene geometry. Our perturbation images are \"universal\" in that they not only\ncorrupt estimates of the network on the dataset they are optimized for, but\nalso generalize to stereo networks with different architectures across\ndifferent datasets. We evaluate our approach on multiple public benchmark\ndatasets and show that our perturbations can increase D1-error (akin to fooling\nrate) of state-of-the-art stereo networks from 1% to as much as 87%. We\ninvestigate the effect of perturbations on the estimated scene geometry and\nidentify object classes that are most vulnerable. Our analysis on the\nactivations of registered points between left and right images led us to find\nthat certain architectural components, i.e. deformable convolution and explicit\nmatching, can increase robustness against adversaries. We demonstrate that by\nsimply designing networks with such components, one can reduce the effect of\nadversaries by up to 60.5%, which rivals the robustness of networks fine-tuned\nwith costly adversarial data augmentation.",
          "link": "http://arxiv.org/abs/2112.06116",
          "publishedOn": "2022-01-11T00:39:34.137Z",
          "wordCount": 665,
          "title": "Stereoscopic Universal Perturbations across Different Architectures and Datasets. (arXiv:2112.06116v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02588",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_J/0/1/0/all/0/1\">Javed Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hafiz_R/0/1/0/all/0/1\">Rehan Hafiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mohsen Ali</a>",
          "description": "This paper presents FogAdapt, a novel approach for domain adaptation of\nsemantic segmentation for dense foggy scenes. Although significant research has\nbeen directed to reduce the domain shift in semantic segmentation, adaptation\nto scenes with adverse weather conditions remains an open question. Large\nvariations in the visibility of the scene due to weather conditions, such as\nfog, smog, and haze, exacerbate the domain shift, thus making unsupervised\nadaptation in such scenarios challenging. We propose a self-entropy and\nmulti-scale information augmented self-supervised domain adaptation method\n(FogAdapt) to minimize the domain shift in foggy scenes segmentation. Supported\nby the empirical evidence that an increase in fog density results in high\nself-entropy for segmentation probabilities, we introduce a self-entropy based\nloss function to guide the adaptation method. Furthermore, inferences obtained\nat different image scales are combined and weighted by the uncertainty to\ngenerate scale-invariant pseudo-labels for the target domain. These\nscale-invariant pseudo-labels are robust to visibility and scale variations. We\nevaluate the proposed model on real clear-weather scenes to real foggy scenes\nadaptation and synthetic non-foggy images to real foggy scenes adaptation\nscenarios. Our experiments demonstrate that FogAdapt significantly outperforms\nthe current state-of-the-art in semantic segmentation of foggy images.\nSpecifically, by considering the standard settings compared to state-of-the-art\n(SOTA) methods, FogAdapt gains 3.8% on Foggy Zurich, 6.0% on Foggy\nDriving-dense, and 3.6% on Foggy Driving in mIoU when adapted from Cityscapes\nto Foggy Zurich.",
          "link": "http://arxiv.org/abs/2201.02588",
          "publishedOn": "2022-01-11T00:39:34.130Z",
          "wordCount": 670,
          "title": "Leveraging Scale-Invariance and Uncertainity with Self-Supervised Domain Adaptation for Semantic Segmentation of Foggy Scenes. (arXiv:2201.02588v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Natsiou_A/0/1/0/all/0/1\">Anastasia Natsiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OLeary_S/0/1/0/all/0/1\">Sean O&#x27;Leary</a>",
          "description": "The rise of deep learning algorithms has led many researchers to withdraw\nfrom using classic signal processing methods for sound generation. Deep\nlearning models have achieved expressive voice synthesis, realistic sound\ntextures, and musical notes from virtual instruments. However, the most\nsuitable deep learning architecture is still under investigation. The choice of\narchitecture is tightly coupled to the audio representations. A sound's\noriginal waveform can be too dense and rich for deep learning models to deal\nwith efficiently - and complexity increases training time and computational\ncost. Also, it does not represent sound in the manner in which it is perceived.\nTherefore, in many cases, the raw audio has been transformed into a compressed\nand more meaningful form using upsampling, feature-extraction, or even by\nadopting a higher level illustration of the waveform. Furthermore, conditional\non the form chosen, additional conditioning representations, different model\narchitectures, and numerous metrics for evaluating the reconstructed sound have\nbeen investigated. This paper provides an overview of audio representations\napplied to sound synthesis using deep learning. Additionally, it presents the\nmost significant methods for developing and evaluating a sound synthesis\narchitecture using deep learning models, always depending on the audio\nrepresentation.",
          "link": "http://arxiv.org/abs/2201.02490",
          "publishedOn": "2022-01-11T00:39:34.124Z",
          "wordCount": 622,
          "title": "Audio representations for deep learning in sound synthesis: A review. (arXiv:2201.02490v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Quan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Songcan Chen</a>",
          "description": "Multi-task learning is to improve the performance of the model by\ntransferring and exploiting common knowledge among tasks. Existing MTL works\nmainly focus on the scenario where label sets among multiple tasks (MTs) are\nusually the same, thus they can be utilized for learning across the tasks.\nWhile almost rare works explore the scenario where each task only has a small\namount of training samples, and their label sets are just partially overlapped\nor even not. Learning such MTs is more challenging because of less correlation\ninformation available among these tasks. For this, we propose a framework to\nlearn these tasks by jointly leveraging both abundant information from a learnt\nauxiliary big task with sufficiently many classes to cover those of all these\ntasks and the information shared among those partially-overlapped tasks. In our\nimplementation of using the same neural network architecture of the learnt\nauxiliary task to learn individual tasks, the key idea is to utilize available\nlabel information to adaptively prune the hidden layer neurons of the auxiliary\nnetwork to construct corresponding network for each task, while accompanying a\njoint learning across individual tasks. Our experimental results demonstrate\nits effectiveness in comparison with the state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2201.02305",
          "publishedOn": "2022-01-11T00:39:34.118Z",
          "wordCount": 619,
          "title": "Learning Multi-Tasks with Inconsistent Labels by using Auxiliary Big Task. (arXiv:2201.02305v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00732",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Buzzicotti_M/0/1/0/all/0/1\">Michele Buzzicotti</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bonaccorso_F/0/1/0/all/0/1\">Fabio Bonaccorso</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Biferale_L/0/1/0/all/0/1\">Luca Biferale</a>",
          "description": "We design a machine learning technique to solve the general problem of\ninferring physical parameters from the observation of turbulent flows, a\nrelevant exercise in many theoretical and applied fields, from engineering to\nearth observation and astrophysics. Our approach is to train the machine\nlearning system to regress the rotation frequency of the flow's reference\nframe, from the observation of the flow's velocity amplitude on a 2d plane\nextracted from the 3d domain. The machine learning approach consists of a Deep\nConvolutional Neural Network (DCNN) of the same kind developed in computer\nvision. The training and validation datasets are produced by means of fully\nresolved direct numerical simulations. This study shows interesting results\nfrom two different points of view. From the machine learning point of view it\nshows the potential of DCNN, reaching good results on such a particularly\ncomplex problem that goes well outside the limits of human vision. Second, from\nthe physics point of view, it provides an example on how machine learning can\nbe exploited in data analysis to infer information that would be inaccessible\notherwise. Indeed, by comparing DCNN with the other possible Bayesian\napproaches, we find that DCNN yields to a much higher inference accuracy in all\nthe examined cases.",
          "link": "http://arxiv.org/abs/2201.00732",
          "publishedOn": "2022-01-11T00:39:34.094Z",
          "wordCount": 628,
          "title": "Inferring Turbulent Parameters via Machine Learning. (arXiv:2201.00732v1 [physics.flu-dyn] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02609",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vaze_S/0/1/0/all/0/1\">Sagar Vaze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>",
          "description": "In this paper, we consider a highly general image recognition setting\nwherein, given a labelled and unlabelled set of images, the task is to\ncategorize all images in the unlabelled set. Here, the unlabelled images may\ncome from labelled classes or from novel ones. Existing recognition methods are\nnot able to deal with this setting, because they make several restrictive\nassumptions, such as the unlabelled instances only coming from known - or\nunknown - classes and the number of unknown classes being known a-priori. We\naddress the more unconstrained setting, naming it 'Generalized Category\nDiscovery', and challenge all these assumptions. We first establish strong\nbaselines by taking state-of-the-art algorithms from novel category discovery\nand adapting them for this task. Next, we propose the use of vision\ntransformers with contrastive representation learning for this open world\nsetting. We then introduce a simple yet effective semi-supervised $k$-means\nmethod to cluster the unlabelled data into seen and unseen classes\nautomatically, substantially outperforming the baselines. Finally, we also\npropose a new approach to estimate the number of classes in the unlabelled\ndata. We thoroughly evaluate our approach on public datasets for generic object\nclassification including CIFAR10, CIFAR100 and ImageNet-100, and for\nfine-grained visual recognition including CUB, Stanford Cars and Herbarium19,\nbenchmarking on this new setting to foster future research.",
          "link": "http://arxiv.org/abs/2201.02609",
          "publishedOn": "2022-01-11T00:39:33.888Z",
          "wordCount": 636,
          "title": "Generalized Category Discovery. (arXiv:2201.02609v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02555",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hughes_A/0/1/0/all/0/1\">Aidan J. Hughes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bull_L/0/1/0/all/0/1\">Lawrence A. Bull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_P/0/1/0/all/0/1\">Paul Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dervilis_N/0/1/0/all/0/1\">Nikolaos Dervilis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worden_K/0/1/0/all/0/1\">Keith Worden</a>",
          "description": "Classification models are a fundamental component of physical-asset\nmanagement technologies such as structural health monitoring (SHM) systems and\ndigital twins. Previous work introduced \\textit{risk-based active learning}, an\nonline approach for the development of statistical classifiers that takes into\naccount the decision-support context in which they are applied. Decision-making\nis considered by preferentially querying data labels according to\n\\textit{expected value of perfect information} (EVPI). Although several\nbenefits are gained by adopting a risk-based active learning approach,\nincluding improved decision-making performance, the algorithms suffer from\nissues relating to sampling bias as a result of the guided querying process.\nThis sampling bias ultimately manifests as a decline in decision-making\nperformance during the later stages of active learning, which in turn\ncorresponds to lost resource/utility.\n\nThe current paper proposes two novel approaches to counteract the effects of\nsampling bias: \\textit{semi-supervised learning}, and \\textit{discriminative\nclassification models}. These approaches are first visualised using a synthetic\ndataset, then subsequently applied to an experimental case study, specifically,\nthe Z24 Bridge dataset. The semi-supervised learning approach is shown to have\nvariable performance; with robustness to sampling bias dependent on the\nsuitability of the generative distributions selected for the model with respect\nto each dataset. In contrast, the discriminative classifiers are shown to have\nexcellent robustness to the effects of sampling bias. Moreover, it was found\nthat the number of inspections made during a monitoring campaign, and therefore\nresource expenditure, could be reduced with the careful selection of the\nstatistical classifiers used within a decision-supporting monitoring system.",
          "link": "http://arxiv.org/abs/2201.02555",
          "publishedOn": "2022-01-11T00:39:33.833Z",
          "wordCount": 686,
          "title": "On robust risk-based active-learning algorithms for enhanced decision support. (arXiv:2201.02555v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tadesse_G/0/1/0/all/0/1\">Girmaw Abebe Tadesse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogallo_W/0/1/0/all/0/1\">William Ogallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wanjiru_C/0/1/0/all/0/1\">Catherine Wanjiru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachira_C/0/1/0/all/0/1\">Charles Wachira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulang_I/0/1/0/all/0/1\">Isaiah Onando Mulang&#x27;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_V/0/1/0/all/0/1\">Vibha Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walcott_Bryant_A/0/1/0/all/0/1\">Aisha Walcott-Bryant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speakman_S/0/1/0/all/0/1\">Skyler Speakman</a>",
          "description": "Anomalous pattern detection aims to identify instances where deviation from\nnormalcy is evident, and is widely applicable across domains. Multiple\nanomalous detection techniques have been proposed in the state of the art.\nHowever, there is a common lack of a principled and scalable feature selection\nmethod for efficient discovery. Existing feature selection techniques are often\nconducted by optimizing the performance of prediction outcomes rather than its\nsystemic deviations from the expected. In this paper, we proposed a\nsparsity-based automated feature selection (SAFS) framework, which encodes\nsystemic outcome deviations via the sparsity of feature-driven odds ratios.\nSAFS is a model-agnostic approach with usability across different discovery\ntechniques. SAFS achieves more than $3\\times$ reduction in computation time\nwhile maintaining detection performance when validated on publicly available\ncritical care dataset. SAFS also results in a superior performance when\ncompared against multiple baselines for feature selection.",
          "link": "http://arxiv.org/abs/2201.02008",
          "publishedOn": "2022-01-08T00:37:51.042Z",
          "wordCount": 576,
          "title": "Sparsity-based Feature Selection for Anomalous Subgroup Discovery. (arXiv:2201.02008v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">ZiCheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">CongYing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">TianDe Guo</a>",
          "description": "Generating images from a single sample, as a newly developing branch of image\nsynthesis, has attracted extensive attention. In this paper, we formulate this\nproblem as sampling from the conditional distribution of a single image, and\npropose a hierarchical framework that simplifies the learning of the intricate\nconditional distributions through the successive learning of the distributions\nabout structure, semantics and texture, making the process of learning and\ngeneration comprehensible. On this basis, we design ExSinGAN composed of three\ncascaded GANs for learning an explainable generative model from a given image,\nwhere the cascaded GANs model the distributions about structure, semantics and\ntexture successively. ExSinGAN is learned not only from the internal patches of\nthe given image as the previous works did, but also from the external prior\nobtained by the GAN inversion technique. Benefiting from the appropriate\ncombination of internal and external information, ExSinGAN has a more powerful\ncapability of generation and competitive generalization ability for the image\nmanipulation tasks compared with prior works.",
          "link": "http://arxiv.org/abs/2105.07350",
          "publishedOn": "2022-01-08T00:37:50.985Z",
          "wordCount": 620,
          "title": "ExSinGAN: Learning an Explainable Generative Model from a Single Image. (arXiv:2105.07350v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Diana Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elgammal_A/0/1/0/all/0/1\">Ahmed Elgammal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazzone_M/0/1/0/all/0/1\">Marian Mazzone</a>",
          "description": "We present a machine learning system that can quantify fine art paintings\nwith a set of visual elements and principles of art. This formal analysis is\nfundamental for understanding art, but developing such a system is challenging.\nPaintings have high visual complexities, but it is also difficult to collect\nenough training data with direct labels. To resolve these practical\nlimitations, we introduce a novel mechanism, called proxy learning, which\nlearns visual concepts in paintings though their general relation to styles.\nThis framework does not require any visual annotation, but only uses style\nlabels and a general relationship between visual concepts and style. In this\npaper, we propose a novel proxy model and reformulate four pre-existing methods\nin the context of proxy learning. Through quantitative and qualitative\ncomparison, we evaluate these methods and compare their effectiveness in\nquantifying the artistic visual concepts, where the general relationship is\nestimated by language models; GloVe or BERT. The language modeling is a\npractical and scalable solution requiring no labeling, but it is inevitably\nimperfect. We demonstrate how the new proxy model is robust to the\nimperfection, while the other models are sensitively affected by it.",
          "link": "http://arxiv.org/abs/2201.01819",
          "publishedOn": "2022-01-08T00:37:50.920Z",
          "wordCount": 671,
          "title": "Formal Analysis of Art: Proxy Learning of Visual Concepts from Style Through Language Models. (arXiv:2201.01819v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lina Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>",
          "description": "Zero-Shot Learning (ZSL) aims to transfer classification capability from seen\nto unseen classes. Recent methods have proved that generalization and\nspecialization are two essential abilities to achieve good performance in ZSL.\nHowever, they all focus on only one of the abilities, resulting in models that\nare either too general with the degraded classifying ability or too specialized\nto generalize to unseen classes. In this paper, we propose an end-to-end\nnetwork with balanced generalization and specialization abilities, termed as\nBGSNet, to take advantage of both abilities, and balance them at instance- and\ndataset-level. Specifically, BGSNet consists of two branches: the\nGeneralization Network (GNet), which applies episodic meta-learning to learn\ngeneralized knowledge, and the Balanced Specialization Network (BSNet), which\nadopts multiple attentive extractors to extract discriminative features and\nfulfill the instance-level balance. A novel self-adjusting diversity loss is\ndesigned to optimize BSNet with less redundancy and more diversity. We further\npropose a differentiable dataset-level balance and update the weights in a\nlinear annealing schedule to simulate network pruning and thus obtain the\noptimal structure for BSNet at a low cost with dataset-level balance achieved.\nExperiments on four benchmark datasets demonstrate our model's effectiveness.\nSufficient component ablations prove the necessity of integrating\ngeneralization and specialization abilities.",
          "link": "http://arxiv.org/abs/2201.01961",
          "publishedOn": "2022-01-08T00:37:50.853Z",
          "wordCount": 628,
          "title": "Balancing Generalization and Specialization in Zero-shot Learning. (arXiv:2201.01961v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2111.03308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duy_K/0/1/0/all/0/1\">Kha Dinh Duy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_T/0/1/0/all/0/1\">Taehyun Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huh_S/0/1/0/all/0/1\">Siwon Huh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hojoon Lee</a>",
          "description": "As machine learning (ML) technologies and applications are rapidly changing\nmany computing domains, security issues associated with ML are also emerging.\nIn the domain of systems security, many endeavors have been made to ensure ML\nmodel and data confidentiality. ML computations are often inevitably performed\nin untrusted environments and entail complex multi-party security requirements.\nHence, researchers have leveraged the Trusted Execution Environments (TEEs) to\nbuild confidential ML computation systems. We conduct a systematic and\ncomprehensive survey by classifying attack vectors and mitigation in\nconfidential ML computation in untrusted environments, analyzing the complex\nsecurity requirements in multi-party scenarios, and summarizing engineering\nchallenges in confidential ML implementation. Lastly, we suggest future\nresearch directions based on our study.",
          "link": "http://arxiv.org/abs/2111.03308",
          "publishedOn": "2022-01-08T00:37:50.737Z",
          "wordCount": 600,
          "title": "Confidential Machine Learning Computation in Untrusted Environments: A Systems Security Perspective. (arXiv:2111.03308v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.03685",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Runge_J/0/1/0/all/0/1\">Jakob Runge</a>",
          "description": "The paper introduces a novel conditional independence (CI) based method for\nlinear and nonlinear, lagged and contemporaneous causal discovery from\nobservational time series in the causally sufficient case. Existing CI-based\nmethods such as the PC algorithm and also common methods from other frameworks\nsuffer from low recall and partially inflated false positives for strong\nautocorrelation which is an ubiquitous challenge in time series. The novel\nmethod, PCMCI$^+$, extends PCMCI [Runge et al., 2019b] to include discovery of\ncontemporaneous links. PCMCI$^+$ improves the reliability of CI tests by\noptimizing the choice of conditioning sets and even benefits from\nautocorrelation. The method is order-independent and consistent in the oracle\ncase. A broad range of numerical experiments demonstrates that PCMCI$^+$ has\nhigher adjacency detection power and especially more contemporaneous\norientation recall compared to other methods while better controlling false\npositives. Optimized conditioning sets also lead to much shorter runtimes than\nthe PC algorithm. PCMCI$^+$ can be of considerable use in many real world\napplication scenarios where often time resolutions are too coarse to resolve\ntime delays and strong autocorrelation is present.",
          "link": "http://arxiv.org/abs/2003.03685",
          "publishedOn": "2022-01-08T00:37:50.202Z",
          "wordCount": 641,
          "title": "Discovering contemporaneous and lagged causal relations in autocorrelated nonlinear time series datasets. (arXiv:2003.03685v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.13578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pele_K/0/1/0/all/0/1\">Kathleen Pele</a> (ECM, MIST), <a href=\"http://arxiv.org/find/cs/1/au:+Baccou_J/0/1/0/all/0/1\">Jean Baccou</a> (IRSN, MIST), <a href=\"http://arxiv.org/find/cs/1/au:+Daridon_L/0/1/0/all/0/1\">Lo&#xef;c Daridon</a> (MIST, M&#xe9;TICE), <a href=\"http://arxiv.org/find/cs/1/au:+Liandrat_J/0/1/0/all/0/1\">Jacques Liandrat</a> (ECM, I2M), <a href=\"http://arxiv.org/find/cs/1/au:+Gouic_T/0/1/0/all/0/1\">Thibaut Le Gouic</a> (ECM, I2M), <a href=\"http://arxiv.org/find/cs/1/au:+Monerie_Y/0/1/0/all/0/1\">Yann Monerie</a> (MIST, M&#xe9;TICE), <a href=\"http://arxiv.org/find/cs/1/au:+Perales_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric P&#xe9;ral&#xe8;s</a> (IRSN, MIST)",
          "description": "This paper is devoted to the construction of a new fast-to-evaluate model for\nthe prediction of 2D crack paths in concrete-like microstructures. The model\ngenerates piecewise linear cracks paths with segmentation points selected using\na Markov chain model. The Markov chain kernel involves local indicators of\nmechanical interest and its parameters are learnt from numerical full-field 2D\nsimulations of craking using a cohesive-volumetric finite element solver called\nXPER. The resulting model exhibits a drastic improvement of CPU time in\ncomparison to simulations from XPER.",
          "link": "http://arxiv.org/abs/2112.13578",
          "publishedOn": "2022-01-08T00:37:49.962Z",
          "wordCount": 561,
          "title": "A probabilistic model for fast-to-evaluate 2D crack path prediction in heterogeneous materials. (arXiv:2112.13578v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalitov_R/0/1/0/all/0/1\">Ruslan Khalitov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhirong Yang</a>",
          "description": "Classification of long sequential data is an important Machine Learning task\nand appears in many application scenarios. Recurrent Neural Networks,\nTransformers, and Convolutional Neural Networks are three major techniques for\nlearning from sequential data. Among these methods, Temporal Convolutional\nNetworks (TCNs) which are scalable to very long sequences have achieved\nremarkable progress in time series regression. However, the performance of TCNs\nfor sequence classification is not satisfactory because they use a skewed\nconnection protocol and output classes at the last position. Such asymmetry\nrestricts their performance for classification which depends on the whole\nsequence. In this work, we propose a symmetric multi-scale architecture called\nCircular Dilated Convolutional Neural Network (CDIL-CNN), where every position\nhas an equal chance to receive information from other positions at the previous\nlayers. Our model gives classification logits in all positions, and we can\napply a simple ensemble learning to achieve a better decision. We have tested\nCDIL-CNN on various long sequential datasets. The experimental results show\nthat our method has superior performance over many state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2201.02143",
          "publishedOn": "2022-01-08T00:37:49.893Z",
          "wordCount": 593,
          "title": "Classification of Long Sequential Data using Circular Dilated Convolutional Neural Networks. (arXiv:2201.02143v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2110.12328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongyu Wang</a>",
          "description": "Spectral clustering is one of the most popular clustering methods. However,\nthe high computational cost due to the involved eigen-decomposition procedure\ncan immediately hinder its applications in large-scale tasks. In this paper we\nuse spectrum-preserving node reduction to accelerate eigen-decomposition and\ngenerate concise representations of data sets. Specifically, we create a small\nnumber of pseudonodes based on spectral similarity. Then, standard spectral\nclustering algorithm is performed on the smaller node set. Finally, each data\npoint in the original data set is assigned to the cluster as its representative\npseudo-node. The proposed framework run in nearly-linear time. Meanwhile, the\nclustering accuracy can be significantly improved by mining concise\nrepresentations. The experimental results show dramatically improved clustering\nperformance when compared with state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2110.12328",
          "publishedOn": "2022-01-08T00:37:49.854Z",
          "wordCount": 556,
          "title": "Improving Spectral Clustering Using Spectrum-Preserving Node Reduction. (arXiv:2110.12328v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01985",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Faury_L/0/1/0/all/0/1\">Louis Faury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abeille_M/0/1/0/all/0/1\">Marc Abeille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jun_K/0/1/0/all/0/1\">Kwang-Sung Jun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calauzenes_C/0/1/0/all/0/1\">Cl&#xe9;ment Calauz&#xe8;nes</a>",
          "description": "Logistic Bandits have recently undergone careful scrutiny by virtue of their\ncombined theoretical and practical relevance. This research effort delivered\nstatistically efficient algorithms, improving the regret of previous strategies\nby exponentially large factors. Such algorithms are however strikingly costly\nas they require $\\Omega(t)$ operations at each round. On the other hand, a\ndifferent line of research focused on computational efficiency\n($\\mathcal{O}(1)$ per-round cost), but at the cost of letting go of the\naforementioned exponential improvements. Obtaining the best of both world is\nunfortunately not a matter of marrying both approaches. Instead we introduce a\nnew learning procedure for Logistic Bandits. It yields confidence sets which\nsufficient statistics can be easily maintained online without sacrificing\nstatistical tightness. Combined with efficient planning mechanisms we design\nfast algorithms which regret performance still match the problem-dependent\nlower-bound of Abeille et al. (2021). To the best of our knowledge, those are\nthe first Logistic Bandit algorithms that simultaneously enjoy statistical and\ncomputational efficiency.",
          "link": "http://arxiv.org/abs/2201.01985",
          "publishedOn": "2022-01-08T00:37:49.840Z",
          "wordCount": 575,
          "title": "Jointly Efficient and Optimal Algorithms for Logistic Bandits. (arXiv:2201.01985v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2110.13656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lan_J/0/1/0/all/0/1\">Jinhe Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Q/0/1/0/all/0/1\">Qingyuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chenhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kunping Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Desheng Wang</a>",
          "description": "Existed pre-trained models have achieved state-of-the-art performance on\nvarious text classification tasks. These models have proven to be useful in\nlearning universal language representations. However, the semantic discrepancy\nbetween similar texts cannot be effectively distinguished by advanced\npre-trained models, which have a great influence on the performance of\nhard-to-distinguish classes. To address this problem, we propose a novel\nContrastive Learning with Label Distance (CLLD) in this work. Inspired by\nrecent advances in contrastive learning, we specifically design a\nclassification method with label distance for learning contrastive classes.\nCLLD ensures the flexibility within the subtle differences that lead to\ndifferent label assignments, and generates the distinct representations for\neach class having similarity simultaneously. Extensive experiments on public\nbenchmarks and internal datasets demonstrate that our method improves the\nperformance of pre-trained models on classification tasks. Importantly, our\nexperiments suggest that the learned label distance relieve the adversarial\nnature of interclasses.",
          "link": "http://arxiv.org/abs/2110.13656",
          "publishedOn": "2022-01-08T00:37:49.796Z",
          "wordCount": 605,
          "title": "CLLD: Contrastive Learning with Label Distance for Text Classification. (arXiv:2110.13656v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01965",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yatong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gautam_T/0/1/0/all/0/1\">Tanmay Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojoudi_S/0/1/0/all/0/1\">Somayeh Sojoudi</a>",
          "description": "The non-convexity of the artificial neural network (ANN) training landscape\nbrings inherent optimization difficulties. While the traditional\nback-propagation stochastic gradient descent (SGD) algorithm and its variants\nare effective in certain cases, they can become stuck at spurious local minima\nand are sensitive to initializations and hyperparameters. Recent work has shown\nthat the training of an ANN with ReLU activations can be reformulated as a\nconvex program, bringing hope to globally optimizing interpretable ANNs.\nHowever, naively solving the convex training formulation has an exponential\ncomplexity, and even an approximation heuristic requires cubic time. In this\nwork, we characterize the quality of this approximation and develop two\nefficient algorithms that train ANNs with global convergence guarantees. The\nfirst algorithm is based on the alternating direction method of multiplier\n(ADMM). It solves both the exact convex formulation and the approximate\ncounterpart. Linear global convergence is achieved, and the initial several\niterations often yield a solution with high prediction accuracy. When solving\nthe approximate formulation, the per-iteration time complexity is quadratic.\nThe second algorithm, based on the \"sampled convex programs\" theory, is simpler\nto implement. It solves unconstrained convex formulations and converges to an\napproximately globally optimal classifier. The non-convexity of the ANN\ntraining landscape exacerbates when adversarial training is considered. We\napply the robust convex optimization theory to convex training and develop\nconvex formulations that train ANNs robust to adversarial inputs. Our analysis\nexplicitly focuses on one-hidden-layer fully connected ANNs, but can extend to\nmore sophisticated architectures.",
          "link": "http://arxiv.org/abs/2201.01965",
          "publishedOn": "2022-01-08T00:37:49.754Z",
          "wordCount": 682,
          "title": "Efficient Global Optimization of Two-layer ReLU Networks: Quadratic-time Algorithms and Adversarial Training. (arXiv:2201.01965v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.14501",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Geisa_A/0/1/0/all/0/1\">Ali Geisa</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mehta_R/0/1/0/all/0/1\">Ronak Mehta</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Helm_H/0/1/0/all/0/1\">Hayden S. Helm</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dey_J/0/1/0/all/0/1\">Jayanta Dey</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Eaton_E/0/1/0/all/0/1\">Eric Eaton</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dick_J/0/1/0/all/0/1\">Jeffery Dick</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Priebe_C/0/1/0/all/0/1\">Carey E. Priebe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vogelstein_J/0/1/0/all/0/1\">Joshua T. Vogelstein</a>",
          "description": "What is learning? 20$^{st}$ century formalizations of learning theory --\nwhich precipitated revolutions in artificial intelligence -- focus primarily on\n$\\mathit{in-distribution}$ learning, that is, learning under the assumption\nthat the training data are sampled from the same distribution as the evaluation\ndistribution. This assumption renders these theories inadequate for\ncharacterizing 21$^{st}$ century real world data problems, which are typically\ncharacterized by evaluation distributions that differ from the training data\ndistributions (referred to as out-of-distribution learning). We therefore make\na small change to existing formal definitions of learnability by relaxing that\nassumption. We then introduce $\\mathbf{learning\\ efficiency}$ (LE) to quantify\nthe amount a learner is able to leverage data for a given problem, regardless\nof whether it is an in- or out-of-distribution problem. We then define and\nprove the relationship between generalized notions of learnability, and show\nhow this framework is sufficiently general to characterize transfer, multitask,\nmeta, continual, and lifelong learning. We hope this unification helps bridge\nthe gap between empirical practice and theoretical guidance in real world\nproblems. Finally, because biological learning continues to outperform machine\nlearning algorithms on certain OOD challenges, we discuss the limitations of\nthis framework vis-\\'a-vis its ability to formalize biological learning,\nsuggesting multiple avenues for future research.",
          "link": "http://arxiv.org/abs/2109.14501",
          "publishedOn": "2022-01-08T00:37:49.623Z",
          "wordCount": 670,
          "title": "Towards a theory of out-of-distribution learning. (arXiv:2109.14501v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.00839",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiangyu Chang</a>",
          "description": "Spectral clustering has been one of the widely used methods for community\ndetection in networks. However, large-scale networks bring computational\nchallenges to the eigenvalue decomposition therein. In this paper, we study the\nspectral clustering using randomized sketching algorithms from a statistical\nperspective, where we typically assume the network data are generated from a\nstochastic block model that is not necessarily of full rank. To do this, we\nfirst use the recently developed sketching algorithms to obtain two randomized\nspectral clustering algorithms, namely, the random projection-based and the\nrandom sampling-based spectral clustering. Then we study the theoretical bounds\nof the resulting algorithms in terms of the approximation error for the\npopulation adjacency matrix, the misclassification error, and the estimation\nerror for the link probability matrix. It turns out that, under mild\nconditions, the randomized spectral clustering algorithms lead to the same\ntheoretical bounds as those of the original spectral clustering algorithm. We\nalso extend the results to degree-corrected stochastic block models. Numerical\nexperiments support our theoretical findings and show the efficiency of\nrandomized methods. A new R package called Rclust is developed and made\navailable to the public.",
          "link": "http://arxiv.org/abs/2002.00839",
          "publishedOn": "2022-01-08T00:37:49.513Z",
          "wordCount": 655,
          "title": "Randomized Spectral Clustering in Large-Scale Stochastic Block Models. (arXiv:2002.00839v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02037",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Smucler_E/0/1/0/all/0/1\">Ezequiel Smucler</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rotnitzky_A/0/1/0/all/0/1\">Andrea Rotnitzky</a>",
          "description": "We study the selection of adjustment sets for estimating the interventional\nmean under an individualized treatment rule. We assume a non-parametric causal\ngraphical model with, possibly, hidden variables and at least one adjustment\nset comprised of observable variables. Moreover, we assume that observable\nvariables have positive costs associated with them. We define the cost of an\nobservable adjustment set as the sum of the costs of the variables that\ncomprise it. We show that in this setting there exist adjustment sets that are\nminimum cost optimal, in the sense that they yield non-parametric estimators of\nthe interventional mean with the smallest asymptotic variance among those that\ncontrol for observable adjustment sets that have minimum cost. Our results are\nbased on the construction of a special flow network associated with the\noriginal causal graph. We show that a minimum cost optimal adjustment set can\nbe found by computing a maximum flow on the network, and then finding the set\nof vertices that are reachable from the source by augmenting paths. The\noptimaladj Python package implements the algorithms introduced in this paper.",
          "link": "http://arxiv.org/abs/2201.02037",
          "publishedOn": "2022-01-08T00:37:49.436Z",
          "wordCount": 612,
          "title": "A note on efficient minimum cost adjustment sets in causal graphical models. (arXiv:2201.02037v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10140",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prabhu_V/0/1/0/all/0/1\">Viraj Prabhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khare_S/0/1/0/all/0/1\">Shivam Khare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kartik_D/0/1/0/all/0/1\">Deeksha Kartik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1\">Judy Hoffman</a>",
          "description": "Most modern approaches for domain adaptive semantic segmentation rely on\ncontinued access to source data during adaptation, which may be infeasible due\nto computational or privacy constraints. We focus on source-free domain\nadaptation for semantic segmentation, wherein a source model must adapt itself\nto a new target domain given only unlabeled target data. We propose\nAugmentation Consistency-guided Self-training (AUGCO), a source-free adaptation\nalgorithm that uses the model's pixel-level predictive consistency across\ndiverse, automatically generated views of each target image along with model\nconfidence to identify reliable pixel predictions, and selectively self-trains\non those. AUGCO achieves state-of-the-art results for source-free adaptation on\n3 standard benchmarks for semantic segmentation, all within a simple to\nimplement and fast to converge method.",
          "link": "http://arxiv.org/abs/2107.10140",
          "publishedOn": "2022-01-08T00:37:49.126Z",
          "wordCount": 575,
          "title": "AUGCO: Augmentation Consistency-guided Self-training for Source-free Domain Adaptive Semantic Segmentation. (arXiv:2107.10140v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vendeville_A/0/1/0/all/0/1\">Antoine Vendeville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guedj_B/0/1/0/all/0/1\">Benjamin Guedj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shi Zhou</a>",
          "description": "We explore a method to influence or even control the diversity of opinions\nwithin a polarised social group. We leverage the voter model in which users\nhold binary opinions and repeatedly update their beliefs based on others they\nconnect with. Stubborn agents who never change their minds (\"zealots\") are also\ndisseminated through the network, which is modelled by a connected graph.\nBuilding on earlier results, we provide a closed-form expression for the\naverage opinion of the group at equilibrium. This leads us to a strategy to\ninject zealots into a polarised network in order to shift the average opinion\ntowards any target value. We account for the possible presence of a backfire\neffect, which may lead the group to react negatively and reinforce its level of\npolarisation in response. Our results are supported by numerical experiments on\nsynthetic data.",
          "link": "http://arxiv.org/abs/2006.07265",
          "publishedOn": "2022-01-08T00:37:49.114Z",
          "wordCount": 669,
          "title": "Towards control of opinion diversity by introducing zealots into a polarised social group. (arXiv:2006.07265v7 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02115",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Refinetti_M/0/1/0/all/0/1\">Maria Refinetti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Goldt_S/0/1/0/all/0/1\">Sebastian Goldt</a>",
          "description": "Autoencoders are the simplest neural network for unsupervised learning, and\nthus an ideal framework for studying feature learning. While a detailed\nunderstanding of the dynamics of linear autoencoders has recently been\nobtained, the study of non-linear autoencoders has been hindered by the\ntechnical difficulty of handling training data with non-trivial correlations -\na fundamental prerequisite for feature extraction. Here, we study the dynamics\nof feature learning in non-linear, shallow autoencoders. We derive a set of\nasymptotically exact equations that describe the generalisation dynamics of\nautoencoders trained with stochastic gradient descent (SGD) in the limit of\nhigh-dimensional inputs. These equations reveal that autoencoders learn the\nleading principal components of their inputs sequentially. An analysis of the\nlong-time dynamics explains the failure of sigmoidal autoencoders to learn with\ntied weights, and highlights the importance of training the bias in ReLU\nautoencoders. Building on previous results for linear networks, we analyse a\nmodification of the vanilla SGD algorithm which allows learning of the exact\nprincipal components. Finally, we show that our equations accurately describe\nthe generalisation dynamics of non-linear autoencoders on realistic datasets\nsuch as CIFAR10.",
          "link": "http://arxiv.org/abs/2201.02115",
          "publishedOn": "2022-01-08T00:37:48.995Z",
          "wordCount": 614,
          "title": "The dynamics of representation learning in shallow, non-linear autoencoders. (arXiv:2201.02115v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thopalli_K/0/1/0/all/0/1\">Kowshik Thopalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1\">Jayaraman J Thiagarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anirudh_R/0/1/0/all/0/1\">Rushil Anirudh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turaga_P/0/1/0/all/0/1\">Pavan K Turaga</a>",
          "description": "Unsupervised domain adaptation (UDA) aims to transfer and adapt knowledge\nfrom a labeled source domain to an unlabeled target domain. Traditionally,\nsubspace-based methods form an important class of solutions to this problem.\nDespite their mathematical elegance and tractability, these methods are often\nfound to be ineffective at producing domain-invariant features with complex,\nreal-world datasets. Motivated by the recent advances in representation\nlearning with deep networks, this paper revisits the use of subspace alignment\nfor UDA and proposes a novel adaptation algorithm that consistently leads to\nimproved generalization. In contrast to existing adversarial training-based DA\nmethods, our approach isolates feature learning and distribution alignment\nsteps, and utilizes a primary-auxiliary optimization strategy to effectively\nbalance the objectives of domain invariance and model fidelity. While providing\na significant reduction in target data and computational requirements, our\nsubspace-based DA performs competitively and sometimes even outperforms\nstate-of-the-art approaches on several standard UDA benchmarks. Furthermore,\nsubspace alignment leads to intrinsically well-regularized models that\ndemonstrate strong generalization even in the challenging partial DA setting.\nFinally, the design of our UDA framework inherently supports progressive\nadaptation to new target domains at test-time, without requiring retraining of\nthe model from scratch. In summary, powered by powerful feature learners and an\neffective optimization strategy, we establish subspace-based DA as a highly\neffective approach for visual recognition.",
          "link": "http://arxiv.org/abs/2201.01806",
          "publishedOn": "2022-01-08T00:37:48.968Z",
          "wordCount": 649,
          "title": "Revisiting Deep Subspace Alignment for Unsupervised Domain Adaptation. (arXiv:2201.01806v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devraj_A/0/1/0/all/0/1\">Adithya M. Devraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1\">Benjamin Van Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kuang Xu</a>",
          "description": "Assuming distributions are Gaussian often facilitates computations that are\notherwise intractable. We consider an agent who is designed to attain a low\ninformation ratio with respect to a bandit environment with a Gaussian prior\ndistribution and a Gaussian likelihood function, but study the agent's\nperformance when applied instead to a Bernoulli bandit. We establish a bound on\nthe increase in Bayesian regret when an agent interacts with the Bernoulli\nbandit, relative to an information-theoretic bound satisfied with the Gaussian\nbandit. If the Gaussian prior distribution and likelihood function are\nsufficiently diffuse, this increase grows with the square-root of the time\nhorizon, and thus the per-timestep increase vanishes. Our results formalize the\nfolklore that so-called Bayesian agents remain effective when instantiated with\ndiffuse misspecified distributions.",
          "link": "http://arxiv.org/abs/2201.01902",
          "publishedOn": "2022-01-08T00:37:48.868Z",
          "wordCount": 542,
          "title": "Gaussian Imagination in Bandit Learning. (arXiv:2201.01902v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Davier_M/0/1/0/all/0/1\">Matthias von Davier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyack_L/0/1/0/all/0/1\">Lillian Tyack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khorramdel_L/0/1/0/all/0/1\">Lale Khorramdel</a>",
          "description": "Automated scoring of free drawings or images as responses has yet to be\nutilized in large-scale assessments of student achievement. In this study, we\npropose artificial neural networks to classify these types of graphical\nresponses from a computer based international mathematics and science\nassessment. We are comparing classification accuracy of convolutional and\nfeedforward approaches. Our results show that convolutional neural networks\n(CNNs) outperform feedforward neural networks in both loss and accuracy. The\nCNN models classified up to 97.71% of the image responses into the appropriate\nscoring category, which is comparable to, if not more accurate, than typical\nhuman raters. These findings were further strengthened by the observation that\nthe most accurate CNN models correctly classified some image responses that had\nbeen incorrectly scored by the human raters. As an additional innovation, we\noutline a method to select human rated responses for the training sample based\non an application of the expected response function derived from item response\ntheory. This paper argues that CNN-based automated scoring of image responses\nis a highly accurate procedure that could potentially replace the workload and\ncost of second human raters for large scale assessments, while improving the\nvalidity and comparability of scoring complex constructed-response items.",
          "link": "http://arxiv.org/abs/2201.01783",
          "publishedOn": "2022-01-08T00:37:48.824Z",
          "wordCount": 640,
          "title": "Automated Scoring of Graphical Open-Ended Responses Using Artificial Neural Networks. (arXiv:2201.01783v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kopparapu_K/0/1/0/all/0/1\">Kavya Kopparapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duenez_Guzman_E/0/1/0/all/0/1\">Edgar A. Du&#xe9;&#xf1;ez-Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matyas_J/0/1/0/all/0/1\">Jayd Matyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vezhnevets_A/0/1/0/all/0/1\">Alexander Sasha Vezhnevets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agapiou_J/0/1/0/all/0/1\">John P. Agapiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKee_K/0/1/0/all/0/1\">Kevin R. McKee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Everett_R/0/1/0/all/0/1\">Richard Everett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marecki_J/0/1/0/all/0/1\">Janusz Marecki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leibo_J/0/1/0/all/0/1\">Joel Z. Leibo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graepel_T/0/1/0/all/0/1\">Thore Graepel</a>",
          "description": "A key challenge in the study of multiagent cooperation is the need for\nindividual agents not only to cooperate effectively, but to decide with whom to\ncooperate. This is particularly critical in situations when other agents have\nhidden, possibly misaligned motivations and goals. Social deduction games offer\nan avenue to study how individuals might learn to synthesize potentially\nunreliable information about others, and elucidate their true motivations. In\nthis work, we present Hidden Agenda, a two-team social deduction game that\nprovides a 2D environment for studying learning agents in scenarios of unknown\nteam alignment. The environment admits a rich set of strategies for both teams.\nReinforcement learning agents trained in Hidden Agenda show that agents can\nlearn a variety of behaviors, including partnering and voting without need for\ncommunication in natural language.",
          "link": "http://arxiv.org/abs/2201.01816",
          "publishedOn": "2022-01-08T00:37:48.726Z",
          "wordCount": 577,
          "title": "Hidden Agenda: a Social Deduction Game with Diverse Learned Equilibria. (arXiv:2201.01816v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boulch_A/0/1/0/all/0/1\">Alexandre Boulch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marlet_R/0/1/0/all/0/1\">Renaud Marlet</a>",
          "description": "Implicit neural networks have been successfully used for surface\nreconstruction from point clouds. However, many of them face scalability issues\nas they encode the isosurface function of a whole object or scene into a single\nlatent vector. To overcome this limitation, a few approaches infer latent\nvectors on a coarse regular 3D grid or on 3D patches, and interpolate them to\nanswer occupancy queries. In doing so, they loose the direct connection with\nthe input points sampled on the surface of objects, and they attach information\nuniformly in space rather than where it matters the most, i.e., near the\nsurface. Besides, relying on fixed patch sizes may require discretization\ntuning. To address these issues, we propose to use point cloud convolutions and\ncompute latent vectors at each input point. We then perform a learning-based\ninterpolation on nearest neighbors using inferred weights. Experiments on both\nobject and scene datasets show that our approach significantly outperforms\nother methods on most classical metrics, producing finer details and better\nreconstructing thinner volumes. The code is available at\nhttps://github.com/valeoai/POCO.",
          "link": "http://arxiv.org/abs/2201.01831",
          "publishedOn": "2022-01-08T00:37:48.650Z",
          "wordCount": 599,
          "title": "POCO: Point Convolution for Surface Reconstruction. (arXiv:2201.01831v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yongho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yongho Choi</a>",
          "description": "In this paper, we propose Five-point stencil CNN (FCNN) containing a\nfive-point stencil kernel and a trainable approximation function. We consider\nreaction-diffusion type equations including heat, Fisher's, Allen-Cahn\nequations, and reaction-diffusion equations with trigonometric functions. Our\nproposed FCNN is trained well using few data and then can predict\nreaction-diffusion evolutions with unseen initial conditions. Also, our FCNN is\ntrained well in the case of using noisy train data. We present various\nsimulation results to demonstrate that our proposed FCNN is working well.",
          "link": "http://arxiv.org/abs/2201.01854",
          "publishedOn": "2022-01-08T00:37:48.581Z",
          "wordCount": 511,
          "title": "FCNN: Five-point stencil CNN for solving reaction-diffusion equations. (arXiv:2201.01854v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10324",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Runge_J/0/1/0/all/0/1\">Jakob Runge</a>",
          "description": "The problem of selecting optimal backdoor adjustment sets to estimate causal\neffects in graphical models with hidden and conditioned variables is addressed.\nPrevious work has defined optimality as achieving the smallest asymptotic\nestimation variance and derived an optimal set for the case without hidden\nvariables. For the case with hidden variables there can be settings where no\noptimal set exists and currently only a sufficient graphical optimality\ncriterion of limited applicability has been derived. In the present work\noptimality is characterized as maximizing a certain adjustment information\nwhich allows to derive a necessary and sufficient graphical criterion for the\nexistence of an optimal adjustment set and a definition and algorithm to\nconstruct it. Further, the optimal set is valid if and only if a valid\nadjustment set exists and has higher (or equal) adjustment information than the\nAdjust-set proposed in Perkovi{\\'c} et al. [Journal of Machine Learning\nResearch, 18: 1--62, 2018] for any graph. The results translate to minimal\nasymptotic estimation variance for a class of estimators whose asymptotic\nvariance follows a certain information-theoretic relation. Numerical\nexperiments indicate that the asymptotic results also hold for relatively small\nsample sizes and that the optimal adjustment set or minimized variants thereof\noften yield better variance also beyond that estimator class. Surprisingly,\namong the randomly created setups more than 90\\% fulfill the optimality\nconditions indicating that also in many real-world scenarios graphical\noptimality may hold. Code is available as part of the python package\n\\url{https://github.com/jakobrunge/tigramite}.",
          "link": "http://arxiv.org/abs/2102.10324",
          "publishedOn": "2022-01-08T00:37:48.575Z",
          "wordCount": 734,
          "title": "Necessary and sufficient graphical conditions for optimal adjustment sets in causal graphical models with hidden variables. (arXiv:2102.10324v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.12710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Ting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>",
          "description": "This paper explores a better codebook for BERT pre-training of vision\ntransformers. The recent work BEiT successfully transfers BERT pre-training\nfrom NLP to the vision field. It directly adopts one simple discrete VAE as the\nvisual tokenizer, but has not considered the semantic level of the resulting\nvisual tokens. By contrast, the discrete tokens in NLP field are naturally\nhighly semantic. This difference motivates us to learn a perceptual codebook.\nAnd we surprisingly find one simple yet effective idea: enforcing perceptual\nsimilarity during the dVAE training. We demonstrate that the visual tokens\ngenerated by the proposed perceptual codebook do exhibit better semantic\nmeanings, and subsequently help pre-training achieve superior transfer\nperformance in various downstream tasks. For example, we achieve 84.5% Top-1\naccuracy on ImageNet-1K with ViT-B backbone, outperforming the competitive\nmethod BEiT by +1.3 with the same pre-training epochs. It can also improve the\nperformance of object detection and segmentation tasks on COCO val by +1.3 box\nAP and +1.0 mask AP, semantic segmentation on ADE20k by +1.0 mIoU. Equipped\nwith a larger backbone ViT-H, we achieve the state-of-the-art performance\n(88.3% Top-1 accuracy) among the methods using only ImageNet-1K data. The code\nand models will be available at https://github.com/microsoft/PeCo.",
          "link": "http://arxiv.org/abs/2111.12710",
          "publishedOn": "2022-01-08T00:37:48.568Z",
          "wordCount": 685,
          "title": "PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers. (arXiv:2111.12710v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sen_J/0/1/0/all/0/1\">Jaydip Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehtab_S/0/1/0/all/0/1\">Sidra Mehtab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_R/0/1/0/all/0/1\">Rajdeep Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1\">Abhishek Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kherwa_P/0/1/0/all/0/1\">Pooja Kherwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Saheel Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berry_P/0/1/0/all/0/1\">Pranay Berry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khurana_S/0/1/0/all/0/1\">Sahil Khurana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sonali Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadotte_D/0/1/0/all/0/1\">David W. W Cadotte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_D/0/1/0/all/0/1\">David W. Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ost_K/0/1/0/all/0/1\">Kalum J. Ost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akinbo_R/0/1/0/all/0/1\">Racheal S. Akinbo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daramola_O/0/1/0/all/0/1\">Oladunni A. Daramola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lainjo_B/0/1/0/all/0/1\">Bongs Lainjo</a>",
          "description": "Recent times are witnessing rapid development in machine learning algorithm\nsystems, especially in reinforcement learning, natural language processing,\ncomputer and robot vision, image processing, speech, and emotional processing\nand understanding. In tune with the increasing importance and relevance of\nmachine learning models, algorithms, and their applications, and with the\nemergence of more innovative uses cases of deep learning and artificial\nintelligence, the current volume presents a few innovative research works and\ntheir applications in real world, such as stock trading, medical and healthcare\nsystems, and software automation. The chapters in the book illustrate how\nmachine learning and deep learning algorithms and models are designed,\noptimized, and deployed. The volume will be useful for advanced graduate and\ndoctoral students, researchers, faculty members of universities, practicing\ndata scientists and data engineers, professionals, and consultants working on\nthe broad areas of machine learning, deep learning, and artificial\nintelligence.",
          "link": "http://arxiv.org/abs/2201.01943",
          "publishedOn": "2022-01-08T00:37:48.561Z",
          "wordCount": 608,
          "title": "Machine Learning: Algorithms, Models, and Applications. (arXiv:2201.01943v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02028",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Harl_M/0/1/0/all/0/1\">Maximilian Harl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herchenbach_M/0/1/0/all/0/1\">Marvin Herchenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruschel_S/0/1/0/all/0/1\">Sven Kruschel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hambauer_N/0/1/0/all/0/1\">Nico Hambauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zschech_P/0/1/0/all/0/1\">Patrick Zschech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraus_M/0/1/0/all/0/1\">Mathias Kraus</a>",
          "description": "In recent years, large pre-trained deep neural networks (DNNs) have\nrevolutionized the field of computer vision (CV). Although these DNNs have been\nshown to be very well suited for general image recognition tasks, application\nin industry is often precluded for three reasons: 1) large pre-trained DNNs are\nbuilt on hundreds of millions of parameters, making deployment on many devices\nimpossible, 2) the underlying dataset for pre-training consists of general\nobjects, while industrial cases often consist of very specific objects, such as\nstructures on solar wafers, 3) potentially biased pre-trained DNNs raise legal\nissues for companies. As a remedy, we study neural networks for CV that we\ntrain from scratch. For this purpose, we use a real-world case from a solar\nwafer manufacturer. We find that our neural networks achieve similar\nperformances as pre-trained DNNs, even though they consist of far fewer\nparameters and do not rely on third-party datasets.",
          "link": "http://arxiv.org/abs/2201.02028",
          "publishedOn": "2022-01-08T00:37:48.545Z",
          "wordCount": 612,
          "title": "A Light in the Dark: Deep Learning Practices for Industrial Computer Vision. (arXiv:2201.02028v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01836",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+GX_Chen_A/0/1/0/all/0/1\">Anthony GX-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chelu_V/0/1/0/all/0/1\">Veronica Chelu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richards_B/0/1/0/all/0/1\">Blake A. Richards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>",
          "description": "Estimating value functions is a core component of reinforcement learning\nalgorithms. Temporal difference (TD) learning algorithms use bootstrapping,\ni.e. they update the value function toward a learning target using value\nestimates at subsequent time-steps. Alternatively, the value function can be\nupdated toward a learning target constructed by separately predicting successor\nfeatures (SF)--a policy-dependent model--and linearly combining them with\ninstantaneous rewards. We focus on bootstrapping targets used when estimating\nvalue functions, and propose a new backup target, the $\\eta$-return mixture,\nwhich implicitly combines value-predictive knowledge (used by TD methods) with\n(successor) feature-predictive knowledge--with a parameter $\\eta$ capturing how\nmuch to rely on each. We illustrate that incorporating predictive knowledge\nthrough an $\\eta\\gamma$-discounted SF model makes more efficient use of sampled\nexperience, compared to either extreme, i.e. bootstrapping entirely on the\nvalue function estimate, or bootstrapping on the product of separately\nestimated successor features and instantaneous reward models. We empirically\nshow this approach leads to faster policy evaluation and better control\nperformance, for tabular and nonlinear function approximations, indicating\nscalability and generality.",
          "link": "http://arxiv.org/abs/2201.01836",
          "publishedOn": "2022-01-08T00:37:48.539Z",
          "wordCount": 610,
          "title": "A Generalized Bootstrap Target for Value-Learning, Efficiently Combining Value and Feature Predictions. (arXiv:2201.01836v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.11240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dowsley_R/0/1/0/all/0/1\">Rafael Dowsley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horst_C/0/1/0/all/0/1\">Caleb Horst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nascimento_A/0/1/0/all/0/1\">Anderson C. A. Nascimento</a>",
          "description": "We propose protocols for obliviously evaluating finite-state machines, i.e.,\nthe evaluation is shared between the provider of the finite-state machine and\nthe provider of the input string in such a manner that neither party learns the\nother's input, and the states being visited are hidden from both. For alphabet\nsize $|\\Sigma|$, number of states $|Q|$, and input length $n$, previous\nsolutions have either required a number of rounds linear in $n$ or\ncommunication $\\Omega(n|\\Sigma||Q|\\log|Q|)$. Our solutions require 2 rounds\nwith communication $O(n(|\\Sigma|+|Q|\\log|Q|))$. We present two different\nsolutions to this problem, a two-party one and a setting with an untrusted but\nnon-colluding helper.",
          "link": "http://arxiv.org/abs/2103.11240",
          "publishedOn": "2022-01-08T00:37:48.533Z",
          "wordCount": 564,
          "title": "Round and Communication Balanced Protocols for Oblivious Evaluation of Finite State Machines. (arXiv:2103.11240v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yong_Z/0/1/0/all/0/1\">Zheng-Xin Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watson_P/0/1/0/all/0/1\">Patrick D. Watson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torrent_T/0/1/0/all/0/1\">Tiago Timponi Torrent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czulo_O/0/1/0/all/0/1\">Oliver Czulo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baker_C/0/1/0/all/0/1\">Collin F. Baker</a>",
          "description": "Frame shift is a cross-linguistic phenomenon in translation which results in\ncorresponding pairs of linguistic material evoking different frames. The\nability to predict frame shifts enables automatic creation of multilingual\nFrameNets through annotation projection. Here, we propose the Frame Shift\nPrediction task and demonstrate that graph attention networks, combined with\nauxiliary training, can learn cross-linguistic frame-to-frame correspondence\nand predict frame shifts.",
          "link": "http://arxiv.org/abs/2201.01837",
          "publishedOn": "2022-01-08T00:37:48.527Z",
          "wordCount": 482,
          "title": "Frame Shift Prediction. (arXiv:2201.01837v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.06077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhijie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jialin Zhang</a>",
          "description": "We study the online influence maximization (OIM) problem in social networks,\nwhere in multiple rounds the learner repeatedly chooses seed nodes to generate\ncascades, observes the cascade feedback, and gradually learns the best seeds\nthat generate the largest cascade. We focus on two major challenges in this\npaper. First, we work with node-level feedback instead of edge-level feedback.\nThe edge-level feedback reveals all edges that pass through information in a\ncascade, where the node-level feedback only reveals the activated nodes with\ntimestamps. The node-level feedback is arguably more realistic since in\npractice it is relatively easy to observe who is influenced but very difficult\nto observe from which relationship (edge) the influence comes from. Second, we\nuse standard offline oracle instead of offline pair-oracle. To compute a good\nseed set for the next round, an offline pair-oracle finds the best seed set and\nthe best parameters within the confidence region simultaneously, and such an\noracle is difficult to compute due to the combinatorial core of OIM problem. So\nwe focus on how to use the standard offline influence maximization oracle which\nfinds the best seed set given the edge parameters as input. In this paper, we\nresolve these challenges for the two most popular diffusion models, the\nindependent cascade (IC) and the linear threshold (LT) model. For the IC model,\nthe past research only achieves edge-level feedback, while we present the first\n$\\widetilde{O}(\\sqrt{T})$-regret algorithm for the node-level feedback.\nBesides, the algorithm only invokes standard offline oracles. For the LT model,\na recent study only provides an OIM solution that meets the first challenge but\nstill requires a pair-oracle. In this paper, we apply a similar technique as in\nthe IC model to replace the pair-oracle with a standard oracle while\nmaintaining $\\widetilde{O}(\\sqrt{T})$-regret.",
          "link": "http://arxiv.org/abs/2109.06077",
          "publishedOn": "2022-01-08T00:37:48.518Z",
          "wordCount": 773,
          "title": "Online Influence Maximization with Node-level Feedback Using Standard Offline Oracles. (arXiv:2109.06077v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Minami_Y/0/1/0/all/0/1\">Yota Minami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaneiwa_K/0/1/0/all/0/1\">Ken Kaneiwa</a>",
          "description": "The Resource Description Framework (RDF) is a framework for describing\nmetadata, such as attributes and relationships of resources on the Web. Machine\nlearning tasks for RDF graphs adopt three methods: (i) support vector machines\n(SVMs) with RDF graph kernels, (ii) RDF graph embeddings, and (iii) relational\ngraph convolutional networks. In this paper, we propose a novel feature vector\n(called a Skip vector) that represents some features of each resource in an RDF\ngraph by extracting various combinations of neighboring edges and nodes. In\norder to make the Skip vector low-dimensional, we select important features for\nclassification tasks based on the information gain ratio of each feature. The\nclassification tasks can be performed by applying the low-dimensional Skip\nvector of each resource to conventional machine learning algorithms, such as\nSVMs, the k-nearest neighbors method, neural networks, random forests, and\nAdaBoost. In our evaluation experiments with RDF data, such as Wikidata,\nDBpedia, and YAGO, we compare our method with RDF graph kernels in an SVM. We\nalso compare our method with the two approaches: RDF graph embeddings such as\nRDF2vec and relational graph convolutional networks on the AIFB, MUTAG, BGS,\nand AM benchmarks.",
          "link": "http://arxiv.org/abs/2201.01996",
          "publishedOn": "2022-01-08T00:37:48.498Z",
          "wordCount": 615,
          "title": "Skip Vectors for RDF Data: Extraction Based on the Complexity of Feature Patterns. (arXiv:2201.01996v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pavlyshenko_B/0/1/0/all/0/1\">Bohdan M. Pavlyshenko</a>",
          "description": "The article describes the approaches for forming different predictive\nfeatures of tweet data sets and using them in the predictive analysis for\ndecision-making support. The graph theory as well as frequent itemsets and\nassociation rules theory is used for forming and retrieving different features\nfrom these datasests. The use of these approaches makes it possible to reveal a\nsemantic structure in tweets related to a specified entity. It is shown that\nquantitative characteristics of semantic frequent itemsets can be used in\npredictive regression models with specified target variables.",
          "link": "http://arxiv.org/abs/2201.02049",
          "publishedOn": "2022-01-08T00:37:48.317Z",
          "wordCount": 513,
          "title": "Forming Predictive Features of Tweets for Decision-Making Support. (arXiv:2201.02049v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khare_A/0/1/0/all/0/1\">Arpit Khare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangwar_A/0/1/0/all/0/1\">Amisha Gangwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sudhakar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1\">Shiv Prakash</a>",
          "description": "Social Media usage has increased to an all-time high level in today's digital\nworld. The majority of the population uses social media tools (like Twitter,\nFacebook, YouTube, etc.) to share their thoughts and experiences with the\ncommunity. Analysing the sentiments and opinions of the common public is very\nimportant for both the government and the business people. This is the reason\nbehind the activeness of many media agencies during the election time for\nperforming various kinds of opinion polls. In this paper, we have worked\ntowards analysing the sentiments of the people of India during the Lok Sabha\nelection of 2019 using the Twitter data of that duration. We have built an\nautomatic tweet analyser using the Transfer Learning technique to handle the\nunsupervised nature of this problem. We have used the Linear Support Vector\nClassifiers method in our Machine Learning model, also, the Term Frequency\nInverse Document Frequency (TF-IDF) methodology for handling the textual data\nof tweets. Further, we have increased the capability of the model to address\nthe sarcastic tweets posted by some of the users, which has not been yet\nconsidered by the researchers in this domain.",
          "link": "http://arxiv.org/abs/2201.02127",
          "publishedOn": "2022-01-08T00:37:48.311Z",
          "wordCount": 634,
          "title": "Sentiment Analysis and Sarcasm Detection of Indian General Election Tweets. (arXiv:2201.02127v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaoyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1\">Yiguang Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+E_W/0/1/0/all/0/1\">Weinan E</a>, <a href=\"http://arxiv.org/find/cs/1/au:+John_Z/0/1/0/all/0/1\">Zhi-Qin John</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianhan Zhang</a>",
          "description": "A deep learning-based model reduction (DeePMR) method for simplifying\nchemical kinetics is proposed and validated using high-temperature\nauto-ignitions, perfectly stirred reactors (PSR), and one-dimensional freely\npropagating flames of n-heptane/air mixtures. The mechanism reduction is\nmodeled as an optimization problem on Boolean space, where a Boolean vector,\neach entry corresponding to a species, represents a reduced mechanism. The\noptimization goal is to minimize the reduced mechanism size given the error\ntolerance of a group of pre-selected benchmark quantities. The key idea of the\nDeePMR is to employ a deep neural network (DNN) to formulate the objective\nfunction in the optimization problem. In order to explore high dimensional\nBoolean space efficiently, an iterative DNN-assisted data sampling and DNN\ntraining procedure are implemented. The results show that DNN-assistance\nimproves sampling efficiency significantly, selecting only $10^5$ samples out\nof $10^{34}$ possible samples for DNN to achieve sufficient accuracy. The\nresults demonstrate the capability of the DNN to recognize key species and\nreasonably predict reduced mechanism performance. The well-trained DNN\nguarantees the optimal reduced mechanism by solving an inverse optimization\nproblem. By comparing ignition delay times, laminar flame speeds, temperatures\nin PSRs, the resulting skeletal mechanism has fewer species (45 species) but\nthe same level of accuracy as the skeletal mechanism (56 species) obtained by\nthe Path Flux Analysis (PFA) method. In addition, the skeletal mechanism can be\nfurther reduced to 28 species if only considering atmospheric,\nnear-stoichiometric conditions (equivalence ratio between 0.6 and 1.2). The\nDeePMR provides an innovative way to perform model reduction and demonstrates\nthe great potential of data-driven methods in the combustion area.",
          "link": "http://arxiv.org/abs/2201.02025",
          "publishedOn": "2022-01-08T00:37:48.301Z",
          "wordCount": 694,
          "title": "A deep learning-based model reduction (DeePMR) method for simplifying chemical kinetics. (arXiv:2201.02025v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01644",
          "author": "<a href=\"http://arxiv.org/find/gr-qc/1/au:+He_Y/0/1/0/all/0/1\">Yang-Hui He</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Ipina_J/0/1/0/all/0/1\">Juan Manuel P&#xe9;rez Ipi&#xf1;a</a>",
          "description": "On the long-established classification problems in general relativity we take\na novel perspective by adopting fruitful techniques from machine learning and\nmodern data-science. In particular, we model Petrov's classification of\nspacetimes, and show that a feed-forward neural network can achieve high degree\nof success. We also show how data visualization techniques with dimensionality\nreduction can help analyze the underlying patterns in the structure of the\ndifferent types of spacetimes.",
          "link": "http://arxiv.org/abs/2201.01644",
          "publishedOn": "2022-01-08T00:37:48.284Z",
          "wordCount": 501,
          "title": "Machine-Learning the Classification of Spacetimes. (arXiv:2201.01644v1 [gr-qc] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02034",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pavlyshenko_B/0/1/0/all/0/1\">Bohdan M. Pavlyshenko</a>",
          "description": "The paper describes the use of Bayesian regression for building time series\nmodels and stacking different predictive models for time series. Using Bayesian\nregression for time series modeling with nonlinear trend was analyzed. This\napproach makes it possible to estimate an uncertainty of time series prediction\nand calculate value at risk characteristics. A hierarchical model for time\nseries using Bayesian regression has been considered. In this approach, one set\nof parameters is the same for all data samples, other parameters can be\ndifferent for different groups of data samples. Such an approach allows using\nthis model in the case of short historical data for specified time series, e.g.\nin the case of new stores or new products in the sales prediction problem. In\nthe study of predictive models stacking, the models ARIMA, Neural Network,\nRandom Forest, Extra Tree were used for the prediction on the first level of\nmodel ensemble. On the second level, time series predictions of these models on\nthe validation set were used for stacking by Bayesian regression. This approach\ngives distributions for regression coefficients of these models. It makes it\npossible to estimate the uncertainty contributed by each model to stacking\nresult. The information about these distributions allows us to select an\noptimal set of stacking models, taking into account the domain knowledge. The\nprobabilistic approach for stacking predictive models allows us to make risk\nassessment for the predictions that are important in a decision-making process.",
          "link": "http://arxiv.org/abs/2201.02034",
          "publishedOn": "2022-01-08T00:37:48.278Z",
          "wordCount": 678,
          "title": "Bayesian Regression Approach for Building and Stacking Predictive Models in Time Series Analytics. (arXiv:2201.02034v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02088",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaochen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jing Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiayi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenzhong Chen</a>",
          "description": "Traditional recommender systems aim to estimate a user's rating to an item\nbased on observed ratings from the population. As with all observational\nstudies, hidden confounders, which are factors that affect both item exposures\nand user ratings, lead to a systematic bias in the estimation. Consequently, a\nnew trend in recommender system research is to negate the influence of\nconfounders from a causal perspective. Observing that confounders in\nrecommendations are usually shared among items and are therefore multi-cause\nconfounders, we model the recommendation as a multi-cause multi-outcome (MCMO)\ninference problem. Specifically, to remedy confounding bias, we estimate\nuser-specific latent variables that render the item exposures independent\nBernoulli trials. The generative distribution is parameterized by a DNN with\nfactorized logistic likelihood and the intractable posteriors are estimated by\nvariational inference. Controlling these factors as substitute confounders,\nunder mild assumptions, can eliminate the bias incurred by multi-cause\nconfounders. Furthermore, we show that MCMO modeling may lead to high variance\ndue to scarce observations associated with the high-dimensional causal space.\nFortunately, we theoretically demonstrate that introducing user features as\npre-treatment variables can substantially improve sample efficiency and\nalleviate overfitting. Empirical studies on simulated and real-world datasets\nshow that the proposed deep causal recommender shows more robustness to\nunobserved confounders than state-of-the-art causal recommenders. Codes and\ndatasets are released at https://github.com/yaochenzhu/deep-deconf.",
          "link": "http://arxiv.org/abs/2201.02088",
          "publishedOn": "2022-01-08T00:37:48.268Z",
          "wordCount": 633,
          "title": "Deep Causal Reasoning for Recommendations. (arXiv:2201.02088v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Phung_T/0/1/0/all/0/1\">Tung Minh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cloos_J/0/1/0/all/0/1\">Jan Cloos</a>",
          "description": "This work presents our approach to train a neural network to detect\nhate-speech texts in Hindi and Bengali. We also explore how transfer learning\ncan be applied to learning these languages, given that they have the same\norigin and thus, are similar to some extend. Even though the whole experiment\nwas conducted with low computational power, the obtained result is comparable\nto the results of other, more expensive, models. Furthermore, since the\ntraining data in use is relatively small and the two languages are almost\nentirely unknown to us, this work can be generalized as an effort to demystify\nlost or alien languages that no human is capable of understanding.",
          "link": "http://arxiv.org/abs/2201.01997",
          "publishedOn": "2022-01-08T00:37:48.261Z",
          "wordCount": 542,
          "title": "An exploratory experiment on Hindi, Bengali hate-speech detection and transfer learning using neural networks. (arXiv:2201.01997v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hahn_T/0/1/0/all/0/1\">Tim von Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mechefske_C/0/1/0/all/0/1\">Chris K Mechefske</a>",
          "description": "Machine learning can be enhanced through the integration of external\nknowledge. This method, called knowledge informed machine learning, is also\napplicable within the field of Prognostics and Health Management (PHM). In this\npaper, the various methods of knowledge informed machine learning, from a PHM\ncontext, are reviewed with the goal of helping the reader understand the\ndomain. In addition, a knowledge informed machine learning technique is\ndemonstrated, using the common IMS and PRONOSTIA bearing data sets, for\nremaining useful life (RUL) prediction. Specifically, knowledge is garnered\nfrom the field of reliability engineering which is represented through the\nWeibull distribution. The knowledge is then integrated into a neural network\nthrough a novel Weibull-based loss function. A thorough statistical analysis of\nthe Weibull-based loss function is conducted, demonstrating the effectiveness\nof the method on the PRONOSTIA data set. However, the Weibull-based loss\nfunction is less effective on the IMS data set. The results, shortcomings, and\nbenefits of the approach are discussed in length. Finally, all the code is\npublicly available for the benefit of other researchers.",
          "link": "http://arxiv.org/abs/2201.01769",
          "publishedOn": "2022-01-08T00:37:48.253Z",
          "wordCount": 618,
          "title": "Knowledge Informed Machine Learning using a Weibull-based Loss Function. (arXiv:2201.01769v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pavlyshenko_B/0/1/0/all/0/1\">Bohdan M. Pavlyshenko</a>",
          "description": "The article describes the use of deep Q-learning models in the problems of\nsales time series analytics. In contrast to supervised machine learning which\nis a kind of passive learning using historical data, Q-learning is a kind of\nactive learning with goal to maximize a reward by optimal sequence of actions.\nModel free Q-learning approach for optimal pricing strategies and supply-demand\nproblems was considered in the work. The main idea of the study is to show that\nusing deep Q-learning approach in time series analytics, the sequence of\nactions can be optimized by maximizing the reward function when the environment\nfor learning agent interaction can be modeled using the parametric model and in\nthe case of using the model which is based on the historical data. In the\npricing optimizing case study environment was modeled using sales dependence on\nextras price and randomly simulated demand. In the pricing optimizing case\nstudy, the environment was modeled using sales dependence on extra price and\nrandomly simulated demand. In the supply-demand case study, it was proposed to\nuse historical demand time series for environment modeling, agent states were\nrepresented by promo actions, previous demand values and weekly seasonality\nfeatures. Obtained results show that using deep Q-learning, we can optimize the\ndecision making process for price optimization and supply-demand problems.\nEnvironment modeling using parametric models and historical data can be used\nfor the cold start of learning agent. On the next steps, after the cold start,\nthe trained agent can be used in real business environment.",
          "link": "http://arxiv.org/abs/2201.02058",
          "publishedOn": "2022-01-08T00:37:48.237Z",
          "wordCount": 666,
          "title": "Sales Time Series Analytics Using Deep Q-Learning. (arXiv:2201.02058v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01973",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chakraborty_S/0/1/0/all/0/1\">Saptarshi Chakraborty</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Paul_D/0/1/0/all/0/1\">Debolina Paul</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Das_S/0/1/0/all/0/1\">Swagatam Das</a>",
          "description": "The problem of linear predictions has been extensively studied for the past\ncentury under pretty generalized frameworks. Recent advances in the robust\nstatistics literature allow us to analyze robust versions of classical linear\nmodels through the prism of Median of Means (MoM). Combining these approaches\nin a piecemeal way might lead to ad-hoc procedures, and the restricted\ntheoretical conclusions that underpin each individual contribution may no\nlonger be valid. To meet these challenges coherently, in this study, we offer a\nunified robust framework that includes a broad variety of linear prediction\nproblems on a Hilbert space, coupled with a generic class of loss functions.\nNotably, we do not require any assumptions on the distribution of the outlying\ndata points ($\\mathcal{O}$) nor the compactness of the support of the inlying\nones ($\\mathcal{I}$). Under mild conditions on the dual norm, we show that for\nmisspecification level $\\epsilon$, these estimators achieve an error rate of\n$O(\\max\\left\\{|\\mathcal{O}|^{1/2}n^{-1/2}, |\\mathcal{I}|^{1/2}n^{-1}\n\\right\\}+\\epsilon)$, matching the best-known rates in literature. This rate is\nslightly slower than the classical rates of $O(n^{-1/2})$, indicating that we\nneed to pay a price in terms of error rates to obtain robust estimates.\nAdditionally, we show that this rate can be improved to achieve so-called\n``fast rates\" under additional assumptions.",
          "link": "http://arxiv.org/abs/2201.01973",
          "publishedOn": "2022-01-08T00:37:48.222Z",
          "wordCount": 640,
          "title": "Robust Linear Predictions: Analyses of Uniform Concentration, Fast Rates and Model Misspecification. (arXiv:2201.01973v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02141",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Er_S/0/1/0/all/0/1\">Siawpeng Er</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1\">Edward Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Minshuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hua Wang</a>",
          "description": "This paper presents a deep learning assisted synthesis approach for direct\nend-to-end generation of RF/mm-wave passive matching network with 3D EM\nstructures. Different from prior approaches that synthesize EM structures from\ntarget circuit component values and target topologies, our proposed approach\nachieves the direct synthesis of the passive network given the network topology\nfrom desired performance values as input. We showcase the proposed synthesis\nNeural Network (NN) model on an on-chip 1:1 transformer-based impedance\nmatching network. By leveraging parameter sharing, the synthesis NN model\nsuccessfully extracts relevant features from the input impedance and load\ncapacitors, and predict the transformer 3D EM geometry in a 45nm SOI process\nthat will match the standard 50$\\Omega$ load to the target input impedance\nwhile absorbing the two loading capacitors. As a proof-of-concept, several\nexample transformer geometries were synthesized, and verified in Ansys HFSS to\nprovide the desired input impedance.",
          "link": "http://arxiv.org/abs/2201.02141",
          "publishedOn": "2022-01-08T00:37:48.216Z",
          "wordCount": 610,
          "title": "Deep Learning Assisted End-to-End Synthesis of mm-Wave Passive Networks with 3D EM Structures: A Study on A Transformer-Based Matching Network. (arXiv:2201.02141v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07263",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Guo_Y/0/1/0/all/0/1\">Yongyi Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Coey_D/0/1/0/all/0/1\">Dominic Coey</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Konutgan_M/0/1/0/all/0/1\">Mikael Konutgan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_W/0/1/0/all/0/1\">Wenting Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schoener_C/0/1/0/all/0/1\">Chris Schoener</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Goldman_M/0/1/0/all/0/1\">Matt Goldman</a>",
          "description": "We consider the problem of variance reduction in randomized controlled\ntrials, through the use of covariates correlated with the outcome but\nindependent of the treatment. We propose a machine learning regression-adjusted\ntreatment effect estimator, which we call MLRATE. MLRATE uses machine learning\npredictors of the outcome to reduce estimator variance. It employs\ncross-fitting to avoid overfitting biases, and we prove consistency and\nasymptotic normality under general conditions. MLRATE is robust to poor\npredictions from the machine learning step: if the predictions are uncorrelated\nwith the outcomes, the estimator performs asymptotically no worse than the\nstandard difference-in-means estimator, while if predictions are highly\ncorrelated with outcomes, the efficiency gains are large. In A/A tests, for a\nset of 48 outcome metrics commonly monitored in Facebook experiments the\nestimator has over 70% lower variance than the simple difference-in-means\nestimator, and about 19% lower variance than the common univariate procedure\nwhich adjusts only for pre-experiment values of the outcome.",
          "link": "http://arxiv.org/abs/2106.07263",
          "publishedOn": "2022-01-08T00:37:48.201Z",
          "wordCount": 607,
          "title": "Machine Learning for Variance Reduction in Online Experiments. (arXiv:2106.07263v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi-Gang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whatmough_P/0/1/0/all/0/1\">Paul N. Whatmough</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuhao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattina_M/0/1/0/all/0/1\">Matthew Mattina</a>",
          "description": "Exploiting sparsity is a key technique in accelerating quantized\nconvolutional neural network (CNN) inference on mobile devices. Prior sparse\nCNN accelerators largely exploit un-structured sparsity and achieve significant\nspeedups. Due to the unbounded, largely unpredictable sparsity patterns,\nhowever, exploiting unstructured sparsity requires complicated hardware design\nwith significant energy and area overhead, which is particularly detrimental to\nmobile/IoT inference scenarios where energy and area efficiency are crucial. We\npropose to exploit structured sparsity, more specifically, Density Bound Block\n(DBB) sparsity for both weights and activations. DBB block tensors bound the\nmaximum number of non-zeros per block. DBB thus exposes statically predictable\nsparsity patterns that enable lean sparsity-exploiting hardware. We propose new\nhardware primitives to implement DBB sparsity for (static) weights and\n(dynamic) activations, respectively, with very low overheads. Building on top\nof the primitives, we describe S2TA, a systolic array-based CNN accelerator\nthat exploits joint weight and activation DBB sparsity and new dimensions of\ndata reuse unavailable on the traditional systolic array. S2TA in 16nm achieves\nmore than 2x speedup and energy reduction compared to a strong baseline of a\nsystolic array with zero-value clock gating, over five popular CNN benchmarks.\nCompared to two recent non-systolic sparse accelerators, Eyeriss v2 (65nm) and\nSparTen (45nm), S2TA in 65nm uses about 2.2x and 3.1x less energy per\ninference, respectively.",
          "link": "http://arxiv.org/abs/2107.07983",
          "publishedOn": "2022-01-08T00:37:48.183Z",
          "wordCount": 686,
          "title": "S2TA: Exploiting Structured Sparsity for Energy-Efficient Mobile CNN Acceleration. (arXiv:2107.07983v2 [cs.AR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jadbabaie_A/0/1/0/all/0/1\">Ali Jadbabaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makur_A/0/1/0/all/0/1\">Anuran Makur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1\">Devavrat Shah</a>",
          "description": "In this work, we study empirical risk minimization (ERM) within a federated\nlearning framework, where a central server minimizes an ERM objective function\nusing training data that is stored across $m$ clients. In this setting, the\nFederated Averaging (FedAve) algorithm is the staple for determining\n$\\epsilon$-approximate solutions to the ERM problem. Similar to standard\noptimization algorithms, the convergence analysis of FedAve only relies on\nsmoothness of the loss function in the optimization parameter. However, loss\nfunctions are often very smooth in the training data too. To exploit this\nadditional smoothness, we propose the Federated Low Rank Gradient Descent\n(FedLRGD) algorithm. Since smoothness in data induces an approximate low rank\nstructure on the loss function, our method first performs a few rounds of\ncommunication between the server and clients to learn weights that the server\ncan use to approximate clients' gradients. Then, our method solves the ERM\nproblem at the server using inexact gradient descent. To show that FedLRGD can\nhave superior performance to FedAve, we present a notion of federated oracle\ncomplexity as a counterpart to canonical oracle complexity. Under some\nassumptions on the loss function, e.g., strong convexity in parameter,\n$\\eta$-H\\\"older smoothness in data, etc., we prove that the federated oracle\ncomplexity of FedLRGD scales like $\\phi m(p/\\epsilon)^{\\Theta(d/\\eta)}$ and\nthat of FedAve scales like $\\phi m(p/\\epsilon)^{3/4}$ (neglecting sub-dominant\nfactors), where $\\phi\\gg 1$ is a \"communication-to-computation ratio,\" $p$ is\nthe parameter dimension, and $d$ is the data dimension. Then, we show that when\n$d$ is small and the loss function is sufficiently smooth in the data, FedLRGD\nbeats FedAve in federated oracle complexity. Finally, in the course of\nanalyzing FedLRGD, we also establish a result on low rank approximation of\nlatent variable models.",
          "link": "http://arxiv.org/abs/2201.01954",
          "publishedOn": "2022-01-08T00:37:48.176Z",
          "wordCount": 713,
          "title": "Federated Optimization of Smooth Loss Functions. (arXiv:2201.01954v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.12435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rahul Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shroff_N/0/1/0/all/0/1\">Ness B. Shroff</a>",
          "description": "We consider reinforcement learning (RL) in Markov Decision Processes in which\nan agent repeatedly interacts with an environment that is modeled by a\ncontrolled Markov process. At each time step $t$, it earns a reward, and also\nincurs a cost-vector consisting of $M$ costs. We design model-based RL\nalgorithms that maximize the cumulative reward earned over a time horizon of\n$T$ time-steps, while simultaneously ensuring that the average values of the\n$M$ cost expenditures are bounded by agent-specified thresholds\n$c^{ub}_i,i=1,2,\\ldots,M$.\n\nIn order to measure the performance of a reinforcement learning algorithm\nthat satisfies the average cost constraints, we define an $M+1$ dimensional\nregret vector that is composed of its reward regret, and $M$ cost regrets. The\nreward regret measures the sub-optimality in the cumulative reward, while the\n$i$-th component of the cost regret vector is the difference between its $i$-th\ncumulative cost expense and the expected cost expenditures $Tc^{ub}_i$.\n\nWe prove that the expected value of the regret vector of UCRL-CMDP, is\nupper-bounded as $\\tilde{O}\\left(T^{2\\slash 3}\\right)$, where $T$ is the time\nhorizon. We further show how to reduce the regret of a desired subset of the\n$M$ costs, at the expense of increasing the regrets of rewards and the\nremaining costs. To the best of our knowledge, ours is the only work that\nconsiders non-episodic RL under average cost constraints, and derive algorithms\nthat can~\\emph{tune the regret vector} according to the agent's requirements on\nits cost regrets.",
          "link": "http://arxiv.org/abs/2002.12435",
          "publishedOn": "2022-01-08T00:37:48.169Z",
          "wordCount": 719,
          "title": "Learning in Markov Decision Processes under Constraints. (arXiv:2002.12435v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01922",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Pengkai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhaowei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goncalves_L/0/1/0/all/0/1\">Luis Goncalves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahadevan_V/0/1/0/all/0/1\">Vijay Mahadevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We present Contrastive Neighborhood Alignment (CNA), a manifold learning\napproach to maintain the topology of learned features whereby data points that\nare mapped to nearby representations by the source (teacher) model are also\nmapped to neighbors by the target (student) model. The target model aims to\nmimic the local structure of the source representation space using a\ncontrastive loss. CNA is an unsupervised learning algorithm that does not\nrequire ground-truth labels for the individual samples. CNA is illustrated in\nthree scenarios: manifold learning, where the model maintains the local\ntopology of the original data in a dimension-reduced space; model distillation,\nwhere a small student model is trained to mimic a larger teacher; and legacy\nmodel update, where an older model is replaced by a more powerful one.\nExperiments show that CNA is able to capture the manifold in a high-dimensional\nspace and improves performance compared to the competing methods in their\ndomains.",
          "link": "http://arxiv.org/abs/2201.01922",
          "publishedOn": "2022-01-08T00:37:48.158Z",
          "wordCount": 578,
          "title": "Contrastive Neighborhood Alignment. (arXiv:2201.01922v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">He Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1\">Congyan Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Songhe Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yidong Li</a>",
          "description": "Differentiable solvers for the linear assignment problem (LAP) have attracted\nmuch research attention in recent years, which are usually embedded into\nlearning frameworks as components. However, previous algorithms, with or\nwithout learning strategies, usually suffer from the degradation of the\noptimality with the increment of the problem size. In this paper, we propose a\nlearnable linear assignment solver based on deep graph networks. Specifically,\nwe first transform the cost matrix to a bipartite graph and convert the\nassignment task to the problem of selecting reliable edges from the constructed\ngraph. Subsequently, a deep graph network is developed to aggregate and update\nthe features of nodes and edges. Finally, the network predicts a label for each\nedge that indicates the assignment relationship. The experimental results on a\nsynthetic dataset reveal that our method outperforms state-of-the-art baselines\nand achieves consistently high accuracy with the increment of the problem size.\nFurthermore, we also embed the proposed solver, in comparison with\nstate-of-the-art baseline solvers, into a popular multi-object tracking (MOT)\nframework to train the tracker in an end-to-end manner. The experimental\nresults on MOT benchmarks illustrate that the proposed LAP solver improves the\ntracker by the largest margin.",
          "link": "http://arxiv.org/abs/2201.02057",
          "publishedOn": "2022-01-08T00:37:48.152Z",
          "wordCount": 622,
          "title": "GLAN: A Graph-based Linear Assignment Network. (arXiv:2201.02057v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yulin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yuni Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kaifa Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiapu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mingquan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kai Zhou</a>",
          "description": "Graph-based Anomaly Detection (GAD) is becoming prevalent due to the powerful\nrepresentation abilities of graphs as well as recent advances in graph mining\ntechniques. These GAD tools, however, expose a new attacking surface,\nironically due to their unique advantage of being able to exploit the relations\namong data. That is, attackers now can manipulate those relations (i.e., the\nstructure of the graph) to allow some target nodes to evade detection. In this\npaper, we exploit this vulnerability by designing a new type of targeted\nstructural poisoning attacks to a representative regression-based GAD system\ntermed OddBall. Specially, we formulate the attack against OddBall as a\nbi-level optimization problem, where the key technical challenge is to\nefficiently solve the problem in a discrete domain. We propose a novel attack\nmethod termed BinarizedAttack based on gradient descent. Comparing to prior\narts, BinarizedAttack can better use the gradient information, making it\nparticularly suitable for solving combinatorial optimization problems.\nFurthermore, we investigate the attack transferability of BinarizedAttack by\nemploying it to attack other representation-learning-based GAD systems. Our\ncomprehensive experiments demonstrate that BinarizedAttack is very effective in\nenabling target nodes to evade graph-based anomaly detection tools with limited\nattackers' budget, and in the black-box transfer attack setting,\nBinarizedAttack is also tested effective and in particular, can significantly\nchange the node embeddings learned by the GAD systems. Our research thus opens\nthe door to studying a new type of attack against security analytic tools that\nrely on graph data.",
          "link": "http://arxiv.org/abs/2106.09989",
          "publishedOn": "2022-01-08T00:37:48.135Z",
          "wordCount": 729,
          "title": "BinarizedAttack: Structural Poisoning Attacks to Graph-based Anomaly Detection. (arXiv:2106.09989v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Power_A/0/1/0/all/0/1\">Alethea Power</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burda_Y/0/1/0/all/0/1\">Yuri Burda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edwards_H/0/1/0/all/0/1\">Harri Edwards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babuschkin_I/0/1/0/all/0/1\">Igor Babuschkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_V/0/1/0/all/0/1\">Vedant Misra</a>",
          "description": "In this paper we propose to study generalization of neural networks on small\nalgorithmically generated datasets. In this setting, questions about data\nefficiency, memorization, generalization, and speed of learning can be studied\nin great detail. In some situations we show that neural networks learn through\na process of \"grokking\" a pattern in the data, improving generalization\nperformance from random chance level to perfect generalization, and that this\nimprovement in generalization can happen well past the point of overfitting. We\nalso study generalization as a function of dataset size and find that smaller\ndatasets require increasing amounts of optimization for generalization. We\nargue that these datasets provide a fertile ground for studying a poorly\nunderstood aspect of deep learning: generalization of overparametrized neural\nnetworks beyond memorization of the finite training dataset.",
          "link": "http://arxiv.org/abs/2201.02177",
          "publishedOn": "2022-01-08T00:37:48.128Z",
          "wordCount": 559,
          "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets. (arXiv:2201.02177v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.01738",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bolland_A/0/1/0/all/0/1\">Adrien Bolland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boukas_I/0/1/0/all/0/1\">Ioannis Boukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_M/0/1/0/all/0/1\">Mathias Berger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_D/0/1/0/all/0/1\">Damien Ernst</a>",
          "description": "We consider the joint design and control of discrete-time stochastic\ndynamical systems over a finite time horizon. We formulate the problem as a\nmulti-step optimization problem under uncertainty seeking to identify a system\ndesign and a control policy that jointly maximize the expected sum of rewards\ncollected over the time horizon considered. The transition function, the reward\nfunction and the policy are all parametrized, assumed known and differentiable\nwith respect to their parameters. We then introduce a deep reinforcement\nlearning algorithm combining policy gradient methods with model-based\noptimization techniques to solve this problem. In essence, our algorithm\niteratively approximates the gradient of the expected return via Monte-Carlo\nsampling and automatic differentiation and takes projected gradient ascent\nsteps in the space of environment and policy parameters. This algorithm is\nreferred to as Direct Environment and Policy Search (DEPS). We assess the\nperformance of our algorithm in three environments concerned with the design\nand control of a mass-spring-damper system, a small-scale off-grid power system\nand a drone, respectively. In addition, our algorithm is benchmarked against a\nstate-of-the-art deep reinforcement learning algorithm used to tackle joint\ndesign and control problems. We show that DEPS performs at least as well or\nbetter in all three environments, consistently yielding solutions with higher\nreturns in fewer iterations. Finally, solutions produced by our algorithm are\nalso compared with solutions produced by an algorithm that does not jointly\noptimize environment and policy parameters, highlighting the fact that higher\nreturns can be achieved when joint optimization is performed.",
          "link": "http://arxiv.org/abs/2006.01738",
          "publishedOn": "2022-01-08T00:37:48.112Z",
          "wordCount": 732,
          "title": "Jointly Learning Environments and Control Policies with Projected Stochastic Gradient Ascent. (arXiv:2006.01738v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.04652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uehara_M/0/1/0/all/0/1\">Masatoshi Uehara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuezhou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wen Sun</a>",
          "description": "This work studies the question of Representation Learning in RL: how can we\nlearn a compact low-dimensional representation such that on top of the\nrepresentation we can perform RL procedures such as exploration and\nexploitation, in a sample efficient manner. We focus on the low-rank Markov\nDecision Processes (MDPs) where the transition dynamics correspond to a\nlow-rank transition matrix. Unlike prior works that assume the representation\nis known (e.g., linear MDPs), here we need to learn the representation for the\nlow-rank MDP. We study both the online RL and offline RL settings. For the\nonline setting, operating with the same computational oracles used in FLAMBE\n(Agarwal et.al), the state-of-art algorithm for learning representations in\nlow-rank MDPs, we propose an algorithm REP-UCB Upper Confidence Bound driven\nRepresentation learning for RL), which significantly improves the sample\ncomplexity from $\\widetilde{O}( A^9 d^7 / (\\epsilon^{10} (1-\\gamma)^{22}))$ for\nFLAMBE to $\\widetilde{O}( A^2 d^4 / (\\epsilon^2 (1-\\gamma)^{5}) )$ with $d$\nbeing the rank of the transition matrix (or dimension of the ground truth\nrepresentation), $A$ being the number of actions, and $\\gamma$ being the\ndiscounted factor. Notably, REP-UCB is simpler than FLAMBE, as it directly\nbalances the interplay between representation learning, exploration, and\nexploitation, while FLAMBE is an explore-then-commit style approach and has to\nperform reward-free exploration step-by-step forward in time. For the offline\nRL setting, we develop an algorithm that leverages pessimism to learn under a\npartial coverage condition: our algorithm is able to compete against any policy\nas long as it is covered by the offline distribution.",
          "link": "http://arxiv.org/abs/2110.04652",
          "publishedOn": "2022-01-08T00:37:48.105Z",
          "wordCount": 719,
          "title": "Representation Learning for Online and Offline RL in Low-rank MDPs. (arXiv:2110.04652v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.07612",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lyu_H/0/1/0/all/0/1\">Hanbaek Lyu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Strohmeier_C/0/1/0/all/0/1\">Christopher Strohmeier</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Needell_D/0/1/0/all/0/1\">Deanna Needell</a>",
          "description": "Online Tensor Factorization (OTF) is a fundamental tool in learning\nlow-dimensional interpretable features from streaming multi-modal data. While\nvarious algorithmic and theoretical aspects of OTF have been investigated\nrecently, a general convergence guarantee to stationary points of the objective\nfunction without any incoherence or sparsity assumptions is still lacking even\nfor the i.i.d. case. In this work, we introduce a novel algorithm that learns a\nCANDECOMP/PARAFAC (CP) basis from a given stream of tensor-valued data under\ngeneral constraints, including nonnegativity constraints that induce\ninterpretability of the learned CP basis. We prove that our algorithm converges\nalmost surely to the set of stationary points of the objective function under\nthe hypothesis that the sequence of data tensors is generated by an underlying\nMarkov chain. Our setting covers the classical i.i.d. case as well as a wide\nrange of application contexts including data streams generated by independent\nor MCMC sampling. Our result closes a gap between OTF and Online Matrix\nFactorization in global convergence analysis \\commHL{for CP-decompositions}.\nExperimentally, we show that our algorithm converges much faster than standard\nalgorithms for nonnegative tensor factorization tasks on both synthetic and\nreal-world data. Also, we demonstrate the utility of our algorithm on a diverse\nset of examples from an image, video, and time-series data, illustrating how\none may learn qualitatively different CP-dictionaries from the same tensor data\nby exploiting the tensor structure in multiple ways.",
          "link": "http://arxiv.org/abs/2009.07612",
          "publishedOn": "2022-01-08T00:37:48.060Z",
          "wordCount": 683,
          "title": "Online nonnegative CP-dictionary learning for Markovian data. (arXiv:2009.07612v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01869",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Fox_G/0/1/0/all/0/1\">Geoffrey Fox</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rundle_J/0/1/0/all/0/1\">John Rundle</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Donnellan_A/0/1/0/all/0/1\">Andrea Donnellan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Feng_B/0/1/0/all/0/1\">Bo Feng</a>",
          "description": "We review previous approaches to nowcasting earthquakes and introduce new\napproaches based on deep learning using three distinct models based on\nrecurrent neural networks and transformers. We discuss different choices for\nobservables and measures presenting promising initial results for a region of\nSouthern California from 1950-2020. Earthquake activity is predicted as a\nfunction of 0.1-degree spatial bins for time periods varying from two weeks to\nfour years. The overall quality is measured by the Nash Sutcliffe Efficiency\ncomparing the deviation of nowcast and observation with the variance over time\nin each spatial region. The software is available as open-source together with\nthe preprocessed data from the USGS.",
          "link": "http://arxiv.org/abs/2201.01869",
          "publishedOn": "2022-01-08T00:37:48.045Z",
          "wordCount": 518,
          "title": "Earthquake Nowcasting with Deep Learning. (arXiv:2201.01869v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16239",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Piotrowski_T/0/1/0/all/0/1\">Tomasz Piotrowski</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cavalcante_R/0/1/0/all/0/1\">Renato L. G. Cavalcante</a>",
          "description": "We derive conditions for the existence of fixed points of nonnegative neural\nnetworks, an important research objective to understand the behavior of neural\nnetworks in modern applications involving autoencoders and loop unrolling\ntechniques, among others. In particular, we show that neural networks with\nnonnegative inputs and nonnegative parameters can be recognized as monotonic\nand (weakly) scalable functions within the framework of nonlinear\nPerron-Frobenius theory. This fact enables us to derive conditions for the\nexistence of a nonempty fixed point set of the nonnegative neural networks, and\nthese conditions are weaker than those obtained recently using arguments in\nconvex analysis, which are typically based on the assumption of nonexpansivity\nof the activation functions. Furthermore, we prove that the shape of the fixed\npoint set of monotonic and weakly scalable neural networks is often an\ninterval, which degenerates to a point for the case of scalable networks. The\nchief results of this paper are verified in numerical simulations, where we\nconsider an autoencoder-type network that first compresses angular power\nspectra in massive MIMO systems, and, second, reconstruct the input spectra\nfrom the compressed signals.",
          "link": "http://arxiv.org/abs/2106.16239",
          "publishedOn": "2022-01-08T00:37:48.033Z",
          "wordCount": 634,
          "title": "Fixed points of nonnegative neural networks. (arXiv:2106.16239v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramanath_R/0/1/0/all/0/1\">Rohan Ramanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keerthi_S/0/1/0/all/0/1\">S. Sathiya Keerthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salomatin_K/0/1/0/all/0/1\">Konstantin Salomatin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_K/0/1/0/all/0/1\">Kinjal Basu</a>",
          "description": "We consider applications involving a large set of instances of projecting\npoints to polytopes. We develop an intuition guided by theoretical and\nempirical analysis to show that when these instances follow certain structures,\na large majority of the projections lie on vertices of the polytopes. To do\nthese projections efficiently we derive a vertex-oriented incremental algorithm\nto project a point onto any arbitrary polytope, as well as give specific\nalgorithms to cater to simplex projection and polytopes where the unit box is\ncut by planes. Such settings are especially useful in web-scale applications\nsuch as optimal matching or allocation problems. Several such problems in\ninternet marketplaces (e-commerce, ride-sharing, food delivery, professional\nservices, advertising, etc.), can be formulated as Linear Programs (LP) with\nsuch polytope constraints that require a projection step in the overall\noptimization process. We show that in the very recent work, the polytopic\nprojection is the most expensive step and our efficient projection algorithms\nhelp in gaining massive improvements in performance.",
          "link": "http://arxiv.org/abs/2103.05277",
          "publishedOn": "2022-01-08T00:37:47.759Z",
          "wordCount": null,
          "title": "Efficient Vertex-Oriented Polytopic Projection for Web-scale Applications. (arXiv:2103.05277v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.12100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rossolini_G/0/1/0/all/0/1\">Giulio Rossolini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biondi_A/0/1/0/all/0/1\">Alessandro Biondi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buttazzo_G/0/1/0/all/0/1\">Giorgio Buttazzo</a>",
          "description": "The great performance of machine learning algorithms and deep neural networks\nin several perception and control tasks is pushing the industry to adopt such\ntechnologies in safety-critical applications, as autonomous robots and\nself-driving vehicles. At present, however, several issues need to be solved to\nmake deep learning methods more trustworthy, predictable, safe, and secure\nagainst adversarial attacks. Although several methods have been proposed to\nimprove the trustworthiness of deep neural networks, most of them are tailored\nfor specific classes of adversarial examples, hence failing to detect other\ncorner cases or unsafe inputs that heavily deviate from the training samples.\nThis paper presents a lightweight monitoring architecture based on coverage\nparadigms to enhance the model robustness against different unsafe inputs. In\nparticular, four coverage analysis methods are proposed and tested in the\narchitecture for evaluating multiple detection logics. Experimental results\nshow that the proposed approach is effective in detecting both powerful\nadversarial examples and out-of-distribution inputs, introducing limited\nextra-execution time and memory requirements.",
          "link": "http://arxiv.org/abs/2101.12100",
          "publishedOn": "2022-01-08T00:37:47.758Z",
          "wordCount": null,
          "title": "Increasing the Confidence of Deep Neural Networks by Coverage Analysis. (arXiv:2101.12100v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dar_Y/0/1/0/all/0/1\">Yehuda Dar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard G. Baraniuk</a>",
          "description": "We study the transfer learning process between two linear regression\nproblems. An important and timely special case is when the regressors are\noverparameterized and perfectly interpolate their training data. We examine a\nparameter transfer mechanism whereby a subset of the parameters of the target\ntask solution are constrained to the values learned for a related source task.\nWe analytically characterize the generalization error of the target task in\nterms of the salient factors in the transfer learning architecture, i.e., the\nnumber of examples available, the number of (free) parameters in each of the\ntasks, the number of parameters transferred from the source to target task, and\nthe relation between the two tasks. Our non-asymptotic analysis shows that the\ngeneralization error of the target task follows a two-dimensional double\ndescent trend (with respect to the number of free parameters in each of the\ntasks) that is controlled by the transfer learning factors. Our analysis points\nto specific cases where the transfer of parameters is beneficial as a\nsubstitute for extra overparameterization (i.e., additional free parameters in\nthe target task). Specifically, we show that the usefulness of a transfer\nlearning setting is fragile and depends on a delicate interplay among the set\nof transferred parameters, the relation between the tasks, and the true\nsolution. We also demonstrate that overparameterized transfer learning is not\nnecessarily more beneficial when the source task is closer or identical to the\ntarget task.",
          "link": "http://arxiv.org/abs/2006.07002",
          "publishedOn": "2022-01-08T00:37:47.734Z",
          "wordCount": null,
          "title": "Double Double Descent: On Generalization Errors in Transfer Learning between Linear Regression Tasks. (arXiv:2006.07002v7 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hanchi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jingjing Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xianghua Xie</a>",
          "description": "Data privacy has become an increasingly important issue in Machine Learning\n(ML), where many approaches have been developed to tackle this challenge, e.g.\ncryptography (Homomorphic Encryption (HE), Differential Privacy (DP), etc.) and\ncollaborative training (Secure Multi-Party Computation (MPC), Distributed\nLearning and Federated Learning (FL)). These techniques have a particular focus\non data encryption or secure local computation. They transfer the intermediate\ninformation to the third party to compute the final result. Gradient exchanging\nis commonly considered to be a secure way of training a robust model\ncollaboratively in Deep Learning (DL). However, recent researches have\ndemonstrated that sensitive information can be recovered from the shared\ngradient. Generative Adversarial Network (GAN), in particular, has shown to be\neffective in recovering such information. However, GAN based techniques require\nadditional information, such as class labels which are generally unavailable\nfor privacy-preserved learning. In this paper, we show that, in the FL system,\nimage-based privacy data can be easily recovered in full from the shared\ngradient only via our proposed Generative Regression Neural Network (GRNN). We\nformulate the attack to be a regression problem and optimize two branches of\nthe generative model by minimizing the distance between gradients. We evaluate\nour method on several image classification tasks. The results illustrate that\nour proposed GRNN outperforms state-of-the-art methods with better stability,\nstronger robustness, and higher accuracy. It also has no convergence\nrequirement to the global FL model. Moreover, we demonstrate information\nleakage using face re-identification. Some defense strategies are also\ndiscussed in this work.",
          "link": "http://arxiv.org/abs/2105.00529",
          "publishedOn": "2022-01-08T00:37:47.734Z",
          "wordCount": null,
          "title": "GRNN: Generative Regression Neural Network -- A Data Leakage Attack for Federated Learning. (arXiv:2105.00529v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.10083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazzaglia_P/0/1/0/all/0/1\">Pietro Mazzaglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verbelen_T/0/1/0/all/0/1\">Tim Verbelen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhoedt_B/0/1/0/all/0/1\">Bart Dhoedt</a>",
          "description": "Active inference is a unifying theory for perception and action resting upon\nthe idea that the brain maintains an internal model of the world by minimizing\nfree energy. From a behavioral perspective, active inference agents can be seen\nas self-evidencing beings that act to fulfill their optimistic predictions,\nnamely preferred outcomes or goals. In contrast, reinforcement learning\nrequires human-designed rewards to accomplish any desired outcome. Although\nactive inference could provide a more natural self-supervised objective for\ncontrol, its applicability has been limited because of the shortcomings in\nscaling the approach to complex environments. In this work, we propose a\ncontrastive objective for active inference that strongly reduces the\ncomputational burden in learning the agent's generative model and planning\nfuture actions. Our method performs notably better than likelihood-based active\ninference in image-based tasks, while also being computationally cheaper and\neasier to train. We compare to reinforcement learning agents that have access\nto human-designed reward functions, showing that our approach closely matches\ntheir performance. Finally, we also show that contrastive methods perform\nsignificantly better in the case of distractors in the environment and that our\nmethod is able to generalize goals to variations in the background.",
          "link": "http://arxiv.org/abs/2110.10083",
          "publishedOn": "2022-01-08T00:37:47.727Z",
          "wordCount": null,
          "title": "Contrastive Active Inference. (arXiv:2110.10083v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.06763",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Chen_J/0/1/0/all/0/1\">Jiafeng Chen</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohong Chen</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Tamer_E/0/1/0/all/0/1\">Elie Tamer</a>",
          "description": "Artificial Neural Networks (ANNs) can be viewed as nonlinear sieves that can\napproximate complex functions of high dimensional variables more effectively\nthan linear sieves. We investigate the computational performance of various\nANNs in nonparametric instrumental variables (NPIV) models of moderately high\ndimensional covariates that are relevant to empirical economics. We present two\nefficient procedures for estimation and inference on a weighted average\nderivative (WAD): an orthogonalized plug-in with optimally-weighted sieve\nminimum distance (OP-OSMD) procedure and a sieve efficient score (ES)\nprocedure. Both estimators for WAD use ANN sieves to approximate the unknown\nNPIV function and are root-n asymptotically normal and first-order equivalent.\nWe provide a detailed practitioner's recipe for implementing both efficient\nprocedures. This involves the choice of tuning parameters for the unknown NPIV,\nthe conditional expectations and the optimal weighting function that are\npresent in both procedures but also the choice of tuning parameters for the\nunknown Riesz representer in the ES procedure. We compare their finite-sample\nperformances in various simulation designs that involve smooth NPIV function of\nup to 13 continuous covariates, different nonlinearities and covariate\ncorrelations. Some Monte Carlo findings include: 1) tuning and optimization are\nmore delicate in ANN estimation; 2) given proper tuning, both ANN estimators\nwith various architectures can perform well; 3) easier to tune ANN OP-OSMD\nestimators than ANN ES estimators; 4) stable inferences are more difficult to\nachieve with ANN (than spline) estimators; 5) there are gaps between current\nimplementations and approximation theories. Finally, we apply ANN NPIV to\nestimate average partial derivatives in two empirical demand examples with\nmultivariate covariates.",
          "link": "http://arxiv.org/abs/2110.06763",
          "publishedOn": "2022-01-08T00:37:47.725Z",
          "wordCount": null,
          "title": "Efficient Estimation in NPIV Models: A Comparison of Various Neural Networks-Based Estimators. (arXiv:2110.06763v3 [econ.EM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1\">Shvetank Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callahan_T/0/1/0/all/0/1\">Tim Callahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bushagour_J/0/1/0/all/0/1\">Joseph Bushagour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banbury_C/0/1/0/all/0/1\">Colby Banbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_A/0/1/0/all/0/1\">Alan V. Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warden_P/0/1/0/all/0/1\">Pete Warden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ansell_T/0/1/0/all/0/1\">Tim Ansell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1\">Vijay Janapa Reddi</a>",
          "description": "We present CFU Playground, a full-stack open-source framework that enables\nrapid and iterative design of machine learning (ML) accelerators for embedded\nML systems. Our toolchain tightly integrates open-source software, RTL\ngenerators, and FPGA tools for synthesis, place, and route. This full-stack\ndevelopment framework gives engineers access to explore bespoke architectures\nthat are customized and co-optimized for embedded ML. The rapid,\ndeploy-profile-optimization feedback loop lets ML hardware and software\ndevelopers achieve significant returns out of a relatively small investment in\ncustomization. Using CFU Playground's design loop, we show substantial speedups\n(55x-75x) and design space exploration between the CPU and accelerator.",
          "link": "http://arxiv.org/abs/2201.01863",
          "publishedOn": "2022-01-08T00:37:47.724Z",
          "wordCount": null,
          "title": "CFU Playground: Full-Stack Open-Source Framework for Tiny Machine Learning (tinyML) Acceleration on FPGAs. (arXiv:2201.01863v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alomar_A/0/1/0/all/0/1\">Abdullah Alomar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamadanian_P/0/1/0/all/0/1\">Pouya Hamadanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasr_Esfahany_A/0/1/0/all/0/1\">Arash Nasr-Esfahany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Anish Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alizadeh_M/0/1/0/all/0/1\">Mohammad Alizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1\">Devavrat Shah</a>",
          "description": "Evaluating the real-world performance of network protocols is challenging.\nRandomized control trials (RCT) are expensive and inaccessible to most\nresearchers, while expert-designed simulators fail to capture complex behaviors\nin real networks. We present CausalSim, a data-driven simulator for network\nprotocols that addresses this challenge. Learning network behavior from\nobservational data is complicated due to the bias introduced by the protocols\nused during data collection. CausalSim uses traces from an initial RCT under a\nset of protocols to learn a causal network model, effectively removing the\nbiases present in the data. Using this model, CausalSim can then simulate any\nprotocol over the same traces (i.e., for counterfactual predictions). Key to\nCausalSim is the novel use of adversarial neural network training that exploits\ndistributional invariances that are present due to the training data coming\nfrom an RCT. Our extensive evaluation of CausalSim on both real and synthetic\ndatasets and two use cases, including more than nine months of real data from\nthe Puffer video streaming system, shows that it provides accurate\ncounterfactual predictions, reducing prediction error by 44% and 53% on average\ncompared to expert-designed and standard supervised learning baselines.",
          "link": "http://arxiv.org/abs/2201.01811",
          "publishedOn": "2022-01-08T00:37:47.697Z",
          "wordCount": 626,
          "title": "CausalSim: Toward a Causal Data-Driven Simulator for Network Protocols. (arXiv:2201.01811v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02185",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banihashem_K/0/1/0/all/0/1\">Kiarash Banihashem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1\">Adish Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_J/0/1/0/all/0/1\">Jiarui Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radanovic_G/0/1/0/all/0/1\">Goran Radanovic</a>",
          "description": "We study reward design strategies for incentivizing a reinforcement learning\nagent to adopt a policy from a set of admissible policies. The goal of the\nreward designer is to modify the underlying reward function cost-efficiently\nwhile ensuring that any approximately optimal deterministic policy under the\nnew reward function is admissible and performs well under the original reward\nfunction. This problem can be viewed as a dual to the problem of optimal reward\npoisoning attacks: instead of forcing an agent to adopt a specific policy, the\nreward designer incentivizes an agent to avoid taking actions that are\ninadmissible in certain states. Perhaps surprisingly, and in contrast to the\nproblem of optimal reward poisoning attacks, we first show that the reward\ndesign problem for admissible policy teaching is computationally challenging,\nand it is NP-hard to find an approximately optimal reward modification. We then\nproceed by formulating a surrogate problem whose optimal solution approximates\nthe optimal solution to the reward design problem in our setting, but is more\namenable to optimization techniques and analysis. For this surrogate problem,\nwe present characterization results that provide bounds on the value of the\noptimal solution. Finally, we design a local search algorithm to solve the\nsurrogate problem and showcase its utility using simulation-based experiments.",
          "link": "http://arxiv.org/abs/2201.02185",
          "publishedOn": "2022-01-08T00:37:47.690Z",
          "wordCount": 625,
          "title": "Admissible Policy Teaching through Reward Design. (arXiv:2201.02185v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Levinas_I/0/1/0/all/0/1\">Itay Levinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Louzoun_Y/0/1/0/all/0/1\">Yoram Louzoun</a>",
          "description": "Multiple methods of finding the vertices belonging to a planted dense\nsubgraph in a random dense $G(n, p)$ graph have been proposed, with an emphasis\non planted cliques. Such methods can identify the planted subgraph in\npolynomial time, but are all limited to several subgraph structures. Here, we\npresent PYGON, a graph neural network-based algorithm, which is insensitive to\nthe structure of the planted subgraph. This is the first algorithm that uses\nadvanced learning tools for recovering dense subgraphs. We show that PYGON can\nrecover cliques of sizes $\\Theta\\left(\\sqrt{n}\\right)$, where $n$ is the size\nof the background graph, comparable with the state of the art. We also show\nthat the same algorithm can recover multiple other planted subgraphs of size\n$\\Theta\\left(\\sqrt{n}\\right)$, in both directed and undirected graphs. We\nsuggest a conjecture that no polynomial time PAC-learning algorithm can detect\nplanted dense subgraphs with size smaller than $O\\left(\\sqrt{n}\\right)$, even\nif in principle one could find dense subgraphs of logarithmic size.",
          "link": "http://arxiv.org/abs/2201.01825",
          "publishedOn": "2022-01-08T00:37:47.684Z",
          "wordCount": 587,
          "title": "Planted Dense Subgraphs in Dense Random Graphs Can Be Recovered using Graph-based Machine Learning. (arXiv:2201.01825v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01855",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_X/0/1/0/all/0/1\">XU Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhao_H/0/1/0/all/0/1\">Huan Zhao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+TU_W/0/1/0/all/0/1\">Weiwei TU</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bo_X/0/1/0/all/0/1\">Xiaochen Bo</a>",
          "description": "Double-strand DNA breaks (DSBs) are a form of DNA damage that can cause\nabnormal chromosomal rearrangements. Recent technologies based on\nhigh-throughput experiments have obvious high costs and technical\nchallenges.Therefore, we design a graph neural network based method to predict\nDSBs (GraphDSB), using DNA sequence features and chromosome structure\ninformation. In order to improve the expression ability of the model, we\nintroduce Jumping Knowledge architecture and several effective structural\nencoding methods. The contribution of structural information to the prediction\nof DSBs is verified by the experiments on datasets from normal human epidermal\nkeratinocytes (NHEK) and chronic myeloid leukemia cell line (K562), and the\nablation studies further demonstrate the effectiveness of the designed\ncomponents in the proposed GraphDSB framework. Finally, we use GNNExplainer to\nanalyze the contribution of node features and topology to DSBs prediction, and\nproved the high contribution of 5-mer DNA sequence features and two chromatin\ninteraction modes.",
          "link": "http://arxiv.org/abs/2201.01855",
          "publishedOn": "2022-01-08T00:37:47.678Z",
          "wordCount": 577,
          "title": "Graph Neural Networks for Double-Strand DNA Breaks Prediction. (arXiv:2201.01855v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Robberechts_P/0/1/0/all/0/1\">Pieter Robberechts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meert_W/0/1/0/all/0/1\">Wannes Meert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">Jesse Davis</a>",
          "description": "Analyzing numerous or long time series is difficult in practice due to the\nhigh storage costs and computational requirements. Therefore, techniques have\nbeen proposed to generate compact similarity-preserving representations of time\nseries, enabling real-time similarity search on large in-memory data\ncollections. However, the existing techniques are not ideally suited for\nassessing similarity when sequences are locally out of phase. In this paper, we\npropose the use of product quantization for efficient similarity-based\ncomparison of time series under time warping. The idea is to first compress the\ndata by partitioning the time series into equal length sub-sequences which are\nrepresented by a short code. The distance between two time series can then be\nefficiently approximated by pre-computed elastic distances between their codes.\nThe partitioning into sub-sequences forces unwanted alignments, which we\naddress with a pre-alignment step using the maximal overlap discrete wavelet\ntransform (MODWT). To demonstrate the efficiency and accuracy of our method, we\nperform an extensive experimental evaluation on benchmark datasets in nearest\nneighbors classification and clustering applications. Overall, the proposed\nsolution emerges as a highly efficient (both in terms of memory usage and\ncomputation time) replacement for elastic measures in time series applications.",
          "link": "http://arxiv.org/abs/2201.01856",
          "publishedOn": "2022-01-08T00:37:47.665Z",
          "wordCount": 607,
          "title": "Elastic Product Quantization for Time Series. (arXiv:2201.01856v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2110.07557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhemin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongxia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bao Wang</a>",
          "description": "The explicit low-rank regularization, e.g., nuclear norm regularization, has\nbeen widely used in imaging sciences. However, it has been found that implicit\nregularization outperforms explicit ones in various image processing tasks.\nAnother issue is that the fixed explicit regularization limits the\napplicability to broad kinds of images since different images favor different\nfeatures captured by using different explicit regularizations. As such, this\npaper proposes a new adaptive and implicit low-rank regularization that\ncaptures the low-rank prior dynamically from the training data. At the core of\nour new adaptive and implicit low-rank regularization is parameterizing the\nLaplacian matrix in the Dirichlet energy-based regularization with a neural\nnetwork, and we call the proposed model \\textit{AIR-Net}. Theoretically, we\nshow that the adaptive regularization of AIR-Net enhances the implicit\nregularization and vanishes at the end of training. We validate AIR-Net's\neffectiveness on various benchmark tasks, indicating that the AIR-Net is\nparticularly favorable for the scenarios when the missing entries are\nnon-uniform. The code can be found at\n\\href{https://github.com/lizhemin15/AIR-Net}{https://github.com/lizhemin15/AIR-Net}.",
          "link": "http://arxiv.org/abs/2110.07557",
          "publishedOn": "2022-01-08T00:37:47.645Z",
          "wordCount": 628,
          "title": "AIR-Net: Adaptive and Implicit Regularization Neural Network for Matrix Completion. (arXiv:2110.07557v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dash_S/0/1/0/all/0/1\">Saloni Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Amit Sharma</a>",
          "description": "Counterfactual examples for an input -- perturbations that change specific\nfeatures but not others -- have been shown to be useful for evaluating bias of\nmachine learning models, e.g., against specific demographic groups. However,\ngenerating counterfactual examples for images is non-trivial due to the\nunderlying causal structure on the various features of an image. To be\nmeaningful, generated perturbations need to satisfy constraints implied by the\ncausal model. We present a method for generating counterfactuals by\nincorporating a structural causal model (SCM) in an improved variant of\nAdversarially Learned Inference (ALI), that generates counterfactuals in\naccordance with the causal relationships between attributes of an image. Based\non the generated counterfactuals, we show how to explain a pre-trained machine\nlearning classifier, evaluate its bias, and mitigate the bias using a\ncounterfactual regularizer. On the Morpho-MNIST dataset, our method generates\ncounterfactuals comparable in quality to prior work on SCM-based\ncounterfactuals (DeepSCM), while on the more complex CelebA dataset our method\noutperforms DeepSCM in generating high-quality valid counterfactuals. Moreover,\ngenerated counterfactuals are indistinguishable from reconstructed images in a\nhuman evaluation experiment and we subsequently use them to evaluate the\nfairness of a standard classifier trained on CelebA data. We show that the\nclassifier is biased w.r.t. skin and hair color, and how counterfactual\nregularization can remove those biases.",
          "link": "http://arxiv.org/abs/2009.08270",
          "publishedOn": "2022-01-08T00:37:47.637Z",
          "wordCount": 699,
          "title": "Evaluating and Mitigating Bias in Image Classifiers: A Causal Perspective Using Counterfactuals. (arXiv:2009.08270v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.14053",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Cava_J/0/1/0/all/0/1\">John Kevin Cava</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Vant_J/0/1/0/all/0/1\">John Vant</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ho_N/0/1/0/all/0/1\">Nicholas Ho</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Shukla_A/0/1/0/all/0/1\">Ankita Shukla</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Turaga_P/0/1/0/all/0/1\">Pavan Turaga</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Maciejewski_R/0/1/0/all/0/1\">Ross Maciejewski</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Singharoy_A/0/1/0/all/0/1\">Abhishek Singharoy</a>",
          "description": "In this paper, we utilized generative models, and reformulate it for problems\nin molecular dynamics (MD) simulation, by introducing an MD potential energy\ncomponent to our generative model. By incorporating potential energy as\ncalculated from TorchMD into a conditional generative framework, we attempt to\nconstruct a low-potential energy route of transformation between the\nhelix~$\\rightarrow$~coil structures of a protein. We show how to add an\nadditional loss function to conditional generative models, motivated by\npotential energy of molecular configurations, and also present an optimization\ntechnique for such an augmented loss function. Our results show the benefit of\nthis additional loss term on synthesizing realistic molecular trajectories.",
          "link": "http://arxiv.org/abs/2111.14053",
          "publishedOn": "2022-01-08T00:37:47.637Z",
          "wordCount": null,
          "title": "Towards Conditional Generation of Minimal Action Potential Pathways for Molecular Dynamics. (arXiv:2111.14053v2 [q-bio.BM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xishuang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1\">Lijun Qian</a>",
          "description": "Social media has become an effective platform to generate and spread fake\nnews that can mislead people and even distort public opinion. Centralized\nmethods for fake news detection, however, cannot effectively protect user\nprivacy during the process of centralized data collection for training models.\nMoreover, it cannot fully involve user feedback in the loop of learning\ndetection models for further enhancing fake news detection. To overcome these\nchallenges, this paper proposed a novel decentralized method, Human-in-the-loop\nBased Swarm Learning (HBSL), to integrate user feedback into the loop of\nlearning and inference for recognizing fake news without violating user privacy\nin a decentralized manner. It consists of distributed nodes that are able to\nindependently learn and detect fake news on local data. Furthermore, detection\nmodels trained on these nodes can be enhanced through decentralized model\nmerging. Experimental results demonstrate that the proposed method outperforms\nthe state-of-the-art decentralized method in regard of detecting fake news on a\nbenchmark dataset.",
          "link": "http://arxiv.org/abs/2201.02048",
          "publishedOn": "2022-01-08T00:37:47.631Z",
          "wordCount": 584,
          "title": "Integrating Human-in-the-loop into Swarm Learning for Decentralized Fake News Detection. (arXiv:2201.02048v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maurera_F/0/1/0/all/0/1\">Fernando Benjam&#xed;n P&#xe9;rez Maurera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dacrema_M/0/1/0/all/0/1\">Maurizio Ferrari Dacrema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremonesi_P/0/1/0/all/0/1\">Paolo Cremonesi</a>",
          "description": "This work explores the reproducibility of CFGAN. CFGAN and its family of\nmodels (TagRec, MTPR, and CRGAN) learn to generate personalized and\nfake-but-realistic rankings of preferences for top-N recommendations by using\nprevious interactions. This work successfully replicates the results published\nin the original paper and discusses the impact of certain differences between\nthe CFGAN framework and the model used in the original evaluation. The absence\nof random noise and the use of real user profiles as condition vectors leaves\nthe generator prone to learn a degenerate solution in which the output vector\nis identical to the input vector, therefore, behaving essentially as a simple\nautoencoder. The work further expands the experimental analysis comparing CFGAN\nagainst a selection of simple and well-known properly optimized baselines,\nobserving that CFGAN is not consistently competitive against them despite its\nhigh computational cost. To ensure the reproducibility of these analyses, this\nwork describes the experimental methodology and publishes all datasets and\nsource code.",
          "link": "http://arxiv.org/abs/2201.01815",
          "publishedOn": "2022-01-08T00:37:47.623Z",
          "wordCount": 586,
          "title": "An Evaluation Study of Generative Adversarial Networks for Collaborative Filtering. (arXiv:2201.01815v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gontier_N/0/1/0/all/0/1\">Nicolas Gontier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>",
          "description": "Pre-trained language models (LMs) often struggle to reason logically or\ngeneralize in a compositional fashion. Recent work suggests that incorporating\nexternal entity knowledge can improve LMs' abilities to reason and generalize.\nHowever, the effect of explicitly providing entity abstraction remains unclear,\nespecially with recent studies suggesting that pre-trained LMs already encode\nsome of that knowledge in their parameters. We study the utility of\nincorporating entity type abstractions into pre-trained Transformers and test\nthese methods on four NLP tasks requiring different forms of logical reasoning:\n(1) compositional language understanding with text-based relational reasoning\n(CLUTRR), (2) abductive reasoning (ProofWriter), (3) multi-hop question\nanswering (HotpotQA), and (4) conversational question answering (CoQA). We\npropose and empirically explore three ways to add such abstraction: (i) as\nadditional input embeddings, (ii) as a separate sequence to encode, and (iii)\nas an auxiliary prediction task for the model. Overall, our analysis\ndemonstrates that models with abstract entity knowledge performs better than\nwithout it. However, our experiments also show that the benefits strongly\ndepend on the technique used and the task at hand. The best abstraction aware\nmodels achieved an overall accuracy of 88.8% and 91.8% compared to the baseline\nmodel achieving 62.3% and 89.8% on CLUTRR and ProofWriter respectively. In\naddition, abstraction-aware models showed improved compositional generalization\nin both interpolation and extrapolation settings. However, for HotpotQA and\nCoQA, we find that F1 scores improve by only 0.5% on average. Our results\nsuggest that the benefit of explicit abstraction is significant in formally\ndefined logical reasoning settings requiring many reasoning hops, but point to\nthe notion that it is less beneficial for NLP tasks having less formal logical\nstructure.",
          "link": "http://arxiv.org/abs/2201.01787",
          "publishedOn": "2022-01-08T00:37:47.616Z",
          "wordCount": null,
          "title": "Does entity abstraction help generative Transformers reason?. (arXiv:2201.01787v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.07258",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1\">Andrew White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "Chemistry research has both high material and computational costs to conduct\nexperiments. Institutions thus consider chemical data to be valuable and there\nhave been few efforts to construct large public datasets for machine learning.\nAnother challenge is that different intuitions are interested in different\nclasses of molecules, creating heterogeneous data that cannot be easily joined\nby conventional distributed training. In this work, we introduce federated\nheterogeneous molecular learning to address these challenges. Federated\nlearning allows end-users to build a global model collaboratively while keeping\nthe training data distributed over isolated clients. Due to the lack of related\nresearch, we first simulate a heterogeneous federated learning benchmark\n(FedChem) by jointly performing scaffold splitting and latent Dirichlet\nallocation on existing datasets for heterogeneously distributed client data.\nOur results on FedChem show that significant learning challenges arise when\nworking with heterogeneous molecules across clients. We then propose a method\nto alleviate the problem, namely Federated Learning by Instance reweighTing\n(FLIT(+)). FLIT(+) can align the local training across heterogeneous clients by\nimproving the performance for uncertain samples. Comprehensive experiments\nconducted on our new benchmark FedChem validate the advantages of this method\nover other federated learning schemes. FedChem should enable a new type of\ncollaboration for improving AI in chemistry that mitigates concerns about\nvaluable chemical data.",
          "link": "http://arxiv.org/abs/2109.07258",
          "publishedOn": "2022-01-08T00:37:47.616Z",
          "wordCount": 669,
          "title": "Federated Learning of Molecular Properties with Graph Neural Networks in a Heterogeneous Setting. (arXiv:2109.07258v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01771",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Desblancs_D/0/1/0/all/0/1\">Dorian Desblancs</a>",
          "description": "Annotating musical beats is a very long in tedious process. In order to\ncombat this problem, we present a new self-supervised learning pretext task for\nbeat tracking and downbeat estimation. This task makes use of Spleeter, an\naudio source separation model, to separate a song's drums from the rest of its\nsignal. The first set of signals are used as positives, and by extension\nnegatives, for contrastive learning pre-training. The drum-less signals, on the\nother hand, are used as anchors. When pre-training a fully-convolutional and\nrecurrent model using this pretext task, an onset function is learned. In some\ncases, this function was found to be mapped to periodic elements in a song. We\nfound that pre-trained models outperformed randomly initialized models when a\nbeat tracking training set was extremely small (less than 10 examples). When\nthat was not the case, pre-training led to a learning speed-up that caused the\nmodel to overfit to the training set. More generally, this work defines new\nperspectives in the realm of musical self-supervised learning. It is notably\none of the first works to use audio source separation as a fundamental\ncomponent of self-supervision.",
          "link": "http://arxiv.org/abs/2201.01771",
          "publishedOn": "2022-01-08T00:37:47.608Z",
          "wordCount": 631,
          "title": "Self-Supervised Beat Tracking in Musical Signals with Polyphonic Contrastive Learning. (arXiv:2201.01771v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ostrovsky_M/0/1/0/all/0/1\">Matan Ostrovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrett_C/0/1/0/all/0/1\">Clark Barrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_G/0/1/0/all/0/1\">Guy Katz</a>",
          "description": "Convolutional neural networks have gained vast popularity due to their\nexcellent performance in the fields of computer vision, image processing, and\nothers. Unfortunately, it is now well known that convolutional networks often\nproduce erroneous results - for example, minor perturbations of the inputs of\nthese networks can result in severe classification errors. Numerous\nverification approaches have been proposed in recent years to prove the absence\nof such errors, but these are typically geared for fully connected networks and\nsuffer from exacerbated scalability issues when applied to convolutional\nnetworks. To address this gap, we present here the Cnn-Abs framework, which is\nparticularly aimed at the verification of convolutional networks. The core of\nCnn-Abs is an abstraction-refinement technique, which simplifies the\nverification problem through the removal of convolutional connections in a way\nthat soundly creates an over-approximation of the original problem; and which\nrestores these connections if the resulting problem becomes too abstract.\nCnn-Abs is designed to use existing verification engines as a backend, and our\nevaluation demonstrates that it can significantly boost the performance of a\nstate-of-the-art DNN verification engine, reducing runtime by 15.7% on average.",
          "link": "http://arxiv.org/abs/2201.01978",
          "publishedOn": "2022-01-08T00:37:47.590Z",
          "wordCount": 616,
          "title": "An Abstraction-Refinement Approach to Verifying Convolutional Neural Networks. (arXiv:2201.01978v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mirajul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ria_N/0/1/0/all/0/1\">Nushrat Jahan Ria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ani_J/0/1/0/all/0/1\">Jannatul Ferdous Ani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masum_A/0/1/0/all/0/1\">Abu Kaisar Mohammad Masum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abujar_S/0/1/0/all/0/1\">Sheikh Abujar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_S/0/1/0/all/0/1\">Syed Akhter Hossain</a>",
          "description": "A deep learning model gives an incredible result for image processing by\nstudying from the trained dataset. Spinach is a leaf vegetable that contains\nvitamins and nutrients. In our research, a Deep learning method has been used\nthat can automatically identify spinach and this method has a dataset of a\ntotal of five species of spinach that contains 3785 images. Four Convolutional\nNeural Network (CNN) models were used to classify our spinach. These models\ngive more accurate results for image classification. Before applying these\nmodels there is some preprocessing of the image data. For the preprocessing of\ndata, some methods need to happen. Those are RGB conversion, filtering, resize\n& rescaling, and categorization. After applying these methods image data are\npre-processed and ready to be used in the classifier algorithms. The accuracy\nof these classifiers is in between 98.68% - 99.79%. Among those models, VGG16\nachieved the highest accuracy of 99.79%.",
          "link": "http://arxiv.org/abs/2201.02093",
          "publishedOn": "2022-01-08T00:37:47.583Z",
          "wordCount": 618,
          "title": "Deep Learning Based Classification System For Recognizing Local Spinach. (arXiv:2201.02093v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xenopoulos_P/0/1/0/all/0/1\">Peter Xenopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_G/0/1/0/all/0/1\">Gromit Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doraiswamy_H/0/1/0/all/0/1\">Harish Doraiswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nonato_L/0/1/0/all/0/1\">Luis Gustavo Nonato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barr_B/0/1/0/all/0/1\">Brian Barr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_C/0/1/0/all/0/1\">Claudio Silva</a>",
          "description": "Local explainability methods -- those which seek to generate an explanation\nfor each prediction -- are becoming increasingly prevalent due to the need for\npractitioners to rationalize their model outputs. However, comparing local\nexplainability methods is difficult since they each generate outputs in various\nscales and dimensions. Furthermore, due to the stochastic nature of some\nexplainability methods, it is possible for different runs of a method to\nproduce contradictory explanations for a given observation. In this paper, we\npropose a topology-based framework to extract a simplified representation from\na set of local explanations. We do so by first modeling the relationship\nbetween the explanation space and the model predictions as a scalar function.\nThen, we compute the topological skeleton of this function. This topological\nskeleton acts as a signature for such functions, which we use to compare\ndifferent explanation methods. We demonstrate that our framework can not only\nreliably identify differences between explainability techniques but also\nprovides stable representations. Then, we show how our framework can be used to\nidentify appropriate parameters for local explainability methods. Our framework\nis simple, does not require complex optimizations, and can be broadly applied\nto most local explanation methods. We believe the practicality and versatility\nof our approach will help promote topology-based approaches as a tool for\nunderstanding and comparing explanation methods.",
          "link": "http://arxiv.org/abs/2201.02155",
          "publishedOn": "2022-01-08T00:37:47.576Z",
          "wordCount": 634,
          "title": "Topological Representations of Local Explanations. (arXiv:2201.02155v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arthur_D/0/1/0/all/0/1\">Davis Arthur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Date_P/0/1/0/all/0/1\">Prasanna Date</a>",
          "description": "Deep learning is one of the most successful and far-reaching strategies used\nin machine learning today. However, the scale and utility of neural networks is\nstill greatly limited by the current hardware used to train them. These\nconcerns have become increasingly pressing as conventional computers quickly\napproach physical limitations that will slow performance improvements in years\nto come. For these reasons, scientists have begun to explore alternative\ncomputing platforms, like quantum computers, for training neural networks. In\nrecent years, variational quantum circuits have emerged as one of the most\nsuccessful approaches to quantum deep learning on noisy intermediate scale\nquantum devices. We propose a hybrid quantum-classical neural network\narchitecture where each neuron is a variational quantum circuit. We empirically\nanalyze the performance of this hybrid neural network on a series of binary\nclassification data sets using a simulated universal quantum computer and a\nstate of the art universal quantum computer. On simulated hardware, we observe\nthat the hybrid neural network achieves roughly 10% higher classification\naccuracy and 20% better minimization of cost than an individual variational\nquantum circuit. On quantum hardware, we observe that each model only performs\nwell when the qubit and gate count is sufficiently small.",
          "link": "http://arxiv.org/abs/2201.01820",
          "publishedOn": "2022-01-08T00:37:47.568Z",
          "wordCount": 619,
          "title": "A Hybrid Quantum-Classical Neural Network Architecture for Binary Classification. (arXiv:2201.01820v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesh_S/0/1/0/all/0/1\">Sumitra Ganesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasula_P/0/1/0/all/0/1\">Pranay Pasula</a>",
          "description": "Continual learning in environments with shifting data distributions is a\nchallenging problem with several real-world applications. In this paper we\nconsider settings in which the data distribution(task) shifts abruptly and the\ntiming of these shifts are not known. Furthermore, we consider a\nsemi-supervised task-agnostic setting in which the learning algorithm has\naccess to both task-segmented and unsegmented data for offline training. We\npropose a novel approach called mixture of Basismodels (MoB) for addressing\nthis problem setting. The core idea is to learn a small set of basis models and\nto construct a dynamic, task-dependent mixture of the models to predict for the\ncurrent task. We also propose a new methodology to detect observations that are\nout-of-distribution with respect to the existing basis models and to\ninstantiate new models as needed. We test our approach in multiple domains and\nshow that it attains better prediction error than existing methods in most\ncases while using fewer models than other multiple model approaches. Moreover,\nwe analyze the latent task representations learned by MoB and show that similar\ntasks tend to cluster in the latent space and that the latent representation\nshifts at the task boundaries when tasks are dissimilar.",
          "link": "http://arxiv.org/abs/2201.01853",
          "publishedOn": "2022-01-08T00:37:47.550Z",
          "wordCount": 619,
          "title": "Mixture of basis for interpretable continual learning with distribution shifts. (arXiv:2201.01853v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Orang_O/0/1/0/all/0/1\">Omid Orang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_P/0/1/0/all/0/1\">Petr&#xf4;nio C&#xe2;ndido de Lima Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guimaraes_F/0/1/0/all/0/1\">Frederico Gadelha Guimar&#xe3;es</a>",
          "description": "Fuzzy Cognitive Maps (FCMs) have emerged as an interpretable signed weighted\ndigraph method consisting of nodes (concepts) and weights which represent the\ndependencies among the concepts. Although FCMs have attained considerable\nachievements in various time series prediction applications, designing an FCM\nmodel with time-efficient training method is still an open challenge. Thus,\nthis paper introduces a novel univariate time series forecasting technique,\nwhich is composed of a group of randomized high order FCM models labeled\nR-HFCM. The novelty of the proposed R-HFCM model is relevant to merging the\nconcepts of FCM and Echo State Network (ESN) as an efficient and particular\nfamily of Reservoir Computing (RC) models, where the least squares algorithm is\napplied to train the model. From another perspective, the structure of R-HFCM\nconsists of the input layer, reservoir layer, and output layer in which only\nthe output layer is trainable while the weights of each sub-reservoir\ncomponents are selected randomly and keep constant during the training process.\nAs case studies, this model considers solar energy forecasting with public data\nfor Brazilian solar stations as well as Malaysia dataset, which includes hourly\nelectric load and temperature data of the power supply company of the city of\nJohor in Malaysia. The experiment also includes the effect of the map size,\nactivation function, the presence of bias and the size of the reservoir on the\naccuracy of R-HFCM method. The obtained results confirm the outperformance of\nthe proposed R-HFCM model in comparison to the other methods. This study\nprovides evidence that FCM can be a new way to implement a reservoir of\ndynamics in time series modelling.",
          "link": "http://arxiv.org/abs/2201.02158",
          "publishedOn": "2022-01-08T00:37:47.542Z",
          "wordCount": 721,
          "title": "Introducing Randomized High Order Fuzzy Cognitive Maps as Reservoir Computing Models: A Case Study in Solar Energy and Load Forecasting. (arXiv:2201.02158v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schnoering_H/0/1/0/all/0/1\">Hugo Schnoering</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inzirillo_H/0/1/0/all/0/1\">Hugo Inzirillo</a>",
          "description": "The study of time series has motivated many researchers, particularly on the\narea of multivariate-analysis. The study of co-movements and dependency between\nrandom variables leads us to develop metrics to describe existing connection\nbetween assets. The most commonly used are correlation and causality. Despite\nthe growing literature, some connections remained still undetected. The\nobjective of this paper is to propose a new representation learning algorithm\ncapable to integrate synchronous and asynchronous relationships.",
          "link": "http://arxiv.org/abs/2201.02040",
          "publishedOn": "2022-01-08T00:37:47.535Z",
          "wordCount": 482,
          "title": "Deep Fusion of Lead-lag Graphs:Application to Cryptocurrencies. (arXiv:2201.02040v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01871",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Alyaev_S/0/1/0/all/0/1\">Sergey Alyaev</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Elsheikh_A/0/1/0/all/0/1\">Ahmed H. Elsheikh</a>",
          "description": "Geosteering of wells requires fast interpretation of geophysical logs which\nis a non-unique inverse problem. Current work presents a proof-of-concept\napproach to multi-modal probabilistic inversion of logs using a single\nevaluation of an artificial deep neural network (DNN). A mixture density DNN\n(MDN) is trained using the \"multiple-trajectory-prediction\" (MTP) loss\nfunctions, which avoids mode collapse typical for traditional MDNs, and allows\nmulti-modal prediction ahead of data. The proposed approach is verified on the\nreal-time stratigraphic inversion of gamma-ray logs. The multi-modal predictor\noutputs several likely inverse solutions/predictions, providing more accurate\nand realistic solutions compared to a deterministic regression using a DNN. For\nthese likely stratigraphic curves, the model simultaneously predicts their\nprobabilities, which are implicitly learned from the training geological data.\nThe stratigraphy predictions and their probabilities obtained in milliseconds\nfrom the MDN can enable better real-time decisions under geological\nuncertainties.",
          "link": "http://arxiv.org/abs/2201.01871",
          "publishedOn": "2022-01-08T00:37:47.528Z",
          "wordCount": 559,
          "title": "Direct multi-modal inversion of geophysical logs using deep learning. (arXiv:2201.01871v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anderson_T/0/1/0/all/0/1\">Thomas Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belay_A/0/1/0/all/0/1\">Adam Belay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1\">Mosharaf Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cidon_A/0/1/0/all/0/1\">Asaf Cidon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_I/0/1/0/all/0/1\">Irene Zhang</a>",
          "description": "The end of Dennard scaling and the slowing of Moore's Law has put the energy\nuse of datacenters on an unsustainable path. Datacenters are already a\nsignificant fraction of worldwide electricity use, with application demand\nscaling at a rapid rate. We argue that substantial reductions in the carbon\nintensity of datacenter computing are possible with a software-centric\napproach: by making energy and carbon visible to application developers on a\nfine-grained basis, by modifying system APIs to make it possible to make\ninformed trade offs between performance and carbon emissions, and by raising\nthe level of application programming to allow for flexible use of more energy\nefficient means of compute and storage. We also lay out a research agenda for\nsystems software to reduce the carbon footprint of datacenter computing.",
          "link": "http://arxiv.org/abs/2201.02120",
          "publishedOn": "2022-01-08T00:37:47.522Z",
          "wordCount": 566,
          "title": "Treehouse: A Case For Carbon-Aware Datacenter Software. (arXiv:2201.02120v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2002.05815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ting_K/0/1/0/all/0/1\">Kai Ming Ting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wells_J/0/1/0/all/0/1\">Jonathan R. Wells</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Ye Zhu</a>",
          "description": "Measuring similarity between two objects is the core operation in existing\nclustering algorithms in grouping similar objects into clusters. This paper\nintroduces a new similarity measure called point-set kernel which computes the\nsimilarity between an object and a set of objects. The proposed clustering\nprocedure utilizes this new measure to characterize every cluster grown from a\nseed object. We show that the new clustering procedure is both effective and\nefficient that enables it to deal with large scale datasets. In contrast,\nexisting clustering algorithms are either efficient or effective. In comparison\nwith the state-of-the-art density-peak clustering and scalable kernel k-means\nclustering, we show that the proposed algorithm is more effective and runs\norders of magnitude faster when applying to datasets of millions of data\npoints, on a commonly used computing machine.",
          "link": "http://arxiv.org/abs/2002.05815",
          "publishedOn": "2022-01-08T00:37:47.515Z",
          "wordCount": 577,
          "title": "Point-Set Kernel Clustering. (arXiv:2002.05815v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01873",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shechter_M/0/1/0/all/0/1\">Meitar Shechter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanocka_R/0/1/0/all/0/1\">Rana Hanocka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzer_G/0/1/0/all/0/1\">Gal Metzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>",
          "description": "We introduce DeepMLS, a space-based deformation technique, guided by a set of\ndisplaced control points. We leverage the power of neural networks to inject\nthe underlying shape geometry into the deformation parameters. The goal of our\ntechnique is to enable a realistic and intuitive shape deformation. Our method\nis built upon moving least-squares (MLS), since it minimizes a weighted sum of\nthe given control point displacements. Traditionally, the influence of each\ncontrol point on every point in space (i.e., the weighting function) is defined\nusing inverse distance heuristics. In this work, we opt to learn the weighting\nfunction, by training a neural network on the control points from a single\ninput shape, and exploit the innate smoothness of neural networks. Our\ngeometry-aware control point deformation is agnostic to the surface\nrepresentation and quality; it can be applied to point clouds or meshes,\nincluding non-manifold and disconnected surface soups. We show that our\ntechnique facilitates intuitive piecewise smooth deformations, which are well\nsuited for manufactured objects. We show the advantages of our approach\ncompared to existing surface and space-based deformation techniques, both\nquantitatively and qualitatively.",
          "link": "http://arxiv.org/abs/2201.01873",
          "publishedOn": "2022-01-08T00:37:47.498Z",
          "wordCount": 606,
          "title": "DeepMLS: Geometry-Aware Control Point Deformation. (arXiv:2201.01873v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hestness_J/0/1/0/all/0/1\">Joel Hestness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Church_K/0/1/0/all/0/1\">Kenneth Church</a>",
          "description": "This paper proposes an efficient approach to learning disentangled\nrepresentations with causal mechanisms based on the difference of conditional\nprobabilities in original and new distributions. We approximate the difference\nwith models' generalization abilities so that it fits in the standard machine\nlearning framework and can be efficiently computed. In contrast to the\nstate-of-the-art approach, which relies on the learner's adaptation speed to\nnew distribution, the proposed approach only requires evaluating the model's\ngeneralization ability. We provide a theoretical explanation for the advantage\nof the proposed method, and our experiments show that the proposed technique is\n1.9--11.0$\\times$ more sample efficient and 9.4--32.4 times quicker than the\nprevious method on various tasks. The source code is available at\n\\url{https://github.com/yuanpeng16/EDCR}.",
          "link": "http://arxiv.org/abs/2201.01942",
          "publishedOn": "2022-01-08T00:37:47.482Z",
          "wordCount": null,
          "title": "Efficiently Disentangle Causal Representations. (arXiv:2201.01942v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02169",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vannella_F/0/1/0/all/0/1\">Filippo Vannella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proutiere_A/0/1/0/all/0/1\">Alexandre Proutiere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jedra_Y/0/1/0/all/0/1\">Yassir Jedra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jaeseong Jeong</a>",
          "description": "Controlling antenna tilts in cellular networks is imperative to reach an\nefficient trade-off between network coverage and capacity. In this paper, we\ndevise algorithms learning optimal tilt control policies from existing data (in\nthe so-called passive learning setting) or from data actively generated by the\nalgorithms (the active learning setting). We formalize the design of such\nalgorithms as a Best Policy Identification (BPI) problem in Contextual Linear\nMulti-Arm Bandits (CL-MAB). An arm represents an antenna tilt update; the\ncontext captures current network conditions; the reward corresponds to an\nimprovement of performance, mixing coverage and capacity; and the objective is\nto identify, with a given level of confidence, an approximately optimal policy\n(a function mapping the context to an arm with maximal reward). For CL-MAB in\nboth active and passive learning settings, we derive information-theoretical\nlower bounds on the number of samples required by any algorithm returning an\napproximately optimal policy with a given level of certainty, and devise\nalgorithms achieving these fundamental limits. We apply our algorithms to the\nRemote Electrical Tilt (RET) optimization problem in cellular networks, and\nshow that they can produce optimal tilt update policy using much fewer data\nsamples than naive or existing rule-based learning algorithms.",
          "link": "http://arxiv.org/abs/2201.02169",
          "publishedOn": "2022-01-08T00:37:47.481Z",
          "wordCount": 622,
          "title": "Learning Optimal Antenna Tilt Control Policies: A Contextual Linear Bandit Approach. (arXiv:2201.02169v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiazheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Ruihai Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smyth_B/0/1/0/all/0/1\">Barry Smyth</a>",
          "description": "Financial forecasting has been an important and active area of machine\nlearning research because of the challenges it presents and the potential\nrewards that even minor improvements in prediction accuracy or forecasting may\nentail. Traditionally, financial forecasting has heavily relied on quantitative\nindicators and metrics derived from structured financial statements. Earnings\nconference call data, including text and audio, is an important source of\nunstructured data that has been used for various prediction tasks using deep\nearning and related approaches. However, current deep learning-based methods\nare limited in the way that they deal with numeric data; numbers are typically\ntreated as plain-text tokens without taking advantage of their underlying\nnumeric structure. This paper describes a numeric-oriented hierarchical\ntransformer model to predict stock returns, and financial risk using\nmulti-modal aligned earnings calls data by taking advantage of the different\ncategories of numbers (monetary, temporal, percentages etc.) and their\nmagnitude. We present the results of a comprehensive evaluation of NumHTML\nagainst several state-of-the-art baselines using a real-world publicly\navailable dataset. The results indicate that NumHTML significantly outperforms\nthe current state-of-the-art across a variety of evaluation metrics and that it\nhas the potential to offer significant financial gains in a practical trading\ncontext.",
          "link": "http://arxiv.org/abs/2201.01770",
          "publishedOn": "2022-01-08T00:37:47.475Z",
          "wordCount": null,
          "title": "NumHTML: Numeric-Oriented Hierarchical Transformer Model for Multi-task Financial Forecasting. (arXiv:2201.01770v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xingyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zixuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1\">David Held</a>",
          "description": "Robotic manipulation of cloth remains challenging for robotics due to the\ncomplex dynamics of the cloth, lack of a low-dimensional state representation,\nand self-occlusions. In contrast to previous model-based approaches that learn\na pixel-based dynamics model or a compressed latent vector dynamics, we propose\nto learn a particle-based dynamics model from a partial point cloud\nobservation. To overcome the challenges of partial observability, we infer\nwhich visible points are connected on the underlying cloth mesh. We then learn\na dynamics model over this visible connectivity graph. Compared to previous\nlearning-based approaches, our model poses strong inductive bias with its\nparticle based representation for learning the underlying cloth physics; it is\ninvariant to visual features; and the predictions can be more easily\nvisualized. We show that our method greatly outperforms previous\nstate-of-the-art model-based and model-free reinforcement learning methods in\nsimulation. Furthermore, we demonstrate zero-shot sim-to-real transfer where we\ndeploy the model trained in simulation on a Franka arm and show that the model\ncan successfully smooth different types of cloth from crumpled configurations.\nVideos can be found on our project website.",
          "link": "http://arxiv.org/abs/2105.10389",
          "publishedOn": "2022-01-08T00:37:47.472Z",
          "wordCount": 653,
          "title": "Learning Visible Connectivity Dynamics for Cloth Smoothing. (arXiv:2105.10389v4 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.13836",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geyer_P/0/1/0/all/0/1\">Philipp Geyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Manav Mahan Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xia Chen</a>",
          "description": "Data-driven models created by machine learning gain in importance in all\nfields of design and engineering. They have high potential to assists\ndecision-makers in creating novel artefacts with a better performance and\nsustainability. However, limited generalization and the black-box nature of\nthese models induce limited explainability and reusability. These drawbacks\nprovide significant barriers retarding adoption in engineering design. To\novercome this situation, we propose a component-based approach to create\npartial component models by machine learning (ML). This component-based\napproach aligns deep learning to systems engineering (SE). By means of the\nexample of energy efficient building design, we first demonstrate\ngeneralization of the component-based method by accurately predicting the\nperformance of designs with random structure different from training data.\nSecond, we illustrate explainability by local sampling, sensitivity information\nand rules derived from low-depth decision trees and by evaluating this\ninformation from an engineering design perspective. The key for explainability\nis that activations at interfaces between the components are interpretable\nengineering quantities. In this way, the hierarchical component system forms a\ndeep neural network (DNN) that directly integrates information for engineering\nexplainability. The large range of possible configurations in composing\ncomponents allows the examination of novel unseen design cases with\nunderstandable data-driven models. The matching of parameter ranges of\ncomponents by similar probability distribution produces reusable,\nwell-generalizing, and trustworthy models. The approach adapts the model\nstructure to engineering methods of systems engineering and domain knowledge.",
          "link": "http://arxiv.org/abs/2108.13836",
          "publishedOn": "2022-01-08T00:37:47.457Z",
          "wordCount": 701,
          "title": "Explainable AI for engineering design: A unified approach of systems engineering and component-based deep learning. (arXiv:2108.13836v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Halperin_I/0/1/0/all/0/1\">Igor Halperin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiayu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>",
          "description": "We suggest a simple practical method to combine the human and artificial\nintelligence to both learn best investment practices of fund managers, and\nprovide recommendations to improve them. Our approach is based on a combination\nof Inverse Reinforcement Learning (IRL) and RL. First, the IRL component learns\nthe intent of fund managers as suggested by their trading history, and recovers\ntheir implied reward function. At the second step, this reward function is used\nby a direct RL algorithm to optimize asset allocation decisions. We show that\nour method is able to improve over the performance of individual fund managers.",
          "link": "http://arxiv.org/abs/2201.01874",
          "publishedOn": "2022-01-08T00:37:47.443Z",
          "wordCount": 537,
          "title": "Combining Reinforcement Learning and Inverse Reinforcement Learning for Asset Allocation Recommendations. (arXiv:2201.01874v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Licata_R/0/1/0/all/0/1\">Richard J. Licata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_P/0/1/0/all/0/1\">Piyush M. Mehta</a>",
          "description": "Machine learning (ML) has often been applied to space weather (SW) problems\nin recent years. SW originates from solar perturbations and is comprised of the\nresulting complex variations they cause within the systems between the Sun and\nEarth. These systems are tightly coupled and not well understood. This creates\na need for skillful models with knowledge about the confidence of their\npredictions. One example of such a dynamical system is the thermosphere, the\nneutral region of Earth's upper atmosphere. Our inability to forecast it has\nsevere repercussions in the context of satellite drag and collision avoidance\noperations for objects in low Earth orbit. Even with (assumed) perfect driver\nforecasts, our incomplete knowledge of the system results in often inaccurate\nneutral mass density predictions. Continuing efforts are being made to improve\nmodel accuracy, but density models rarely provide estimates of uncertainty. In\nthis work, we propose two techniques to develop nonlinear ML models to predict\nthermospheric density while providing calibrated uncertainty estimates: Monte\nCarlo (MC) dropout and direct prediction of the probability distribution, both\nusing the negative logarithm of predictive density (NLPD) loss function. We\nshow the performance for models trained on local and global datasets. This\nshows that NLPD provides similar results for both techniques but the direct\nprobability method has a much lower computational cost. For the global model\nregressed on the SET HASDM density database, we achieve errors of 11% on\nindependent test data with well-calibrated uncertainty estimates. Using an\nin-situ CHAMP density dataset, both techniques provide test error on the order\nof 13%. The CHAMP models (on independent data) are within 2% of perfect\ncalibration for all prediction intervals tested. This model can also be used to\nobtain global predictions with uncertainties at a given epoch.",
          "link": "http://arxiv.org/abs/2201.02067",
          "publishedOn": "2022-01-08T00:37:47.410Z",
          "wordCount": 712,
          "title": "Uncertainty Quantification Techniques for Space Weather Modeling: Thermospheric Density Application. (arXiv:2201.02067v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aditya Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_B/0/1/0/all/0/1\">B. Uma Shankar</a>",
          "description": "Acquiring information on large areas on the earth's surface through satellite\ncameras allows us to see much more than we can see while standing on the\nground. This assists us in detecting and monitoring the physical\ncharacteristics of an area like land-use patterns, atmospheric conditions,\nforest cover, and many unlisted aspects. The obtained images not only keep\ntrack of continuous natural phenomena but are also crucial in tackling the\nglobal challenge of severe deforestation. Among which Amazon basin accounts for\nthe largest share every year. Proper data analysis would help limit detrimental\neffects on the ecosystem and biodiversity with a sustainable healthy\natmosphere. This report aims to label the satellite image chips of the Amazon\nrainforest with atmospheric and various classes of land cover or land use\nthrough different machine learning and superior deep learning models.\nEvaluation is done based on the F2 metric, while for loss function, we have\nboth sigmoid cross-entropy as well as softmax cross-entropy. Images are fed\nindirectly to the machine learning classifiers after only features are\nextracted using pre-trained ImageNet architectures. Whereas for deep learning\nmodels, ensembles of fine-tuned ImageNet pre-trained models are used via\ntransfer learning. Our best score was achieved so far with the F2 metric is\n0.927.",
          "link": "http://arxiv.org/abs/2201.01971",
          "publishedOn": "2022-01-08T00:37:46.961Z",
          "wordCount": 640,
          "title": "Multi-Label Classification on Remote-Sensing Images. (arXiv:2201.01971v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zengyi Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Dawei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chuchu Fan</a>",
          "description": "Control certificates based on barrier functions have been a powerful tool to\ngenerate probably safe control policies for dynamical systems. However,\nexisting methods based on barrier certificates are normally for white-box\nsystems with differentiable dynamics, which makes them inapplicable to many\npractical applications where the system is a black-box and cannot be accurately\nmodeled. On the other side, model-free reinforcement learning (RL) methods for\nblack-box systems suffer from lack of safety guarantees and low sampling\nefficiency. In this paper, we propose a novel method that can learn safe\ncontrol policies and barrier certificates for black-box dynamical systems,\nwithout requiring for an accurate system model. Our method re-designs the loss\nfunction to back-propagate gradient to the control policy even when the\nblack-box dynamical system is non-differentiable, and we show that the safety\ncertificates hold on the black-box system. Empirical results in simulation show\nthat our method can significantly improve the performance of the learned\npolicies by achieving nearly 100% safety and goal reaching rates using much\nfewer training samples, compared to state-of-the-art black-box safe control\nmethods. Our learned agents can also generalize to unseen scenarios while\nkeeping the original performance. The source code can be found at\nhttps://github.com/Zengyi-Qin/bcbf.",
          "link": "http://arxiv.org/abs/2201.01918",
          "publishedOn": "2022-01-08T00:37:46.947Z",
          "wordCount": 632,
          "title": "SABLAS: Learning Safe Control for Black-box Dynamical Systems. (arXiv:2201.01918v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_C/0/1/0/all/0/1\">Cheng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Licheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chang-Chun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujiao Huang</a>",
          "description": "Over the year, people have been using deep learning to tackle inversion\nproblems, and we see the framework has been applied to build relationship\nbetween recording wavefield and velocity (Yang et al., 2016). Here we will\nextend the work from 2 perspectives, one is deriving a more appropriate loss\nfunction, as we now, pixel-2-pixel comparison might not be the best choice to\ncharacterize image structure, and we will elaborate on how to construct cost\nfunction to capture high level feature to enhance the model performance.\nAnother dimension is searching for the more appropriate neural architecture,\nwhich is a subset of an even bigger picture, the automatic machine learning, or\nAutoML. There are several famous networks, U-net, ResNet (He et al., 2016) and\nDenseNet (Huang et al., 2017), and they achieve phenomenal results for certain\nproblems, yet it's hard to argue they are the best for inversion problems\nwithout thoroughly searching within certain space. Here we will be showing our\narchitecture search results for inversion.",
          "link": "http://arxiv.org/abs/2201.01772",
          "publishedOn": "2022-01-08T00:37:46.938Z",
          "wordCount": 577,
          "title": "Neural Architecture Search for Inversion. (arXiv:2201.01772v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01669",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Darici_E/0/1/0/all/0/1\">Esin Darici</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rasmussen_N/0/1/0/all/0/1\">Nicholas Rasmussen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+J%2E_J/0/1/0/all/0/1\">Jennifer Ranjani J.</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jaclyn Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chaudhari_G/0/1/0/all/0/1\">Gunvant Chaudhari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajput_A/0/1/0/all/0/1\">Akanksha Rajput</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Govindan_P/0/1/0/all/0/1\">Praveen Govindan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yamaura_M/0/1/0/all/0/1\">Minami Yamaura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gomezjurado_L/0/1/0/all/0/1\">Laura Gomezjurado</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khanzada_A/0/1/0/all/0/1\">Amil Khanzada</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pilanci_M/0/1/0/all/0/1\">Mert Pilanci</a>",
          "description": "The Covid-19 pandemic has been a scourge upon humanity, claiming the lives of\nmore than 5 million people worldwide. Although vaccines are being distributed\nworldwide, there is an apparent need for affordable screening techniques to\nserve parts of the world that do not have access to traditional medicine.\nArtificial Intelligence can provide a solution utilizing cough sounds as the\nprimary screening mode. This paper presents multiple models that have achieved\nrelatively respectable perfor mance on the largest evaluation dataset currently\npresented in academic literature. Moreover, we also show that performance\nincreases with training data size, showing the need for the world wide\ncollection of data to help combat the Covid-19 pandemic with non-traditional\nmeans.",
          "link": "http://arxiv.org/abs/2201.01669",
          "publishedOn": "2022-01-07T00:40:43.353Z",
          "wordCount": 618,
          "title": "Using Deep Learning with Large Aggregated Datasets for COVID-19 Classification from Cough. (arXiv:2201.01669v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10154",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Krajnak_V/0/1/0/all/0/1\">Vladim&#xed;r Kraj&#x148;&#xe1;k</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Naik_S/0/1/0/all/0/1\">Shibabrat Naik</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wiggins_S/0/1/0/all/0/1\">Stephen Wiggins</a>",
          "description": "In this paper, we use support vector machines (SVM) to develop a machine\nlearning framework to discover phase space structures that distinguish between\ndistinct reaction pathways. The SVM model is trained using data from\ntrajectories of Hamilton's equations and works well even with relatively few\ntrajectories. Moreover, this framework is specifically designed to require\nminimal a priori knowledge of the dynamics in a system. This makes our approach\ncomputationally better suited than existing methods for high-dimensional\nsystems and systems where integrating trajectories is expensive. We benchmark\nour approach on Chesnavich's CH$_4^+$ Hamiltonian.",
          "link": "http://arxiv.org/abs/2107.10154",
          "publishedOn": "2022-01-07T00:40:43.327Z",
          "wordCount": 541,
          "title": "Predicting trajectory behaviour via machine-learned invariant manifolds. (arXiv:2107.10154v2 [physics.chem-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.03384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengzhou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1\">Fenglei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weiwen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaodong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>",
          "description": "Image denoising is a prerequisite for downstream tasks in many fields.\nLow-dose and photon-counting computed tomography (CT) denoising can optimize\ndiagnostic performance at minimized radiation dose. Supervised deep denoising\nmethods are popular but require paired clean or noisy samples that are often\nunavailable in practice. Limited by the independent noise assumption, current\nunsupervised denoising methods cannot process correlated noises as in CT\nimages. Here we propose the first-of-its-kind similarity-based unsupervised\ndeep denoising approach, referred to as Noise2Sim, that works in a nonlocal and\nnonlinear fashion to suppress not only independent but also correlated noises.\nTheoretically, Noise2Sim is asymptotically equivalent to supervised learning\nmethods under mild conditions. Experimentally, Nosie2Sim recovers intrinsic\nfeatures from noisy low-dose CT and photon-counting CT images as effectively as\nor even better than supervised learning methods on practical datasets visually,\nquantitatively and statistically. Noise2Sim is a general unsupervised denoising\napproach and has great potential in diverse applications.",
          "link": "http://arxiv.org/abs/2011.03384",
          "publishedOn": "2022-01-07T00:40:43.252Z",
          "wordCount": null,
          "title": "Suppression of Correlated Noise with Similarity-based Unsupervised Deep Learning. (arXiv:2011.03384v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.10638",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Watcharasupat_K/0/1/0/all/0/1\">Karn N. Watcharasupat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerch_A/0/1/0/all/0/1\">Alexander Lerch</a>",
          "description": "Latte (for LATent Tensor Evaluation) is a Python library for evaluation of\nlatent-based generative models in the fields of disentanglement learning and\ncontrollable generation. Latte is compatible with both PyTorch and\nTensorFlow/Keras, and provides both functional and modular APIs that can be\neasily extended to support other deep learning frameworks. Using NumPy-based\nand framework-agnostic implementation, Latte ensures reproducible, consistent,\nand deterministic metric calculations regardless of the deep learning framework\nof choice.",
          "link": "http://arxiv.org/abs/2112.10638",
          "publishedOn": "2022-01-07T00:40:43.251Z",
          "wordCount": null,
          "title": "Latte: Cross-framework Python Package for Evaluation of Latent-Based Generative Models. (arXiv:2112.10638v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Davaslioglu_K/0/1/0/all/0/1\">Kemal Davaslioglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erpek_T/0/1/0/all/0/1\">Tugba Erpek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagduyu_Y/0/1/0/all/0/1\">Yalin E. Sagduyu</a>",
          "description": "An end-to-end communications system based on Orthogonal Frequency Division\nMultiplexing (OFDM) is modeled as an autoencoder (AE) for which the transmitter\n(coding and modulation) and receiver (demodulation and decoding) are\nrepresented as deep neural networks (DNNs) of the encoder and decoder,\nrespectively. This AE communications approach is shown to outperform\nconventional communications in terms of bit error rate (BER) under practical\nscenarios regarding channel and interference effects as well as training data\nand embedded implementation constraints. A generative adversarial network (GAN)\nis trained to augment the training data when there is not enough training data\navailable. Also, the performance is evaluated in terms of the DNN model\nquantization and the corresponding memory requirements for embedded\nimplementation. Then, interference training and randomized smoothing are\nintroduced to train the AE communications to operate under unknown and dynamic\ninterference (jamming) effects on potentially multiple OFDM symbols. Relative\nto conventional communications, up to 36 dB interference suppression for a\nchannel reuse of four can be achieved by the AE communications with\ninterference training and randomized smoothing. AE communications is also\nextended to the multiple-input multiple-output (MIMO) case and its BER\nperformance gain with and without interference effects is demonstrated compared\nto conventional MIMO communications.",
          "link": "http://arxiv.org/abs/2201.01388",
          "publishedOn": "2022-01-07T00:40:43.234Z",
          "wordCount": null,
          "title": "End-to-End Autoencoder Communications with Optimized Interference Suppression. (arXiv:2201.01388v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01525",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gowda_D/0/1/0/all/0/1\">Dhananjaya Gowda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bollepalli_B/0/1/0/all/0/1\">Bajibabu Bollepalli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kadiri_S/0/1/0/all/0/1\">Sudarsana Reddy Kadiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alku_P/0/1/0/all/0/1\">Paavo Alku</a>",
          "description": "Formant tracking is investigated in this study by using trackers based on\ndynamic programming (DP) and deep neural nets (DNNs). Using the DP approach,\nsix formant estimation methods were first compared. The six methods include\nlinear prediction (LP) algorithms, weighted LP algorithms and the recently\ndeveloped quasi-closed phase forward-backward (QCP-FB) method. QCP-FB gave the\nbest performance in the comparison. Therefore, a novel formant tracking\napproach, which combines benefits of deep learning and signal processing based\non QCP-FB, was proposed. In this approach, the formants predicted by a\nDNN-based tracker from a speech frame are refined using the peaks of the\nall-pole spectrum computed by QCP-FB from the same frame. Results show that the\nproposed DNN-based tracker performed better both in detection rate and\nestimation error for the lowest three formants compared to reference formant\ntrackers. Compared to the popular Wavesurfer, for example, the proposed tracker\ngave a reduction of 29%, 48% and 35% in the estimation error for the lowest\nthree formants, respectively.",
          "link": "http://arxiv.org/abs/2201.01525",
          "publishedOn": "2022-01-07T00:40:43.234Z",
          "wordCount": null,
          "title": "Formant Tracking Using Quasi-Closed Phase Forward-Backward Linear Prediction Analysis and Deep Neural Networks. (arXiv:2201.01525v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2001.09782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tien-Dung Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_Huu_T/0/1/0/all/0/1\">Tram Truong-Huu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hien Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Khanh Tran</a>",
          "description": "Deep learning has achieved great success in many applications. However, its\ndeployment in practice has been hurdled by two issues: the privacy of data that\nhas to be aggregated centrally for model training and high communication\noverhead due to transmission of a large amount of data usually geographically\ndistributed. Addressing both issues is challenging and most existing works\ncould not provide an efficient solution. In this paper, we develop FedPC, a\nFederated Deep Learning Framework for Privacy Preservation and Communication\nEfficiency. The framework allows a model to be learned on multiple private\ndatasets while not revealing any information of training data, even with\nintermediate data. The framework also minimizes the amount of data exchanged to\nupdate the model. We formally prove the convergence of the learning model when\ntraining with FedPC and its privacy-preserving property. We perform extensive\nexperiments to evaluate the performance of FedPC in terms of the approximation\nto the upper-bound performance (when training centrally) and communication\noverhead. The results show that FedPC maintains the performance approximation\nof the models within $8.5\\%$ of the centrally-trained models when data is\ndistributed to 10 computing nodes. FedPC also reduces the communication\noverhead by up to $42.20\\%$ compared to existing works.",
          "link": "http://arxiv.org/abs/2001.09782",
          "publishedOn": "2022-01-07T00:40:43.234Z",
          "wordCount": null,
          "title": "A Federated Deep Learning Framework for Privacy Preservation and Communication Efficiency. (arXiv:2001.09782v3 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guesmi_A/0/1/0/all/0/1\">Amira Guesmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khasawneh_K/0/1/0/all/0/1\">Khaled N. Khasawneh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abu_Ghazaleh_N/0/1/0/all/0/1\">Nael Abu-Ghazaleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alouani_I/0/1/0/all/0/1\">Ihsen Alouani</a>",
          "description": "Advances in deep learning have enabled a wide range of promising\napplications. However, these systems are vulnerable to Adversarial Machine\nLearning (AML) attacks; adversarially crafted perturbations to their inputs\ncould cause them to misclassify. Several state-of-the-art adversarial attacks\nhave demonstrated that they can reliably fool classifiers making these attacks\na significant threat. Adversarial attack generation algorithms focus primarily\non creating successful examples while controlling the noise magnitude and\ndistribution to make detection more difficult. The underlying assumption of\nthese attacks is that the adversarial noise is generated offline, making their\nexecution time a secondary consideration. However, recently, just-in-time\nadversarial attacks where an attacker opportunistically generates adversarial\nexamples on the fly have been shown to be possible. This paper introduces a new\nproblem: how do we generate adversarial noise under real-time constraints to\nsupport such real-time adversarial attacks? Understanding this problem improves\nour understanding of the threat these attacks pose to real-time systems and\nprovides security evaluation benchmarks for future defenses. Therefore, we\nfirst conduct a run-time analysis of adversarial generation algorithms.\nUniversal attacks produce a general attack offline, with no online overhead,\nand can be applied to any input; however, their success rate is limited because\nof their generality. In contrast, online algorithms, which work on a specific\ninput, are computationally expensive, making them inappropriate for operation\nunder time constraints. Thus, we propose ROOM, a novel Real-time Online-Offline\nattack construction Model where an offline component serves to warm up the\nonline algorithm, making it possible to generate highly successful attacks\nunder time constraints.",
          "link": "http://arxiv.org/abs/2201.01621",
          "publishedOn": "2022-01-07T00:40:43.233Z",
          "wordCount": null,
          "title": "ROOM: Adversarial Machine Learning Attacks Under Real-Time Constraints. (arXiv:2201.01621v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/1812.02099",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chalopin_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;mie Chalopin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chepoi_V/0/1/0/all/0/1\">Victor Chepoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1\">Shay Moran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warmuth_M/0/1/0/all/0/1\">Manfred K. Warmuth</a>",
          "description": "We examine connections between combinatorial notions that arise in machine\nlearning and topological notions in cubical/simplicial geometry. These\nconnections enable to export results from geometry to machine learning. Our\nfirst main result is based on a geometric construction by Tracy Hall (2004) of\na partial shelling of the cross-polytope which can not be extended. We use it\nto derive a maximum class of VC dimension 3 that has no corners. This refutes\nseveral previous works in machine learning from the past 11 years. In\nparticular, it implies that all previous constructions of optimal unlabeled\nsample compression schemes for maximum classes are erroneous.\n\nOn the positive side we present a new construction of an unlabeled sample\ncompression scheme for maximum classes. We leave as open whether our unlabeled\nsample compression scheme extends to ample (a.k.a. lopsided or extremal)\nclasses, which represent a natural and far-reaching generalization of maximum\nclasses. Towards resolving this question, we provide a geometric\ncharacterization in terms of unique sink orientations of the 1-skeletons of\nassociated cubical complexes.",
          "link": "http://arxiv.org/abs/1812.02099",
          "publishedOn": "2022-01-07T00:40:43.233Z",
          "wordCount": null,
          "title": "Unlabeled sample compression schemes and corner peelings for ample and maximum classes. (arXiv:1812.02099v2 [cs.DM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stoychev_S/0/1/0/all/0/1\">Samuil Stoychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunes_H/0/1/0/all/0/1\">Hatice Gunes</a>",
          "description": "Deep neural networks have proved hugely successful, achieving human-like\nperformance on a variety of tasks. However, they are also computationally\nexpensive, which has motivated the development of model compression techniques\nwhich reduce the resource consumption associated with deep learning models.\nNevertheless, recent studies have suggested that model compression can have an\nadverse effect on algorithmic fairness, amplifying existing biases in machine\nlearning models. With this project we aim to extend those studies to the\ncontext of facial expression recognition. To do that, we set up a neural\nnetwork classifier to perform facial expression recognition and implement\nseveral model compression techniques on top of it. We then run experiments on\ntwo facial expression datasets, namely the Extended Cohn-Kanade Dataset (CK+DB)\nand the Real-World Affective Faces Database (RAF-DB), to examine the individual\nand combined effect that compression techniques have on the model size,\naccuracy and fairness. Our experimental results show that: (i) Compression and\nquantisation achieve significant reduction in model size with minimal impact on\noverall accuracy for both CK+DB and RAF-DB; (ii) in terms of model accuracy,\nthe classifier trained and tested on RAF-DB seems more robust to compression\ncompared to the CK+ DB; (iii) for RAF-DB, the different compression strategies\ndo not seem to increase the gap in predictive performance across the sensitive\nattributes of gender, race and age which is in contrast with the results on the\nCK+DB, where compression seems to amplify existing biases for gender. We\nanalyse the results and discuss the potential reasons for our findings.",
          "link": "http://arxiv.org/abs/2201.01709",
          "publishedOn": "2022-01-07T00:40:43.232Z",
          "wordCount": null,
          "title": "The Effect of Model Compression on Fairness in Facial Expression Recognition. (arXiv:2201.01709v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.04621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bashivan_P/0/1/0/all/0/1\">Pouya Bashivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayat_R/0/1/0/all/0/1\">Reza Bayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_A/0/1/0/all/0/1\">Adam Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1\">Kartik Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faramarzi_M/0/1/0/all/0/1\">Mojtaba Faramarzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laleh_T/0/1/0/all/0/1\">Touraj Laleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richards_B/0/1/0/all/0/1\">Blake Aaron Richards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1\">Irina Rish</a>",
          "description": "Neural networks are known to be vulnerable to adversarial attacks -- slight\nbut carefully constructed perturbations of the inputs which can drastically\nimpair the network's performance. Many defense methods have been proposed for\nimproving robustness of deep networks by training them on adversarially\nperturbed inputs. However, these models often remain vulnerable to new types of\nattacks not seen during training, and even to slightly stronger versions of\npreviously seen attacks. In this work, we propose a novel approach to\nadversarial robustness, which builds upon the insights from the domain\nadaptation field. Our method, called Adversarial Feature Desensitization (AFD),\naims at learning features that are invariant towards adversarial perturbations\nof the inputs. This is achieved through a game where we learn features that are\nboth predictive and robust (insensitive to adversarial attacks), i.e. cannot be\nused to discriminate between natural and adversarial data. Empirical results on\nseveral benchmarks demonstrate the effectiveness of the proposed approach\nagainst a wide range of attack types and attack strengths. Our code is\navailable at https://github.com/BashivanLab/afd.",
          "link": "http://arxiv.org/abs/2006.04621",
          "publishedOn": "2022-01-07T00:40:43.232Z",
          "wordCount": null,
          "title": "Adversarial Feature Desensitization. (arXiv:2006.04621v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.14457",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pramanik_S/0/1/0/all/0/1\">Subhojeet Pramanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mujumdar_S/0/1/0/all/0/1\">Shashank Mujumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1\">Hima Patel</a>",
          "description": "Recent approaches in literature have exploited the multi-modal information in\ndocuments (text, layout, image) to serve specific downstream document tasks.\nHowever, they are limited by their - (i) inability to learn cross-modal\nrepresentations across text, layout and image dimensions for documents and (ii)\ninability to process multi-page documents. Pre-training techniques have been\nshown in Natural Language Processing (NLP) domain to learn generic textual\nrepresentations from large unlabelled datasets, applicable to various\ndownstream NLP tasks. In this paper, we propose a multi-task learning-based\nframework that utilizes a combination of self-supervised and supervised\npre-training tasks to learn a generic document representation applicable to\nvarious downstream document tasks. Specifically, we introduce Document Topic\nModelling and Document Shuffle Prediction as novel pre-training tasks to learn\nrich image representations along with the text and layout representations for\ndocuments. We utilize the Longformer network architecture as the backbone to\nencode the multi-modal information from multi-page documents in an end-to-end\nfashion. We showcase the applicability of our pre-training framework on a\nvariety of different real-world document tasks such as document classification,\ndocument information extraction, and document retrieval. We evaluate our\nframework on different standard document datasets and conduct exhaustive\nexperiments to compare performance against various ablations of our framework\nand state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2009.14457",
          "publishedOn": "2022-01-07T00:40:43.231Z",
          "wordCount": null,
          "title": "Towards a Multi-modal, Multi-task Learning based Pre-training Framework for Document Representation Learning. (arXiv:2009.14457v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14539",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chatzimparmpas_A/0/1/0/all/0/1\">Angelos Chatzimparmpas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_R/0/1/0/all/0/1\">Rafael M. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kucher_K/0/1/0/all/0/1\">Kostiantyn Kucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerren_A/0/1/0/all/0/1\">Andreas Kerren</a>",
          "description": "The machine learning (ML) life cycle involves a series of iterative steps,\nfrom the effective gathering and preparation of the data, including complex\nfeature engineering processes, to the presentation and improvement of results,\nwith various algorithms to choose from in every step. Feature engineering in\nparticular can be very beneficial for ML, leading to numerous improvements such\nas boosting the predictive results, decreasing computational times, reducing\nexcessive noise, and increasing the transparency behind the decisions taken\nduring the training. Despite that, while several visual analytics tools exist\nto monitor and control the different stages of the ML life cycle (especially\nthose related to data and algorithms), feature engineering support remains\ninadequate. In this paper, we present FeatureEnVi, a visual analytics system\nspecifically designed to assist with the feature engineering process. Our\nproposed system helps users to choose the most important feature, to transform\nthe original features into powerful alternatives, and to experiment with\ndifferent feature generation combinations. Additionally, data space slicing\nallows users to explore the impact of features on both local and global scales.\nFeatureEnVi utilizes multiple automatic feature selection techniques;\nfurthermore, it visually guides users with statistical evidence about the\ninfluence of each feature (or subsets of features). The final outcome is the\nextraction of heavily engineered features, evaluated by multiple validation\nmetrics. The usefulness and applicability of FeatureEnVi are demonstrated with\ntwo use cases and a case study. We also report feedback from interviews with\ntwo ML experts and a visualization researcher who assessed the effectiveness of\nour system.",
          "link": "http://arxiv.org/abs/2103.14539",
          "publishedOn": "2022-01-07T00:40:43.231Z",
          "wordCount": null,
          "title": "FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise Selection and Semi-Automatic Extraction Approaches. (arXiv:2103.14539v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Sixu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qinbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhaomin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bingsheng He</a>",
          "description": "This paper presents and characterizes an Open Application Repository for\nFederated Learning (OARF), a benchmark suite for federated machine learning\nsystems. Previously available benchmarks for federated learning have focused\nmainly on synthetic datasets and use a limited number of applications. OARF\nmimics more realistic application scenarios with publicly available data sets\nas different data silos in image, text and structured data. Our\ncharacterization shows that the benchmark suite is diverse in data size,\ndistribution, feature distribution and learning task complexity. The extensive\nevaluations with reference implementations show the future research\nopportunities for important aspects of federated learning systems. We have\ndeveloped reference implementations, and evaluated the important aspects of\nfederated learning, including model accuracy, communication cost, throughput\nand convergence time. Through these evaluations, we discovered some interesting\nfindings such as federated learning can effectively increase end-to-end\nthroughput.",
          "link": "http://arxiv.org/abs/2006.07856",
          "publishedOn": "2022-01-07T00:40:43.230Z",
          "wordCount": null,
          "title": "The OARF Benchmark Suite: Characterization and Implications for Federated Learning Systems. (arXiv:2006.07856v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.13297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Borde_H/0/1/0/all/0/1\">Haitz S&#xe1;ez de Oc&#xe1;riz Borde</a>",
          "description": "Memory replay may be key to learning in biological brains, which manage to\nlearn new tasks continually without catastrophically interfering with previous\nknowledge. On the other hand, artificial neural networks suffer from\ncatastrophic forgetting and tend to only perform well on tasks that they were\nrecently trained on. In this work we explore the application of latent space\nbased memory replay for classification using artificial neural networks. We are\nable to preserve good performance in previous tasks by storing only a small\npercentage of the original data in a compressed latent space version.",
          "link": "http://arxiv.org/abs/2111.13297",
          "publishedOn": "2022-01-07T00:40:43.230Z",
          "wordCount": null,
          "title": "Latent Space based Memory Replay for Continual Learning in Artificial Neural Networks. (arXiv:2111.13297v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01728",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Elmahdy_A/0/1/0/all/0/1\">Adel Elmahdy</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ahn_J/0/1/0/all/0/1\">Junhyung Ahn</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Suh_C/0/1/0/all/0/1\">Changho Suh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mohajer_S/0/1/0/all/0/1\">Soheil Mohajer</a>",
          "description": "We consider a matrix completion problem that exploits social or item\nsimilarity graphs as side information. We develop a universal, parameter-free,\nand computationally efficient algorithm that starts with hierarchical graph\nclustering and then iteratively refines estimates both on graph clustering and\nmatrix ratings. Under a hierarchical stochastic block model that well respects\npractically-relevant social graphs and a low-rank rating matrix model (to be\ndetailed), we demonstrate that our algorithm achieves the information-theoretic\nlimit on the number of observed matrix entries (i.e., optimal sample\ncomplexity) that is derived by maximum likelihood estimation together with a\nlower-bound impossibility result. One consequence of this result is that\nexploiting the hierarchical structure of social graphs yields a substantial\ngain in sample complexity relative to the one that simply identifies different\ngroups without resorting to the relational structure across them. We conduct\nextensive experiments both on synthetic and real-world datasets to corroborate\nour theoretical results as well as to demonstrate significant performance\nimprovements over other matrix completion algorithms that leverage graph side\ninformation.",
          "link": "http://arxiv.org/abs/2201.01728",
          "publishedOn": "2022-01-07T00:40:43.227Z",
          "wordCount": 644,
          "title": "Matrix Completion with Hierarchical Graph Side Information. (arXiv:2201.01728v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amato_D/0/1/0/all/0/1\">Domenico Amato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosco_G/0/1/0/all/0/1\">Giosu&#xe8; Lo Bosco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giancarlo_R/0/1/0/all/0/1\">Raffaele Giancarlo</a>",
          "description": "The Searching on Sorted Data ({\\bf SOSD}, in short) is a highly engineered\nsoftware platform for benchmarking Learned Indexes, those latter being a novel\nand quite effective proposal of how to search in a sorted table by combining\nMachine Learning techniques with classic Algorithms. In such a platform and in\nthe related benchmarking experiments, following a natural and intuitive choice,\nthe final search stage is performed via the Standard (textbook) Binary Search\nprocedure. However, recent studies, that do not use Machine Learning\npredictions, indicate that Uniform Binary Search, streamlined to avoid\n\\vir{branching} in the main loop, is superior in performance to its Standard\ncounterpart when the table to be searched into is relatively small, e.g.,\nfitting in L1 or L2 cache. Analogous results hold for k-ary Search, even on\nlarge tables. One would expect an analogous behaviour within Learned Indexes.\nVia a set of extensive experiments, coherent with the State of the Art, we show\nthat for Learned Indexes, and as far as the {\\bf SOSD} software is concerned,\nthe use of the Standard routine (either Binary or k-ary Search) is superior to\nthe Uniform one, across all the internal memory levels. This fact provides a\nquantitative justification of the natural choice made so far. Our experiments\nalso indicate that Uniform Binary and k-ary Search can be advantageous to use\nin order to save space in Learned Indexes, while granting a good performance in\ntime. Our findings are of methodological relevance for this novel and\nfast-growing area and informative to practitioners interested in using Learned\nIndexes in application domains, e.g., Data Bases and Search Engines.",
          "link": "http://arxiv.org/abs/2201.01554",
          "publishedOn": "2022-01-07T00:40:43.220Z",
          "wordCount": 740,
          "title": "Standard Vs Uniform Binary Search and Their Variants in Learned Static Indexing: The Case of the Searching on Sorted Data Benchmarking Software Platform. (arXiv:2201.01554v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2110.12734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yaya Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaosu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>",
          "description": "Adversarial attacks make their success in \"fooling\" DNNs and among them,\ngradient-based algorithms become one of the mainstreams. Based on the linearity\nhypothesis [12], under $\\ell_\\infty$ constraint, $sign$ operation applied to\nthe gradients is a good choice for generating perturbations. However, the\nside-effect from such operation exists since it leads to the bias of direction\nbetween the real gradients and the perturbations. In other words, current\nmethods contain a gap between real gradients and actual noises, which leads to\nbiased and inefficient attacks. Therefore in this paper, based on the Taylor\nexpansion, the bias is analyzed theoretically and the correction of $\\sign$,\ni.e., Fast Gradient Non-sign Method (FGNM), is further proposed. Notably, FGNM\nis a general routine, which can seamlessly replace the conventional $sign$\noperation in gradient-based attacks with negligible extra computational cost.\nExtensive experiments demonstrate the effectiveness of our methods.\nSpecifically, ours outperform them by \\textbf{27.5\\%} at most and\n\\textbf{9.5\\%} on average. Our anonymous code is publicly available:\n\\url{https://git.io/mm-fgnm}.",
          "link": "http://arxiv.org/abs/2110.12734",
          "publishedOn": "2022-01-07T00:40:43.212Z",
          "wordCount": 616,
          "title": "Fast Gradient Non-sign Methods. (arXiv:2110.12734v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01363",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McDonald_A/0/1/0/all/0/1\">Andrew W.E. McDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shokoufandeh_A/0/1/0/all/0/1\">Ali Shokoufandeh</a>",
          "description": "It has been argued by Thom and Palm that sparsely-connected neural networks\n(SCNs) show improved performance over fully-connected networks (FCNs).\nSuper-regular networks (SRNs) are neural networks composed of a set of stacked\nsparse layers of (epsilon, delta)-super-regular pairs, and randomly permuted\nnode order. Using the Blow-up Lemma, we prove that as a result of the\nindividual super-regularity of each pair of layers, SRNs guarantee a number of\nproperties that make them suitable replacements for FCNs for many tasks. These\nguarantees include edge uniformity across all large-enough subsets, minimum\nnode in- and out-degree, input-output sensitivity, and the ability to embed\npre-trained constructs. Indeed, SRNs have the capacity to act like FCNs, and\neliminate the need for costly regularization schemes like Dropout. We show that\nSRNs perform similarly to X-Nets via readily reproducible experiments, and\noffer far greater guarantees and control over network structure.",
          "link": "http://arxiv.org/abs/2201.01363",
          "publishedOn": "2022-01-07T00:40:43.194Z",
          "wordCount": 557,
          "title": "Sparse Super-Regular Networks. (arXiv:2201.01363v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01449",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Braatz_J/0/1/0/all/0/1\">Jon Braatz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Stephanie Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_J/0/1/0/all/0/1\">Jeanne Shen</a>",
          "description": "In recent years, deep learning has successfully been applied to automate a\nwide variety of tasks in diagnostic histopathology. However, fast and reliable\nlocalization of small-scale regions-of-interest (ROI) has remained a key\nchallenge, as discriminative morphologic features often occupy only a small\nfraction of a gigapixel-scale whole-slide image (WSI). In this paper, we\npropose a sparse WSI analysis method for the rapid identification of high-power\nROI for WSI-level classification. We develop an evaluation framework inspired\nby the early classification literature, in order to quantify the tradeoff\nbetween diagnostic performance and inference time for sparse analytic\napproaches. We test our method on a common but time-consuming task in pathology\n- that of diagnosing gastric intestinal metaplasia (GIM) on hematoxylin and\neosin (H&E)-stained slides from endoscopic biopsy specimens. GIM is a\nwell-known precursor lesion along the pathway to development of gastric cancer.\nWe performed a thorough evaluation of the performance and inference time of our\napproach on a test set of GIM-positive and GIM-negative WSI, finding that our\nmethod successfully detects GIM in all positive WSI, with a WSI-level\nclassification area under the receiver operating characteristic curve (AUC) of\n0.98 and an average precision (AP) of 0.95. Furthermore, we show that our\nmethod can attain these metrics in under one minute on a standard CPU. Our\nresults are applicable toward the goal of developing neural networks that can\neasily be deployed in clinical settings to support pathologists in quickly\nlocalizing and diagnosing small-scale morphologic features in WSI.",
          "link": "http://arxiv.org/abs/2201.01449",
          "publishedOn": "2022-01-07T00:40:43.187Z",
          "wordCount": 694,
          "title": "Deep Learning-Based Sparse Whole-Slide Image Analysis for the Diagnosis of Gastric Intestinal Metaplasia. (arXiv:2201.01449v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nado_Z/0/1/0/all/0/1\">Zachary Nado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Band_N/0/1/0/all/0/1\">Neil Band</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_M/0/1/0/all/0/1\">Mark Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Djolonga_J/0/1/0/all/0/1\">Josip Djolonga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusenberry_M/0/1/0/all/0/1\">Michael W. Dusenberry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farquhar_S/0/1/0/all/0/1\">Sebastian Farquhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qixuan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filos_A/0/1/0/all/0/1\">Angelos Filos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havasi_M/0/1/0/all/0/1\">Marton Havasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1\">Rodolphe Jenatton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jerfel_G/0/1/0/all/0/1\">Ghassen Jerfel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jeremiah Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mariet_Z/0/1/0/all/0/1\">Zelda Mariet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nixon_J/0/1/0/all/0/1\">Jeremy Nixon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padhy_S/0/1/0/all/0/1\">Shreyas Padhy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudner_T/0/1/0/all/0/1\">Tim G. J. Rudner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sbahi_F/0/1/0/all/0/1\">Faris Sbahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yeming Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenzel_F/0/1/0/all/0/1\">Florian Wenzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1\">Kevin Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sculley_D/0/1/0/all/0/1\">D. Sculley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_J/0/1/0/all/0/1\">Jasper Snoek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1\">Dustin Tran</a>",
          "description": "High-quality estimates of uncertainty and robustness are crucial for numerous\nreal-world applications, especially for deep learning which underlies many\ndeployed ML systems. The ability to compare techniques for improving these\nestimates is therefore very important for research and practice alike. Yet,\ncompetitive comparisons of methods are often lacking due to a range of reasons,\nincluding: compute availability for extensive tuning, incorporation of\nsufficiently many baselines, and concrete documentation for reproducibility. In\nthis paper we introduce Uncertainty Baselines: high-quality implementations of\nstandard and state-of-the-art deep learning methods on a variety of tasks. As\nof this writing, the collection spans 19 methods across 9 tasks, each with at\nleast 5 metrics. Each baseline is a self-contained experiment pipeline with\neasily reusable and extendable components. Our goal is to provide immediate\nstarting points for experimentation with new methods or applications.\nAdditionally we provide model checkpoints, experiment outputs as Python\nnotebooks, and leaderboards for comparing results. Code available at\nhttps://github.com/google/uncertainty-baselines.",
          "link": "http://arxiv.org/abs/2106.04015",
          "publishedOn": "2022-01-07T00:40:43.180Z",
          "wordCount": 667,
          "title": "Uncertainty Baselines: Benchmarks for Uncertainty & Robustness in Deep Learning. (arXiv:2106.04015v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.09416",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinxing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qiang Cheng</a>",
          "description": "Feature selection, as a vital dimension reduction technique, reduces data\ndimension by identifying an essential subset of input features, which can\nfacilitate interpretable insights into learning and inference processes.\nAlgorithmic stability is a key characteristic of an algorithm regarding its\nsensitivity to perturbations of input samples. In this paper, we propose an\ninnovative unsupervised feature selection algorithm attaining this stability\nwith provable guarantees. The architecture of our algorithm consists of a\nfeature scorer and a feature selector. The scorer trains a neural network (NN)\nto globally score all the features, and the selector adopts a dependent sub-NN\nto locally evaluate the representation abilities for selecting features.\nFurther, we present algorithmic stability analysis and show that our algorithm\nhas a performance guarantee via a generalization error bound. Extensive\nexperimental results on real-world datasets demonstrate superior generalization\nperformance of our proposed algorithm to strong baseline methods. Also, the\nproperties revealed by our theoretical analysis and the stability of our\nalgorithm-selected features are empirically confirmed.",
          "link": "http://arxiv.org/abs/2010.09416",
          "publishedOn": "2022-01-07T00:40:43.173Z",
          "wordCount": 619,
          "title": "Algorithmic Stability and Generalization of an Unsupervised Feature Selection Algorithm. (arXiv:2010.09416v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01733",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yaldiz_C/0/1/0/all/0/1\">Cem Okan Yaldiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yildiz_Y/0/1/0/all/0/1\">Yildiray Yildiz</a>",
          "description": "This paper proposes a method for modeling human driver interactions that\nrelies on multi-output gaussian processes. The proposed method is developed as\na refinement of the game theoretical hierarchical reasoning approach called\n\"level-k reasoning\" which conventionally assigns discrete levels of behaviors\nto agents. Although it is shown to be an effective modeling tool, the level-k\nreasoning approach may pose undesired constraints for predicting human decision\nmaking due to a limited number (usually 2 or 3) of driver policies it extracts.\nThe proposed approach is put forward to fill this gap in the literature by\nintroducing a continuous domain framework that enables an infinite policy\nspace. By using the approach presented in this paper, more accurate driver\nmodels can be obtained, which can then be employed for creating high fidelity\nsimulation platforms for the validation of autonomous vehicle control\nalgorithms. The proposed method is validated on a real traffic dataset and\ncompared with the conventional level-k approach to demonstrate its\ncontributions and implications.",
          "link": "http://arxiv.org/abs/2201.01733",
          "publishedOn": "2022-01-07T00:40:43.166Z",
          "wordCount": 600,
          "title": "Modeling Human Driver Interactions Using an Infinite Policy Space Through Gaussian Processes. (arXiv:2201.01733v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>",
          "description": "Audio-based automatic speech recognition (ASR) degrades significantly in\nnoisy environments and is particularly vulnerable to interfering speech, as the\nmodel cannot determine which speaker to transcribe. Audio-visual speech\nrecognition (AVSR) systems improve robustness by complementing the audio stream\nwith the visual information that is invariant to noise and helps the model\nfocus on the desired speaker. However, previous AVSR work focused solely on the\nsupervised learning setup; hence the progress was hindered by the amount of\nlabeled data available. In this work, we present a self-supervised AVSR\nframework built upon Audio-Visual HuBERT (AV-HuBERT), a state-of-the-art\naudio-visual speech representation learning model. On the largest available\nAVSR benchmark dataset LRS3, our approach outperforms prior state-of-the-art by\n~50% (28.0% vs. 14.1%) using less than 10% of labeled data (433hr vs. 30hr) in\nthe presence of babble noise, while reducing the WER of an audio-based model by\nover 75% (25.8% vs. 5.8%) on average.",
          "link": "http://arxiv.org/abs/2201.01763",
          "publishedOn": "2022-01-07T00:40:43.159Z",
          "wordCount": 575,
          "title": "Robust Self-Supervised Audio-Visual Speech Recognition. (arXiv:2201.01763v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02416",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Konstantinidis_K/0/1/0/all/0/1\">Konstantinos Konstantinidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthy_A/0/1/0/all/0/1\">Aditya Ramamoorthy</a>",
          "description": "State-of-the-art machine learning models are routinely trained on large-scale\ndistributed clusters. Crucially, such systems can be compromised when some of\nthe computing devices exhibit abnormal (Byzantine) behavior and return\narbitrary results to the parameter server (PS). This behavior may be attributed\nto a plethora of reasons, including system failures and orchestrated attacks.\nExisting work suggests robust aggregation and/or computational redundancy to\nalleviate the effect of distorted gradients. However, most of these schemes are\nineffective when an adversary knows the task assignment and can choose the\nattacked workers judiciously to induce maximal damage. Our proposed method\nAspis assigns gradient computations to worker nodes using a subset-based\nassignment which allows for multiple consistency checks on the behavior of a\nworker node. Examination of the calculated gradients and post-processing\n(clique-finding in an appropriately constructed graph) by the central node\nallows for efficient detection and subsequent exclusion of adversaries from the\ntraining process. We prove the Byzantine resilience and detection guarantees of\nAspis under weak and strong attacks and extensively evaluate the system on\nvarious large-scale training scenarios. The principal metric for our\nexperiments is the test accuracy, for which we demonstrate a significant\nimprovement of about 30% compared to many state-of-the-art approaches on the\nCIFAR-10 dataset. The corresponding reduction of the fraction of corrupted\ngradients ranges from 16% to 99%.",
          "link": "http://arxiv.org/abs/2108.02416",
          "publishedOn": "2022-01-07T00:40:43.150Z",
          "wordCount": null,
          "title": "Aspis: A Robust Detection System for Distributed Learning. (arXiv:2108.02416v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.13460",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alfarisi_O/0/1/0/all/0/1\">Omar Alfarisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raza_A/0/1/0/all/0/1\">Aikifa Raza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouzzane_D/0/1/0/all/0/1\">Djamel Ouzzane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongxia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sassi_M/0/1/0/all/0/1\">Mohamed Sassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tiejun Zhang</a>",
          "description": "Permeability has a dominant influence on the flow properties of a natural\nfluid. Lattice Boltzmann simulator determines permeability from the nano and\nmicropore network. The simulator holds millions of flow dynamics calculations\nwith its accumulated errors and high consumption of computing power. To\nefficiently and consistently predict permeability, we propose a morphology\ndecoder, a parallel and serial flow reconstruction of machine learning\nsegmented heterogeneous Cretaceous texture from 3D micro computerized\ntomography and nuclear magnetic resonance images. For 3D vision, we introduce\ncontrollable-measurable-volume as new supervised segmentation, in which a\nunique set of voxel intensity corresponds to grain and pore throat sizes. The\nmorphology decoder demarks and aggregates the morphologies boundaries in a\nnovel way to produce permeability. Morphology decoder method consists of five\nnovel processes, which describes in this paper, these novel processes are: (1)\nGeometrical 3D Permeability, (2) Machine Learning guided 3D Properties\nRecognition of Rock Morphology, (3) 3D Image Properties Integration Model for\nPermeability, (4) MRI Permeability Imager, and (5) Morphology Decoder (the\nprocess that integrates the other four novel processes).",
          "link": "http://arxiv.org/abs/2111.13460",
          "publishedOn": "2022-01-07T00:40:43.150Z",
          "wordCount": null,
          "title": "Morphology Decoder: A Machine Learning Guided 3D Vision Quantifying Heterogenous Rock Permeability for Planetary Surveillance and Robotic Functions. (arXiv:2111.13460v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shih_A/0/1/0/all/0/1\">Andy Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1\">Dorsa Sadigh</a>",
          "description": "While advances in multi-agent learning have enabled the training of\nincreasingly complex agents, most existing techniques produce a final policy\nthat is not designed to adapt to a new partner's strategy. However, we would\nlike our AI agents to adjust their strategy based on the strategies of those\naround them. In this work, we study the problem of conditional multi-agent\nimitation learning, where we have access to joint trajectory demonstrations at\ntraining time, and we must interact with and adapt to new partners at test\ntime. This setting is challenging because we must infer a new partner's\nstrategy and adapt our policy to that strategy, all without knowledge of the\nenvironment reward or dynamics. We formalize this problem of conditional\nmulti-agent imitation learning, and propose a novel approach to address the\ndifficulties of scalability and data scarcity. Our key insight is that\nvariations across partners in multi-agent games are often highly structured,\nand can be represented via a low-rank subspace. Leveraging tools from tensor\ndecomposition, our model learns a low-rank subspace over ego and partner agent\nstrategies, then infers and adapts to a new partner strategy by interpolating\nin the subspace. We experiments with a mix of collaborative tasks, including\nbandits, particle, and Hanabi environments. Additionally, we test our\nconditional policies against real human partners in a user study on the\nOvercooked game. Our model adapts better to new partners compared to baselines,\nand robustly handles diverse settings ranging from discrete/continuous actions\nand static/online evaluation with AI/human partners.",
          "link": "http://arxiv.org/abs/2201.01448",
          "publishedOn": "2022-01-07T00:40:43.032Z",
          "wordCount": null,
          "title": "Conditional Imitation Learning for Multi-Agent Games. (arXiv:2201.01448v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ororbia_A/0/1/0/all/0/1\">Alexander Ororbia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kifer_D/0/1/0/all/0/1\">Daniel Kifer</a>",
          "description": "Neural generative models can be used to learn complex probability\ndistributions from data, to sample from them, and to produce probability\ndensity estimates. We propose a computational framework for developing neural\ngenerative models inspired by the theory of predictive processing in the brain.\nAccording to predictive processing theory, the neurons in the brain form a\nhierarchy in which neurons in one level form expectations about sensory inputs\nfrom another level. These neurons update their local models based on\ndifferences between their expectations and the observed signals. In a similar\nway, artificial neurons in our generative models predict what neighboring\nneurons will do, and adjust their parameters based on how well the predictions\nmatched reality. In this work, we show that the neural generative models\nlearned within our framework perform well in practice across several benchmark\ndatasets and metrics and either remain competitive with or significantly\noutperform other generative models with similar functionality (such as the\nvariational auto-encoder).",
          "link": "http://arxiv.org/abs/2012.03405",
          "publishedOn": "2022-01-07T00:40:43.032Z",
          "wordCount": null,
          "title": "The Neural Coding Framework for Learning Generative Models. (arXiv:2012.03405v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pietikainen_M/0/1/0/all/0/1\">Matti Pietik&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silven_O/0/1/0/all/0/1\">Olli Silven</a>",
          "description": "Artificial intelligence (AI) has become a part of everyday conversation and\nour lives. It is considered as the new electricity that is revolutionizing the\nworld. AI is heavily invested in both industry and academy. However, there is\nalso a lot of hype in the current AI debate. AI based on so-called deep\nlearning has achieved impressive results in many problems, but its limits are\nalready visible. AI has been under research since the 1940s, and the industry\nhas seen many ups and downs due to over-expectations and related\ndisappointments that have followed.\n\nThe purpose of this book is to give a realistic picture of AI, its history,\nits potential and limitations. We believe that AI is a helper, not a ruler of\nhumans. We begin by describing what AI is and how it has evolved over the\ndecades. After fundamentals, we explain the importance of massive data for the\ncurrent mainstream of artificial intelligence. The most common representations\nfor AI, methods, and machine learning are covered. In addition, the main\napplication areas are introduced. Computer vision has been central to the\ndevelopment of AI. The book provides a general introduction to computer vision,\nand includes an exposure to the results and applications of our own research.\nEmotions are central to human intelligence, but little use has been made in AI.\nWe present the basics of emotional intelligence and our own research on the\ntopic. We discuss super-intelligence that transcends human understanding,\nexplaining why such achievement seems impossible on the basis of present\nknowledge,and how AI could be improved. Finally, a summary is made of the\ncurrent state of AI and what to do in the future. In the appendix, we look at\nthe development of AI education, especially from the perspective of contents at\nour own university.",
          "link": "http://arxiv.org/abs/2201.01466",
          "publishedOn": "2022-01-07T00:40:43.031Z",
          "wordCount": null,
          "title": "Challenges of Artificial Intelligence -- From Machine Learning and Computer Vision to Emotional Intelligence. (arXiv:2201.01466v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thyssens_D/0/1/0/all/0/1\">Daniela Thyssens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falkner_J/0/1/0/all/0/1\">Jonas Falkner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_Thieme_L/0/1/0/all/0/1\">Lars Schmidt-Thieme</a>",
          "description": "Learning to solve combinatorial optimization problems, such as the vehicle\nrouting problem, offers great computational advantages over classical\noperations research solvers and heuristics. The recently developed deep\nreinforcement learning approaches either improve an initially given solution\niteratively or sequentially construct a set of individual tours. However, most\nof the existing learning-based approaches are not able to work for a fixed\nnumber of vehicles and thus bypass the complex assignment problem of the\ncustomers onto an apriori given number of available vehicles. On the other\nhand, this makes them less suitable for real applications, as many logistic\nservice providers rely on solutions provided for a specific bounded fleet size\nand cannot accommodate short term changes to the number of vehicles. In\ncontrast we propose a powerful supervised deep learning framework that\nconstructs a complete tour plan from scratch while respecting an apriori fixed\nnumber of available vehicles. In combination with an efficient post-processing\nscheme, our supervised approach is not only much faster and easier to train but\nalso achieves competitive results that incorporate the practical aspect of\nvehicle costs. In thorough controlled experiments we compare our method to\nmultiple state-of-the-art approaches where we demonstrate stable performance,\nwhile utilizing less vehicles and shed some light on existent inconsistencies\nin the experimentation protocols of the related work.",
          "link": "http://arxiv.org/abs/2201.01529",
          "publishedOn": "2022-01-07T00:40:43.031Z",
          "wordCount": null,
          "title": "Supervised Permutation Invariant Networks for Solving the CVRP with Bounded Fleet Size. (arXiv:2201.01529v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Milosevic_N/0/1/0/all/0/1\">Nikola Milosevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thielemann_W/0/1/0/all/0/1\">Wolfgang Thielemann</a>",
          "description": "Biomedical research is growing in such an exponential pace that scientists,\nresearchers and practitioners are no more able to cope with the amount of\npublished literature in the domain. The knowledge presented in the literature\nneeds to be systematized in such a ways that claims and hypothesis can be\neasily found, accessed and validated. Knowledge graphs can provide such\nframework for semantic knowledge representation from literature. However, in\norder to build knowledge graph, it is necessary to extract knowledge in form of\nrelationships between biomedical entities and normalize both entities and\nrelationship types. In this paper, we present and compare few rule-based and\nmachine learning-based (Naive Bayes, Random Forests as examples of traditional\nmachine learning methods and T5-based model as an example of modern deep\nlearning) methods for scalable relationship extraction from biomedical\nliterature for the integration into the knowledge graphs. We examine how\nresilient are these various methods to unbalanced and fairly small datasets,\nshowing that T5 model handles well both small datasets, due to its pre-training\non large C4 dataset as well as unbalanced data. The best performing model was\nT5 model fine-tuned on balanced data, with reported F1-score of 0.88.",
          "link": "http://arxiv.org/abs/2201.01647",
          "publishedOn": "2022-01-07T00:40:43.031Z",
          "wordCount": null,
          "title": "Relationship extraction for knowledge graph creation from biomedical literature. (arXiv:2201.01647v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.05319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_U/0/1/0/all/0/1\">Uiwon Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heeseung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1\">Dahuin Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1\">Hyemi Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyungyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>",
          "description": "Generative adversarial networks (GANs) with clustered latent spaces can\nperform conditional generation in a completely unsupervised manner. In the real\nworld, the salient attributes of unlabeled data can be imbalanced. However,\nmost of existing unsupervised conditional GANs cannot cluster attributes of\nthese data in their latent spaces properly because they assume uniform\ndistributions of the attributes. To address this problem, we theoretically\nderive Stein latent optimization that provides reparameterizable gradient\nestimations of the latent distribution parameters assuming a Gaussian mixture\nprior in a continuous latent space. Structurally, we introduce an encoder\nnetwork and novel unsupervised conditional contrastive loss to ensure that data\ngenerated from a single mixture component represent a single attribute. We\nconfirm that the proposed method, named Stein Latent Optimization for GANs\n(SLOGAN), successfully learns balanced or imbalanced attributes and achieves\nstate-of-the-art unsupervised conditional generation performance even in the\nabsence of attribute information (e.g., the imbalance ratio). Moreover, we\ndemonstrate that the attributes to be learned can be manipulated using a small\namount of probe data.",
          "link": "http://arxiv.org/abs/2106.05319",
          "publishedOn": "2022-01-07T00:40:43.031Z",
          "wordCount": null,
          "title": "Stein Latent Optimization for Generative Adversarial Networks. (arXiv:2106.05319v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.01620",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Mishra_Sharma_S/0/1/0/all/0/1\">Siddharth Mishra-Sharma</a>",
          "description": "Astrometry -- the precise measurement of positions and motions of celestial\nobjects -- has emerged as a promising avenue for characterizing the dark matter\npopulation in our Galaxy. By leveraging recent advances in simulation-based\ninference and neural network architectures, we introduce a novel method to\nsearch for global dark matter-induced gravitational lensing signatures in\nastrometric datasets. Our method based on neural likelihood-ratio estimation\nshows significantly enhanced sensitivity to a cold dark matter population and\nmore favorable scaling with measurement noise compared to existing approaches\nbased on two-point correlation statistics. We demonstrate the real-world\nviability of our method by showing it to be robust to non-trivial modeled as\nwell as unmodeled noise features expected in astrometric measurements. This\nestablishes machine learning as a powerful tool for characterizing dark matter\nusing astrometric data.",
          "link": "http://arxiv.org/abs/2110.01620",
          "publishedOn": "2022-01-07T00:40:43.031Z",
          "wordCount": null,
          "title": "Inferring dark matter substructure with astrometric lensing beyond the power spectrum. (arXiv:2110.01620v2 [astro-ph.CO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Saurabh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_N/0/1/0/all/0/1\">Nishant Sinha</a>",
          "description": "TryOnGAN is a recent virtual try-on approach, which generates highly\nrealistic images and outperforms most previous approaches. In this article, we\nreproduce the TryOnGAN implementation and probe it along diverse angles: impact\nof transfer learning, variants of conditioning image generation with poses and\nproperties of latent space interpolation. Some of these facets have never been\nexplored in literature earlier. We find that transfer helps training initially\nbut gains are lost as models train longer and pose conditioning via\nconcatenation performs better. The latent space self-disentangles the pose and\nthe style features and enables style transfer across poses. Our code and models\nare available in open source.",
          "link": "http://arxiv.org/abs/2201.01703",
          "publishedOn": "2022-01-07T00:40:43.030Z",
          "wordCount": null,
          "title": "Probing TryOnGAN. (arXiv:2201.01703v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.06326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bauerle_A/0/1/0/all/0/1\">Alex B&#xe4;uerle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albus_P/0/1/0/all/0/1\">Patrick Albus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stork_R/0/1/0/all/0/1\">Raphael St&#xf6;rk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seufert_T/0/1/0/all/0/1\">Tina Seufert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1\">Timo Ropinski</a>",
          "description": "Due to the success of deep learning (DL) and its growing job market, students\nand researchers from many areas are interested in learning about DL\ntechnologies. Visualization has proven to be of great help during this learning\nprocess. While most current educational visualizations are targeted towards one\nspecific architecture or use case, recurrent neural networks (RNNs), which are\ncapable of processing sequential data, are not covered yet. This is despite the\nfact that tasks on sequential data, such as text and function analysis, are at\nthe forefront of DL research. Therefore, we propose exploRNN, the first\ninteractively explorable educational visualization for RNNs. On the basis of\nmaking learning easier and more fun, we define educational objectives targeted\ntowards understanding RNNs. We use these objectives to form guidelines for the\nvisual design process. By means of exploRNN, which is accessible online, we\nprovide an overview of the training process of RNNs at a coarse level, while\nalso allowing a detailed inspection of the data flow within LSTM cells. In an\nempirical study, we assessed 37 subjects in a between-subjects design to\ninvestigate the learning outcomes and cognitive load of exploRNN compared to a\nclassic text-based learning environment. While learners in the text group are\nahead in superficial knowledge acquisition, exploRNN is particularly helpful\nfor deeper understanding of the learning content. In addition, the complex\ncontent in exploRNN is perceived as significantly easier and causes less\nextraneous load than in the text group. The study shows that for difficult\nlearning material such as recurrent networks, where deep understanding is\nimportant, interactive visualizations such as exploRNN can be helpful.",
          "link": "http://arxiv.org/abs/2012.06326",
          "publishedOn": "2022-01-07T00:40:43.030Z",
          "wordCount": null,
          "title": "exploRNN: Understanding Recurrent Neural Networks through Visual Exploration. (arXiv:2012.06326v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.11800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ren Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qiannan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_G/0/1/0/all/0/1\">Guanqun Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1\">Fang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>",
          "description": "Knowledge Graph Embedding (KGE) aims to learn representations for entities\nand relations. Most KGE models have gained great success, especially on\nextrapolation scenarios. Specifically, given an unseen triple (h, r, t), a\ntrained model can still correctly predict t from (h, r, ?), or h from (?, r,\nt), such extrapolation ability is impressive. However, most existing KGE works\nfocus on the design of delicate triple modeling function, which mainly tells us\nhow to measure the plausibility of observed triples, but offers limited\nexplanation of why the methods can extrapolate to unseen data, and what are the\nimportant factors to help KGE extrapolate. Therefore in this work, we attempt\nto study the KGE extrapolation of two problems: 1. How does KGE extrapolate to\nunseen data? 2. How to design the KGE model with better extrapolation ability?\nFor the problem 1, we first discuss the impact factors for extrapolation and\nfrom relation, entity and triple level respectively, propose three Semantic\nEvidences (SEs), which can be observed from train set and provide important\nsemantic information for extrapolation. Then we verify the effectiveness of SEs\nthrough extensive experiments on several typical KGE methods. For the problem\n2, to make better use of the three levels of SE, we propose a novel GNN-based\nKGE model, called Semantic Evidence aware Graph Neural Network (SE-GNN). In\nSE-GNN, each level of SE is modeled explicitly by the corresponding neighbor\npattern, and merged sufficiently by the multi-layer aggregation, which\ncontributes to obtaining more extrapolative knowledge representation. Finally,\nthrough extensive experiments on FB15k-237 and WN18RR datasets, we show that\nSE-GNN achieves state-of-the-art performance on Knowledge Graph Completion task\nand performs a better extrapolation ability.",
          "link": "http://arxiv.org/abs/2109.11800",
          "publishedOn": "2022-01-07T00:40:43.030Z",
          "wordCount": null,
          "title": "How Does Knowledge Graph Embedding Extrapolate to Unseen Data: a Semantic Evidence View. (arXiv:2109.11800v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.15256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takeishi_Y/0/1/0/all/0/1\">Yoshinari Takeishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iida_M/0/1/0/all/0/1\">Masazumi Iida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeuchi_J/0/1/0/all/0/1\">Jun&#x27;ichi Takeuchi</a>",
          "description": "We argue the Fisher information matrix (FIM) of one hidden layer networks\nwith the ReLU activation function. Let $W$ denote the $d \\times p$ weight\nmatrix from the $d$-dimensional input to the hidden layer consisting of $p$\nneurons, and $v$ the $p$-dimensional weight vector from the hidden layer to the\nscalar output. We focus on the FIM of $v$, which we denote as $I$. When $p$ is\nlarge, under certain conditions, the following approximately holds. 1) There\nare three major clusters in the eigenvalue distribution. 2) Since $I$ is\nnon-negative owing to the ReLU, the first eigenvalue is the Perron-Frobenius\neigenvalue. 3) For the cluster of the next maximum values, the eigenspace is\nspanned by the row vectors of $W$. 4) The direct sum of the eigenspace of the\nfirst eigenvalue and that of the third cluster is spanned by the set of all the\nvectors obtained as the Hadamard product of any pair of the row vectors of $W$.\nWe confirmed by numerical simulation that the above is approximately correct\nwhen the number of hidden nodes is about 10000.",
          "link": "http://arxiv.org/abs/2111.15256",
          "publishedOn": "2022-01-07T00:40:43.030Z",
          "wordCount": null,
          "title": "Approximate Spectral Decomposition of Fisher Information Matrix for Simple ReLU Networks. (arXiv:2111.15256v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_L/0/1/0/all/0/1\">Long Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>",
          "description": "This work studies the bias issue of pseudo-labeling, a natural phenomenon\nthat widely occurs but often overlooked by prior research. Pseudo-labels are\ngenerated when a classifier trained on source data is transferred to unlabeled\ntarget data. We observe heavy long-tailed pseudo-labels when a semi-supervised\nlearning model FixMatch predicts labels on the unlabeled set even though the\nunlabeled data is curated to be balanced. Without intervention, the training\nmodel inherits the bias from the pseudo-labels and end up being sub-optimal. To\neliminate the model bias, we propose a simple yet effective method DebiasMatch,\ncomprising of an adaptive debiasing module and an adaptive marginal loss. The\nstrength of debiasing and the size of margins can be automatically adjusted by\nmaking use of an online updated queue. Benchmarked on ImageNet-1K, DebiasMatch\nsignificantly outperforms previous state-of-the-arts by more than 26% and 8.7%\non semi-supervised learning (0.2% annotated data) and zero-shot learning tasks\nrespectively.",
          "link": "http://arxiv.org/abs/2201.01490",
          "publishedOn": "2022-01-07T00:40:43.029Z",
          "wordCount": null,
          "title": "Debiased Learning from Naturally Imbalanced Pseudo-Labels for Zero-Shot and Semi-Supervised Learning. (arXiv:2201.01490v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.09430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinxing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qiang Cheng</a>",
          "description": "Feature selection reduces the dimensionality of data by identifying a subset\nof the most informative features. In this paper, we propose an innovative\nframework for unsupervised feature selection, called fractal autoencoders\n(FAE). It trains a neural network to pinpoint informative features for global\nexploring of representability and for local excavating of diversity.\nArchitecturally, FAE extends autoencoders by adding a one-to-one scoring layer\nand a small sub-neural network for feature selection in an unsupervised\nfashion. With such a concise architecture, FAE achieves state-of-the-art\nperformances; extensive experimental results on fourteen datasets, including\nvery high-dimensional data, have demonstrated the superiority of FAE over\nexisting contemporary methods for unsupervised feature selection. In\nparticular, FAE exhibits substantial advantages on gene expression data\nexploration, reducing measurement cost by about $15$\\% over the widely used\nL1000 landmark genes. Further, we show that the FAE framework is easily\nextensible with an application.",
          "link": "http://arxiv.org/abs/2010.09430",
          "publishedOn": "2022-01-07T00:40:43.029Z",
          "wordCount": null,
          "title": "Fractal Autoencoders for Feature Selection. (arXiv:2010.09430v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Farui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Shichao Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_M/0/1/0/all/0/1\">Meng Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>",
          "description": "GPUs are widely used to accelerate the training of machine learning\nworkloads. As modern machine learning models become increasingly larger, they\nrequire a longer time to train, leading to higher GPU energy consumption. This\npaper presents GPOEO, an online GPU energy optimization framework for machine\nlearning training workloads. GPOEO dynamically determines the optimal energy\nconfiguration by employing novel techniques for online measurement,\nmulti-objective prediction modeling, and search optimization. To characterize\nthe target workload behavior, GPOEO utilizes GPU performance counters. To\nreduce the performance counter profiling overhead, it uses an analytical model\nto detect the training iteration change and only collects performance counter\ndata when an iteration shift is detected. GPOEO employs multi-objective models\nbased on gradient boosting and a local search algorithm to find a trade-off\nbetween execution time and energy consumption. We evaluate the GPOEO by\napplying it to 71 machine learning workloads from two AI benchmark suites\nrunning on an NVIDIA RTX3080Ti GPU. Compared with the NVIDIA default scheduling\nstrategy, GPOEO delivers a mean energy saving of 16.2% with a modest average\nexecution time increase of 5.1%.",
          "link": "http://arxiv.org/abs/2201.01684",
          "publishedOn": "2022-01-07T00:40:43.028Z",
          "wordCount": null,
          "title": "Dynamic GPU Energy Optimization for Machine Learning Training Workloads. (arXiv:2201.01684v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2007.12098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_N/0/1/0/all/0/1\">Neha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Karren Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uhler_C/0/1/0/all/0/1\">Caroline Uhler</a>",
          "description": "In this paper, we present Super-OT, a novel approach to computational lineage\ntracing that combines a supervised learning framework with optimal transport\nbased on Generative Adversarial Networks (GANs). Unlike previous approaches to\nlineage tracing, Super-OT has the flexibility to integrate paired data. We\nbenchmark Super-OT based on single-cell RNA-seq data against Waddington-OT, a\npopular approach for lineage tracing that also employs optimal transport. We\nshow that Super-OT achieves gains over Waddington-OT in predicting the class\noutcome of cells during differentiation, since it allows the integration of\nadditional information during training.",
          "link": "http://arxiv.org/abs/2007.12098",
          "publishedOn": "2022-01-07T00:40:43.027Z",
          "wordCount": 573,
          "title": "Optimal Transport using GANs for Lineage Tracing. (arXiv:2007.12098v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01666",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mai_V/0/1/0/all/0/1\">Vincent Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mani_K/0/1/0/all/0/1\">Kaustubh Mani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paull_L/0/1/0/all/0/1\">Liam Paull</a>",
          "description": "In model-free deep reinforcement learning (RL) algorithms, using noisy value\nestimates to supervise policy evaluation and optimization is detrimental to the\nsample efficiency. As this noise is heteroscedastic, its effects can be\nmitigated using uncertainty-based weights in the optimization process. Previous\nmethods rely on sampled ensembles, which do not capture all aspects of\nuncertainty. We provide a systematic analysis of the sources of uncertainty in\nthe noisy supervision that occurs in RL, and introduce inverse-variance RL, a\nBayesian framework which combines probabilistic ensembles and Batch Inverse\nVariance weighting. We propose a method whereby two complementary uncertainty\nestimation methods account for both the Q-value and the environment\nstochasticity to better mitigate the negative impacts of noisy supervision. Our\nresults show significant improvement in terms of sample efficiency on discrete\nand continuous control tasks.",
          "link": "http://arxiv.org/abs/2201.01666",
          "publishedOn": "2022-01-07T00:40:43.002Z",
          "wordCount": 558,
          "title": "Sample Efficient Deep Reinforcement Learning via Uncertainty Estimation. (arXiv:2201.01666v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01689",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Davison_A/0/1/0/all/0/1\">Andrew Davison</a>",
          "description": "A common approach to solving tasks, such as node classification or link\nprediction, on a large network begins by learning a Euclidean embedding of the\nnodes of the network, from which regular machine learning methods can be\napplied. For unsupervised random walk methods such as DeepWalk and node2vec,\nadding a $\\ell_2$ penalty on the embedding vectors to the loss leads to\nimproved downstream task performance. In this paper we study the effects of\nthis regularization and prove that, under exchangeability assumptions on the\ngraph, it asymptotically leads to learning a nuclear-norm-type penalized\ngraphon. In particular, the exact form of the penalty depends on the choice of\nsubsampling method used within stochastic gradient descent to learn the\nembeddings. We also illustrate empirically that concatenating node covariates\nto $\\ell_2$ regularized node2vec embeddings leads to comparable, if not\nsuperior, performance to methods which incorporate node covariates and the\nnetwork structure in a non-linear manner.",
          "link": "http://arxiv.org/abs/2201.01689",
          "publishedOn": "2022-01-07T00:40:42.996Z",
          "wordCount": 575,
          "title": "Asymptotics of $\\ell_2$ Regularized Network Embeddings. (arXiv:2201.01689v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_A/0/1/0/all/0/1\">Aiqing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Pengzhan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yifa Tang</a>",
          "description": "Measure-preserving neural networks are well-developed invertible models,\nhowever, their approximation capabilities remain unexplored. This paper\nrigorously analyses the approximation capabilities of existing\nmeasure-preserving neural networks including NICE and RevNets. It is shown that\nfor compact $U \\subset \\R^D$ with $D\\geq 2$, the measure-preserving neural\nnetworks are able to approximate arbitrary measure-preserving map $\\psi: U\\to\n\\R^D$ which is bounded and injective in the $L^p$-norm. In particular, any\ncontinuously differentiable injective map with $\\pm 1$ determinant of Jacobian\nare measure-preserving, thus can be approximated.",
          "link": "http://arxiv.org/abs/2106.10911",
          "publishedOn": "2022-01-07T00:40:42.989Z",
          "wordCount": 520,
          "title": "Approximation capabilities of measure-preserving neural networks. (arXiv:2106.10911v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03410",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Farrell_S/0/1/0/all/0/1\">Spencer Farrell</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Mitnitski_A/0/1/0/all/0/1\">Arnold Mitnitski</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Rockwood_K/0/1/0/all/0/1\">Kenneth Rockwood</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Rutenberg_A/0/1/0/all/0/1\">Andrew Rutenberg</a>",
          "description": "We have built a computational model for individual aging trajectories of\nhealth and survival, which contains physical, functional, and biological\nvariables, and is conditioned on demographic, lifestyle, and medical background\ninformation. We combine techniques of modern machine learning with an\ninterpretable interaction network, where health variables are coupled by\nexplicit pair-wise interactions within a stochastic dynamical system. Our\ndynamic joint interpretable network (DJIN) model is scalable to large\nlongitudinal data sets, is predictive of individual high-dimensional health\ntrajectories and survival from baseline health states, and infers an\ninterpretable network of directed interactions between the health variables.\nThe network identifies plausible physiological connections between health\nvariables as well as clusters of strongly connected health variables. We use\nEnglish Longitudinal Study of Aging (ELSA) data to train our model and show\nthat it performs better than multiple dedicated linear models for health\noutcomes and survival. We compare our model with flexible lower-dimensional\nlatent-space models to explore the dimensionality required to accurately model\naging health outcomes. Our DJIN model can be used to generate synthetic\nindividuals that age realistically, to impute missing data, and to simulate\nfuture aging outcomes given arbitrary initial health states.",
          "link": "http://arxiv.org/abs/2105.03410",
          "publishedOn": "2022-01-07T00:40:42.982Z",
          "wordCount": 632,
          "title": "Interpretable machine learning for high-dimensional trajectories of aging health. (arXiv:2105.03410v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.13264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1\">Rishabh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarzer_M/0/1/0/all/0/1\">Max Schwarzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1\">Pablo Samuel Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellemare_M/0/1/0/all/0/1\">Marc G. Bellemare</a>",
          "description": "Deep reinforcement learning (RL) algorithms are predominantly evaluated by\ncomparing their relative performance on a large suite of tasks. Most published\nresults on deep RL benchmarks compare point estimates of aggregate performance\nsuch as mean and median scores across tasks, ignoring the statistical\nuncertainty implied by the use of a finite number of training runs. Beginning\nwith the Arcade Learning Environment (ALE), the shift towards\ncomputationally-demanding benchmarks has led to the practice of evaluating only\na small number of runs per task, exacerbating the statistical uncertainty in\npoint estimates. In this paper, we argue that reliable evaluation in the few\nrun deep RL regime cannot ignore the uncertainty in results without running the\nrisk of slowing down progress in the field. We illustrate this point using a\ncase study on the Atari 100k benchmark, where we find substantial discrepancies\nbetween conclusions drawn from point estimates alone versus a more thorough\nstatistical analysis. With the aim of increasing the field's confidence in\nreported results with a handful of runs, we advocate for reporting interval\nestimates of aggregate performance and propose performance profiles to account\nfor the variability in results, as well as present more robust and efficient\naggregate metrics, such as interquartile mean scores, to achieve small\nuncertainty in results. Using such statistical tools, we scrutinize performance\nevaluations of existing algorithms on other widely used RL benchmarks including\nthe ALE, Procgen, and the DeepMind Control Suite, again revealing discrepancies\nin prior comparisons. Our findings call for a change in how we evaluate\nperformance in deep RL, for which we present a more rigorous evaluation\nmethodology, accompanied with an open-source library rliable, to prevent\nunreliable results from stagnating the field.",
          "link": "http://arxiv.org/abs/2108.13264",
          "publishedOn": "2022-01-07T00:40:42.976Z",
          "wordCount": 780,
          "title": "Deep Reinforcement Learning at the Edge of the Statistical Precipice. (arXiv:2108.13264v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sangalli_S/0/1/0/all/0/1\">Sara Sangalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdil_E/0/1/0/all/0/1\">Ertunc Erdil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoetker_A/0/1/0/all/0/1\">Andreas Hoetker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donati_O/0/1/0/all/0/1\">Olivio Donati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konukoglu_E/0/1/0/all/0/1\">Ender Konukoglu</a>",
          "description": "Deep neural networks (DNNs) are notorious for making more mistakes for the\nclasses that have substantially fewer samples than the others during training.\nSuch class imbalance is ubiquitous in clinical applications and very crucial to\nhandle because the classes with fewer samples most often correspond to critical\ncases (e.g., cancer) where misclassifications can have severe consequences. Not\nto miss such cases, binary classifiers need to be operated at high True\nPositive Rates (TPRs) by setting a higher threshold, but this comes at the cost\nof very high False Positive Rates (FPRs) for problems with class imbalance.\nExisting methods for learning under class imbalance most often do not take this\ninto account. We argue that prediction accuracy should be improved by\nemphasizing reducing FPRs at high TPRs for problems where misclassification of\nthe positive, i.e. critical, class samples are associated with higher cost. To\nthis end, we pose the training of a DNN for binary classification as a\nconstrained optimization problem and introduce a novel constraint that can be\nused with existing loss functions to enforce maximal area under the ROC curve\n(AUC) through prioritizing FPR reduction at high TPR. We solve the resulting\nconstrained optimization problem using an Augmented Lagrangian method (ALM).\nGoing beyond binary, we also propose two possible extensions of the proposed\nconstraint for multi-class classification problems. We present experimental\nresults for image-based binary and multi-class classification applications\nusing an in-house medical imaging dataset, CIFAR10, and CIFAR100. Our results\ndemonstrate that the proposed method improves the baselines in majority of the\ncases by attaining higher accuracy on critical classes while reducing the\nmisclassification rate for the non-critical class samples.",
          "link": "http://arxiv.org/abs/2102.12894",
          "publishedOn": "2022-01-07T00:40:42.945Z",
          "wordCount": 761,
          "title": "Constrained Optimization to Train Neural Networks on Critical and Under-Represented Classes. (arXiv:2102.12894v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.02542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soremekun_E/0/1/0/all/0/1\">Ezekiel Soremekun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udeshi_S/0/1/0/all/0/1\">Sakshi Udeshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1\">Sudipta Chattopadhyay</a>",
          "description": "Software often produces biased outputs. In particular, machine learning (ML)\nbased software are known to produce erroneous predictions when processing\ndiscriminatory inputs. Such unfair program behavior can be caused by societal\nbias. In the last few years, Amazon, Microsoft and Google have provided\nsoftware services that produce unfair outputs, mostly due to societal bias\n(e.g. gender or race). In such events, developers are saddled with the task of\nconducting fairness testing. Fairness testing is challenging; developers are\ntasked with generating discriminatory inputs that reveal and explain biases.\n\nWe propose a grammar-based fairness testing approach (called ASTRAEA) which\nleverages context-free grammars to generate discriminatory inputs that reveal\nfairness violations in software systems. Using probabilistic grammars, ASTRAEA\nalso provides fault diagnosis by isolating the cause of observed software bias.\nASTRAEA's diagnoses facilitate the improvement of ML fairness.\n\nASTRAEA was evaluated on 18 software systems that provide three major natural\nlanguage processing (NLP) services. In our evaluation, ASTRAEA generated\nfairness violations with a rate of ~18%. ASTRAEA generated over 573K\ndiscriminatory test cases and found over 102K fairness violations. Furthermore,\nASTRAEA improves software fairness by ~76%, via model-retraining.",
          "link": "http://arxiv.org/abs/2010.02542",
          "publishedOn": "2022-01-07T00:40:42.937Z",
          "wordCount": 653,
          "title": "Astraea: Grammar-based Fairness Testing. (arXiv:2010.02542v4 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wenju Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1\">Yangli-ao Geng</a>",
          "description": "The exemplar-free class incremental learning requires classification models\nto learn new class knowledge incrementally without retaining any old samples.\nRecently, the framework based on parallel one-class classifiers (POC), which\ntrains a one-class classifier (OCC) independently for each category, has\nattracted extensive attention, since it can naturally avoid catastrophic\nforgetting. POC, however, suffers from weak discriminability and comparability\ndue to its independent training strategy for different OOCs. To meet this\nchallenge, we propose a new framework, named Discriminative and Comparable\nOne-class classifiers for Incremental Learning (DisCOIL). DisCOIL follows the\nbasic principle of POC, but it adopts variational auto-encoders (VAE) instead\nof other well-established one-class classifiers (e.g. deep SVDD), because a\ntrained VAE can not only identify the probability of an input sample belonging\nto a class but also generate pseudo samples of the class to assist in learning\nnew tasks. With this advantage, DisCOIL trains a new-class VAE in contrast with\nthe old-class VAEs, which forces the new-class VAE to reconstruct better for\nnew-class samples but worse for the old-class pseudo samples, thus enhancing\nthe comparability. Furthermore, DisCOIL introduces a hinge reconstruction loss\nto ensure the discriminability. We evaluate our method extensively on MNIST,\nCIFAR10, and Tiny-ImageNet. The experimental results show that DisCOIL achieves\nstate-of-the-art performance.",
          "link": "http://arxiv.org/abs/2201.01488",
          "publishedOn": "2022-01-07T00:40:42.930Z",
          "wordCount": 637,
          "title": "Exemplar-free Class Incremental Learning via Discriminative and Comparable One-class Classifiers. (arXiv:2201.01488v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.09762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianwu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangopadhyay_A/0/1/0/all/0/1\">Aryya Gangopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busart_C/0/1/0/all/0/1\">Carl E. Busart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_J/0/1/0/all/0/1\">Jade Freeman</a>",
          "description": "Cloud computing has become a major approach to enable reproducible\ncomputational experiments because of its support of on-demand hardware and\nsoftware resource provisioning. Yet there are still two main difficulties in\nreproducing big data applications in the cloud. The first is how to automate\nend-to-end execution of big data analytics in the cloud including virtual\ndistributed environment provisioning, network and security group setup, and big\ndata analytics pipeline description and execution. The second is an application\ndeveloped for one cloud, such as AWS or Azure, is difficult to reproduce in\nanother cloud, a.k.a. vendor lock-in problem. To tackle these problems, we\nleverage serverless computing and containerization techniques for automatic\nscalable big data application execution and reproducibility, and utilize the\nadapter design pattern to enable application portability and reproducibility\nacross different clouds. Based on the approach, we propose and develop an\nopen-source toolkit that supports 1) on-demand distributed hardware and\nsoftware environment provisioning, 2) automatic data and configuration storage\nfor each execution, 3) flexible client modes based on user preferences, 4)\nexecution history query, and 5) simple reproducibility of existing executions\nin the same environment or a different environment. We did extensive\nexperiments on both AWS and Azure using three big data analytics applications\nthat run on a virtual CPU/GPU cluster. Three main behaviors of our toolkit were\nbenchmarked: i) execution overhead ratio for reproducibility support, ii)\ndifferences of reproducing the same application on AWS and Azure in terms of\nexecution time, budgetary cost and cost-performance ratio, iii) differences\nbetween scale-out and scale-up approach for the same application on AWS and\nAzure.",
          "link": "http://arxiv.org/abs/2112.09762",
          "publishedOn": "2022-01-07T00:40:42.914Z",
          "wordCount": 714,
          "title": "Reproducible and Portable Big Data Analytics in the Cloud. (arXiv:2112.09762v1 [cs.DC] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1\">Daochen Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1\">Kwei-Herng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaixiong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>",
          "description": "We study time-series classification (TSC), a fundamental task of time-series\ndata mining. Prior work has approached TSC from two major directions: (1)\nsimilarity-based methods that classify time-series based on the nearest\nneighbors, and (2) deep learning models that directly learn the representations\nfor classification in a data-driven manner. Motivated by the different working\nmechanisms within these two research lines, we aim to connect them in such a\nway as to jointly model time-series similarities and learn the representations.\nThis is a challenging task because it is unclear how we should efficiently\nleverage similarity information. To tackle the challenge, we propose\nSimilarity-Aware Time-Series Classification (SimTSC), a conceptually simple and\ngeneral framework that models similarity information with graph neural networks\n(GNNs). Specifically, we formulate TSC as a node classification problem in\ngraphs, where the nodes correspond to time-series, and the links correspond to\npair-wise similarities. We further design a graph construction strategy and a\nbatch training algorithm with negative sampling to improve training efficiency.\nWe instantiate SimTSC with ResNet as the backbone and Dynamic Time Warping\n(DTW) as the similarity measure. Extensive experiments on the full UCR datasets\nand several multivariate datasets demonstrate the effectiveness of\nincorporating similarity information into deep learning models in both\nsupervised and semi-supervised settings. Our code is available at\nhttps://github.com/daochenzha/SimTSC",
          "link": "http://arxiv.org/abs/2201.01413",
          "publishedOn": "2022-01-07T00:40:42.890Z",
          "wordCount": null,
          "title": "Towards Similarity-Aware Time-Series Classification. (arXiv:2201.01413v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16101",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "In the paper, we propose a class of faster adaptive Gradient Descent Ascent\n(GDA) methods for solving the nonconvex-strongly-concave minimax problems based\non unified adaptive matrices, which include almost existing coordinate-wise and\nglobal adaptive learning rates. Specifically, we propose a fast Adaptive\nGradient Decent Ascent (AdaGDA) method based on the basic momentum technique,\nwhich reaches a lower gradient complexity of $O(\\kappa^4\\epsilon^{-4})$ for\nfinding an $\\epsilon$-stationary point without large batches, which improves\nthe results of the existing adaptive GDA methods by a factor of\n$O(\\sqrt{\\kappa})$. At the same time, we present an accelerated version of\nAdaGDA (VR-AdaGDA) method based on the momentum-based variance reduced\ntechnique, which achieves a lower gradient complexity of\n$O(\\kappa^{4.5}\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point\nwithout large batches, which improves the results of the existing adaptive GDA\nmethods by a factor of $O(\\epsilon^{-1})$. Moreover, we prove that our\nVR-AdaGDA method reaches the best known gradient complexity of\n$O(\\kappa^{3}\\epsilon^{-3})$ with the mini-batch size $O(\\kappa^3)$. In\nparticular, we provide an effective convergence analysis framework for our\nadaptive GDA methods. Some experimental results on policy evaluation and fair\nclassifier tasks demonstrate the efficiency of our algorithms.",
          "link": "http://arxiv.org/abs/2106.16101",
          "publishedOn": "2022-01-07T00:40:42.888Z",
          "wordCount": 649,
          "title": "AdaGDA: Faster Adaptive Gradient Descent Ascent Methods for Minimax Optimization. (arXiv:2106.16101v4 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Talby_H/0/1/0/all/0/1\">Hasham Ul Haq Veysel Kocaman David Talby</a>",
          "description": "Adverse drug reactions / events (ADR/ADE) have a major impact on patient\nhealth and health care costs. Detecting ADR's as early as possible and sharing\nthem with regulators, pharma companies, and healthcare providers can prevent\nmorbidity and save many lives. While most ADR's are not reported via formal\nchannels, they are often documented in a variety of unstructured conversations\nsuch as social media posts by patients, customer support call transcripts, or\nCRM notes of meetings between healthcare providers and pharma sales reps. In\nthis paper, we propose a natural language processing (NLP) solution that\ndetects ADR's in such unstructured free-text conversations, which improves on\nprevious work in three ways. First, a new Named Entity Recognition (NER) model\nobtains new state-of-the-art accuracy for ADR and Drug entity extraction on the\nADE, CADEC, and SMM4H benchmark datasets (91.75%, 78.76%, and 83.41% F1 scores\nrespectively). Second, two new Relation Extraction (RE) models are introduced -\none based on BioBERT while the other utilizing crafted features over a Fully\nConnected Neural Network (FCNN) - are shown to perform on par with existing\nstate-of-the-art models, and outperform them when trained with a supplementary\nclinician-annotated RE dataset. Third, a new text classification model, for\ndeciding if a conversation includes an ADR, obtains new state-of-the-art\naccuracy on the CADEC dataset (86.69% F1 score). The complete solution is\nimplemented as a unified NLP pipeline in a production-grade library built on\ntop of Apache Spark, making it natively scalable and able to process millions\nof batch or streaming records on commodity clusters.",
          "link": "http://arxiv.org/abs/2201.01405",
          "publishedOn": "2022-01-07T00:40:42.881Z",
          "wordCount": 693,
          "title": "Mining Adverse Drug Reactions from Unstructured Mediums at Scale. (arXiv:2201.01405v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.11598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khiari_J/0/1/0/all/0/1\">Jihed Khiari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olaverri_Monreal_C/0/1/0/all/0/1\">Cristina Olaverri-Monreal</a>",
          "description": "Travel time is a crucial measure in transportation. Accurate travel time\nprediction is also fundamental for operation and advanced information systems.\nA variety of solutions exist for short-term travel time predictions such as\nsolutions that utilize real-time GPS data and optimization methods to track the\npath of a vehicle. However, reliable long-term predictions remain challenging.\nWe show in this paper the applicability and usefulness of travel time i.e.\ndelivery time prediction for postal services. We investigate several methods\nsuch as linear regression models and tree based ensembles such as random\nforest, bagging, and boosting, that allow to predict delivery time by\nconducting extensive experiments and considering many usability scenarios.\nResults reveal that travel time prediction can help mitigate high delays in\npostal services. We show that some boosting algorithms, such as light gradient\nboosting and catboost, have a higher performance in terms of accuracy and\nruntime efficiency than other baselines such as linear regression models,\nbagging regressor and random forest.",
          "link": "http://arxiv.org/abs/2009.11598",
          "publishedOn": "2022-01-07T00:40:42.874Z",
          "wordCount": null,
          "title": "Boosting Algorithms for Delivery Time Prediction in Transportation Logistics. (arXiv:2009.11598v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01588",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wachter_E/0/1/0/all/0/1\">Eduardo Weber Wachter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasap_S/0/1/0/all/0/1\">Server Kasap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolozali_S/0/1/0/all/0/1\">Sefki Kolozali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaojun Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsan_S/0/1/0/all/0/1\">Shoaib Ehsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_Maier_K/0/1/0/all/0/1\">Klaus McDonald-Maier</a>",
          "description": "The emergence of new nanoscale technologies has imposed significant\nchallenges to designing reliable electronic systems in radiation environments.\nA few types of radiation like Total Ionizing Dose (TID) effects often cause\npermanent damages on such nanoscale electronic devices, and current\nstate-of-the-art technologies to tackle TID make use of expensive\nradiation-hardened devices. This paper focuses on a novel and different\napproach: using machine learning algorithms on consumer electronic level Field\nProgrammable Gate Arrays (FPGAs) to tackle TID effects and monitor them to\nreplace before they stop working. This condition has a research challenge to\nanticipate when the board results in a total failure due to TID effects. We\nobserved internal measurements of the FPGA boards under gamma radiation and\nused three different anomaly detection machine learning (ML) algorithms to\ndetect anomalies in the sensor measurements in a gamma-radiated environment.\nThe statistical results show a highly significant relationship between the\ngamma radiation exposure levels and the board measurements. Moreover, our\nanomaly detection results have shown that a One-Class Support Vector Machine\nwith Radial Basis Function Kernel has an average Recall score of 0.95. Also,\nall anomalies can be detected before the boards stop working.",
          "link": "http://arxiv.org/abs/2201.01588",
          "publishedOn": "2022-01-07T00:40:42.873Z",
          "wordCount": 621,
          "title": "Using Machine Learning for Anomaly Detection on a System-on-Chip under Gamma Radiation. (arXiv:2201.01588v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ziemann_I/0/1/0/all/0/1\">Ingvar Ziemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandberg_H/0/1/0/all/0/1\">Henrik Sandberg</a>",
          "description": "This paper presents local minimax regret lower bounds for adaptively\ncontrolling linear-quadratic-Gaussian (LQG) systems. We consider smoothly\nparametrized instances and provide an understanding of when logarithmic regret\nis impossible which is both instance specific and flexible enough to take\nproblem structure into account. This understanding relies on two key notions:\nThat of local-uninformativeness; when the optimal policy does not provide\nsufficient excitation for identification of the optimal policy, and yields a\ndegenerate Fisher information matrix; and that of\ninformation-regret-boundedness, when the small eigenvalues of a\npolicy-dependent information matrix are boundable in terms of the regret of\nthat policy. Combined with a reduction to Bayesian estimation and application\nof Van Trees' inequality, these two conditions are sufficient for proving\nregret bounds on order of magnitude $\\sqrt{T}$ in the time horizon, $T$. This\nmethod yields lower bounds that exhibit tight dimensional dependencies and\nscale naturally with control-theoretic problem constants. For instance, we are\nable to prove that systems operating near marginal stability are fundamentally\nhard to learn to control. We further show that large classes of systems satisfy\nthese conditions, among them any state-feedback system with both $A$- and\n$B$-matrices unknown. Most importantly, we also establish that a nontrivial\nclass of partially observable systems, essentially those that are\nover-actuated, satisfy these conditions, thus providing a $\\sqrt{T}$ lower\nbound also valid for partially observable systems. Finally, we turn to two\nsimple examples which demonstrate that our lower bound captures classical\ncontrol-theoretic intuition: our lower bounds diverge for systems operating\nnear marginal stability or with large filter gain -- these can be arbitrarily\nhard to (learn to) control.",
          "link": "http://arxiv.org/abs/2201.01680",
          "publishedOn": "2022-01-07T00:40:42.864Z",
          "wordCount": 690,
          "title": "Regret Lower Bounds for Learning Linear Quadratic Gaussian Systems. (arXiv:2201.01680v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Do_V/0/1/0/all/0/1\">Virginie Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corbett_Davies_S/0/1/0/all/0/1\">Sam Corbett-Davies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atif_J/0/1/0/all/0/1\">Jamal Atif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usunier_N/0/1/0/all/0/1\">Nicolas Usunier</a>",
          "description": "Recommender systems are facing scrutiny because of their growing impact on\nthe opportunities we have access to. Current audits for fairness are limited to\ncoarse-grained parity assessments at the level of sensitive groups. We propose\nto audit for envy-freeness, a more granular criterion aligned with individual\npreferences: every user should prefer their recommendations to those of other\nusers. Since auditing for envy requires to estimate the preferences of users\nbeyond their existing recommendations, we cast the audit as a new pure\nexploration problem in multi-armed bandits. We propose a sample-efficient\nalgorithm with theoretical guarantees that it does not deteriorate user\nexperience. We also study the trade-offs achieved on real-world recommendation\ndatasets.",
          "link": "http://arxiv.org/abs/2104.14527",
          "publishedOn": "2022-01-07T00:40:42.825Z",
          "wordCount": null,
          "title": "Online certification of preference-based fairness for personalized recommender systems. (arXiv:2104.14527v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.05558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alfarisi_O/0/1/0/all/0/1\">Omar Alfarisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aung_Z/0/1/0/all/0/1\">Zeyar Aung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sassi_M/0/1/0/all/0/1\">Mohamed Sassi</a>",
          "description": "For defining the optimal machine learning algorithm, the decision was not\neasy for which we shall choose. To help future researchers, we describe in this\npaper the optimal among the best of the algorithms. We built a synthetic data\nset and performed the supervised machine learning runs for five different\nalgorithms. For heterogeneous rock fabric, we identified Random Forest, among\nothers, to be the appropriate algorithm.",
          "link": "http://arxiv.org/abs/2111.05558",
          "publishedOn": "2022-01-07T00:40:42.825Z",
          "wordCount": null,
          "title": "Deducing Optimal Classification Algorithm for Heterogeneous Fabric. (arXiv:2111.05558v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.13838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suk_J/0/1/0/all/0/1\">Joe Suk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kpotufe_S/0/1/0/all/0/1\">Samory Kpotufe</a>",
          "description": "In bandits with distribution shifts, one aims to automatically detect an\nunknown number $L$ of changes in reward distribution, and restart exploration\nwhen necessary. While this problem remained open for many years, a recent\nbreakthrough of Auer et al. (2018, 2019) provide the first adaptive procedure\nto guarantee an optimal (dynamic) regret $\\sqrt{LT}$, for $T$ rounds, with no\nknowledge of $L$. However, not all distributional shifts are equally severe,\ne.g., suppose no best arm switches occur, then we cannot rule out that a regret\n$O(\\sqrt{T})$ may remain possible; in other words, is it possible to achieve\ndynamic regret that optimally scales only with an unknown number of severe\nshifts? This unfortunately has remained elusive, despite various attempts (Auer\net al., 2019, Foster et al., 2020).\n\nWe resolve this problem in the case of two-armed bandits: we derive an\nadaptive procedure that guarantees a dynamic regret of order\n$\\tilde{O}(\\sqrt{\\tilde{L} T})$, where $\\tilde L \\ll L$ captures an unknown\nnumber of severe best arm changes, i.e., with significant switches in rewards,\nand which last sufficiently long to actually require a restart. As a\nconsequence, for any number $L$ of distributional shifts outside of these\nsevere shifts, our procedure achieves regret just $\\tilde{O}(\\sqrt{T})\\ll\n\\tilde{O}(\\sqrt{LT})$.\n\nFinally, we note that our notion of severe shift applies in both classical\nsettings of stochastic switching bandits and of adversarial bandits.",
          "link": "http://arxiv.org/abs/2112.13838",
          "publishedOn": "2022-01-07T00:40:42.825Z",
          "wordCount": null,
          "title": "Tracking Most Severe Arm Changes in Bandits. (arXiv:2112.13838v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.11323",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Fujita_K/0/1/0/all/0/1\">Kazuhiro Fujita</a>",
          "description": "A mesh-free approach for modelling beam-wall interactions in particle\naccelerators is proposed. The key idea of our method is to use a deep neural\nnetwork as a surrogate for the solution to a set of partial differential\nequations involving the particle beam, and the surface impedance concept. The\nproposed approach is applied to the coupling impedance of an accelerator vacuum\nchamber with thin conductive coating, and also verified in comparison with the\nexisting analytical formula.",
          "link": "http://arxiv.org/abs/2112.11323",
          "publishedOn": "2022-01-07T00:40:42.824Z",
          "wordCount": 530,
          "title": "Physics-informed neural network method for modelling beam-wall interactions. (arXiv:2112.11323v2 [physics.acc-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.13492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Viet-Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh H. T. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khong_A/0/1/0/all/0/1\">Andy W. H. Khong</a>",
          "description": "We introduce a block-online variant of the temporal feature-wise linear\nmodulation (TFiLM) model to achieve bandwidth extension. The proposed\narchitecture simplifies the UNet backbone of the TFiLM to reduce inference time\nand employs an efficient transformer at the bottleneck to alleviate performance\ndegradation. We also utilize self-supervised pretraining and data augmentation\nto enhance the quality of bandwidth extended signals and reduce the sensitivity\nwith respect to downsampling methods. Experiment results on the VCTK dataset\nshow that the proposed method outperforms several recent baselines in both\nintrusive and non-intrusive metrics. Pretraining and filter augmentation also\nhelp stabilize and enhance the overall performance.",
          "link": "http://arxiv.org/abs/2110.13492",
          "publishedOn": "2022-01-07T00:40:42.817Z",
          "wordCount": 573,
          "title": "TUNet: A Block-online Bandwidth Extension Model based on Transformers and Self-supervised Pretraining. (arXiv:2110.13492v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01741",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bamler_R/0/1/0/all/0/1\">Robert Bamler</a>",
          "description": "Entropy coding is the backbone data compression. Novel machine-learning based\ncompression methods often use a new entropy coder called Asymmetric Numeral\nSystems (ANS) [Duda et al., 2015], which provides very close to optimal\nbitrates and simplifies [Townsend et al., 2019] advanced compression techniques\nsuch as bits-back coding. However, researchers with a background in machine\nlearning often struggle to understand how ANS works, which prevents them from\nexploiting its full versatility. This paper is meant as an educational resource\nto make ANS more approachable by presenting it from a new perspective of latent\nvariable models and the so-called bits-back trick. We guide the reader step by\nstep to a complete implementation of ANS in the Python programming language,\nwhich we then generalize for more advanced use cases. We also present and\nempirically evaluate an open-source library of various entropy coders designed\nfor both research and production use. Related teaching videos and problem sets\nare available online.",
          "link": "http://arxiv.org/abs/2201.01741",
          "publishedOn": "2022-01-07T00:40:42.799Z",
          "wordCount": 598,
          "title": "Understanding Entropy Coding With Asymmetric Numeral Systems (ANS): a Statistician's Perspective. (arXiv:2201.01741v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yuning You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yang Shen</a>",
          "description": "Self-supervision is recently surging at its new frontier of graph learning.\nIt facilitates graph representations beneficial to downstream tasks; but its\nsuccess could hinge on domain knowledge for handcraft or the often expensive\ntrials and errors. Even its state-of-the-art representative, graph contrastive\nlearning (GraphCL), is not completely free of those needs as GraphCL uses a\nprefabricated prior reflected by the ad-hoc manual selection of graph data\naugmentations. Our work aims at advancing GraphCL by answering the following\nquestions: How to represent the space of graph augmented views? What principle\ncan be relied upon to learn a prior in that space? And what framework can be\nconstructed to learn the prior in tandem with contrastive learning?\nAccordingly, we have extended the prefabricated discrete prior in the\naugmentation set, to a learnable continuous prior in the parameter space of\ngraph generators, assuming that graph priors per se, similar to the concept of\nimage manifolds, can be learned by data generation. Furthermore, to form\ncontrastive views without collapsing to trivial solutions due to the prior\nlearnability, we have leveraged both principles of information minimization\n(InfoMin) and information bottleneck (InfoBN) to regularize the learned priors.\nEventually, contrastive learning, InfoMin, and InfoBN are incorporated\norganically into one framework of bi-level optimization. Our principled and\nautomated approach has proven to be competitive against the state-of-the-art\ngraph self-supervision methods, including GraphCL, on benchmarks of small\ngraphs; and shown even better generalizability on large-scale graphs, without\nresorting to human expertise or downstream validation. Our code is publicly\nreleased at https://github.com/Shen-Lab/GraphCL_Automated.",
          "link": "http://arxiv.org/abs/2201.01702",
          "publishedOn": "2022-01-07T00:40:42.780Z",
          "wordCount": null,
          "title": "Bringing Your Own View: Graph Contrastive Learning without Prefabricated Data Augmentations. (arXiv:2201.01702v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01486",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sharvani Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangwar_A/0/1/0/all/0/1\">Amisha Gangwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_R/0/1/0/all/0/1\">Richa Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sudhakar Singh</a>",
          "description": "Communication is defined as the act of sharing or exchanging information,\nideas or feelings. To establish communication between two people, both of them\nare required to have knowledge and understanding of a common language. But in\nthe case of deaf and dumb people, the means of communication are different.\nDeaf is the inability to hear and dumb is the inability to speak. They\ncommunicate using sign language among themselves and with normal people but\nnormal people do not take seriously the importance of sign language. Not\neveryone possesses the knowledge and understanding of sign language which makes\ncommunication difficult between a normal person and a deaf and dumb person. To\novercome this barrier, one can build a model based on machine learning. A model\ncan be trained to recognize different gestures of sign language and translate\nthem into English. This will help a lot of people in communicating and\nconversing with deaf and dumb people. The existing Indian Sing Language\nRecognition systems are designed using machine learning algorithms with single\nand double-handed gestures but they are not real-time. In this paper, we\npropose a method to create an Indian Sign Language dataset using a webcam and\nthen using transfer learning, train a TensorFlow model to create a real-time\nSign Language Recognition system. The system achieves a good level of accuracy\neven with a limited size dataset.",
          "link": "http://arxiv.org/abs/2201.01486",
          "publishedOn": "2022-01-07T00:40:42.779Z",
          "wordCount": null,
          "title": "Sign Language Recognition System using TensorFlow Object Detection API. (arXiv:2201.01486v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07636",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_W/0/1/0/all/0/1\">Wenkai Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lu_J/0/1/0/all/0/1\">Jie Lu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sutherland_D/0/1/0/all/0/1\">Danica J. Sutherland</a>",
          "description": "Modern kernel-based two-sample tests have shown great success in\ndistinguishing complex, high-dimensional distributions with appropriate learned\nkernels. Previous work has demonstrated that this kernel learning procedure\nsucceeds, assuming a considerable number of observed samples from each\ndistribution. In realistic scenarios with very limited numbers of data samples,\nhowever, it can be challenging to identify a kernel powerful enough to\ndistinguish complex distributions. We address this issue by introducing the\nproblem of meta two-sample testing (M2ST), which aims to exploit (abundant)\nauxiliary data on related tasks to find an algorithm that can quickly identify\na powerful test on new target tasks. We propose two specific algorithms for\nthis task: a generic scheme which improves over baselines and a more tailored\napproach which performs even better. We provide both theoretical justification\nand empirical evidence that our proposed meta-testing schemes out-perform\nlearning kernel-based tests directly from scarce observations, and identify\nwhen such schemes will be successful.",
          "link": "http://arxiv.org/abs/2106.07636",
          "publishedOn": "2022-01-07T00:40:42.779Z",
          "wordCount": null,
          "title": "Meta Two-Sample Testing: Learning Kernels for Testing with Limited Data. (arXiv:2106.07636v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04520",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruihan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1\">Kilian Q. Weinberger</a>",
          "description": "Machine learning models often encounter distribution shifts when deployed in\nthe real world. In this paper, we focus on adaptation to label distribution\nshift in the online setting, where the test-time label distribution is\ncontinually changing and the model must dynamically adapt to it without\nobserving the true label. Leveraging a novel analysis, we show that the lack of\ntrue label does not hinder estimation of the expected test loss, which enables\nthe reduction of online label shift adaptation to conventional online learning.\nInformed by this observation, we propose adaptation algorithms inspired by\nclassical online learning techniques such as Follow The Leader (FTL) and Online\nGradient Descent (OGD) and derive their regret bounds. We empirically verify\nour findings under both simulated and real world label distribution shifts and\nshow that OGD is particularly effective and robust to a variety of challenging\nlabel shift scenarios.",
          "link": "http://arxiv.org/abs/2107.04520",
          "publishedOn": "2022-01-07T00:40:42.779Z",
          "wordCount": null,
          "title": "Online Adaptation to Label Distribution Shift. (arXiv:2107.04520v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01305",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Wadekar_D/0/1/0/all/0/1\">Digvijay Wadekar</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Thiele_L/0/1/0/all/0/1\">Leander Thiele</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Villaescusa_Navarro_F/0/1/0/all/0/1\">Francisco Villaescusa-Navarro</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Hill_J/0/1/0/all/0/1\">J. Colin Hill</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Spergel_D/0/1/0/all/0/1\">David N. Spergel</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Cranmer_M/0/1/0/all/0/1\">Miles Cranmer</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Battaglia_N/0/1/0/all/0/1\">Nicholas Battaglia</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Angles_Alcazar_D/0/1/0/all/0/1\">Daniel Angl&#xe9;s-Alc&#xe1;zar</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Hernquist_L/0/1/0/all/0/1\">Lars Hernquist</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Ho_S/0/1/0/all/0/1\">Shirley Ho</a>",
          "description": "Complex systems (stars, supernovae, galaxies, and clusters) often exhibit low\nscatter relations between observable properties (e.g., luminosity, velocity\ndispersion, oscillation period, temperature). These scaling relations can\nilluminate the underlying physics and can provide observational tools for\nestimating masses and distances. Machine learning can provide a systematic way\nto search for new scaling relations (or for simple extensions to existing\nrelations) in abstract high-dimensional parameter spaces. We use a machine\nlearning tool called symbolic regression (SR), which models the patterns in a\ngiven dataset in the form of analytic equations. We focus on the\nSunyaev-Zeldovich flux$-$cluster mass relation ($Y_\\mathrm{SZ}-M$), the scatter\nin which affects inference of cosmological parameters from cluster abundance\ndata. Using SR on the data from the IllustrisTNG hydrodynamical simulation, we\nfind a new proxy for cluster mass which combines $Y_\\mathrm{SZ}$ and\nconcentration of ionized gas ($c_\\mathrm{gas}$): $M \\propto\nY_\\mathrm{conc}^{3/5} \\equiv Y_\\mathrm{SZ}^{3/5} (1-A\\, c_\\mathrm{gas})$.\n$Y_\\mathrm{conc}$ reduces the scatter in the predicted $M$ by $\\sim 20-30$% for\nlarge clusters ($M\\gtrsim 10^{14}\\, h^{-1} \\, M_\\odot$) at both high and low\nredshifts, as compared to using just $Y_\\mathrm{SZ}$. We show that the\ndependence on $c_\\mathrm{gas}$ is linked to cores of clusters exhibiting larger\nscatter than their outskirts. Finally, we test $Y_\\mathrm{conc}$ on clusters\nfrom simulations of the CAMELS project and show that $Y_\\mathrm{conc}$ is\nrobust against variations in cosmology, astrophysics, subgrid physics, and\ncosmic variance. Our results and methodology can be useful for accurate\nmultiwavelength cluster mass estimation from current and upcoming CMB and X-ray\nsurveys like ACT, SO, SPT, eROSITA and CMB-S4.",
          "link": "http://arxiv.org/abs/2201.01305",
          "publishedOn": "2022-01-07T00:40:42.526Z",
          "wordCount": 744,
          "title": "Augmenting astrophysical scaling relations with machine learning : application to reducing the SZ flux-mass scatter. (arXiv:2201.01305v1 [astro-ph.CO])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hieu Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_H/0/1/0/all/0/1\">Hans Walker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1\">Dung Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_P/0/1/0/all/0/1\">Peter Chin</a>",
          "description": "Although deep neural networks have achieved great performance on\nclassification tasks, recent studies showed that well trained networks can be\nfooled by adding subtle noises. This paper introduces a new approach to improve\nneural network robustness by applying the recovery process on top of the\nnaturally trained classifier. In this approach, images will be intentionally\ncorrupted by some significant operator and then be recovered before passing\nthrough the classifiers. SARGAN -- an extension on Generative Adversarial\nNetworks (GAN) is capable of denoising radar signals. This paper will show that\nSARGAN can also recover corrupted images by removing the adversarial effects.\nOur results show that this approach does improve the performance of naturally\ntrained networks.",
          "link": "http://arxiv.org/abs/2201.01399",
          "publishedOn": "2022-01-07T00:40:42.519Z",
          "wordCount": 553,
          "title": "Corrupting Data to Remove Deceptive Perturbation: Using Preprocessing Method to Improve System Robustness. (arXiv:2201.01399v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01652",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lyu_H/0/1/0/all/0/1\">Hanbaek Lyu</a>",
          "description": "Stochastic majorization-minimization (SMM) is an online extension of the\nclassical principle of majorization-minimization, which consists of sampling\ni.i.d. data points from a fixed data distribution and minimizing a recursively\ndefined majorizing surrogate of an objective function. In this paper, we\nintroduce stochastic block majorization-minimization, where the surrogates can\nnow be only block multi-convex and a single block is optimized at a time within\na diminishing radius. Relaxing the standard strong convexity requirements for\nsurrogates in SMM, our framework gives wider applicability including online\nCANDECOMP/PARAFAC (CP) dictionary learning and yields greater computational\nefficiency especially when the problem dimension is large. We provide an\nextensive convergence analysis on the proposed algorithm, which we derive under\npossibly dependent data streams, relaxing the standard i.i.d. assumption on\ndata samples. We show that the proposed algorithm converges almost surely to\nthe set of stationary points of a nonconvex objective under constraints at a\nrate $O((\\log n)^{1+\\eps}/n^{1/2})$ for the empirical loss function and\n$O((\\log n)^{1+\\eps}/n^{1/4})$ for the expected loss function, where $n$\ndenotes the number of data samples processed. Under some additional assumption,\nthe latter convergence rate can be improved to $O((\\log n)^{1+\\eps}/n^{1/2})$.\nOur results provide first convergence rate bounds for various online matrix and\ntensor decomposition algorithms under a general Markovian data setting.",
          "link": "http://arxiv.org/abs/2201.01652",
          "publishedOn": "2022-01-07T00:40:42.491Z",
          "wordCount": 632,
          "title": "Convergence and Complexity of Stochastic Block Majorization-Minimization. (arXiv:2201.01652v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xian Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1\">Hai Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">JiaMing Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanhui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>",
          "description": "Self-attention shows outstanding competence in capturing long-range\nrelationships while enhancing performance on vision tasks, such as image\nclassification and image captioning. However, the self-attention module highly\nrelies on the dot product multiplication and dimension alignment among\nquery-key-value features, which cause two problems: (1) The dot product\nmultiplication results in exhaustive and redundant computation. (2) Due to the\nvisual feature map often appearing as a multi-dimensional tensor, reshaping the\nscale of the tensor feature to adapt to the dimension alignment might destroy\nthe internal structure of the tensor feature map. To address these problems,\nthis paper proposes a self-attention plug-in module with its variants, namely,\nSynthesizing Tensor Transformations (STT), for directly processing image tensor\nfeatures. Without computing the dot-product multiplication among\nquery-key-value, the basic STT is composed of the tensor transformation to\nlearn the synthetic attention weight from visual information. The effectiveness\nof STT series is validated on the image classification and image caption.\nExperiments show that the proposed STT achieves competitive performance while\nkeeping robustness compared to self-attention based above vision tasks.",
          "link": "http://arxiv.org/abs/2201.01410",
          "publishedOn": "2022-01-07T00:40:42.484Z",
          "wordCount": 604,
          "title": "Synthesizing Tensor Transformations for Visual Self-attention. (arXiv:2201.01410v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Si-si Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian-wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xin Zuo</a>",
          "description": "Recent years have witnessed growing interests in online incremental learning.\nHowever, there are three major challenges in this area. The first major\ndifficulty is concept drift, that is, the probability distribution in the\nstreaming data would change as the data arrives. The second major difficulty is\ncatastrophic forgetting, that is, forgetting what we have learned before when\nlearning new knowledge. The last one we often ignore is the learning of the\nlatent representation. Only good latent representation can improve the\nprediction accuracy of the model. Our research builds on this observation and\nattempts to overcome these difficulties. To this end, we propose an Adaptive\nOnline Incremental Learning for evolving data streams (AOIL). We use\nauto-encoder with the memory module, on the one hand, we obtained the latent\nfeatures of the input, on the other hand, according to the reconstruction loss\nof the auto-encoder with memory module, we could successfully detect the\nexistence of concept drift and trigger the update mechanism, adjust the model\nparameters in time. In addition, we divide features, which are derived from the\nactivation of the hidden layers, into two parts, which are used to extract the\ncommon and private features respectively. By means of this approach, the model\ncould learn the private features of the new coming instances, but do not forget\nwhat we have learned in the past (shared features), which reduces the\noccurrence of catastrophic forgetting. At the same time, to get the fusion\nfeature vector we use the self-attention mechanism to effectively fuse the\nextracted features, which further improved the latent representation learning.",
          "link": "http://arxiv.org/abs/2201.01633",
          "publishedOn": "2022-01-07T00:40:42.477Z",
          "wordCount": 684,
          "title": "Adaptive Online Incremental Learning for Evolving Data Streams. (arXiv:2201.01633v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jaemin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanchun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sung-Ju Lee</a>",
          "description": "Federated Learning (FL) trains a machine learning model on distributed\nclients without exposing individual data. Unlike centralized training that is\nusually based on carefully-organized data, FL deals with on-device data that\nare often unfiltered and imbalanced. As a result, conventional FL training\nprotocol that treats all data equally leads to a waste of local computational\nresources and slows down the global learning process. To this end, we propose\nFedBalancer, a systematic FL framework that actively selects clients' training\nsamples. Our sample selection strategy prioritizes more \"informative\" data\nwhile respecting privacy and computational capabilities of clients. To better\nutilize the sample selection to speed up global training, we further introduce\nan adaptive deadline control scheme that predicts the optimal deadline for each\nround with varying client train data. Compared with existing FL algorithms with\ndeadline configuration methods, our evaluation on five datasets from three\ndifferent domains shows that FedBalancer improves the time-to-accuracy\nperformance by 1.22~4.62x while improving the model accuracy by 1.0~3.3%. We\nalso show that FedBalancer is readily applicable to other FL approaches by\ndemonstrating that FedBalancer improves the convergence speed and accuracy when\noperating jointly with three different FL algorithms.",
          "link": "http://arxiv.org/abs/2201.01601",
          "publishedOn": "2022-01-07T00:40:42.470Z",
          "wordCount": 616,
          "title": "Sample Selection with Deadline Control for Efficient Federated Learning on Heterogeneous Clients. (arXiv:2201.01601v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01387",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Faradonbeh_M/0/1/0/all/0/1\">Mohamad Kazem Shirani Faradonbeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Modi_A/0/1/0/all/0/1\">Aditya Modi</a>",
          "description": "Learning-based control of linear systems received a lot of attentions\nrecently. In popular settings, the true dynamical models are unknown to the\ndecision-maker and need to be interactively learned by applying control inputs\nto the systems. Unlike the matured literature of efficient reinforcement\nlearning policies for adaptive control of a single system, results on joint\nlearning of multiple systems are not currently available. Especially, the\nimportant problem of fast and reliable joint-stabilization remains unaddressed\nand so is the focus of this work. We propose a novel joint learning-based\nstabilization algorithm for quickly learning stabilizing policies for all\nsystems understudy, from the data of unstable state trajectories. The presented\nprocedure is shown to be notably effective such that it stabilizes the family\nof dynamical systems in an extremely short time period.",
          "link": "http://arxiv.org/abs/2201.01387",
          "publishedOn": "2022-01-07T00:40:42.463Z",
          "wordCount": 565,
          "title": "Joint Learning-Based Stabilization of Multiple Unknown Linear Systems. (arXiv:2201.01387v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01450",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koley_P/0/1/0/all/0/1\">Paramita Koley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiti_A/0/1/0/all/0/1\">Aurghya Maiti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Sourangshu Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_N/0/1/0/all/0/1\">Niloy Ganguly</a>",
          "description": "This paper investigates the dynamics of competition among organizations with\nunequal expertise. Multi-agent reinforcement learning has been used to simulate\nand understand the impact of various incentive schemes designed to offset such\ninequality. We design Touch-Mark, a game based on well-known\nmulti-agent-particle-environment, where two teams (weak, strong) with unequal\nbut changing skill levels compete against each other. For training such a game,\nwe propose a novel controller assisted multi-agent reinforcement learning\nalgorithm \\our\\, which empowers each agent with an ensemble of policies along\nwith a supervised controller that by selectively partitioning the sample space,\ntriggers intelligent role division among the teammates. Using C-MADDPG as an\nunderlying framework, we propose an incentive scheme for the weak team such\nthat the final rewards of both teams become the same. We find that in spite of\nthe incentive, the final reward of the weak team falls short of the strong\nteam. On inspecting, we realize that an overall incentive scheme for the weak\nteam does not incentivize the weaker agents within that team to learn and\nimprove. To offset this, we now specially incentivize the weaker player to\nlearn and as a result, observe that the weak team beyond an initial phase\nperforms at par with the stronger team. The final goal of the paper has been to\nformulate a dynamic incentive scheme that continuously balances the reward of\nthe two teams. This is achieved by devising an incentive scheme enriched with\nan RL agent which takes minimum information from the environment.",
          "link": "http://arxiv.org/abs/2201.01450",
          "publishedOn": "2022-01-07T00:40:42.456Z",
          "wordCount": 678,
          "title": "Offsetting Unequal Competition through RL-assisted Incentive Schemes. (arXiv:2201.01450v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alcoforado_A/0/1/0/all/0/1\">Alexandre Alcoforado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraz_T/0/1/0/all/0/1\">Thomas Palmeira Ferraz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerber_R/0/1/0/all/0/1\">Rodrigo Gerber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bustos_E/0/1/0/all/0/1\">Enzo Bustos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1\">Andr&#xe9; Seidel Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veloso_B/0/1/0/all/0/1\">Bruno Miguel Veloso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siqueira_F/0/1/0/all/0/1\">Fabio Levy Siqueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1\">Anna Helena Reali Costa</a>",
          "description": "Traditional text classification approaches often require a good amount of\nlabeled data, which is difficult to obtain, especially in restricted domains or\nless widespread languages. This lack of labeled data has led to the rise of\nlow-resource methods, that assume low data availability in natural language\nprocessing. Among them, zero-shot learning stands out, which consists of\nlearning a classifier without any previously labeled data. The best results\nreported with this approach use language models such as Transformers, but fall\ninto two problems: high execution time and inability to handle long texts as\ninput. This paper proposes a new model, ZeroBERTo, which leverages an\nunsupervised clustering step to obtain a compressed data representation before\nthe classification task. We show that ZeroBERTo has better performance for long\ninputs and shorter execution time, outperforming XLM-R by about 12% in the F1\nscore in the FolhaUOL dataset. Keywords: Low-Resource NLP, Unlabeled data,\nZero-Shot Learning, Topic Modeling, Transformers.",
          "link": "http://arxiv.org/abs/2201.01337",
          "publishedOn": "2022-01-07T00:40:42.294Z",
          "wordCount": 610,
          "title": "ZeroBERTo -- Leveraging Zero-Shot Text Classification by Topic Modeling. (arXiv:2201.01337v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pfrommer_D/0/1/0/all/0/1\">Daniel Pfrommer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matni_N/0/1/0/all/0/1\">Nikolai Matni</a>",
          "description": "We introduce Variational State-Space Filters (VSSF), a new method for\nunsupervised learning, identification, and filtering of latent Markov state\nspace models from raw pixels. We present a theoretically sound framework for\nlatent state space inference under heterogeneous sensor configurations. The\nresulting model can integrate an arbitrary subset of the sensor measurements\nused during training, enabling the learning of semi-supervised state\nrepresentations, thus enforcing that certain components of the learned latent\nstate space to agree with interpretable measurements. From this framework we\nderive L-VSSF, an explicit instantiation of this model with linear latent\ndynamics and Gaussian distribution parameterizations. We experimentally\ndemonstrate L-VSSF's ability to filter in latent space beyond the sequence\nlength of the training dataset across several different test environments.",
          "link": "http://arxiv.org/abs/2201.01353",
          "publishedOn": "2022-01-07T00:40:42.287Z",
          "wordCount": 549,
          "title": "Linear Variational State Space Filtering. (arXiv:2201.01353v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01347",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ma_H/0/1/0/all/0/1\">Hengbo Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_B/0/1/0/all/0/1\">Bike Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tomizuka_M/0/1/0/all/0/1\">Masayoshi Tomizuka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sreenath_K/0/1/0/all/0/1\">Koushil Sreenath</a>",
          "description": "Control barrier functions (CBFs) have become a popular tool to enforce safety\nof a control system. CBFs are commonly utilized in a quadratic program\nformulation (CBF-QP) as safety-critical constraints. A class $\\mathcal{K}$\nfunction in CBFs usually needs to be tuned manually in order to balance the\ntrade-off between performance and safety for each environment. However, this\nprocess is often heuristic and can become intractable for high relative-degree\nsystems. Moreover, it prevents the CBF-QP from generalizing to different\nenvironments in the real world. By embedding the optimization procedure of the\nCBF-QP as a differentiable layer within a deep learning architecture, we\npropose a differentiable optimization-based safety-critical control framework\nthat enables generalization to new environments with forward invariance\nguarantees. Finally, we validate the proposed control design with 2D double and\nquadruple integrator systems in various environments.",
          "link": "http://arxiv.org/abs/2201.01347",
          "publishedOn": "2022-01-07T00:40:42.268Z",
          "wordCount": 575,
          "title": "Learning Differentiable Safety-Critical Control using Control Barrier Functions for Generalization to Novel Environments. (arXiv:2201.01347v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01300",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Villaescusa_Navarro_F/0/1/0/all/0/1\">Francisco Villaescusa-Navarro</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Genel_S/0/1/0/all/0/1\">Shy Genel</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Angles_Alcazar_D/0/1/0/all/0/1\">Daniel Angl&#xe9;s-Alc&#xe1;zar</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Perez_L/0/1/0/all/0/1\">Lucia A. Perez</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Villanueva_Domingo_P/0/1/0/all/0/1\">Pablo Villanueva-Domingo</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wadekar_D/0/1/0/all/0/1\">Digvijay Wadekar</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Shao_H/0/1/0/all/0/1\">Helen Shao</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Mohammad_F/0/1/0/all/0/1\">Faizan G. Mohammad</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Hassan_S/0/1/0/all/0/1\">Sultan Hassan</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Moser_E/0/1/0/all/0/1\">Emily Moser</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Lau_E/0/1/0/all/0/1\">Erwin T. Lau</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Valle_L/0/1/0/all/0/1\">Luis Fernando Machado Poletti Valle</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Nicola_A/0/1/0/all/0/1\">Andrina Nicola</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Thiele_L/0/1/0/all/0/1\">Leander Thiele</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Jo_Y/0/1/0/all/0/1\">Yongseok Jo</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Philcox_O/0/1/0/all/0/1\">Oliver H. E. Philcox</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Oppenheimer_B/0/1/0/all/0/1\">Benjamin D. Oppenheimer</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Tillman_M/0/1/0/all/0/1\">Megan Tillman</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Hahn_C/0/1/0/all/0/1\">ChangHoon Hahn</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kaushal_N/0/1/0/all/0/1\">Neerav Kaushal</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Pisani_A/0/1/0/all/0/1\">Alice Pisani</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Gebhardt_M/0/1/0/all/0/1\">Matthew Gebhardt</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Delgado_A/0/1/0/all/0/1\">Ana Maria Delgado</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Caliendo_J/0/1/0/all/0/1\">Joyce Caliendo</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kreisch_C/0/1/0/all/0/1\">Christina Kreisch</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wong_K/0/1/0/all/0/1\">Kaze W.K. Wong</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Coulton_W/0/1/0/all/0/1\">William R. Coulton</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Eickenberg_M/0/1/0/all/0/1\">Michael Eickenberg</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Parimbelli_G/0/1/0/all/0/1\">Gabriele Parimbelli</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Ni_Y/0/1/0/all/0/1\">Yueying Ni</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Steinwandel_U/0/1/0/all/0/1\">Ulrich P. Steinwandel</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Torre_V/0/1/0/all/0/1\">Valentina La Torre</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Dave_R/0/1/0/all/0/1\">Romeel Dave</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Battaglia_N/0/1/0/all/0/1\">Nicholas Battaglia</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Nagai_D/0/1/0/all/0/1\">Daisuke Nagai</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Spergel_D/0/1/0/all/0/1\">David N. Spergel</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Hernquist_L/0/1/0/all/0/1\">Lars Hernquist</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Burkhart_B/0/1/0/all/0/1\">Blakesley Burkhart</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Narayanan_D/0/1/0/all/0/1\">Desika Narayanan</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wandelt_B/0/1/0/all/0/1\">Benjamin Wandelt</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Somerville_R/0/1/0/all/0/1\">Rachel S. Somerville</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Bryan_G/0/1/0/all/0/1\">Greg L. Bryan</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Viel_M/0/1/0/all/0/1\">Matteo Viel</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Li_Y/0/1/0/all/0/1\">Yin Li</a>, et al. (3 additional authors not shown)",
          "description": "The Cosmology and Astrophysics with MachinE Learning Simulations (CAMELS)\nproject was developed to combine cosmology with astrophysics through thousands\nof cosmological hydrodynamic simulations and machine learning. CAMELS contains\n4,233 cosmological simulations, 2,049 N-body and 2,184 state-of-the-art\nhydrodynamic simulations that sample a vast volume in parameter space. In this\npaper we present the CAMELS public data release, describing the characteristics\nof the CAMELS simulations and a variety of data products generated from them,\nincluding halo, subhalo, galaxy, and void catalogues, power spectra, bispectra,\nLyman-$\\alpha$ spectra, probability distribution functions, halo radial\nprofiles, and X-rays photon lists. We also release over one thousand catalogues\nthat contain billions of galaxies from CAMELS-SAM: a large collection of N-body\nsimulations that have been combined with the Santa Cruz Semi-Analytic Model. We\nrelease all the data, comprising more than 350 terabytes and containing 143,922\nsnapshots, millions of halos, galaxies and summary statistics. We provide\nfurther technical details on how to access, download, read, and process the\ndata at \\url{https://camels.readthedocs.io}.",
          "link": "http://arxiv.org/abs/2201.01300",
          "publishedOn": "2022-01-07T00:40:41.942Z",
          "wordCount": 727,
          "title": "The CAMELS project: public data release. (arXiv:2201.01300v1 [astro-ph.CO])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhijin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1\">Xiaoming Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jianhua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Geoffrey Ye Li</a>",
          "description": "Semantic communication, regarded as the breakthrough beyond Shannon paradigm,\naims at the successful transmission of semantic information conveyed by the\nsource rather than the accurate reception of each single symbol or bit\nregardless of its meaning. This article provides an overview on semantic\ncommunications. After a brief review on Shannon information theory, we discuss\nsemantic communications with theory, frameworks, and system design enabled by\ndeep learning. Different from the symbol/bit error rate used for measuring the\nconventional communication systems, new performance metrics for semantic\ncommunications are also discussed. The article is concluded by several open\nquestions.",
          "link": "http://arxiv.org/abs/2201.01389",
          "publishedOn": "2022-01-07T00:40:41.919Z",
          "wordCount": 518,
          "title": "Semantic Communications: Principles and Challenges. (arXiv:2201.01389v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yan Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chao Liu</a>",
          "description": "Graph neural networks can be effectively applied to find solutions for many\nreal-world problems across widely diverse fields. The success of graph neural\nnetworks is linked to the message-passing mechanism on the graph, however, the\nmessage-aggregating behavior is still not entirely clear in most algorithms. To\nimprove functionality, we propose a new transparent network called Graph\nDecipher to investigate the message-passing mechanism by prioritizing in two\nmain components: the graph structure and node attributes, at the graph,\nfeature, and global levels on a graph under the node classification task.\nHowever, the computation burden now becomes the most significant issue because\nthe relevance of both graph structure and node attributes are computed on a\ngraph. In order to solve this issue, only relevant representative node\nattributes are extracted by graph feature filters, allowing calculations to be\nperformed in a category-oriented manner. Experiments on seven datasets show\nthat Graph Decipher achieves state-of-the-art performance while imposing a\nsubstantially lower computation burden under the node classification task.\nAdditionally, since our algorithm has the ability to explore the representative\nnode attributes by category, it is utilized to alleviate the imbalanced node\nclassification problem on multi-class graph datasets.",
          "link": "http://arxiv.org/abs/2201.01381",
          "publishedOn": "2022-01-07T00:40:41.912Z",
          "wordCount": 626,
          "title": "Graph Decipher: A transparent dual-attention graph neural network to understand the message-passing mechanism for the node classification. (arXiv:2201.01381v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01323",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Akella_P/0/1/0/all/0/1\">Prithvi Akella</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ubellacker_W/0/1/0/all/0/1\">Wyatt Ubellacker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ames_A/0/1/0/all/0/1\">Aaron D. Ames</a>",
          "description": "In this letter, the authors propose a two-step approach to evaluate and\nverify a true system's capacity to satisfy its operational objective.\nSpecifically, whenever the system objective has a quantifiable measure of\nsatisfaction, i.e. a signal temporal logic specification, a barrier function,\netc - the authors develop two separate optimization problems solvable via a\nBayesian Optimization procedure detailed within. This dual approach has the\nadded benefit of quantifying the Sim2Real Gap between a system simulator and\nits hardware counterpart. Our contributions are twofold. First, we show\nrepeatability with respect to our outlined optimization procedure in solving\nthese optimization problems. Second, we show that the same procedure can\ndiscriminate between different environments by identifying the Sim2Real Gap\nbetween a simulator and its hardware counterpart operating in different\nenvironments.",
          "link": "http://arxiv.org/abs/2201.01323",
          "publishedOn": "2022-01-07T00:40:41.904Z",
          "wordCount": 562,
          "title": "Test and Evaluation of Quadrupedal Walking Gaits through Sim2Real Gap Quantification. (arXiv:2201.01323v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abyane_A/0/1/0/all/0/1\">Amin Eslami Abyane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Derui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Souza_R/0/1/0/all/0/1\">Roberto Medeiros de Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemmati_H/0/1/0/all/0/1\">Hadi Hemmati</a>",
          "description": "Federated learning (FL) is a widely adopted distributed learning paradigm in\npractice, which intends to preserve users' data privacy while leveraging the\nentire dataset of all participants for training. In FL, multiple models are\ntrained independently on the users and aggregated centrally to update a global\nmodel in an iterative process. Although this approach is excellent at\npreserving privacy by design, FL still tends to suffer from quality issues such\nas attacks or byzantine faults. Some recent attempts have been made to address\nsuch quality challenges on the robust aggregation techniques for FL. However,\nthe effectiveness of state-of-the-art (SOTA) robust FL techniques is still\nunclear and lacks a comprehensive study. Therefore, to better understand the\ncurrent quality status and challenges of these SOTA FL techniques in the\npresence of attacks and faults, in this paper, we perform a large-scale\nempirical study to investigate the SOTA FL's quality from multiple angles of\nattacks, simulated faults (via mutation operators), and aggregation (defense)\nmethods. In particular, we perform our study on two generic image datasets and\none real-world federated medical image dataset. We also systematically\ninvestigate the effect of the distribution of attacks/faults over users and the\nindependent and identically distributed (IID) factors, per dataset, on the\nrobustness results. After a large-scale analysis with 496 configurations, we\nfind that most mutators on each individual user have a negligible effect on the\nfinal model. Moreover, choosing the most robust FL aggregator depends on the\nattacks and datasets. Finally, we illustrate that it is possible to achieve a\ngeneric solution that works almost as well or even better than any single\naggregator on all attacks and configurations with a simple ensemble model of\naggregators.",
          "link": "http://arxiv.org/abs/2201.01409",
          "publishedOn": "2022-01-07T00:40:41.897Z",
          "wordCount": 722,
          "title": "Towards Understanding Quality Challenges of the Federated Learning: A First Look from the Lens of Robustness. (arXiv:2201.01409v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01416",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gim_U/0/1/0/all/0/1\">UJu Gim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">YeongHyeon Park</a>",
          "description": "Deep learning methods can classify various unstructured data such as images,\nlanguage, and voice as input data. As the task of classifying anomalies becomes\nmore important in the real world, various methods exist for classifying using\ndeep learning with data collected in the real world. As the task of classifying\nanomalies becomes more important in the real world, there are various methods\nfor classifying using deep learning with data collected in the real world.\nAmong the various methods, the representative approach is a method of\nextracting and learning the main features based on a transition model from\npre-trained models, and a method of learning an autoencoderbased structure only\nwith normal data and classifying it as abnormal through a threshold value.\nHowever, if the dataset is imbalanced, even the state-of-the-arts models do not\nachieve good performance. This can be addressed by augmenting normal and\nabnormal features in imbalanced data as features with strong distinction. We\nuse the features of the autoencoder to train latent vectors from low to high\ndimensionality. We train normal and abnormal data as a feature that has a\nstrong distinction among the features of imbalanced data. We propose a latent\nvector expansion autoencoder model that improves classification performance at\nimbalanced data. The proposed method shows performance improvement compared to\nthe basic autoencoder using imbalanced anomaly dataset.",
          "link": "http://arxiv.org/abs/2201.01416",
          "publishedOn": "2022-01-07T00:40:41.889Z",
          "wordCount": 661,
          "title": "Latent Vector Expansion using Autoencoder for Anomaly Detection. (arXiv:2201.01416v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Magoarou_L/0/1/0/all/0/1\">Luc Le Magoarou</a> (IRT b-com), <a href=\"http://arxiv.org/find/cs/1/au:+Yassine_T/0/1/0/all/0/1\">Taha Yassine</a> (IRT b-com, INSA Rennes, IETR), <a href=\"http://arxiv.org/find/cs/1/au:+Paquelet_S/0/1/0/all/0/1\">St&#xe9;phane Paquelet</a> (IRT b-com), <a href=\"http://arxiv.org/find/cs/1/au:+Crussiere_M/0/1/0/all/0/1\">Matthieu Crussi&#xe8;re</a> (IRT b-com, INSA Rennes, IETR)",
          "description": "Massive MIMO systems are highly efficient but critically rely on accurate\nchannel state information (CSI) at the base station in order to determine\nappropriate precoders. CSI acquisition requires sending pilot symbols which\ninduce an important overhead. In this paper, a method whose objective is to\ndetermine an appropriate precoder from the knowledge of the user's location\nonly is proposed. Such a way to determine precoders is known as location based\nbeamforming. It allows to reduce or even eliminate the need for pilot symbols,\ndepending on how the location is obtained. the proposed method learns a direct\nmapping from location to precoder in a supervised way. It involves a neural\nnetwork with a specific structure based on random Fourier features allowing to\nlearn functions containing high spatial frequencies. It is assessed empirically\nand yields promising results on realistic synthetic channels. As opposed to\npreviously proposed methods, it allows to handle both line-of-sight (LOS) and\nnon-line-of-sight (NLOS) channels.",
          "link": "http://arxiv.org/abs/2201.01386",
          "publishedOn": "2022-01-07T00:40:41.882Z",
          "wordCount": 611,
          "title": "Deep learning for location based beamforming with NLOS channels. (arXiv:2201.01386v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01441",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zongheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_W/0/1/0/all/0/1\">Wei-Lin Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_S/0/1/0/all/0/1\">Sifei Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_G/0/1/0/all/0/1\">Gautam Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Michael Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1\">Ion Stoica</a>",
          "description": "Query optimizers are a performance-critical component in every database\nsystem. Due to their complexity, optimizers take experts months to write and\nyears to refine. In this work, we demonstrate for the first time that learning\nto optimize queries without learning from an expert optimizer is both possible\nand efficient. We present Balsa, a query optimizer built by deep reinforcement\nlearning. Balsa first learns basic knowledge from a simple,\nenvironment-agnostic simulator, followed by safe learning in real execution. On\nthe Join Order Benchmark, Balsa matches the performance of two expert query\noptimizers, both open-source and commercial, with two hours of learning, and\noutperforms them by up to 2.8$\\times$ in workload runtime after a few more\nhours. Balsa thus opens the possibility of automatically learning to optimize\nin future compute environments where expert-designed optimizers do not exist.",
          "link": "http://arxiv.org/abs/2201.01441",
          "publishedOn": "2022-01-07T00:40:41.855Z",
          "wordCount": 562,
          "title": "Balsa: Learning a Query Optimizer Without Expert Demonstrations. (arXiv:2201.01441v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rohlfs_C/0/1/0/all/0/1\">Chris Rohlfs</a>",
          "description": "This paper introduces a new neural network-based estimation approach that is\ninspired by the biological phenomenon whereby humans and animals vary the\nlevels of attention and effort that they dedicate to a problem depending upon\nits difficulty. The proposed approach leverages alternate models' internal\nlevels of confidence in their own projections. If the least costly model is\nconfident in its classification, then that is the classification used; if not,\nthe model with the next lowest cost of implementation is run, and so on. This\nuse of successively more complex models -- together with the models' internal\npropensity scores to evaluate their likelihood of being correct -- makes it\npossible to substantially reduce resource use while maintaining high standards\nfor classification accuracy. The approach is applied to the digit recognition\nproblem from Google's Street View House Numbers dataset, using Multilayer\nPerceptron (MLP) neural networks trained on high- and low-resolution versions\nof the digit images. The algorithm examines the low-resolution images first,\nonly moving to higher resolution images if the classification from the initial\nlow-resolution pass does not have a high degree of confidence. For the MLPs\nconsidered here, this sequential approach enables a reduction in resource usage\nof more than 50\\% without any sacrifice in classification accuracy.",
          "link": "http://arxiv.org/abs/2201.01415",
          "publishedOn": "2022-01-07T00:40:41.847Z",
          "wordCount": 649,
          "title": "Problem-dependent attention and effort in neural networks with an application to image resolution. (arXiv:2201.01415v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Ningyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuoguang Yang</a>",
          "description": "In the multi-armed bandit framework, there are two formulations that are\ncommonly employed to handle time-varying reward distributions: adversarial\nbandit and nonstationary bandit. Although their oracles, algorithms, and regret\nanalysis differ significantly, we provide a unified formulation in this paper\nthat smoothly bridges the two as special cases. The formulation uses an oracle\nthat takes the best-fixed arm within time windows. Depending on the window\nsize, it turns into the oracle in hindsight in the adversarial bandit and\ndynamic oracle in the nonstationary bandit. We provide algorithms that attain\nthe optimal regret with the matching lower bound.",
          "link": "http://arxiv.org/abs/2201.01628",
          "publishedOn": "2022-01-07T00:40:41.840Z",
          "wordCount": 511,
          "title": "Bridging Adversarial and Nonstationary Multi-armed Bandit. (arXiv:2201.01628v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yan Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chao Liu</a>",
          "description": "Static graph neural networks have been widely used in modeling and\nrepresentation learning of graph structure data. However, many real-world\nproblems, such as social networks, financial transactions, recommendation\nsystems, etc., are dynamic, that is, nodes and edges are added or deleted over\ntime. Therefore, in recent years, dynamic graph neural networks have received\nmore and more attention from researchers. In this work, we propose a novel\ndynamic graph neural network, Efficient-Dyn. It adaptively encodes temporal\ninformation into a sequence of patches with an equal amount of\ntemporal-topological structure. Therefore, while avoiding the use of snapshots\nto cause information loss, it also achieves a finer time granularity, which is\nclose to what continuous networks could provide. In addition, we also designed\na lightweight module, Sparse Temporal Transformer, to compute node\nrepresentations through both structural neighborhoods and temporal dynamics.\nSince the fully-connected attention conjunction is simplified, the computation\ncost is far lower than the current state-of-the-arts. Link prediction\nexperiments are conducted on both continuous and discrete graph datasets.\nThrough comparing with several state-of-the-art graph embedding baselines, the\nexperimental results demonstrate that Efficient-Dyn has a faster inference\nspeed while having competitive performance.",
          "link": "http://arxiv.org/abs/2201.01384",
          "publishedOn": "2022-01-07T00:40:41.833Z",
          "wordCount": 607,
          "title": "Efficient-Dyn: Dynamic Graph Representation Learning via Event-based Temporal Sparse Attention Network. (arXiv:2201.01384v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Okerinde_A/0/1/0/all/0/1\">Ademola Okerinde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoggatt_S/0/1/0/all/0/1\">Sam Hoggatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkireddy_D/0/1/0/all/0/1\">Divya Vani Lakkireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_N/0/1/0/all/0/1\">Nolan Brubaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">William Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_L/0/1/0/all/0/1\">Lior Shamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiseman_B/0/1/0/all/0/1\">Brian Spiseman</a>",
          "description": "In recent years, self-supervised learning has had significant success in\napplications involving computer vision and natural language processing. The\ntype of pretext task is important to this boost in performance. One common\npretext task is the measure of similarity and dissimilarity between pairs of\nimages. In this scenario, the two images that make up the negative pair are\nvisibly different to humans. However, in entomology, species are nearly\nindistinguishable and thus hard to differentiate. In this study, we explored\nthe performance of a Siamese neural network using contrastive loss by learning\nto push apart embeddings of bumblebee species pair that are dissimilar, and\npull together similar embeddings. Our experimental results show a 61% F1-score\non zero-shot instances, a performance showing 11% improvement on samples of\nclasses that share intersections with the training set.",
          "link": "http://arxiv.org/abs/2201.01391",
          "publishedOn": "2022-01-07T00:40:41.817Z",
          "wordCount": 564,
          "title": "Self-Supervised Approach to Addressing Zero-Shot Learning Problem. (arXiv:2201.01391v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2112.01254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jihun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yoonsang Lee</a>",
          "description": "The neural network-based approach to solving partial differential equations\nhas attracted considerable attention due to its simplicity and flexibility in\nrepresenting the solution of the partial differential equation. In training a\nneural network, the network learns global features corresponding to\nlow-frequency components while high-frequency components are approximated at a\nmuch slower rate. For a class of equations in which the solution contains a\nwide range of scales, the network training process can suffer from slow\nconvergence and low accuracy due to its inability to capture the high-frequency\ncomponents. In this work, we propose a hierarchical approach to improve the\nconvergence rate and accuracy of the neural network solution to partial\ndifferential equations. The proposed method comprises multi-training levels in\nwhich a newly introduced neural network is guided to learn the residual of the\nprevious level approximation. By the nature of neural networks' training\nprocess, the high-level correction is inclined to capture the high-frequency\ncomponents. We validate the efficiency and robustness of the proposed\nhierarchical approach through a suite of linear and nonlinear partial\ndifferential equations.",
          "link": "http://arxiv.org/abs/2112.01254",
          "publishedOn": "2022-01-06T00:40:23.770Z",
          "wordCount": 623,
          "title": "Hierarchical Learning to Solve Partial Differential Equations Using Physics-Informed Neural Networks. (arXiv:2112.01254v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bjorck_J/0/1/0/all/0/1\">Johan Bjorck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1\">Carla P. Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1\">Kilian Q. Weinberger</a>",
          "description": "In computer vision and natural language processing, innovations in model\narchitecture that increase model capacity have reliably translated into gains\nin performance. In stark contrast with this trend, state-of-the-art\nreinforcement learning (RL) algorithms often use small MLPs, and gains in\nperformance typically originate from algorithmic innovations. It is natural to\nhypothesize that small datasets in RL necessitate simple models to avoid\noverfitting; however, this hypothesis is untested. In this paper we investigate\nhow RL agents are affected by exchanging the small MLPs with larger modern\nnetworks with skip connections and normalization, focusing specifically on\nactor-critic algorithms. We empirically verify that naively adopting such\narchitectures leads to instabilities and poor performance, likely contributing\nto the popularity of simple models in practice. However, we show that dataset\nsize is not the limiting factor, and instead argue that instability from taking\ngradients through the critic is the culprit. We demonstrate that spectral\nnormalization (SN) can mitigate this issue and enable stable training with\nlarge modern architectures. After smoothing with SN, larger models yield\nsignificant performance improvements -- suggesting that more \"easy\" gains may\nbe had by focusing on model architectures in addition to algorithmic\ninnovations.",
          "link": "http://arxiv.org/abs/2106.01151",
          "publishedOn": "2022-01-06T00:40:23.765Z",
          "wordCount": 660,
          "title": "Towards Deeper Deep Reinforcement Learning with Spectral Normalization. (arXiv:2106.01151v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.10040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_T/0/1/0/all/0/1\">Taisuke Kobayashi</a>",
          "description": "Deep reinforcement learning (DRL) is one promising approach to teaching\nrobots to perform complex tasks. Because methods that directly reuse the stored\nexperience data cannot follow the change of the environment in robotic problems\nwith a time-varying environment, online DRL is required. The eligibility traces\nmethod is well known as an online learning technique for improving sample\nefficiency in traditional reinforcement learning with linear regressors rather\nthan DRL. The dependency between parameters of deep neural networks would\ndestroy the eligibility traces, which is why they are not integrated with DRL.\nAlthough replacing the gradient with the most influential one rather than\naccumulating the gradients as the eligibility traces can alleviate this\nproblem, the replacing operation reduces the number of reuses of previous\nexperiences. To address these issues, this study proposes a new eligibility\ntraces method that can be used even in DRL while maintaining high sample\nefficiency. When the accumulated gradients differ from those computed using the\nlatest parameters, the proposed method takes into account the divergence\nbetween the past and latest parameters to adaptively decay the eligibility\ntraces. Bregman divergences between outputs computed by the past and latest\nparameters are exploited due to the infeasible computational cost of the\ndivergence between the past and latest parameters. In addition, a generalized\nmethod with multiple time-scale traces is designed for the first time. This\ndesign allows for the replacement of the most influential adaptively\naccumulated (decayed) eligibility traces.",
          "link": "http://arxiv.org/abs/2008.10040",
          "publishedOn": "2022-01-06T00:40:23.759Z",
          "wordCount": 685,
          "title": "Adaptive and Multiple Time-scale Eligibility Traces for Online Deep Reinforcement Learning. (arXiv:2008.10040v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.05058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Besic_B/0/1/0/all/0/1\">Borna Be&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>",
          "description": "Dynamic objects have a significant impact on the robot's perception of the\nenvironment which degrades the performance of essential tasks such as\nlocalization and mapping. In this work, we address this problem by synthesizing\nplausible color, texture and geometry in regions occluded by dynamic objects.\nWe propose the novel geometry-aware DynaFill architecture that follows a\ncoarse-to-fine topology and incorporates our gated recurrent feedback mechanism\nto adaptively fuse information from previous timesteps. We optimize our\narchitecture using adversarial training to synthesize fine realistic textures\nwhich enables it to hallucinate color and depth structure in occluded regions\nonline in a spatially and temporally coherent manner, without relying on future\nframe information. Casting our inpainting problem as an image-to-image\ntranslation task, our model also corrects regions correlated with the presence\nof dynamic objects in the scene, such as shadows or reflections. We introduce a\nlarge-scale hyperrealistic dataset with RGB-D images, semantic segmentation\nlabels, camera poses as well as groundtruth RGB-D information of occluded\nregions. Extensive quantitative and qualitative evaluations show that our\napproach achieves state-of-the-art performance, even in challenging weather\nconditions. Furthermore, we present results for retrieval-based visual\nlocalization with the synthesized images that demonstrate the utility of our\napproach.",
          "link": "http://arxiv.org/abs/2008.05058",
          "publishedOn": "2022-01-06T00:40:23.754Z",
          "wordCount": 693,
          "title": "Dynamic Object Removal and Spatio-Temporal RGB-D Inpainting via Geometry-Aware Adversarial Learning. (arXiv:2008.05058v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhambhoria_R/0/1/0/all/0/1\">Rohan Bhambhoria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahan_S/0/1/0/all/0/1\">Samuel Dahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>",
          "description": "Over the past several years, legal applications of deep learning have been on\nthe rise. However, as with other high-stakes decision making areas, the\nrequirement for interpretability is of crucial importance. Current models\nutilized by legal practitioners are more of the conventional machine learning\ntype, wherein they are inherently interpretable, yet unable to harness the\nperformance capabilities of data-driven deep learning models. In this work, we\nutilize deep learning models in the area of trademark law to shed light on the\nissue of likelihood of confusion between trademarks. Specifically, we introduce\na model-agnostic interpretable intermediate layer, a technique which proves to\nbe effective for legal documents. Furthermore, we utilize weakly supervised\nlearning by means of a curriculum learning strategy, effectively demonstrating\nthe improved performance of a deep learning model. This is in contrast to the\nconventional models which are only able to utilize the limited number of\nexpensive manually-annotated samples by legal experts. Although the methods\npresented in this work tackles the task of risk of confusion for trademarks, it\nis straightforward to extend them to other fields of law, or more generally, to\nother similar high-stakes application scenarios.",
          "link": "http://arxiv.org/abs/2201.01164",
          "publishedOn": "2022-01-06T00:40:23.747Z",
          "wordCount": 609,
          "title": "Interpretable Low-Resource Legal Decision Making. (arXiv:2201.01164v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+El_Merhi_A/0/1/0/all/0/1\">Ali El-Merhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herges_H/0/1/0/all/0/1\">Helena Odenstedt Herg&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Block_L/0/1/0/all/0/1\">Linda Block</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elam_M/0/1/0/all/0/1\">Mikael Elam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vithal_R/0/1/0/all/0/1\">Richard Vithal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liljencrantz_J/0/1/0/all/0/1\">Jaquette Liljencrantz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staron_M/0/1/0/all/0/1\">Miroslaw Staron</a>",
          "description": "Machine learning can be used to analyse physiological data for several\npurposes. Detection of cerebral ischemia is an achievement that would have high\nimpact on patient care. We attempted to study if collection of continous\nphysiological data from non-invasive monitors, and analysis with machine\nlearning could detect cerebral ischemia in tho different setting, during\nsurgery for carotid endarterectomy and during endovascular thrombectomy in\nacute stroke. We compare the results from the two different group and one\npatient from each group in details. While results from CEA-patients are\nconsistent, those from thrombectomy patients are not and frequently contain\nextreme values such as 1.0 in accuracy. We conlcude that this is a result of\nshort duration of the procedure and abundance of data with bad quality\nresulting in small data sets. These results can therefore not be trusted.",
          "link": "http://arxiv.org/abs/2201.01060",
          "publishedOn": "2022-01-06T00:40:23.725Z",
          "wordCount": 576,
          "title": "Trusting Machine Learning Results from Medical Procedures in the Operating Room. (arXiv:2201.01060v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zimin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kommrusch_S/0/1/0/all/0/1\">Steve Kommrusch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monperrus_M/0/1/0/all/0/1\">Martin Monperrus</a>",
          "description": "In this paper, we address the problem of automatic repair of software\nvulnerabilities with deep learning. The major problem with data-driven\nvulnerability repair is that the few existing datasets of known confirmed\nvulnerabilities consist of only a few thousand examples. However, training a\ndeep learning model often requires hundreds of thousands of examples. In this\nwork, we leverage the intuition that the bug fixing task and the vulnerability\nfixing task are related and that the knowledge learned from bug fixes can be\ntransferred to fixing vulnerabilities. In the machine learning community, this\ntechnique is called transfer learning. In this paper, we propose an approach\nfor repairing security vulnerabilities named VRepair which is based on transfer\nlearning. VRepair is first trained on a large bug fix corpus and is then tuned\non a vulnerability fix dataset, which is an order of magnitude smaller. In our\nexperiments, we show that a model trained only on a bug fix corpus can already\nfix some vulnerabilities. Then, we demonstrate that transfer learning improves\nthe ability to repair vulnerable C functions. We also show that the transfer\nlearning model performs better than a model trained with a denoising task and\nfine-tuned on the vulnerability fixing task. To sum up, this paper shows that\ntransfer learning works well for repairing security vulnerabilities in C\ncompared to learning on a small dataset.",
          "link": "http://arxiv.org/abs/2104.08308",
          "publishedOn": "2022-01-06T00:40:23.717Z",
          "wordCount": 689,
          "title": "Neural Transfer Learning for Repairing Security Vulnerabilities in C Code. (arXiv:2104.08308v3 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04615",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">Mohit Kumar</a>",
          "description": "This paper considers the problem of differentially private semi-supervised\ntransfer and multi-task learning. The notion of \\emph{membership-mapping} has\nbeen developed using measure theory basis to learn data representation via a\nfuzzy membership function. An alternative conception of deep autoencoder,\nreferred to as \\emph{Conditionally Deep Membership-Mapping Autoencoder\n(CDMMA)}, is considered for transferrable deep learning. Under\npractice-oriented settings, an analytical solution for the learning of CDMMA\ncan be derived by means of variational optimization. The paper proposes a\ntransfer and multi-task learning approach that combines CDMMA with a tailored\nnoise adding mechanism to achieve a given level of privacy-loss bound with the\nminimum perturbation of the data. Numerous experiments were carried out using\nMNIST, USPS, Office, and Caltech256 datasets to verify the competitive robust\nperformance of the proposed methodology.",
          "link": "http://arxiv.org/abs/2105.04615",
          "publishedOn": "2022-01-06T00:40:23.710Z",
          "wordCount": 589,
          "title": "Differentially Private Transferrable Deep Learning with Membership-Mappings. (arXiv:2105.04615v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brau_F/0/1/0/all/0/1\">Fabio Brau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossolini_G/0/1/0/all/0/1\">Giulio Rossolini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biondi_A/0/1/0/all/0/1\">Alessandro Biondi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buttazzo_G/0/1/0/all/0/1\">Giorgio Buttazzo</a>",
          "description": "Although Deep Neural Networks (DNNs) have shown incredible performance in\nperceptive and control tasks, several trustworthy issues are still open. One of\nthe most discussed topics is the existence of adversarial perturbations, which\nhas opened an interesting research line on provable techniques capable of\nquantifying the robustness of a given input. In this regard, the Euclidean\ndistance of the input from the classification boundary denotes a well-proved\nrobustness assessment as the minimal affordable adversarial perturbation.\nUnfortunately, computing such a distance is highly complex due the non-convex\nnature of NNs. Despite several methods have been proposed to address this\nissue, to the best of our knowledge, no provable results have been presented to\nestimate and bound the error committed. This paper addresses this issue by\nproposing two lightweight strategies to find the minimal adversarial\nperturbation. Differently from the state-of-the-art, the proposed approach\nallows formulating an error estimation theory of the approximate distance with\nrespect to the theoretical one. Finally, a substantial set of experiments is\nreported to evaluate the performance of the algorithms and support the\ntheoretical findings. The obtained results show that the proposed strategies\napproximate the theoretical distance for samples close to the classification\nboundary, leading to provable robustness guarantees against any adversarial\nattacks.",
          "link": "http://arxiv.org/abs/2201.01235",
          "publishedOn": "2022-01-06T00:40:23.703Z",
          "wordCount": 653,
          "title": "On the Minimal Adversarial Perturbation for Deep Neural Networks with Provable Estimation Error. (arXiv:2201.01235v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1\">Ting Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1\">Tong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spathis_D/0/1/0/all/0/1\">Dimitris Spathis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bondareva_E/0/1/0/all/0/1\">Erika Bondareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_C/0/1/0/all/0/1\">Chlo&#xeb; Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_J/0/1/0/all/0/1\">Jagmohan Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grammenos_A/0/1/0/all/0/1\">Andreas Grammenos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasthanasombat_A/0/1/0/all/0/1\">Apinan Hasthanasombat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Floto_A/0/1/0/all/0/1\">Andres Floto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cicuta_P/0/1/0/all/0/1\">Pietro Cicuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mascolo_C/0/1/0/all/0/1\">Cecilia Mascolo</a>",
          "description": "Recent work has shown the potential of the use of audio data in screening for\nCOVID-19. However, very little exploration has been done of monitoring disease\nprogression, especially recovery in COVID-19 through audio. Tracking disease\nprogression characteristics and patterns of recovery could lead to tremendous\ninsights and more timely treatment or treatment adjustment, as well as better\nresources management in health care systems.\n\nThe primary objective of this study is to explore the potential of\nlongitudinal audio dynamics for COVID-19 monitoring using sequential deep\nlearning techniques, focusing on prediction of disease progression and,\nespecially, recovery trend prediction. We analysed crowdsourced respiratory\naudio data from 212 individuals over 5 days to 385 days, alongside their\nself-reported COVID-19 test results. We first explore the benefits of capturing\nlongitudinal dynamics of audio biomarkers for COVID-19 detection. The strong\nperformance, yielding an AUC-ROC of 0.79, sensitivity of 0.75 and specificity\nof 0.70, supports the effectiveness of the approach compared to methods that do\nnot leverage longitudinal dynamics. We further examine the predicted disease\nprogression trajectory, which displays high consistency with the longitudinal\ntest results with a correlation of 0.76 in the test cohort, and 0.86 in a\nsubset of the test cohort with 12 participants who report disease recovery.\n\nOur findings suggest that monitoring COVID-19 progression via longitudinal\naudio data has enormous potential in the tracking of individuals' disease\nprogression and recovery.",
          "link": "http://arxiv.org/abs/2201.01232",
          "publishedOn": "2022-01-06T00:40:23.685Z",
          "wordCount": 719,
          "title": "COVID-19 Disease Progression Prediction via Audio Signals: A Longitudinal Study. (arXiv:2201.01232v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15427",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nadjahi_K/0/1/0/all/0/1\">Kimia Nadjahi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1\">Alain Durmus</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jacob_P/0/1/0/all/0/1\">Pierre E. Jacob</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Badeau_R/0/1/0/all/0/1\">Roland Badeau</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Simsekli_U/0/1/0/all/0/1\">Umut &#x15e;im&#x15f;ekli</a>",
          "description": "The Sliced-Wasserstein distance (SW) is being increasingly used in machine\nlearning applications as an alternative to the Wasserstein distance and offers\nsignificant computational and statistical benefits. Since it is defined as an\nexpectation over random projections, SW is commonly approximated by Monte\nCarlo. We adopt a new perspective to approximate SW by making use of the\nconcentration of measure phenomenon: under mild assumptions, one-dimensional\nprojections of a high-dimensional random vector are approximately Gaussian.\nBased on this observation, we develop a simple deterministic approximation for\nSW. Our method does not require sampling a number of random projections, and is\ntherefore both accurate and easy to use compared to the usual Monte Carlo\napproximation. We derive nonasymptotical guarantees for our approach, and show\nthat the approximation error goes to zero as the dimension increases, under a\nweak dependence condition on the data distribution. We validate our theoretical\nfindings on synthetic datasets, and illustrate the proposed approximation on a\ngenerative modeling problem.",
          "link": "http://arxiv.org/abs/2106.15427",
          "publishedOn": "2022-01-06T00:40:23.680Z",
          "wordCount": 611,
          "title": "Fast Approximation of the Sliced-Wasserstein Distance Using Concentration of Random Projections. (arXiv:2106.15427v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01218",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Su Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoque_S/0/1/0/all/0/1\">Sanaul Hoque</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deravi_F/0/1/0/all/0/1\">Farzin Deravi</a>",
          "description": "A novel instance-based method for the classification of\nelectroencephalography (EEG) signals is presented and evaluated in this paper.\nThe non-stationary nature of the EEG signals, coupled with the demanding task\nof pattern recognition with limited training data as well as the potentially\nnoisy signal acquisition conditions, have motivated the work reported in this\nstudy. The proposed adaptive template enhancement mechanism transforms the\nfeature-level instances by treating each feature dimension separately, hence\nresulting in improved class separation and better query-class matching. The\nproposed new instance-based learning algorithm is compared with a few related\nalgorithms in a number of scenarios. A clinical grade 64-electrode EEG\ndatabase, as well as a low-quality (high-noise level) EEG database obtained\nwith a low-cost system using a single dry sensor have been used for evaluations\nin biometric person recognition. The proposed approach demonstrates\nsignificantly improved classification accuracy in both identification and\nverification scenarios. In particular, this new method is seen to provide a\ngood classification performance for noisy EEG data, indicating its potential\nsuitability for a wide range of applications.",
          "link": "http://arxiv.org/abs/2201.01218",
          "publishedOn": "2022-01-06T00:40:23.674Z",
          "wordCount": 606,
          "title": "Adaptive Template Enhancement for Improved Person Recognition using Small Datasets. (arXiv:2201.01218v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00976",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shengwen Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chenhui Hu</a>",
          "description": "Machine learning (ML) has been pervasively researched nowadays and it has\nbeen applied in many aspects of real life. Nevertheless, issues of model and\ndata still accompany the development of ML. For instance, training of\ntraditional ML models is limited to the access of data sets, which are\ngenerally proprietary; published ML models may soon be out of date without\nupdate of new data and continuous training; malicious data contributors may\nupload wrongly labeled data that leads to undesirable training results; and the\nabuse of private data and data leakage also exit. With the utilization of\nblockchain, an emerging and swiftly developing technology, these problems can\nbe efficiently solved. In this paper, we conduct a survey of the convergence of\ncollaborative ML and blockchain. We investigate different ways of combination\nof these two technologies, and their fields of application. We also discuss the\nlimitations of current research and their future directions.",
          "link": "http://arxiv.org/abs/2201.00976",
          "publishedOn": "2022-01-06T00:40:23.668Z",
          "wordCount": 569,
          "title": "Survey on the Convergence of Machine Learning and Blockchain. (arXiv:2201.00976v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alkan_O/0/1/0/all/0/1\">&#xd6;znur Alkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dennis Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matteti_M/0/1/0/all/0/1\">Massimiliano Matteti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_R/0/1/0/all/0/1\">Rahul Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daly_E/0/1/0/all/0/1\">Elizabeth M. Daly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_D/0/1/0/all/0/1\">Diptikalyan Saha</a>",
          "description": "Machine learning models may involve decision boundaries that change over time\ndue to updates to rules and regulations, such as in loan approvals or claims\nmanagement. However, in such scenarios, it may take time for sufficient\ntraining data to accumulate in order to retrain the model to reflect the new\ndecision boundaries. While work has been done to reinforce existing decision\nboundaries, very little has been done to cover these scenarios where decision\nboundaries of the ML models should change in order to reflect new rules. In\nthis paper, we focus on user-provided feedback rules as a way to expedite the\nML models update process, and we formally introduce the problem of\npre-processing training data to edit an ML model in response to feedback rules\nsuch that once the model is retrained on the pre-processed data, its decision\nboundaries align more closely with the rules. To solve this problem, we propose\na novel data augmentation method, the Feedback Rule-Based Oversampling\nTechnique. Extensive experiments using different ML models and real world\ndatasets demonstrate the effectiveness of the method, in particular the benefit\nof augmentation and the ability to handle many feedback rules.",
          "link": "http://arxiv.org/abs/2201.01070",
          "publishedOn": "2022-01-06T00:40:23.662Z",
          "wordCount": 621,
          "title": "FROTE: Feedback Rule-Driven Oversampling for Editing Models. (arXiv:2201.01070v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xianglin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruofan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhenfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jin Song Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1\">Hong Mei</a>",
          "description": "Understanding how the predictions of deep learning models are formed during\nthe training process is crucial to improve model performance and fix model\ndefects, especially when we need to investigate nontrivial training strategies\nsuch as active learning, and track the root cause of unexpected training\nresults such as performance degeneration.\n\nIn this work, we propose a time-travelling visual solution DeepVisualInsight\n(DVI), aiming to manifest the spatio-temporal causality while training a deep\nlearning image classifier. The spatio-temporal causality demonstrates how the\ngradient-descent algorithm and various training data sampling techniques can\ninfluence and reshape the layout of learnt input representation and the\nclassification boundaries in consecutive epochs. Such causality allows us to\nobserve and analyze the whole learning process in the visible low dimensional\nspace. Technically, we propose four spatial and temporal properties and design\nour visualization solution to satisfy them. These properties preserve the most\nimportant information when inverse-)projecting input samples between the\nvisible low-dimensional and the invisible high-dimensional space, for causal\nanalyses. Our extensive experiments show that, comparing to baseline\napproaches, we achieve the best visualization performance regarding the\nspatial/temporal properties and visualization efficiency. Moreover, our case\nstudy shows that our visual solution can well reflect the characteristics of\nvarious training scenarios, showing good potential of DVI as a debugging tool\nfor analyzing deep learning training processes.",
          "link": "http://arxiv.org/abs/2201.01155",
          "publishedOn": "2022-01-06T00:40:23.657Z",
          "wordCount": 663,
          "title": "DeepVisualInsight: Time-Travelling Visualization for Spatio-Temporal Causality of Deep Classification Training. (arXiv:2201.01155v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zahavy_T/0/1/0/all/0/1\">Tom Zahavy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODonoghue_B/0/1/0/all/0/1\">Brendan O&#x27;Donoghue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barreto_A/0/1/0/all/0/1\">Andre Barreto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mnih_V/0/1/0/all/0/1\">Volodymyr Mnih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flennerhag_S/0/1/0/all/0/1\">Sebastian Flennerhag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satinder Singh</a>",
          "description": "Finding different solutions to the same problem is a key aspect of\nintelligence associated with creativity and adaptation to novel situations. In\nreinforcement learning, a set of diverse policies can be useful for\nexploration, transfer, hierarchy, and robustness. We propose Diverse Successive\nPolicies, a method for discovering policies that are diverse in the space of\nSuccessor Features, while assuring that they are near optimal. We formalize the\nproblem as a Constrained Markov Decision Process (CMDP) where the goal is to\nfind policies that maximize diversity, characterized by an intrinsic diversity\nreward, while remaining near-optimal with respect to the extrinsic reward of\nthe MDP. We also analyze how recently proposed robustness and discrimination\nrewards perform and find that they are sensitive to the initialization of the\nprocedure and may converge to sub-optimal solutions. To alleviate this, we\npropose new explicit diversity rewards that aim to minimize the correlation\nbetween the Successor Features of the policies in the set. We compare the\ndifferent diversity mechanisms in the DeepMind Control Suite and find that the\ntype of explicit diversity we are proposing is important to discover distinct\nbehavior, like for example different locomotion patterns.",
          "link": "http://arxiv.org/abs/2106.00669",
          "publishedOn": "2022-01-06T00:40:23.635Z",
          "wordCount": 652,
          "title": "Discovering Diverse Nearly Optimal Policies with Successor Features. (arXiv:2106.00669v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patterson_J/0/1/0/all/0/1\">John Patterson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avery_C/0/1/0/all/0/1\">Chris Avery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grear_T/0/1/0/all/0/1\">Tyler Grear</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1\">Donald J. Jacobs</a>",
          "description": "The effect of bias on hypothesis formation is characterized for an automated\ndata-driven projection pursuit neural network to extract and select features\nfor binary classification of data streams. This intelligent exploratory process\npartitions a complete vector state space into disjoint subspaces to create\nworking hypotheses quantified by similarities and differences observed between\ntwo groups of labeled data streams. Data streams are typically time sequenced,\nand may exhibit complex spatio-temporal patterns. For example, given atomic\ntrajectories from molecular dynamics simulation, the machine's task is to\nquantify dynamical mechanisms that promote function by comparing protein\nmutants, some known to function while others are nonfunctional. Utilizing\nsynthetic two-dimensional molecules that mimic the dynamics of functional and\nnonfunctional proteins, biases are identified and controlled in both the\nmachine learning model and selected training data under different contexts. The\nrefinement of a working hypothesis converges to a statistically robust\nmultivariate perception of the data based on a context-dependent perspective.\nIncluding diverse perspectives during data exploration enhances\ninterpretability of the multivariate characterization of similarities and\ndifferences.",
          "link": "http://arxiv.org/abs/2201.00889",
          "publishedOn": "2022-01-06T00:40:23.628Z",
          "wordCount": 612,
          "title": "Biased Hypothesis Formation From Projection Pursuit. (arXiv:2201.00889v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.09001",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1\">Hyung Ju Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Seyoung Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1\">Min Sue Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_H/0/1/0/all/0/1\">Hyeontae Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Haeun Lee</a>",
          "description": "A model that can immediately measure the severity of an epidemic would be of\ngreat help to the medical system. In this paper, we will use machine learning\nto create and analyze a model that can immediately measure the severity of\nSARS-CoV-2 patients. Since our model uses nationwide dataset that consists of\nbasic personal information of patients, it is of great significance for the\npatient to be able to check their own severity. Moreover, based on the\npredicted severity score, we use our model to inform the patients to visit the\nappropriate clinic center.",
          "link": "http://arxiv.org/abs/2109.09001",
          "publishedOn": "2022-01-06T00:40:23.622Z",
          "wordCount": 623,
          "title": "Machine Learning-Based COVID-19 Patients Triage Algorithm using Patient-Generated Health Data from Nationwide Multicenter Database. (arXiv:2109.09001v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Melodia_L/0/1/0/all/0/1\">Luciano Melodia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenz_R/0/1/0/all/0/1\">Richard Lenz</a>",
          "description": "In this paper, we use topological data analysis techniques to construct a\nsuitable neural network classifier for the task of learning sensor signals of\nentire power plants according to their reference designation system. We use\nrepresentations of persistence diagrams to derive necessary preprocessing steps\nand visualize the large amounts of data. We derive deep architectures with\none-dimensional convolutional layers combined with stacked long short-term\nmemories as residual networks suitable for processing the persistence features.\nWe combine three separate sub-networks, obtaining as input the time series\nitself and a representation of the persistent homology for the zeroth and first\ndimension. We give a mathematical derivation for most of the used\nhyper-parameters. For validation, numerical experiments were performed with\nsensor data from four power plants of the same construction type.",
          "link": "http://arxiv.org/abs/2106.02493",
          "publishedOn": "2022-01-06T00:40:23.616Z",
          "wordCount": 613,
          "title": "Homological Time Series Analysis of Sensor Signals from Power Plants. (arXiv:2106.02493v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lu Sun</a>",
          "description": "A variety of modern applications exhibit multi-view multi-label learning,\nwhere each sample has multi-view features, and multiple labels are correlated\nvia common views. In recent years, several methods have been proposed to cope\nwith it and achieve much success, but still suffer from two key problems: 1)\nlack the ability to deal with the incomplete multi-view weak-label data, in\nwhich only a subset of features and labels are provided for each sample; 2)\nignore the presence of noisy views and tail labels usually occurring in\nreal-world problems. In this paper, we propose a novel method, named CEMENT, to\novercome the limitations. For 1), CEMENT jointly embeds incomplete views and\nweak labels into distinct low-dimensional subspaces, and then correlates them\nvia Hilbert-Schmidt Independence Criterion (HSIC). For 2), CEMEMT adaptively\nlearns the weights of embeddings to capture noisy views, and explores an\nadditional sparse component to model tail labels, making the low-rankness\navailable in the multi-label setting. We develop an alternating algorithm to\nsolve the proposed optimization problem. Experimental results on seven\nreal-world datasets demonstrate the effectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2201.01079",
          "publishedOn": "2022-01-06T00:40:23.595Z",
          "wordCount": 600,
          "title": "CEMENT: Incomplete Multi-View Weak-Label Learning with Long Tail Labels. (arXiv:2201.01079v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chi_N/0/1/0/all/0/1\">Nathan A. Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1\">Peter Washington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kline_A/0/1/0/all/0/1\">Aaron Kline</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Husic_A/0/1/0/all/0/1\">Arman Husic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_C/0/1/0/all/0/1\">Cathy Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chloe He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunlap_K/0/1/0/all/0/1\">Kaitlyn Dunlap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wall_D/0/1/0/all/0/1\">Dennis Wall</a>",
          "description": "Autism spectrum disorder (ASD) is a neurodevelopmental disorder which results\nin altered behavior, social development, and communication patterns. In past\nyears, autism prevalence has tripled, with 1 in 54 children now affected. Given\nthat traditional diagnosis is a lengthy, labor-intensive process, significant\nattention has been given to developing systems that automatically screen for\nautism. Prosody abnormalities are among the clearest signs of autism, with\naffected children displaying speech idiosyncrasies including echolalia,\nmonotonous intonation, atypical pitch, and irregular linguistic stress\npatterns. In this work, we present a suite of machine learning approaches to\ndetect autism in self-recorded speech audio captured from autistic and\nneurotypical (NT) children in home environments. We consider three methods to\ndetect autism in child speech: first, Random Forests trained on extracted audio\nfeatures (including Mel-frequency cepstral coefficients); second, convolutional\nneural networks (CNNs) trained on spectrograms; and third, fine-tuned wav2vec\n2.0--a state-of-the-art Transformer-based ASR model. We train our classifiers\non our novel dataset of cellphone-recorded child speech audio curated from\nStanford's Guess What? mobile game, an app designed to crowdsource videos of\nautistic and neurotypical children in a natural home environment. The Random\nForest classifier achieves 70% accuracy, the fine-tuned wav2vec 2.0 model\nachieves 77% accuracy, and the CNN achieves 79% accuracy when classifying\nchildren's audio as either ASD or NT. Our models were able to predict autism\nstatus when training on a varied selection of home audio clips with\ninconsistent recording quality, which may be more generalizable to real world\nconditions. These results demonstrate that machine learning methods offer\npromise in detecting autism automatically from speech without specialized\nequipment.",
          "link": "http://arxiv.org/abs/2201.00927",
          "publishedOn": "2022-01-06T00:40:23.589Z",
          "wordCount": 712,
          "title": "Classifying Autism from Crowdsourced Semi-Structured Speech Recordings: A Machine Learning Approach. (arXiv:2201.00927v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_A/0/1/0/all/0/1\">Arpita Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patro_G/0/1/0/all/0/1\">Gourab K Patro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_N/0/1/0/all/0/1\">Niloy Ganguly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gummadi_K/0/1/0/all/0/1\">Krishna P. Gummadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Abhijnan Chakraborty</a>",
          "description": "Many online platforms today (such as Amazon, Netflix, Spotify, LinkedIn, and\nAirBnB) can be thought of as two-sided markets with producers and customers of\ngoods and services. Traditionally, recommendation services in these platforms\nhave focused on maximizing customer satisfaction by tailoring the results\naccording to the personalized preferences of individual customers. However, our\ninvestigation reinforces the fact that such customer-centric design of these\nservices may lead to unfair distribution of exposure to the producers, which\nmay adversely impact their well-being. On the other hand, a pure\nproducer-centric design might become unfair to the customers. As more and more\npeople are depending on such platforms to earn a living, it is important to\nensure fairness to both producers and customers. In this work, by mapping a\nfair personalized recommendation problem to a constrained version of the\nproblem of fairly allocating indivisible goods, we propose to provide fairness\nguarantees for both sides. Formally, our proposed {\\em FairRec} algorithm\nguarantees Maxi-Min Share ($\\alpha$-MMS) of exposure for the producers, and\nEnvy-Free up to One Item (EF1) fairness for the customers. Extensive\nevaluations over multiple real-world datasets show the effectiveness of {\\em\nFairRec} in ensuring two-sided fairness while incurring a marginal loss in\noverall recommendation quality. Finally, we present a modification of FairRec\n(named as FairRecPlus) that at the cost of additional computation time,\nimproves the recommendation performance for the customers, while maintaining\nthe same fairness guarantees.",
          "link": "http://arxiv.org/abs/2201.01180",
          "publishedOn": "2022-01-06T00:40:23.583Z",
          "wordCount": 681,
          "title": "Towards Fair Recommendation in Two-Sided Platforms. (arXiv:2201.01180v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masud_S/0/1/0/all/0/1\">Sarah Masud</a>",
          "description": "Since the proliferation of social media usage, hate speech has become a major\ncrisis. Hateful content can spread quickly and create an environment of\ndistress and hostility. Further, what can be considered hateful is contextual\nand varies with time. While online hate speech reduces the ability of already\nmarginalised groups to participate in discussion freely, offline hate speech\nleads to hate crimes and violence against individuals and communities. The\nmultifaceted nature of hate speech and its real-world impact have already\npiqued the interest of the data mining and machine learning communities.\nDespite our best efforts, hate speech remains an evasive issue for researchers\nand practitioners alike. This article presents methodological challenges that\nhinder building automated hate mitigation systems. These challenges inspired\nour work in the broader area of combating hateful content on the web. We\ndiscuss a series of our proposed solutions to limit the spread of hate speech\non social media.",
          "link": "http://arxiv.org/abs/2201.00961",
          "publishedOn": "2022-01-06T00:40:23.578Z",
          "wordCount": 592,
          "title": "Nipping in the Bud: Detection, Diffusion and Mitigation of Hate Speech on Social Media. (arXiv:2201.00961v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01018",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Shang_J/0/1/0/all/0/1\">Jiayu Shang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sun_Y/0/1/0/all/0/1\">Yanni Sun</a>",
          "description": "Prokaryotic viruses, which infect bacteria and archaea, are key players in\nmicrobial communities. Predicting the hosts of prokaryotic viruses helps\ndecipher the dynamic relationship between microbes. Although there are\nexperimental methods for host identification, they are either labor-intensive\nor require the cultivation of the host cells, creating a need for computational\nhost prediction. Despite some promising results, computational host prediction\nremains a challenge because of the limited known interactions and the sheer\namount of sequenced phages by high-throughput sequencing technologies. The\nstate-of-the-art methods can only achieve 43% accuracy at the species level.\nThis work presents CHERRY, a tool formulating host prediction as link\nprediction in a knowledge graph. As a virus-prokaryotic interaction prediction\ntool, CHERRY can be applied to predict hosts for newly discovered viruses and\nalso the viruses infecting antibiotic-resistant bacteria. We demonstrated the\nutility of CHERRY for both applications and compared its performance with the\nstate-of-the-art methods in different scenarios. To our best knowledge, CHERRY\nhas the highest accuracy in identifying virus-prokaryote interactions. It\noutperforms all the existing methods at the species level with an accuracy\nincrease of 37%. In addition, CHERRY's performance is more stable on short\ncontigs than other tools.",
          "link": "http://arxiv.org/abs/2201.01018",
          "publishedOn": "2022-01-06T00:40:23.572Z",
          "wordCount": 628,
          "title": "CHERRY: a Computational metHod for accuratE pRediction of virus-pRokarYotic interactions using a graph encoder-decoder model. (arXiv:2201.01018v1 [q-bio.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2112.11294",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hendriksen_M/0/1/0/all/0/1\">Mariya Hendriksen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bleeker_M/0/1/0/all/0/1\">Maurits Bleeker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakulenko_S/0/1/0/all/0/1\">Svitlana Vakulenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noord_N/0/1/0/all/0/1\">Nanne van Noord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuiper_E/0/1/0/all/0/1\">Ernst Kuiper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>",
          "description": "E-commerce provides rich multimodal data that is barely leveraged in\npractice. One aspect of this data is a category tree that is being used in\nsearch and recommendation. However, in practice, during a user's session there\nis often a mismatch between a textual and a visual representation of a given\ncategory. Motivated by the problem, we introduce the task of category-to-image\nretrieval in e-commerce and propose a model for the task, CLIP-ITA. The model\nleverages information from multiple modalities (textual, visual, and attribute\nmodality) to create product representations. We explore how adding information\nfrom multiple modalities (textual, visual, and attribute modality) impacts the\nmodel's performance. In particular, we observe that CLIP-ITA significantly\noutperforms a comparable model that leverages only the visual modality and a\ncomparable model that leverages the visual and attribute modality.",
          "link": "http://arxiv.org/abs/2112.11294",
          "publishedOn": "2022-01-06T00:40:23.551Z",
          "wordCount": 601,
          "title": "Extending CLIP for Category-to-image Retrieval in E-commerce. (arXiv:2112.11294v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yongchun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Deqing Wang</a>",
          "description": "While Unsupervised Domain Adaptation (UDA) algorithms, i.e., there are only\nlabeled data from source domains, have been actively studied in recent years,\nmost algorithms and theoretical results focus on Single-source Unsupervised\nDomain Adaptation (SUDA). However, in the practical scenario, labeled data can\nbe typically collected from multiple diverse sources, and they might be\ndifferent not only from the target domain but also from each other. Thus,\ndomain adapters from multiple sources should not be modeled in the same way.\nRecent deep learning based Multi-source Unsupervised Domain Adaptation (MUDA)\nalgorithms focus on extracting common domain-invariant representations for all\ndomains by aligning distribution of all pairs of source and target domains in a\ncommon feature space. However, it is often very hard to extract the same\ndomain-invariant representations for all domains in MUDA. In addition, these\nmethods match distributions without considering domain-specific decision\nboundaries between classes. To solve these problems, we propose a new framework\nwith two alignment stages for MUDA which not only respectively aligns the\ndistributions of each pair of source and target domains in multiple specific\nfeature spaces, but also aligns the outputs of classifiers by utilizing the\ndomain-specific decision boundaries. Extensive experiments demonstrate that our\nmethod can achieve remarkable results on popular benchmark datasets for image\nclassification.",
          "link": "http://arxiv.org/abs/2201.01003",
          "publishedOn": "2022-01-06T00:40:23.521Z",
          "wordCount": 651,
          "title": "Aligning Domain-specific Distribution and Classifier for Cross-domain Classification from Multiple Sources. (arXiv:2201.01003v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garofalo_A/0/1/0/all/0/1\">Angelo Garofalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ottavi_G/0/1/0/all/0/1\">Gianmarco Ottavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conti_F/0/1/0/all/0/1\">Francesco Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karunaratne_G/0/1/0/all/0/1\">Geethan Karunaratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boybat_I/0/1/0/all/0/1\">Irem Boybat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1\">Luca Benini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_D/0/1/0/all/0/1\">Davide Rossi</a>",
          "description": "Deployment of modern TinyML tasks on small battery-constrained IoT devices\nrequires high computational energy efficiency. Analog In-Memory Computing (IMC)\nusing non-volatile memory (NVM) promises major efficiency improvements in deep\nneural network (DNN) inference and serves as on-chip memory storage for DNN\nweights. However, IMC's functional flexibility limitations and their impact on\nperformance, energy, and area efficiency are not yet fully understood at the\nsystem level. To target practical end-to-end IoT applications, IMC arrays must\nbe enclosed in heterogeneous programmable systems, introducing new system-level\nchallenges which we aim at addressing in this work. We present a heterogeneous\ntightly-coupled clustered architecture integrating 8 RISC-V cores, an in-memory\ncomputing accelerator (IMA), and digital accelerators. We benchmark the system\non a highly heterogeneous workload such as the Bottleneck layer from a\nMobileNetV2, showing 11.5x performance and 9.5x energy efficiency improvements,\ncompared to highly optimized parallel execution on the cores. Furthermore, we\nexplore the requirements for end-to-end inference of a full mobile-grade DNN\n(MobileNetV2) in terms of IMC array resources, by scaling up our heterogeneous\narchitecture to a multi-array accelerator. Our results show that our solution,\non the end-to-end inference of the MobileNetV2, is one order of magnitude\nbetter in terms of execution latency than existing programmable architectures\nand two orders of magnitude better than state-of-the-art heterogeneous\nsolutions integrating in-memory computing analog cores.",
          "link": "http://arxiv.org/abs/2201.01089",
          "publishedOn": "2022-01-06T00:40:23.513Z",
          "wordCount": 683,
          "title": "A Heterogeneous In-Memory Computing Cluster For Flexible End-to-End Inference of Real-World Deep Neural Networks. (arXiv:2201.01089v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01660",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingxing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenrui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Junni Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hongkai Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>",
          "description": "Spectral-based graph neural networks (SGNNs) have been attracting increasing\nattention in graph representation learning. However, existing SGNNs are limited\nin implementing graph filters with rigid transforms (e.g., graph Fourier or\npredefined graph wavelet transforms) and cannot adapt to signals residing on\ngraphs and tasks at hand. In this paper, we propose a novel class of graph\nneural networks that realizes graph filters with adaptive graph wavelets.\nSpecifically, the adaptive graph wavelets are learned with neural\nnetwork-parameterized lifting structures, where structure-aware attention-based\nlifting operations (i.e., prediction and update operations) are developed to\njointly consider graph structures and node features. We propose to lift based\non diffusion wavelets to alleviate the structural information loss induced by\npartitioning non-bipartite graphs. By design, the locality and sparsity of the\nresulting wavelet transform as well as the scalability of the lifting structure\nare guaranteed. We further derive a soft-thresholding filtering operation by\nlearning sparse graph representations in terms of the learned wavelets,\nyielding a localized, efficient, and scalable wavelet-based graph filters. To\nensure that the learned graph representations are invariant to node\npermutations, a layer is employed at the input of the networks to reorder the\nnodes according to their local topology information. We evaluate the proposed\nnetworks in both node-level and graph-level representation learning tasks on\nbenchmark citation and bioinformatics graph datasets. Extensive experiments\ndemonstrate the superiority of the proposed networks over existing SGNNs in\nterms of accuracy, efficiency, and scalability.",
          "link": "http://arxiv.org/abs/2108.01660",
          "publishedOn": "2022-01-06T00:40:23.484Z",
          "wordCount": 701,
          "title": "Graph Neural Networks With Lifting-based Adaptive Graph Wavelets. (arXiv:2108.01660v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ginart_A/0/1/0/all/0/1\">Antonio Ginart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maaten_L/0/1/0/all/0/1\">Laurens van der Maaten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chuan Guo</a>",
          "description": "Recent data-extraction attacks have exposed that language models can memorize\nsome training samples verbatim. This is a vulnerability that can compromise the\nprivacy of the model's training data. In this work, we introduce SubMix: a\npractical protocol for private next-token prediction designed to prevent\nprivacy violations by language models that were fine-tuned on a private corpus\nafter pre-training on a public corpus. We show that SubMix limits the leakage\nof information that is unique to any individual user in the private corpus via\na relaxation of group differentially private prediction. Importantly, SubMix\nadmits a tight, data-dependent privacy accounting mechanism, which allows it to\nthwart existing data-extraction attacks while maintaining the utility of the\nlanguage model. SubMix is the first protocol that maintains privacy even when\npublicly releasing tens of thousands of next-token predictions made by large\ntransformer-based models such as GPT-2.",
          "link": "http://arxiv.org/abs/2201.00971",
          "publishedOn": "2022-01-06T00:40:23.456Z",
          "wordCount": 568,
          "title": "Submix: Practical Private Prediction for Large-Scale Language Models. (arXiv:2201.00971v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tavakoli_M/0/1/0/all/0/1\">Mohammadamin Tavakoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmakov_A/0/1/0/all/0/1\">Alexander Shmakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceccarelli_F/0/1/0/all/0/1\">Francesco Ceccarelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldi_P/0/1/0/all/0/1\">Pierre Baldi</a>",
          "description": "It is fundamental for science and technology to be able to predict chemical\nreactions and their properties. To achieve such skills, it is important to\ndevelop good representations of chemical reactions, or good deep learning\narchitectures that can learn such representations automatically from the data.\nThere is currently no universal and widely adopted method for robustly\nrepresenting chemical reactions. Most existing methods suffer from one or more\ndrawbacks, such as: (1) lacking universality; (2) lacking robustness; (3)\nlacking interpretability; or (4) requiring excessive manual pre-processing.\nHere we exploit graph-based representations of molecular structures to develop\nand test a hypergraph attention neural network approach to solve at once the\nreaction representation and property-prediction problems, alleviating the\naforementioned drawbacks. We evaluate this hypergraph representation in three\nexperiments using three independent data sets of chemical reactions. In all\nexperiments, the hypergraph-based approach matches or outperforms other\nrepresentations and their corresponding models of chemical reactions while\nyielding interpretable multi-level representations.",
          "link": "http://arxiv.org/abs/2201.01196",
          "publishedOn": "2022-01-06T00:40:23.413Z",
          "wordCount": null,
          "title": "Rxn Hypergraph: a Hypergraph Attention Model for Chemical Reaction Representation. (arXiv:2201.01196v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Curry_M/0/1/0/all/0/1\">Michael Curry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trott_A/0/1/0/all/0/1\">Alexander Trott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phade_S/0/1/0/all/0/1\">Soham Phade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Stephan Zheng</a>",
          "description": "Real economies can be seen as a sequential imperfect-information game with\nmany heterogeneous, interacting strategic agents of various agent types, such\nas consumers, firms, and governments. Dynamic general equilibrium models are\ncommon economic tools to model the economic activity, interactions, and\noutcomes in such systems. However, existing analytical and computational\nmethods struggle to find explicit equilibria when all agents are strategic and\ninteract, while joint learning is unstable and challenging. Amongst others, a\nkey reason is that the actions of one economic agent may change the reward\nfunction of another agent, e.g., a consumer's expendable income changes when\nfirms change prices or governments change taxes. We show that multi-agent deep\nreinforcement learning (RL) can discover stable solutions that are epsilon-Nash\nequilibria for a meta-game over agent types, in economic simulations with many\nagents, through the use of structured learning curricula and efficient GPU-only\nsimulation and training. Conceptually, our approach is more flexible and does\nnot need unrealistic assumptions, e.g., market clearing, that are commonly used\nfor analytical tractability. Our GPU implementation enables training and\nanalyzing economies with a large number of agents within reasonable time\nframes, e.g., training completes within a day. We demonstrate our approach in\nreal-business-cycle models, a representative family of DGE models, with 100\nworker-consumers, 10 firms, and a government who taxes and redistributes. We\nvalidate the learned meta-game epsilon-Nash equilibria through approximate\nbest-response analyses, show that RL policies align with economic intuitions,\nand that our approach is constructive, e.g., by explicitly learning a spectrum\nof meta-game epsilon-Nash equilibria in open RBC models.",
          "link": "http://arxiv.org/abs/2201.01163",
          "publishedOn": "2022-01-06T00:40:23.412Z",
          "wordCount": null,
          "title": "Finding General Equilibria in Many-Agent Economic Simulations Using Deep Reinforcement Learning. (arXiv:2201.01163v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2112.08447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoeiness_H/0/1/0/all/0/1\">Henrik Hoeiness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gjerde_K/0/1/0/all/0/1\">Kristoffer Gjerde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oggiano_L/0/1/0/all/0/1\">Luca Oggiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giljarhus_K/0/1/0/all/0/1\">Knut Erik Teigen Giljarhus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruocco_M/0/1/0/all/0/1\">Massimiliano Ruocco</a>",
          "description": "Approximating wind flows using computational fluid dynamics (CFD) methods can\nbe time-consuming. Creating a tool for interactively designing prototypes while\nobserving the wind flow change requires simpler models to simulate faster.\nInstead of running numerical approximations resulting in detailed calculations,\ndata-driven methods and deep learning might be able to give similar results in\na fraction of the time. This work rephrases the problem from computing 3D flow\nfields using CFD to a 2D image-to-image translation-based problem on the\nbuilding footprints to predict the flow field at pedestrian height level. We\ninvestigate the use of generative adversarial networks (GAN), such as Pix2Pix\n[1] and CycleGAN [2] representing state-of-the-art for image-to-image\ntranslation task in various domains as well as U-Net autoencoder [3]. The\nmodels can learn the underlying distribution of a dataset in a data-driven\nmanner, which we argue can help the model learn the underlying\nReynolds-averaged Navier-Stokes (RANS) equations from CFD. We experiment on\nnovel simulated datasets on various three-dimensional bluff-shaped buildings\nwith and without height information. Moreover, we present an extensive\nqualitative and quantitative evaluation of the generated images for a selection\nof models and compare their performance with the simulations delivered by CFD.\nWe then show that adding positional data to the input can produce more accurate\nresults by proposing a general framework for injecting such information on the\ndifferent architectures. Furthermore, we show that the models performances\nimprove by applying attention mechanisms and spectral normalization to\nfacilitate stable training.",
          "link": "http://arxiv.org/abs/2112.08447",
          "publishedOn": "2022-01-06T00:40:23.408Z",
          "wordCount": null,
          "title": "Positional Encoding Augmented GAN for the Assessment of Wind Flow for Pedestrian Comfort in Urban Areas. (arXiv:2112.08447v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01203",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Mohan_D/0/1/0/all/0/1\">Devina Mohan</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Scaife_A/0/1/0/all/0/1\">Anna M. M. Scaife</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Porter_F/0/1/0/all/0/1\">Fiona Porter</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Walmsley_M/0/1/0/all/0/1\">Mike Walmsley</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Bowles_M/0/1/0/all/0/1\">Micah Bowles</a>",
          "description": "In this work we use variational inference to quantify the degree of\nuncertainty in deep learning model predictions of radio galaxy classification.\nWe show that the level of model posterior variance for individual test samples\nis correlated with human uncertainty when labelling radio galaxies. We explore\nthe model performance and uncertainty calibration for a variety of different\nweight priors and suggest that a sparse prior produces more well-calibrated\nuncertainty estimates. Using the posterior distributions for individual\nweights, we show that we can prune 30% of the fully-connected layer weights\nwithout significant loss of performance by removing the weights with the lowest\nsignal-to-noise ratio (SNR). We demonstrate that a larger degree of pruning can\nbe achieved using a Fisher information based ranking, but we note that both\npruning methods affect the uncertainty calibration for Fanaroff-Riley type I\nand type II radio galaxies differently. Finally we show that, like other work\nin this field, we experience a cold posterior effect, whereby the posterior\nmust be down-weighted to achieve good predictive performance. We examine\nwhether adapting the cost function to accommodate model misspecification can\ncompensate for this effect, but find that it does not make a significant\ndifference. We also examine the effect of principled data augmentation and find\nthat this improves upon the baseline but also does not compensate for the\nobserved effect. We interpret this as the cold posterior effect being due to\nthe overly effective curation of our training sample leading to likelihood\nmisspecification, and raise this as a potential issue for Bayesian deep\nlearning approaches to radio galaxy classification in future.",
          "link": "http://arxiv.org/abs/2201.01203",
          "publishedOn": "2022-01-06T00:40:23.407Z",
          "wordCount": null,
          "title": "Quantifying Uncertainty in Deep Learning Approaches to Radio Galaxy Classification. (arXiv:2201.01203v1 [astro-ph.CO])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00849",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shenwang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Bo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tingfa Xu</a>",
          "description": "Corrupted labels and class imbalance are commonly encountered in practically\ncollected training data, which easily leads to over-fitting of deep neural\nnetworks (DNNs). Existing approaches alleviate these issues by adopting a\nsample re-weighting strategy, which is to re-weight sample by designing\nweighting function. However, it is only applicable for training data containing\nonly either one type of data biases. In practice, however, biased samples with\ncorrupted labels and of tailed classes commonly co-exist in training data. How\nto handle them simultaneously is a key but under-explored problem. In this\npaper, we find that these two types of biased samples, though have similar\ntransient loss, have distinguishable trend and characteristics in loss curves,\nwhich could provide valuable priors for sample weight assignment. Motivated by\nthis, we delve into the loss curves and propose a novel probe-and-allocate\ntraining strategy: In the probing stage, we train the network on the whole\nbiased training data without intervention, and record the loss curve of each\nsample as an additional attribute; In the allocating stage, we feed the\nresulting attribute to a newly designed curve-perception network, named\nCurveNet, to learn to identify the bias type of each sample and assign proper\nweights through meta-learning adaptively. The training speed of meta learning\nalso blocks its application. To solve it, we propose a method named skip layer\nmeta optimization (SLMO) to accelerate training speed by skipping the bottom\nlayers. Extensive synthetic and real experiments well validate the proposed\nmethod, which achieves state-of-the-art performance on multiple challenging\nbenchmarks.",
          "link": "http://arxiv.org/abs/2201.00849",
          "publishedOn": "2022-01-06T00:40:23.403Z",
          "wordCount": 691,
          "title": "Delving into Sample Loss Curve to Embrace Noisy and Imbalanced Data. (arXiv:2201.00849v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+A_R/0/1/0/all/0/1\">Rajagopal A</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_N/0/1/0/all/0/1\">Nirmala V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedamanickam_A/0/1/0/all/0/1\">Arun Muthuraj Vedamanickam</a>",
          "description": "There is amazing progress in Deep Learning based models for Image captioning\nand Low Light image enhancement. For the first time in literature, this paper\ndevelops a Deep Learning model that translates night scenes to sentences,\nopening new possibilities for AI applications in the safety of visually\nimpaired women. Inspired by Image Captioning and Visual Question Answering, a\nnovel Interactive Image Captioning is developed. A user can make the AI focus\non any chosen person of interest by influencing the attention scoring.\nAttention context vectors are computed from CNN feature vectors and\nuser-provided start word. The Encoder-Attention-Decoder neural network learns\nto produce captions from low brightness images. This paper demonstrates how\nwomen safety can be enabled by researching a novel AI capability in the\nInteractive Vision-Language model for perception of the environment in the\nnight.",
          "link": "http://arxiv.org/abs/2201.00969",
          "publishedOn": "2022-01-06T00:40:23.393Z",
          "wordCount": null,
          "title": "Interactive Attention AI to translate low light photos to captions for night scene understanding in women safety. (arXiv:2201.00969v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahate_A/0/1/0/all/0/1\">Anil Rahate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walambe_R/0/1/0/all/0/1\">Rahee Walambe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanna_S/0/1/0/all/0/1\">Sheela Ramanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotecha_K/0/1/0/all/0/1\">Ketan Kotecha</a>",
          "description": "Multimodal deep learning systems which employ multiple modalities like text,\nimage, audio, video, etc., are showing better performance in comparison with\nindividual modalities (i.e., unimodal) systems. Multimodal machine learning\ninvolves multiple aspects: representation, translation, alignment, fusion, and\nco-learning. In the current state of multimodal machine learning, the\nassumptions are that all modalities are present, aligned, and noiseless during\ntraining and testing time. However, in real-world tasks, typically, it is\nobserved that one or more modalities are missing, noisy, lacking annotated\ndata, have unreliable labels, and are scarce in training or testing and or\nboth. This challenge is addressed by a learning paradigm called multimodal\nco-learning. The modeling of a (resource-poor) modality is aided by exploiting\nknowledge from another (resource-rich) modality using transfer of knowledge\nbetween modalities, including their representations and predictive models.\nCo-learning being an emerging area, there are no dedicated reviews explicitly\nfocusing on all challenges addressed by co-learning. To that end, in this work,\nwe provide a comprehensive survey on the emerging area of multimodal\nco-learning that has not been explored in its entirety yet. We review\nimplementations that overcome one or more co-learning challenges without\nexplicitly considering them as co-learning challenges. We present the\ncomprehensive taxonomy of multimodal co-learning based on the challenges\naddressed by co-learning and associated implementations. The various techniques\nemployed to include the latest ones are reviewed along with some of the\napplications and datasets. Our final goal is to discuss challenges and\nperspectives along with the important ideas and directions for future work that\nwe hope to be beneficial for the entire research community focusing on this\nexciting domain.",
          "link": "http://arxiv.org/abs/2107.13782",
          "publishedOn": "2022-01-06T00:40:23.392Z",
          "wordCount": null,
          "title": "Multimodal Co-learning: Challenges, Applications with Datasets, Recent Advances and Future Directions. (arXiv:2107.13782v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01195",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Winkler_L/0/1/0/all/0/1\">Ludwig Winkler</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Muller_K/0/1/0/all/0/1\">Klaus-Robert M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sauceda_H/0/1/0/all/0/1\">Huziel E. Sauceda</a>",
          "description": "Molecular dynamics simulations are a cornerstone in science, allowing to\ninvestigate from the system's thermodynamics to analyse intricate molecular\ninteractions. In general, to create extended molecular trajectories can be a\ncomputationally expensive process, for example, when running $ab-initio$\nsimulations. Hence, repeating such calculations to either obtain more accurate\nthermodynamics or to get a higher resolution in the dynamics generated by a\nfine-grained quantum interaction can be time- and computationally-consuming. In\nthis work, we explore different machine learning (ML) methodologies to increase\nthe resolution of molecular dynamics trajectories on-demand within a\npost-processing step. As a proof of concept, we analyse the performance of\nbi-directional neural networks such as neural ODEs, Hamiltonian networks,\nrecurrent neural networks and LSTMs, as well as the uni-directional variants as\na reference, for molecular dynamics simulations (here: the MD17 dataset). We\nhave found that Bi-LSTMs are the best performing models; by utilizing the local\ntime-symmetry of thermostated trajectories they can even learn long-range\ncorrelations and display high robustness to noisy dynamics across molecular\ncomplexity. Our models can reach accuracies of up to 10$^{-4}$ angstroms in\ntrajectory interpolation, while faithfully reconstructing several full cycles\nof unseen intricate high-frequency molecular vibrations, rendering the\ncomparison between the learned and reference trajectories indistinguishable.\nThe results reported in this work can serve (1) as a baseline for larger\nsystems, as well as (2) for the construction of better MD integrators.",
          "link": "http://arxiv.org/abs/2201.01195",
          "publishedOn": "2022-01-06T00:40:23.390Z",
          "wordCount": null,
          "title": "Super-resolution in Molecular Dynamics Trajectory Reconstruction with Bi-Directional Neural Networks. (arXiv:2201.01195v1 [physics.comp-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David A. Noever</a>",
          "description": "Change detection methods applied to monitoring key infrastructure like\nairport runways represent an important capability for disaster relief and urban\nplanning. The present work identifies two generative adversarial networks (GAN)\narchitectures that translate reversibly between plausible runway maps and\nsatellite imagery. We illustrate the training capability using paired images\n(satellite-map) from the same point of view and using the Pix2Pix architecture\nor conditional GANs. In the absence of available pairs, we likewise show that\nCycleGAN architectures with four network heads (discriminator-generator pairs)\ncan also provide effective style transfer from raw image pixels to outline or\nfeature maps. To emphasize the runway and tarmac boundaries, we experimentally\nshow that the traditional grey-tan map palette is not a required training input\nbut can be augmented by higher contrast mapping palettes (red-black) for\nsharper runway boundaries. We preview a potentially novel use case (called\n\"sketch2satellite\") where a human roughly draws the current runway boundaries\nand automates the machine output of plausible satellite images. Finally, we\nidentify examples of faulty runway maps where the published satellite and\nmapped runways disagree but an automated update renders the correct map using\nGANs.",
          "link": "http://arxiv.org/abs/2201.00848",
          "publishedOn": "2022-01-06T00:40:23.389Z",
          "wordCount": null,
          "title": "Runway Extraction and Improved Mapping from Space Imagery. (arXiv:2201.00848v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yongchun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingwu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhiping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenjuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>",
          "description": "In image classification, it is often expensive and time-consuming to acquire\nsufficient labels. To solve this problem, domain adaptation often provides an\nattractive option given a large amount of labeled data from a similar nature\nbut different domain. Existing approaches mainly align the distributions of\nrepresentations extracted by a single structure and the representations may\nonly contain partial information, e.g., only contain part of the saturation,\nbrightness, and hue information. Along this line, we propose\nMulti-Representation Adaptation which can dramatically improve the\nclassification accuracy for cross-domain image classification and specially\naims to align the distributions of multiple representations extracted by a\nhybrid structure named Inception Adaptation Module (IAM). Based on this, we\npresent Multi-Representation Adaptation Network (MRAN) to accomplish the\ncross-domain image classification task via multi-representation alignment which\ncan capture the information from different aspects. In addition, we extend\nMaximum Mean Discrepancy (MMD) to compute the adaptation loss. Our approach can\nbe easily implemented by extending most feed-forward models with IAM, and the\nnetwork can be trained efficiently via back-propagation. Experiments conducted\non three benchmark image datasets demonstrate the effectiveness of MRAN. The\ncode has been available at https://github.com/easezyc/deep-transfer-learning.",
          "link": "http://arxiv.org/abs/2201.01002",
          "publishedOn": "2022-01-06T00:40:23.367Z",
          "wordCount": null,
          "title": "Multi-Representation Adaptation Network for Cross-domain Image Classification. (arXiv:2201.01002v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.12676",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mehari_T/0/1/0/all/0/1\">Temesgen Mehari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strodthoff_N/0/1/0/all/0/1\">Nils Strodthoff</a>",
          "description": "Clinical 12-lead electrocardiography (ECG) is one of the most widely\nencountered kinds of biosignals. Despite the increased availability of public\nECG datasets, label scarcity remains a central challenge in the field.\nSelf-supervised learning represents a promising way to alleviate this issue. In\nthis work, we put forward the first comprehensive assessment of self-supervised\nrepresentation learning from clinical 12-lead ECG data. To this end, we adapt\nstate-of-the-art self-supervised methods based on instance discrimination and\nlatent forecasting to the ECG domain. In a first step, we learn contrastive\nrepresentations and evaluate their quality based on linear evaluation\nperformance on a recently established, comprehensive, clinical ECG\nclassification task. In a second step, we analyze the impact of self-supervised\npretraining on finetuned ECG classifiers as compared to purely supervised\nperformance. For the best-performing method, an adaptation of contrastive\npredictive coding, we find a linear evaluation performance only 0.5% below\nsupervised performance. For the finetuned models, we find improvements in\ndownstream performance of roughly 1% compared to supervised performance, label\nefficiency, as well as robustness against physiological noise. This work\nclearly establishes the feasibility of extracting discriminative\nrepresentations from ECG data via self-supervised learning and the numerous\nadvantages when finetuning such representations on downstream tasks as compared\nto purely supervised training. As first comprehensive assessment of its kind in\nthe ECG domain carried out exclusively on publicly available datasets, we hope\nto establish a first step towards reproducible progress in the rapidly evolving\nfield of representation learning for biosignals.",
          "link": "http://arxiv.org/abs/2103.12676",
          "publishedOn": "2022-01-06T00:40:23.353Z",
          "wordCount": null,
          "title": "Self-supervised representation learning from 12-lead ECG data. (arXiv:2103.12676v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kikuchi_Y/0/1/0/all/0/1\">Yusuke Kikuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jinglin Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1\">Qiong Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Rui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xin Guo</a>",
          "description": "Retinal vascular diseases affect the well-being of human body and sometimes\nprovide vital signs of otherwise undetected bodily damage. Recently, deep\nlearning techniques have been successfully applied for detection of diabetic\nretinopathy (DR). The main obstacle of applying deep learning techniques to\ndetect most other retinal vascular diseases is the limited amount of data\navailable. In this paper, we propose a transfer learning technique that aims to\nutilize the feature similarities for detecting retinal vascular diseases. We\nchoose the well-studied DR detection as a source task and identify the early\ndetection of retinopathy of prematurity (ROP) as the target task. Our\nexperimental results demonstrate that our DR-pretrained approach dominates in\nall metrics the conventional ImageNet-pretrained transfer learning approach,\ncurrently adopted in medical image analysis. Moreover, our approach is more\nrobust with respect to the stochasticity in the training process and with\nrespect to reduced training samples. This study suggests the potential of our\nproposed transfer learning approach for a broad range of retinal vascular\ndiseases or pathologies, where data is limited.",
          "link": "http://arxiv.org/abs/2201.01250",
          "publishedOn": "2022-01-06T00:40:23.349Z",
          "wordCount": null,
          "title": "Transfer Learning for Retinal Vascular Disease Detection: A Pilot Study with Diabetic Retinopathy and Retinopathy of Prematurity. (arXiv:2201.01250v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2005.08334",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Llorente_F/0/1/0/all/0/1\">Fernando Llorente</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Martino_L/0/1/0/all/0/1\">Luca Martino</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Delgado_D/0/1/0/all/0/1\">David Delgado</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lopez_Santiago_J/0/1/0/all/0/1\">Javier Lopez-Santiago</a>",
          "description": "This is an up-to-date introduction to, and overview of, marginal likelihood\ncomputation for model selection and hypothesis testing. Computing normalizing\nconstants of probability models (or ratio of constants) is a fundamental issue\nin many applications in statistics, applied mathematics, signal processing and\nmachine learning. This article provides a comprehensive study of the\nstate-of-the-art of the topic. We highlight limitations, benefits, connections\nand differences among the different techniques. Problems and possible solutions\nwith the use of improper priors are also described. Some of the most relevant\nmethodologies are compared through theoretical comparisons and numerical\nexperiments.",
          "link": "http://arxiv.org/abs/2005.08334",
          "publishedOn": "2022-01-06T00:40:23.345Z",
          "wordCount": null,
          "title": "Marginal likelihood computation for model selection and hypothesis testing: an extensive review. (arXiv:2005.08334v4 [stat.CO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1\">Xueguang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baisero_A/0/1/0/all/0/1\">Andrea Baisero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yuchen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amato_C/0/1/0/all/0/1\">Christopher Amato</a>",
          "description": "Centralized Training for Decentralized Execution, where training is done in a\ncentralized offline fashion, has become a popular solution paradigm in\nMulti-Agent Reinforcement Learning. Many such methods take the form of\nactor-critic with state-based critics, since centralized training allows access\nto the true system state, which can be useful during training despite not being\navailable at execution time. State-based critics have become a common empirical\nchoice, albeit one which has had limited theoretical justification or analysis.\nIn this paper, we show that state-based critics can introduce bias in the\npolicy gradient estimates, potentially undermining the asymptotic guarantees of\nthe algorithm. We also show that, even if the state-based critics do not\nintroduce any bias, they can still result in a larger gradient variance,\ncontrary to the common intuition. Finally, we show the effects of the theories\nin practice by comparing different forms of centralized critics on a wide range\nof common benchmarks, and detail how various environmental properties are\nrelated to the effectiveness of different types of critics.",
          "link": "http://arxiv.org/abs/2201.01221",
          "publishedOn": "2022-01-06T00:40:23.344Z",
          "wordCount": null,
          "title": "A Deeper Understanding of State-Based Critics in Multi-Agent Reinforcement Learning. (arXiv:2201.01221v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.15991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kursa_M/0/1/0/all/0/1\">Miron Bartosz Kursa</a>",
          "description": "Kendall transformation is a conversion of an ordered feature into a vector of\npairwise order relations between individual values. This way, it preserves\nranking of observations and represents it in a categorical form.\n\nSuch transformation allows for generalisation of methods requiring strictly\ncategorical input, especially in the limit of small number of observations,\nwhen discretisation becomes problematic. In particular, many approaches of\ninformation theory can be directly applied to Kendall-transformed continuous\ndata without relying on differential entropy or any additional parameters.\nMoreover, by filtering information to this contained in ranking, Kendall\ntransformation leads to a better robustness at a reasonable cost of dropping\nsophisticated interactions which are anyhow unlikely to be correctly estimated.\n\nIn bivariate analysis, Kendall transformation can be related to popular\nnon-parametric methods, showing the soundness of the approach. The paper also\ndemonstrates its efficiency in multivariate problems, as well as provides an\nexample analysis of a real-world data.",
          "link": "http://arxiv.org/abs/2006.15991",
          "publishedOn": "2022-01-06T00:40:23.344Z",
          "wordCount": null,
          "title": "Kendall transformation: a robust representation of continuous data for information theory. (arXiv:2006.15991v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Taywade_K/0/1/0/all/0/1\">Kshitija Taywade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_B/0/1/0/all/0/1\">Brent Harrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagh_A/0/1/0/all/0/1\">Adib Bagh</a>",
          "description": "We investigate the use of a multi-agent multi-armed bandit (MA-MAB) setting\nfor modeling repeated Cournot oligopoly games, where the firms acting as agents\nchoose from the set of arms representing production quantity (a discrete\nvalue). Agents interact with separate and independent bandit problems. In this\nformulation, each agent makes sequential choices among arms to maximize its own\nreward. Agents do not have any information about the environment; they can only\nsee their own rewards after taking an action. However, the market demand is a\nstationary function of total industry output, and random entry or exit from the\nmarket is not allowed. Given these assumptions, we found that an\n$\\epsilon$-greedy approach offers a more viable learning mechanism than other\ntraditional MAB approaches, as it does not require any additional knowledge of\nthe system to operate. We also propose two novel approaches that take advantage\nof the ordered action space: $\\epsilon$-greedy+HL and $\\epsilon$-greedy+EL.\nThese new approaches help firms to focus on more profitable actions by\neliminating less profitable choices and hence are designed to optimize the\nexploration. We use computer simulations to study the emergence of various\nequilibria in the outcomes and do the empirical analysis of joint cumulative\nregrets.",
          "link": "http://arxiv.org/abs/2201.01182",
          "publishedOn": "2022-01-06T00:40:23.343Z",
          "wordCount": null,
          "title": "Modelling Cournot Games as Multi-agent Multi-armed Bandits. (arXiv:2201.01182v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01246",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Dou_T/0/1/0/all/0/1\">Tong Dou</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhang_G/0/1/0/all/0/1\">Guofeng Zhang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Cui_W/0/1/0/all/0/1\">Wei Cui</a>",
          "description": "Recent work has begun to explore the potential of parametrized quantum\ncircuits (PQCs) as general function approximators. In this work, we propose a\nquantum-classical deep network structure to enhance classical CNN model\ndiscriminability. The convolutional layer uses linear filters to scan the input\ndata. Moreover, we build PQC, which is a more potent function approximator,\nwith more complex structures to capture the features within the receptive\nfield. The feature maps are obtained by sliding the PQCs over the input in a\nsimilar way as CNN. We also give a training algorithm for the proposed model.\nThe hybrid models used in our design are validated by numerical simulation. We\ndemonstrate the reasonable classification performances on MNIST and we compare\nthe performances with models in different settings. The results disclose that\nthe model with ansatz in high expressibility achieves lower cost and higher\naccuracy.",
          "link": "http://arxiv.org/abs/2201.01246",
          "publishedOn": "2022-01-06T00:40:23.342Z",
          "wordCount": null,
          "title": "Efficient Quantum Feature Extraction for CNN-based Learning. (arXiv:2201.01246v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kissas_G/0/1/0/all/0/1\">Georgios Kissas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seidman_J/0/1/0/all/0/1\">Jacob Seidman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guilhoto_L/0/1/0/all/0/1\">Leonardo Ferreira Guilhoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preciado_V/0/1/0/all/0/1\">Victor M. Preciado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_G/0/1/0/all/0/1\">George J. Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perdikaris_P/0/1/0/all/0/1\">Paris Perdikaris</a>",
          "description": "Supervised operator learning is an emerging machine learning paradigm with\napplications to modeling the evolution of spatio-temporal dynamical systems and\napproximating general black-box relationships between functional data. We\npropose a novel operator learning method, LOCA (Learning Operators with Coupled\nAttention), motivated from the recent success of the attention mechanism. In\nour architecture, the input functions are mapped to a finite set of features\nwhich are then averaged with attention weights that depend on the output query\nlocations. By coupling these attention weights together with an integral\ntransform, LOCA is able to explicitly learn correlations in the target output\nfunctions, enabling us to approximate nonlinear operators even when the number\nof output function in the training set measurements is very small. Our\nformulation is accompanied by rigorous approximation theoretic guarantees on\nthe universal expressiveness of the proposed model. Empirically, we evaluate\nthe performance of LOCA on several operator learning scenarios involving\nsystems governed by ordinary and partial differential equations, as well as a\nblack-box climate prediction problem. Through these scenarios we demonstrate\nstate of the art accuracy, robustness with respect to noisy input data, and a\nconsistently small spread of errors over testing data sets, even for\nout-of-distribution prediction tasks.",
          "link": "http://arxiv.org/abs/2201.01032",
          "publishedOn": "2022-01-06T00:40:23.340Z",
          "wordCount": null,
          "title": "Learning Operators with Coupled Attention. (arXiv:2201.01032v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01289",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenwu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengtao Xie</a>",
          "description": "Conventional machine learning (ML) relies heavily on manual design from\nmachine learning experts to decide learning tasks, data, models, optimization\nalgorithms, and evaluation metrics, which is labor-intensive, time-consuming,\nand cannot learn autonomously like humans. In education science, self-directed\nlearning, where human learners select learning tasks and materials on their own\nwithout requiring hands-on guidance, has been shown to be more effective than\npassive teacher-guided learning. Inspired by the concept of self-directed human\nlearning, we introduce the principal concept of Self-directed Machine Learning\n(SDML) and propose a framework for SDML. Specifically, we design SDML as a\nself-directed learning process guided by self-awareness, including internal\nawareness and external awareness. Our proposed SDML process benefits from self\ntask selection, self data selection, self model selection, self optimization\nstrategy selection and self evaluation metric selection through self-awareness\nwithout human guidance. Meanwhile, the learning performance of the SDML process\nserves as feedback to further improve self-awareness. We propose a mathematical\nformulation for SDML based on multi-level optimization. Furthermore, we present\ncase studies together with potential applications of SDML, followed by\ndiscussing future research directions. We expect that SDML could enable\nmachines to conduct human-like self-directed learning and provide a new\nperspective towards artificial general intelligence.",
          "link": "http://arxiv.org/abs/2201.01289",
          "publishedOn": "2022-01-06T00:40:23.339Z",
          "wordCount": null,
          "title": "Self-directed Machine Learning. (arXiv:2201.01289v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1912.11090",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hoop_M/0/1/0/all/0/1\">Maarten V. de Hoop</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lassas_M/0/1/0/all/0/1\">Matti Lassas</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wong_C/0/1/0/all/0/1\">Christopher A. Wong</a>",
          "description": "We develop a theoretical analysis for special neural network architectures,\ntermed operator recurrent neural networks, for approximating nonlinear\nfunctions whose inputs are linear operators. Such functions commonly arise in\nsolution algorithms for inverse boundary value problems. Traditional neural\nnetworks treat input data as vectors, and thus they do not effectively capture\nthe multiplicative structure associated with the linear operators that\ncorrespond to the data in such inverse problems. We therefore introduce a new\nfamily that resembles a standard neural network architecture, but where the\ninput data acts multiplicatively on vectors. Motivated by compact operators\nappearing in boundary control and the analysis of inverse boundary value\nproblems for the wave equation, we promote structure and sparsity in selected\nweight matrices in the network. After describing this architecture, we study\nits representation properties as well as its approximation properties. We\nfurthermore show that an explicit regularization can be introduced that can be\nderived from the mathematical analysis of the mentioned inverse problems, and\nwhich leads to certain guarantees on the generalization properties. We observe\nthat the sparsity of the weight matrices improves the generalization estimates.\nLastly, we discuss how operator recurrent networks can be viewed as a deep\nlearning analogue to deterministic algorithms such as boundary control for\nreconstructing the unknown wavespeed in the acoustic wave equation from\nboundary measurements.",
          "link": "http://arxiv.org/abs/1912.11090",
          "publishedOn": "2022-01-06T00:40:23.307Z",
          "wordCount": null,
          "title": "Deep learning architectures for nonlinear operator functions and nonlinear inverse problems. (arXiv:1912.11090v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01044",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yiran Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaal_N/0/1/0/all/0/1\">Nicole Schaal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hefenbrock_M/0/1/0/all/0/1\">Michael Hefenbrock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yexu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_T/0/1/0/all/0/1\">Till Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Likun Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beigl_M/0/1/0/all/0/1\">Michael Beigl</a>",
          "description": "To this day, a variety of approaches for providing local interpretability of\nblack-box machine learning models have been introduced. Unfortunately, all of\nthese methods suffer from one or more of the following deficiencies: They are\neither difficult to understand themselves, they work on a per-feature basis and\nignore the dependencies between features and/or they only focus on those\nfeatures asserting the decision made by the model. To address these points,\nthis work introduces a reinforcement learning-based approach called Monte Carlo\ntree search for eXplainable Artificial Intelligent (McXai) to explain the\ndecisions of any black-box classification model (classifier). Our method\nleverages Monte Carlo tree search and models the process of generating\nexplanations as two games. In one game, the reward is maximized by finding\nfeature sets that support the decision of the classifier, while in the second\ngame, finding feature sets leading to alternative decisions maximizes the\nreward. The result is a human friendly representation as a tree structure, in\nwhich each node represents a set of features to be studied with smaller\nexplanations at the top of the tree. Our experiments show, that the features\nfound by our method are more informative with respect to classifications than\nthose found by classical approaches like LIME and SHAP. Furthermore, by also\nidentifying misleading features, our approach is able to guide towards improved\nrobustness of the black-box model in many situations.",
          "link": "http://arxiv.org/abs/2201.01044",
          "publishedOn": "2022-01-06T00:40:23.306Z",
          "wordCount": null,
          "title": "McXai: Local model-agnostic explanation as two games. (arXiv:2201.01044v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01266",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hatamizadeh_A/0/1/0/all/0/1\">Ali Hatamizadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nath_V/0/1/0/all/0/1\">Vishwesh Nath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1\">Yucheng Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roth_H/0/1/0/all/0/1\">Holger Roth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>",
          "description": "Semantic segmentation of brain tumors is a fundamental medical image analysis\ntask involving multiple MRI imaging modalities that can assist clinicians in\ndiagnosing the patient and successively studying the progression of the\nmalignant entity. In recent years, Fully Convolutional Neural Networks (FCNNs)\napproaches have become the de facto standard for 3D medical image segmentation.\nThe popular \"U-shaped\" network architecture has achieved state-of-the-art\nperformance benchmarks on different 2D and 3D semantic segmentation tasks and\nacross various imaging modalities. However, due to the limited kernel size of\nconvolution layers in FCNNs, their performance of modeling long-range\ninformation is sub-optimal, and this can lead to deficiencies in the\nsegmentation of tumors with variable sizes. On the other hand, transformer\nmodels have demonstrated excellent capabilities in capturing such long-range\ninformation in multiple domains, including natural language processing and\ncomputer vision. Inspired by the success of vision transformers and their\nvariants, we propose a novel segmentation model termed Swin UNEt TRansformers\n(Swin UNETR). Specifically, the task of 3D brain tumor semantic segmentation is\nreformulated as a sequence to sequence prediction problem wherein multi-modal\ninput data is projected into a 1D sequence of embedding and used as an input to\na hierarchical Swin transformer as the encoder. The swin transformer encoder\nextracts features at five different resolutions by utilizing shifted windows\nfor computing self-attention and is connected to an FCNN-based decoder at each\nresolution via skip connections. We have participated in BraTS 2021\nsegmentation challenge, and our proposed model ranks among the top-performing\napproaches in the validation phase. Code: https://monai.io/research/swin-unetr",
          "link": "http://arxiv.org/abs/2201.01266",
          "publishedOn": "2022-01-06T00:40:23.306Z",
          "wordCount": null,
          "title": "Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images. (arXiv:2201.01266v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_V/0/1/0/all/0/1\">Vasileios Iosifidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Arjun Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1\">Eirini Ntoutsi</a>",
          "description": "Data-driven AI systems can lead to discrimination on the basis of protected\nattributes like gender or race. One reason for this behavior is the encoded\nsocietal biases in the training data (e.g., females are underrepresented),\nwhich is aggravated in the presence of unbalanced class distributions (e.g.,\n\"granted\" is the minority class). State-of-the-art fairness-aware machine\nlearning approaches focus on preserving the \\emph{overall} classification\naccuracy while improving fairness. In the presence of class-imbalance, such\nmethods may further aggravate the problem of discrimination by denying an\nalready underrepresented group (e.g., \\textit{females}) the fundamental rights\nof equal social privileges (e.g., equal credit opportunity).\n\nTo this end, we propose AdaFair, a fairness-aware boosting ensemble that\nchanges the data distribution at each round, taking into account not only the\nclass errors but also the fairness-related performance of the model defined\ncumulatively based on the partial ensemble. Except for the in-training boosting\nof the group discriminated over each round, AdaFair directly tackles imbalance\nduring the post-training phase by optimizing the number of ensemble learners\nfor balanced error performance (BER). AdaFair can facilitate different\nparity-based fairness notions and mitigate effectively discriminatory outcomes.\nOur experiments show that our approach can achieve parity in terms of\nstatistical parity, equal opportunity, and disparate mistreatment while\nmaintaining good predictive performance for all classes.",
          "link": "http://arxiv.org/abs/2201.01148",
          "publishedOn": "2022-01-06T00:40:23.305Z",
          "wordCount": null,
          "title": "Parity-based Cumulative Fairness-aware Boosting. (arXiv:2201.01148v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.11408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Junfeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ting_W/0/1/0/all/0/1\">Wei-Te Ting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kung_H/0/1/0/all/0/1\">H.T. Kung</a>",
          "description": "Various approaches have been proposed for out-of-distribution (OOD) detection\nby augmenting models, input examples, training sets, and optimization\nobjectives. Deviating from existing work, we have a simple hypothesis that\nstandard off-the-shelf models may already contain sufficient information about\nthe training set distribution which can be leveraged for reliable OOD\ndetection. Our empirical study on validating this hypothesis, which measures\nthe model activation's mean for OOD and in-distribution (ID) mini-batches,\nsurprisingly finds that activation means of OOD mini-batches consistently\ndeviate more from those of the training data. In addition, training data's\nactivation means can be computed offline efficiently or retrieved from batch\nnormalization layers as a `free lunch'. Based upon this observation, we propose\na novel metric called Neural Mean Discrepancy (NMD), which compares neural\nmeans of the input examples and training data. Leveraging the simplicity of\nNMD, we propose an efficient OOD detector that computes neural means by a\nstandard forward pass followed by a lightweight classifier. Extensive\nexperiments show that NMD outperforms state-of-the-art OOD approaches across\nmultiple datasets and model architectures in terms of both detection accuracy\nand computational cost.",
          "link": "http://arxiv.org/abs/2104.11408",
          "publishedOn": "2022-01-06T00:40:23.304Z",
          "wordCount": null,
          "title": "Neural Mean Discrepancy for Efficient Out-of-Distribution Detection. (arXiv:2104.11408v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.14679",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rotem_N/0/1/0/all/0/1\">Nadav Rotem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cummins_C/0/1/0/all/0/1\">Chris Cummins</a>",
          "description": "Profile guided optimization is an effective technique for improving the\noptimization ability of compilers based on dynamic behavior, but collecting\nprofile data is expensive, cumbersome, and requires regular updating to remain\nfresh. We present a novel statistical approach to inferring branch\nprobabilities that improves the performance of programs that are compiled\nwithout profile guided optimizations. We perform offline training using\ninformation that is collected from a large corpus of binaries that have branch\nprobabilities information. The learned model is used by the compiler to predict\nthe branch probabilities of regular uninstrumented programs, which the compiler\ncan then use to inform optimization decisions. We integrate our technique\ndirectly in LLVM, supplementing the existing human-engineered compiler\nheuristics. We evaluate our technique on a suite of benchmarks, demonstrating\nsome gains over compiling without profile information. In deployment, our\ntechnique requires no profiling runs and has negligible effect on compilation\ntime.",
          "link": "http://arxiv.org/abs/2112.14679",
          "publishedOn": "2022-01-06T00:40:23.303Z",
          "wordCount": null,
          "title": "Profile Guided Optimization without Profiles: A Machine Learning Approach. (arXiv:2112.14679v2 [cs.PL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yongchun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_D/0/1/0/all/0/1\">Dongbo Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1\">Bowen Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>",
          "description": "With the explosive growth of the e-commerce industry, detecting online\ntransaction fraud in real-world applications has become increasingly important\nto the development of e-commerce platforms. The sequential behavior history of\nusers provides useful information in differentiating fraudulent payments from\nregular ones. Recently, some approaches have been proposed to solve this\nsequence-based fraud detection problem. However, these methods usually suffer\nfrom two problems: the prediction results are difficult to explain and the\nexploitation of the internal information of behaviors is insufficient. To\ntackle the above two problems, we propose a Hierarchical Explainable Network\n(HEN) to model users' behavior sequences, which could not only improve the\nperformance of fraud detection but also make the inference process\ninterpretable. Meanwhile, as e-commerce business expands to new domains, e.g.,\nnew countries or new markets, one major problem for modeling user behavior in\nfraud detection systems is the limitation of data collection, e.g., very few\ndata/labels available. Thus, in this paper, we further propose a transfer\nframework to tackle the cross-domain fraud detection problem, which aims to\ntransfer knowledge from existing domains (source domains) with enough and\nmature data to improve the performance in the new domain (target domain). Our\nproposed method is a general transfer framework that could not only be applied\nupon HEN but also various existing models in the Embedding & MLP paradigm.\nBased on 90 transfer task experiments, we also demonstrate that our transfer\nframework could not only contribute to the cross-domain fraud detection task\nwith HEN, but also be universal and expandable for various existing models.",
          "link": "http://arxiv.org/abs/2201.01004",
          "publishedOn": "2022-01-06T00:40:23.302Z",
          "wordCount": null,
          "title": "Modeling Users' Behavior Sequences with Hierarchical Explainable Network for Cross-domain Fraud Detection. (arXiv:2201.01004v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bloemheuvel_S/0/1/0/all/0/1\">Stefan Bloemheuvel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoogen_J/0/1/0/all/0/1\">Jurgen van den Hoogen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jozinovic_D/0/1/0/all/0/1\">Dario Jozinovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michelini_A/0/1/0/all/0/1\">Alberto Michelini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atzmueller_M/0/1/0/all/0/1\">Martin Atzmueller</a>",
          "description": "Machine learning, with its advances in Deep Learning has shown great\npotential in analysing time series in the past. However, in many scenarios,\nadditional information is available that can potentially improve predictions,\nby incorporating it into the learning methods. This is crucial for data that\narises from e.g., sensor networks that contain information about sensor\nlocations. Then, such spatial information can be exploited by modeling it via\ngraph structures, along with the sequential (time) information. Recent advances\nin adapting Deep Learning to graphs have shown promising potential in various\ngraph-related tasks. However, these methods have not been adapted for time\nseries related tasks to a great extent. Specifically, most attempts have\nessentially consolidated around Spatial-Temporal Graph Neural Networks for time\nseries forecasting with small sequence lengths. Generally, these architectures\nare not suited for regression or classification tasks that contain large\nsequences of data. Therefore, in this work, we propose an architecture capable\nof processing these long sequences in a multivariate time series regression\ntask, using the benefits of Graph Neural Networks to improve predictions. Our\nmodel is tested on two seismic datasets that contain earthquake waveforms,\nwhere the goal is to predict intensity measurements of ground shaking at a set\nof stations. Our findings demonstrate promising results of our approach, which\nare discussed in depth with an additional ablation study.",
          "link": "http://arxiv.org/abs/2201.00818",
          "publishedOn": "2022-01-06T00:40:23.298Z",
          "wordCount": null,
          "title": "Multivariate Time Series Regression with Graph Neural Networks. (arXiv:2201.00818v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hanhan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1\">Tian Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1\">Vaneet Aggarwal</a>",
          "description": "Value function factorization via centralized training and decentralized\nexecution is promising for solving cooperative multi-agent reinforcement tasks.\nOne of the approaches in this area, QMIX, has become state-of-the-art and\nachieved the best performance on the StarCraft II micromanagement benchmark.\nHowever, the monotonic-mixing of per agent estimates in QMIX is known to\nrestrict the joint action Q-values it can represent, as well as the\ninsufficient global state information for single agent value function\nestimation, often resulting in suboptimality. To this end, we present LSF-SAC,\na novel framework that features a variational inference-based\ninformation-sharing mechanism as extra state information to assist individual\nagents in the value function factorization. We demonstrate that such latent\nindividual state information sharing can significantly expand the power of\nvalue function factorization, while fully decentralized execution can still be\nmaintained in LSF-SAC through a soft-actor-critic design. We evaluate LSF-SAC\non the StarCraft II micromanagement challenge and demonstrate that it\noutperforms several state-of-the-art methods in challenging collaborative\ntasks. We further set extensive ablation studies for locating the key factors\naccounting for its performance improvements. We believe that this new insight\ncan lead to new local value estimation methods and variational deep learning\nalgorithms. A demo video and code of implementation can be found at\nhttps://sites.google.com/view/sacmm.",
          "link": "http://arxiv.org/abs/2201.01247",
          "publishedOn": "2022-01-06T00:40:23.298Z",
          "wordCount": null,
          "title": "Value Functions Factorization with Latent State Information Sharing in Decentralized Multi-Agent Policy Gradients. (arXiv:2201.01247v1 [cs.MA])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01288",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenwu Zhu</a>",
          "description": "Graph machine learning has been extensively studied in both academic and\nindustry. However, as the literature on graph learning booms with a vast number\nof emerging methods and techniques, it becomes increasingly difficult to\nmanually design the optimal machine learning algorithm for different\ngraph-related tasks. To tackle the challenge, automated graph machine learning,\nwhich aims at discovering the best hyper-parameter and neural architecture\nconfiguration for different graph tasks/data without manual design, is gaining\nan increasing number of attentions from the research community. In this paper,\nwe extensively discuss automated graph machine approaches, covering\nhyper-parameter optimization (HPO) and neural architecture search (NAS) for\ngraph machine learning. We briefly overview existing libraries designed for\neither graph machine learning or automated machine learning respectively, and\nfurther in depth introduce AutoGL, our dedicated and the world's first\nopen-source library for automated graph machine learning. Last but not least,\nwe share our insights on future research directions for automated graph machine\nlearning. This paper is the first systematic and comprehensive discussion of\napproaches, libraries as well as directions for automated graph machine\nlearning.",
          "link": "http://arxiv.org/abs/2201.01288",
          "publishedOn": "2022-01-06T00:40:23.298Z",
          "wordCount": null,
          "title": "Automated Graph Machine Learning: Approaches, Libraries and Directions. (arXiv:2201.01288v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2111.13630",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thaler_F/0/1/0/all/0/1\">Franz Thaler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Payer_C/0/1/0/all/0/1\">Christian Payer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bischof_H/0/1/0/all/0/1\">Horst Bischof</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stern_D/0/1/0/all/0/1\">Darko Stern</a>",
          "description": "Even though many semantic segmentation methods exist that are able to perform\nwell on many medical datasets, often, they are not designed for direct use in\nclinical practice. The two main concerns are generalization to unseen data with\na different visual appearance, e.g., images acquired using a different scanner,\nand efficiency in terms of computation time and required Graphics Processing\nUnit (GPU) memory. In this work, we employ a multi-organ segmentation model\nbased on the SpatialConfiguration-Net (SCN), which integrates prior knowledge\nof the spatial configuration among the labelled organs to resolve spurious\nresponses in the network outputs. Furthermore, we modified the architecture of\nthe segmentation model to reduce its memory footprint as much as possible\nwithout drastically impacting the quality of the predictions. Lastly, we\nimplemented a minimal inference script for which we optimized both, execution\ntime and required GPU memory.",
          "link": "http://arxiv.org/abs/2111.13630",
          "publishedOn": "2022-01-06T00:40:23.297Z",
          "wordCount": null,
          "title": "Efficient Multi-Organ Segmentation Using SpatialConfiguration-Net with Low GPU Memory Requirements. (arXiv:2111.13630v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01272",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Natarov_R/0/1/0/all/0/1\">R. Natarov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sudakov_O/0/1/0/all/0/1\">O. Sudakov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dyka_Z/0/1/0/all/0/1\">Z. Dyka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kabin_I/0/1/0/all/0/1\">I. Kabin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maksymyuk_O/0/1/0/all/0/1\">O. Maksymyuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iegorova_O/0/1/0/all/0/1\">O. Iegorova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krishtal_O/0/1/0/all/0/1\">O. Krishtal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Langendorfer_P/0/1/0/all/0/1\">P. Langend&#xf6;rfer</a>",
          "description": "Resilience aspects of remote electroencephalography sampling are considered.\nThe possibility to use motion sensors data and measurement of industrial power\nnetwork interference for detection of failed sampling channels is demonstrated.\nNo significant correlation between signals of failed channels and motion\nsensors data is shown. Level of 50 Hz spectral component from failed channels\nsignificantly differs from level of 50 Hz component of normally operating\nchannel. Conclusions about application of these results for increasing\nresilience of electroencephalography sampling is made.",
          "link": "http://arxiv.org/abs/2201.01272",
          "publishedOn": "2022-01-06T00:40:23.293Z",
          "wordCount": null,
          "title": "Resilience Aspects in Distributed Wireless Electroencephalographic Sampling. (arXiv:2201.01272v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_R/0/1/0/all/0/1\">Rujing Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_O/0/1/0/all/0/1\">Ou Wu</a>",
          "description": "Weighting strategy prevails in machine learning. For example, a common\napproach in robust machine learning is to exert lower weights on samples which\nare likely to be noisy or quite hard. This study reveals another undiscovered\nstrategy, namely, compensating. Various incarnations of compensating have been\nutilized but it has not been explicitly revealed. Learning with compensating is\ncalled compensation learning and a systematic taxonomy is constructed for it in\nthis study. In our taxonomy, compensation learning is divided on the basis of\nthe compensation targets, directions, inference manners, and granularity\nlevels. Many existing learning algorithms including some classical ones can be\nviewed or understood at least partially as compensation techniques.\nFurthermore, a family of new learning algorithms can be obtained by plugging\nthe compensation learning into existing learning algorithms. Specifically, two\nconcrete new learning algorithms are proposed for robust machine learning.\nExtensive experiments on image classification and text sentiment analysis\nverify the effectiveness of the two new algorithms. Compensation learning can\nalso be used in other various learning scenarios, such as imbalance learning,\nclustering, regression, and so on.",
          "link": "http://arxiv.org/abs/2107.11921",
          "publishedOn": "2022-01-06T00:40:23.288Z",
          "wordCount": null,
          "title": "Compensation Learning. (arXiv:2107.11921v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.10912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Christen_P/0/1/0/all/0/1\">Peter Christen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnell_R/0/1/0/all/0/1\">Rainer Schnell</a>",
          "description": "Databases covering all individuals of a population are increasingly used for\nresearch studies in domains ranging from public health to the social sciences.\nThere is also growing interest by governments and businesses to use population\ndata to support data-driven decision making. The massive size of such databases\nis often mistaken as a guarantee for valid inferences on the population of\ninterest. However, population data have characteristics that make them\nchallenging to use, including various assumptions being made how such data were\ncollected and what types of processing have been applied to them. Furthermore,\nthe full potential of population data can often only be unlocked when such data\nare linked to other databases, a process that adds fresh challenges. This\narticle discusses a diverse range of misconceptions about population data that\nwe believe anybody who works with such data needs to be aware of. Many of these\nmisconceptions are not well documented in scientific publications but only\ndiscussed anecdotally among researchers and practitioners. We conclude with a\nset of recommendations for inference when using population data.",
          "link": "http://arxiv.org/abs/2112.10912",
          "publishedOn": "2022-01-06T00:40:23.288Z",
          "wordCount": null,
          "title": "Common Misconceptions about Population Data. (arXiv:2112.10912v2 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.05820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumatani_K/0/1/0/all/0/1\">Kenichi Kumatani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gmyr_R/0/1/0/all/0/1\">Robert Gmyr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salinas_F/0/1/0/all/0/1\">Felipe Cruz Salinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wei Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Devang Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_E/0/1/0/all/0/1\">Eric Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yu Shi</a>",
          "description": "The sparsely-gated Mixture of Experts (MoE) can magnify a network capacity\nwith a little computational complexity. In this work, we investigate how\nmulti-lingual Automatic Speech Recognition (ASR) networks can be scaled up with\na simple routing algorithm in order to achieve better accuracy. More\nspecifically, we apply the sparsely-gated MoE technique to two types of\nnetworks: Sequence-to-Sequence Transformer (S2S-T) and Transformer Transducer\n(T-T). We demonstrate through a set of ASR experiments on multiple language\ndata that the MoE networks can reduce the relative word error rates by 16.3%\nand 4.6% with the S2S-T and T-T, respectively. Moreover, we thoroughly\ninvestigate the effect of the MoE on the T-T architecture in various\nconditions: streaming mode, non-streaming mode, the use of language ID and the\nlabel decoder with the MoE.",
          "link": "http://arxiv.org/abs/2112.05820",
          "publishedOn": "2022-01-06T00:40:23.287Z",
          "wordCount": null,
          "title": "Building a great multi-lingual teacher with sparsely-gated mixture of experts for speech recognition. (arXiv:2112.05820v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shiyao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhaohui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zehui Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jiawen Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kejia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niyato_D/0/1/0/all/0/1\">Dusit Niyato</a>",
          "description": "Air access networks have been recognized as a significant driver of various\nInternet of Things (IoT) services and applications. In particular, the aerial\ncomputing network infrastructure centered on the Internet of Drones has set off\na new revolution in automatic image recognition. This emerging technology\nrelies on sharing ground truth labeled data between Unmanned Aerial Vehicle\n(UAV) swarms to train a high-quality automatic image recognition model.\nHowever, such an approach will bring data privacy and data availability\nchallenges. To address these issues, we first present a Semi-supervised\nFederated Learning (SSFL) framework for privacy-preserving UAV image\nrecognition. Specifically, we propose model parameters mixing strategy to\nimprove the naive combination of FL and semi-supervised learning methods under\ntwo realistic scenarios (labels-at-client and labels-at-server), which is\nreferred to as Federated Mixing (FedMix). Furthermore, there are significant\ndifferences in the number, features, and distribution of local data collected\nby UAVs using different camera modules in different environments, i.e.,\nstatistical heterogeneity. To alleviate the statistical heterogeneity problem,\nwe propose an aggregation rule based on the frequency of the client's\nparticipation in training, namely the FedFreq aggregation rule, which can\nadjust the weight of the corresponding local model according to its frequency.\nNumerical results demonstrate that the performance of our proposed method is\nsignificantly better than those of the current baseline and is robust to\ndifferent non-IID levels of client data.",
          "link": "http://arxiv.org/abs/2201.01230",
          "publishedOn": "2022-01-06T00:40:23.286Z",
          "wordCount": null,
          "title": "Robust Semi-supervised Federated Learning for Images Automatic Recognition in Internet of Drones. (arXiv:2201.01230v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01190",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ai_X/0/1/0/all/0/1\">Xing Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chengyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hancock_E/0/1/0/all/0/1\">Edwin R Hancock</a>",
          "description": "Graph Neural Networks (GNNs) are recently proposed neural network structures\nfor the processing of graph-structured data. Due to their employed neighbor\naggregation strategy, existing GNNs focus on capturing node-level information\nand neglect high-level information. Existing GNNs therefore suffer from\nrepresentational limitations caused by the Local Permutation Invariance (LPI)\nproblem. To overcome these limitations and enrich the features captured by\nGNNs, we propose a novel GNN framework, referred to as the Two-level GNN\n(TL-GNN). This merges subgraph-level information with node-level information.\nMoreover, we provide a mathematical analysis of the LPI problem which\ndemonstrates that subgraph-level information is beneficial to overcoming the\nproblems associated with LPI. A subgraph counting method based on the dynamic\nprogramming algorithm is also proposed, and this has time complexity is O(n^3),\nn is the number of nodes of a graph. Experiments show that TL-GNN outperforms\nexisting GNNs and achieves state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2201.01190",
          "publishedOn": "2022-01-06T00:40:23.004Z",
          "wordCount": null,
          "title": "Two-level Graph Neural Network. (arXiv:2201.01190v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yongchun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhiyuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhiping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>",
          "description": "Many few-shot learning approaches have been designed under the meta-learning\nframework, which learns from a variety of learning tasks and generalizes to new\ntasks. These meta-learning approaches achieve the expected performance in the\nscenario where all samples are drawn from the same distributions (i.i.d.\nobservations). However, in real-world applications, few-shot learning paradigm\noften suffers from data shift, i.e., samples in different tasks, even in the\nsame task, could be drawn from various data distributions. Most existing\nfew-shot learning approaches are not designed with the consideration of data\nshift, and thus show downgraded performance when data distribution shifts.\nHowever, it is non-trivial to address the data shift problem in few-shot\nlearning, due to the limited number of labeled samples in each task. Targeting\nat addressing this problem, we propose a novel metric-based meta-learning\nframework to extract task-specific representations and task-shared\nrepresentations with the help of knowledge graph. The data shift within/between\ntasks can thus be combated by the combination of task-shared and task-specific\nrepresentations. The proposed model is evaluated on popular benchmarks and two\nconstructed new challenging datasets. The evaluation results demonstrate its\nremarkable performance.",
          "link": "http://arxiv.org/abs/2101.11354",
          "publishedOn": "2022-01-06T00:40:22.989Z",
          "wordCount": null,
          "title": "Combat Data Shift in Few-shot Learning with Knowledge Graph. (arXiv:2101.11354v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.03825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hanxun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1\">Sarah Monazam Erfani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1\">James Bailey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xingjun Ma</a>",
          "description": "Deep neural networks (DNNs) are known to be vulnerable to adversarial\nattacks. A range of defense methods have been proposed to train adversarially\nrobust DNNs, among which adversarial training has demonstrated promising\nresults. However, despite preliminary understandings developed for adversarial\ntraining, it is still not clear, from the architectural perspective, what\nconfigurations can lead to more robust DNNs. In this paper, we address this gap\nvia a comprehensive investigation on the impact of network width and depth on\nthe robustness of adversarially trained DNNs. Specifically, we make the\nfollowing key observations: 1) more parameters (higher model capacity) does not\nnecessarily help adversarial robustness; 2) reducing capacity at the last stage\n(the last group of blocks) of the network can actually improve adversarial\nrobustness; and 3) under the same parameter budget, there exists an optimal\narchitectural configuration for adversarial robustness. We also provide a\ntheoretical analysis explaning why such network configuration can help\nrobustness. These architectural insights can help design adversarially robust\nDNNs. Code is available at \\url{https://github.com/HanxunH/RobustWRN}.",
          "link": "http://arxiv.org/abs/2110.03825",
          "publishedOn": "2022-01-06T00:40:22.989Z",
          "wordCount": null,
          "title": "Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks. (arXiv:2110.03825v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.09011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolczyk_M/0/1/0/all/0/1\">Maciej Wo&#x142;czyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proszewska_M/0/1/0/all/0/1\">Magdalena Proszewska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maziarka_L/0/1/0/all/0/1\">&#x141;ukasz Maziarka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zieba_M/0/1/0/all/0/1\">Maciej Zi&#x119;ba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wielopolski_P/0/1/0/all/0/1\">Patryk Wielopolski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurczab_R/0/1/0/all/0/1\">Rafa&#x142; Kurczab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smieja_M/0/1/0/all/0/1\">Marek &#x15a;mieja</a>",
          "description": "Modern generative models achieve excellent quality in a variety of tasks\nincluding image or text generation and chemical molecule modeling. However,\nexisting methods often lack the essential ability to generate examples with\nrequested properties, such as the age of the person in the photo or the weight\nof the generated molecule. Incorporating such additional conditioning factors\nwould require rebuilding the entire architecture and optimizing the parameters\nfrom scratch. Moreover, it is difficult to disentangle selected attributes so\nthat to perform edits of only one attribute while leaving the others unchanged.\nTo overcome these limitations we propose PluGeN (Plugin Generative Network), a\nsimple yet effective generative technique that can be used as a plugin to\npre-trained generative models. The idea behind our approach is to transform the\nentangled latent representation using a flow-based module into a\nmulti-dimensional space where the values of each attribute are modeled as an\nindependent one-dimensional distribution. In consequence, PluGeN can generate\nnew samples with desired attributes as well as manipulate labeled attributes of\nexisting examples. Due to the disentangling of the latent representation, we\nare even able to generate samples with rare or unseen combinations of\nattributes in the dataset, such as a young person with gray hair, men with\nmake-up, or women with beards. We combined PluGeN with GAN and VAE models and\napplied it to conditional generation and manipulation of images and chemical\nmolecule modeling. Experiments demonstrate that PluGeN preserves the quality of\nbackbone models while adding the ability to control the values of labeled\nattributes.",
          "link": "http://arxiv.org/abs/2109.09011",
          "publishedOn": "2022-01-06T00:40:22.957Z",
          "wordCount": 702,
          "title": "PluGeN: Multi-Label Conditional Generation From Pre-Trained Models. (arXiv:2109.09011v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.08170",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gao_S/0/1/0/all/0/1\">Shangqian Gao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "In the paper, we propose a class of accelerated zeroth-order and first-order\nmomentum methods for both nonconvex mini-optimization and minimax-optimization.\nSpecifically, we propose a new accelerated zeroth-order momentum (Acc-ZOM)\nmethod for black-box mini-optimization. Moreover, we prove that our Acc-ZOM\nmethod achieves a lower query complexity of $\\tilde{O}(d^{3/4}\\epsilon^{-3})$\nfor finding an $\\epsilon$-stationary point, which improves the best known\nresult by a factor of $O(d^{1/4})$ where $d$ denotes the variable dimension. In\nparticular, the Acc-ZOM does not require large batches required in the existing\nzeroth-order stochastic algorithms. Meanwhile, we propose an accelerated\n\\textbf{zeroth-order} momentum descent ascent (Acc-ZOMDA) method for\n\\textbf{black-box} minimax-optimization, which obtains a query complexity of\n$\\tilde{O}((d_1+d_2)^{3/4}\\kappa_y^{4.5}\\epsilon^{-3})$ without large batches\nfor finding an $\\epsilon$-stationary point, where $d_1$ and $d_2$ denote\nvariable dimensions and $\\kappa_y$ is condition number. Moreover, we propose an\naccelerated \\textbf{first-order} momentum descent ascent (Acc-MDA) method for\n\\textbf{white-box} minimax optimization, which has a gradient complexity of\n$\\tilde{O}(\\kappa_y^{4.5}\\epsilon^{-3})$ without large batches for finding an\n$\\epsilon$-stationary point. In particular, our Acc-MDA can obtain a lower\ngradient complexity of $\\tilde{O}(\\kappa_y^{2.5}\\epsilon^{-3})$ with a batch\nsize $O(\\kappa_y^4)$. Extensive experimental results on the black-box\nadversarial attack to deep neural networks (DNNs) and poisoning attack\ndemonstrate efficiency of our algorithms.",
          "link": "http://arxiv.org/abs/2008.08170",
          "publishedOn": "2022-01-06T00:40:22.935Z",
          "wordCount": null,
          "title": "Accelerated Zeroth-Order and First-Order Momentum Methods from Mini to Minimax Optimization. (arXiv:2008.08170v5 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyungmoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Seunghoon Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>",
          "description": "Deep metric learning aims to learn an embedding space where the distance\nbetween data reflects their class equivalence, even when their classes are\nunseen during training. However, the limited number of classes available in\ntraining precludes generalization of the learned embedding space. Motivated by\nthis, we introduce a new data augmentation approach that synthesizes novel\nclasses and their embedding vectors. Our approach can provide rich semantic\ninformation to an embedding model and improve its generalization by augmenting\ntraining data with novel classes unavailable in the original data. We implement\nthis idea by learning and exploiting a conditional generative model, which,\ngiven a class label and a noise, produces a random embedding vector of the\nclass. Our proposed generator allows the loss to use richer class relations by\naugmenting realistic and diverse classes, resulting in better generalization to\nunseen samples. Experimental results on public benchmark datasets demonstrate\nthat our method clearly enhances the performance of proxy-based losses.",
          "link": "http://arxiv.org/abs/2201.01008",
          "publishedOn": "2022-01-06T00:40:22.931Z",
          "wordCount": null,
          "title": "Learning to Generate Novel Classes for Deep Metric Learning. (arXiv:2201.01008v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2110.02753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vincent_Cuaz_C/0/1/0/all/0/1\">C&#xe9;dric Vincent-Cuaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flamary_R/0/1/0/all/0/1\">R&#xe9;mi Flamary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corneli_M/0/1/0/all/0/1\">Marco Corneli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vayer_T/0/1/0/all/0/1\">Titouan Vayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courty_N/0/1/0/all/0/1\">Nicolas Courty</a>",
          "description": "Comparing structured objects such as graphs is a fundamental operation\ninvolved in many learning tasks. To this end, the Gromov-Wasserstein (GW)\ndistance, based on Optimal Transport (OT), has proven to be successful in\nhandling the specific nature of the associated objects. More specifically,\nthrough the nodes connectivity relations, GW operates on graphs, seen as\nprobability measures over specific spaces. At the core of OT is the idea of\nconservation of mass, which imposes a coupling between all the nodes from the\ntwo considered graphs. We argue in this paper that this property can be\ndetrimental for tasks such as graph dictionary or partition learning, and we\nrelax it by proposing a new semi-relaxed Gromov-Wasserstein divergence. Aside\nfrom immediate computational benefits, we discuss its properties, and show that\nit can lead to an efficient graph dictionary learning algorithm. We empirically\ndemonstrate its relevance for complex tasks on graphs such as partitioning,\nclustering and completion.",
          "link": "http://arxiv.org/abs/2110.02753",
          "publishedOn": "2022-01-06T00:40:22.923Z",
          "wordCount": null,
          "title": "Semi-relaxed Gromov-Wasserstein divergence with applications on graphs. (arXiv:2110.02753v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.14285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongrui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>",
          "description": "Despite Graph Neural Networks (GNNs) have achieved remarkable accuracy,\nwhether the results are trustworthy is still unexplored. Previous studies\nsuggest that many modern neural networks are over-confident on the predictions,\nhowever, surprisingly, we discover that GNNs are primarily in the opposite\ndirection, i.e., GNNs are under-confident. Therefore, the confidence\ncalibration for GNNs is highly desired. In this paper, we propose a novel\ntrustworthy GNN model by designing a topology-aware post-hoc calibration\nfunction. Specifically, we first verify that the confidence distribution in a\ngraph has homophily property, and this finding inspires us to design a\ncalibration GNN model (CaGCN) to learn the calibration function. CaGCN is able\nto obtain a unique transformation from logits of GNNs to the calibrated\nconfidence for each node, meanwhile, such transformation is able to preserve\nthe order between classes, satisfying the accuracy-preserving property.\nMoreover, we apply the calibration GNN to self-training framework, showing that\nmore trustworthy pseudo labels can be obtained with the calibrated confidence\nand further improve the performance. Extensive experiments demonstrate the\neffectiveness of our proposed model in terms of both calibration and accuracy.",
          "link": "http://arxiv.org/abs/2109.14285",
          "publishedOn": "2022-01-06T00:40:22.919Z",
          "wordCount": null,
          "title": "Be Confident! Towards Trustworthy Graph Neural Networks via Confidence Calibration. (arXiv:2109.14285v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04635",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anthony_T/0/1/0/all/0/1\">Thomas Anthony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eccles_T/0/1/0/all/0/1\">Tom Eccles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tacchetti_A/0/1/0/all/0/1\">Andrea Tacchetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kramar_J/0/1/0/all/0/1\">J&#xe1;nos Kram&#xe1;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemp_I/0/1/0/all/0/1\">Ian Gemp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudson_T/0/1/0/all/0/1\">Thomas C. Hudson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porcel_N/0/1/0/all/0/1\">Nicolas Porcel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanctot_M/0/1/0/all/0/1\">Marc Lanctot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perolat_J/0/1/0/all/0/1\">Julien P&#xe9;rolat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Everett_R/0/1/0/all/0/1\">Richard Everett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werpachowski_R/0/1/0/all/0/1\">Roman Werpachowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satinder Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graepel_T/0/1/0/all/0/1\">Thore Graepel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachrach_Y/0/1/0/all/0/1\">Yoram Bachrach</a>",
          "description": "Recent advances in deep reinforcement learning (RL) have led to considerable\nprogress in many 2-player zero-sum games, such as Go, Poker and Starcraft. The\npurely adversarial nature of such games allows for conceptually simple and\nprincipled application of RL methods. However real-world settings are\nmany-agent, and agent interactions are complex mixtures of common-interest and\ncompetitive aspects. We consider Diplomacy, a 7-player board game designed to\naccentuate dilemmas resulting from many-agent interactions. It also features a\nlarge combinatorial action space and simultaneous moves, which are challenging\nfor RL algorithms. We propose a simple yet effective approximate best response\noperator, designed to handle large combinatorial action spaces and simultaneous\nmoves. We also introduce a family of policy iteration methods that approximate\nfictitious play. With these methods, we successfully apply RL to Diplomacy: we\nshow that our agents convincingly outperform the previous state-of-the-art, and\ngame theoretic equilibrium analysis shows that the new process yields\nconsistent improvements.",
          "link": "http://arxiv.org/abs/2006.04635",
          "publishedOn": "2022-01-06T00:40:22.917Z",
          "wordCount": 665,
          "title": "Learning to Play No-Press Diplomacy with Best Response Policy Iteration. (arXiv:2006.04635v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benz_P/0/1/0/all/0/1\">Philipp Benz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenguo Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karjauv_A/0/1/0/all/0/1\">Adil Karjauv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>",
          "description": "The intriguing phenomenon of adversarial examples has attracted significant\nattention in machine learning and what might be more surprising to the\ncommunity is the existence of universal adversarial perturbations (UAPs), i.e.\na single perturbation to fool the target DNN for most images. With the focus on\nUAP against deep classifiers, this survey summarizes the recent progress on\nuniversal adversarial attacks, discussing the challenges from both the attack\nand defense sides, as well as the reason for the existence of UAP. We aim to\nextend this work as a dynamic survey that will regularly update its content to\nfollow new works regarding UAP or universal attack in a wide range of domains,\nsuch as image, audio, video, text, etc. Relevant updates will be discussed at:\nhttps://bit.ly/2SbQlLG. We welcome authors of future works in this field to\ncontact us for including your new finding.",
          "link": "http://arxiv.org/abs/2103.01498",
          "publishedOn": "2022-01-06T00:40:22.906Z",
          "wordCount": 607,
          "title": "A Survey On Universal Adversarial Attack. (arXiv:2103.01498v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kireev_K/0/1/0/all/0/1\">Klim Kireev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andriushchenko_M/0/1/0/all/0/1\">Maksym Andriushchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flammarion_N/0/1/0/all/0/1\">Nicolas Flammarion</a>",
          "description": "The literature on robustness towards common corruptions shows no consensus on\nwhether adversarial training can improve the performance in this setting.\nFirst, we show that, when used with an appropriately selected perturbation\nradius, $\\ell_p$ adversarial training can serve as a strong baseline against\ncommon corruptions improving both accuracy and calibration. Then we explain why\nadversarial training performs better than data augmentation with simple\nGaussian noise which has been observed to be a meaningful baseline on common\ncorruptions. Related to this, we identify the $\\sigma$-overfitting phenomenon\nwhen Gaussian augmentation overfits to a particular standard deviation used for\ntraining which has a significant detrimental effect on common corruption\naccuracy. We discuss how to alleviate this problem and then how to further\nenhance $\\ell_p$ adversarial training by introducing an efficient relaxation of\nadversarial training with learned perceptual image patch similarity as the\ndistance metric. Through experiments on CIFAR-10 and ImageNet-100, we show that\nour approach does not only improve the $\\ell_p$ adversarial training baseline\nbut also has cumulative gains with data augmentation methods such as AugMix,\nDeepAugment, ANT, and SIN, leading to state-of-the-art performance on common\ncorruptions.\n\nThe code of our experiments is publicly available at\nhttps://github.com/tml-epfl/adv-training-corruptions.",
          "link": "http://arxiv.org/abs/2103.02325",
          "publishedOn": "2022-01-06T00:40:22.898Z",
          "wordCount": 670,
          "title": "On the effectiveness of adversarial training against common corruptions. (arXiv:2103.02325v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01294",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Trung-Hieu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berberich_J/0/1/0/all/0/1\">Jan Berberich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_S/0/1/0/all/0/1\">Sven Simon</a>",
          "description": "Light field (LF) imaging, which captures both spatial and angular information\nof a scene, is undoubtedly beneficial to numerous applications. Although\nvarious techniques have been proposed for LF acquisition, achieving both\nangularly and spatially high-resolution LF remains a technology challenge. In\nthis paper, a learning-based approach applied to 3D epipolar image (EPI) is\nproposed to reconstruct high-resolution LF. Through a 2-stage super-resolution\nframework, the proposed approach effectively addresses various LF\nsuper-resolution (SR) problems, i.e., spatial SR, angular SR, and\nangular-spatial SR. While the first stage provides flexible options to\nup-sample EPI volume to the desired resolution, the second stage, which\nconsists of a novel EPI volume-based refinement network (EVRN), substantially\nenhances the quality of the high-resolution EPI volume. An extensive evaluation\non 90 challenging synthetic and real-world light field scenes from 7 published\ndatasets shows that the proposed approach outperforms state-of-the-art methods\nto a large extend for both spatial and angular super-resolution problem, i.e.,\nan average peak signal to noise ratio improvement of more than 2.0 dB, 1.4 dB,\nand 3.14 dB in spatial SR $\\times 2$, spatial SR $\\times 4$, and angular SR\nrespectively. The reconstructed 4D light field demonstrates a balanced\nperformance distribution across all perspective images and presents superior\nvisual quality compared to the previous works.",
          "link": "http://arxiv.org/abs/2201.01294",
          "publishedOn": "2022-01-06T00:40:22.869Z",
          "wordCount": 652,
          "title": "3DVSR: 3D EPI Volume-based Approach for Angular and Spatial Light field Image Super-resolution. (arXiv:2201.01294v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01140",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanhua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wojtczak_D/0/1/0/all/0/1\">Dominik Wojtczak</a>",
          "description": "The rapid mutation of the influenza virus threatens public health.\nReassortment among viruses with different hosts can lead to a fatal pandemic.\nHowever, it is difficult to detect the original host of the virus during or\nafter an outbreak as influenza viruses can circulate between different species.\nTherefore, early and rapid detection of the viral host would help reduce the\nfurther spread of the virus. We use various machine learning models with\nfeatures derived from the position-specific scoring matrix (PSSM) and features\nlearned from word embedding and word encoding to infer the origin host of\nviruses. The results show that the performance of the PSSM-based model reaches\nthe MCC around 95%, and the F1 around 96%. The MCC obtained using the model\nwith word embedding is around 96%, and the F1 is around 97%.",
          "link": "http://arxiv.org/abs/2201.01140",
          "publishedOn": "2022-01-06T00:40:22.847Z",
          "wordCount": 589,
          "title": "Predicting Influenza A Viral Host Using PSSM and Word Embeddings. (arXiv:2201.01140v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2112.09490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karaderi_T/0/1/0/all/0/1\">Tayfun Karaderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burghardt_T/0/1/0/all/0/1\">Tilo Burghardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiang_A/0/1/0/all/0/1\">Allison Y. Hsiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramaer_J/0/1/0/all/0/1\">Jacob Ramaer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_D/0/1/0/all/0/1\">Daniela N. Schmidt</a>",
          "description": "We apply deep metric learning for the first time to the prob-lem of\nclassifying planktic foraminifer shells on microscopic images. This species\nrecognition task is an important information source and scientific pillar for\nreconstructing past climates. All foraminifer CNN recognition pipelines in the\nliterature produce black-box classifiers that lack visualisation options for\nhuman experts and cannot be applied to open set problems. Here, we benchmark\nmetric learning against these pipelines, produce the first scientific\nvisualisation of the phenotypic planktic foraminifer morphology space, and\ndemonstrate that metric learning can be used to cluster species unseen during\ntraining. We show that metric learning out-performs all published CNN-based\nstate-of-the-art benchmarks in this domain. We evaluate our approach on the\n34,640 expert-annotated images of the Endless Forams public library of 35\nmodern planktic foraminifera species. Our results on this data show leading 92%\naccuracy (at 0.84 F1-score) in reproducing expert labels on withheld test data,\nand 66.5% accuracy (at 0.70 F1-score) when clustering species never encountered\nin training. We conclude that metric learning is highly effective for this\ndomain and serves as an important tool towards expert-in-the-loop automation of\nmicrofossil identification. Key code, network weights, and data splits are\npublished with this paper for full reproducibility.",
          "link": "http://arxiv.org/abs/2112.09490",
          "publishedOn": "2022-01-06T00:40:22.839Z",
          "wordCount": null,
          "title": "Visual Microfossil Identification via Deep Metric Learning. (arXiv:2112.09490v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lucieri_A/0/1/0/all/0/1\">Adriano Lucieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajwa_M/0/1/0/all/0/1\">Muhammad Naseer Bajwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braun_S/0/1/0/all/0/1\">Stephan Alexander Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_M/0/1/0/all/0/1\">Muhammad Imran Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1\">Andreas Dengel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Sheraz Ahmed</a>",
          "description": "One principal impediment in the successful deployment of AI-based\nComputer-Aided Diagnosis (CAD) systems in clinical workflows is their lack of\ntransparent decision making. Although commonly used eXplainable AI methods\nprovide some insight into opaque algorithms, such explanations are usually\nconvoluted and not readily comprehensible except by highly trained experts. The\nexplanation of decisions regarding the malignancy of skin lesions from\ndermoscopic images demands particular clarity, as the underlying medical\nproblem definition is itself ambiguous. This work presents ExAID (Explainable\nAI for Dermatology), a novel framework for biomedical image analysis, providing\nmulti-modal concept-based explanations consisting of easy-to-understand textual\nexplanations supplemented by visual maps justifying the predictions. ExAID\nrelies on Concept Activation Vectors to map human concepts to those learnt by\narbitrary Deep Learning models in latent space, and Concept Localization Maps\nto highlight concepts in the input space. This identification of relevant\nconcepts is then used to construct fine-grained textual explanations\nsupplemented by concept-wise location information to provide comprehensive and\ncoherent multi-modal explanations. All information is comprehensively presented\nin a diagnostic interface for use in clinical routines. An educational mode\nprovides dataset-level explanation statistics and tools for data and model\nexploration to aid medical research and education. Through rigorous\nquantitative and qualitative evaluation of ExAID, we show the utility of\nmulti-modal explanations for CAD-assisted scenarios even in case of wrong\npredictions. We believe that ExAID will provide dermatologists an effective\nscreening tool that they both understand and trust. Moreover, it will be the\nbasis for similar applications in other biomedical imaging fields.",
          "link": "http://arxiv.org/abs/2201.01249",
          "publishedOn": "2022-01-06T00:40:22.838Z",
          "wordCount": 703,
          "title": "ExAID: A Multimodal Explanation Framework for Computer-Aided Diagnosis of Skin Lesions. (arXiv:2201.01249v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Siwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1\">Kukjin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hyun-Soo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Byunghan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>",
          "description": "In recent years, proposed studies on time-series anomaly detection (TAD)\nreport high F1 scores on benchmark TAD datasets, giving the impression of clear\nimprovements in TAD. However, most studies apply a peculiar evaluation protocol\ncalled point adjustment (PA) before scoring. In this paper, we theoretically\nand experimentally reveal that the PA protocol has a great possibility of\noverestimating the detection performance; that is, even a random anomaly score\ncan easily turn into a state-of-the-art TAD method. Therefore, the comparison\nof TAD methods after applying the PA protocol can lead to misguided rankings.\nFurthermore, we question the potential of existing TAD methods by showing that\nan untrained model obtains comparable detection performance to the existing\nmethods even when PA is forbidden. Based on our findings, we propose a new\nbaseline and an evaluation protocol. We expect that our study will help a\nrigorous evaluation of TAD and lead to further improvement in future\nresearches.",
          "link": "http://arxiv.org/abs/2109.05257",
          "publishedOn": "2022-01-06T00:40:22.828Z",
          "wordCount": 612,
          "title": "Towards a Rigorous Evaluation of Time-series Anomaly Detection. (arXiv:2109.05257v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Keramatfar_A/0/1/0/all/0/1\">Abdalsamad Keramatfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafiee_M/0/1/0/all/0/1\">Mohadeseh Rafiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirkhani_H/0/1/0/all/0/1\">Hossein Amirkhani</a>",
          "description": "Recently, graph neural networks have become a hot topic in machine learning\ncommunity. This paper presents a Scopus based bibliometric overview of the GNNs\nresearch since 2004, when GNN papers were first published. The study aims to\nevaluate GNN research trend, both quantitatively and qualitatively. We provide\nthe trend of research, distribution of subjects, active and influential authors\nand institutions, sources of publications, most cited documents, and hot\ntopics. Our investigations reveal that the most frequent subject categories in\nthis field are computer science, engineering, telecommunications, linguistics,\noperations research and management science, information science and library\nscience, business and economics, automation and control systems, robotics, and\nsocial sciences. In addition, the most active source of GNN publications is\nLecture Notes in Computer Science. The most prolific or impactful institutions\nare found in the United States, China, and Canada. We also provide must read\npapers and future directions. Finally, the application of graph convolutional\nnetworks and attention mechanism are now among hot topics of GNN research.",
          "link": "http://arxiv.org/abs/2201.01188",
          "publishedOn": "2022-01-06T00:40:22.797Z",
          "wordCount": 589,
          "title": "Graph Neural Networks: a bibliometrics overview. (arXiv:2201.01188v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1\">Andrew R. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitanyi_P/0/1/0/all/0/1\">Paul M.B. Vit&#xe1;nyi</a>",
          "description": "For each partition of a data set into a given number of parts there is a\npartition such that every part is as much as possible a good model (an\n\"algorithmic sufficient statistic\") for the data in that part. Since this can\nbe done for every number between one and the number of data, the result is a\nfunction, the cluster structure function. It maps the number of parts of a\npartition to values related to the deficiencies of being good models by the\nparts. Such a function starts with a value at least zero for no partition of\nthe data set and descents to zero for the partition of the data set into\nsingleton parts. The optimal clustering is the one chosen to minimize the\ncluster structure function. The theory behind the method is expressed in\nalgorithmic information theory (Kolmogorov complexity). In practice the\nKolmogorov complexities involved are approximated by a concrete compressor. We\ngive examples using real data sets: the MNIST handwritten digits and the\nsegmentation of real cells as used in stem cell research.",
          "link": "http://arxiv.org/abs/2201.01222",
          "publishedOn": "2022-01-06T00:40:22.787Z",
          "wordCount": 591,
          "title": "The cluster structure function. (arXiv:2201.01222v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingchen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuechen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thrampoulidis_C/0/1/0/all/0/1\">Christos Thrampoulidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiasi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1\">Samet Oymak</a>",
          "description": "Imbalanced datasets are commonplace in modern machine learning problems. The\npresence of under-represented classes or groups with sensitive attributes\nresults in concerns about generalization and fairness. Such concerns are\nfurther exacerbated by the fact that large capacity deep nets can perfectly fit\nthe training data and appear to achieve perfect accuracy and fairness during\ntraining, but perform poorly during test. To address these challenges, we\npropose AutoBalance, a bi-level optimization framework that automatically\ndesigns a training loss function to optimize a blend of accuracy and\nfairness-seeking objectives. Specifically, a lower-level problem trains the\nmodel weights, and an upper-level problem tunes the loss function by monitoring\nand optimizing the desired objective over the validation data. Our loss design\nenables personalized treatment for classes/groups by employing a parametric\ncross-entropy loss and individualized data augmentation schemes. We evaluate\nthe benefits and performance of our approach for the application scenarios of\nimbalanced and group-sensitive classification. Extensive empirical evaluations\ndemonstrate the benefits of AutoBalance over state-of-the-art approaches. Our\nexperimental findings are complemented with theoretical insights on loss\nfunction design and the benefits of train-validation split. All code is\navailable open-source.",
          "link": "http://arxiv.org/abs/2201.01212",
          "publishedOn": "2022-01-06T00:40:22.769Z",
          "wordCount": 602,
          "title": "AutoBalance: Optimized Loss Functions for Imbalanced Data. (arXiv:2201.01212v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Normandin_F/0/1/0/all/0/1\">Fabrice Normandin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golemo_F/0/1/0/all/0/1\">Florian Golemo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostapenko_O/0/1/0/all/0/1\">Oleksiy Ostapenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1\">Pau Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riemer_M/0/1/0/all/0/1\">Matthew D Riemer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hurtado_J/0/1/0/all/0/1\">Julio Hurtado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khetarpal_K/0/1/0/all/0/1\">Khimya Khetarpal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindeborg_R/0/1/0/all/0/1\">Ryan Lindeborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cecchi_L/0/1/0/all/0/1\">Lucas Cecchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lesort_T/0/1/0/all/0/1\">Timoth&#xe9;e Lesort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charlin_L/0/1/0/all/0/1\">Laurent Charlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1\">Irina Rish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caccia_M/0/1/0/all/0/1\">Massimo Caccia</a>",
          "description": "The field of Continual Learning (CL) seeks to develop algorithms that\naccumulate knowledge and skills over time through interaction with\nnon-stationary environments. In practice, a plethora of evaluation procedures\n(settings) and algorithmic solutions (methods) exist, each with their own\npotentially disjoint set of assumptions. This variety makes measuring progress\nin CL difficult. We propose a taxonomy of settings, where each setting is\ndescribed as a set of assumptions. A tree-shaped hierarchy emerges from this\nview, where more general settings become the parents of those with more\nrestrictive assumptions. This makes it possible to use inheritance to share and\nreuse research, as developing a method for a given setting also makes it\ndirectly applicable onto any of its children. We instantiate this idea as a\npublicly available software framework called Sequoia, which features a wide\nvariety of settings from both the Continual Supervised Learning (CSL) and\nContinual Reinforcement Learning (CRL) domains. Sequoia also includes a growing\nsuite of methods which are easy to extend and customize, in addition to more\nspecialized methods from external libraries. We hope that this new paradigm and\nits first implementation can help unify and accelerate research in CL. You can\nhelp us grow the tree by visiting github.com/lebrice/Sequoia.",
          "link": "http://arxiv.org/abs/2108.01005",
          "publishedOn": "2022-01-06T00:40:22.757Z",
          "wordCount": 682,
          "title": "Sequoia: A Software Framework to Unify Continual Learning Research. (arXiv:2108.01005v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habermann_M/0/1/0/all/0/1\">Marc Habermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudnev_V/0/1/0/all/0/1\">Viktor Rudnev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_K/0/1/0/all/0/1\">Kripasindhu Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiatao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>",
          "description": "We propose Neural Actor (NA), a new method for high-quality synthesis of\nhumans from arbitrary viewpoints and under arbitrary controllable poses. Our\nmethod is built upon recent neural scene representation and rendering works\nwhich learn representations of geometry and appearance from only 2D images.\nWhile existing works demonstrated compelling rendering of static scenes and\nplayback of dynamic scenes, photo-realistic reconstruction and rendering of\nhumans with neural implicit methods, in particular under user-controlled novel\nposes, is still difficult. To address this problem, we utilize a coarse body\nmodel as the proxy to unwarp the surrounding 3D space into a canonical pose. A\nneural radiance field learns pose-dependent geometric deformations and pose-\nand view-dependent appearance effects in the canonical space from multi-view\nvideo input. To synthesize novel views of high fidelity dynamic geometry and\nappearance, we leverage 2D texture maps defined on the body model as latent\nvariables for predicting residual deformations and the dynamic appearance.\nExperiments demonstrate that our method achieves better quality than the\nstate-of-the-arts on playback as well as novel pose synthesis, and can even\ngeneralize well to new poses that starkly differ from the training poses.\nFurthermore, our method also supports body shape control of the synthesized\nresults.",
          "link": "http://arxiv.org/abs/2106.02019",
          "publishedOn": "2022-01-06T00:40:22.478Z",
          "wordCount": null,
          "title": "Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control. (arXiv:2106.02019v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00879",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joseph_G/0/1/0/all/0/1\">Geethu Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gursoy_M/0/1/0/all/0/1\">M. Cenk Gursoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_P/0/1/0/all/0/1\">Pramod K. Varshney</a>",
          "description": "We address the problem of monitoring a set of binary stochastic processes and\ngenerating an alert when the number of anomalies among them exceeds a\nthreshold. For this, the decision-maker selects and probes a subset of the\nprocesses to obtain noisy estimates of their states (normal or anomalous).\nBased on the received observations, the decisionmaker first determines whether\nto declare that the number of anomalies has exceeded the threshold or to\ncontinue taking observations. When the decision is to continue, it then decides\nwhether to collect observations at the next time instant or defer it to a later\ntime. If it chooses to collect observations, it further determines the subset\nof processes to be probed. To devise this three-step sequential decision-making\nprocess, we use a Bayesian formulation wherein we learn the posterior\nprobability on the states of the processes. Using the posterior probability, we\nconstruct a Markov decision process and solve it using deep actor-critic\nreinforcement learning. Via numerical experiments, we demonstrate the superior\nperformance of our algorithm compared to the traditional model-based\nalgorithms.",
          "link": "http://arxiv.org/abs/2201.00879",
          "publishedOn": "2022-01-06T00:40:21.511Z",
          "wordCount": 602,
          "title": "Monitoring and Anomaly Detection Actor-Critic Based Controlled Sensing. (arXiv:2201.00879v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1\">Yifei Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We study the off-policy evaluation (OPE) problem in reinforcement learning\nwith linear function approximation, which aims to estimate the value function\nof a target policy based on the offline data collected by a behavior policy. We\npropose to incorporate the variance information of the value function to\nimprove the sample efficiency of OPE. More specifically, for time-inhomogeneous\nepisodic linear Markov decision processes (MDPs), we propose an algorithm,\nVA-OPE, which uses the estimated variance of the value function to reweight the\nBellman residual in Fitted Q-Iteration. We show that our algorithm achieves a\ntighter error bound than the best-known result. We also provide a fine-grained\ncharacterization of the distribution shift between the behavior policy and the\ntarget policy. Extensive numerical experiments corroborate our theory.",
          "link": "http://arxiv.org/abs/2106.11960",
          "publishedOn": "2022-01-06T00:40:21.505Z",
          "wordCount": 585,
          "title": "Variance-Aware Off-Policy Evaluation with Linear Function Approximation. (arXiv:2106.11960v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qunxi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yifei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>",
          "description": "Continuous-depth neural networks, such as the Neural Ordinary Differential\nEquations (ODEs), have aroused a great deal of interest from the communities of\nmachine learning and data science in recent years, which bridge the connection\nbetween deep neural networks and dynamical systems. In this article, we\nintroduce a new sort of continuous-depth neural network, called the Neural\nPiecewise-Constant Delay Differential Equations (PCDDEs). Here, unlike the\nrecently proposed framework of the Neural Delay Differential Equations (DDEs),\nwe transform the single delay into the piecewise-constant delay(s). The Neural\nPCDDEs with such a transformation, on one hand, inherit the strength of\nuniversal approximating capability in Neural DDEs. On the other hand, the\nNeural PCDDEs, leveraging the contributions of the information from the\nmultiple previous time steps, further promote the modeling capability without\naugmenting the network dimension. With such a promotion, we show that the\nNeural PCDDEs do outperform the several existing continuous-depth neural\nframeworks on the one-dimensional piecewise-constant delay population dynamics\nand real-world datasets, including MNIST, CIFAR10, and SVHN.",
          "link": "http://arxiv.org/abs/2201.00960",
          "publishedOn": "2022-01-06T00:40:21.498Z",
          "wordCount": 605,
          "title": "Neural Piecewise-Constant Delay Differential Equations. (arXiv:2201.00960v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01145",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>",
          "description": "Learning to optimize the area under the receiver operating characteristics\ncurve (AUC) performance for imbalanced data has attracted much attention in\nrecent years. Although there have been several methods of AUC optimization,\nscaling up AUC optimization is still an open issue due to its pairwise learning\nstyle. Maximizing AUC in the large-scale dataset can be considered as a\nnon-convex and expensive problem. Inspired by the characteristic of pairwise\nlearning, the cheap AUC optimization task with a small-scale dataset sampled\nfrom the large-scale dataset is constructed to promote the AUC accuracy of the\noriginal, large-scale, and expensive AUC optimization task. This paper develops\nan evolutionary multitasking framework (termed EMTAUC) to make full use of\ninformation among the constructed cheap and expensive tasks to obtain higher\nperformance. In EMTAUC, one mission is to optimize AUC from the sampled\ndataset, and the other is to maximize AUC from the original dataset. Moreover,\ndue to the cheap task containing limited knowledge, a strategy for dynamically\nadjusting the data structure of inexpensive tasks is proposed to introduce more\nknowledge into the multitasking AUC optimization environment. The performance\nof the proposed method is evaluated on a series of binary classification\ndatasets. The experimental results demonstrate that EMTAUC is highly\ncompetitive to single task methods and online methods. Supplementary materials\nand source code implementation of EMTAUC can be accessed at\nhttps://github.com/xiaofangxd/EMTAUC.",
          "link": "http://arxiv.org/abs/2201.01145",
          "publishedOn": "2022-01-06T00:40:21.441Z",
          "wordCount": 666,
          "title": "Evolutionary Multitasking AUC Optimization. (arXiv:2201.01145v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01036",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wu_S/0/1/0/all/0/1\">Shihao Wu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhu_Z/0/1/0/all/0/1\">Ziwei Zhu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_L/0/1/0/all/0/1\">Ling Zhou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Song_P/0/1/0/all/0/1\">Peter X.-K. Song</a>",
          "description": "Fusing regression coefficients into homogenous groups can unveil those\ncoefficients that share a common value within each group. Such groupwise\nhomogeneity reduces the intrinsic dimension of the parameter space and\nunleashes sharper statistical accuracy. We propose and investigate a new\ncombinatorial grouping approach called $L_0$-Fusion that is amenable to mixed\ninteger optimization (MIO). On the statistical aspect, we identify a\nfundamental quantity called grouping sensitivity that underpins the difficulty\nof recovering the true groups. We show that $L_0$-Fusion achieves grouping\nconsistency under the weakest possible requirement of the grouping sensitivity:\nif this requirement is violated, then the minimax risk of group\nmisspecification will fail to converge to zero. Moreover, we show that in the\nhigh-dimensional regime, one can apply $L_0$-Fusion coupled with a sure\nscreening set of features without any essential loss of statistical efficiency,\nwhile reducing the computational cost substantially. On the algorithmic aspect,\nwe provide a MIO formulation for $L_0$-Fusion along with a warm start strategy.\nSimulation and real data analysis demonstrate that $L_0$-Fusion exhibits\nsuperiority over its competitors in terms of grouping accuracy.",
          "link": "http://arxiv.org/abs/2201.01036",
          "publishedOn": "2022-01-06T00:40:21.402Z",
          "wordCount": 602,
          "title": "Supervised Homogeneity Fusion: a Combinatorial Approach. (arXiv:2201.01036v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Haemin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1\">Sean Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Soyi Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joongheon Kim</a>",
          "description": "A successful deployment of drones provides an ideal solution for surveillance\nsystems. Using drones for surveillance can provide access to areas that may be\ndifficult or impossible to reach by humans or in-land vehicles gathering images\nor video recordings of a specific target in their coverage. Therefore, we\nintroduces a data delivery drone to transfer collected surveillance data in\nharsh communication conditions. This paper proposes a Myerson auction-based\nasynchronous data delivery in an aerial distributed data platform in\nsurveillance systems taking battery limitation and long flight constraints into\naccount. In this paper, multiple delivery drones compete to offer data transfer\nto a single fixed-location surveillance drone. Our proposed Myerson\nauction-based algorithm, which uses the truthful second-price auction (SPA) as\na baseline, is to maximize the seller's revenue while meeting several desirable\nproperties, i.e., individual rationality and incentive compatibility while\npursuing truthful operations. On top of these SPA-based operations, a deep\nlearning-based framework is additionally designed for delivery performance\nimprovements.",
          "link": "http://arxiv.org/abs/2201.01170",
          "publishedOn": "2022-01-06T00:40:21.361Z",
          "wordCount": 602,
          "title": "Neural Myerson Auction for Truthful and Energy-Efficient Autonomous Aerial Data Delivery. (arXiv:2201.01170v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00945",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heintz_J/0/1/0/all/0/1\">Joos Heintz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ocar_H/0/1/0/all/0/1\">Hvara Ocar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pardo_L/0/1/0/all/0/1\">Luis Miguel Pardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paredes_A/0/1/0/all/0/1\">Andres Rojas Paredes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segura_E/0/1/0/all/0/1\">Enrique Carlos Segura</a>",
          "description": "We define the notion of a continuously differentiable perfect learning\nalgorithm for multilayer neural network architectures and show that such\nalgorithms don't exist provided that the length of the data set exceeds the\nnumber of involved parameters and the activation functions are logistic, tanh\nor sin.",
          "link": "http://arxiv.org/abs/2201.00945",
          "publishedOn": "2022-01-06T00:40:21.355Z",
          "wordCount": 476,
          "title": "An unfeasability view of neural network learning. (arXiv:2201.00945v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09276",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Koehler_F/0/1/0/all/0/1\">Frederic Koehler</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_L/0/1/0/all/0/1\">Lijia Zhou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sutherland_D/0/1/0/all/0/1\">Danica J. Sutherland</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Srebro_N/0/1/0/all/0/1\">Nathan Srebro</a>",
          "description": "We consider interpolation learning in high-dimensional linear regression with\nGaussian data, and prove a generic uniform convergence guarantee on the\ngeneralization error of interpolators in an arbitrary hypothesis class in terms\nof the class's Gaussian width. Applying the generic bound to Euclidean norm\nballs recovers the consistency result of Bartlett et al. (2020) for\nminimum-norm interpolators, and confirms a prediction of Zhou et al. (2020) for\nnear-minimal-norm interpolators in the special case of Gaussian data. We\ndemonstrate the generality of the bound by applying it to the simplex,\nobtaining a novel consistency result for minimum l1-norm interpolators (basis\npursuit). Our results show how norm-based generalization bounds can explain and\nbe used to analyze benign overfitting, at least in some settings.",
          "link": "http://arxiv.org/abs/2106.09276",
          "publishedOn": "2022-01-06T00:40:21.311Z",
          "wordCount": 585,
          "title": "Uniform Convergence of Interpolators: Gaussian Width, Norm Bounds, and Benign Overfitting. (arXiv:2106.09276v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00904",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Doleglo_K/0/1/0/all/0/1\">Kamil Doleg&#x142;o</a>, <a href=\"http://arxiv.org/find/math/1/au:+Paszynska_A/0/1/0/all/0/1\">Anna Paszy&#x144;ska</a>, <a href=\"http://arxiv.org/find/math/1/au:+Paszynski_M/0/1/0/all/0/1\">Maciej Paszy&#x144;ski</a>, <a href=\"http://arxiv.org/find/math/1/au:+Demkowicz_L/0/1/0/all/0/1\">Leszek Demkowicz</a>",
          "description": "This paper deals with the following important research question.\nTraditionally, the neural network employs non-linear activation functions\nconcatenated with linear operators to approximate a given physical phenomenon.\nThey \"fill the space\" with the concatenations of the activation functions and\nlinear operators and adjust their coefficients to approximate the physical\nphenomena. We claim that it is better to \"fill the space\" with linear\ncombinations of smooth higher-order B-splines base functions as employed by\nisogeometric analysis and utilize the neural networks to adjust the\ncoefficients of linear combinations. In other words, the possibilities of using\nneural networks for approximating the B-spline base functions' coefficients and\nby approximating the solution directly are evaluated. Solving differential\nequations with neural networks has been proposed by Maziar Raissi et al. in\n2017 by introducing Physics-informed Neural Networks (PINN), which naturally\nencode underlying physical laws as prior information. Approximation of\ncoefficients using a function as an input leverages the well-known capability\nof neural networks being universal function approximators. In essence, in the\nPINN approach the network approximates the value of the given field at a given\npoint. We present an alternative approach, where the physcial quantity is\napproximated as a linear combination of smooth B-spline basis functions, and\nthe neural network approximates the coefficients of B-splines. This research\ncompares results from the DNN approximating the coefficients of the linear\ncombination of B-spline basis functions, with the DNN approximating the\nsolution directly. We show that our approach is cheaper and more accurate when\napproximating smooth physical fields.",
          "link": "http://arxiv.org/abs/2201.00904",
          "publishedOn": "2022-01-06T00:40:21.241Z",
          "wordCount": 691,
          "title": "Deep neural networks for smooth approximation of physics with higher order and continuity B-spline base functions. (arXiv:2201.00904v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2103.00673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1\">Yuexiang Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chong You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhihui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Q/0/1/0/all/0/1\">Qing Qu</a>",
          "description": "Normalization techniques have become a basic component in modern\nconvolutional neural networks (ConvNets). In particular, many recent works\ndemonstrate that promoting the orthogonality of the weights helps train deep\nmodels and improve robustness. For ConvNets, most existing methods are based on\npenalizing or normalizing weight matrices derived from concatenating or\nflattening the convolutional kernels. These methods often destroy or ignore the\nbenign convolutional structure of the kernels; therefore, they are often\nexpensive or impractical for deep ConvNets. In contrast, we introduce a simple\nand efficient \"Convolutional Normalization\" (ConvNorm) method that can fully\nexploit the convolutional structure in the Fourier domain and serve as a simple\nplug-and-play module to be conveniently incorporated into any ConvNets. Our\nmethod is inspired by recent work on preconditioning methods for convolutional\nsparse coding and can effectively promote each layer's channel-wise isometry.\nFurthermore, we show that our ConvNorm can reduce the layerwise spectral norm\nof the weight matrices and hence improve the Lipschitzness of the network,\nleading to easier training and improved robustness for deep ConvNets. Applied\nto classification under noise corruptions and generative adversarial network\n(GAN), we show that the ConvNorm improves the robustness of common ConvNets\nsuch as ResNet and the performance of GAN. We verify our findings via numerical\nexperiments on CIFAR and ImageNet.",
          "link": "http://arxiv.org/abs/2103.00673",
          "publishedOn": "2022-01-06T00:40:21.219Z",
          "wordCount": 705,
          "title": "Convolutional Normalization: Improving Deep Convolutional Network Robustness and Training. (arXiv:2103.00673v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biyik_E/0/1/0/all/0/1\">Erdem B&#x131;y&#x131;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talati_A/0/1/0/all/0/1\">Aditi Talati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1\">Dorsa Sadigh</a>",
          "description": "Reward learning is a fundamental problem in human-robot interaction to have\nrobots that operate in alignment with what their human user wants. Many\npreference-based learning algorithms and active querying techniques have been\nproposed as a solution to this problem. In this paper, we present APReL, a\nlibrary for active preference-based reward learning algorithms, which enable\nresearchers and practitioners to experiment with the existing techniques and\neasily develop their own algorithms for various modules of the problem. APReL\nis available at https://github.com/Stanford-ILIAD/APReL.",
          "link": "http://arxiv.org/abs/2108.07259",
          "publishedOn": "2022-01-06T00:40:21.160Z",
          "wordCount": 549,
          "title": "APReL: A Library for Active Preference-based Reward Learning Algorithms. (arXiv:2108.07259v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00844",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Azeraf_E/0/1/0/all/0/1\">Elie Azeraf</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Monfrini_E/0/1/0/all/0/1\">Emmanuel Monfrini</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pieczynski_W/0/1/0/all/0/1\">Wojciech Pieczynski</a>",
          "description": "We deal with Bayesian generative and discriminative classifiers. Given a\nmodel distribution $p(x, y)$, with the observation $y$ and the target $x$, one\ncomputes generative classifiers by firstly considering $p(x, y)$ and then using\nthe Bayes rule to calculate $p(x | y)$. A discriminative model is directly\ngiven by $p(x | y)$, which is used to compute discriminative classifiers.\nHowever, recent works showed that the Bayesian Maximum Posterior classifier\ndefined from the Naive Bayes (NB) or Hidden Markov Chain (HMC), both generative\nmodels, can also match the discriminative classifier definition. Thus, there\nare situations in which dividing classifiers into \"generative\" and\n\"discriminative\" is somewhat misleading. Indeed, such a distinction is rather\nrelated to the way of computing classifiers, not to the classifiers themselves.\nWe present a general theoretical result specifying how a generative classifier\ninduced from a generative model can also be computed in a discriminative way\nfrom the same model. Examples of NB and HMC are found again as particular\ncases, and we apply the general result to two original extensions of NB, and\ntwo extensions of HMC, one of which being original. Finally, we shortly\nillustrate the interest of the new discriminative way of computing classifiers\nin the Natural Language Processing (NLP) framework.",
          "link": "http://arxiv.org/abs/2201.00844",
          "publishedOn": "2022-01-06T00:40:21.119Z",
          "wordCount": 624,
          "title": "Deriving discriminative classifiers from generative models. (arXiv:2201.00844v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00820",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mannam_V/0/1/0/all/0/1\">Varun Mannam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brandt_J/0/1/0/all/0/1\">Jacob Brandt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smith_C/0/1/0/all/0/1\">Cody J. Smith</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Howard_S/0/1/0/all/0/1\">Scott Howard</a>",
          "description": "Fluorescence microscopy has been a significant tool to observe long-term\nimaging of embryos (in vivo) growth over time. However, cumulative exposure is\nphototoxic to such sensitive live samples. While techniques like light-sheet\nfluorescence microscopy (LSFM) allow for reduced exposure, it is not well\nsuited for deep imaging models. Other computational techniques are\ncomputationally expensive and often lack restoration quality. To address this\nchallenge, one can use various low-dosage imaging techniques that are developed\nto achieve the 3D volume reconstruction using a few slices in the axial\ndirection (z-axis); however, they often lack restoration quality. Also,\nacquiring dense images (with small steps) in the axial direction is\ncomputationally expensive. To address this challenge, we present a compressive\nsensing (CS) based approach to fully reconstruct 3D volumes with the same\nsignal-to-noise ratio (SNR) with less than half of the excitation dosage. We\npresent the theory and experimentally validate the approach. To demonstrate our\ntechnique, we capture a 3D volume of the RFP labeled neurons in the zebrafish\nembryo spinal cord (30um thickness) with the axial sampling of 0.1um using a\nconfocal microscope. From the results, we observe the CS-based approach\nachieves accurate 3D volume reconstruction from less than 20% of the entire\nstack optical sections. The developed CS-based methodology in this work can be\neasily applied to other deep imaging modalities such as two-photon and\nlight-sheet microscopy, where reducing sample photo-toxicity is a critical\nchallenge.",
          "link": "http://arxiv.org/abs/2201.00820",
          "publishedOn": "2022-01-06T00:40:20.669Z",
          "wordCount": 691,
          "title": "Low dosage 3D volume fluorescence microscopy imaging using compressive sensing. (arXiv:2201.00820v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berke_A/0/1/0/all/0/1\">Alex Berke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doorley_R/0/1/0/all/0/1\">Ronan Doorley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larson_K/0/1/0/all/0/1\">Kent Larson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moro_E/0/1/0/all/0/1\">Esteban Moro</a>",
          "description": "Location data collected from mobile devices represent mobility behaviors at\nindividual and societal levels. These data have important applications ranging\nfrom transportation planning to epidemic modeling. However, issues must be\novercome to best serve these use cases: The data often represent a limited\nsample of the population and use of the data jeopardizes privacy.\n\nTo address these issues, we present and evaluate a system for generating\nsynthetic mobility data using a deep recurrent neural network (RNN) which is\ntrained on real location data. The system takes a population distribution as\ninput and generates mobility traces for a corresponding synthetic population.\n\nRelated generative approaches have not solved the challenges of capturing\nboth the patterns and variability in individuals' mobility behaviors over\nlonger time periods, while also balancing the generation of realistic data with\nprivacy. Our system leverages RNNs' ability to generate complex and novel\nsequences while retaining patterns from training data. Also, the model\nintroduces randomness used to calibrate the variation between the synthetic and\nreal data at the individual level. This is to both capture variability in human\nmobility, and protect user privacy.\n\nLocation based services (LBS) data from more than 22,700 mobile devices were\nused in an experimental evaluation across utility and privacy metrics. We show\nthe generated mobility data retain the characteristics of the real data, while\nvarying from the real data at the individual level, and where this amount of\nvariation matches the variation within the real data.",
          "link": "http://arxiv.org/abs/2201.01139",
          "publishedOn": "2022-01-06T00:40:20.619Z",
          "wordCount": 700,
          "title": "Generating synthetic mobility data for a realistic population with RNNs to improve utility and privacy. (arXiv:2201.01139v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2110.13769",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1\">Luoluo Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Simhon_E/0/1/0/all/0/1\">Eran Simhon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kulkarni_C/0/1/0/all/0/1\">Chaitanya Kulkarni</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Noren_D/0/1/0/all/0/1\">David Noren</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mans_R/0/1/0/all/0/1\">Ronny Mans</a>",
          "description": "In the hospital setting, a small percentage of recurrent frequent patients\ncontribute to a disproportional amount of healthcare resource usage. Moreover,\nin many of these cases, patient outcomes can be greatly improved by reducing\nreoccurring visits, especially when they are associated with substance abuse,\nmental health, and medical factors that could be improved by social-behavioral\ninterventions, outpatient or preventative care. Additionally, health care costs\ncan be reduced significantly with fewer preventable recurrent visits.\n\nTo address this, we developed a computationally efficient and interpretable\nframework that both identifies recurrent patients with high utilization and\ndetermines which comorbidities contribute most to their recurrent visits.\nSpecifically, we present a novel algorithm, called the minimum similarity\nassociation rules (MSAR), balancing confidence-support trade-off, to determine\nthe conditions most associated with reoccurring Emergency department (ED) and\ninpatient visits. We validate MSAR on a large Electric Health Record (EHR)\ndataset.",
          "link": "http://arxiv.org/abs/2110.13769",
          "publishedOn": "2022-01-05T00:39:37.220Z",
          "wordCount": null,
          "title": "Identify comorbidities associated with recurrent ED and in-patient visits. (arXiv:2110.13769v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.10767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shichang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiangyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_N/0/1/0/all/0/1\">Neal Naixue Xiong</a>",
          "description": "Fine-grained IP geolocation systems often rely on some linear delay-distance\nrules. They are not easy to generalize in network environments where the\ndelay-distance relationship is non-linear. Recently, researchers begin to pay\nattention to learning-based IP geolocation systems. These data-driven\nalgorithms leverage multi-layer perceptron (MLP) to model non-linear\nrelationships. However, MLP is not so suitable for modeling computer networks\nbecause networks are fundamentally graph-typed data. MLP-based IP geolocation\nsystems only treat IP addresses as isolated data instances, forgoing the\nconnection information between IP addresses. This would lead to sub-optimal\nrepresentations and limit the geolocation performance.\n\nGraph convolutional network (GCN) is an emerging deep learning method for\ngraph-typed data presentation. In this work, we research how to model computer\nnetworks for fine-grained IP geolocation with GCN. First, we formulate the IP\ngeolocation task as an attributed graph node regression problem. Then, a\nGCN-based IP geolocation system named GCN-Geo is proposed to predict the\nlocation of each IP address. GCN-Geo consists of a preprocessor, an encoder,\ngraph convolutional (GC) layers and a decoder. The preprocessor and the encoder\ntransform raw measurement data into the initial graph embeddings. GC layers\nrefine the initial graph node embeddings by explicitly modeling the connection\ninformation between IP addresses. The proposed decoder can relieve the\nconverging problem of GCN-Geo by considering some prior knowledge about target\nIP addresses. Finally, the experimental results in three real-world datasets\nshow that: GCN-Geo clearly outperforms the state-of-art rule-based and\nlearning-based baselines on all three datasets w.r.t. average, median and max\nerror distances. This work verifies the potential of GCN in fine-grained IP\ngeolocation.",
          "link": "http://arxiv.org/abs/2112.10767",
          "publishedOn": "2022-01-05T00:39:37.194Z",
          "wordCount": null,
          "title": "GCN-Geo: A Graph Convolution Network-based Fine-grained IP Geolocation System. (arXiv:2112.10767v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.12489",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gholamian_S/0/1/0/all/0/1\">Sina Gholamian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ward_P/0/1/0/all/0/1\">Paul A. S. Ward</a>",
          "description": "Logs are widely used to record runtime information of software systems, such\nas the timestamp and the importance of an event, the unique ID of the source of\nthe log, and a part of the state of a task's execution. The rich information of\nlogs enables system developers (and operators) to monitor the runtime behaviors\nof their systems and further track down system problems and perform analysis on\nlog data in production settings. However, the prior research on utilizing logs\nis scattered and that limits the ability of new researchers in this field to\nquickly get to the speed and hampers currently active researchers to advance\nthis field further. Therefore, this paper surveys and provides a systematic\nliterature review and mapping of the contemporary logging practices and log\nstatements' mining and monitoring techniques and their applications such as in\nsystem failure detection and diagnosis. We study a large number of conference\nand journal papers that appeared on top-level peer-reviewed venues.\nAdditionally, we draw high-level trends of ongoing research and categorize\npublications into subdivisions. In the end, and based on our holistic\nobservations during this survey, we provide a set of challenges and\nopportunities that will lead the researchers in academia and industry in moving\nthe field forward.",
          "link": "http://arxiv.org/abs/2110.12489",
          "publishedOn": "2022-01-05T00:39:37.193Z",
          "wordCount": null,
          "title": "A Comprehensive Survey of Logging in Software: From Logging Statements Automation to Log Mining and Analysis. (arXiv:2110.12489v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.07355",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hardalac_F/0/1/0/all/0/1\">F&#x131;rat Hardala&#xe7;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uysal_F/0/1/0/all/0/1\">Fatih Uysal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peker_O/0/1/0/all/0/1\">Ozan Peker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ciceklidag_M/0/1/0/all/0/1\">Murat &#xc7;i&#xe7;eklida&#x11f;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tolunay_T/0/1/0/all/0/1\">Tolga Tolunay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tokgoz_N/0/1/0/all/0/1\">Nil Tokg&#xf6;z</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kutbay_U/0/1/0/all/0/1\">U&#x11f;urhan Kutbay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demirciler_B/0/1/0/all/0/1\">Boran Demirciler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mert_F/0/1/0/all/0/1\">Fatih Mert</a>",
          "description": "Wrist fractures are common cases in hospitals, particularly in emergency\nservices. Physicians need images from various medical devices, and patients\nmedical history and physical examination to diagnose these fractures correctly\nand apply proper treatment. This study aims to perform fracture detection using\ndeep learning on wrist Xray images to assist physicians not specialized in the\nfield, working in emergency services in particular, in diagnosis of fractures.\nFor this purpose, 20 different detection procedures were performed using deep\nlearning based object detection models on dataset of wrist Xray images obtained\nfrom Gazi University Hospital. DCN, Dynamic R_CNN, Faster R_CNN, FSAF, Libra\nR_CNN, PAA, RetinaNet, RegNet and SABL deep learning based object detection\nmodels with various backbones were used herein. To further improve detection\nprocedures in the study, 5 different ensemble models were developed, which were\nlater used to reform an ensemble model to develop a detection model unique to\nour study, titled wrist fracture detection combo (WFD_C). Based on 26 different\nmodels for fracture detection, the highest result of detection was 0.8639\naverage precision (AP50) in WFD_C model developed. This study is supported by\nHuawei Turkey R&D Center within the scope of the ongoing cooperation project\ncoded 071813 among Gazi University, Huawei and Medskor.",
          "link": "http://arxiv.org/abs/2111.07355",
          "publishedOn": "2022-01-05T00:39:37.178Z",
          "wordCount": null,
          "title": "Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models. (arXiv:2111.07355v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.12855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Podschwadt_R/0/1/0/all/0/1\">Robert Podschwadt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takabi_D/0/1/0/all/0/1\">Daniel Takabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Peizhao Hu</a>",
          "description": "Outsourced computation for neural networks allows users access to state of\nthe art models without needing to invest in specialized hardware and know-how.\nThe problem is that the users lose control over potentially privacy sensitive\ndata. With homomorphic encryption (HE) computation can be performed on\nencrypted data without revealing its content. In this systematization of\nknowledge, we take an in-depth look at approaches that combine neural networks\nwith HE for privacy preservation. We categorize the changes to neural network\nmodels and architectures to make them computable over HE and how these changes\nimpact performance. We find numerous challenges to HE based privacy-preserving\ndeep learning such as computational overhead, usability, and limitations posed\nby the encryption schemes.",
          "link": "http://arxiv.org/abs/2112.12855",
          "publishedOn": "2022-01-05T00:39:37.172Z",
          "wordCount": null,
          "title": "SoK: Privacy-preserving Deep Learning with Homomorphic Encryption. (arXiv:2112.12855v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.04731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yujun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kuangqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zihang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y. F. Tan</a>",
          "description": "Class Incremental Learning (CIL) aims at learning a multi-class classifier in\na phase-by-phase manner, in which only data of a subset of the classes are\nprovided at each phase. Previous works mainly focus on mitigating forgetting in\nphases after the initial one. However, we find that improving CIL at its\ninitial phase is also a promising direction. Specifically, we experimentally\nshow that directly encouraging CIL Learner at the initial phase to output\nsimilar representations as the model jointly trained on all classes can greatly\nboost the CIL performance. Motivated by this, we study the difference between a\nna\\\"ively-trained initial-phase model and the oracle model. Specifically, since\none major difference between these two models is the number of training\nclasses, we investigate how such difference affects the model representations.\nWe find that, with fewer training classes, the data representations of each\nclass lie in a long and narrow region; with more training classes, the\nrepresentations of each class scatter more uniformly. Inspired by this\nobservation, we propose Class-wise Decorrelation (CwD) that effectively\nregularizes representations of each class to scatter more uniformly, thus\nmimicking the model jointly trained with all classes (i.e., the oracle model).\nOur CwD is simple to implement and easy to plug into existing methods.\nExtensive experiments on various benchmark datasets show that CwD consistently\nand significantly improves the performance of existing state-of-the-art methods\nby around 1\\% to 3\\%. Code will be released.",
          "link": "http://arxiv.org/abs/2112.04731",
          "publishedOn": "2022-01-05T00:39:37.161Z",
          "wordCount": null,
          "title": "Mimicking the Oracle: An Initial Phase Decorrelation Approach for Class Incremental Learning. (arXiv:2112.04731v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hazimeh_H/0/1/0/all/0/1\">Hussein Hazimeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1\">Aakanksha Chowdhery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sathiamoorthy_M/0/1/0/all/0/1\">Maheswaran Sathiamoorthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yihua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazumder_R/0/1/0/all/0/1\">Rahul Mazumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lichan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed H. Chi</a>",
          "description": "The Mixture-of-Experts (MoE) architecture is showing promising results in\nimproving parameter sharing in multi-task learning (MTL) and in scaling\nhigh-capacity neural networks. State-of-the-art MoE models use a trainable\nsparse gate to select a subset of the experts for each input example. While\nconceptually appealing, existing sparse gates, such as Top-k, are not smooth.\nThe lack of smoothness can lead to convergence and statistical performance\nissues when training with gradient-based methods. In this paper, we develop\nDSelect-k: a continuously differentiable and sparse gate for MoE, based on a\nnovel binary encoding formulation. The gate can be trained using first-order\nmethods, such as stochastic gradient descent, and offers explicit control over\nthe number of experts to select. We demonstrate the effectiveness of DSelect-k\non both synthetic and real MTL datasets with up to $128$ tasks. Our experiments\nindicate that DSelect-k can achieve statistically significant improvements in\nprediction and expert selection over popular MoE gates. Notably, on a\nreal-world, large-scale recommender system, DSelect-k achieves over $22\\%$\nimprovement in predictive performance compared to Top-k. We provide an\nopen-source implementation of DSelect-k.",
          "link": "http://arxiv.org/abs/2106.03760",
          "publishedOn": "2022-01-05T00:39:37.159Z",
          "wordCount": null,
          "title": "DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning. (arXiv:2106.03760v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.14142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lifu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bo Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xing Cao</a>",
          "description": "Recurrent Neural Network (RNN) is a fundamental structure in deep learning.\nRecently, some works study the training process of over-parameterized neural\nnetworks, and show that over-parameterized networks can learn functions in some\nnotable concept classes with a provable generalization error bound. In this\npaper, we analyze the training and generalization for RNNs with random\ninitialization, and provide the following improvements over recent works:\n\n1) For a RNN with input sequence $x=(X_1,X_2,...,X_L)$, previous works study\nto learn functions that are summation of $f(\\beta^T_lX_l)$ and require\nnormalized conditions that $||X_l||\\leq\\epsilon$ with some very small\n$\\epsilon$ depending on the complexity of $f$. In this paper, using detailed\nanalysis about the neural tangent kernel matrix, we prove a generalization\nerror bound to learn such functions without normalized conditions and show that\nsome notable concept classes are learnable with the numbers of iterations and\nsamples scaling almost-polynomially in the input length $L$.\n\n2) Moreover, we prove a novel result to learn N-variables functions of input\nsequence with the form $f(\\beta^T[X_{l_1},...,X_{l_N}])$, which do not belong\nto the \"additive\" concept class, i,e., the summation of function $f(X_l)$. And\nwe show that when either $N$ or $l_0=\\max(l_1,..,l_N)-\\min(l_1,..,l_N)$ is\nsmall, $f(\\beta^T[X_{l_1},...,X_{l_N}])$ will be learnable with the number\niterations and samples scaling almost-polynomially in the input length $L$.",
          "link": "http://arxiv.org/abs/2109.14142",
          "publishedOn": "2022-01-05T00:39:37.157Z",
          "wordCount": null,
          "title": "On the Provable Generalization of Recurrent Neural Networks. (arXiv:2109.14142v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.10302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sunwoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chaoyang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1\">Salman Avestimehr</a>",
          "description": "In Federated Learning, a common approach for aggregating local models across\nclients is periodic averaging of the full model parameters. It is, however,\nknown that different layers of neural networks can have a different degree of\nmodel discrepancy across the clients. The conventional full aggregation scheme\ndoes not consider such a difference and synchronizes the whole model parameters\nat once, resulting in inefficient network bandwidth consumption. Aggregating\nthe parameters that are similar across the clients does not make meaningful\ntraining progress while increasing the communication cost. We propose FedLAMA,\na layer-wise model aggregation scheme for scalable Federated Learning. FedLAMA\nadaptively adjusts the aggregation interval in a layer-wise manner, jointly\nconsidering the model discrepancy and the communication cost. The layer-wise\naggregation method enables to finely control the aggregation interval to relax\nthe aggregation frequency without a significant impact on the model accuracy.\nOur empirical study shows that FedLAMA reduces the communication cost by up to\n60% for IID data and 70% for non-IID data while achieving a comparable accuracy\nto FedAvg.",
          "link": "http://arxiv.org/abs/2110.10302",
          "publishedOn": "2022-01-05T00:39:37.157Z",
          "wordCount": null,
          "title": "Layer-wise Adaptive Model Aggregation for Scalable Federated Learning. (arXiv:2110.10302v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.09169",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Weihao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koleczek_D/0/1/0/all/0/1\">David Koleczek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradhan_S/0/1/0/all/0/1\">Siddhant Pradhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perello_N/0/1/0/all/0/1\">Nicholas Perello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chettiar_V/0/1/0/all/0/1\">Vivek Chettiar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohra_V/0/1/0/all/0/1\">Vishal Rohra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajaram_A/0/1/0/all/0/1\">Aaslesha Rajaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Soundararajan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_H/0/1/0/all/0/1\">H M Sajjad Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandak_Y/0/1/0/all/0/1\">Yash Chandak</a>",
          "description": "Shared autonomy refers to approaches for enabling an autonomous agent to\ncollaborate with a human with the aim of improving human performance. However,\nbesides improving performance, it may often also be beneficial that the agent\nconcurrently accounts for preserving the user's experience or satisfaction of\ncollaboration. In order to address this additional goal, we examine approaches\nfor improving the user experience by constraining the number of interventions\nby the autonomous agent. We propose two model-free reinforcement learning\nmethods that can account for both hard and soft constraints on the number of\ninterventions. We show that not only does our method outperform the existing\nbaseline, but also eliminates the need to manually tune a black-box\nhyperparameter for controlling the level of assistance. We also provide an\nin-depth analysis of intervention scenarios in order to further illuminate\nsystem understanding.",
          "link": "http://arxiv.org/abs/2112.09169",
          "publishedOn": "2022-01-05T00:39:37.156Z",
          "wordCount": null,
          "title": "On Optimizing Interventions in Shared Autonomy. (arXiv:2112.09169v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wainakh_A/0/1/0/all/0/1\">Aidmar Wainakh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ventola_F/0/1/0/all/0/1\">Fabrizio Ventola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mussig_T/0/1/0/all/0/1\">Till M&#xfc;&#xdf;ig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keim_J/0/1/0/all/0/1\">Jens Keim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordero_C/0/1/0/all/0/1\">Carlos Garcia Cordero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_E/0/1/0/all/0/1\">Ephraim Zimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grube_T/0/1/0/all/0/1\">Tim Grube</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhlhauser_M/0/1/0/all/0/1\">Max M&#xfc;hlh&#xe4;user</a>",
          "description": "Federated learning enables multiple users to build a joint model by sharing\ntheir model updates (gradients), while their raw data remains local on their\ndevices. In contrast to the common belief that this provides privacy benefits,\nwe here add to the very recent results on privacy risks when sharing gradients.\nSpecifically, we investigate Label Leakage from Gradients (LLG), a novel attack\nto extract the labels of the users' training data from their shared gradients.\nThe attack exploits the direction and magnitude of gradients to determine the\npresence or absence of any label. LLG is simple yet effective, capable of\nleaking potential sensitive information represented by labels, and scales well\nto arbitrary batch sizes and multiple classes. We mathematically and\nempirically demonstrate the validity of the attack under different settings.\nMoreover, empirical results show that LLG successfully extracts labels with\nhigh accuracy at the early stages of model training. We also discuss different\ndefense mechanisms against such leakage. Our findings suggest that gradient\ncompression is a practical technique to mitigate the attack.",
          "link": "http://arxiv.org/abs/2105.09369",
          "publishedOn": "2022-01-05T00:39:37.154Z",
          "wordCount": null,
          "title": "User-Level Label Leakage from Gradients in Federated Learning. (arXiv:2105.09369v4 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We study reinforcement learning (RL) with linear function approximation under\nthe adaptivity constraint. We consider two popular limited adaptivity models:\nthe batch learning model and the rare policy switch model, and propose two\nefficient online RL algorithms for episodic linear Markov decision processes,\nwhere the transition probability and the reward function can be represented as\na linear function of some known feature mapping. In specific, for the batch\nlearning model, our proposed LSVI-UCB-Batch algorithm achieves an $\\tilde\nO(\\sqrt{d^3H^3T} + dHT/B)$ regret, where $d$ is the dimension of the feature\nmapping, $H$ is the episode length, $T$ is the number of interactions and $B$\nis the number of batches. Our result suggests that it suffices to use only\n$\\sqrt{T/dH}$ batches to obtain $\\tilde O(\\sqrt{d^3H^3T})$ regret. For the rare\npolicy switch model, our proposed LSVI-UCB-RareSwitch algorithm enjoys an\n$\\tilde O(\\sqrt{d^3H^3T[1+T/(dH)]^{dH/B}})$ regret, which implies that $dH\\log\nT$ policy switches suffice to obtain the $\\tilde O(\\sqrt{d^3H^3T})$ regret. Our\nalgorithms achieve the same regret as the LSVI-UCB algorithm (Jin et al.,\n2019), yet with a substantially smaller amount of adaptivity. We also establish\na lower bound for the batch learning model, which suggests that the dependency\non $B$ in our regret bound is tight.",
          "link": "http://arxiv.org/abs/2101.02195",
          "publishedOn": "2022-01-05T00:39:37.091Z",
          "wordCount": null,
          "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation Under Adaptivity Constraints. (arXiv:2101.02195v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yihang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>",
          "description": "Deep neural networks (DNNs) have shown superior performances on various\nmultimodal learning problems. However, it often requires huge efforts to adapt\nDNNs to individual multimodal tasks by manually engineering unimodal features\nand designing multimodal feature fusion strategies. This paper proposes Bilevel\nMultimodal Neural Architecture Search (BM-NAS) framework, which makes the\narchitecture of multimodal fusion models fully searchable via a bilevel\nsearching scheme. At the upper level, BM-NAS selects the inter/intra-modal\nfeature pairs from the pretrained unimodal backbones. At the lower level,\nBM-NAS learns the fusion strategy for each feature pair, which is a combination\nof predefined primitive operations. The primitive operations are elaborately\ndesigned and they can be flexibly combined to accommodate various effective\nfeature fusion modules such as multi-head attention (Transformer) and Attention\non Attention (AoA). Experimental results on three multimodal tasks demonstrate\nthe effectiveness and efficiency of the proposed BM-NAS framework. BM-NAS\nachieves competitive performances with much less search time and fewer model\nparameters in comparison with the existing generalized multimodal NAS methods.",
          "link": "http://arxiv.org/abs/2104.09379",
          "publishedOn": "2022-01-05T00:39:37.091Z",
          "wordCount": null,
          "title": "BM-NAS: Bilevel Multimodal Neural Architecture Search. (arXiv:2104.09379v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiafan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We study the reinforcement learning problem for discounted Markov Decision\nProcesses (MDPs) under the tabular setting. We propose a model-based algorithm\nnamed UCBVI-$\\gamma$, which is based on the \\emph{optimism in the face of\nuncertainty principle} and the Bernstein-type bonus. We show that\nUCBVI-$\\gamma$ achieves an $\\tilde{O}\\big({\\sqrt{SAT}}/{(1-\\gamma)^{1.5}}\\big)$\nregret, where $S$ is the number of states, $A$ is the number of actions,\n$\\gamma$ is the discount factor and $T$ is the number of steps. In addition, we\nconstruct a class of hard MDPs and show that for any algorithm, the expected\nregret is at least $\\tilde{\\Omega}\\big({\\sqrt{SAT}}/{(1-\\gamma)^{1.5}}\\big)$.\nOur upper bound matches the minimax lower bound up to logarithmic factors,\nwhich suggests that UCBVI-$\\gamma$ is nearly minimax optimal for discounted\nMDPs.",
          "link": "http://arxiv.org/abs/2010.00587",
          "publishedOn": "2022-01-05T00:39:37.090Z",
          "wordCount": null,
          "title": "Nearly Minimax Optimal Reinforcement Learning for Discounted MDPs. (arXiv:2010.00587v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.13984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1\">Buyun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1\">Tim Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Ju Sun</a>",
          "description": "Optimizing nonconvex (NCVX) problems, especially nonsmooth and constrained\nones, is an essential part of machine learning. However, it can be hard to\nreliably solve such problems without optimization expertise. Existing\ngeneral-purpose NCVX optimization packages are powerful but typically cannot\nhandle nonsmoothness. GRANSO is among the first optimization solvers targeting\ngeneral nonsmooth NCVX problems with nonsmooth constraints, but, as it is\nimplemented in MATLAB and requires the user to provide analytical gradients,\nGRANSO is often not a convenient choice in machine learning (especially deep\nlearning) applications. To greatly lower the technical barrier, we introduce a\nnew software package called NCVX, whose initial release contains the solver\nPyGRANSO, a PyTorch-enabled port of GRANSO incorporating auto-differentiation,\nGPU acceleration, tensor input, and support for new QP solvers. NCVX is built\non freely available and widely used open-source frameworks, and as a highlight,\ncan solve general constrained deep learning problems, the first of its kind.\nNCVX is available at https://ncvx.org, with detailed documentation and numerous\nexamples from machine learning and other fields.",
          "link": "http://arxiv.org/abs/2111.13984",
          "publishedOn": "2022-01-05T00:39:37.090Z",
          "wordCount": null,
          "title": "NCVX: A User-Friendly and Scalable Package for Nonconvex Optimization in Machine Learning. (arXiv:2111.13984v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.00973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianmeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zhihan Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jing Bai</a>",
          "description": "Graph Neural Networks (GNNs) have shown advantages in various graph-based\napplications. Most existing GNNs assume strong homophily of graph structure and\napply permutation-invariant local aggregation of neighbors to learn a\nrepresentation for each node. However, they fail to generalize to heterophilic\ngraphs, where most neighboring nodes have different labels or features, and the\nrelevant nodes are distant. Few recent studies attempt to address this problem\nby combining multiple hops of hidden representations of central nodes (i.e.,\nmulti-hop-based approaches) or sorting the neighboring nodes based on attention\nscores (i.e., ranking-based approaches). As a result, these approaches have\nsome apparent limitations. On the one hand, multi-hop-based approaches do not\nexplicitly distinguish relevant nodes from a large number of multi-hop\nneighborhoods, leading to a severe over-smoothing problem. On the other hand,\nranking-based models do not joint-optimize node ranking with end tasks and\nresult in sub-optimal solutions. In this work, we present Graph Pointer Neural\nNetworks (GPNN) to tackle the challenges mentioned above. We leverage a pointer\nnetwork to select the most relevant nodes from a large amount of multi-hop\nneighborhoods, which constructs an ordered sequence according to the\nrelationship with the central node. 1D convolution is then applied to extract\nhigh-level features from the node sequence. The pointer-network-based ranker in\nGPNN is joint-optimized with other parts in an end-to-end manner. Extensive\nexperiments are conducted on six public node classification datasets with\nheterophilic graphs. The results show that GPNN significantly improves the\nclassification performance of state-of-the-art methods. In addition, analyses\nalso reveal the privilege of the proposed GPNN in filtering out irrelevant\nneighbors and reducing over-smoothing.",
          "link": "http://arxiv.org/abs/2110.00973",
          "publishedOn": "2022-01-05T00:39:37.086Z",
          "wordCount": null,
          "title": "Graph Pointer Neural Networks. (arXiv:2110.00973v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.09794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kirk_R/0/1/0/all/0/1\">Robert Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Amy Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grefenstette_E/0/1/0/all/0/1\">Edward Grefenstette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1\">Tim Rockt&#xe4;schel</a>",
          "description": "The study of generalisation in deep Reinforcement Learning (RL) aims to\nproduce RL algorithms whose policies generalise well to novel unseen situations\nat deployment time, avoiding overfitting to their training environments.\nTackling this is vital if we are to deploy reinforcement learning algorithms in\nreal world scenarios, where the environment will be diverse, dynamic and\nunpredictable. This survey is an overview of this nascent field. We provide a\nunifying formalism and terminology for discussing different generalisation\nproblems, building upon previous works. We go on to categorise existing\nbenchmarks for generalisation, as well as current methods for tackling the\ngeneralisation problem. Finally, we provide a critical discussion of the\ncurrent state of the field, including recommendations for future work. Among\nother conclusions, we argue that taking a purely procedural content generation\napproach to benchmark design is not conducive to progress in generalisation, we\nsuggest fast online adaptation and tackling RL-specific problems as some areas\nfor future work on methods for generalisation, and we recommend building\nbenchmarks in underexplored problem settings such as offline RL generalisation\nand reward-function variation.",
          "link": "http://arxiv.org/abs/2111.09794",
          "publishedOn": "2022-01-05T00:39:37.086Z",
          "wordCount": null,
          "title": "A Survey of Generalisation in Deep Reinforcement Learning. (arXiv:2111.09794v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09078",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Cai_H/0/1/0/all/0/1\">Haoyuan Cai</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ye_Q/0/1/0/all/0/1\">Qi Ye</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Deng_D/0/1/0/all/0/1\">Dong-Ling Deng</a>",
          "description": "Quantum computers hold unprecedented potentials for machine learning\napplications. Here, we prove that physical quantum circuits are PAC (probably\napproximately correct) learnable on a quantum computer via empirical risk\nminimization: to learn a parametric quantum circuit with at most $n^c$ gates\nand each gate acting on a constant number of qubits, the sample complexity is\nbounded by $\\tilde{O}(n^{c+1})$. In particular, we explicitly construct a\nfamily of variational quantum circuits with $O(n^{c+1})$ elementary gates\narranged in a fixed pattern, which can represent all physical quantum circuits\nconsisting of at most $n^c$ elementary gates. Our results provide a valuable\nguide for quantum machine learning in both theory and practice.",
          "link": "http://arxiv.org/abs/2107.09078",
          "publishedOn": "2022-01-05T00:39:37.081Z",
          "wordCount": null,
          "title": "Sample Complexity of Learning Parametric Quantum Circuits. (arXiv:2107.09078v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_K/0/1/0/all/0/1\">Kaiming Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chih-Hang J. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Arieh_D/0/1/0/all/0/1\">David Ben-Arieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1\">Ashesh Sinha</a>",
          "description": "Bayesian Optimization algorithm has become a promising approach for nonlinear\nglobal optimization problems and many machine learning applications. Over the\npast few years, improvements and enhancements have been brought forward and\nthey have shown some promising results in solving the complex dynamic problems,\nsystems of ordinary differential equations where the objective functions are\ncomputationally expensive to evaluate. Besides, the straightforward\nimplementation of the Bayesian Optimization algorithm performs well merely for\noptimization problems with 10-20 dimensions. The study presented in this paper\nproposes a new high dimensional Bayesian Optimization algorithm combining\nRecurrent neural networks, which is expected to predict the optimal solution\nfor the global optimization problems with high dimensional or time series\ndecision models. The proposed RNN-BO algorithm can solve the optimal control\nproblems in the lower dimension space and then learn from the historical data\nusing the recurrent neural network to learn the historical optimal solution\ndata and predict the optimal control strategy for any new initial system value\nsetting. In addition, accurately and quickly providing the optimal control\nstrategy is essential to effectively and efficiently control the epidemic\nspread while minimizing the associated financial costs. Therefore, to verify\nthe effectiveness of the proposed algorithm, computational experiments are\ncarried out on a deterministic SEIR epidemic model and a stochastic SIS optimal\ncontrol model. Finally, we also discuss the impacts of different numbers of the\nRNN layers and training epochs on the trade-off between solution quality and\nrelated computational efforts.",
          "link": "http://arxiv.org/abs/2201.00147",
          "publishedOn": "2022-01-05T00:39:37.080Z",
          "wordCount": null,
          "title": "High-dimensional Bayesian Optimization Algorithm with Recurrent Neural Network for Disease Control Models in Time Series. (arXiv:2201.00147v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00414",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chuanbo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahbod_A/0/1/0/all/0/1\">Amirreza Mahbod</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ellinger_I/0/1/0/all/0/1\">Isabella Ellinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Galdran_A/0/1/0/all/0/1\">Adrian Galdran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gopalakrishnan_S/0/1/0/all/0/1\">Sandeep Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niezgoda_J/0/1/0/all/0/1\">Jeffrey Niezgoda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Z/0/1/0/all/0/1\">Zeyun Yu</a>",
          "description": "Acute and chronic wounds with varying etiologies burden the healthcare\nsystems economically. The advanced wound care market is estimated to reach $22\nbillion by 2024. Wound care professionals provide proper diagnosis and\ntreatment with heavy reliance on images and image documentation. Segmentation\nof wound boundaries in images is a key component of the care and diagnosis\nprotocol since it is important to estimate the area of the wound and provide\nquantitative measurement for the treatment. Unfortunately, this process is very\ntime-consuming and requires a high level of expertise. Recently automatic wound\nsegmentation methods based on deep learning have shown promising performance\nbut require large datasets for training and it is unclear which methods perform\nbetter. To address these issues, we propose the Foot Ulcer Segmentation\nchallenge (FUSeg) organized in conjunction with the 2021 International\nConference on Medical Image Computing and Computer Assisted Intervention\n(MICCAI). We built a wound image dataset containing 1,210 foot ulcer images\ncollected over 2 years from 889 patients. It is pixel-wise annotated by wound\ncare experts and split into a training set with 1010 images and a testing set\nwith 200 images for evaluation. Teams around the world developed automated\nmethods to predict wound segmentations on the testing set of which annotations\nwere kept private. The predictions were evaluated and ranked based on the\naverage Dice coefficient. The FUSeg challenge remains an open challenge as a\nbenchmark for wound segmentation after the conference.",
          "link": "http://arxiv.org/abs/2201.00414",
          "publishedOn": "2022-01-05T00:39:37.079Z",
          "wordCount": null,
          "title": "FUSeg: The Foot Ulcer Segmentation Challenge. (arXiv:2201.00414v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2111.12950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ando_S/0/1/0/all/0/1\">Shin Ando</a>",
          "description": "This paper proposes an information-theoretic loss for learning deep neural\nnetworks. We propose a loss function based on the Information Bottleneck\nprinciple and a max-margin loss with an aim to increase the class separability\nin the embedded space. While deep neural network models have excelled in\nsupervised learning tasks with large-scale labeled data available, they are\nprone to practical issues in testing samples outside of classes shown in\ntraining, e.g., anomaly detection and out-of-distribution detection. In such\ntasks, it is not sufficient to merely discriminate between known classes. Our\nintuition is to represent the known classes in compact and separated embedded\nregions in order to decrease the possibility of known and unseen classes\nlargely overlapping in the embedded space. We show that the IB-based loss\nfunction reflects the inter-class distances as well as the compactness within\nclasses, thus will extend the extending models of the existing deep data\ndescription models. Our empirical study shows that the proposed model improves\nthe segmentation of normal classes in the deep feature space which contributes\nto identifying the out-of-distribution samples.",
          "link": "http://arxiv.org/abs/2111.12950",
          "publishedOn": "2022-01-05T00:39:37.079Z",
          "wordCount": null,
          "title": "Deep Representation Learning with an Information-theoretic Loss. (arXiv:2111.12950v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joyce_R/0/1/0/all/0/1\">Robert J. Joyce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1\">Edward Raff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicholas_C/0/1/0/all/0/1\">Charles Nicholas</a>",
          "description": "Although groups of strongly correlated antivirus engines are known to exist,\nat present there is limited understanding of how or why these correlations came\nto be. Using a corpus of 25 million VirusTotal reports representing over a\ndecade of antivirus scan data, we challenge prevailing wisdom that these\ncorrelations primarily originate from \"first-order\" interactions such as\nantivirus vendors copying the labels of leading vendors. We introduce the\nTemporal Rank-1 Similarity Matrix decomposition (R1SM-T) in order to\ninvestigate the origins of these correlations and to model how consensus\namongst antivirus engines changes over time. We reveal that first-order\ninteractions do not explain as much behavior in antivirus correlation as\npreviously thought, and that the relationships between antivirus engines are\nhighly volatile. We make recommendations on items in need of future study and\nconsideration based on our findings.",
          "link": "http://arxiv.org/abs/2201.00757",
          "publishedOn": "2022-01-05T00:39:37.078Z",
          "wordCount": null,
          "title": "Rank-1 Similarity Matrix Decomposition For Modeling Changes in Antivirus Consensus Through Time. (arXiv:2201.00757v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00644",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Heremans_E/0/1/0/all/0/1\">Elisabeth R. M. Heremans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phan_H/0/1/0/all/0/1\">Huy Phan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ansari_A/0/1/0/all/0/1\">Amir H. Ansari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Borzee_P/0/1/0/all/0/1\">Pascal Borz&#xe9;e</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buyse_B/0/1/0/all/0/1\">Bertien Buyse</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Testelmans_D/0/1/0/all/0/1\">Dries Testelmans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vos_M/0/1/0/all/0/1\">Maarten De Vos</a>",
          "description": "Objective: With the rapid rise of wearable sleep monitoring devices with\nnon-conventional electrode configurations, there is a need for automated\nalgorithms that can perform sleep staging on configurations with small amounts\nof labeled data. Transfer learning has the ability to adapt neural network\nweights from a source modality (e.g. standard electrode configuration) to a new\ntarget modality (e.g. non-conventional electrode configuration). Methods: We\npropose feature matching, a new transfer learning strategy as an alternative to\nthe commonly used finetuning approach. This method consists of training a model\nwith larger amounts of data from the source modality and few paired samples of\nsource and target modality. For those paired samples, the model extracts\nfeatures of the target modality, matching these to the features from the\ncorresponding samples of the source modality. Results: We compare feature\nmatching to finetuning for three different target domains, with two different\nneural network architectures, and with varying amounts of training data.\nParticularly on small cohorts (i.e. 2 - 5 labeled recordings in the\nnon-conventional recording setting), feature matching systematically\noutperforms finetuning with mean relative differences in accuracy ranging from\n0.4% to 4.7% for the different scenarios and datasets. Conclusion: Our findings\nsuggest that feature matching outperforms finetuning as a transfer learning\napproach, especially in very low data regimes. Significance: As such, we\nconclude that feature matching is a promising new method for wearable sleep\nstaging with novel devices.",
          "link": "http://arxiv.org/abs/2201.00644",
          "publishedOn": "2022-01-05T00:39:37.076Z",
          "wordCount": null,
          "title": "Feature matching as improved transfer learning technique for wearable EEG. (arXiv:2201.00644v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00227",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hosseini_H/0/1/0/all/0/1\">Hesamoddin Hosseini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Monsefi_R/0/1/0/all/0/1\">Reza Monsefi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shadroo_S/0/1/0/all/0/1\">Shabnam Shadroo</a>",
          "description": "Lung cancer has been one of the most prevalent disease in recent years.\nAccording to the research of this field, more than 200,000 cases are identified\neach year in the US. Uncontrolled multiplication and growth of the lung cells\nresult in malignant tumour formation. Recently, deep learning algorithms,\nespecially Convolutional Neural Networks (CNN), have become a superior way to\nautomatically diagnose disease. The purpose of this article is to review\ndifferent models that lead to different accuracy and sensitivity in the\ndiagnosis of early-stage lung cancer and to help physicians and researchers in\nthis field. The main purpose of this work is to identify the challenges that\nexist in lung cancer based on deep learning. The survey is systematically\nwritten that combines regular mapping and literature review to review 32\nconference and journal articles in the field from 2016 to 2021. After analysing\nand reviewing the articles, the questions raised in the articles are being\nanswered. This research is superior to other review articles in this field due\nto the complete review of relevant articles and systematic write up.",
          "link": "http://arxiv.org/abs/2201.00227",
          "publishedOn": "2022-01-05T00:39:37.075Z",
          "wordCount": null,
          "title": "Deep Learning Applications for Lung Cancer Diagnosis: A systematic review. (arXiv:2201.00227v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.02976",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gao_R/0/1/0/all/0/1\">Rui Gao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Calmon_F/0/1/0/all/0/1\">Flavio P. Calmon</a>",
          "description": "Machine learning models trained by different optimization algorithms under\ndifferent data distributions can exhibit distinct generalization behaviors. In\nthis paper, we analyze the generalization of models trained by noisy iterative\nalgorithms. We derive distribution-dependent generalization bounds by\nconnecting noisy iterative algorithms to additive noise channels found in\ncommunication and information theory. Our generalization bounds shed light on\nseveral applications, including differentially private stochastic gradient\ndescent (DP-SGD), federated learning, and stochastic gradient Langevin dynamics\n(SGLD). We demonstrate our bounds through numerical experiments, showing that\nthey can help understand recent empirical observations of the generalization\nphenomena of neural networks.",
          "link": "http://arxiv.org/abs/2102.02976",
          "publishedOn": "2022-01-05T00:39:37.075Z",
          "wordCount": null,
          "title": "Generalization Bounds for Noisy Iterative Algorithms Using Properties of Additive Noise Channels. (arXiv:2102.02976v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.11684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tripuraneni_N/0/1/0/all/0/1\">Nilesh Tripuraneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "Meta-learning, or learning-to-learn, seeks to design algorithms that can\nutilize previous experience to rapidly learn new skills or adapt to new\nenvironments. Representation learning -- a key tool for performing\nmeta-learning -- learns a data representation that can transfer knowledge\nacross multiple tasks, which is essential in regimes where data is scarce.\nDespite a recent surge of interest in the practice of meta-learning, the\ntheoretical underpinnings of meta-learning algorithms are lacking, especially\nin the context of learning transferable representations. In this paper, we\nfocus on the problem of multi-task linear regression -- in which multiple\nlinear regression models share a common, low-dimensional linear representation.\nHere, we provide provably fast, sample-efficient algorithms to address the dual\nchallenges of (1) learning a common set of features from multiple, related\ntasks, and (2) transferring this knowledge to new, unseen tasks. Both are\ncentral to the general problem of meta-learning. Finally, we complement these\nresults by providing information-theoretic lower bounds on the sample\ncomplexity of learning these linear features.",
          "link": "http://arxiv.org/abs/2002.11684",
          "publishedOn": "2022-01-05T00:39:37.065Z",
          "wordCount": null,
          "title": "Provable Meta-Learning of Linear Representations. (arXiv:2002.11684v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.05472",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Maguolo_G/0/1/0/all/0/1\">Gianluca Maguolo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paci_M/0/1/0/all/0/1\">Michelangelo Paci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nanni_L/0/1/0/all/0/1\">Loris Nanni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bonan_L/0/1/0/all/0/1\">Ludovico Bonan</a>",
          "description": "Audio data augmentation is a key step in training deep neural networks for\nsolving audio classification tasks. In this paper, we introduce Audiogmenter, a\nnovel audio data augmentation library in MATLAB. We provide 15 different\naugmentation algorithms for raw audio data and 8 for spectrograms. We\nefficiently implemented several augmentation techniques whose usefulness has\nbeen extensively proved in the literature. To the best of our knowledge, this\nis the largest MATLAB audio data augmentation library freely available. We\nvalidate the efficiency of our algorithms evaluating them on the ESC-50\ndataset. The toolbox and its documentation can be downloaded at\nhttps://github.com/LorisNanni/Audiogmenter.",
          "link": "http://arxiv.org/abs/1912.05472",
          "publishedOn": "2022-01-05T00:39:37.064Z",
          "wordCount": null,
          "title": "Audiogmenter: a MATLAB Toolbox for Audio Data Augmentation. (arXiv:1912.05472v4 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">You Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quek_C/0/1/0/all/0/1\">Chai Quek</a>",
          "description": "The widely applied density peak clustering (DPC) algorithm makes an intuitive\ncluster formation assumption that cluster centers are often surrounded by data\npoints with lower local density and far away from other data points with higher\nlocal density. However, this assumption suffers from one limitation that it is\noften problematic when identifying clusters with lower density because they\nmight be easily merged into other clusters with higher density. As a result,\nDPC may not be able to identify clusters with variational density. To address\nthis issue, we propose a variational density peak clustering (VDPC) algorithm,\nwhich is designed to systematically and autonomously perform the clustering\ntask on datasets with various types of density distributions. Specifically, we\nfirst propose a novel method to identify the representatives among all data\npoints and construct initial clusters based on the identified representatives\nfor further analysis of the clusters' property. Furthermore, we divide all data\npoints into different levels according to their local density and propose a\nunified clustering framework by combining the advantages of both DPC and\nDBSCAN. Thus, all the identified initial clusters spreading across different\ndensity levels are systematically processed to form the final clusters. To\nevaluate the effectiveness of the proposed VDPC algorithm, we conduct extensive\nexperiments using 20 datasets including eight synthetic, six real-world and six\nimage datasets. The experimental results show that VDPC outperforms two\nclassical algorithms (i.e., DPC and DBSCAN) and four state-of-the-art extended\nDPC algorithms.",
          "link": "http://arxiv.org/abs/2201.00641",
          "publishedOn": "2022-01-05T00:39:37.063Z",
          "wordCount": null,
          "title": "VDPC: Variational Density Peak Clustering Algorithm. (arXiv:2201.00641v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2110.11182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xuanlong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1\">Gianni Franchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldea_E/0/1/0/all/0/1\">Emanuel Aldea</a>",
          "description": "It has become critical for deep learning algorithms to quantify their output\nuncertainties to satisfy reliability constraints and provide accurate results.\nUncertainty estimation for regression has received less attention than\nclassification due to the more straightforward standardized output of the\nlatter class of tasks and their high importance. However, regression problems\nare encountered in a wide range of applications in computer vision. We propose\nSLURP, a generic approach for regression uncertainty estimation via a side\nlearner that exploits the output and the intermediate representations generated\nby the main task model. We test SLURP on two critical regression tasks in\ncomputer vision: monocular depth and optical flow estimation. In addition, we\nconduct exhaustive benchmarks comprising transfer to different datasets and the\naddition of aleatoric noise. The results show that our proposal is generic and\nreadily applicable to various regression problems and has a low computational\ncost with respect to existing solutions.",
          "link": "http://arxiv.org/abs/2110.11182",
          "publishedOn": "2022-01-05T00:39:37.062Z",
          "wordCount": null,
          "title": "SLURP: Side Learning Uncertainty for Regression Problems. (arXiv:2110.11182v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qixin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zengde Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zaiyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yu Yang</a>",
          "description": "In this paper, we revisit the constrained and stochastic continuous\nsubmodular maximization in both offline and online settings. For each\n$\\gamma$-weakly DR-submodular function $f$, we use the factor-revealing\noptimization equation to derive an optimal auxiliary function $F$, whose\nstationary points provide a $(1-e^{-\\gamma})$-approximation to the global\nmaximum value (denoted as $OPT$) of problem\n$\\max_{\\boldsymbol{x}\\in\\mathcal{C}}f(\\boldsymbol{x})$. Naturally, the\nprojected (mirror) gradient ascent relied on this non-oblivious function\nachieves $(1-e^{-\\gamma}-\\epsilon^{2})OPT-\\epsilon$ after $O(1/\\epsilon^{2})$\niterations, beating the traditional\n$(\\frac{\\gamma^{2}}{1+\\gamma^{2}})$-approximation gradient ascent\n\\citep{hassani2017gradient} for submodular maximization. Similarly, based on\n$F$, the classical Frank-Wolfe algorithm equipped with variance reduction\ntechnique \\citep{mokhtari2018conditional} also returns a solution with\nobjective value larger than $(1-e^{-\\gamma}-\\epsilon^{2})OPT-\\epsilon$ after\n$O(1/\\epsilon^{3})$ iterations. In the online setting, we first consider the\nadversarial delays for stochastic gradient feedback, under which we propose a\nboosting online gradient algorithm with the same non-oblivious search,\nachieving a regret of $\\sqrt{D}$ (where $D$ is the sum of delays of gradient\nfeedback) against a $(1-e^{-\\gamma})$-approximation to the best feasible\nsolution in hindsight. Finally, extensive numerical experiments demonstrate the\nefficiency of our boosting methods.",
          "link": "http://arxiv.org/abs/2201.00703",
          "publishedOn": "2022-01-05T00:39:37.061Z",
          "wordCount": null,
          "title": "Continuous Submodular Maximization: Boosting via Non-oblivious Function. (arXiv:2201.00703v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chavan_A/0/1/0/all/0/1\">Arnav Chavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric Xing</a>",
          "description": "This paper explores the feasibility of finding an optimal sub-model from a\nvision transformer and introduces a pure vision transformer slimming (ViT-Slim)\nframework that can search such a sub-structure from the original model\nend-to-end across multiple dimensions, including the input tokens, MHSA and MLP\nmodules with state-of-the-art performance. Our method is based on a learnable\nand unified l1 sparsity constraint with pre-defined factors to reflect the\nglobal importance in the continuous searching space of different dimensions.\nThe searching process is highly efficient through a single-shot training\nscheme. For instance, on DeiT-S, ViT-Slim only takes ~43 GPU hours for\nsearching process, and the searched structure is flexible with diverse\ndimensionalities in different modules. Then, a budget threshold is employed\naccording to the requirements of accuracy-FLOPs trade-off on running devices,\nand a re-training process is performed to obtain the final models. The\nextensive experiments show that our ViT-Slim can compress up to 40% of\nparameters and 40% FLOPs on various vision transformers while increasing the\naccuracy by ~0.6% on ImageNet. We also demonstrate the advantage of our\nsearched models on several downstream datasets. Our source code will be\npublicly available.",
          "link": "http://arxiv.org/abs/2201.00814",
          "publishedOn": "2022-01-05T00:39:37.057Z",
          "wordCount": null,
          "title": "Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space. (arXiv:2201.00814v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.02065",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xin Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1\">Risto Miikkulainen</a>",
          "description": "As neural network classifiers are deployed in real-world applications, it is\ncrucial that their failures can be detected reliably. One practical solution is\nto assign confidence scores to each prediction, then use these scores to filter\nout possible misclassifications. However, existing confidence metrics are not\nyet sufficiently reliable for this role. This paper presents a new framework\nthat produces a quantitative metric for detecting misclassification errors.\nThis framework, RED, builds an error detector on top of the base classifier and\nestimates uncertainty of the detection scores using Gaussian Processes.\nExperimental comparisons with other error detection methods on 125 UCI datasets\ndemonstrate that this approach is effective. Further implementations on two\nprobabilistic base classifiers and two large deep learning architecture in\nvision tasks further confirm that the method is robust and scalable. Third, an\nempirical analysis of RED with out-of-distribution and adversarial samples\nshows that the method can be used not only to detect errors but also to\nunderstand where they come from. RED can thereby be used to improve\ntrustworthiness of neural network classifiers more broadly in the future.",
          "link": "http://arxiv.org/abs/2010.02065",
          "publishedOn": "2022-01-05T00:39:37.056Z",
          "wordCount": null,
          "title": "Detecting Misclassification Errors in Neural Networks with a Gaussian Process Model. (arXiv:2010.02065v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05690",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_J/0/1/0/all/0/1\">Juntao Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Cheng_Y/0/1/0/all/0/1\">Yingda Cheng</a>, <a href=\"http://arxiv.org/find/math/1/au:+Christlieb_A/0/1/0/all/0/1\">Andrew J. Christlieb</a>, <a href=\"http://arxiv.org/find/math/1/au:+Roberts_L/0/1/0/all/0/1\">Luke F. Roberts</a>",
          "description": "In this paper, we take a data-driven approach and apply machine learning to\nthe moment closure problem for radiative transfer equation in slab geometry.\nInstead of learning the unclosed high order moment, we propose to directly\nlearn the gradient of the high order moment using neural networks. This new\napproach is consistent with the exact closure we derive for the free streaming\nlimit and also provides a natural output normalization. A variety of benchmark\ntests, including the variable scattering problem, the Gaussian source problem\nwith both periodic and reflecting boundaries, and the two-material problem,\nshow both good accuracy and generalizability of our machine learning closure\nmodel.",
          "link": "http://arxiv.org/abs/2105.05690",
          "publishedOn": "2022-01-05T00:39:37.056Z",
          "wordCount": null,
          "title": "Machine learning moment closure models for the radiative transfer equation I: directly learning a gradient based closure. (arXiv:2105.05690v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.10274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Ruocheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>",
          "description": "Online reviews enable consumers to engage with companies and provide\nimportant feedback. Due to the complexity of the high-dimensional text, these\nreviews are often simplified as a single numerical score, e.g., ratings or\nsentiment scores. This work empirically examines the causal effects of\nuser-generated online reviews on a granular level: we consider multiple\naspects, e.g., the Food and Service of a restaurant. Understanding consumers'\nopinions toward different aspects can help evaluate business performance in\ndetail and strategize business operations effectively. Specifically, we aim to\nanswer interventional questions such as What will the restaurant popularity be\nif the quality w.r.t. its aspect Service is increased by 10%? The defining\nchallenge of causal inference with observational data is the presence of\n\"confounder\", which might not be observed or measured, e.g., consumers'\npreference to food type, rendering the estimated effects biased and\nhigh-variance. To address this challenge, we have recourse to the multi-modal\nproxies such as the consumer profile information and interactions between\nconsumers and businesses. We show how to effectively leverage the rich\ninformation to identify and estimate causal effects of multiple aspects\nembedded in online reviews. Empirical evaluations on synthetic and real-world\ndata corroborate the efficacy and shed light on the actionable insight of the\nproposed approach.",
          "link": "http://arxiv.org/abs/2112.10274",
          "publishedOn": "2022-01-05T00:39:37.055Z",
          "wordCount": null,
          "title": "Estimating Causal Effects of Multi-Aspect Online Reviews with Multi-Modal Proxies. (arXiv:2112.10274v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.11660",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hunmin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhipeng Cai</a>",
          "description": "Deep neural networks have a wide range of applications in solving various\nreal-world tasks and have achieved satisfactory results, in domains such as\ncomputer vision, image classification, and natural language processing.\nMeanwhile, the security and robustness of neural networks have become\nimperative, as diverse researches have shown the vulnerable aspects of neural\nnetworks. Case in point, in Natural language processing tasks, the neural\nnetwork may be fooled by an attentively modified text, which has a high\nsimilarity to the original one. As per previous research, most of the studies\nare focused on the image domain; Different from image adversarial attacks, the\ntext is represented in a discrete sequence, traditional image attack methods\nare not applicable in the NLP field. In this paper, we propose a word-level NLP\nsentiment classifier attack model, which includes a self-attention\nmechanism-based word selection method and a greedy search algorithm for word\nsubstitution. We experiment with our attack model by attacking GRU and 1D-CNN\nvictim models on IMDB datasets. Experimental results demonstrate that our model\nachieves a higher attack success rate and more efficient than previous methods\ndue to the efficient word selection algorithms are employed and minimized the\nword substitute number. Also, our model is transferable, which can be used in\nthe image domain with several modifications.",
          "link": "http://arxiv.org/abs/2112.11660",
          "publishedOn": "2022-01-05T00:39:37.055Z",
          "wordCount": null,
          "title": "An Attention Score Based Attacker for Black-box NLP Classifier. (arXiv:2112.11660v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Foley_H/0/1/0/all/0/1\">Harrison Foley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowl_L/0/1/0/all/0/1\">Liam Fowl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1\">Gavin Taylor</a>",
          "description": "Data poisoning for reinforcement learning has historically focused on general\nperformance degradation, and targeted attacks have been successful via\nperturbations that involve control of the victim's policy and rewards. We\nintroduce an insidious poisoning attack for reinforcement learning which causes\nagent misbehavior only at specific target states - all while minimally\nmodifying a small fraction of training observations without assuming any\ncontrol over policy or reward. We accomplish this by adapting a recent\ntechnique, gradient alignment, to reinforcement learning. We test our method\nand demonstrate success in two Atari games of varying difficulty.",
          "link": "http://arxiv.org/abs/2201.00762",
          "publishedOn": "2022-01-05T00:39:37.051Z",
          "wordCount": null,
          "title": "Execute Order 66: Targeted Data Poisoning for Reinforcement Learning. (arXiv:2201.00762v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.02470",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_C/0/1/0/all/0/1\">Chi-Hua Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1\">Zhanyu Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sun_W/0/1/0/all/0/1\">Will Wei Sun</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cheng_G/0/1/0/all/0/1\">Guang Cheng</a>",
          "description": "Devising dynamic pricing policy with always valid online statistical learning\nprocedure is an important and as yet unresolved problem. Most existing dynamic\npricing policy, which focus on the faithfulness of adopted customer choice\nmodels, exhibit a limited capability for adapting the online uncertainty of\nlearned statistical model during pricing process. In this paper, we propose a\nnovel approach for designing dynamic pricing policy based regularized online\nstatistical learning with theoretical guarantees. The new approach overcomes\nthe challenge of continuous monitoring of online Lasso procedure and possesses\nseveral appealing properties. In particular, we make the decisive observation\nthat the always-validity of pricing decisions builds and thrives on the online\nregularization scheme. Our proposed online regularization scheme equips the\nproposed optimistic online regularized maximum likelihood pricing (OORMLP)\npricing policy with three major advantages: encode market noise knowledge into\npricing process optimism; empower online statistical learning with\nalways-validity over all decision points; envelop prediction error process with\ntime-uniform non-asymptotic oracle inequalities. This type of non-asymptotic\ninference results allows us to design more sample-efficient and robust dynamic\npricing algorithms in practice. In theory, the proposed OORMLP algorithm\nexploits the sparsity structure of high-dimensional models and secures a\nlogarithmic regret in a decision horizon. These theoretical advances are made\npossible by proposing an optimistic online Lasso procedure that resolves\ndynamic pricing problems at the process level, based on a novel use of\nnon-asymptotic martingale concentration. In experiments, we evaluate OORMLP in\ndifferent synthetic and real pricing problem settings, and demonstrate that\nOORMLP advances the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2007.02470",
          "publishedOn": "2022-01-05T00:39:37.050Z",
          "wordCount": null,
          "title": "Online Regularization towards Always-Valid High-Dimensional Dynamic Pricing. (arXiv:2007.02470v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Compagnoni_E/0/1/0/all/0/1\">Enea Monzio Compagnoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biggio_L/0/1/0/all/0/1\">Luca Biggio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orvieto_A/0/1/0/all/0/1\">Antonio Orvieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1\">Thomas Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teichmann_J/0/1/0/all/0/1\">Josef Teichmann</a>",
          "description": "Time series analysis is a widespread task in Natural Sciences, Social\nSciences, and Engineering. A fundamental problem is finding an expressive yet\nefficient-to-compute representation of the input time series to use as a\nstarting point to perform arbitrary downstream tasks. In this paper, we build\nupon recent works that use the Signature of a path as a feature map and\ninvestigate a computationally efficient technique to approximate these features\nbased on linear random projections. We present several theoretical results to\njustify our approach and empirically validate that our random projections can\neffectively retrieve the underlying Signature of a path. We show the surprising\nperformance of the proposed random features on several tasks, including (1)\nmapping the controls of stochastic differential equations to the corresponding\nsolutions and (2) using the Randomized Signatures as time series representation\nfor classification tasks. When compared to corresponding truncated Signature\napproaches, our Randomizes Signatures are more computationally efficient in\nhigh dimensions and often lead to better accuracy and faster training. Besides\nproviding a new tool to extract Signatures and further validating the high\nlevel of expressiveness of such features, we believe our results provide\ninteresting conceptual links between several existing research areas,\nsuggesting new intriguing directions for future investigations.",
          "link": "http://arxiv.org/abs/2201.00384",
          "publishedOn": "2022-01-05T00:39:37.042Z",
          "wordCount": null,
          "title": "Randomized Signature Layers for Signal Extraction in Time Series Data. (arXiv:2201.00384v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00668",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boffa_M/0/1/0/all/0/1\">Matteo Boffa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houidi_Z/0/1/0/all/0/1\">Zied Ben Houidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krolikowski_J/0/1/0/all/0/1\">Jonatan Krolikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_D/0/1/0/all/0/1\">Dario Rossi</a>",
          "description": "Recent years have witnessed the promise that reinforcement learning, coupled\nwith Graph Neural Network (GNN) architectures, could learn to solve hard\ncombinatorial optimization problems: given raw input data and an evaluator to\nguide the process, the idea is to automatically learn a policy able to return\nfeasible and high-quality outputs. Recent work have shown promising results but\nthe latter were mainly evaluated on the travelling salesman problem (TSP) and\nsimilar abstract variants such as Split Delivery Vehicle Routing Problem\n(SDVRP). In this paper, we analyze how and whether recent neural architectures\ncan be applied to graph problems of practical importance. We thus set out to\nsystematically \"transfer\" these architectures to the Power and Channel\nAllocation Problem (PCAP), which has practical relevance for, e.g., radio\nresource allocation in wireless networks. Our experimental results suggest that\nexisting architectures (i) are still incapable of capturing graph structural\nfeatures and (ii) are not suitable for problems where the actions on the graph\nchange the graph attributes. On a positive note, we show that augmenting the\nstructural representation of problems with Distance Encoding is a promising\nstep towards the still-ambitious goal of learning multi-purpose autonomous\nsolvers.",
          "link": "http://arxiv.org/abs/2201.00668",
          "publishedOn": "2022-01-05T00:39:37.042Z",
          "wordCount": null,
          "title": "Neural combinatorial optimization beyond the TSP: Existing architectures under-represent graph structure. (arXiv:2201.00668v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1\">Lawrence Staib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1\">James S. Duncan</a>",
          "description": "Automated segmentation in medical image analysis is a challenging task that\nrequires a large amount of manually labeled data. However, manually annotating\nmedical data is often laborious, and most existing learning-based approaches\nfail to accurately delineate object boundaries without effective geometric\nconstraints. Contrastive learning, a sub-area of self-supervised learning, has\nrecently been noted as a promising direction in multiple application fields. In\nthis work, we present a novel Contrastive Voxel-wise Representation\nDistillation (CVRD) method with geometric constraints to learn global-local\nvisual representations for volumetric medical image segmentation with limited\nannotations. Our framework can effectively learn global and local features by\ncapturing 3D spatial context and rich anatomical information. Specifically, we\nintroduce a voxel-to-volume contrastive algorithm to learn global information\nfrom 3D images, and propose to perform local voxel-to-voxel distillation to\nexplicitly make use of local cues in the embedding space. Moreover, we\nintegrate an elastic interaction-based active contour model as a geometric\nregularization term to enable fast and reliable object delineations in an\nend-to-end learning manner. Results on the Atrial Segmentation Challenge\ndataset demonstrate superiority of our proposed scheme, especially in a setting\nwith a very limited number of annotated data. The code will be available at\nhttps://github.com/charlesyou999648/CVRD.",
          "link": "http://arxiv.org/abs/2105.07059",
          "publishedOn": "2022-01-05T00:39:37.038Z",
          "wordCount": null,
          "title": "Momentum Contrastive Voxel-wise Representation Learning for Semi-supervised Volumetric Medical Image Segmentation. (arXiv:2105.07059v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.14189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Songwei Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shlok Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1\">David Jacobs</a>",
          "description": "Unsupervised learning has recently made exceptional progress because of the\ndevelopment of more effective contrastive learning methods. However, CNNs are\nprone to depend on low-level features that humans deem non-semantic. This\ndependency has been conjectured to induce a lack of robustness to image\nperturbations or domain shift. In this paper, we show that by generating\ncarefully designed negative samples, contrastive learning can learn more robust\nrepresentations with less dependence on such features. Contrastive learning\nutilizes positive pairs that preserve semantic information while perturbing\nsuperficial features in the training images. Similarly, we propose to generate\nnegative samples in a reversed way, where only the superfluous instead of the\nsemantic features are preserved. We develop two methods, texture-based and\npatch-based augmentations, to generate negative samples. These samples achieve\nbetter generalization, especially under out-of-domain settings. We also analyze\nour method and the generated texture-based samples, showing that texture\nfeatures are indispensable in classifying particular ImageNet classes and\nespecially finer classes. We also show that model bias favors texture and shape\nfeatures differently under different test settings. Our code, trained models,\nand ImageNet-Texture dataset can be found at\nhttps://github.com/SongweiGe/Contrastive-Learning-with-Non-Semantic-Negatives.",
          "link": "http://arxiv.org/abs/2110.14189",
          "publishedOn": "2022-01-05T00:39:37.035Z",
          "wordCount": null,
          "title": "Robust Contrastive Learning Using Negative Samples with Diminished Semantics. (arXiv:2110.14189v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blake_C/0/1/0/all/0/1\">Charlie Blake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurin_V/0/1/0/all/0/1\">Vitaly Kurin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igl_M/0/1/0/all/0/1\">Maximilian Igl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1\">Shimon Whiteson</a>",
          "description": "Recent research has shown that graph neural networks (GNNs) can learn\npolicies for locomotion control that are as effective as a typical multi-layer\nperceptron (MLP), with superior transfer and multi-task performance (Wang et\nal., 2018; Huang et al., 2020). Results have so far been limited to training on\nsmall agents, with the performance of GNNs deteriorating rapidly as the number\nof sensors and actuators grows. A key motivation for the use of GNNs in the\nsupervised learning setting is their applicability to large graphs, but this\nbenefit has not yet been realised for locomotion control. We identify the\nweakness with a common GNN architecture that causes this poor scaling:\noverfitting in the MLPs within the network that encode, decode, and propagate\nmessages. To combat this, we introduce Snowflake, a GNN training method for\nhigh-dimensional continuous control that freezes parameters in parts of the\nnetwork that suffer from overfitting. Snowflake significantly boosts the\nperformance of GNNs for locomotion control on large agents, now matching the\nperformance of MLPs, and with superior transfer properties.",
          "link": "http://arxiv.org/abs/2103.01009",
          "publishedOn": "2022-01-05T00:39:37.012Z",
          "wordCount": null,
          "title": "Snowflake: Scaling GNNs to High-Dimensional Continuous Control via Parameter Freezing. (arXiv:2103.01009v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Siyang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harding_S/0/1/0/all/0/1\">Seth Austin Harding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haibin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shih-wei Liao</a>",
          "description": "Many complex multi-agent systems such as robot swarms control and autonomous\nvehicle coordination can be modeled as Multi-Agent Reinforcement Learning\n(MARL) tasks. QMIX, a widely popular MARL algorithm, has been used as a\nbaseline for the benchmark environments, e.g., Starcraft Multi-Agent Challenge\n(SMAC), Difficulty-Enhanced Predator-Prey (DEPP). Recent variants of QMIX\ntarget relaxing the monotonicity constraint of QMIX, allowing for performance\nimprovement in SMAC. In this paper, we investigate the code-level optimizations\nof these variants and the monotonicity constraint. (1) We find that such\nimprovements of the variants are significantly affected by various code-level\noptimizations. (2) The experiment results show that QMIX with normalized\noptimizations outperforms other works in SMAC; (3) beyond the common wisdom\nfrom these works, the monotonicity constraint can improve sample efficiency in\nSMAC and DEPP. We also discuss why monotonicity constraints work well in purely\ncooperative tasks with a theoretical analysis. We open-source the code at\n\\url{https://github.com/hijkzzz/pymarl2}.",
          "link": "http://arxiv.org/abs/2102.03479",
          "publishedOn": "2022-01-05T00:39:37.011Z",
          "wordCount": null,
          "title": "Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2102.03479v18 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tavares_B/0/1/0/all/0/1\">B&#xe1;rbara Tavares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_C/0/1/0/all/0/1\">Cl&#xe1;udia Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marques_M/0/1/0/all/0/1\">Manuel Marques</a>",
          "description": "Bike Sharing Systems (BSSs) are emerging as an innovative transportation\nservice. Ensuring the proper functioning of a BSS is crucial given that these\nsystems are committed to eradicating many of the current global concerns, by\npromoting environmental and economic sustainability and contributing to\nimproving the life quality of the population. Good knowledge of users'\ntransition patterns is a decisive contribution to the quality and operability\nof the service. The analogous and unbalanced users' transition patterns cause\nthese systems to suffer from bicycle imbalance, leading to a drastic customer\nloss in the long term. Strategies for bicycle rebalancing become important to\ntackle this problem and for this, bicycle traffic prediction is essential, as\nit allows to operate more efficiently and to react in advance. In this work, we\npropose a bicycle trips predictor based on Graph Neural Network embeddings,\ntaking into consideration station groupings, meteorology conditions,\ngeographical distances, and trip patterns. We evaluated our approach in the New\nYork City BSS (CitiBike) data and compared it with four baselines, including\nthe non-clustered approach. To address our problem's specificities, we\ndeveloped the Adaptive Transition Constraint Clustering Plus (AdaTC+)\nalgorithm, eliminating shortcomings of previous work. Our experiments evidence\nthe clustering pertinence (88% accuracy compared with 83% without clustering)\nand which clustering technique best suits this problem. Accuracy on the Link\nPrediction task is always higher for AdaTC+ than benchmark clustering methods\nwhen the stations are the same, while not degrading performance when the\nnetwork is upgraded, in a mismatch with the trained model.",
          "link": "http://arxiv.org/abs/2201.00720",
          "publishedOn": "2022-01-05T00:39:37.010Z",
          "wordCount": null,
          "title": "A Cluster-Based Trip Prediction Graph Neural Network Model for Bike Sharing Systems. (arXiv:2201.00720v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yunpu Zhao</a>",
          "description": "Automating machine learning has achieved remarkable technological\ndevelopments in recent years, and building an automated machine learning\npipeline is now an essential task. The model ensemble is the technique of\ncombining multiple models to get a better and more robust model. However,\nexisting automated machine learning tends to be simplistic in handling the\nmodel ensemble, where the ensemble strategy is fixed, such as stacked\ngeneralization. There have been many techniques on different ensemble methods,\nespecially ensemble selection, and the fixed ensemble strategy limits the upper\nlimit of the model's performance. In this article, we present a novel framework\nfor automated machine learning. Our framework incorporates advances in dynamic\nensemble selection, and to our best knowledge, our approach is the first in the\nfield of AutoML to search and optimize ensemble strategies. In the comparison\nexperiments, our method outperforms the state-of-the-art automated machine\nlearning frameworks with the same CPU time in 42 classification datasets from\nthe OpenML platform. Ablation experiments on our framework validate the\neffectiveness of our proposed method.",
          "link": "http://arxiv.org/abs/2201.00207",
          "publishedOn": "2022-01-05T00:39:37.005Z",
          "wordCount": null,
          "title": "AutoDES: AutoML Pipeline Generation of Classification with Dynamic Ensemble Strategy Selection. (arXiv:2201.00207v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2003.13917",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_J/0/1/0/all/0/1\">Jun Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_X/0/1/0/all/0/1\">Xiaoli Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1\">Chin-Hui Lee</a>",
          "description": "Recent studies have highlighted adversarial examples as ubiquitous threats to\nthe deep neural network (DNN) based speech recognition systems. In this work,\nwe present a U-Net based attention model, U-Net$_{At}$, to enhance adversarial\nspeech signals. Specifically, we evaluate the model performance by\ninterpretable speech recognition metrics and discuss the model performance by\nthe augmented adversarial training. Our experiments show that our proposed\nU-Net$_{At}$ improves the perceptual evaluation of speech quality (PESQ) from\n1.13 to 2.78, speech transmission index (STI) from 0.65 to 0.75, short-term\nobjective intelligibility (STOI) from 0.83 to 0.96 on the task of speech\nenhancement with adversarial speech examples. We conduct experiments on the\nautomatic speech recognition (ASR) task with adversarial audio attacks. We find\nthat (i) temporal features learned by the attention network are capable of\nenhancing the robustness of DNN based ASR models; (ii) the generalization power\nof DNN based ASR model could be enhanced by applying adversarial training with\nan additive adversarial data augmentation. The ASR metric on word-error-rates\n(WERs) shows that there is an absolute 2.22 $\\%$ decrease under gradient-based\nperturbation, and an absolute 2.03 $\\%$ decrease, under evolutionary-optimized\nperturbation, which suggests that our enhancement models with adversarial\ntraining can further secure a resilient ASR system.",
          "link": "http://arxiv.org/abs/2003.13917",
          "publishedOn": "2022-01-05T00:39:37.005Z",
          "wordCount": null,
          "title": "Characterizing Speech Adversarial Examples Using Self-Attention U-Net Enhancement. (arXiv:2003.13917v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1\">Ibrahim Alabdulmohsin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1\">Mario Lucic</a>",
          "description": "We present a scalable post-processing algorithm for debiasing trained models,\nincluding deep neural networks (DNNs), which we prove to be near-optimal by\nbounding its excess Bayes risk. We empirically validate its advantages on\nstandard benchmark datasets across both classical algorithms as well as modern\nDNN architectures and demonstrate that it outperforms previous post-processing\nmethods while performing on par with in-processing. In addition, we show that\nthe proposed algorithm is particularly effective for models trained at scale\nwhere post-processing is a natural and practical choice.",
          "link": "http://arxiv.org/abs/2106.12887",
          "publishedOn": "2022-01-05T00:39:37.004Z",
          "wordCount": null,
          "title": "A Near-Optimal Algorithm for Debiasing Trained Machine Learning Models. (arXiv:2106.12887v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.14602",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dianzhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhrin_O/0/1/0/all/0/1\">Ostap Okhrin</a>",
          "description": "In the autonomous driving field, the fusion of human knowledge into Deep\nReinforcement Learning (DRL) is often based on the human demonstration recorded\nin the simulated environment. This limits the generalization and the\nfeasibility of application in real-world traffic. We proposed a two-stage DRL\nmethod, that learns from real-world human driving to achieve performance that\nis superior to the pure DRL agent. Training a DRL agent is done within a\nframework for CARLA with Robot Operating System (ROS). For evaluation, we\ndesigned different real-world driving scenarios to compare the proposed\ntwo-stage DRL agent with the pure DRL agent. After extracting the 'good'\nbehavior from the human driver, such as anticipation in a signalized\nintersection, the agent becomes more efficient and drives safer, which makes\nthis autonomous agent more adapt to Human-Robot Interaction (HRI) traffic.",
          "link": "http://arxiv.org/abs/2112.14602",
          "publishedOn": "2022-01-05T00:39:37.003Z",
          "wordCount": null,
          "title": "DDPG car-following model with real-world human driving experience in CARLA. (arXiv:2112.14602v1 [cs.RO] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stock_P/0/1/0/all/0/1\">Pierre Stock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gribonval_R/0/1/0/all/0/1\">R&#xe9;mi Gribonval</a>",
          "description": "Neural networks with the Rectified Linear Unit (ReLU) nonlinearity are\ndescribed by a vector of parameters $\\theta$, and realized as a piecewise\nlinear continuous function $R_{\\theta}: x \\in \\mathbb R^{d} \\mapsto\nR_{\\theta}(x) \\in \\mathbb R^{k}$. Natural scalings and permutations operations\non the parameters $\\theta$ leave the realization unchanged, leading to\nequivalence classes of parameters that yield the same realization. These\nconsiderations in turn lead to the notion of identifiability -- the ability to\nrecover (the equivalence class of) $\\theta$ from the sole knowledge of its\nrealization $R_{\\theta}$. The overall objective of this paper is to introduce\nan embedding for ReLU neural networks of any depth, $\\Phi(\\theta)$, that is\ninvariant to scalings and that provides a locally linear parameterization of\nthe realization of the network. Leveraging these two key properties, we derive\nsome conditions under which a deep ReLU network is indeed locally identifiable\nfrom the knowledge of the realization on a finite set of samples $x_{i} \\in\n\\mathbb R^{d}$. We study the shallow case in more depth, establishing necessary\nand sufficient conditions for the network to be identifiable from a bounded\nsubset $\\mathcal X \\subseteq \\mathbb R^{d}$.",
          "link": "http://arxiv.org/abs/2107.09370",
          "publishedOn": "2022-01-05T00:39:37.000Z",
          "wordCount": null,
          "title": "An Embedding of ReLU Networks and an Analysis of their Identifiability. (arXiv:2107.09370v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+LeJeune_D/0/1/0/all/0/1\">Daniel LeJeune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javadi_H/0/1/0/all/0/1\">Hamid Javadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard G. Baraniuk</a>",
          "description": "Among the most successful methods for sparsifying deep (neural) networks are\nthose that adaptively mask the network weights throughout training. By\nexamining this masking, or dropout, in the linear case, we uncover a duality\nbetween such adaptive methods and regularization through the so-called\n\"$\\eta$-trick\" that casts both as iteratively reweighted optimizations. We show\nthat any dropout strategy that adapts to the weights in a monotonic way\ncorresponds to an effective subquadratic regularization penalty, and therefore\nleads to sparse solutions. We obtain the effective penalties for several\npopular sparsification strategies, which are remarkably similar to classical\npenalties commonly used in sparse optimization. Considering variational dropout\nas a case study, we demonstrate similar empirical behavior between the adaptive\ndropout method and classical methods on the task of deep network\nsparsification, validating our theory.",
          "link": "http://arxiv.org/abs/2106.07769",
          "publishedOn": "2022-01-05T00:39:36.997Z",
          "wordCount": null,
          "title": "The Flip Side of the Reweighted Coin: Duality of Adaptive Dropout and Regularization. (arXiv:2106.07769v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rashid_N/0/1/0/all/0/1\">Nafiul Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demirel_B/0/1/0/all/0/1\">Berken Utku Demirel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1\">Mohammad Abdullah Al Faruque</a>",
          "description": "Human Activity Recognition (HAR) is one of the key applications of health\nmonitoring that requires continuous use of wearable devices to track daily\nactivities. This paper proposes an Adaptive CNN for energy-efficient HAR (AHAR)\nsuitable for low-power edge devices. Unlike traditional early exit architecture\nthat makes the exit decision based on classification confidence, AHAR proposes\na novel adaptive architecture that uses an output block predictor to select a\nportion of the baseline architecture to use during the inference phase.\nExperimental results show that traditional early exit architectures suffer from\nperformance loss whereas our adaptive architecture provides similar or better\nperformance as the baseline one while being energy-efficient. We validate our\nmethodology in classifying locomotion activities from two datasets- Opportunity\nand w-HAR. Compared to the fog/cloud computing approaches for the Opportunity\ndataset, our baseline and adaptive architecture shows a comparable weighted F1\nscore of 91.79%, and 91.57%, respectively. For the w-HAR dataset, our baseline\nand adaptive architecture outperforms the state-of-the-art works with a\nweighted F1 score of 97.55%, and 97.64%, respectively. Evaluation on real\nhardware shows that our baseline architecture is significantly energy-efficient\n(422.38x less) and memory-efficient (14.29x less) compared to the works on the\nOpportunity dataset. For the w-HAR dataset, our baseline architecture requires\n2.04x less energy and 2.18x less memory compared to the state-of-the-art work.\nMoreover, experimental results show that our adaptive architecture is 12.32%\n(Opportunity) and 11.14% (w-HAR) energy-efficient than our baseline while\nproviding similar (Opportunity) or better (w-HAR) performance with no\nsignificant memory overhead.",
          "link": "http://arxiv.org/abs/2102.01875",
          "publishedOn": "2022-01-05T00:39:36.995Z",
          "wordCount": null,
          "title": "AHAR: Adaptive CNN for Energy-efficient Human Activity Recognition in Low-power Edge Devices. (arXiv:2102.01875v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rieger_P/0/1/0/all/0/1\">Phillip Rieger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thien Duc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miettinen_M/0/1/0/all/0/1\">Markus Miettinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeghi_A/0/1/0/all/0/1\">Ahmad-Reza Sadeghi</a>",
          "description": "Federated Learning (FL) allows multiple clients to collaboratively train a\nNeural Network (NN) model on their private data without revealing the data.\nRecently, several targeted poisoning attacks against FL have been introduced.\nThese attacks inject a backdoor into the resulting model that allows\nadversary-controlled inputs to be misclassified. Existing countermeasures\nagainst backdoor attacks are inefficient and often merely aim to exclude\ndeviating models from the aggregation. However, this approach also removes\nbenign models of clients with deviating data distributions, causing the\naggregated model to perform poorly for such clients.\n\nTo address this problem, we propose DeepSight, a novel model filtering\napproach for mitigating backdoor attacks. It is based on three novel techniques\nthat allow to characterize the distribution of data used to train model updates\nand seek to measure fine-grained differences in the internal structure and\noutputs of NNs. Using these techniques, DeepSight can identify suspicious model\nupdates. We also develop a scheme that can accurately cluster model updates.\nCombining the results of both components, DeepSight is able to identify and\neliminate model clusters containing poisoned models with high attack impact. We\nalso show that the backdoor contributions of possibly undetected poisoned\nmodels can be effectively mitigated with existing weight clipping-based\ndefenses. We evaluate the performance and effectiveness of DeepSight and show\nthat it can mitigate state-of-the-art backdoor attacks with a negligible impact\non the model's performance on benign data.",
          "link": "http://arxiv.org/abs/2201.00763",
          "publishedOn": "2022-01-05T00:39:36.993Z",
          "wordCount": null,
          "title": "DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection. (arXiv:2201.00763v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00766",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boschini_M/0/1/0/all/0/1\">Matteo Boschini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonicelli_L/0/1/0/all/0/1\">Lorenzo Bonicelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buzzega_P/0/1/0/all/0/1\">Pietro Buzzega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porrello_A/0/1/0/all/0/1\">Angelo Porrello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calderara_S/0/1/0/all/0/1\">Simone Calderara</a>",
          "description": "The staple of human intelligence is the capability of acquiring knowledge in\na continuous fashion. In stark contrast, Deep Networks forget catastrophically\nand, for this reason, the sub-field of Class-Incremental Continual Learning\nfosters methods that learn a sequence of tasks incrementally, blending\nsequentially-gained knowledge into a comprehensive prediction.\n\nThis work aims at assessing and overcoming the pitfalls of our previous\nproposal Dark Experience Replay (DER), a simple and effective approach that\ncombines rehearsal and Knowledge Distillation. Inspired by the way our minds\nconstantly rewrite past recollections and set expectations for the future, we\nendow our model with the abilities to i) revise its replay memory to welcome\nnovel information regarding past data ii) pave the way for learning yet unseen\nclasses.\n\nWe show that the application of these strategies leads to remarkable\nimprovements; indeed, the resulting method - termed eXtended-DER (X-DER) -\noutperforms the state of the art on both standard benchmarks (such as CIFAR-100\nand miniImagenet) and a novel one here introduced. To gain a better\nunderstanding, we further provide extensive ablation studies that corroborate\nand extend the findings of our previous research (e.g. the value of Knowledge\nDistillation and flatter minima in continual learning setups).",
          "link": "http://arxiv.org/abs/2201.00766",
          "publishedOn": "2022-01-05T00:39:36.990Z",
          "wordCount": null,
          "title": "Class-Incremental Continual Learning into the eXtended DER-verse. (arXiv:2201.00766v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.01854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Taghibakhshi_A/0/1/0/all/0/1\">Ali Taghibakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacLachlan_S/0/1/0/all/0/1\">Scott MacLachlan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olson_L/0/1/0/all/0/1\">Luke Olson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_M/0/1/0/all/0/1\">Matthew West</a>",
          "description": "Large sparse linear systems of equations are ubiquitous in science and\nengineering, such as those arising from discretizations of partial differential\nequations. Algebraic multigrid (AMG) methods are one of the most common methods\nof solving such linear systems, with an extensive body of underlying\nmathematical theory. A system of linear equations defines a graph on the set of\nunknowns and each level of a multigrid solver requires the selection of an\nappropriate coarse graph along with restriction and interpolation operators\nthat map to and from the coarse representation. The efficiency of the multigrid\nsolver depends critically on this selection and many selection methods have\nbeen developed over the years. Recently, it has been demonstrated that it is\npossible to directly learn the AMG interpolation and restriction operators,\ngiven a coarse graph selection. In this paper, we consider the complementary\nproblem of learning to coarsen graphs for a multigrid solver, a necessary step\nin developing fully learnable AMG methods. We propose a method using a\nreinforcement learning (RL) agent based on graph neural networks (GNNs), which\ncan learn to perform graph coarsening on small planar training graphs and then\nbe applied to unstructured large planar graphs, assuming bounded node degree.\nWe demonstrate that this method can produce better coarse graphs than existing\nalgorithms, even as the graph size increases and other properties of the graph\nare varied. We also propose an efficient inference procedure for performing\ngraph coarsening that results in linear time complexity in graph size.",
          "link": "http://arxiv.org/abs/2106.01854",
          "publishedOn": "2022-01-05T00:39:36.990Z",
          "wordCount": null,
          "title": "Optimization-Based Algebraic Multigrid Coarsening Using Reinforcement Learning. (arXiv:2106.01854v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jagannath_A/0/1/0/all/0/1\">Anu Jagannath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagannath_J/0/1/0/all/0/1\">Jithin Jagannath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Prem Sagar Pattanshetty Vasanth Kumar</a>",
          "description": "Fifth generation (5G) networks and beyond envisions massive Internet of\nThings (IoT) rollout to support disruptive applications such as extended\nreality (XR), augmented/virtual reality (AR/VR), industrial automation,\nautonomous driving, and smart everything which brings together massive and\ndiverse IoT devices occupying the radio frequency (RF) spectrum. Along with\nspectrum crunch and throughput challenges, such a massive scale of wireless\ndevices exposes unprecedented threat surfaces. RF fingerprinting is heralded as\na candidate technology that can be combined with cryptographic and zero-trust\nsecurity measures to ensure data privacy, confidentiality, and integrity in\nwireless networks. Motivated by the relevance of this subject in the future\ncommunication networks, in this work, we present a comprehensive survey of RF\nfingerprinting approaches ranging from a traditional view to the most recent\ndeep learning (DL) based algorithms. Existing surveys have mostly focused on a\nconstrained presentation of the wireless fingerprinting approaches, however,\nmany aspects remain untold. In this work, however, we mitigate this by\naddressing every aspect - background on signal intelligence (SIGINT),\napplications, relevant DL algorithms, systematic literature review of RF\nfingerprinting techniques spanning the past two decades, discussion on\ndatasets, and potential research avenues - necessary to elucidate this topic to\nthe reader in an encyclopedic manner.",
          "link": "http://arxiv.org/abs/2201.00680",
          "publishedOn": "2022-01-05T00:39:36.989Z",
          "wordCount": null,
          "title": "A Comprehensive Survey on Radio Frequency (RF) Fingerprinting: Traditional Approaches, Deep Learning, and Open Challenges. (arXiv:2201.00680v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00690",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Albanese_F/0/1/0/all/0/1\">Federico Albanese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feuerstein_E/0/1/0/all/0/1\">Esteban Feuerstein</a>",
          "description": "Social networks play a fundamental role in propagation of information and\nnews. Characterizing the content of the messages becomes vital for different\ntasks, like breaking news detection, personalized message recommendation, fake\nusers detection, information flow characterization and others. However, Twitter\nposts are short and often less coherent than other text documents, which makes\nit challenging to apply text mining algorithms to these datasets efficiently.\nTweet-pooling (aggregating tweets into longer documents) has been shown to\nimprove automatic topic decomposition, but the performance achieved in this\ntask varies depending on the pooling method.\n\nIn this paper, we propose a new pooling scheme for topic modeling in Twitter,\nwhich groups tweets whose authors belong to the same community (group of users\nwho mainly interact with each other but not with other groups) on a user\ninteraction graph. We present a complete evaluation of this methodology, state\nof the art schemes and previous pooling models in terms of the cluster quality,\ndocument retrieval tasks performance and supervised machine learning\nclassification score. Results show that our Community polling method\noutperformed other methods on the majority of metrics in two heterogeneous\ndatasets, while also reducing the running time. This is useful when dealing\nwith big amounts of noisy and short user-generated social media texts. Overall,\nour findings contribute to an improved methodology for identifying the latent\ntopics in a Twitter dataset, without the need of modifying the basic machinery\nof a topic decomposition model.",
          "link": "http://arxiv.org/abs/2201.00690",
          "publishedOn": "2022-01-05T00:39:36.986Z",
          "wordCount": null,
          "title": "Improved Topic modeling in Twitter through Community Pooling. (arXiv:2201.00690v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04261",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Deperrois_N/0/1/0/all/0/1\">Nicolas Deperrois</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Petrovici_M/0/1/0/all/0/1\">Mihai A. Petrovici</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Senn_W/0/1/0/all/0/1\">Walter Senn</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jordan_J/0/1/0/all/0/1\">Jakob Jordan</a>",
          "description": "Humans and other animals learn to extract general concepts from sensory\nexperience without extensive teaching. This ability is thought to be\nfacilitated by offline states like sleep where previous experiences are\nsystemically replayed. However, the characteristic creative nature of dreams\nsuggests that learning semantic representations may go beyond merely replaying\nprevious experiences. We support this hypothesis by implementing a cortical\narchitecture inspired by generative adversarial networks (GANs). Learning in\nour model is organized across three different global brain states mimicking\nwakefulness, NREM and REM sleep, optimizing different, but complementary\nobjective functions. We train the model on standard datasets of natural images\nand evaluate the quality of the learned representations. Our results suggest\nthat generating new, virtual sensory inputs via adversarial dreaming during REM\nsleep is essential for extracting semantic concepts, while replaying episodic\nmemories via perturbed dreaming during NREM sleep improves the robustness of\nlatent representations. The model provides a new computational perspective on\nsleep states, memory replay and dreams and suggests a cortical implementation\nof GANs.",
          "link": "http://arxiv.org/abs/2109.04261",
          "publishedOn": "2022-01-05T00:39:36.985Z",
          "wordCount": null,
          "title": "Learning cortical representations through perturbed and adversarial dreaming. (arXiv:2109.04261v2 [q-bio.NC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06976",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moghadam_M/0/1/0/all/0/1\">Monireh Mohebbi Moghadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boroomand_B/0/1/0/all/0/1\">Bahar Boroomand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalali_M/0/1/0/all/0/1\">Mohammad Jalali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zareian_A/0/1/0/all/0/1\">Arman Zareian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DaeiJavad_A/0/1/0/all/0/1\">Alireza DaeiJavad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manshaei_M/0/1/0/all/0/1\">Mohammad Hossein Manshaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krunz_M/0/1/0/all/0/1\">Marwan Krunz</a>",
          "description": "Generative Adversarial Networks (GANs) have recently attracted considerable\nattention in the AI community due to its ability to generate high-quality data\nof significant statistical resemblance to real data. Fundamentally, GAN is a\ngame between two neural networks trained in an adversarial manner to reach a\nzero-sum Nash equilibrium profile. Despite the improvement accomplished in GANs\nin the last few years, several issues remain to be solved. This paper reviews\nthe literature on the game theoretic aspects of GANs and addresses how game\ntheory models can address specific challenges of generative model and improve\nthe GAN's performance. We first present some preliminaries, including the basic\nGAN model and some game theory background. We then present taxonomy to classify\nstate-of-the-art solutions into three main categories: modified game models,\nmodified architectures, and modified learning methods. The classification is\nbased on modifications made to the basic GAN model by proposed game-theoretic\napproaches in the literature. We then explore the objectives of each category\nand discuss recent works in each category. Finally, we discuss the remaining\nchallenges in this field and present future research directions.",
          "link": "http://arxiv.org/abs/2106.06976",
          "publishedOn": "2022-01-05T00:39:36.981Z",
          "wordCount": null,
          "title": "Game of GANs: Game-Theoretical Models for Generative Adversarial Networks. (arXiv:2106.06976v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malvar_S/0/1/0/all/0/1\">Sara Malvar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meneghini_J/0/1/0/all/0/1\">Julio Romano Meneghini</a>",
          "description": "At the end of 2019, the latest novel coronavirus Sars-CoV-2 emerged as a\nsignificant acute respiratory disease that has become a global pandemic.\nCountries like Brazil have had difficulty in dealing with the virus due to the\nhigh socioeconomic difference of states and municipalities. Therefore, this\nstudy presents a new approach using different machine learning and deep\nlearning algorithms applied to Brazilian COVID-19 data. First, a clustering\nalgorithm is used to identify counties with similar sociodemographic behavior,\nwhile Benford's law is used to check for data manipulation. Based on these\nresults we are able to correctly model SARIMA models based on the clusters to\npredict new daily cases. The unsupervised machine learning techniques optimized\nthe process of defining the parameters of the SARIMA model. This framework can\nalso be useful to propose confinement scenarios during the so-called second\nwave. We have used the 645 counties from S\\~ao Paulo state, the most populous\nstate in Brazil. However, this methodology can be used in other states or\ncountries. This paper demonstrates how different techniques of machine\nlearning, deep learning, data mining and statistics can be used together to\nproduce important results when dealing with pandemic data. Although the\nfindings cannot be used exclusively to assess and influence policy decisions,\nthey offer an alternative to the ineffective measures that have been used.",
          "link": "http://arxiv.org/abs/2201.00715",
          "publishedOn": "2022-01-05T00:39:36.975Z",
          "wordCount": null,
          "title": "Machine learning approaches for localized lockdown during COVID-19: a case study analysis. (arXiv:2201.00715v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.12020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Duo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fekri_F/0/1/0/all/0/1\">Faramarz Fekri</a>",
          "description": "The actor-critic RL is widely used in various robotic control tasks. By\nviewing the actor-critic RL from the perspective of variational inference (VI),\nthe policy network is trained to obtain the approximate posterior of actions\ngiven the optimality criteria. However, in practice, the actor-critic RL may\nyield suboptimal policy estimates due to the amortization gap and insufficient\nexploration. In this work, inspired by the previous use of Hamiltonian Monte\nCarlo (HMC) in VI, we propose to integrate the policy network of actor-critic\nRL with HMC, which is termed as {\\it Hamiltonian Policy}. As such we propose to\nevolve actions from the base policy according to HMC, and our proposed method\nhas many benefits. First, HMC can improve the policy distribution to better\napproximate the posterior and hence reduce the amortization gap. Second, HMC\ncan also guide the exploration more to the regions of action spaces with higher\nQ values, enhancing the exploration efficiency. Further, instead of directly\napplying HMC into RL, we propose a new leapfrog operator to simulate the\nHamiltonian dynamics. Finally, in safe RL problems, we find that the proposed\nmethod can not only improve the achieved return, but also reduce safety\nconstraint violations by discarding potentially unsafe actions. With\ncomprehensive empirical experiments on continuous control baselines, including\nMuJoCo and PyBullet Roboschool, we show that the proposed approach is a\ndata-efficient and easy-to-implement improvement over previous actor-critic\nmethods.",
          "link": "http://arxiv.org/abs/2103.12020",
          "publishedOn": "2022-01-05T00:39:36.972Z",
          "wordCount": null,
          "title": "Improving Actor-Critic Reinforcement Learning via Hamiltonian Monte Carlo Method. (arXiv:2103.12020v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shuyin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xinyu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zizhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giem_E/0/1/0/all/0/1\">Elisabeth Giem</a>",
          "description": "This paper present a strong data mining method based on rough set, which can\nrealize feature selection, classification and knowledge representation at the\nsame time. Rough set has good interpretability, and is a popular method for\nfeature selections. But low efficiency and low accuracy are its main drawbacks\nthat limits its application ability. In this paper,corresponding to the\naccuracy, we first find the ineffectiveness of rough set because of\noverfitting, especially in processing noise attribute, and propose a robust\nmeasurement for an attribute, called relative importance.we proposed the\nconcept of \"rough concept tree\" for knowledge representation and\nclassification. Experimental results on public benchmark data sets show that\nthe proposed framework achieves higher accurcy than seven popular or the\nstate-of-the-art feature selection methods.",
          "link": "http://arxiv.org/abs/2201.00436",
          "publishedOn": "2022-01-05T00:39:36.950Z",
          "wordCount": null,
          "title": "An Efficient and Accurate Rough Set for Feature Selection, Classification and Knowledge Representation. (arXiv:2201.00436v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.01750",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shaojie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jing Yuan</a>",
          "description": "Running machine learning algorithms on large and rapidly growing volumes of\ndata is often computationally expensive, one common trick to reduce the size of\na data set, and thus reduce the computational cost of machine learning\nalgorithms, is \\emph{probability sampling}. It creates a sampled data set by\nincluding each data point from the original data set with a known probability.\nAlthough the benefit of running machine learning algorithms on the reduced data\nset is obvious, one major concern is that the performance of the solution\nobtained from samples might be much worse than that of the optimal solution\nwhen using the full data set. In this paper, we examine the performance loss\ncaused by probability sampling in the context of adaptive submodular\nmaximization. We consider a simple probability sampling method which selects\neach data point with probability $r\\in[0,1]$. If we set the sampling rate\n$r=1$, our problem reduces to finding a solution based on the original full\ndata set. We define sampling gap as the largest ratio between the optimal\nsolution obtained from the full data set and the optimal solution obtained from\nthe samples, over independence systems. %It captures the performance loss of\nthe optimal solution caused by the probability sampling. Our main contribution\nis to show that if the utility function is policywise submodular, then for a\ngiven sampling rate $r$, the sampling gap is both upper bounded and lower\nbounded by $1/r$. One immediate implication of our result is that if we can\nfind an $\\alpha$-approximation solution based on a sampled data set (which is\nsampled at sampling rate $r$), then this solution achieves an $\\alpha r$\napproximation ratio against the optimal solution when using the full data set.",
          "link": "http://arxiv.org/abs/2104.01750",
          "publishedOn": "2022-01-05T00:39:36.945Z",
          "wordCount": null,
          "title": "Optimal Sampling Gaps for Adaptive Submodular Maximization. (arXiv:2104.01750v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.13236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demirkiran_F/0/1/0/all/0/1\">Ferhat Demirk&#x131;ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cayir_A/0/1/0/all/0/1\">Aykut &#xc7;ay&#x131;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_U/0/1/0/all/0/1\">U&#x11f;ur &#xdc;nal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dag_H/0/1/0/all/0/1\">Hasan Da&#x11f;</a>",
          "description": "Classification of malware families is crucial for a comprehensive\nunderstanding of how they can infect devices, computers, or systems. Thus,\nmalware identification enables security researchers and incident responders to\ntake precautions against malware and accelerate mitigation. API call sequences\nmade by malware are widely utilized features by machine and deep learning\nmodels for malware classification as these sequences represent the behavior of\nmalware. However, traditional machine and deep learning models remain incapable\nof capturing sequence relationships between API calls. On the other hand, the\ntransformer-based models process sequences as a whole and learn relationships\nbetween API calls due to multi-head attention mechanisms and positional\nembeddings. Our experiments demonstrate that the transformer model with one\ntransformer block layer surpassed the widely used base architecture, LSTM.\nMoreover, BERT or CANINE, pre-trained transformer models, outperformed in\nclassifying highly imbalanced malware families according to evaluation metrics,\nF1-score, and AUC score. Furthermore, the proposed bagging-based random\ntransformer forest (RTF), an ensemble of BERT or CANINE, has reached the\nstate-of-the-art evaluation scores on three out of four datasets, particularly\nstate-of-the-art F1-score of 0.6149 on one of the commonly used benchmark\ndataset.",
          "link": "http://arxiv.org/abs/2112.13236",
          "publishedOn": "2022-01-05T00:39:36.944Z",
          "wordCount": null,
          "title": "An Ensemble of Pre-trained Transformer Models For Imbalanced Multiclass Malware Classification. (arXiv:2112.13236v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.02346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1\">Erwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">James J. Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stavrou_G/0/1/0/all/0/1\">Georgios-Ilias Stavrou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_P/0/1/0/all/0/1\">Peter Y. K. Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constantinides_G/0/1/0/all/0/1\">George A. Constantinides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelfattah_M/0/1/0/all/0/1\">Mohamed S. Abdelfattah</a>",
          "description": "FPGA-specific DNN architectures using the native LUTs as independently\ntrainable inference operators have been shown to achieve favorable\narea-accuracy and energy-accuracy tradeoffs. The first work in this area,\nLUTNet, exhibited state-of-the-art performance for standard DNN benchmarks. In\nthis paper, we propose the learned optimization of such LUT-based topologies,\nresulting in higher-efficiency designs than via the direct use of\noff-the-shelf, hand-designed networks. Existing implementations of this class\nof architecture require the manual specification of the number of inputs per\nLUT, K. Choosing appropriate K a priori is challenging, and doing so at even\nhigh granularity, e.g. per layer, is a time-consuming and error-prone process\nthat leaves FPGAs' spatial flexibility underexploited. Furthermore, prior works\nsee LUT inputs connected randomly, which does not guarantee a good choice of\nnetwork topology. To address these issues, we propose logic shrinkage, a\nfine-grained netlist pruning methodology enabling K to be automatically learned\nfor every LUT in a neural network targeted for FPGA inference. By removing LUT\ninputs determined to be of low importance, our method increases the efficiency\nof the resultant accelerators. Our GPU-friendly solution to LUT input removal\nis capable of processing large topologies during their training with negligible\nslowdown. With logic shrinkage, we better the area and energy efficiency of the\nbest-performing LUTNet implementation of the CNV network classifying CIFAR-10\nby 1.54x and 1.31x, respectively, while matching its accuracy. This\nimplementation also reaches 2.71x the area efficiency of an equally accurate,\nheavily pruned BNN. On ImageNet with the Bi-Real Net architecture, employment\nof logic shrinkage results in a post-synthesis area reduction of 2.67x vs\nLUTNet, allowing for implementation that was previously impossible on today's\nlargest FPGAs.",
          "link": "http://arxiv.org/abs/2112.02346",
          "publishedOn": "2022-01-05T00:39:36.938Z",
          "wordCount": null,
          "title": "Logic Shrinkage: Learned FPGA Netlist Sparsity for Efficient Neural Network Inference. (arXiv:2112.02346v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Adaszewski_S/0/1/0/all/0/1\">Stanislaw Adaszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuner_P/0/1/0/all/0/1\">Pascal Kuner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaeger_R/0/1/0/all/0/1\">Ralf J. Jaeger</a>",
          "description": "We use a text dataset consisting of 23 news categories relevant to pharma\ninformation science, in order to compare the fine-tuning performance of\nmultiple transformer models in a classification task. Using a well-balanced\ndataset with multiple autoregressive and autocoding transformation models, we\ncompare their fine-tuning performance. To validate the winning approach, we\nperform diagnostics of model behavior on mispredicted instances, including\ninspection of category-wise metrics, evaluation of prediction certainty and\nassessment of latent space representations. Lastly, we propose an ensemble\nmodel consisting of the top performing individual predictors and demonstrate\nthat this approach offers a modest improvement in the F1 metric.",
          "link": "http://arxiv.org/abs/2201.00688",
          "publishedOn": "2022-01-05T00:39:36.937Z",
          "wordCount": null,
          "title": "Automatic Pharma News Categorization. (arXiv:2201.00688v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10840",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tsukurimichi_T/0/1/0/all/0/1\">Toshiaki Tsukurimichi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Inatsu_Y/0/1/0/all/0/1\">Yu Inatsu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Duy_V/0/1/0/all/0/1\">Vo Nguyen Le Duy</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Takeuchi_I/0/1/0/all/0/1\">Ichiro Takeuchi</a>",
          "description": "In practical data analysis under noisy environment, it is common to first use\nrobust methods to identify outliers, and then to conduct further analysis after\nremoving the outliers. In this paper, we consider statistical inference of the\nmodel estimated after outliers are removed, which can be interpreted as a\nselective inference (SI) problem. To use conditional SI framework, it is\nnecessary to characterize the events of how the robust method identifies\noutliers. Unfortunately, the existing methods cannot be directly used here\nbecause they are applicable to the case where the selection events can be\nrepresented by linear/quadratic constraints. In this paper, we propose a\nconditional SI method for popular robust regressions by using homotopy method.\nWe show that the proposed conditional SI method is applicable to a wide class\nof robust regression and outlier detection methods and has good empirical\nperformance on both synthetic data and real data experiments.",
          "link": "http://arxiv.org/abs/2104.10840",
          "publishedOn": "2022-01-05T00:39:36.935Z",
          "wordCount": null,
          "title": "Conditional Selective Inference for Robust Regression and Outlier Detection using Piecewise-Linear Homotopy Continuation. (arXiv:2104.10840v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1907.09358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mogadala_A/0/1/0/all/0/1\">Aditya Mogadala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalimuthu_M/0/1/0/all/0/1\">Marimuthu Kalimuthu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>",
          "description": "Interest in Artificial Intelligence (AI) and its applications has seen\nunprecedented growth in the last few years. This success can be partly\nattributed to the advancements made in the sub-fields of AI such as machine\nlearning, computer vision, and natural language processing. Much of the growth\nin these fields has been made possible with deep learning, a sub-area of\nmachine learning that uses artificial neural networks. This has created\nsignificant interest in the integration of vision and language. In this survey,\nwe focus on ten prominent tasks that integrate language and vision by\ndiscussing their problem formulation, methods, existing datasets, evaluation\nmeasures, and compare the results obtained with corresponding state-of-the-art\nmethods. Our efforts go beyond earlier surveys which are either task-specific\nor concentrate only on one type of visual content, i.e., image or video.\nFurthermore, we also provide some potential future directions in this field of\nresearch with an anticipation that this survey stimulates innovative thoughts\nand ideas to address the existing challenges and build new applications.",
          "link": "http://arxiv.org/abs/1907.09358",
          "publishedOn": "2022-01-05T00:39:36.934Z",
          "wordCount": null,
          "title": "Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods. (arXiv:1907.09358v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smelko_A/0/1/0/all/0/1\">Adam &#x160;melko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molnarova_S/0/1/0/all/0/1\">So&#x148;a Moln&#xe1;rov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kratochvil_M/0/1/0/all/0/1\">Miroslav Kratochv&#xed;l</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koladiya_A/0/1/0/all/0/1\">Abhishek Koladiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musil_J/0/1/0/all/0/1\">Jan Musil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krulis_M/0/1/0/all/0/1\">Martin Kruli&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrasek_J/0/1/0/all/0/1\">Ji&#x159;&#xed; Vondr&#xe1;&#x161;ek</a>",
          "description": "Dimensionality reduction methods have found vast application as visualization\ntools in diverse areas of science. Although many different methods exist, their\nperformance is often insufficient for providing quick insight into many\ncontemporary datasets, and the unsupervised mode of use prevents the users from\nutilizing the methods for dataset exploration and fine-tuning the details for\nimproved visualization quality. We present BlosSOM, a high-performance\nsemi-supervised dimensionality reduction software for interactive\nuser-steerable visualization of high-dimensional datasets with millions of\nindividual data points. BlosSOM builds on a GPU-accelerated implementation of\nthe EmbedSOM algorithm, complemented by several landmark-based algorithms for\ninterfacing the unsupervised model learning algorithms with the user\nsupervision. We show the application of BlosSOM on realistic datasets, where it\nhelps to produce high-quality visualizations that incorporate user-specified\nlayout and focus on certain features. We believe the semi-supervised\ndimensionality reduction will improve the data visualization possibilities for\nscience areas such as single-cell cytometry, and provide a fast and efficient\nbase methodology for new directions in dataset exploration and annotation.",
          "link": "http://arxiv.org/abs/2201.00701",
          "publishedOn": "2022-01-05T00:39:36.928Z",
          "wordCount": null,
          "title": "Scalable semi-supervised dimensionality reduction with GPU-accelerated EmbedSOM. (arXiv:2201.00701v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1912.10036",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Elbir_A/0/1/0/all/0/1\">Ahmet M. Elbir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mishra_K/0/1/0/all/0/1\">Kumar Vijay Mishra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shankar_M/0/1/0/all/0/1\">M. R. Bhavani Shankar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ottersten_B/0/1/0/all/0/1\">Bj&#xf6;rn Ottersten</a>",
          "description": "Hybrid analog and digital beamforming transceivers are instrumental in\naddressing the challenge of expensive hardware and high training overheads in\nthe next generation millimeter-wave (mm-Wave) massive MIMO (multiple-input\nmultiple-output) systems. However, lack of fully digital beamforming in hybrid\narchitectures and short coherence times at mm-Wave impose additional\nconstraints on the channel estimation. Prior works on addressing these\nchallenges have focused largely on narrowband channels wherein\noptimization-based or greedy algorithms were employed to derive hybrid\nbeamformers. In this paper, we introduce a deep learning (DL) approach for\nchannel estimation and hybrid beamforming for frequency-selective, wideband\nmm-Wave systems. In particular, we consider a massive MIMO Orthogonal Frequency\nDivision Multiplexing (MIMO-OFDM) system and propose three different DL\nframeworks comprising convolutional neural networks (CNNs), which accept the\nraw data of received signal as input and yield channel estimates and the hybrid\nbeamformers at the output. We also introduce both offline and online prediction\nschemes. Numerical experiments demonstrate that, compared to the current\nstate-of-the-art optimization and DL methods, our approach provides higher\nspectral efficiency, lesser computational cost and fewer number of pilot\nsignals, and higher tolerance against the deviations in the received pilot\ndata, corrupted channel matrix, and propagation environment.",
          "link": "http://arxiv.org/abs/1912.10036",
          "publishedOn": "2022-01-05T00:39:36.927Z",
          "wordCount": null,
          "title": "A Family of Deep Learning Architectures for Channel Estimation and Hybrid Beamforming in Multi-Carrier mm-Wave Massive MIMO. (arXiv:1912.10036v6 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.04926",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Li_X/0/1/0/all/0/1\">Xiao Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Milzarek_A/0/1/0/all/0/1\">Andre Milzarek</a>, <a href=\"http://arxiv.org/find/math/1/au:+Qiu_J/0/1/0/all/0/1\">Junwen Qiu</a>",
          "description": "We study the random reshuffling (RR) method for smooth nonconvex optimization\nproblems with a finite-sum structure. Though this method is widely utilized in\npractice such as the training of neural networks, its convergence behavior is\nonly understood in several limited settings. In this paper, under the\nwell-known Kurdyka-Lojasiewicz (KL) inequality, we establish strong limit-point\nconvergence results for RR with appropriate diminishing step sizes, namely, the\nwhole sequence of iterates generated by RR is convergent and converges to a\nsingle stationary point in an almost sure sense. In addition, we derive the\ncorresponding rate of convergence, depending on the KL exponent and the\nsuitably selected diminishing step sizes. When the KL exponent lies in\n$[0,\\frac12]$, the convergence is at a rate of $\\mathcal{O}(t^{-1})$ with $t$\ncounting the iteration number. When the KL exponent belongs to $(\\frac12,1)$,\nour derived convergence rate is of the form $\\mathcal{O}(t^{-q})$ with $q\\in\n(0,1)$ depending on the KL exponent. The standard KL inequality-based\nconvergence analysis framework only applies to algorithms with a certain\ndescent property. We conduct a novel convergence analysis for the non-descent\nRR method with diminishing step sizes based on the KL inequality, which\ngeneralizes the standard KL framework. We summarize our main steps and core\nideas in an informal analysis framework, which is of independent interest. As a\ndirect application of this framework, we also establish similar strong\nlimit-point convergence results for the reshuffled proximal point method.",
          "link": "http://arxiv.org/abs/2110.04926",
          "publishedOn": "2022-01-05T00:39:36.927Z",
          "wordCount": null,
          "title": "Convergence of Random Reshuffling Under The Kurdyka-{\\L}ojasiewicz Inequality. (arXiv:2110.04926v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11765",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1\">Tri Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1\">Simon Kornblith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walter_M/0/1/0/all/0/1\">Matthew R. Walter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maire_M/0/1/0/all/0/1\">Michael Maire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khademi_M/0/1/0/all/0/1\">Maryam Khademi</a>",
          "description": "Self-supervised representation learning has made significant leaps fueled by\nprogress in contrastive learning, which seeks to learn transformations that\nembed positive input pairs nearby, while pushing negative pairs far apart.\nWhile positive pairs can be generated reliably (e.g., as different views of the\nsame image), it is difficult to accurately establish negative pairs, defined as\nsamples from different images regardless of their semantic content or visual\nfeatures. A fundamental problem in contrastive learning is mitigating the\neffects of false negatives. Contrasting false negatives induces two critical\nissues in representation learning: discarding semantic information and slow\nconvergence. In this paper, we propose novel approaches to identify false\nnegatives, as well as two strategies to mitigate their effect, i.e. false\nnegative elimination and attraction, while systematically performing rigorous\nevaluations to study this problem in detail. Our method exhibits consistent\nimprovements over existing contrastive learning-based methods. Without labels,\nwe identify false negatives with 40% accuracy among 1000 semantic classes on\nImageNet, and achieve 5.8% absolute improvement in top-1 accuracy over the\nprevious state-of-the-art when finetuning with 1% labels. Our code is available\nat https://github.com/google-research/fnc.",
          "link": "http://arxiv.org/abs/2011.11765",
          "publishedOn": "2022-01-05T00:39:36.926Z",
          "wordCount": null,
          "title": "Boosting Contrastive Self-Supervised Learning with False Negative Cancellation. (arXiv:2011.11765v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kashani_S/0/1/0/all/0/1\">Shlomo Kashani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1\">Amir Ivry</a>",
          "description": "The second edition of Deep Learning Interviews is home to hundreds of\nfully-solved problems, from a wide range of key topics in AI. It is designed to\nboth rehearse interview or exam specific topics and provide machine learning\nM.Sc./Ph.D. students, and those awaiting an interview a well-organized overview\nof the field. The problems it poses are tough enough to cut your teeth on and\nto dramatically improve your skills-but they're framed within thought-provoking\nquestions and engaging stories. That is what makes the volume so specifically\nvaluable to students and job seekers: it provides them with the ability to\nspeak confidently and quickly on any relevant topic, to answer technical\nquestions clearly and correctly, and to fully understand the purpose and\nmeaning of interview questions and answers. Those are powerful, indispensable\nadvantages to have when walking into the interview room. The book's contents is\na large inventory of numerous topics relevant to DL job interviews and graduate\nlevel exams. That places this work at the forefront of the growing trend in\nscience to teach a core set of practical mathematical and computational skills.\nIt is widely accepted that the training of every computer scientist must\ninclude the fundamental theorems of ML, and AI appears in the curriculum of\nnearly every university. This volume is designed as an excellent reference for\ngraduates of such programs.",
          "link": "http://arxiv.org/abs/2201.00650",
          "publishedOn": "2022-01-05T00:39:36.922Z",
          "wordCount": null,
          "title": "Deep Learning Interviews: Hundreds of fully solved job interview questions from a wide range of key topics in AI. (arXiv:2201.00650v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2110.06850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bohang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Du Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>",
          "description": "Recently, Zhang et al.(2021) developed a new neural network architecture\nbased on $\\ell_\\infty$-distance functions, which naturally possesses certified\n$\\ell_\\infty$ robustness by its construction. Despite rigorous theoretical\nguarantees, the model so far can only achieve comparable performance to\nconventional networks. In this paper, we make the following two contributions:\n$\\mathrm{(i)}$ We demonstrate that $\\ell_\\infty$-distance nets enjoy a\nfundamental advantage in certified robustness over conventional networks (under\ntypical certification approaches); $\\mathrm{(ii)}$ With an improved training\nprocess we are able to significantly boost the certified accuracy of\n$\\ell_\\infty$-distance nets. Our training approach largely alleviates the\noptimization problem that arose in the previous training scheme, in particular,\nthe unexpected large Lipschitz constant due to the use of a crucial trick\ncalled $\\ell_p$-relaxation. The core of our training approach is a novel\nobjective function that combines scaled cross-entropy loss and clipped hinge\nloss with a decaying mixing coefficient. Experiments show that using the\nproposed training strategy, the certified accuracy of $\\ell_\\infty$-distance\nnet can be dramatically improved from 33.30% to 40.06% on CIFAR-10\n($\\epsilon=8/255$), meanwhile outperforming other approaches in this area by a\nlarge margin. Our results clearly demonstrate the effectiveness and potential\nof $\\ell_\\infty$-distance net for certified robustness. Codes are available at\nhttps://github.com/zbh2047/L_inf-dist-net-v2.",
          "link": "http://arxiv.org/abs/2110.06850",
          "publishedOn": "2022-01-05T00:39:36.917Z",
          "wordCount": null,
          "title": "Boosting the Certified Robustness of L-infinity Distance Nets. (arXiv:2110.06850v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.13926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nickel_D/0/1/0/all/0/1\">David Nickel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Frank Po-Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseinalipour_S/0/1/0/all/0/1\">Seyyedali Hosseinalipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michelusi_N/0/1/0/all/0/1\">Nicolo Michelusi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1\">Christopher G. Brinton</a>",
          "description": "Federated learning (FL) has emerged as a popular methodology for distributing\nmachine learning across wireless edge devices. In this work, we consider\noptimizing the tradeoff between model performance and resource utilization in\nFL, under device-server communication delays and device computation\nheterogeneity. Our proposed StoFedDelAv algorithm incorporates a local-global\nmodel combiner into the FL synchronization step. We theoretically characterize\nthe convergence behavior of StoFedDelAv and obtain the optimal combiner\nweights, which consider the global model delay and expected local gradient\nerror at each device. We then formulate a network-aware optimization problem\nwhich tunes the minibatch sizes of the devices to jointly minimize energy\nconsumption and machine learning training loss, and solve the non-convex\nproblem through a series of convex approximations. Our simulations reveal that\nStoFedDelAv outperforms the current art in FL in terms of model convergence\nspeed and network resource utilization when the minibatch size and the combiner\nweights are adjusted. Additionally, our method can reduce the number of uplink\ncommunication rounds required during the model training period to reach the\nsame accuracy.",
          "link": "http://arxiv.org/abs/2112.13926",
          "publishedOn": "2022-01-05T00:39:36.915Z",
          "wordCount": null,
          "title": "Resource-Efficient and Delay-Aware Federated Learning Design under Edge Heterogeneity. (arXiv:2112.13926v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.13771",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haotao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kossaifi_J/0/1/0/all/0/1\">Jean Kossaifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>",
          "description": "Data augmentation is a simple yet effective way to improve the robustness of\ndeep neural networks (DNNs). Diversity and hardness are two complementary\ndimensions of data augmentation to achieve robustness. For example, AugMix\nexplores random compositions of a diverse set of augmentations to enhance\nbroader coverage, while adversarial training generates adversarially hard\nsamples to spot the weakness. Motivated by this, we propose a data augmentation\nframework, termed AugMax, to unify the two aspects of diversity and hardness.\nAugMax first randomly samples multiple augmentation operators and then learns\nan adversarial mixture of the selected operators. Being a stronger form of data\naugmentation, AugMax leads to a significantly augmented input distribution\nwhich makes model training more challenging. To solve this problem, we further\ndesign a disentangled normalization module, termed DuBIN\n(Dual-Batch-and-Instance Normalization), that disentangles the instance-wise\nfeature heterogeneity arising from AugMax. Experiments show that AugMax-DuBIN\nleads to significantly improved out-of-distribution robustness, outperforming\nprior arts by 3.03%, 3.49%, 1.82% and 0.71% on CIFAR10-C, CIFAR100-C, Tiny\nImageNet-C and ImageNet-C. Codes and pretrained models are available:\nhttps://github.com/VITA-Group/AugMax.",
          "link": "http://arxiv.org/abs/2110.13771",
          "publishedOn": "2022-01-05T00:39:36.914Z",
          "wordCount": null,
          "title": "AugMax: Adversarial Composition of Random Augmentations for Robust Training. (arXiv:2110.13771v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morais_J/0/1/0/all/0/1\">Jo&#xe3;o Morais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathi_K/0/1/0/all/0/1\">Kaushal Rathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_B/0/1/0/all/0/1\">Bhuvaneshwar Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajesh_S/0/1/0/all/0/1\">Shantanu Rajesh</a>",
          "description": "How to find places that are not indexed by Google Maps? We propose an\nintuitive method and framework to locate places based on their distinctive\nspatial features. The method uses satellite and street view images in machine\nvision approaches to classify locations. If we can classify locations, we just\nneed to repeat for non-overlapping locations in our area of interest. We assess\nthe proposed system in finding Parkour spots in the campus of Arizona State\nUniversity. The results are very satisfactory, having found more than 25 new\nParkour spots, with a rate of true positives above 60%.",
          "link": "http://arxiv.org/abs/2201.00377",
          "publishedOn": "2022-01-05T00:39:36.911Z",
          "wordCount": null,
          "title": "Parkour Spot ID: Feature Matching in Satellite and Street view images using Deep Learning. (arXiv:2201.00377v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.01870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1\">Julian Togelius</a>",
          "description": "Counterfactual Regret Minimization (CFR) has achieved many fascinating\nresults in solving large-scale Imperfect Information Games (IIGs). Neural\nnetwork approximation CFR (neural CFR) is one of the promising techniques that\ncan reduce computation and memory consumption by generalizing decision\ninformation between similar states. Current neural CFR algorithms have to\napproximate cumulative regrets. However, efficient and accurate approximation\nin a large-scale IIG is still a tough challenge. In this paper, a new CFR\nvariant, Recursive CFR (ReCFR), is proposed. In ReCFR, Recursive Substitute\nValues (RSVs) are learned and used to replace cumulative regrets. It is proven\nthat ReCFR can converge to a Nash equilibrium at a rate of\n$O(\\frac{1}{\\sqrt{T}})$. Based on ReCFR, a new model-free neural CFR with\nbootstrap learning, Neural ReCFR-B, is proposed. Due to the recursive and\nnon-cumulative nature of RSVs, Neural ReCFR-B has lower-variance training\ntargets than other neural CFRs. Experimental results show that Neural ReCFR-B\nis competitive with the state-of-the-art neural CFR algorithms at a much lower\ntraining cost.",
          "link": "http://arxiv.org/abs/2012.01870",
          "publishedOn": "2022-01-05T00:39:36.910Z",
          "wordCount": null,
          "title": "Model-free Neural Counterfactual Regret Minimization with Bootstrap Learning. (arXiv:2012.01870v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00404",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Jha_R/0/1/0/all/0/1\">Ranjeet Ranjan Jha</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bhardwaj_A/0/1/0/all/0/1\">Abhishek Bhardwaj</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Garg_D/0/1/0/all/0/1\">Devin Garg</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bhavsar_A/0/1/0/all/0/1\">Arnav Bhavsar</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Nigam_A/0/1/0/all/0/1\">Aditya Nigam</a>",
          "description": "Resting-state fMRI is commonly used for diagnosing Autism Spectrum Disorder\n(ASD) by using network-based functional connectivity. It has been shown that\nASD is associated with brain regions and their inter-connections. However,\ndiscriminating based on connectivity patterns among imaging data of the control\npopulation and that of ASD patients' brains is a non-trivial task. In order to\ntackle said classification task, we propose a novel deep learning architecture\n(MHATC) consisting of multi-head attention and temporal consolidation modules\nfor classifying an individual as a patient of ASD. The devised architecture\nresults from an in-depth analysis of the limitations of current deep neural\nnetwork solutions for similar applications. Our approach is not only robust but\ncomputationally efficient, which can allow its adoption in a variety of other\nresearch and clinical settings.",
          "link": "http://arxiv.org/abs/2201.00404",
          "publishedOn": "2022-01-05T00:39:36.908Z",
          "wordCount": null,
          "title": "MHATC: Autism Spectrum Disorder identification utilizing multi-head attention encoder along with temporal consolidation modules. (arXiv:2201.00404v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00012",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peschl_M/0/1/0/all/0/1\">Markus Peschl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zgonnikov_A/0/1/0/all/0/1\">Arkady Zgonnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliehoek_F/0/1/0/all/0/1\">Frans A. Oliehoek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siebert_L/0/1/0/all/0/1\">Luciano C. Siebert</a>",
          "description": "Inferring reward functions from demonstrations and pairwise preferences are\nauspicious approaches for aligning Reinforcement Learning (RL) agents with\nhuman intentions. However, state-of-the art methods typically focus on learning\na single reward model, thus rendering it difficult to trade off different\nreward functions from multiple experts. We propose Multi-Objective Reinforced\nActive Learning (MORAL), a novel method for combining diverse demonstrations of\nsocial norms into a Pareto-optimal policy. Through maintaining a distribution\nover scalarization weights, our approach is able to interactively tune a deep\nRL agent towards a variety of preferences, while eliminating the need for\ncomputing multiple policies. We empirically demonstrate the effectiveness of\nMORAL in two scenarios, which model a delivery and an emergency task that\nrequire an agent to act in the presence of normative conflicts. Overall, we\nconsider our research a step towards multi-objective RL with learned rewards,\nbridging the gap between current reward learning and machine ethics literature.",
          "link": "http://arxiv.org/abs/2201.00012",
          "publishedOn": "2022-01-05T00:39:36.901Z",
          "wordCount": null,
          "title": "MORAL: Aligning AI with Human Norms through Multi-Objective Reinforced Active Learning. (arXiv:2201.00012v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roros_C/0/1/0/all/0/1\">Constantine J. Roros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kak_A/0/1/0/all/0/1\">Avinash C. Kak</a>",
          "description": "We propose a recurrent neural network-based spatio-temporal framework named\nmaskGRU for the detection and tracking of small objects in videos. While there\nhave been many developments in the area of object tracking in recent years,\ntracking a small moving object amid other moving objects and actors (such as a\nball amid moving players in sports footage) continues to be a difficult task.\nExisting spatio-temporal networks, such as convolutional Gated Recurrent Units\n(convGRUs), are difficult to train and have trouble accurately tracking small\nobjects under such conditions. To overcome these difficulties, we developed the\nmaskGRU framework that uses a weighted sum of the internal hidden state\nproduced by a convGRU and a 3-channel mask of the tracked object's predicted\nbounding box as the hidden state to be used at the next time step of the\nunderlying convGRU. We believe the technique of incorporating a mask into the\nhidden state through a weighted sum has two benefits: controlling the effect of\nexploding gradients and introducing an attention-like mechanism into the\nnetwork by indicating where in the previous video frame the object is located.\nOur experiments show that maskGRU outperforms convGRU at tracking objects that\nare small relative to the video resolution even in the presence of other moving\nobjects.",
          "link": "http://arxiv.org/abs/2201.00467",
          "publishedOn": "2022-01-05T00:39:36.898Z",
          "wordCount": null,
          "title": "maskGRU: Tracking Small Objects in the Presence of Large Background Motions. (arXiv:2201.00467v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00044",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenghao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1\">Hongyuan Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1\">Jason Eisner</a>",
          "description": "We propose an approach to modeling irregularly spaced sequences of discrete\nevents. We begin with a continuous-time variant of the Transformer, which was\noriginally formulated (Vaswani et al., 2017) for sequences without timestamps.\nWe embed a possible event (or other boolean fact) at time $t$ by using\nattention over the events that occurred at times $< t$ (and the facts that were\ntrue when they occurred). We control this attention using pattern-matching\nlogic rules that relate events and facts that share participants. These rules\ndetermine which previous events will be attended to, as well as how to\ntransform the embeddings of the events and facts into the attentional queries,\nkeys, and values. Other logic rules describe how to change the set of facts in\nresponse to events. Our approach closely follows Mei et al. (2020a), and adopts\ntheir Datalog Through Time formalism for logic rules. As in that work, a domain\nexpert first writes a set of logic rules that establishes the set of possible\nevents and other facts at each time $t$. Each possible event or other fact is\nembedded using a neural architecture that is derived from the rules that\nestablished it. Our only difference from Mei et al. (2020a) is that we derive a\nflatter, attention-based neural architecture whereas they used a more serial\nLSTM architecture. We find that our attention-based approach performs about\nequally well on the RoboCup dataset, where the logic rules play an important\nrole in improving performance. We also compared these two methods with two\nprevious attention-based methods (Zuo et al., 2020; Zhang et al., 2020a) on\nsimpler synthetic and real domains without logic rules, and found our proposed\napproach to be at least as good, and sometimes better, than each of the other\nthree methods.",
          "link": "http://arxiv.org/abs/2201.00044",
          "publishedOn": "2022-01-05T00:39:36.897Z",
          "wordCount": null,
          "title": "Transformer Embeddings of Irregularly Spaced Events and Their Participants. (arXiv:2201.00044v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ankile_L/0/1/0/all/0/1\">Lars Lien Ankile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krange_K/0/1/0/all/0/1\">Kjartan Krange</a>",
          "description": "This paper presents an ensemble forecasting method that shows strong results\non the M4Competition dataset by decreasing feature and model selection\nassumptions, termed DONUT(DO Not UTilize human assumptions). Our assumption\nreductions, consisting mainly of auto-generated features and a more diverse\nmodel pool for the ensemble, significantly outperforms the\nstatistical-feature-based ensemble method FFORMA by Montero-Manso et al.\n(2020). Furthermore, we investigate feature extraction with a Long short-term\nmemory Network(LSTM) Autoencoder and find that such features contain crucial\ninformation not captured by traditional statistical feature approaches. The\nensemble weighting model uses both LSTM features and statistical features to\ncombine the models accurately. Analysis of feature importance and interaction\nshow a slight superiority for LSTM features over the statistical ones alone.\nClustering analysis shows that different essential LSTM features are different\nfrom most statistical features and each other. We also find that increasing the\nsolution space of the weighting model by augmenting the ensemble with new\nmodels is something the weighting model learns to use, explaining part of the\naccuracy gains. Lastly, we present a formal ex-post-facto analysis of optimal\ncombination and selection for ensembles, quantifying differences through linear\noptimization on the M4 dataset. We also include a short proof that model\ncombination is superior to model selection, a posteriori.",
          "link": "http://arxiv.org/abs/2201.00426",
          "publishedOn": "2022-01-05T00:39:36.896Z",
          "wordCount": null,
          "title": "The DONUT Approach to EnsembleCombination Forecasting. (arXiv:2201.00426v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruiqi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_Y/0/1/0/all/0/1\">Yash Raj Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lieder_F/0/1/0/all/0/1\">Falk Lieder</a>",
          "description": "People's decisions about how to allocate their limited computational\nresources are essential to human intelligence. An important component of this\nmetacognitive ability is deciding whether to continue thinking about what to do\nand move on to the next decision. Here, we show that people acquire this\nability through learning and reverse-engineer the underlying learning\nmechanisms. Using a process-tracing paradigm that externalises human planning,\nwe find that people quickly adapt how much planning they perform to the cost\nand benefit of planning. To discover the underlying metacognitive learning\nmechanisms we augmented a set of reinforcement learning models with\nmetacognitive features and performed Bayesian model selection. Our results\nsuggest that the metacognitive ability to adjust the amount of planning might\nbe learned through a policy-gradient mechanism that is guided by metacognitive\npseudo-rewards that communicate the value of planning.",
          "link": "http://arxiv.org/abs/2201.00764",
          "publishedOn": "2022-01-05T00:39:36.896Z",
          "wordCount": null,
          "title": "Have I done enough planning or should I plan more?. (arXiv:2201.00764v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00007",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hailin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Defang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>",
          "description": "Knowledge distillation is initially introduced to utilize additional\nsupervision from a single teacher model for the student model training. To\nboost the student performance, some recent variants attempt to exploit diverse\nknowledge sources from multiple teachers. However, existing studies mainly\nintegrate knowledge from diverse sources by averaging over multiple teacher\npredictions or combining them using other various label-free strategies, which\nmay mislead student in the presence of low-quality teacher predictions. To\ntackle this problem, we propose Confidence-Aware Multi-teacher Knowledge\nDistillation (CA-MKD), which adaptively assigns sample-wise reliability for\neach teacher prediction with the help of ground-truth labels, with those\nteacher predictions close to one-hot labels assigned large weights. Besides,\nCA-MKD incorporates intermediate layers to further improve student performance.\nExtensive experiments show that our CA-MKD consistently outperforms all\ncompared state-of-the-art methods across various teacher-student architectures.",
          "link": "http://arxiv.org/abs/2201.00007",
          "publishedOn": "2022-01-05T00:39:36.895Z",
          "wordCount": null,
          "title": "Confidence-Aware Multi-Teacher Knowledge Distillation. (arXiv:2201.00007v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00162",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Symeonidis_G/0/1/0/all/0/1\">G. Symeonidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nerantzis_E/0/1/0/all/0/1\">E. Nerantzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazakis_A/0/1/0/all/0/1\">A. Kazakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papakostas_G/0/1/0/all/0/1\">G.A. Papakostas</a>",
          "description": "This paper is an overview of the Machine Learning Operations (MLOps) area.\nOur aim is to define the operation and the components of such systems by\nhighlighting the current problems and trends. In this context, we present the\ndifferent tools and their usefulness in order to provide the corresponding\nguidelines. Moreover, the connection between MLOps and AutoML (Automated\nMachine Learning) is identified and how this combination could work is\nproposed.",
          "link": "http://arxiv.org/abs/2201.00162",
          "publishedOn": "2022-01-05T00:39:36.895Z",
          "wordCount": null,
          "title": "MLOps -- Definitions, Tools and Challenges. (arXiv:2201.00162v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00719",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mulay_A/0/1/0/all/0/1\">Ajinkya K Mulay</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lane_S/0/1/0/all/0/1\">Sean Lane</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hennes_E/0/1/0/all/0/1\">Erin Hennes</a>",
          "description": "It is increasingly acknowledged that a priori statistical power estimation\nfor planned studies with multiple model parameters is inherently a multivariate\nproblem. Power for individual parameters of interest cannot be reliably\nestimated univariately because sampling variably in, correlation with, and\nvariance explained relative to one parameter will impact the power for another\nparameter, all usual univariate considerations being equal. Explicit solutions\nin such cases, especially for models with many parameters, are either\nimpractical or impossible to solve, leaving researchers with the prevailing\nmethod of simulating power. However, point estimates for a vector of model\nparameters are uncertain, and the impact of inaccuracy is unknown. In such\ncases, sensitivity analysis is recommended such that multiple combinations of\npossible observable parameter vectors are simulated to understand power\ntrade-offs. A limitation to this approach is that it is computationally\nexpensive to generate sufficient sensitivity combinations to accurately map the\npower trade-off function in increasingly high dimensional spaces for the models\nthat social scientists estimate. This paper explores the efficient estimation\nand graphing of statistical power for a study over varying model parameter\ncombinations. Optimally powering a study is crucial to ensure a minimum\nprobability of finding the hypothesized effect. We first demonstrate the impact\nof varying parameter values on power for specific hypotheses of interest and\nquantify the computational intensity of computing such a graph for a given\nlevel of precision. Finally, we propose a simple and generalizable machine\nlearning inspired solution to cut the computational cost to less than 7\\% of\nwhat could be called a brute force approach. [abridged]",
          "link": "http://arxiv.org/abs/2201.00719",
          "publishedOn": "2022-01-05T00:39:36.894Z",
          "wordCount": null,
          "title": "PowerGraph: Using neural networks and principal components to multivariate statistical power trade-offs. (arXiv:2201.00719v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yanchao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Ruijie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1\">Andrew Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Furong Huang</a>",
          "description": "In many reinforcement learning (RL) applications, the observation space is\nspecified by human developers and restricted by physical realizations, and may\nthus be subject to dramatic changes over time (e.g. increased number of\nobservable features). However, when the observation space changes, the previous\npolicy will likely fail due to the mismatch of input features, and another\npolicy must be trained from scratch, which is inefficient in terms of\ncomputation and sample complexity. Following theoretical insights, we propose a\nnovel algorithm which extracts the latent-space dynamics in the source task,\nand transfers the dynamics model to the target task to use as a model-based\nregularizer. Our algorithm works for drastic changes of observation space (e.g.\nfrom vector-based observation to image-based observation), without any\ninter-task mapping or any prior knowledge of the target task. Empirical results\nshow that our algorithm significantly improves the efficiency and stability of\nlearning in the target task.",
          "link": "http://arxiv.org/abs/2201.00248",
          "publishedOn": "2022-01-05T00:39:36.893Z",
          "wordCount": null,
          "title": "Transfer RL across Observation Feature Spaces via Model-Based Regularization. (arXiv:2201.00248v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1\">Quan Z. Sheng</a>",
          "description": "Knowledge graph embedding (KGE) has drawn great attention due to its\npotential in automatic knowledge graph (KG) completion and knowledge-driven\ntasks. However, recent KGE models suffer from high training cost and large\nstorage space, thus limiting their practicality in real-world applications. To\naddress this challenge, based on the latest findings in the field of\nContrastive Learning, we propose a novel KGE training framework called\nHardness-aware Low-dimensional Embedding (HaLE). Instead of the traditional\nNegative Sampling, we design a new loss function based on query sampling that\ncan balance two important training targets, Alignment and Uniformity.\nFurthermore, we analyze the hardness-aware ability of recent low-dimensional\nhyperbolic models and propose a lightweight hardness-aware activation\nmechanism, which can help the KGE models focus on hard instances and speed up\nconvergence. The experimental results show that in the limited training time,\nHaLE can effectively improve the performance and training speed of KGE models\non five commonly-used datasets. The HaLE-trained models can obtain a high\nprediction accuracy after training few minutes and are competitive compared to\nthe state-of-the-art models in both low- and high-dimensional conditions.",
          "link": "http://arxiv.org/abs/2201.00565",
          "publishedOn": "2022-01-05T00:39:36.893Z",
          "wordCount": null,
          "title": "Swift and Sure: Hardness-aware Contrastive Learning for Low-dimensional Knowledge Graph Embeddings. (arXiv:2201.00565v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00458",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Afshar_P/0/1/0/all/0/1\">Parnian Afshar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohammadi_A/0/1/0/all/0/1\">Arash Mohammadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Farahani_K/0/1/0/all/0/1\">Keyvan Farahani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kirby_J/0/1/0/all/0/1\">Justin Kirby</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oikonomou_A/0/1/0/all/0/1\">Anastasia Oikonomou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Asif_A/0/1/0/all/0/1\">Amir Asif</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wee_L/0/1/0/all/0/1\">Leonard Wee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dekker_A/0/1/0/all/0/1\">Andre Dekker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haque_M/0/1/0/all/0/1\">Mohammad Ariful Haque</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hossain_S/0/1/0/all/0/1\">Shahruk Hossain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasan_M/0/1/0/all/0/1\">Md. Kamrul Hasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamal_U/0/1/0/all/0/1\">Uday Kamal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_W/0/1/0/all/0/1\">Winston Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1\">Jhih-Yuan Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahman_M/0/1/0/all/0/1\">M. Sohel Rahman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ibtehaz_N/0/1/0/all/0/1\">Nabil Ibtehaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Foisol_S/0/1/0/all/0/1\">Sh. M. Amir Foisol</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lam_K/0/1/0/all/0/1\">Kin-Man Lam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guang_Z/0/1/0/all/0/1\">Zhong Guang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1\">Runze Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Channappayya_S/0/1/0/all/0/1\">Sumohana S. Channappayya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1\">Shashank Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dev_C/0/1/0/all/0/1\">Chander Dev</a>",
          "description": "Lung cancer is one of the deadliest cancers, and in part its effective\ndiagnosis and treatment depend on the accurate delineation of the tumor.\nHuman-centered segmentation, which is currently the most common approach, is\nsubject to inter-observer variability, and is also time-consuming, considering\nthe fact that only experts are capable of providing annotations. Automatic and\nsemi-automatic tumor segmentation methods have recently shown promising\nresults. However, as different researchers have validated their algorithms\nusing various datasets and performance metrics, reliably evaluating these\nmethods is still an open challenge. The goal of the Lung-Originated Tumor\nSegmentation from Computed Tomography Scan (LOTUS) Benchmark created through\n2018 IEEE Video and Image Processing (VIP) Cup competition, is to provide a\nunique dataset and pre-defined metrics, so that different researchers can\ndevelop and evaluate their methods in a unified fashion. The 2018 VIP Cup\nstarted with a global engagement from 42 countries to access the competition\ndata. At the registration stage, there were 129 members clustered into 28 teams\nfrom 10 countries, out of which 9 teams made it to the final stage and 6 teams\nsuccessfully completed all the required tasks. In a nutshell, all the\nalgorithms proposed during the competition, are based on deep learning models\ncombined with a false positive reduction technique. Methods developed by the\nthree finalists show promising results in tumor segmentation, however, more\neffort should be put into reducing the false positive rate. This competition\nmanuscript presents an overview of the VIP-Cup challenge, along with the\nproposed algorithms and results.",
          "link": "http://arxiv.org/abs/2201.00458",
          "publishedOn": "2022-01-05T00:39:36.892Z",
          "wordCount": null,
          "title": "Lung-Originated Tumor Segmentation from Computed Tomography Scan (LOTUS) Benchmark. (arXiv:2201.00458v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pauli_P/0/1/0/all/0/1\">Patricia Pauli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funcke_N/0/1/0/all/0/1\">Niklas Funcke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gramlich_D/0/1/0/all/0/1\">Dennis Gramlich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Msalmi_M/0/1/0/all/0/1\">Mohamed Amine Msalmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allgower_F/0/1/0/all/0/1\">Frank Allg&#xf6;wer</a>",
          "description": "This paper is concerned with the training of neural networks (NNs) under\nsemidefinite constraints. This type of training problems has recently gained\npopularity since semidefinite constraints can be used to verify interesting\nproperties for NNs that include, e.g., the estimation of an upper bound on the\nLipschitz constant, which relates to the robustness of an NN, or the stability\nof dynamic systems with NN controllers. The utilized semidefinite constraints\nare based on sector constraints satisfied by the underlying activation\nfunctions. Unfortunately, one of the biggest bottlenecks of these new results\nis the required computational effort for incorporating the semidefinite\nconstraints into the training of NNs which is limiting their scalability to\nlarge NNs. We address this challenge by developing interior point methods for\nNN training that we implement using barrier functions for semidefinite\nconstraints. In order to efficiently compute the gradients of the barrier\nterms, we exploit the structure of the semidefinite constraints. In\nexperiments, we demonstrate the superior efficiency of our training method over\nprevious approaches, which allows us, e.g., to use semidefinite constraints in\nthe training of Wasserstein generative adversarial networks, where the\ndiscriminator must satisfy a Lipschitz condition.",
          "link": "http://arxiv.org/abs/2201.00632",
          "publishedOn": "2022-01-05T00:39:36.892Z",
          "wordCount": null,
          "title": "Neural network training under semidefinite constraints. (arXiv:2201.00632v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Woo_H/0/1/0/all/0/1\">Honguk Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyunsung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sangwoo Cho</a>",
          "description": "Recently, deep reinforcement learning (RL) has proven its feasibility in\nsolving combinatorial optimization problems (COPs). The learning-to-rank\ntechniques have been studied in the field of information retrieval. While\nseveral COPs can be formulated as the prioritization of input items, as is\ncommon in the information retrieval, it has not been fully explored how the\nlearning-to-rank techniques can be incorporated into deep RL for COPs. In this\npaper, we present the learning-to-rank distillation-based COP framework, where\na high-performance ranking policy obtained by RL for a COP can be distilled\ninto a non-iterative, simple model, thereby achieving a low-latency COP solver.\nSpecifically, we employ the approximated ranking distillation to render a\nscore-based ranking model learnable via gradient descent. Furthermore, we use\nthe efficient sequence sampling to improve the inference performance with a\nlimited delay. With the framework, we demonstrate that a distilled model not\nonly achieves comparable performance to its respective, high-performance RL,\nbut also provides several times faster inferences. We evaluate the framework\nwith several COPs such as priority-based task scheduling and multidimensional\nknapsack, demonstrating the benefits of the framework in terms of inference\nlatency and performance.",
          "link": "http://arxiv.org/abs/2201.00695",
          "publishedOn": "2022-01-05T00:39:36.891Z",
          "wordCount": null,
          "title": "An Efficient Combinatorial Optimization Model Using Learning-to-Rank Distillation. (arXiv:2201.00695v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1\">Shuhan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Letian Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">S.-H. Gary Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruiyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_C/0/1/0/all/0/1\">Chih-Chieh Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wen-Chih Peng</a>",
          "description": "We study the forecasting problem for traffic with dynamic, possibly\nperiodical, and joint spatial-temporal dependency between regions. Given the\naggregated inflow and outflow traffic of regions in a city from time slots 0 to\nt-1, we predict the traffic at time t at any region. Prior arts in the area\noften consider the spatial and temporal dependencies in a decoupled manner or\nare rather computationally intensive in training with a large number of\nhyper-parameters to tune. We propose ST-TIS, a novel, lightweight, and accurate\nSpatial-Temporal Transformer with information fusion and region sampling for\ntraffic forecasting. ST-TIS extends the canonical Transformer with information\nfusion and region sampling. The information fusion module captures the complex\nspatial-temporal dependency between regions. The region sampling module is to\nimprove the efficiency and prediction accuracy, cutting the computation\ncomplexity for dependency learning from $O(n^2)$ to $O(n\\sqrt{n})$, where n is\nthe number of regions. With far fewer parameters than state-of-the-art models,\nthe offline training of our model is significantly faster in terms of tuning\nand computation (with a reduction of up to $90\\%$ on training time and network\nparameters). Notwithstanding such training efficiency, extensive experiments\nshow that ST-TIS is substantially more accurate in online prediction than\nstate-of-the-art approaches (with an average improvement of up to $11\\%$ on\nRMSE, $14\\%$ on MAPE).",
          "link": "http://arxiv.org/abs/2201.00008",
          "publishedOn": "2022-01-05T00:39:36.885Z",
          "wordCount": null,
          "title": "A Lightweight and Accurate Spatial-Temporal Transformer for Traffic Forecasting. (arXiv:2201.00008v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00299",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Huaxiu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1\">Weixin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>",
          "description": "Machine learning algorithms typically assume that training and test examples\nare drawn from the same distribution. However, distribution shift is a common\nproblem in real-world applications and can cause models to perform dramatically\nworse at test time. In this paper, we specifically consider the problems of\ndomain shifts and subpopulation shifts (eg. imbalanced data). While prior works\noften seek to explicitly regularize internal representations and predictors of\nthe model to be domain invariant, we instead aim to regularize the whole\nfunction without restricting the model's internal representations. This leads\nto a simple mixup-based technique which learns invariant functions via\nselective augmentation called LISA. LISA selectively interpolates samples\neither with the same labels but different domains or with the same domain but\ndifferent labels. We analyze a linear setting and theoretically show how LISA\nleads to a smaller worst-group error. Empirically, we study the effectiveness\nof LISA on nine benchmarks ranging from subpopulation shifts to domain shifts,\nand we find that LISA consistently outperforms other state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2201.00299",
          "publishedOn": "2022-01-05T00:39:36.880Z",
          "wordCount": null,
          "title": "Improving Out-of-Distribution Robustness via Selective Augmentation. (arXiv:2201.00299v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00698",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nanzhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Qinzhuo Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Haibin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongxiao Zhang</a>",
          "description": "Large-scale or high-resolution geologic models usually comprise a huge number\nof grid blocks, which can be computationally demanding and time-consuming to\nsolve with numerical simulators. Therefore, it is advantageous to upscale\ngeologic models (e.g., hydraulic conductivity) from fine-scale (high-resolution\ngrids) to coarse-scale systems. Numerical upscaling methods have been proven to\nbe effective and robust for coarsening geologic models, but their efficiency\nremains to be improved. In this work, a deep-learning-based method is proposed\nto upscale the fine-scale geologic models, which can assist to improve\nupscaling efficiency significantly. In the deep learning method, a deep\nconvolutional neural network (CNN) is trained to approximate the relationship\nbetween the coarse grid of hydraulic conductivity fields and the hydraulic\nheads, which can then be utilized to replace the numerical solvers while\nsolving the flow equations for each coarse block. In addition, physical laws\n(e.g., governing equations and periodic boundary conditions) can also be\nincorporated into the training process of the deep CNN model, which is termed\nthe theory-guided convolutional neural network (TgCNN). With the physical\ninformation considered, dependence on the data volume of training the deep\nlearning models can be reduced greatly. Several subsurface flow cases are\nintroduced to test the performance of the proposed deep-learning-based\nupscaling method, including 2D and 3D cases, and isotropic and anisotropic\ncases. The results show that the deep learning method can provide equivalent\nupscaling accuracy to the numerical method, and efficiency can be improved\nsignificantly compared to numerical upscaling.",
          "link": "http://arxiv.org/abs/2201.00698",
          "publishedOn": "2022-01-05T00:39:36.880Z",
          "wordCount": null,
          "title": "Deep-learning-based upscaling method for geologic models via theory-guided convolutional neural network. (arXiv:2201.00698v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1\">Serguei Barannikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trofimov_I/0/1/0/all/0/1\">Ilya Trofimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balabin_N/0/1/0/all/0/1\">Nikita Balabin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>",
          "description": "Comparison of data representations is a complex multi-aspect problem that has\nnot enjoyed a complete solution yet. We propose a method for comparing two data\nrepresentations. We introduce the Representation Topology Divergence (RTD),\nmeasuring the dissimilarity in multi-scale topology between two point clouds of\nequal size with a one-to-one correspondence between points. The data point\nclouds are allowed to lie in different ambient spaces. The RTD is one of the\nfew TDA-based practical methods applicable to real machine learning datasets.\nExperiments show that the proposed RTD agrees with the intuitive assessment of\ndata representation similarity and is sensitive to its topological structure.\nWe apply RTD to gain insights on neural networks representations in computer\nvision and NLP domains for various problems: training dynamics analysis, data\ndistribution shift, transfer learning, ensemble learning, disentanglement\nassessment.",
          "link": "http://arxiv.org/abs/2201.00058",
          "publishedOn": "2022-01-05T00:39:36.876Z",
          "wordCount": null,
          "title": "Representation Topology Divergence: A Method for Comparing Neural Network Representations. (arXiv:2201.00058v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00730",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Sejourne_T/0/1/0/all/0/1\">Thibault S&#xe9;journ&#xe9;</a>, <a href=\"http://arxiv.org/find/math/1/au:+Vialard_F/0/1/0/all/0/1\">Fran&#xe7;ois-Xavier Vialard</a>, <a href=\"http://arxiv.org/find/math/1/au:+Peyre_G/0/1/0/all/0/1\">Gabriel Peyr&#xe9;</a>",
          "description": "Unbalanced optimal transport (UOT) extends optimal transport (OT) to take\ninto account mass variations to compare distributions. This is crucial to make\nOT successful in ML applications, making it robust to data normalization and\noutliers. The baseline algorithm is Sinkhorn, but its convergence speed might\nbe significantly slower for UOT than for OT. In this work, we identify the\ncause for this deficiency, namely the lack of a global normalization of the\niterates, which equivalently corresponds to a translation of the dual OT\npotentials. Our first contribution leverages this idea to develop a provably\naccelerated Sinkhorn algorithm (coined 'translation invariant Sinkhorn') for\nUOT, bridging the computational gap with OT. Our second contribution focusses\non 1-D UOT and proposes a Frank-Wolfe solver applied to this translation\ninvariant formulation. The linear oracle of each steps amounts to solving a 1-D\nOT problems, resulting in a linear time complexity per iteration. Our last\ncontribution extends this method to the computation of UOT barycenter of 1-D\nmeasures. Numerical simulations showcase the convergence speed improvement\nbrought by these three approaches.",
          "link": "http://arxiv.org/abs/2201.00730",
          "publishedOn": "2022-01-05T00:39:36.874Z",
          "wordCount": null,
          "title": "Faster Unbalanced Optimal Transport: Translation invariant Sinkhorn and 1-D Frank-Wolfe. (arXiv:2201.00730v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2007.06823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jospin_L/0/1/0/all/0/1\">Laurent Valentin Jospin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1\">Wray Buntine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boussaid_F/0/1/0/all/0/1\">Farid Boussaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laga_H/0/1/0/all/0/1\">Hamid Laga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>",
          "description": "Modern deep learning methods constitute incredibly powerful tools to tackle a\nmyriad of challenging problems. However, since deep learning methods operate as\nblack boxes, the uncertainty associated with their predictions is often\nchallenging to quantify. Bayesian statistics offer a formalism to understand\nand quantify the uncertainty associated with deep neural network predictions.\nThis tutorial provides an overview of the relevant literature and a complete\ntoolset to design, implement, train, use and evaluate Bayesian Neural Networks,\ni.e. Stochastic Artificial Neural Networks trained using Bayesian methods.",
          "link": "http://arxiv.org/abs/2007.06823",
          "publishedOn": "2022-01-05T00:39:36.872Z",
          "wordCount": null,
          "title": "Hands-on Bayesian Neural Networks -- a Tutorial for Deep Learning Users. (arXiv:2007.06823v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tabaghi_P/0/1/0/all/0/1\">Puoya Tabaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_E/0/1/0/all/0/1\">Eli Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jianhao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1\">Olgica Milenkovic</a>",
          "description": "Embedding methods for product spaces are powerful techniques for\nlow-distortion and low-dimensional representation of complex data structures.\nHere, we address the new problem of linear classification in product space\nforms -- products of Euclidean, spherical, and hyperbolic spaces. First, we\ndescribe novel formulations for linear classifiers on a Riemannian manifold\nusing geodesics and Riemannian metrics which generalize straight lines and\ninner products in vector spaces. Second, we prove that linear classifiers in\n$d$-dimensional space forms of any curvature have the same expressive power,\ni.e., they can shatter exactly $d+1$ points. Third, we formalize linear\nclassifiers in product space forms, describe the first known perceptron and\nsupport vector machine classifiers for such spaces and establish rigorous\nconvergence results for perceptrons. Moreover, we prove that the\nVapnik-Chervonenkis dimension of linear classifiers in a product space form of\ndimension $d$ is \\emph{at least} $d+1$. We support our theoretical findings\nwith simulations on several datasets, including synthetic data, image data, and\nsingle-cell RNA sequencing (scRNA-seq) data. The results show that\nclassification in low-dimensional product space forms for scRNA-seq data\noffers, on average, a performance improvement of $\\sim15\\%$ when compared to\nthat in Euclidean spaces of the same dimension.",
          "link": "http://arxiv.org/abs/2102.10204",
          "publishedOn": "2022-01-05T00:39:36.872Z",
          "wordCount": null,
          "title": "Linear Classifiers in Product Space Forms. (arXiv:2102.10204v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00168",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian-wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xi-hao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Run-kun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xionglin Luo</a>",
          "description": "Multi-view learning attempts to generate a model with a better performance by\nexploiting the consensus and/or complementarity among multi-view data. However,\nin terms of complementarity, most existing approaches only can find\nrepresentations with single complementarity rather than complementary\ninformation with diversity. In this paper, to utilize both complementarity and\nconsistency simultaneously, give free rein to the potential of deep learning in\ngrasping diversity-promoting complementarity for multi-view representation\nlearning, we propose a novel supervised multi-view representation learning\nalgorithm, called Self-Attention Multi-View network with Diversity-Promoting\nComplementarity (SAMVDPC), which exploits the consistency by a group of\nencoders, uses self-attention to find complementary information entailing\ndiversity. Extensive experiments conducted on eight real-world datasets have\ndemonstrated the effectiveness of our proposed method, and show its superiority\nover several baseline methods, which only consider single complementary\ninformation.",
          "link": "http://arxiv.org/abs/2201.00168",
          "publishedOn": "2022-01-05T00:39:36.869Z",
          "wordCount": null,
          "title": "Self-attention Multi-view Representation Learning with Diversity-promoting Complementarity. (arXiv:2201.00168v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2005.08081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>",
          "description": "In sequence-to-sequence learning, e.g., natural language generation, the\ndecoder relies on the attention mechanism to efficiently extract information\nfrom the encoder. While it is common practice to draw information from only the\nlast encoder layer, recent work has proposed to use representations from\ndifferent encoder layers for diversified levels of information. Nonetheless,\nthe decoder still obtains only a single view of the source sequences, which\nmight lead to insufficient training of the encoder layer stack due to the\nhierarchy bypassing problem. In this work, we propose layer-wise multi-view\ndecoding, where for each decoder layer, together with the representations from\nthe last encoder layer, which serve as a global view, those from other encoder\nlayers are supplemented for a stereoscopic view of the source sequences.\nSystematic experiments and analyses show that we successfully address the\nhierarchy bypassing problem, require almost negligible parameter increase, and\nsubstantially improve the performance of sequence-to-sequence learning with\ndeep representations on five diverse tasks, i.e., machine translation,\nabstractive summarization, image captioning, video captioning, and medical\nreport generation. In particular, our approach achieves new state-of-the-art\nresults on eight benchmark datasets, including a low-resource machine\ntranslation dataset and two low-resource medical report generation datasets.",
          "link": "http://arxiv.org/abs/2005.08081",
          "publishedOn": "2022-01-05T00:39:36.869Z",
          "wordCount": null,
          "title": "Layer-Wise Multi-View Decoding for Improved Natural Language Generation. (arXiv:2005.08081v6 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00798",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Zhang_P/0/1/0/all/0/1\">Puhan Zhang</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Chern_G/0/1/0/all/0/1\">Gia-Wei Chern</a>",
          "description": "We outline the general framework of machine learning (ML) methods for\nmulti-scale dynamical modeling of condensed matter systems, and in particular\nof strongly correlated electron models. Complex spatial temporal behaviors in\nthese systems often arise from the interplay between quasi-particles and the\nemergent dynamical classical degrees of freedom, such as local lattice\ndistortions, spins, and order-parameters. Central to the proposed framework is\nthe ML energy model that, by successfully emulating the time-consuming\nelectronic structure calculation, can accurately predict a local energy based\non the classical field in the intermediate neighborhood. In order to properly\ninclude the symmetry of the electron Hamiltonian, a crucial component of the ML\nenergy model is the descriptor that transforms the neighborhood configuration\ninto invariant feature variables, which are input to the learning model. A\ngeneral theory of the descriptor for the classical fields is formulated, and\ntwo types of models are distinguished depending on the presence or absence of\nan internal symmetry for the classical field. Several specific approaches to\nthe descriptor of the classical fields are presented. Our focus is on the\ngroup-theoretical method that offers a systematic and rigorous approach to\ncompute invariants based on the bispectrum coefficients. We propose an\nefficient implementation of the bispectrum method based on the concept of\nreference irreducible representations. Finally, the implementations of the\nvarious descriptors are demonstrated on well-known electronic lattice models.",
          "link": "http://arxiv.org/abs/2201.00798",
          "publishedOn": "2022-01-05T00:39:36.845Z",
          "wordCount": null,
          "title": "Descriptors for Machine Learning Model of Generalized Force Field in Condensed Matter Systems. (arXiv:2201.00798v1 [cond-mat.str-el])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Szczecinski_L/0/1/0/all/0/1\">Leszek Szczecinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roatis_I/0/1/0/all/0/1\">Iris-Ioana Roatis</a>",
          "description": "In this work we study the ranking algorithm used by F\\'ed\\'eration\nInternationale de Football Association (FIFA); we analyze the parameters it\ncurrently uses, show the formal probabilistic model from which it can be\nderived, and optimize the latter. In particular, analyzing the games since the\nintroduction of the algorithm in 2018, we conclude that the game's \"importance\"\n(as defined by FIFA) used in the algorithm is counterproductive from the point\nof view of the predictive capability of the algorithm. We also postulate the\nalgorithm to be rooted in the formal modelling principle, where the Davidson\nmodel proposed in 1970 seems to be an excellent candidate, preserving the form\nof the algorithm currently used. The results indicate that the predictive\ncapability of the algorithm is notably improved by using the home-field\nadvantage and the explicit model for the draws in the game. Moderate, but\nnotable improvement may be attained by introducing the weighting of the results\nwith the goal differential, which although not rooted in a formal modelling\nprinciple, is compatible with the current algorithm and can be tuned to the\ncharacteristics of the football competition.",
          "link": "http://arxiv.org/abs/2201.00691",
          "publishedOn": "2022-01-05T00:39:36.835Z",
          "wordCount": null,
          "title": "FIFA ranking: Evaluation and path forward. (arXiv:2201.00691v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh-Dat Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Cong_T/0/1/0/all/0/1\">Thanh Le-Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">ThanhVu H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_X/0/1/0/all/0/1\">Xuan-Bach D. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_Q/0/1/0/all/0/1\">Quyet-Thang Huynh</a>",
          "description": "Graph Neural Networks (GNNs) have recently emerged as a robust framework for\ngraph-structured data. They have been applied to many problems such as\nknowledge graph analysis, social networks recommendation, and even Covid19\ndetection and vaccine developments. However, unlike other deep neural networks\nsuch as Feed Forward Neural Networks (FFNNs), few analyses such as verification\nand property inferences exist, potentially due to dynamic behaviors of GNNs,\nwhich can take arbitrary graphs as input, whereas FFNNs which only take fixed\nsize numerical vectors as inputs.\n\nThis paper proposes an approach to analyze GNNs by converting them into FFNNs\nand reusing existing FFNNs analyses. We discuss various designs to ensure the\nscalability and accuracy of the conversions. We illustrate our method on a\nstudy case of node classification. We believe that our approach opens new\nresearch directions for understanding and analyzing GNNs.",
          "link": "http://arxiv.org/abs/2201.00115",
          "publishedOn": "2022-01-05T00:39:36.832Z",
          "wordCount": null,
          "title": "Toward the Analysis of Graph Neural Networks. (arXiv:2201.00115v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2112.10775",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_M/0/1/0/all/0/1\">Meirui Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>",
          "description": "Multiple medical institutions collaboratively training a model using\nfederated learning (FL) has become a promising solution for maximizing the\npotential of data-driven models, yet the non-independent and identically\ndistributed (non-iid) data in medical images is still an outstanding challenge\nin real-world practice. The feature heterogeneity caused by diverse scanners or\nprotocols introduces a drift in the learning process, in both local (client)\nand global (server) optimizations, which harms the convergence as well as model\nperformance. Many previous works have attempted to address the non-iid issue by\ntackling the drift locally or globally, but how to jointly solve the two\nessentially coupled drifts is still unclear. In this work, we concentrate on\nhandling both local and global drifts and introduce a new harmonizing framework\ncalled HarmoFL. First, we propose to mitigate the local update drift by\nnormalizing amplitudes of images transformed into the frequency domain to mimic\na unified imaging setting, in order to generate a harmonized feature space\nacross local clients. Second, based on harmonized features, we design a client\nweight perturbation guiding each local model to reach a flat optimum, where a\nneighborhood area of the local optimal solution has a uniformly low loss.\nWithout any extra communication cost, the perturbation assists the global model\nto optimize towards a converged optimal solution by aggregating several local\nflat optima. We have theoretically analyzed the proposed method and empirically\nconducted extensive experiments on three medical image classification and\nsegmentation tasks, showing that HarmoFL outperforms a set of recent\nstate-of-the-art methods with promising convergence behavior. Code is available\nat https://github.com/med-air/HarmoFL.",
          "link": "http://arxiv.org/abs/2112.10775",
          "publishedOn": "2022-01-05T00:39:36.830Z",
          "wordCount": null,
          "title": "HarmoFL: Harmonizing Local and Global Drifts in Federated Learning on Heterogeneous Medical Images. (arXiv:2112.10775v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingzhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hailong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_Z/0/1/0/all/0/1\">Zhongzhi Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_D/0/1/0/all/0/1\">Depei Qian</a>",
          "description": "Deploying various deep learning (DL) models efficiently has boosted the\nresearch on DL compilers. The difficulty of generating optimized tensor codes\ndrives DL compiler to ask for the auto-tuning approaches, and the increasing\ndemands require increasing auto-tuning efficiency and quality. Currently, the\nDL compilers partition the input DL models into several subgraphs and leverage\nthe auto-tuning to find the optimal tensor codes of these subgraphs. However,\nexisting auto-tuning approaches usually regard subgraphs as individual ones and\noverlook the similarities across them, and thus fail to exploit better tensor\ncodes under limited time budgets. We propose FamilySeer, an auto-tuning\nframework for DL compilers that can generate better tensor codes even with\nlimited time budgets. FamilySeer exploits the similarities and differences\namong subgraphs can organize them into subgraph families, where the tuning of\none subgraph can also improve other subgraphs within the same family. The cost\nmodel of each family gets more purified training samples generated by the\nfamily and becomes more accurate so that the costly measurements on real\nhardware can be replaced with the lightweight estimation through cost model.\nOur experiments show that FamilySeer can generate model codes with the same\ncode performance more efficiently than state-of-the-art auto-tuning frameworks.",
          "link": "http://arxiv.org/abs/2201.00194",
          "publishedOn": "2022-01-05T00:39:36.829Z",
          "wordCount": null,
          "title": "FamilySeer: Towards Optimized Tensor Codes by Exploiting Computation Subgraph Similarity. (arXiv:2201.00194v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08966",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sigrist_F/0/1/0/all/0/1\">Fabio Sigrist</a>",
          "description": "Latent Gaussian models and boosting are widely used techniques in statistics\nand machine learning. Tree-boosting shows excellent prediction accuracy on many\ndata sets, but potential drawbacks are that it assumes conditional independence\nof samples, produces discontinuous predictions for, e.g., spatial data, and it\ncan have difficulty with high-cardinality categorical variables. Latent\nGaussian models, such as Gaussian process and grouped random effects models,\nare flexible prior models which explicitly model dependence among samples and\nwhich allow for efficient learning of predictor functions and for making\nprobabilistic predictions. However, existing latent Gaussian models usually\nassume either a zero or a linear prior mean function which can be an\nunrealistic assumption. This article introduces a novel approach that combines\nboosting and latent Gaussian models to remedy the above-mentioned drawbacks and\nto leverage the advantages of both techniques. We obtain increased prediction\naccuracy compared to existing approaches in both simulated and real-world data\nexperiments.",
          "link": "http://arxiv.org/abs/2105.08966",
          "publishedOn": "2022-01-05T00:39:36.829Z",
          "wordCount": null,
          "title": "Latent Gaussian Model Boosting. (arXiv:2105.08966v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00627",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Duan_T/0/1/0/all/0/1\">Tiehang Duan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenyi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Srihari_S/0/1/0/all/0/1\">Sargur N. Srihari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Hui Yang</a>",
          "description": "EEG decoding systems based on deep neural networks have been widely used in\ndecision making of brain computer interfaces (BCI). Their predictions, however,\ncan be unreliable given the significant variance and noise in EEG signals.\nPrevious works on EEG analysis mainly focus on the exploration of noise pattern\nin the source signal, while the uncertainty during the decoding process is\nlargely unexplored. Automatically detecting and quantifying such decoding\nuncertainty is important for BCI motor imagery applications such as robotic arm\ncontrol etc. In this work, we proposed an uncertainty estimation model (UE-EEG)\nto explore the uncertainty during the EEG decoding process, which considers\nboth the uncertainty in the input signal and the uncertainty in the model. The\nmodel utilized dropout oriented method for model uncertainty estimation, and\nBayesian neural network is adopted for modeling the uncertainty of input data.\nThe model can be integrated into current widely used deep learning classifiers\nwithout change of architecture. We performed extensive experiments for\nuncertainty estimation in both intra-subject EEG decoding and cross-subject EEG\ndecoding on two public motor imagery datasets, where the proposed model\nachieves significant improvement on the quality of estimated uncertainty and\ndemonstrates the proposed UE-EEG is a useful tool for BCI applications.",
          "link": "http://arxiv.org/abs/2201.00627",
          "publishedOn": "2022-01-05T00:39:36.824Z",
          "wordCount": null,
          "title": "Uncertainty Detection in EEG Neural Decoding Models. (arXiv:2201.00627v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2002.03781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yancey_R/0/1/0/all/0/1\">Robin Elizabeth Yancey</a>",
          "description": "Each woman living in the United States has about 1 in 8 chance of developing\ninvasive breast cancer. The mitotic cell count is one of the most common tests\nto assess the aggressiveness or grade of breast cancer. In this prognosis,\nhistopathology images must be examined by a pathologist using high-resolution\nmicroscopes to count the cells. Unfortunately, can be an exhaustive task with\npoor reproducibility, especially for non-experts. Deep learning networks have\nrecently been adapted to medical applications which are able to automatically\nlocalize these regions of interest. However, these region-based networks lack\nthe ability to take advantage of the segmentation features produced by a full\nimage CNN which are often used as a sole method of detection. Therefore, the\nproposed method leverages Faster RCNN for object detection while fusing\nsegmentation features generated by a UNet with RGB image features to achieve an\nF-score of 0.508 on the MITOS-ATYPIA 2014 mitosis counting challenge dataset,\noutperforming state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2002.03781",
          "publishedOn": "2022-01-05T00:39:36.816Z",
          "wordCount": null,
          "title": "Deep Feature Fusion for Mitosis Counting. (arXiv:2002.03781v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00286",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raman_A/0/1/0/all/0/1\">Arun Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shagrithaya_K/0/1/0/all/0/1\">Keerthan Shagrithaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatnagar_S/0/1/0/all/0/1\">Shalabh Bhatnagar</a>",
          "description": "In this paper, we use concepts from supervisory control theory of discrete\nevent systems to propose a method to learn optimal control policies for a\nfinite-state Markov Decision Process (MDP) in which (only) certain sequences of\nactions are deemed unsafe (respectively safe). We assume that the set of action\nsequences that are deemed unsafe and/or safe are given in terms of a\nfinite-state automaton; and propose a supervisor that disables a subset of\nactions at every state of the MDP so that the constraints on action sequence\nare satisfied. Then we present a version of the Q-learning algorithm for\nlearning optimal policies in the presence of non-Markovian action-sequence and\nstate constraints, where we use the development of reward machines to handle\nthe state constraints. We illustrate the method using an example that captures\nthe utility of automata-based methods for non-Markovian state and action\nspecifications for reinforcement learning and show the results of simulations\nin this setting.",
          "link": "http://arxiv.org/abs/2201.00286",
          "publishedOn": "2022-01-05T00:39:36.805Z",
          "wordCount": null,
          "title": "Reinforcement Learning for Task Specifications with Action-Constraints. (arXiv:2201.00286v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2111.14760",
          "author": "<a href=\"http://arxiv.org/find/hep-ph/1/au:+Wang_L/0/1/0/all/0/1\">Lingxiao Wang</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Shi_S/0/1/0/all/0/1\">Shuzhe Shi</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Zhou_K/0/1/0/all/0/1\">Kai Zhou</a>",
          "description": "Reconstructing spectral functions from Euclidean Green's functions is an\nimportant inverse problem in many-body physics. However, the inversion is\nproved to be ill-posed in the realistic systems with noisy Green's functions.\nIn this Letter, we propose an automatic differentiation(AD) framework as a\ngeneric tool for the spectral reconstruction from propagator observable.\nExploiting the neural networks' regularization as a non-local smoothness\nregulator of the spectral function, we represent spectral functions by neural\nnetworks and use the propagator's reconstruction error to optimize the network\nparameters unsupervisedly. In the training process, except for the\npositive-definite form for the spectral function, there are no other explicit\nphysical priors embedded into the neural networks. The reconstruction\nperformance is assessed through relative entropy and mean square error for two\ndifferent network representations. Compared to the maximum entropy method, the\nAD framework achieves better performance in the large-noise situation. It is\nnoted that the freedom of introducing non-local regularization is an inherent\nadvantage of the present framework and may lead to substantial improvements in\nsolving inverse problems.",
          "link": "http://arxiv.org/abs/2111.14760",
          "publishedOn": "2022-01-05T00:39:36.792Z",
          "wordCount": null,
          "title": "Reconstructing spectral functions via automatic differentiation. (arXiv:2111.14760v2 [hep-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.05002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bonyadi_M/0/1/0/all/0/1\">Mohammad Reza Bonyadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziaei_M/0/1/0/all/0/1\">Maryam Ziaei</a>",
          "description": "Reinforcement learning agents learn by encouraging behaviours which maximize\ntheir total reward, usually provided by the environment. In many environments,\nhowever, the reward is provided after a series of actions rather than each\nsingle action, leading the agent to experience ambiguity in terms of whether\nthose actions are effective, an issue known as the credit assignment problem.\nIn this paper, we propose two strategies inspired by behavioural psychology to\nenable the agent to intrinsically estimate more informative reward values for\nactions with no reward. The first strategy, called self-punishment (SP),\ndiscourages the agent from making mistakes that lead to undesirable terminal\nstates. The second strategy, called the rewards backfill (RB), backpropagates\nthe rewards between two rewarded actions. We prove that, under certain\nassumptions and regardless of the reinforcement learning algorithm used, these\ntwo strategies maintain the order of policies in the space of all possible\npolicies in terms of their total reward, and, by extension, maintain the\noptimal policy. Hence, our proposed strategies integrate with any reinforcement\nlearning algorithm that learns a value or action-value function through\nexperience. We incorporated these two strategies into three popular deep\nreinforcement learning approaches and evaluated the results on thirty Atari\ngames. After parameter tuning, our results indicate that the proposed\nstrategies improve the tested methods in over 65 percent of tested games by up\nto over 25 times performance improvement.",
          "link": "http://arxiv.org/abs/2004.05002",
          "publishedOn": "2022-01-05T00:39:36.758Z",
          "wordCount": null,
          "title": "Self Punishment and Reward Backfill for Deep Q-Learning. (arXiv:2004.05002v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00402",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lu_H/0/1/0/all/0/1\">Han Lu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_Z/0/1/0/all/0/1\">Zenan Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wang_R/0/1/0/all/0/1\">Runzhong Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ren_Q/0/1/0/all/0/1\">Qibing Ren</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>",
          "description": "Combinatorial optimization (CO) is a long-standing challenging task not only\nin its inherent complexity (e.g. NP-hard) but also the possible sensitivity to\ninput conditions. In this paper, we take an initiative on developing the\nmechanisms for adversarial attack and defense towards combinatorial\noptimization solvers, whereby the solver is treated as a black-box function and\nthe original problem's underlying graph structure (which is often available and\nassociated with the problem instance, e.g. DAG, TSP) is attacked under a given\nbudget. In particular, we present a simple yet effective defense strategy to\nmodify the graph structure to increase the robustness of solvers, which shows\nits universal effectiveness across tasks and solvers.",
          "link": "http://arxiv.org/abs/2201.00402",
          "publishedOn": "2022-01-05T00:39:36.720Z",
          "wordCount": null,
          "title": "Mind Your Solver! On Adversarial Attack and Defense for Combinatorial Optimization. (arXiv:2201.00402v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12906",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jafarzadeh_M/0/1/0/all/0/1\">Mohsen Jafarzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamija_A/0/1/0/all/0/1\">Akshay Raj Dhamija</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_S/0/1/0/all/0/1\">Steve Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunchun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_T/0/1/0/all/0/1\">Touqeer Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boult_T/0/1/0/all/0/1\">Terrance E. Boult</a>",
          "description": "In open-world learning, an agent starts with a set of known classes, detects,\nand manages things that it does not know, and learns them over time from a\nnon-stationary stream of data. Open-world learning is related to but also\ndistinct from a multitude of other learning problems and this paper briefly\nanalyzes the key differences between a wide range of problems including\nincremental learning, generalized novelty discovery, and generalized zero-shot\nlearning. This paper formalizes various open-world learning problems including\nopen-world learning without labels. These open-world problems can be addressed\nwith modifications to known elements, we present a new framework that enables\nagents to combine various modules for novelty-detection,\nnovelty-characterization, incremental learning, and instance management to\nlearn new classes from a stream of unlabeled data in an unsupervised manner,\nsurvey how to adapt a few state-of-the-art techniques to fit the framework and\nuse them to define seven baselines for performance on the open-world learning\nwithout labels problem. We then discuss open-world learning quality and analyze\nhow that can improve instance management. We also discuss some of the general\nambiguity issues that occur in open-world learning without labels.",
          "link": "http://arxiv.org/abs/2011.12906",
          "publishedOn": "2022-01-05T00:39:36.720Z",
          "wordCount": null,
          "title": "A Review of Open-World Learning and Steps Toward Open-World Learning Without Labels. (arXiv:2011.12906v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01734",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Passino_F/0/1/0/all/0/1\">Francesco Sanna Passino</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Heard_N/0/1/0/all/0/1\">Nicholas A. Heard</a>",
          "description": "Spectral embedding of network adjacency matrices often produces node\nrepresentations living approximately around low-dimensional submanifold\nstructures. In particular, hidden substructure is expected to arise when the\ngraph is generated from a latent position model. Furthermore, the presence of\ncommunities within the network might generate community-specific submanifold\nstructures in the embedding, but this is not explicitly accounted for in most\nstatistical models for networks. In this article, a class of models called\nlatent structure block models (LSBM) is proposed to address such scenarios,\nallowing for graph clustering when community-specific one dimensional manifold\nstructure is present. LSBMs focus on a specific class of latent space model,\nthe random dot product graph (RDPG), and assign a latent submanifold to the\nlatent positions of each community. A Bayesian model for the embeddings arising\nfrom LSBMs is discussed, and shown to have a good performance on simulated and\nreal world network data. The model is able to correctly recover the underlying\ncommunities living in a one-dimensional manifold, even when the parametric form\nof the underlying curves is unknown, achieving remarkable results on a variety\nof real data.",
          "link": "http://arxiv.org/abs/2107.01734",
          "publishedOn": "2022-01-05T00:39:36.711Z",
          "wordCount": null,
          "title": "Latent structure blockmodels for Bayesian spectral graph clustering. (arXiv:2107.01734v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_A/0/1/0/all/0/1\">Alex Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Derek He</a>",
          "description": "Meta-learning traditionally relies on backpropagation through entire tasks to\niteratively improve a model's learning dynamics. However, this approach is\ncomputationally intractable when scaled to complex tasks. We propose a\ndistributed evolutionary meta-learning strategy using Tensor Processing Units\n(TPUs) that is highly parallel and scalable to arbitrarily long tasks with no\nincrease in memory cost. Using a Prototypical Network trained with evolution\nstrategies on the Omniglot dataset, we achieved an accuracy of 98.4% on a\n5-shot classification problem. Our algorithm used as much as 40 times less\nmemory than automatic differentiation to compute the gradient, with the\nresulting model achieving accuracy within 1.3% of a backpropagation-trained\nequivalent (99.6%). We observed better classification accuracy as high as 99.1%\nwith larger population configurations. We further experimentally validate the\nstability and performance of ES-ProtoNet across a variety of training\nconditions (varying population size, model size, number of workers, shot, way,\nES hyperparameters, etc.). Our contributions are twofold: we provide the first\nassessment of evolutionary meta-learning in a supervised setting, and create a\ngeneral framework for distributed evolution strategies on TPUs.",
          "link": "http://arxiv.org/abs/2201.00093",
          "publishedOn": "2022-01-05T00:39:36.710Z",
          "wordCount": null,
          "title": "Distributed Evolution Strategies Using TPUs for Meta-Learning. (arXiv:2201.00093v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00087",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Chung_M/0/1/0/all/0/1\">Moo K. Chung</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_S/0/1/0/all/0/1\">Shih-Gu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Carroll_I/0/1/0/all/0/1\">Ian C. Carroll</a>, <a href=\"http://arxiv.org/find/math/1/au:+Calhoun_V/0/1/0/all/0/1\">Vince D. Calhoun</a>, <a href=\"http://arxiv.org/find/math/1/au:+Goldsmith_H/0/1/0/all/0/1\">H. Hill Goldsmith</a>",
          "description": "We present the novel Wasserstein graph clustering for dynamically changing\ngraphs. The Wasserstein clustering penalizes the topological discrepancy\nbetween graphs. The Wasserstein clustering is shown to outperform the widely\nused k-means clustering. The method applied in more accurate determination of\nthe state spaces of dynamically changing functional brain networks.",
          "link": "http://arxiv.org/abs/2201.00087",
          "publishedOn": "2022-01-05T00:39:36.680Z",
          "wordCount": null,
          "title": "Dynamic Persistent Homology for Brain Networks via Wasserstein Graph Clustering. (arXiv:2201.00087v1 [math.AT])"
        },
        {
          "id": "http://arxiv.org/abs/2109.10259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yihang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>",
          "description": "Contrastive learning has been widely applied to graph representation\nlearning, where the view generators play a vital role in generating effective\ncontrastive samples. Most of the existing contrastive learning methods employ\npre-defined view generation methods, e.g., node drop or edge perturbation,\nwhich usually cannot adapt to input data or preserve the original semantic\nstructures well. To address this issue, we propose a novel framework named\nAutomated Graph Contrastive Learning (AutoGCL) in this paper. Specifically,\nAutoGCL employs a set of learnable graph view generators orchestrated by an\nauto augmentation strategy, where every graph view generator learns a\nprobability distribution of graphs conditioned by the input. While the graph\nview generators in AutoGCL preserve the most representative structures of the\noriginal graph in generation of every contrastive sample, the auto augmentation\nlearns policies to introduce adequate augmentation variances in the whole\ncontrastive learning procedure. Furthermore, AutoGCL adopts a joint training\nstrategy to train the learnable view generators, the graph encoder, and the\nclassifier in an end-to-end manner, resulting in topological heterogeneity yet\nsemantic similarity in the generation of contrastive samples. Extensive\nexperiments on semi-supervised learning, unsupervised learning, and transfer\nlearning demonstrate the superiority of our AutoGCL framework over the\nstate-of-the-arts in graph contrastive learning. In addition, the visualization\nresults further confirm that the learnable view generators can deliver more\ncompact and semantically meaningful contrastive samples compared against the\nexisting view generation methods.",
          "link": "http://arxiv.org/abs/2109.10259",
          "publishedOn": "2022-01-05T00:39:36.680Z",
          "wordCount": null,
          "title": "AutoGCL: Automated Graph Contrastive Learning via Learnable View Generators. (arXiv:2109.10259v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00622",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Huang_J/0/1/0/all/0/1\">Jessie Huang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Busch_E/0/1/0/all/0/1\">Erica L. Busch</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wallenstein_T/0/1/0/all/0/1\">Tom Wallenstein</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gerasimiuk_M/0/1/0/all/0/1\">Michal Gerasimiuk</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Benz_A/0/1/0/all/0/1\">Andrew Benz</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lajoie_G/0/1/0/all/0/1\">Guillaume Lajoie</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wolf_G/0/1/0/all/0/1\">Guy Wolf</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Turk_Browne_N/0/1/0/all/0/1\">Nicholas B. Turk-Browne</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Krishnaswamy_S/0/1/0/all/0/1\">Smita Krishnaswamy</a>",
          "description": "Functional magnetic resonance imaging (fMRI) is a notoriously noisy\nmeasurement of brain activity because of the large variations between\nindividuals, signals marred by environmental differences during collection, and\nspatiotemporal averaging required by the measurement resolution. In addition,\nthe data is extremely high dimensional, with the space of the activity\ntypically having much lower intrinsic dimension. In order to understand the\nconnection between stimuli of interest and brain activity, and analyze\ndifferences and commonalities between subjects, it becomes important to learn a\nmeaningful embedding of the data that denoises, and reveals its intrinsic\nstructure. Specifically, we assume that while noise varies significantly\nbetween individuals, true responses to stimuli will share common,\nlow-dimensional features between subjects which are jointly discoverable.\nSimilar approaches have been exploited previously but they have mainly used\nlinear methods such as PCA and shared response modeling (SRM). In contrast, we\npropose a neural network called MRMD-AE (manifold-regularized multiple decoder,\nautoencoder), that learns a common embedding from multiple subjects in an\nexperiment while retaining the ability to decode to individual raw fMRI\nsignals. We show that our learned common space represents an extensible\nmanifold (where new points not seen during training can be mapped), improves\nthe classification accuracy of stimulus features of unseen timepoints, as well\nas improves cross-subject translation of fMRI signals. We believe this\nframework can be used for many downstream applications such as guided\nbrain-computer interface (BCI) training in the future.",
          "link": "http://arxiv.org/abs/2201.00622",
          "publishedOn": "2022-01-05T00:39:36.650Z",
          "wordCount": null,
          "title": "Learning shared neural manifolds from multi-subject FMRI data. (arXiv:2201.00622v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00387",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Wei_L/0/1/0/all/0/1\">Linchuan Wei</a>, <a href=\"http://arxiv.org/find/math/1/au:+Atamturk_A/0/1/0/all/0/1\">Alper Atamt&#xfc;rk</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gomez_A/0/1/0/all/0/1\">Andr&#xe9;s G&#xf3;mez</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kucukyavuz_S/0/1/0/all/0/1\">Simge K&#xfc;&#xe7;&#xfc;kyavuz</a>",
          "description": "We consider the convex quadratic optimization problem with indicator\nvariables and arbitrary constraints on the indicators. We show that a convex\nhull description of the associated mixed-integer set in an extended space with\na quadratic number of additional variables consists of a single positive\nsemidefinite constraint (explicitly stated) and linear constraints. In\nparticular, convexification of this class of problems reduces to describing a\npolyhedral set in an extended formulation. We also give descriptions in the\noriginal space of variables: we provide a description based on an infinite\nnumber of conic-quadratic inequalities, which are \"finitely generated.\" In\nparticular, it is possible to characterize whether a given inequality is\nnecessary to describe the convex-hull. The new theory presented here unifies\nseveral previously established results, and paves the way toward utilizing\npolyhedral methods to analyze the convex hull of mixed-integer nonlinear sets.",
          "link": "http://arxiv.org/abs/2201.00387",
          "publishedOn": "2022-01-05T00:39:36.649Z",
          "wordCount": null,
          "title": "On the convex hull of convex quadratic optimization problems with indicators. (arXiv:2201.00387v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bell_O/0/1/0/all/0/1\">Okezue Bell</a>",
          "description": "In recent years, optimization problems have become increasingly more\nprevalent due to the need for more powerful computational methods. With the\nmore recent advent of technology such as artificial intelligence, new\nmetaheuristics are needed that enhance the capabilities of classical\nalgorithms. More recently, researchers have been looking at Charles Darwin's\ntheory of natural selection and evolution as a means of enhancing current\napproaches using machine learning. In 1960, the first genetic algorithm was\ndeveloped by John H. Holland and his student. We explore the mathematical\nintuition of the genetic algorithm in developing systems capable of evolving\nusing Gaussian mutation, as well as its implications in solving optimization\nproblems.",
          "link": "http://arxiv.org/abs/2201.00285",
          "publishedOn": "2022-01-05T00:39:36.648Z",
          "wordCount": null,
          "title": "Applications of Gaussian Mutation for Self Adaptation in Evolutionary Genetic Algorithms. (arXiv:2201.00285v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Omar_M/0/1/0/all/0/1\">Marwan Omar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Soohyeon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyang_D/0/1/0/all/0/1\">DaeHun Nyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohaisen_D/0/1/0/all/0/1\">David Mohaisen</a>",
          "description": "Recent natural language processing (NLP) techniques have accomplished high\nperformance on benchmark datasets, primarily due to the significant improvement\nin the performance of deep learning. The advances in the research community\nhave led to great enhancements in state-of-the-art production systems for NLP\ntasks, such as virtual assistants, speech recognition, and sentiment analysis.\nHowever, such NLP systems still often fail when tested with adversarial\nattacks. The initial lack of robustness exposed troubling gaps in current\nmodels' language understanding capabilities, creating problems when NLP systems\nare deployed in real life. In this paper, we present a structured overview of\nNLP robustness research by summarizing the literature in a systemic way across\nvarious dimensions. We then take a deep-dive into the various dimensions of\nrobustness, across techniques, metrics, embeddings, and benchmarks. Finally, we\nargue that robustness should be multi-dimensional, provide insights into\ncurrent research, identify gaps in the literature to suggest directions worth\npursuing to address these gaps.",
          "link": "http://arxiv.org/abs/2201.00768",
          "publishedOn": "2022-01-05T00:39:36.646Z",
          "wordCount": null,
          "title": "Robust Natural Language Processing: Recent Advances, Challenges, and Future Directions. (arXiv:2201.00768v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2003.05783",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nadjahi_K/0/1/0/all/0/1\">Kimia Nadjahi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1\">Alain Durmus</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chizat_L/0/1/0/all/0/1\">L&#xe9;na&#xef;c Chizat</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kolouri_S/0/1/0/all/0/1\">Soheil Kolouri</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shahrampour_S/0/1/0/all/0/1\">Shahin Shahrampour</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Simsekli_U/0/1/0/all/0/1\">Umut &#x15e;im&#x15f;ekli</a>",
          "description": "The idea of slicing divergences has been proven to be successful when\ncomparing two probability measures in various machine learning applications\nincluding generative modeling, and consists in computing the expected value of\na `base divergence' between one-dimensional random projections of the two\nmeasures. However, the topological, statistical, and computational consequences\nof this technique have not yet been well-established. In this paper, we aim at\nbridging this gap and derive various theoretical properties of sliced\nprobability divergences. First, we show that slicing preserves the metric\naxioms and the weak continuity of the divergence, implying that the sliced\ndivergence will share similar topological properties. We then precise the\nresults in the case where the base divergence belongs to the class of integral\nprobability metrics. On the other hand, we establish that, under mild\nconditions, the sample complexity of a sliced divergence does not depend on the\nproblem dimension. We finally apply our general results to several base\ndivergences, and illustrate our theory on both synthetic and real data\nexperiments.",
          "link": "http://arxiv.org/abs/2003.05783",
          "publishedOn": "2022-01-05T00:39:36.645Z",
          "wordCount": null,
          "title": "Statistical and Topological Properties of Sliced Probability Divergences. (arXiv:2003.05783v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Sungwon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Hyeonho Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungwon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_M/0/1/0/all/0/1\">Meeyoung Cha</a>",
          "description": "Anomaly detection aims at identifying deviant instances from the normal data\ndistribution. Many advances have been made in the field, including the\ninnovative use of unsupervised contrastive learning. However, existing methods\ngenerally assume clean training data and are limited when the data contain\nunknown anomalies. This paper presents Elsa, a novel semi-supervised anomaly\ndetection approach that unifies the concept of energy-based models with\nunsupervised contrastive learning. Elsa instills robustness against any data\ncontamination by a carefully designed fine-tuning step based on the new energy\nfunction that forces the normal data to be divided into classes of prototypes.\nExperiments on multiple contamination scenarios show the proposed model\nachieves SOTA performance. Extensive analyses also verify the contribution of\neach component in the proposed model. Beyond the experiments, we also offer a\ntheoretical interpretation of why contrastive learning alone cannot detect\nanomalies under data contamination.",
          "link": "http://arxiv.org/abs/2103.15296",
          "publishedOn": "2022-01-05T00:39:36.644Z",
          "wordCount": null,
          "title": "Elsa: Energy-based learning for semi-supervised anomaly detection. (arXiv:2103.15296v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belkin_M/0/1/0/all/0/1\">Mikhail Belkin</a>",
          "description": "Modern machine learning systems such as deep neural networks are often highly\nover-parameterized so that they can fit the noisy training data exactly, yet\nthey can still achieve small test errors in practice. In this paper, we study\nthis \"benign overfitting\" phenomenon of the maximum margin classifier for\nlinear classification problems. Specifically, we consider data generated from\nsub-Gaussian mixtures, and provide a tight risk bound for the maximum margin\nlinear classifier in the over-parameterized setting. Our results precisely\ncharacterize the condition under which benign overfitting can occur in linear\nclassification problems, and improve on previous work. They also have direct\nimplications for over-parameterized logistic regression.",
          "link": "http://arxiv.org/abs/2104.13628",
          "publishedOn": "2022-01-05T00:39:36.639Z",
          "wordCount": null,
          "title": "Risk Bounds for Over-parameterized Maximum Margin Classification on Sub-Gaussian Mixtures. (arXiv:2104.13628v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stanev_V/0/1/0/all/0/1\">Valentin Stanev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skau_E/0/1/0/all/0/1\">Erik Skau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeuchi_I/0/1/0/all/0/1\">Ichiro Takeuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexandrov_B/0/1/0/all/0/1\">Boian S. Alexandrov</a>",
          "description": "We utilize a recently developed topic modeling method called SeNMFk,\nextending the standard Non-negative Matrix Factorization (NMF) methods by\nincorporating the semantic structure of the text, and adding a robust system\nfor determining the number of topics. With SeNMFk, we were able to extract\ncoherent topics validated by human experts. From these topics, a few are\nrelatively general and cover broad concepts, while the majority can be\nprecisely mapped to specific scientific effects or measurement techniques. The\ntopics also differ by ubiquity, with only three topics prevalent in almost 40\npercent of the abstract, while each specific topic tends to dominate a small\nsubset of the abstracts. These results demonstrate the ability of SeNMFk to\nproduce a layered and nuanced analysis of large scientific corpora.",
          "link": "http://arxiv.org/abs/2201.00687",
          "publishedOn": "2022-01-05T00:39:36.638Z",
          "wordCount": null,
          "title": "Topic Analysis of Superconductivity Literature by Semantic Non-negative Matrix Factorization. (arXiv:2201.00687v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00486",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Taywade_K/0/1/0/all/0/1\">Kshitija Taywade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_B/0/1/0/all/0/1\">Brent Harrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldsmith_J/0/1/0/all/0/1\">Judy Goldsmith</a>",
          "description": "Many past attempts at modeling repeated Cournot games assume that demand is\nstationary. This does not align with real-world scenarios in which market\ndemands can evolve over a product's lifetime for a myriad of reasons. In this\npaper, we model repeated Cournot games with non-stationary demand such that\nfirms/agents face separate instances of non-stationary multi-armed bandit\nproblem. The set of arms/actions that an agent can choose from represents\ndiscrete production quantities; here, the action space is ordered. Agents are\nindependent and autonomous, and cannot observe anything from the environment;\nthey can only see their own rewards after taking an action, and only work\ntowards maximizing these rewards. We propose a novel algorithm 'Adaptive with\nWeighted Exploration (AWE) $\\epsilon$-greedy' which is remotely based on the\nwell-known $\\epsilon$-greedy approach. This algorithm detects and quantifies\nchanges in rewards due to varying market demand and varies learning rate and\nexploration rate in proportion to the degree of changes in demand, thus\nenabling agents to better identify new optimal actions. For efficient\nexploration, it also deploys a mechanism for weighing actions that takes\nadvantage of the ordered action space. We use simulations to study the\nemergence of various equilibria in the market. In addition, we study the\nscalability of our approach in terms number of total agents in the system and\nthe size of action space. We consider both symmetric and asymmetric firms in\nour models. We found that using our proposed method, agents are able to swiftly\nchange their course of action according to the changes in demand, and they also\nengage in collusive behavior in many simulations.",
          "link": "http://arxiv.org/abs/2201.00486",
          "publishedOn": "2022-01-05T00:39:36.635Z",
          "wordCount": null,
          "title": "Using Non-Stationary Bandits for Learning in Repeated Cournot Games with Non-Stationary Demand. (arXiv:2201.00486v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00292",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Xu_S/0/1/0/all/0/1\">Shizhou Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Strohmer_T/0/1/0/all/0/1\">Thomas Strohmer</a>",
          "description": "As machine learning powered decision making is playing an increasingly\nimportant role in our daily lives, it is imperative to strive for fairness of\nthe underlying data processing and algorithms. We propose a pre-processing\nalgorithm for fair data representation via which L2- objective supervised\nlearning algorithms result in an estimation of the Pareto frontier between\nprediction error and statistical disparity. In particular, the present work\napplies the optimal positive definite affine transport maps to approach the\npost-processing Wasserstein barycenter characterization of the optimal fair\nL2-objective supervised learning via a pre-processing data deformation. We call\nthe resulting data Wasserstein pseudo-barycenter. Furthermore, we show that the\nWasserstein geodesics from the learning outcome marginals to the barycenter\ncharacterizes the Pareto frontier between L2-loss and total Wasserstein\ndistance among learning outcome marginals. Thereby, an application of McCann\ninterpolation generalizes the pseudo-barycenter to a family of data\nrepresentations via which L2-objective supervised learning algorithms result in\nthe Pareto frontier. Numerical simulations underscore the advantages of the\nproposed data representation: (1) the pre-processing step is compositive with\narbitrary L2-objective supervised learning methods and unseen data; (2) the\nfair representation protects data privacy by preventing the training machine\nfrom direct or indirect access to the sensitive information of the data; (3)\nthe optimal affine map results in efficient computation of fair supervised\nlearning on high-dimensional data; (4) experimental results shed light on the\nfairness of L2-objective unsupervised learning via the proposed fair data\nrepresentation.",
          "link": "http://arxiv.org/abs/2201.00292",
          "publishedOn": "2022-01-05T00:39:36.633Z",
          "wordCount": null,
          "title": "Fair Data Representation for Machine Learning at the Pareto Frontier. (arXiv:2201.00292v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00531",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shekar_A/0/1/0/all/0/1\">Arvind Kumar Shekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lake_L/0/1/0/all/0/1\">Laureen Lake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gou_L/0/1/0/all/0/1\">Liang Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Liu Ren</a>",
          "description": "The advent of Convolutional Neural Networks (CNNs) has led to their\napplication in several domains. One noteworthy application is the perception\nsystem for autonomous driving that relies on the predictions from CNNs.\nPractitioners evaluate the generalization ability of such CNNs by calculating\nvarious metrics on an independent test dataset. A test dataset is often chosen\nbased on only one precondition, i.e., its elements are not a part of the\ntraining data. Such a dataset may contain objects that are both similar and\nnovel w.r.t. the training dataset. Nevertheless, existing works do not reckon\nthe novelty of the test samples and treat them all equally for evaluating\ngeneralization. Such novelty-based evaluations are of significance to validate\nthe fitness of a CNN in autonomous driving applications. Hence, we propose a\nCNN generalization scoring framework that considers novelty of objects in the\ntest dataset. We begin with the representation learning technique to reduce the\nimage data into a low-dimensional space. It is on this space we estimate the\nnovelty of the test samples. Finally, we calculate the generalization score as\na combination of the test data prediction performance and novelty. We perform\nan experimental study of the same for our traffic light detection application.\nIn addition, we systematically visualize the results for an interpretable\nnotion of novelty.",
          "link": "http://arxiv.org/abs/2201.00531",
          "publishedOn": "2022-01-05T00:39:36.629Z",
          "wordCount": null,
          "title": "Novelty-based Generalization Evaluation for Traffic Light Detection. (arXiv:2201.00531v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Min Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhengfei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yun Zhou</a>",
          "description": "It is well-known that deep neural networks (DNNs) have shown remarkable\nsuccess in many fields. However, when adding an imperceptible magnitude\nperturbation on the model input, the model performance might get rapid\ndecrease. To address this issue, a randomness technique has been proposed\nrecently, named Stochastic Neural Networks (SNNs). Specifically, SNNs inject\nrandomness into the model to defend against unseen attacks and improve the\nadversarial robustness. However, existed studies on SNNs mainly focus on\ninjecting fixed or learnable noises to model weights/activations. In this\npaper, we find that the existed SNNs performances are largely bottlenecked by\nthe feature representation ability. Surprisingly, simply maximizing the\nvariance per dimension of the feature distribution leads to a considerable\nboost beyond all previous methods, which we named maximize feature distribution\nvariance stochastic neural network (MFDV-SNN). Extensive experiments on\nwell-known white- and black-box attacks show that MFDV-SNN achieves a\nsignificant improvement over existing methods, which indicates that it is a\nsimple but effective method to improve model robustness.",
          "link": "http://arxiv.org/abs/2201.00148",
          "publishedOn": "2022-01-05T00:39:36.610Z",
          "wordCount": 596,
          "title": "Rethinking Feature Uncertainty in Stochastic Neural Networks for Adversarial Robustness. (arXiv:2201.00148v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00519",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiyong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>",
          "description": "Stochastic weight averaging (SWA) is recognized as a simple while one\neffective approach to improve the generalization of stochastic gradient descent\n(SGD) for training deep neural networks (DNNs). A common insight to explain its\nsuccess is that averaging weights following an SGD process equipped with\ncyclical or high constant learning rates can discover wider optima, which then\nlead to better generalization. We give a new insight that does not concur with\nthe above one. We characterize that SWA's performance is highly dependent on to\nwhat extent the SGD process that runs before SWA converges, and the operation\nof weight averaging only contributes to variance reduction. This new insight\nsuggests practical guides on better algorithm design. As an instantiation, we\nshow that following an SGD process with insufficient convergence, running SWA\nmore times leads to continual incremental benefits in terms of generalization.\nOur findings are corroborated by extensive experiments across different network\narchitectures, including a baseline CNN, PreResNet-164, WideResNet-28-10,\nVGG16, ResNet-50, ResNet-152, DenseNet-161, and different datasets including\nCIFAR-{10,100}, and Imagenet.",
          "link": "http://arxiv.org/abs/2201.00519",
          "publishedOn": "2022-01-05T00:39:36.599Z",
          "wordCount": null,
          "title": "Stochastic Weight Averaging Revisited. (arXiv:2201.00519v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.14868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_A/0/1/0/all/0/1\">Aolin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raginsky_M/0/1/0/all/0/1\">Maxim Raginsky</a>",
          "description": "We analyze the best achievable performance of Bayesian learning under\ngenerative models by defining and upper-bounding the minimum excess risk (MER):\nthe gap between the minimum expected loss attainable by learning from data and\nthe minimum expected loss that could be achieved if the model realization were\nknown. The definition of MER provides a principled way to define different\nnotions of uncertainties in Bayesian learning, including the aleatoric\nuncertainty and the minimum epistemic uncertainty. Two methods for deriving\nupper bounds for the MER are presented. The first method, generally suitable\nfor Bayesian learning with a parametric generative model, upper-bounds the MER\nby the conditional mutual information between the model parameters and the\nquantity being predicted given the observed data. It allows us to quantify the\nrate at which the MER decays to zero as more data becomes available. Under\nrealizable models, this method also relates the MER to the richness of the\ngenerative function class, notably the VC dimension in binary classification.\nThe second method, particularly suitable for Bayesian learning with a\nparametric predictive model, relates the MER to the minimum estimation error of\nthe model parameters from data. It explicitly shows how the uncertainty in\nmodel parameter estimation translates to the MER and to the final prediction\nuncertainty. We also extend the definition and analysis of MER to the setting\nwith multiple model families and the setting with nonparametric models. Along\nthe discussions we draw some comparisons between the MER in Bayesian learning\nand the excess risk in frequentist learning.",
          "link": "http://arxiv.org/abs/2012.14868",
          "publishedOn": "2022-01-05T00:39:36.599Z",
          "wordCount": null,
          "title": "Minimum Excess Risk in Bayesian Learning. (arXiv:2012.14868v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patil_V/0/1/0/all/0/1\">Vrishabh Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mintz_Y/0/1/0/all/0/1\">Yonatan Mintz</a>",
          "description": "Artificial Neural Networks (ANNs) are prevalent machine learning models that\nhave been applied across various real world classification tasks. ANNs require\na large amount of data to have strong out of sample performance, and many\nalgorithms for training ANN parameters are based on stochastic gradient descent\n(SGD). However, the SGD ANNs that tend to perform best on prediction tasks are\ntrained in an end to end manner that requires a large number of model\nparameters and random initialization. This means training ANNs is very time\nconsuming and the resulting models take a lot of memory to deploy. In order to\ntrain more parsimonious ANN models, we propose the use of alternative methods\nfrom the constrained optimization literature for ANN training and pretraining.\nIn particular, we propose novel mixed integer programming (MIP) formulations\nfor training fully-connected ANNs. Our formulations can account for both binary\nactivation and rectified linear unit (ReLU) activation ANNs, and for the use of\na log likelihood loss. We also develop a layer-wise greedy approach, a\ntechnique adapted for reducing the number of layers in the ANN, for model\npretraining using our MIP formulations. We then present numerical experiments\ncomparing our MIP based methods against existing SGD based approaches and show\nthat we are able to achieve models with competitive out of sample performance\nthat are significantly more parsimonious.",
          "link": "http://arxiv.org/abs/2201.00723",
          "publishedOn": "2022-01-05T00:39:36.598Z",
          "wordCount": null,
          "title": "A Mixed Integer Programming Approach to Training Dense Neural Networks. (arXiv:2201.00723v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00318",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miyajiwala_A/0/1/0/all/0/1\">Aamir Miyajiwala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladkat_A/0/1/0/all/0/1\">Arnav Ladkat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagadale_S/0/1/0/all/0/1\">Samiksha Jagadale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>",
          "description": "Text classification is a fundamental Natural Language Processing task that\nhas a wide variety of applications, where deep learning approaches have\nproduced state-of-the-art results. While these models have been heavily\ncriticized for their black-box nature, their robustness to slight perturbations\nin input text has been a matter of concern. In this work, we carry out a\ndata-focused study evaluating the impact of systematic practical perturbations\non the performance of the deep learning based text classification models like\nCNN, LSTM, and BERT-based algorithms. The perturbations are induced by the\naddition and removal of unwanted tokens like punctuation and stop-words that\nare minimally associated with the final performance of the model. We show that\nthese deep learning approaches including BERT are sensitive to such legitimate\ninput perturbations on four standard benchmark datasets SST2, TREC-6, BBC News,\nand tweet_eval. We observe that BERT is more susceptible to the removal of\ntokens as compared to the addition of tokens. Moreover, LSTM is slightly more\nsensitive to input perturbations as compared to CNN based model. The work also\nserves as a practical guide to assessing the impact of discrepancies in\ntrain-test conditions on the final performance of models.",
          "link": "http://arxiv.org/abs/2201.00318",
          "publishedOn": "2022-01-05T00:39:36.592Z",
          "wordCount": null,
          "title": "On Sensitivity of Deep Learning Based Text Classification Algorithms to Practical Input Perturbations. (arXiv:2201.00318v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_B/0/1/0/all/0/1\">Bao Hieu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Cong_T/0/1/0/all/0/1\">Thanh Le-Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huu Manh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duc Anh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh Hung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phi Le Nguyen</a>",
          "description": "In the last decades, scene text recognition has gained worldwide attention\nfrom both the academic community and actual users due to its importance in a\nwide range of applications. Despite achievements in optical character\nrecognition, scene text recognition remains challenging due to inherent\nproblems such as distortions or irregular layout. Most of the existing\napproaches mainly leverage recurrence or convolution-based neural networks.\nHowever, while recurrent neural networks (RNNs) usually suffer from slow\ntraining speed due to sequential computation and encounter problems as\nvanishing gradient or bottleneck, CNN endures a trade-off between complexity\nand performance. In this paper, we introduce SAFL, a self-attention-based\nneural network model with the focal loss for scene text recognition, to\novercome the limitation of the existing approaches. The use of focal loss\ninstead of negative log-likelihood helps the model focus more on low-frequency\nsamples training. Moreover, to deal with the distortions and irregular texts,\nwe exploit Spatial TransformerNetwork (STN) to rectify text before passing to\nthe recognition network. We perform experiments to compare the performance of\nthe proposed model with seven benchmarks. The numerical results show that our\nmodel achieves the best performance.",
          "link": "http://arxiv.org/abs/2201.00132",
          "publishedOn": "2022-01-05T00:39:36.585Z",
          "wordCount": 646,
          "title": "SAFL: A Self-Attention Scene Text Recognizer with Focal Loss. (arXiv:2201.00132v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.00617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Adams_H/0/1/0/all/0/1\">Henry Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farnell_E/0/1/0/all/0/1\">Elin Farnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Story_B/0/1/0/all/0/1\">Brittany Story</a>",
          "description": "A support vector machine (SVM) is an algorithm which finds a hyperplane that\noptimally separates labeled data points in $\\mathbb{R}^n$ into positive and\nnegative classes. The data points on the margin of this separating hyperplane\nare called support vectors. We connect the possible configurations of support\nvectors to Radon's theorem, which provides guarantees for when a set of points\ncan be divided into two classes (positive and negative) whose convex hulls\nintersect. If the convex hulls of the positive and negative support vectors are\nprojected onto a separating hyperplane, the projections intersect in at least\none point if and only if the hyperplane is optimal. Further, with a particular\ntype of general position, we show that (a) the projected convex hulls of the\nsupport vectors intersect in exactly one point, (b) the support vectors are\nstable under perturbation, (c) there are at most $n+1$ support vectors, and (d)\nevery number of support vectors from 2 up to $n+1$ is possible. Finally, we\nperform computer simulations studying the expected number of support vectors,\nand their configurations, for randomly generated data. We observe that as the\ndistance between classes of points increases for this type of randomly\ngenerated data, configurations with two support vectors become the most likely\nconfigurations.",
          "link": "http://arxiv.org/abs/2011.00617",
          "publishedOn": "2022-01-05T00:39:36.563Z",
          "wordCount": 660,
          "title": "Support vector machines and Radon's theorem. (arXiv:2011.00617v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_A/0/1/0/all/0/1\">Aolin Xu</a>",
          "description": "We study the continuity property of the generalized entropy as a function of\nthe underlying probability distribution, defined with an action space and a\nloss function, and use this property to answer the basic questions in\nstatistical learning theory: the excess risk analyses for various learning\nmethods. We first derive upper and lower bounds for the entropy difference of\ntwo distributions in terms of several commonly used f-divergences, the\nWasserstein distance, a distance that depends on the action space and the loss\nfunction, and the Bregman divergence generated by the entropy, which also\ninduces bounds in terms of the Euclidean distance between the two\ndistributions. Examples are given along with the discussion of each general\nresult, comparisons are made with the existing entropy difference bounds, and\nnew mutual information upper bounds are derived based on the new results. We\nthen apply the entropy difference bounds to the theory of statistical learning.\nIt is shown that the excess risks in the two popular learning paradigms, the\nfrequentist learning and the Bayesian learning, both can be studied with the\ncontinuity property of different forms of the generalized entropy. The analysis\nis then extended to the continuity of generalized conditional entropy. The\nextension provides performance bounds for Bayes decision making with mismatched\ndistributions. It also leads to excess risk bounds for a third paradigm of\nlearning, where the decision rule is optimally designed under the projection of\nthe empirical distribution to a predefined family of distributions. We thus\nestablish a unified method of excess risk analysis for the three major\nparadigms of statistical learning, through the continuity of generalized\nentropy.",
          "link": "http://arxiv.org/abs/2012.15829",
          "publishedOn": "2022-01-05T00:39:36.553Z",
          "wordCount": 735,
          "title": "Continuity of Generalized Entropy and Statistical Learning. (arXiv:2012.15829v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schwalbe_G/0/1/0/all/0/1\">Gesina Schwalbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wirth_C/0/1/0/all/0/1\">Christian Wirth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_U/0/1/0/all/0/1\">Ute Schmid</a>",
          "description": "One major drawback of deep neural networks (DNNs) for use in sensitive\napplication domains is their black-box nature. This makes it hard to verify or\nmonitor complex, symbolic requirements. In this work, we present a simple, yet\neffective, approach to verify whether a trained convolutional neural network\n(CNN) respects specified symbolic background knowledge. The knowledge may\nconsist of any fuzzy predicate logic rules. For this, we utilize methods from\nexplainable artificial intelligence (XAI): First, using concept embedding\nanalysis, the output of a computer vision CNN is post-hoc enriched by concept\noutputs; second, logical rules from prior knowledge are fuzzified to serve as\ncontinuous-valued functions on the concept outputs. These can be evaluated with\nlittle computational overhead. We demonstrate three diverse use-cases of our\nmethod on stateof-the-art object detectors: Finding corner cases, utilizing the\nrules for detecting and localizing DNN misbehavior during runtime, and\ncomparing the logical consistency of DNNs. The latter is used to find related\ndifferences between EfficientDet D1 and Mask R-CNN object detectors. We show\nthat this approach benefits from fuzziness and calibrating the concept outputs.",
          "link": "http://arxiv.org/abs/2201.00572",
          "publishedOn": "2022-01-05T00:39:36.540Z",
          "wordCount": 635,
          "title": "Concept Embeddings for Fuzzy Logic Verification of Deep Neural Networks in Perception Tasks. (arXiv:2201.00572v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>",
          "description": "Although it is well known that exploration plays a key role in Reinforcement\nLearning (RL), prevailing exploration strategies for continuous control tasks\nin RL are mainly based on naive isotropic Gaussian noise regardless of the\ncausality relationship between action space and the task and consider all\ndimensions of actions equally important. In this work, we propose to conduct\ninterventions on the primal action space to discover the causal relationship\nbetween the action space and the task reward. We propose the method of\nState-Wise Action Refined (SWAR), which addresses the issue of action space\nredundancy and promote causality discovery in RL. We formulate causality\ndiscovery in RL tasks as a state-dependent action space selection problem and\npropose two practical algorithms as solutions. The first approach, TD-SWAR,\ndetects task-related actions during temporal difference learning, while the\nsecond approach, Dyn-SWAR, reveals important actions through dynamic model\nprediction. Empirically, both methods provide approaches to understand the\ndecisions made by RL agents and improve learning efficiency in action-redundant\ntasks.",
          "link": "http://arxiv.org/abs/2201.00354",
          "publishedOn": "2022-01-05T00:39:36.518Z",
          "wordCount": 576,
          "title": "Toward Causal-Aware RL: State-Wise Action-Refined Temporal Difference. (arXiv:2201.00354v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.10834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Almeida_A/0/1/0/all/0/1\">Adolfo Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villiers_J/0/1/0/all/0/1\">Johan Pieter de Villiers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Allan De Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velayudan_M/0/1/0/all/0/1\">Mergandran Velayudan</a>",
          "description": "Following the popularisation of media streaming, a number of video streaming\nservices are continuously buying new video content to mine the potential profit\nfrom them. As such, the newly added content has to be handled well to be\nrecommended to suitable users. In this paper, we address the new item\ncold-start problem by exploring the potential of various deep learning features\nto provide video recommendations. The deep learning features investigated\ninclude features that capture the visual-appearance, audio and motion\ninformation from video content. We also explore different fusion methods to\nevaluate how well these feature modalities can be combined to fully exploit the\ncomplementary information captured by them. Experiments on a real-world video\ndataset for movie recommendations show that deep learning features outperform\nhand-crafted features. In particular, recommendations generated with deep\nlearning audio features and action-centric deep learning features are superior\nto MFCC and state-of-the-art iDT features. In addition, the combination of\nvarious deep learning features with hand-crafted features and textual metadata\nyields significant improvement in recommendations compared to combining only\nthe former.",
          "link": "http://arxiv.org/abs/2011.10834",
          "publishedOn": "2022-01-05T00:39:36.510Z",
          "wordCount": 656,
          "title": "The complementarity of a diverse range of deep learning features extracted from video content for video recommendation. (arXiv:2011.10834v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Ziyang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yihao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>",
          "description": "Reinforcement learning (RL) has drawn increasing interests in recent years\ndue to its tremendous success in various applications. However, standard RL\nalgorithms can only be applied for single reward function, and cannot adapt to\nan unseen reward function quickly. In this paper, we advocate a general\noperator view of reinforcement learning, which enables us to directly\napproximate the operator that maps from reward function to value function. The\nbenefit of learning the operator is that we can incorporate any new reward\nfunction as input and attain its corresponding value function in a zero-shot\nmanner. To approximate this special type of operator, we design a number of\nnovel operator neural network architectures based on its theoretical\nproperties. Our design of operator networks outperform the existing methods and\nthe standard design of general purpose operator network, and we demonstrate the\nbenefit of our operator deep Q-learning framework in several tasks including\nreward transferring for offline policy evaluation (OPE) and reward transferring\nfor offline policy optimization in a range of tasks.",
          "link": "http://arxiv.org/abs/2201.00236",
          "publishedOn": "2022-01-05T00:39:36.480Z",
          "wordCount": 593,
          "title": "Operator Deep Q-Learning: Zero-Shot Reward Transferring in Reinforcement Learning. (arXiv:2201.00236v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zejiang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kung_S/0/1/0/all/0/1\">Sun-Yuan Kung</a>",
          "description": "Vision transformers (ViT) have recently attracted considerable attentions,\nbut the huge computational cost remains an issue for practical deployment.\nPrevious ViT pruning methods tend to prune the model along one dimension\nsolely, which may suffer from excessive reduction and lead to sub-optimal model\nquality. In contrast, we advocate a multi-dimensional ViT compression paradigm,\nand propose to harness the redundancy reduction from attention head, neuron and\nsequence dimensions jointly. We firstly propose a statistical dependence based\npruning criterion that is generalizable to different dimensions for identifying\ndeleterious components. Moreover, we cast the multi-dimensional compression as\nan optimization, learning the optimal pruning policy across the three\ndimensions that maximizes the compressed model's accuracy under a computational\nbudget. The problem is solved by our adapted Gaussian process search with\nexpected improvement. Experimental results show that our method effectively\nreduces the computational cost of various ViT models. For example, our method\nreduces 40\\% FLOPs without top-1 accuracy loss for DeiT and T2T-ViT models,\noutperforming previous state-of-the-arts.",
          "link": "http://arxiv.org/abs/2201.00043",
          "publishedOn": "2022-01-05T00:39:36.473Z",
          "wordCount": 580,
          "title": "Multi-Dimensional Model Compression of Vision Transformer. (arXiv:2201.00043v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00726",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moghaddam_M/0/1/0/all/0/1\">Mohammad A. Moghaddam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferre_T/0/1/0/all/0/1\">Ty P. A. Ferre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kewei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsani_M/0/1/0/all/0/1\">Mohammad Reza Ehsani</a>",
          "description": "We examine the ability of machine learning (ML) and deep learning (DL)\nalgorithms to infer surface/ground exchange flux based on subsurface\ntemperature observations. The observations and fluxes are produced from a\nhigh-resolution numerical model representing conditions in the Columbia River\nnear the Department of Energy Hanford site located in southeastern Washington\nState. Random measurement error, of varying magnitude, is added to the\nsynthetic temperature observations. The results indicate that both ML and DL\nmethods can be used to infer the surface/ground exchange flux. DL methods,\nespecially convolutional neural networks, outperform the ML methods when used\nto interpret noisy temperature data with a smoothing filter applied. However,\nthe ML methods also performed well and they are can better identify a reduced\nnumber of important observations, which could be useful for measurement network\noptimization. Surprisingly, the ML and DL methods better inferred upward flux\nthan downward flux. This is in direct contrast to previous findings using\nnumerical models to infer flux from temperature observations and it may suggest\nthat combined use of ML or DL inference with numerical inference could improve\nflux estimation beneath river systems.",
          "link": "http://arxiv.org/abs/2201.00726",
          "publishedOn": "2022-01-05T00:39:36.466Z",
          "wordCount": 642,
          "title": "Application of Machine Learning Methods in Inferring Surface Water Groundwater Exchanges using High Temporal Resolution Temperature Measurements. (arXiv:2201.00726v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00578",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Niggli_M/0/1/0/all/0/1\">Matthias Niggli</a>",
          "description": "Patent data provides rich information about technical inventions, but does\nnot disclose the ethnic origin of inventors. In this paper, I use supervised\nlearning techniques to infer this information. To do so, I construct a dataset\nof 95'202 labeled names and train an artificial recurrent neural network with\nlong-short-term memory (LSTM) to predict ethnic origins based on names. The\ntrained network achieves an overall performance of 91% across 17 ethnic\norigins. I use this model to classify and investigate the ethnic origins of\n2.68 million inventors and provide novel descriptive evidence regarding their\nethnic origin composition over time and across countries and technological\nfields. The global ethnic origin composition has become more diverse over the\nlast decades, which was mostly due to a relative increase of Asian origin\ninventors. Furthermore, the prevalence of foreign-origin inventors is\nespecially high in the USA, but has also increased in other high-income\neconomies. This increase was mainly driven by an inflow of non-western\ninventors into emerging high-technology fields for the USA, but not for other\nhigh-income countries.",
          "link": "http://arxiv.org/abs/2201.00578",
          "publishedOn": "2022-01-05T00:39:36.460Z",
          "wordCount": 593,
          "title": "'Moving On' -- Investigating Inventors' Ethnic Origins Using Supervised Learning. (arXiv:2201.00578v1 [econ.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Subhabrata Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caur_S/0/1/0/all/0/1\">Samiya Caur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1\">Soumen Chakrabarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>",
          "description": "Detecting and labeling stance in social media text is strongly motivated by\nhate speech detection, poll prediction, engagement forecasting, and concerted\npropaganda detection. Today's best neural stance detectors need large volumes\nof training data, which is difficult to curate given the fast-changing\nlandscape of social media text and issues on which users opine. Homophily\nproperties over the social network provide strong signal of coarse-grained\nuser-level stance. But semi-supervised approaches for tweet-level stance\ndetection fail to properly leverage homophily. In light of this, We present\nSANDS, a new semi-supervised stance detector. SANDS starts from very few\nlabeled tweets. It builds multiple deep feature views of tweets. It also uses a\ndistant supervision signal from the social network to provide a surrogate loss\nsignal to the component learners. We prepare two new tweet datasets comprising\nover 236,000 politically tinted tweets from two demographics (US and India)\nposted by over 87,000 users, their follower-followee graph, and over 8,000\ntweets annotated by linguists. SANDS achieves a macro-F1 score of 0.55 (0.49)\non US (India)-based datasets, outperforming 17 baselines (including variants of\nSANDS) substantially, particularly for minority stance labels and noisy text.\nNumerous ablation experiments on SANDS disentangle the dynamics of textual and\nnetwork-propagated stance signals.",
          "link": "http://arxiv.org/abs/2201.00614",
          "publishedOn": "2022-01-05T00:39:36.452Z",
          "wordCount": 634,
          "title": "Semi-supervised Stance Detection of Tweets Via Distant Network Supervision. (arXiv:2201.00614v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiqiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Han Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>",
          "description": "Unsupervised anomaly detection aims to build models to effectively detect\nunseen anomalies by only training on the normal data. Although previous\nreconstruction-based methods have made fruitful progress, their generalization\nability is limited due to two critical challenges. First, the training dataset\nonly contains normal patterns, which limits the model generalization ability.\nSecond, the feature representations learned by existing models often lack\nrepresentativeness which hampers the ability to preserve the diversity of\nnormal patterns. In this paper, we propose a novel approach called Adaptive\nMemory Network with Self-supervised Learning (AMSL) to address these challenges\nand enhance the generalization ability in unsupervised anomaly detection. Based\non the convolutional autoencoder structure, AMSL incorporates a self-supervised\nlearning module to learn general normal patterns and an adaptive memory fusion\nmodule to learn rich feature representations. Experiments on four public\nmultivariate time series datasets demonstrate that AMSL significantly improves\nthe performance compared to other state-of-the-art methods. Specifically, on\nthe largest CAP sleep stage detection dataset with 900 million samples, AMSL\noutperforms the second-best baseline by \\textbf{4}\\%+ in both accuracy and F1\nscore. Apart from the enhanced generalization ability, AMSL is also more robust\nagainst input noise.",
          "link": "http://arxiv.org/abs/2201.00464",
          "publishedOn": "2022-01-05T00:39:36.429Z",
          "wordCount": 630,
          "title": "Adaptive Memory Networks with Self-supervised Learning for Unsupervised Anomaly Detection. (arXiv:2201.00464v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xi_D/0/1/0/all/0/1\">Dongbo Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hengshu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pengpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>",
          "description": "Recent years have witnessed the increasing popularity of Location-based\nSocial Network (LBSN) services, which provides unparalleled opportunities to\nbuild personalized Point-of-Interest (POI) recommender systems. Existing POI\nrecommendation and location prediction tasks utilize past information for\nfuture recommendation or prediction from a single direction perspective, while\nthe missing POI category identification task needs to utilize the check-in\ninformation both before and after the missing category. Therefore, a\nlong-standing challenge is how to effectively identify the missing POI\ncategories at any time in the real-world check-in data of mobile users. To this\nend, in this paper, we propose a novel neural network approach to identify the\nmissing POI categories by integrating both bi-directional global non-personal\ntransition patterns and personal preferences of users. Specifically, we\ndelicately design an attention matching cell to model how well the check-in\ncategory information matches their non-personal transition patterns and\npersonal preferences. Finally, we evaluate our model on two real-world\ndatasets, which clearly validate its effectiveness compared with the\nstate-of-the-art baselines. Furthermore, our model can be naturally extended to\naddress next POI category recommendation and prediction tasks with competitive\nperformance.",
          "link": "http://arxiv.org/abs/2201.00014",
          "publishedOn": "2022-01-05T00:39:36.423Z",
          "wordCount": 637,
          "title": "Exploiting Bi-directional Global Transition Patterns and Personal Preferences for Missing POI Category Identification. (arXiv:2201.00014v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Delaunoy_A/0/1/0/all/0/1\">Arnaud Delaunoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Louppe_G/0/1/0/all/0/1\">Gilles Louppe</a>",
          "description": "Computing the Bayesian posterior of a neural network is a challenging task\ndue to the high-dimensionality of the parameter space. Anchored ensembles\napproximate the posterior by training an ensemble of neural networks on\nanchored losses designed for the optima to follow the Bayesian posterior.\nTraining an ensemble, however, becomes computationally expensive as its number\nof members grows since the full training procedure is repeated for each member.\nIn this note, we present Sequential Anchored Ensembles (SAE), a lightweight\nalternative to anchored ensembles. Instead of training each member of the\nensemble from scratch, the members are trained sequentially on losses sampled\nwith high auto-correlation, hence enabling fast convergence of the neural\nnetworks and efficient approximation of the Bayesian posterior. SAE outperform\nanchored ensembles, for a given computational budget, on some benchmarks while\nshowing comparable performance on the others and achieved 2nd and 3rd place in\nthe light and extended tracks of the NeurIPS 2021 Approximate Inference in\nBayesian Deep Learning competition.",
          "link": "http://arxiv.org/abs/2201.00649",
          "publishedOn": "2022-01-05T00:39:36.416Z",
          "wordCount": 574,
          "title": "SAE: Sequential Anchored Ensembles. (arXiv:2201.00649v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00145",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>",
          "description": "In 1954, Alston S. Householder published Principles of Numerical Analysis,\none of the first modern treatments on matrix decomposition that favored a\n(block) LU decomposition-the factorization of a matrix into the product of\nlower and upper triangular matrices. And now, matrix decomposition has become a\ncore technology in machine learning, largely due to the development of the back\npropagation algorithm in fitting a neural network. The sole aim of this survey\nis to give a self-contained introduction to concepts and mathematical tools in\nnumerical linear algebra and matrix analysis in order to seamlessly introduce\nmatrix decomposition techniques and their applications in subsequent sections.\nHowever, we clearly realize our inability to cover all the useful and\ninteresting results concerning matrix decomposition and given the paucity of\nscope to present this discussion, e.g., the separated analysis of the Euclidean\nspace, Hermitian space, Hilbert space, and things in the complex domain. We\nrefer the reader to literature in the field of linear algebra for a more\ndetailed introduction to the related fields.",
          "link": "http://arxiv.org/abs/2201.00145",
          "publishedOn": "2022-01-05T00:39:36.409Z",
          "wordCount": 585,
          "title": "Matrix Decomposition and Applications. (arXiv:2201.00145v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00230",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Duan_J/0/1/0/all/0/1\">Juntao Duan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Popescu_I/0/1/0/all/0/1\">Ionel Popescu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Matzinger_H/0/1/0/all/0/1\">Heinrich Matzinger</a>",
          "description": "It is well known the sample covariance has a consistent bias in the spectrum,\nfor example spectrum of Wishart matrix follows the Marchenko-Pastur law. We in\nthis work introduce an iterative algorithm 'Concent' that actively eliminate\nthis bias and recover the true spectrum for small and moderate dimensions.",
          "link": "http://arxiv.org/abs/2201.00230",
          "publishedOn": "2022-01-05T00:39:36.409Z",
          "wordCount": null,
          "title": "Recover the spectrum of covariance matrix: a non-asymptotic iterative method. (arXiv:2201.00230v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nassif_A/0/1/0/all/0/1\">Ali Bou Nassif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soudan_B/0/1/0/all/0/1\">Bassel Soudan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azzeh_M/0/1/0/all/0/1\">Mohammad Azzeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attilli_I/0/1/0/all/0/1\">Imtinan Attilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AlMulla_O/0/1/0/all/0/1\">Omar AlMulla</a>",
          "description": "Electrical utilities depend on short-term demand forecasting to proactively\nadjust production and distribution in anticipation of major variations. This\nsystematic review analyzes 240 works published in scholarly journals between\n2000 and 2019 that focus on applying Artificial Intelligence (AI), statistical,\nand hybrid models to short-term load forecasting (STLF). This work represents\nthe most comprehensive review of works on this subject to date. A complete\nanalysis of the literature is conducted to identify the most popular and\naccurate techniques as well as existing gaps. The findings show that although\nArtificial Neural Networks (ANN) continue to be the most commonly used\nstandalone technique, researchers have been exceedingly opting for hybrid\ncombinations of different techniques to leverage the combined advantages of\nindividual methods. The review demonstrates that it is commonly possible with\nthese hybrid combinations to achieve prediction accuracy exceeding 99%. The\nmost successful duration for short-term forecasting has been identified as\nprediction for a duration of one day at an hourly interval. The review has\nidentified a deficiency in access to datasets needed for training of the\nmodels. A significant gap has been identified in researching regions other than\nAsia, Europe, North America, and Australia.",
          "link": "http://arxiv.org/abs/2201.00437",
          "publishedOn": "2022-01-05T00:39:36.389Z",
          "wordCount": 636,
          "title": "Artificial Intelligence and Statistical Techniques in Short-Term Load Forecasting: A Review. (arXiv:2201.00437v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Redder_A/0/1/0/all/0/1\">Adrian Redder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramaswamy_A/0/1/0/all/0/1\">Arunselvan Ramaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karl_H/0/1/0/all/0/1\">Holger Karl</a>",
          "description": "We present sufficient conditions that ensure convergence of the multi-agent\nDeep Deterministic Policy Gradient (DDPG) algorithm. It is an example of one of\nthe most popular paradigms of Deep Reinforcement Learning (DeepRL) for tackling\ncontinuous action spaces: the actor-critic paradigm. In the setting considered\nherein, each agent observes a part of the global state space in order to take\nlocal actions, for which it receives local rewards. For every agent, DDPG\ntrains a local actor (policy) and a local critic (Q-function). The analysis\nshows that multi-agent DDPG using neural networks to approximate the local\npolicies and critics converge to limits with the following properties: The\ncritic limits minimize the average squared Bellman loss; the actor limits\nparameterize a policy that maximizes the local critic's approximation of\n$Q_i^*$, where $i$ is the agent index. The averaging is with respect to a\nprobability distribution over the global state-action space. It captures the\nasymptotics of all local training processes. Finally, we extend the analysis to\na fully decentralized setting where agents communicate over a wireless network\nprone to delays and losses; a typical scenario in, e.g., robotic applications.",
          "link": "http://arxiv.org/abs/2201.00570",
          "publishedOn": "2022-01-05T00:39:36.367Z",
          "wordCount": 606,
          "title": "Asymptotic Convergence of Deep Multi-Agent Actor-Critic Algorithms. (arXiv:2201.00570v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_A/0/1/0/all/0/1\">Aosong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tassiulas_L/0/1/0/all/0/1\">Leandros Tassiulas</a>",
          "description": "Graph kernels are historically the most widely-used technique for graph\nclassification tasks. However, these methods suffer from limited performance\nbecause of the hand-crafted combinatorial features of graphs. In recent years,\ngraph neural networks (GNNs) have become the state-of-the-art method in\ndownstream graph-related tasks due to their superior performance. Most GNNs are\nbased on Message Passing Neural Network (MPNN) frameworks. However, recent\nstudies show that MPNNs can not exceed the power of the Weisfeiler-Lehman (WL)\nalgorithm in graph isomorphism test. To address the limitations of existing\ngraph kernel and GNN methods, in this paper, we propose a novel GNN framework,\ntermed \\textit{Kernel Graph Neural Networks} (KerGNNs), which integrates graph\nkernels into the message passing process of GNNs. Inspired by convolution\nfilters in convolutional neural networks (CNNs), KerGNNs adopt trainable hidden\ngraphs as graph filters which are combined with subgraphs to update node\nembeddings using graph kernels. In addition, we show that MPNNs can be viewed\nas special cases of KerGNNs. We apply KerGNNs to multiple graph-related tasks\nand use cross-validation to make fair comparisons with benchmarks. We show that\nour method achieves competitive performance compared with existing\nstate-of-the-art methods, demonstrating the potential to increase the\nrepresentation ability of GNNs. We also show that the trained graph filters in\nKerGNNs can reveal the local graph structures of the dataset, which\nsignificantly improves the model interpretability compared with conventional\nGNN models.",
          "link": "http://arxiv.org/abs/2201.00491",
          "publishedOn": "2022-01-05T00:39:36.357Z",
          "wordCount": 648,
          "title": "KerGNNs: Interpretable Graph Neural Networks with Graph Kernels. (arXiv:2201.00491v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iyer_A/0/1/0/all/0/1\">Abhiram Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grewal_K/0/1/0/all/0/1\">Karan Grewal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velu_A/0/1/0/all/0/1\">Akash Velu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Souza_L/0/1/0/all/0/1\">Lucas Oliveira Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forest_J/0/1/0/all/0/1\">Jeremy Forest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_S/0/1/0/all/0/1\">Subutai Ahmad</a>",
          "description": "A key challenge for AI is to build embodied systems that operate in\ndynamically changing environments. Such systems must adapt to changing task\ncontexts and learn continuously. Although standard deep learning systems\nachieve state of the art results on static benchmarks, they often struggle in\ndynamic scenarios. In these settings, error signals from multiple contexts can\ninterfere with one another, ultimately leading to a phenomenon known as\ncatastrophic forgetting. In this article we investigate biologically inspired\narchitectures as solutions to these problems. Specifically, we show that the\nbiophysical properties of dendrites and local inhibitory systems enable\nnetworks to dynamically restrict and route information in a context-specific\nmanner. Our key contributions are as follows. First, we propose a novel\nartificial neural network architecture that incorporates active dendrites and\nsparse representations into the standard deep learning framework. Next, we\nstudy the performance of this architecture on two separate benchmarks requiring\ntask-based adaptation: Meta-World, a multi-task reinforcement learning\nenvironment where a robotic agent must learn to solve a variety of manipulation\ntasks simultaneously; and a continual learning benchmark in which the model's\nprediction task changes throughout training. Analysis on both benchmarks\ndemonstrates the emergence of overlapping but distinct and sparse subnetworks,\nallowing the system to fluidly learn multiple tasks with minimal forgetting.\nOur neural implementation marks the first time a single architecture has\nachieved competitive results on both multi-task and continual learning\nsettings. Our research sheds light on how biological properties of neurons can\ninform deep learning systems to address dynamic scenarios that are typically\nimpossible for traditional ANNs to solve.",
          "link": "http://arxiv.org/abs/2201.00042",
          "publishedOn": "2022-01-05T00:39:36.349Z",
          "wordCount": 706,
          "title": "Avoiding Catastrophe: Active Dendrites Enable Multi-Task Learning in Dynamic Environments. (arXiv:2201.00042v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00111",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeon_E/0/1/0/all/0/1\">Eun Som Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Som_A/0/1/0/all/0/1\">Anirudh Som</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_A/0/1/0/all/0/1\">Ankita Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasanaj_K/0/1/0/all/0/1\">Kristina Hasanaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buman_M/0/1/0/all/0/1\">Matthew P. Buman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turaga_P/0/1/0/all/0/1\">Pavan Turaga</a>",
          "description": "Deep neural networks are parametrized by several thousands or millions of\nparameters, and have shown tremendous success in many classification problems.\nHowever, the large number of parameters makes it difficult to integrate these\nmodels into edge devices such as smartphones and wearable devices. To address\nthis problem, knowledge distillation (KD) has been widely employed, that uses a\npre-trained high capacity network to train a much smaller network, suitable for\nedge devices. In this paper, for the first time, we study the applicability and\nchallenges of using KD for time-series data for wearable devices. Successful\napplication of KD requires specific choices of data augmentation methods during\ntraining. However, it is not yet known if there exists a coherent strategy for\nchoosing an augmentation approach during KD. In this paper, we report the\nresults of a detailed study that compares and contrasts various common choices\nand some hybrid data augmentation strategies in KD based human activity\nanalysis. Research in this area is often limited as there are not many\ncomprehensive databases available in the public domain from wearable devices.\nOur study considers databases from small scale publicly available to one\nderived from a large scale interventional study into human activity and\nsedentary behavior. We find that the choice of data augmentation techniques\nduring KD have a variable level of impact on end performance, and find that the\noptimal network choice as well as data augmentation strategies are specific to\na dataset at hand. However, we also conclude with a general set of\nrecommendations that can provide a strong baseline performance across\ndatabases.",
          "link": "http://arxiv.org/abs/2201.00111",
          "publishedOn": "2022-01-05T00:39:36.341Z",
          "wordCount": 702,
          "title": "Role of Data Augmentation Strategies in Knowledge Distillation for Wearable Sensor Data. (arXiv:2201.00111v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00378",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ferrer_Cid_P/0/1/0/all/0/1\">Pau Ferrer-Cid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barcelo_Ordinas_J/0/1/0/all/0/1\">Jose M. Barcelo-Ordinas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garcia_Vidal_J/0/1/0/all/0/1\">Jorge Garcia-Vidal</a>",
          "description": "Air pollution monitoring platforms play a very important role in preventing\nand mitigating the effects of pollution. Recent advances in the field of graph\nsignal processing have made it possible to describe and analyze air pollution\nmonitoring networks using graphs. One of the main applications is the\nreconstruction of the measured signal in a graph using a subset of sensors.\nReconstructing the signal using information from sensor neighbors can help\nimprove the quality of network data, examples are filling in missing data with\ncorrelated neighboring nodes, or correcting a drifting sensor with neighboring\nsensors that are more accurate. This paper compares the use of various types of\ngraph signal reconstruction methods applied to real data sets of Spanish air\npollution reference stations. The methods considered are Laplacian\ninterpolation, graph signal processing low-pass based graph signal\nreconstruction, and kernel-based graph signal reconstruction, and are compared\non actual air pollution data sets measuring O3, NO2, and PM10. The ability of\nthe methods to reconstruct the signal of a pollutant is shown, as well as the\ncomputational cost of this reconstruction. The results indicate the superiority\nof methods based on kernel-based graph signal reconstruction, as well as the\ndifficulties of the methods to scale in an air pollution monitoring network\nwith a large number of low-cost sensors. However, we show that scalability can\nbe overcome with simple methods, such as partitioning the network using a\nclustering algorithm.",
          "link": "http://arxiv.org/abs/2201.00378",
          "publishedOn": "2022-01-05T00:39:36.333Z",
          "wordCount": 676,
          "title": "Graph Signal Reconstruction Techniques for IoT Air Pollution Monitoring Platforms. (arXiv:2201.00378v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_H/0/1/0/all/0/1\">Huanlai Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhiwen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_R/0/1/0/all/0/1\">Rong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zonghai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bowen Zhao</a>",
          "description": "This paper proposes an efficient federated distillation learning system\n(EFDLS) for multi-task time series classification (TSC). EFDLS consists of a\ncentral server and multiple mobile users, where different users may run\ndifferent TSC tasks. EFDLS has two novel components, namely a feature-based\nstudent-teacher (FBST) framework and a distance-based weights matching (DBWM)\nscheme. Within each user, the FBST framework transfers knowledge from its\nteacher's hidden layers to its student's hidden layers via knowledge\ndistillation, with the teacher and student having identical network structure.\nFor each connected user, its student model's hidden layers' weights are\nuploaded to the EFDLS server periodically. The DBWM scheme is deployed on the\nserver, with the least square distance used to measure the similarity between\nthe weights of two given models. This scheme finds a partner for each connected\nuser such that the user's and its partner's weights are the closest among all\nthe weights uploaded. The server exchanges and sends back the user's and its\npartner's weights to these two users which then load the received weights to\ntheir teachers' hidden layers. Experimental results show that the proposed\nEFDLS achieves excellent performance on a set of selected UCR2018 datasets\nregarding top-1 accuracy.",
          "link": "http://arxiv.org/abs/2201.00011",
          "publishedOn": "2022-01-05T00:39:36.311Z",
          "wordCount": 627,
          "title": "An Efficient Federated Distillation Learning System for Multi-task Time Series Classification. (arXiv:2201.00011v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00363",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mattos_J/0/1/0/all/0/1\">Joao Pedro Rodrigues Mattos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcacini_R/0/1/0/all/0/1\">Ricardo M. Marcacini</a>",
          "description": "Event analysis from news and social networks is very useful for a wide range\nof social studies and real-world applications. Recently, event graphs have been\nexplored to model event datasets and their complex relationships, where events\nare vertices connected to other vertices representing locations, people's\nnames, dates, and various other event metadata. Graph representation learning\nmethods are promising for extracting latent features from event graphs to\nenable the use of different classification algorithms. However, existing\nmethods fail to meet essential requirements for event graphs, such as (i)\ndealing with semi-supervised graph embedding to take advantage of some labeled\nevents, (ii) automatically determining the importance of the relationships\nbetween event vertices and their metadata vertices, as well as (iii) dealing\nwith the graph heterogeneity. This paper presents GNEE (GAT Neural Event\nEmbeddings), a method that combines Graph Attention Networks and Graph\nRegularization. First, an event graph regularization is proposed to ensure that\nall graph vertices receive event features, thereby mitigating the graph\nheterogeneity drawback. Second, semi-supervised graph embedding with\nself-attention mechanism considers existing labeled events, as well as learns\nthe importance of relationships in the event graph during the representation\nlearning process. A statistical analysis of experimental results with five\nreal-world event graphs and six graph embedding methods shows that our GNEE\noutperforms state-of-the-art semi-supervised graph embedding methods.",
          "link": "http://arxiv.org/abs/2201.00363",
          "publishedOn": "2022-01-05T00:39:36.303Z",
          "wordCount": 652,
          "title": "Semi-Supervised Graph Attention Networks for Event Representation Learning. (arXiv:2201.00363v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tjoa_E/0/1/0/all/0/1\">Erico Tjoa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khok_H/0/1/0/all/0/1\">Hong Jing Khok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chouhan_T/0/1/0/all/0/1\">Tushar Chouhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuntai_G/0/1/0/all/0/1\">Guan Cuntai</a>",
          "description": "This paper introduces the Confidence Optimization (CO) score to directly\nmeasure the contribution of heatmaps/saliency maps to the classification\nperformance of a model. Common heatmap generation methods used in the\neXplainable Artificial Intelligence (XAI) community are tested through a\nprocess we call the Augmentative eXplanation (AX). We find a surprising\n\\textit{gap} in CO scores distribution on these heatmap methods. The gap\npotentially serves as a novel indicator for the correctness of deep neural\nnetwork (DNN) prediction. We further introduces Generative AX (GAX) method to\ngenerate saliency maps capable of attaining high CO scores. Using GAX, we also\nqualitatively demonstrate the unintuitiveness of DNN architectures.",
          "link": "http://arxiv.org/abs/2201.00009",
          "publishedOn": "2022-01-05T00:39:36.292Z",
          "wordCount": 529,
          "title": "Augmentative eXplanation and the Distributional Gap of Confidence Optimization Score. (arXiv:2201.00009v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ohana_B/0/1/0/all/0/1\">Bruno Ohana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_J/0/1/0/all/0/1\">Jack Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baker_N/0/1/0/all/0/1\">Nicole Baker</a>",
          "description": "Recent advances in artificial intelligence applied to biomedical text are\nopening exciting opportunities for improving pharmacovigilance activities\ncurrently burdened by the ever growing volumes of real world data. To fully\nrealize these opportunities, existing regulatory guidance and industry best\npractices should be taken into consideration in order to increase the overall\ntrustworthiness of the system and enable broader adoption. In this paper we\npresent a case study on how to operationalize existing guidance for validated\nAI systems in pharmacovigilance focusing on the specific task of medical\nliterature monitoring (MLM) of adverse events from the scientific literature.\nWe describe an AI system designed with the goal of reducing effort in MLM\nactivities built in close collaboration with subject matter experts and\nconsidering guidance for validated systems in pharmacovigilance and AI\ntransparency. In particular we make use of public disclosures as a useful risk\ncontrol measure to mitigate system misuse and earn user trust. In addition we\npresent experimental results showing the system can significantly remove\nscreening effort while maintaining high levels of recall (filtering 55% of\nirrelevant articles on average, for a target recall of 0.99 on suspected\nadverse articles) and provide a robust method for tuning the desired recall to\nsuit a particular risk profile.",
          "link": "http://arxiv.org/abs/2201.00692",
          "publishedOn": "2022-01-05T00:39:36.284Z",
          "wordCount": 646,
          "title": "Validation and Transparency in AI systems for pharmacovigilance: a case study applied to the medical literature monitoring of adverse events. (arXiv:2201.00692v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alkhatib_O/0/1/0/all/0/1\">Ola Alkhatib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahmar_A/0/1/0/all/0/1\">Ayman Alahmar</a>",
          "description": "Hospital length of stay (LOS) is one of the most essential healthcare metrics\nthat reflects the hospital quality of service and helps improve hospital\nscheduling and management. LOS prediction helps in cost management because\npatients who remain in hospitals usually do so in hospital units where\nresources are severely limited. In this study, we reviewed papers on LOS\nprediction using machine learning and statistical approaches. Our literature\nreview considers research studies that focus on LOS prediction for stroke\npatients. Some of the surveyed studies revealed that authors reached\ncontradicting conclusions. For example, the age of the patient was considered\nan important predictor of LOS for stroke patients in some studies, while other\nstudies concluded that age was not a significant factor. Therefore, additional\nresearch is required in this domain to further understand the predictors of LOS\nfor stroke patients.",
          "link": "http://arxiv.org/abs/2201.00005",
          "publishedOn": "2022-01-05T00:39:36.218Z",
          "wordCount": 578,
          "title": "A Literature Review on Length of Stay Prediction for Stroke Patients using Machine Learning and Statistical Approaches. (arXiv:2201.00005v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00084",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_T/0/1/0/all/0/1\">Taira Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanioka_K/0/1/0/all/0/1\">Kensuke Tanioka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hiwa_S/0/1/0/all/0/1\">Satoru Hiwa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hiroyasu_T/0/1/0/all/0/1\">Tomoyuki Hiroyasu</a>",
          "description": "Endoscopic images typically contain several artifacts. The artifacts\nsignificantly impact image analysis result in computer-aided diagnosis.\nConvolutional neural networks (CNNs), a type of deep learning, can removes such\nartifacts. Various architectures have been proposed for the CNNs, and the\naccuracy of artifact removal varies depending on the choice of architecture.\nTherefore, it is necessary to determine the artifact removal accuracy,\ndepending on the selected architecture. In this study, we focus on endoscopic\nsurgical instruments as artifacts, and determine and discuss the artifact\nremoval accuracy using seven different CNN architectures.",
          "link": "http://arxiv.org/abs/2201.00084",
          "publishedOn": "2022-01-05T00:39:36.212Z",
          "wordCount": 535,
          "title": "Performance Comparison of Deep Learning Architectures for Artifact Removal in Gastrointestinal Endoscopic Imaging. (arXiv:2201.00084v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_V/0/1/0/all/0/1\">Vivek Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundararaman_D/0/1/0/all/0/1\">Dhanasekar Sundararaman</a>",
          "description": "Neural machine translation (NMT) systems aim to map text from one language\ninto another. While there are a wide variety of applications of NMT, one of the\nmost important is translation of natural language. A distinguishing factor of\nnatural language is that words are typically ordered according to the rules of\nthe grammar of a given language. Although many advances have been made in\ndeveloping NMT systems for translating natural language, little research has\nbeen done on understanding how the word ordering of and lexical similarity\nbetween the source and target language affect translation performance. Here, we\ninvestigate these relationships on a variety of low-resource language pairs\nfrom the OpenSubtitles2016 database, where the source language is English, and\nfind that the more similar the target language is to English, the greater the\ntranslation performance. In addition, we study the impact of providing NMT\nmodels with part of speech of words (POS) in the English sequence and find\nthat, for Transformer-based models, the more dissimilar the target language is\nfrom English, the greater the benefit provided by POS.",
          "link": "http://arxiv.org/abs/2201.00075",
          "publishedOn": "2022-01-05T00:39:36.201Z",
          "wordCount": 597,
          "title": "How do lexical semantics affect translation? An empirical study. (arXiv:2201.00075v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.06428",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ibriga_H/0/1/0/all/0/1\">Hilda S Ibriga</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sun_W/0/1/0/all/0/1\">Will Wei Sun</a>",
          "description": "We aim to provably complete a sparse and highly-missing tensor in the\npresence of covariate information along tensor modes. Our motivation comes from\nonline advertising where users click-through-rates (CTR) on ads over various\ndevices form a CTR tensor that has about 96% missing entries and has many zeros\non non-missing entries, which makes the standalone tensor completion method\nunsatisfactory. Beside the CTR tensor, additional ad features or user\ncharacteristics are often available. In this paper, we propose\nCovariate-assisted Sparse Tensor Completion (COSTCO) to incorporate covariate\ninformation for the recovery of the sparse tensor. The key idea is to jointly\nextract latent components from both the tensor and the covariate matrix to\nlearn a synthetic representation. Theoretically, we derive the error bound for\nthe recovered tensor components and explicitly quantify the improvements on\nboth the reveal probability condition and the tensor recovery accuracy due to\ncovariates. Finally, we apply COSTCO to an advertisement dataset consisting of\na CTR tensor and ad covariate matrix, leading to 23% accuracy improvement over\nthe baseline. An important by-product is that ad latent components from COSTCO\nreveal interesting ad clusters, which are useful for better ad targeting.",
          "link": "http://arxiv.org/abs/2103.06428",
          "publishedOn": "2022-01-05T00:39:36.193Z",
          "wordCount": 629,
          "title": "Covariate-assisted Sparse Tensor Completion. (arXiv:2103.06428v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07976",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Risheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_P/0/1/0/all/0/1\">Pan Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaoming Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Shangzhi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>",
          "description": "In recent years, a variety of gradient-based methods have been developed to\nsolve Bi-Level Optimization (BLO) problems in machine learning and computer\nvision areas. However, the theoretical correctness and practical effectiveness\nof these existing approaches always rely on some restrictive conditions (e.g.,\nLower-Level Singleton, LLS), which could hardly be satisfied in real-world\napplications. Moreover, previous literature only proves theoretical results\nbased on their specific iteration strategies, thus lack a general recipe to\nuniformly analyze the convergence behaviors of different gradient-based BLOs.\nIn this work, we formulate BLOs from an optimistic bi-level viewpoint and\nestablish a new gradient-based algorithmic framework, named Bi-level Descent\nAggregation (BDA), to partially address the above issues. Specifically, BDA\nprovides a modularized structure to hierarchically aggregate both the upper-\nand lower-level subproblems to generate our bi-level iterative dynamics.\nTheoretically, we establish a general convergence analysis template and derive\na new proof recipe to investigate the essential theoretical properties of\ngradient-based BLO methods. Furthermore, this work systematically explores the\nconvergence behavior of BDA in different optimization scenarios, i.e.,\nconsidering various solution qualities (i.e., global/local/stationary solution)\nreturned from solving approximation subproblems. Extensive experiments justify\nour theoretical results and demonstrate the superiority of the proposed\nalgorithm for hyper-parameter optimization and meta-learning tasks. Source code\nis available at https://github.com/vis-opt-group/BDA.",
          "link": "http://arxiv.org/abs/2102.07976",
          "publishedOn": "2022-01-05T00:39:36.125Z",
          "wordCount": null,
          "title": "A General Descent Aggregation Framework for Gradient-based Bi-level Optimization. (arXiv:2102.07976v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00418",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shetty_S/0/1/0/all/0/1\">Sannidhi Shetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raunak Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laban_R/0/1/0/all/0/1\">Ronald Melwin Laban</a>",
          "description": "Prognostication of medical problems using the clinical data by leveraging the\nMachine Learning techniques with stellar precision is one of the most important\nreal world challenges at the present time. Considering the medical problem of\nPolycystic Ovary Syndrome also known as PCOS is an emerging problem in women\naged from 15 to 49. Diagnosing this disorder by using various Boosting Ensemble\nMethods is something we have presented in this paper. A detailed and\ncompendious differentiation between Adaptive Boost, Gradient Boosting Machine,\nXGBoost and CatBoost with their respective performance metrics highlighting the\nhidden anomalies in the data and its effects on the result is something we have\npresented in this paper. Metrics like Confusion Matrix, Precision, Recall, F1\nScore, FPR, RoC Curve and AUC have been used in this paper.",
          "link": "http://arxiv.org/abs/2201.00418",
          "publishedOn": "2022-01-05T00:39:36.105Z",
          "wordCount": null,
          "title": "Succinct Differentiation of Disparate Boosting Ensemble Learning Methods for Prognostication of Polycystic Ovary Syndrome Diagnosis. (arXiv:2201.00418v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sohoni_N/0/1/0/all/0/1\">Nimit Sohoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1\">Maziar Sanjabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1\">Nicolas Ballas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1\">Aditya Grover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1\">Shaoliang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1\">Hamed Firooz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>",
          "description": "While neural networks have shown remarkable success on classification tasks\nin terms of average-case performance, they often fail to perform well on\ncertain groups of the data. Such group information may be expensive to obtain;\nthus, recent works in robustness and fairness have proposed ways to improve\nworst-group performance even when group labels are unavailable for the training\ndata. However, these methods generally underperform methods that utilize group\ninformation at training time. In this work, we assume access to a small number\nof group labels alongside a larger dataset without group labels. We propose\nBARACK, a simple two-step framework to utilize this partial group information\nto improve worst-group performance: train a model to predict the missing group\nlabels for the training data, and then use these predicted group labels in a\nrobust optimization objective. Theoretically, we provide generalization bounds\nfor our approach in terms of the worst-group performance, showing how the\ngeneralization error scales with respect to both the total number of training\npoints and the number of training points with group labels. Empirically, our\nmethod outperforms the baselines that do not use group information, even when\nonly 1-33% of points have group labels. We provide ablation studies to support\nthe robustness and extensibility of our framework.",
          "link": "http://arxiv.org/abs/2201.00072",
          "publishedOn": "2022-01-05T00:39:35.414Z",
          "wordCount": 631,
          "title": "BARACK: Partially Supervised Group Robustness With Guarantees. (arXiv:2201.00072v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shunhua Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Binghui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinstein_O/0/1/0/all/0/1\">Omri Weinstein</a>",
          "description": "A common challenge in large-scale supervised learning, is how to exploit new\nincremental data to a pre-trained model, without re-training the model from\nscratch. Motivated by this problem, we revisit the canonical problem of dynamic\nleast-squares regression (LSR), where the goal is to learn a linear model over\nincremental training data. In this setup, data and labels $(\\mathbf{A}^{(t)},\n\\mathbf{b}^{(t)}) \\in \\mathbb{R}^{t \\times d}\\times \\mathbb{R}^t$ evolve in an\nonline fashion ($t\\gg d$), and the goal is to efficiently maintain an\n(approximate) solution to $\\min_{\\mathbf{x}^{(t)}} \\| \\mathbf{A}^{(t)}\n\\mathbf{x}^{(t)} - \\mathbf{b}^{(t)} \\|_2$ for all $t\\in [T]$. Our main result\nis a dynamic data structure which maintains an arbitrarily small constant\napproximate solution to dynamic LSR with amortized update time $O(d^{1+o(1)})$,\nalmost matching the running time of the static (sketching-based) solution. By\ncontrast, for exact (or even $1/\\mathrm{poly}(n)$-accuracy) solutions, we show\na separation between the static and dynamic settings, namely, that dynamic LSR\nrequires $\\Omega(d^{2-o(1)})$ amortized update time under the OMv Conjecture\n(Henzinger et al., STOC'15). Our data structure is conceptually simple, easy to\nimplement, and fast both in theory and practice, as corroborated by experiments\nover both synthetic and real-world datasets.",
          "link": "http://arxiv.org/abs/2201.00228",
          "publishedOn": "2022-01-05T00:39:35.381Z",
          "wordCount": 601,
          "title": "Dynamic Least-Squares Regression. (arXiv:2201.00228v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Godwin_T/0/1/0/all/0/1\">Toby Godwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizos_G/0/1/0/all/0/1\">Georgios Rizos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baird_A/0/1/0/all/0/1\">Alice Baird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Futaisi_N/0/1/0/all/0/1\">Najla D. Al Futaisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brisse_V/0/1/0/all/0/1\">Vincent Brisse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bjoern W. Schuller</a>",
          "description": "Despite advances in deep algorithmic music generation, evaluation of\ngenerated samples often relies on human evaluation, which is subjective and\ncostly. We focus on designing a homogeneous, objective framework for evaluating\nsamples of algorithmically generated music. Any engineered measures to evaluate\ngenerated music typically attempt to define the samples' musicality, but do not\ncapture qualities of music such as theme or mood. We do not seek to assess the\nmusical merit of generated music, but instead explore whether generated samples\ncontain meaningful information pertaining to emotion or mood/theme. We achieve\nthis by measuring the change in predictive performance of a music mood/theme\nclassifier after augmenting its training data with generated samples. We\nanalyse music samples generated by three models -- SampleRNN, Jukebox, and DDSP\n-- and employ a homogeneous framework across all methods to allow for objective\ncomparison. This is the first attempt at augmenting a music genre\nclassification dataset with conditionally generated music. We investigate the\nclassification performance improvement using deep music generation and the\nability of the generators to make emotional music by using an additional,\nemotion annotation of the dataset. Finally, we use a classifier trained on real\ndata to evaluate the label validity of class-conditionally generated samples.",
          "link": "http://arxiv.org/abs/2201.00052",
          "publishedOn": "2022-01-05T00:39:35.375Z",
          "wordCount": 646,
          "title": "Evaluating Deep Music Generation Methods Using Data Augmentation. (arXiv:2201.00052v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian-wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Hao-jie Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Run-kun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiong-lin Luo</a>",
          "description": "Multi-view learning can cover all features of data samples more\ncomprehensively, so multi-view learning has attracted widespread attention.\nTraditional subspace clustering methods, such as sparse subspace clustering\n(SSC) and low-ranking subspace clustering (LRSC), cluster the affinity matrix\nfor a single view, thus ignoring the problem of fusion between views. In our\narticle, we propose a new Multiview Subspace Adaptive Learning based on\nAttention and Autoencoder (MSALAA). This method combines a deep autoencoder and\na method for aligning the self-representations of various views in Multi-view\nLow-Rank Sparse Subspace Clustering (MLRSSC), which can not only increase the\ncapability to non-linearity fitting, but also can meets the principles of\nconsistency and complementarity of multi-view learning. We empirically observe\nsignificant improvement over existing baseline methods on six real-life\ndatasets.",
          "link": "http://arxiv.org/abs/2201.00171",
          "publishedOn": "2022-01-05T00:39:35.368Z",
          "wordCount": 548,
          "title": "Multi-view Subspace Adaptive Learning via Autoencoder and Attention. (arXiv:2201.00171v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_D/0/1/0/all/0/1\">Duy-Hoa Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemp_M/0/1/0/all/0/1\">Madonna Kemp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truran_D/0/1/0/all/0/1\">Donna Truran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koopman_B/0/1/0/all/0/1\">Bevan Koopman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metke_Jimenez_A/0/1/0/all/0/1\">Alejandro Metke-Jimenez</a>",
          "description": "Finding concepts in large clinical ontologies can be challenging when queries\nuse different vocabularies. A search algorithm that overcomes this problem is\nuseful in applications such as concept normalisation and ontology matching,\nwhere concepts can be referred to in different ways, using different synonyms.\nIn this paper, we present a deep learning based approach to build a semantic\nsearch system for large clinical ontologies. We propose a Triplet-BERT model\nand a method that generates training data directly from the ontologies. The\nmodel is evaluated using five real benchmark data sets and the results show\nthat our approach achieves high results on both free text to concept and\nconcept to concept searching tasks, and outperforms all baseline methods.",
          "link": "http://arxiv.org/abs/2201.00118",
          "publishedOn": "2022-01-05T00:39:35.342Z",
          "wordCount": 545,
          "title": "Semantic Search for Large Scale Clinical Ontologies. (arXiv:2201.00118v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_E/0/1/0/all/0/1\">Enyan Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+jIN_W/0/1/0/all/0/1\">Wei jIN</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Suhang Wang</a>",
          "description": "Graph Neural Networks (GNNs) have shown their great ability in modeling graph\nstructured data. However, real-world graphs usually contain structure noises\nand have limited labeled nodes. The performance of GNNs would drop\nsignificantly when trained on such graphs, which hinders the adoption of GNNs\non many applications. Thus, it is important to develop noise-resistant GNNs\nwith limited labeled nodes. However, the work on this is rather limited.\nTherefore, we study a novel problem of developing robust GNNs on noisy graphs\nwith limited labeled nodes. Our analysis shows that both the noisy edges and\nlimited labeled nodes could harm the message-passing mechanism of GNNs. To\nmitigate these issues, we propose a novel framework which adopts the noisy\nedges as supervision to learn a denoised and dense graph, which can down-weight\nor eliminate noisy edges and facilitate message passing of GNNs to alleviate\nthe issue of limited labeled nodes. The generated edges are further used to\nregularize the predictions of unlabeled nodes with label smoothness to better\ntrain GNNs. Experimental results on real-world datasets demonstrate the\nrobustness of the proposed framework on noisy graphs with limited labeled\nnodes.",
          "link": "http://arxiv.org/abs/2201.00232",
          "publishedOn": "2022-01-05T00:39:35.316Z",
          "wordCount": 609,
          "title": "Towards Robust Graph Neural Networks for Noisy Graphs with Sparse Labels. (arXiv:2201.00232v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yunhui Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1\">Zijun Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuanzhi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Bo Yuan</a>",
          "description": "The dynamic job-shop scheduling problem (DJSP) is a class of scheduling tasks\nthat specifically consider the inherent uncertainties such as changing order\nrequirements and possible machine breakdown in realistic smart manufacturing\nsettings. Since traditional methods cannot dynamically generate effective\nscheduling strategies in face of the disturbance of environments, we formulate\nthe DJSP as a Markov decision process (MDP) to be tackled by reinforcement\nlearning (RL). For this purpose, we propose a flexible hybrid framework that\ntakes disjunctive graphs as states and a set of general dispatching rules as\nthe action space with minimum prior domain knowledge. The attention mechanism\nis used as the graph representation learning (GRL) module for the feature\nextraction of states, and the double dueling deep Q-network with prioritized\nreplay and noisy networks (D3QPN) is employed to map each state to the most\nappropriate dispatching rule. Furthermore, we present Gymjsp, a public\nbenchmark based on the well-known OR-Library, to provide a standardized\noff-the-shelf facility for RL and DJSP research communities. Comprehensive\nexperiments on various DJSP instances confirm that our proposed framework is\nsuperior to baseline algorithms with smaller makespan across all instances and\nprovide empirical justification for the validity of the various components in\nthe hybrid framework.",
          "link": "http://arxiv.org/abs/2201.00548",
          "publishedOn": "2022-01-05T00:39:35.295Z",
          "wordCount": 638,
          "title": "Hybrid intelligence for dynamic job-shop scheduling with deep reinforcement learning and attention mechanism. (arXiv:2201.00548v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jianming Deng</a>",
          "description": "There is a general trend of applying reinforcement learning (RL) techniques\nfor traffic signal control (TSC). Recently, most studies pay attention to the\nneural network design and rarely concentrate on the state representation. Does\nthe design of state representation has a good impact on TSC? In this paper, we\n(1) propose an effective state representation as queue length of vehicles with\nintensive knowledge; (2) present a TSC method called MaxQueue based on our\nstate representation approach; (3) develop a general RL-based TSC template\ncalled QL-XLight with queue length as state and reward and generate QL-FRAP,\nQL-CoLight, and QL-DQN by our QL-XLight template based on traditional and\nlatest RL models.Through comprehensive experiments on multiple real-world\ndatasets, we demonstrate that: (1) our MaxQueue method outperforms the latest\nRL based methods; (2) QL-FRAP and QL-CoLight achieves a new state-of-the-art\n(SOTA). In general, state representation with intensive knowledge is also\nessential for TSC methods. Our code is released on Github.",
          "link": "http://arxiv.org/abs/2201.00006",
          "publishedOn": "2022-01-05T00:39:35.229Z",
          "wordCount": 587,
          "title": "Knowledge intensive state design for traffic signal control. (arXiv:2201.00006v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiyang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botta_N/0/1/0/all/0/1\">Nicola Botta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_C/0/1/0/all/0/1\">Cezar Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">George H. Chen</a>",
          "description": "Outlier detection refers to the identification of data points that deviate\nfrom a general data distribution. Existing unsupervised approaches often suffer\nfrom high computational cost, complex hyperparameter tuning, and limited\ninterpretability, especially when working with large, high-dimensional\ndatasets. To address these issues, we present a simple yet effective algorithm\ncalled ECOD (Empirical-Cumulative-distribution-based Outlier Detection), which\nis inspired by the fact that outliers are often the \"rare events\" that appear\nin the tails of a distribution. In a nutshell, ECOD first estimates the\nunderlying distribution of the input data in a nonparametric fashion by\ncomputing the empirical cumulative distribution per dimension of the data. ECOD\nthen uses these empirical distributions to estimate tail probabilities per\ndimension for each data point. Finally, ECOD computes an outlier score of each\ndata point by aggregating estimated tail probabilities across dimensions. Our\ncontributions are as follows: (1) we propose a novel outlier detection method\ncalled ECOD, which is both parameter-free and easy to interpret; (2) we perform\nextensive experiments on 30 benchmark datasets, where we find that ECOD\noutperforms 11 state-of-the-art baselines in terms of accuracy, efficiency, and\nscalability; and (3) we release an easy-to-use and scalable (with distributed\nsupport) Python implementation for accessibility and reproducibility.",
          "link": "http://arxiv.org/abs/2201.00382",
          "publishedOn": "2022-01-05T00:39:34.994Z",
          "wordCount": 647,
          "title": "ECOD: Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions. (arXiv:2201.00382v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Astudillo_R/0/1/0/all/0/1\">Raul Astudillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frazier_P/0/1/0/all/0/1\">Peter I. Frazier</a>",
          "description": "Bayesian optimization (BO) is a framework for global optimization of\nexpensive-to-evaluate objective functions. Classical BO methods assume that the\nobjective function is a black box. However, internal information about\nobjective function computation is often available. For example, when optimizing\na manufacturing line's throughput with simulation, we observe the number of\nparts waiting at each workstation, in addition to the overall throughput.\nRecent BO methods leverage such internal information to dramatically improve\nperformance. We call these \"grey-box\" BO methods because they treat objective\ncomputation as partially observable and even modifiable, blending the black-box\napproach with so-called \"white-box\" first-principles knowledge of objective\nfunction computation. This tutorial describes these methods, focusing on BO of\ncomposite objective functions, where one can observe and selectively evaluate\nindividual constituents that feed into the overall objective; and\nmulti-fidelity BO, where one can evaluate cheaper approximations of the\nobjective function by varying parameters of the evaluation oracle.",
          "link": "http://arxiv.org/abs/2201.00272",
          "publishedOn": "2022-01-05T00:39:34.987Z",
          "wordCount": 592,
          "title": "Thinking inside the box: A tutorial on grey-box Bayesian optimization. (arXiv:2201.00272v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00435",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Pai_S/0/1/0/all/0/1\">Saeel S. Pai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Banthiya_A/0/1/0/all/0/1\">Abhijeet Banthiya</a>",
          "description": "Heat transfer characteristics of nanofluids have been extensively studied\nsince the 1990s. Research investigations show that the suspended nanoparticles\nsignificantly alter the suspension's thermal properties. The thermal\nconductivity of nanofluids is one of the properties that is generally found to\nbe greater than that of the base fluid. This increase in thermal conductivity\nis found to depend on several parameters. Several theories have been proposed\nto model the thermal conductivities of nanofluids, but there is no reliable\nuniversal theory yet to model the anomalous thermal conductivity of nanofluids.\nIn recent years, supervised data-driven methods have been successfully employed\nto create surrogate models across various scientific disciplines, especially\nfor modeling difficult-to-understand phenomena. These supervised learning\nmethods allow the models to capture highly non-linear phenomena. In this work,\nwe have taken advantage of existing correlations and used them concurrently\nwith available experimental results to develop more robust surrogate models for\npredicting the thermal conductivity of nanofluids. Artificial neural networks\nare trained using the transfer learning approach to predict the thermal\nconductivity enhancement of nanofluids with spherical particles for 32\ndifferent particle-fluid combinations (8 particles materials and 4 fluids). The\nlarge amount of lower accuracy data generated from correlations is used to\ncoarse-tune the model parameters, and the limited amount of more trustworthy\nexperimental data is used to fine-tune the model parameters. The transfer\nlearning-based models' results are compared with those from baseline models\nwhich are trained only on experimental data using a goodness of fit metric. It\nis found that the transfer learning models perform better with goodness of fit\nvalues of 0.93 as opposed to 0.83 from the baseline models.",
          "link": "http://arxiv.org/abs/2201.00435",
          "publishedOn": "2022-01-05T00:39:34.834Z",
          "wordCount": 693,
          "title": "Transfer-learning-based Surrogate Model for Thermal Conductivity of Nanofluids. (arXiv:2201.00435v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00016",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xingyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yi Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiaqi Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tieqiao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>",
          "description": "Log anomaly detection is a key component in the field of artificial\nintelligence for IT operations (AIOps). Considering log data of variant\ndomains, retraining the whole network for unknown domains is inefficient in\nreal industrial scenarios especially for low-resource domains. However,\nprevious deep models merely focused on extracting the semantics of log sequence\nin the same domain, leading to poor generalization on multi-domain logs.\nTherefore, we propose a unified Transformer-based framework for log anomaly\ndetection (\\ourmethod{}), which is comprised of the pretraining and\nadapter-based tuning stage. Our model is first pretrained on the source domain\nto obtain shared semantic knowledge of log data. Then, we transfer the\npretrained model to the target domain via the adapter-based tuning. The\nproposed method is evaluated on three public datasets including one source\ndomain and two target domains. The experimental results demonstrate that our\nsimple yet efficient approach, with fewer trainable parameters and lower\ntraining costs in the target domain, achieves state-of-the-art performance on\nthree benchmarks.",
          "link": "http://arxiv.org/abs/2201.00016",
          "publishedOn": "2022-01-05T00:39:34.827Z",
          "wordCount": 596,
          "title": "TransLog: A Unified Transformer-based Framework for Log Anomaly Detection. (arXiv:2201.00016v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00620",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Sweeney_L/0/1/0/all/0/1\">Lorin Sweeney</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Matran_Fernandez_A/0/1/0/all/0/1\">Ana Matran-Fernandez</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Halder_S/0/1/0/all/0/1\">Sebastian Halder</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Herrera_A/0/1/0/all/0/1\">Alba G. Seco de Herrera</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Smeaton_A/0/1/0/all/0/1\">Alan Smeaton</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Healy_G/0/1/0/all/0/1\">Graham Healy</a>",
          "description": "The aim of the Memorability-EEG pilot subtask at MediaEval'2021 is to promote\ninterest in the use of neural signals -- either alone or in combination with\nother data sources -- in the context of predicting video memorability by\nhighlighting the utility of EEG data. The dataset created consists of\npre-extracted features from EEG recordings of subjects while watching a subset\nof videos from Predicting Media Memorability subtask 1. This demonstration\npilot gives interested researchers a sense of how neural signals can be used\nwithout any prior domain knowledge, and enables them to do so in a future\nmemorability task. The dataset can be used to support the exploration of novel\nmachine learning and processing strategies for predicting video memorability,\nwhile potentially increasing interdisciplinary interest in the subject of\nmemorability, and opening the door to new combined EEG-computer vision\napproaches.",
          "link": "http://arxiv.org/abs/2201.00620",
          "publishedOn": "2022-01-05T00:39:34.730Z",
          "wordCount": 589,
          "title": "Overview of the EEG Pilot Subtask at MediaEval 2021: Predicting Media Memorability. (arXiv:2201.00620v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00604",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rabadan_M/0/1/0/all/0/1\">Miquel Mart&#xed; i Rabad&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bujwid_S/0/1/0/all/0/1\">Sebastian Bujwid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pieropan_A/0/1/0/all/0/1\">Alessandro Pieropan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizpour_H/0/1/0/all/0/1\">Hossein Azizpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maki_A/0/1/0/all/0/1\">Atsuto Maki</a>",
          "description": "Most semi-supervised learning methods over-sample labeled data when\nconstructing training mini-batches. This paper studies whether this common\npractice improves learning and how. We compare it to an alternative setting\nwhere each mini-batch is uniformly sampled from all the training data, labeled\nor not, which greatly reduces direct supervision from true labels in typical\nlow-label regimes. However, this simpler setting can also be seen as more\ngeneral and even necessary in multi-task problems where over-sampling labeled\ndata would become intractable. Our experiments on semi-supervised CIFAR-10\nimage classification using FixMatch show a performance drop when using the\nuniform sampling approach which diminishes when the amount of labeled data or\nthe training time increases. Further, we analyse the training dynamics to\nunderstand how over-sampling of labeled data compares to uniform sampling. Our\nmain finding is that over-sampling is especially beneficial early in training\nbut gets less important in the later stages when more pseudo-labels become\ncorrect. Nevertheless, we also find that keeping some true labels remains\nimportant to avoid the accumulation of confirmation errors from incorrect\npseudo-labels.",
          "link": "http://arxiv.org/abs/2201.00604",
          "publishedOn": "2022-01-05T00:39:34.705Z",
          "wordCount": 609,
          "title": "An analysis of over-sampling labeled data in semi-supervised learning with FixMatch. (arXiv:2201.00604v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cholakov_R/0/1/0/all/0/1\">Radostin Cholakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolev_T/0/1/0/all/0/1\">Todor Kolev</a>",
          "description": "There is an increasing interest in the application of deep learning\narchitectures to tabular data. One of the state-of-the-art solutions is\nTabTransformer which incorporates an attention mechanism to better track\nrelationships between categorical features and then makes use of a standard MLP\nto output its final logits. In this paper we propose multiple modifications to\nthe original TabTransformer performing better on binary classification tasks\nfor three separate datasets with more than 1% AUROC gains. Inspired by gated\nMLP, linear projections are implemented in the MLP block and multiple\nactivation functions are tested. We also evaluate the importance of specific\nhyper parameters during training.",
          "link": "http://arxiv.org/abs/2201.00199",
          "publishedOn": "2022-01-05T00:39:34.686Z",
          "wordCount": 528,
          "title": "The GatedTabTransformer. An enhanced deep learning architecture for tabular modeling. (arXiv:2201.00199v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Y/0/1/0/all/0/1\">Yangjun Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1\">Yann Dubois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maddison_C/0/1/0/all/0/1\">Chris J. Maddison</a>",
          "description": "Machine learning systems often experience a distribution shift between\ntraining and testing. In this paper, we introduce a simple variational\nobjective whose optima are exactly the set of all representations on which risk\nminimizers are guaranteed to be robust to any distribution shift that preserves\nthe Bayes predictor, e.g., covariate shifts. Our objective has two components.\nFirst, a representation must remain discriminative for the task, i.e., some\npredictor must be able to simultaneously minimize the source and target risk.\nSecond, the representation's marginal support needs to be the same across\nsource and target. We make this practical by designing self-supervised learning\nmethods that only use unlabelled data and augmentations to train robust\nrepresentations. Our objectives achieve state-of-the-art results on DomainBed,\nand give insights into the robustness of recent methods, such as CLIP.",
          "link": "http://arxiv.org/abs/2201.00057",
          "publishedOn": "2022-01-05T00:39:34.517Z",
          "wordCount": 556,
          "title": "Optimal Representations for Covariate Shift. (arXiv:2201.00057v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00801",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Havens_A/0/1/0/all/0/1\">Aaron Havens</a>, <a href=\"http://arxiv.org/find/math/1/au:+Keivan_D/0/1/0/all/0/1\">Darioush Keivan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Seiler_P/0/1/0/all/0/1\">Peter Seiler</a>, <a href=\"http://arxiv.org/find/math/1/au:+Dullerud_G/0/1/0/all/0/1\">Geir Dullerud</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hu_B/0/1/0/all/0/1\">Bin Hu</a>",
          "description": "Many existing region-of-attraction (ROA) analysis tools find difficulty in\naddressing feedback systems with large-scale neural network (NN) policies\nand/or high-dimensional sensing modalities such as cameras. In this paper, we\ntailor the projected gradient descent (PGD) attack method developed in the\nadversarial learning community as a general-purpose ROA analysis tool for\nlarge-scale nonlinear systems and end-to-end perception-based control. We show\nthat the ROA analysis can be approximated as a constrained maximization problem\nwhose goal is to find the worst-case initial condition which shifts the\nterminal state the most. Then we present two PGD-based iterative methods which\ncan be used to solve the resultant constrained maximization problem. Our\nanalysis is not based on Lyapunov theory, and hence requires minimum\ninformation of the problem structures. In the model-based setting, we show that\nthe PGD updates can be efficiently performed using back-propagation. In the\nmodel-free setting (which is more relevant to ROA analysis of perception-based\ncontrol), we propose a finite-difference PGD estimate which is general and only\nrequires a black-box simulator for generating the trajectories of the\nclosed-loop system given any initial state. We demonstrate the scalability and\ngenerality of our analysis tool on several numerical examples with large-scale\nNN policies and high-dimensional image observations. We believe that our\nproposed analysis serves as a meaningful initial step toward further\nunderstanding of closed-loop stability of large-scale nonlinear systems and\nperception-based control.",
          "link": "http://arxiv.org/abs/2201.00801",
          "publishedOn": "2022-01-05T00:39:34.508Z",
          "wordCount": 666,
          "title": "Revisiting PGD Attacks for Stability Analysis of Large-Scale Nonlinear Systems and Perception-Based Control. (arXiv:2201.00801v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kamoutsi_A/0/1/0/all/0/1\">Angeliki Kamoutsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banjac_G/0/1/0/all/0/1\">Goran Banjac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lygeros_J/0/1/0/all/0/1\">John Lygeros</a>",
          "description": "We consider large-scale Markov decision processes (MDPs) with an unknown cost\nfunction and employ stochastic convex optimization tools to address the problem\nof imitation learning, which consists of learning a policy from a finite set of\nexpert demonstrations.\n\nWe adopt the apprenticeship learning formalism, which carries the assumption\nthat the true cost function can be represented as a linear combination of some\nknown features. Existing inverse reinforcement learning algorithms come with\nstrong theoretical guarantees, but are computationally expensive because they\nuse reinforcement learning or planning algorithms as a subroutine. On the other\nhand, state-of-the-art policy gradient based algorithms (like IM-REINFORCE,\nIM-TRPO, and GAIL), achieve significant empirical success in challenging\nbenchmark tasks, but are not well understood in terms of theory. With an\nemphasis on non-asymptotic guarantees of performance, we propose a method that\ndirectly learns a policy from expert demonstrations, bypassing the intermediate\nstep of learning the cost function, by formulating the problem as a single\nconvex optimization problem over occupancy measures. We develop a\ncomputationally efficient algorithm and derive high confidence regret bounds on\nthe quality of the extracted policy, utilizing results from stochastic convex\noptimization and recent works in approximate linear programming for solving\nforward MDPs.",
          "link": "http://arxiv.org/abs/2201.00039",
          "publishedOn": "2022-01-05T00:39:34.187Z",
          "wordCount": 640,
          "title": "Stochastic convex optimization for provably efficient apprenticeship learning. (arXiv:2201.00039v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00217",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_M/0/1/0/all/0/1\">Minshuo Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liao_W/0/1/0/all/0/1\">Wenjing Liao</a>",
          "description": "Learning operators between infinitely dimensional spaces is an important\nlearning task arising in wide applications in machine learning, imaging\nscience, mathematical modeling and simulations, etc. This paper studies the\nnonparametric estimation of Lipschitz operators using deep neural networks.\nNon-asymptotic upper bounds are derived for the generalization error of the\nempirical risk minimizer over a properly chosen network class. Under the\nassumption that the target operator exhibits a low dimensional structure, our\nerror bounds decay as the training sample size increases, with an attractive\nfast rate depending on the intrinsic dimension in our estimation. Our\nassumptions cover most scenarios in real applications and our results give rise\nto fast rates by exploiting low dimensional structures of data in operator\nestimation. We also investigate the influence of network structures (e.g.,\nnetwork width, depth, and sparsity) on the generalization error of the neural\nnetwork estimator and propose a general suggestion on the choice of network\nstructures to maximize the learning efficiency quantitatively.",
          "link": "http://arxiv.org/abs/2201.00217",
          "publishedOn": "2022-01-05T00:39:34.181Z",
          "wordCount": 584,
          "title": "Deep Nonparametric Estimation of Operators between Infinite Dimensional Spaces. (arXiv:2201.00217v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ackerman_S/0/1/0/all/0/1\">Samuel Ackerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farchi_E/0/1/0/all/0/1\">Eitan Farchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raz_O/0/1/0/all/0/1\">Orna Raz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shehory_O/0/1/0/all/0/1\">Onn Shehory</a>",
          "description": "The crafting of machine learning (ML) based systems requires statistical\ncontrol throughout its life cycle. Careful quantification of business\nrequirements and identification of key factors that impact the business\nrequirements reduces the risk of a project failure. The quantification of\nbusiness requirements results in the definition of random variables\nrepresenting the system key performance indicators that need to be analyzed\nthrough statistical experiments. In addition, available data for training and\nexperiment results impact the design of the system. Once the system is\ndeveloped, it is tested and continually monitored to ensure it meets its\nbusiness requirements. This is done through the continued application of\nstatistical experiments to analyze and control the key performance indicators.\nThis book teaches the art of crafting and developing ML based systems. It\nadvocates an \"experiment first\" approach stressing the need to define\nstatistical experiments from the beginning of the project life cycle. It also\ndiscusses in detail how to apply statistical control on the ML based system\nthroughout its lifecycle.",
          "link": "http://arxiv.org/abs/2201.00355",
          "publishedOn": "2022-01-05T00:39:34.173Z",
          "wordCount": 587,
          "title": "Experiment Based Crafting and Analyzing of Machine Learning Solutions. (arXiv:2201.00355v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00063",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gazzaz_S/0/1/0/all/0/1\">Samaa Gazzaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chakraborty_V/0/1/0/all/0/1\">Vishal Chakraborty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nawab_F/0/1/0/all/0/1\">Faisal Nawab</a>",
          "description": "Emerging edge applications require both a fast response latency and complex\nprocessing. This is infeasible without expensive hardware that can process\ncomplex operations -- such as object detection -- within a short time. Many\napproach this problem by addressing the complexity of the models -- via model\ncompression, pruning and quantization -- or compressing the input. In this\npaper, we propose a different perspective when addressing the performance\nchallenges. Croesus is a multi-stage approach to edge-cloud systems that\nprovides the ability to find the balance between accuracy and performance.\nCroesus consists of two stages (that can be generalized to multiple stages): an\ninitial and a final stage. The initial stage performs the computation in\nreal-time using approximate/best-effort computation at the edge. The final\nstage performs the full computation at the cloud, and uses the results to\ncorrect any errors made at the initial stage. In this paper, we demonstrate the\nimplications of such an approach on a video analytics use-case and show how\nmulti-stage processing yields a better balance between accuracy and\nperformance. Moreover, we study the safety of multi-stage transactions via two\nproposals: multi-stage serializability (MS-SR) and multi-stage invariant\nconfluence with Apologies (MS-IA).",
          "link": "http://arxiv.org/abs/2201.00063",
          "publishedOn": "2022-01-05T00:39:34.154Z",
          "wordCount": 641,
          "title": "Croesus: Multi-Stage Processing and Transactions for Video-Analytics in Edge-Cloud Systems. (arXiv:2201.00063v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00455",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadeghian_B/0/1/0/all/0/1\">Bejan Sadeghian</a>",
          "description": "Significant work has been placed in the Q&A NLP space to build models that\nare more robust to adversarial attacks. Two key areas of focus are in\ngenerating adversarial data for the purposes of training against these\nsituations or modifying existing architectures to build robustness within. This\npaper introduces an approach that joins these two ideas together to train a\ncritic model for use in an almost reinforcement learning framework. Using the\nAdversarial SQuAD \"Add One Sent\" dataset we show that there are some promising\nsigns for this method in protecting against Adversarial attacks.",
          "link": "http://arxiv.org/abs/2201.00455",
          "publishedOn": "2022-01-05T00:39:34.147Z",
          "wordCount": 520,
          "title": "Actor-Critic Network for Q&A in an Adversarial Environment. (arXiv:2201.00455v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_K/0/1/0/all/0/1\">Kushagra Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Avideep Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1\">Piyush Rai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhishek Kumar</a>",
          "description": "Diffusion Probabilistic models have been shown to generate state-of-the-art\nresults on several competitive image synthesis benchmarks but lack a\nlow-dimensional, interpretable latent space, and are slow at generation. On the\nother hand, Variational Autoencoders (VAEs) typically have access to a\nlow-dimensional latent space but exhibit poor sample quality. Despite recent\nadvances, VAEs usually require high-dimensional hierarchies of the latent codes\nto generate high-quality samples. We present DiffuseVAE, a novel generative\nframework that integrates VAE within a diffusion model framework, and leverage\nthis to design a novel conditional parameterization for diffusion models. We\nshow that the resulting model can improve upon the unconditional diffusion\nmodel in terms of sampling efficiency while also equipping diffusion models\nwith the low-dimensional VAE inferred latent code. Furthermore, we show that\nthe proposed model can generate high-resolution samples and exhibits synthesis\nquality comparable to state-of-the-art models on standard benchmarks. Lastly,\nwe show that the proposed method can be used for controllable image synthesis\nand also exhibits out-of-the-box capabilities for downstream tasks like image\nsuper-resolution and denoising. For reproducibility, our source code is\npublicly available at \\url{https://github.com/kpandey008/DiffuseVAE}.",
          "link": "http://arxiv.org/abs/2201.00308",
          "publishedOn": "2022-01-05T00:39:34.141Z",
          "wordCount": 608,
          "title": "DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents. (arXiv:2201.00308v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2111.08878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulshrestha_A/0/1/0/all/0/1\">Ankit Kulshrestha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safro_I/0/1/0/all/0/1\">Ilya Safro</a>",
          "description": "The rapid growth of data in the recent years has led to the development of\ncomplex learning algorithms that are often used to make decisions in real\nworld. While the positive impact of the algorithms has been tremendous, there\nis a need to mitigate any bias arising from either training samples or implicit\nassumptions made about the data samples. This need becomes critical when\nalgorithms are used in automated decision making systems that can hugely impact\npeople's lives.\n\nMany approaches have been proposed to make learning algorithms fair by\ndetecting and mitigating bias in different stages of optimization. However, due\nto a lack of a universal definition of fairness, these algorithms optimize for\na particular interpretation of fairness which makes them limited for real world\nuse. Moreover, an underlying assumption that is common to all algorithms is the\napparent equivalence of achieving fairness and removing bias. In other words,\nthere is no user defined criteria that can be incorporated into the\noptimization procedure for producing a fair algorithm. Motivated by these\nshortcomings of existing methods, we propose the CONFAIR procedure that\nproduces a fair algorithm by incorporating user constraints into the\noptimization procedure. Furthermore, we make the process interpretable by\nestimating the most predictive features from data. We demonstrate the efficacy\nof our approach on several real world datasets using different fairness\ncriteria.",
          "link": "http://arxiv.org/abs/2111.08878",
          "publishedOn": "2022-01-03T07:15:44.633Z",
          "wordCount": 678,
          "title": "CONFAIR: Configurable and Interpretable Algorithmic Fairness. (arXiv:2111.08878v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.12933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1\">Tal Ridnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharir_G/0/1/0/all/0/1\">Gilad Sharir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Cohen_A/0/1/0/all/0/1\">Avi Ben-Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1\">Emanuel Ben-Baruch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>",
          "description": "In this paper, we introduce ML-Decoder, a new attention-based classification\nhead. ML-Decoder predicts the existence of class labels via queries, and\nenables better utilization of spatial data compared to global average pooling.\nBy redesigning the decoder architecture, and using a novel group-decoding\nscheme, ML-Decoder is highly efficient, and can scale well to thousands of\nclasses. Compared to using a larger backbone, ML-Decoder consistently provides\na better speed-accuracy trade-off. ML-Decoder is also versatile - it can be\nused as a drop-in replacement for various classification heads, and generalize\nto unseen classes when operated with word queries. Novel query augmentations\nfurther improve its generalization ability. Using ML-Decoder, we achieve\nstate-of-the-art results on several classification tasks: on MS-COCO\nmulti-label, we reach 91.4% mAP; on NUS-WIDE zero-shot, we reach 31.1% ZSL mAP;\nand on ImageNet single-label, we reach with vanilla ResNet50 backbone a new top\nscore of 80.7%, without extra data or distillation. Public code is available\nat: https://github.com/Alibaba-MIIL/ML_Decoder",
          "link": "http://arxiv.org/abs/2111.12933",
          "publishedOn": "2022-01-03T07:15:44.611Z",
          "wordCount": 610,
          "title": "ML-Decoder: Scalable and Versatile Classification Head. (arXiv:2111.12933v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eysenbach_B/0/1/0/all/0/1\">Benjamin Eysenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>",
          "description": "Reinforcement learning (RL) algorithms assume that users specify tasks by\nmanually writing down a reward function. However, this process can be laborious\nand demands considerable technical expertise. Can we devise RL algorithms that\ninstead enable users to specify tasks simply by providing examples of\nsuccessful outcomes? In this paper, we derive a control algorithm that\nmaximizes the future probability of these successful outcome examples. Prior\nwork has approached similar problems with a two-stage process, first learning a\nreward function and then optimizing this reward function using another RL\nalgorithm. In contrast, our method directly learns a value function from\ntransitions and successful outcomes, without learning this intermediate reward\nfunction. Our method therefore requires fewer hyperparameters to tune and lines\nof code to debug. We show that our method satisfies a new data-driven Bellman\nequation, where examples take the place of the typical reward function term.\nExperiments show that our approach outperforms prior methods that learn\nexplicit reward functions.",
          "link": "http://arxiv.org/abs/2103.12656",
          "publishedOn": "2022-01-03T07:15:44.409Z",
          "wordCount": 621,
          "title": "Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification. (arXiv:2103.12656v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiafei Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Samson Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hui Li Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongyuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Cheston Tan</a>",
          "description": "There has been an emerging paradigm shift from the era of \"internet AI\" to\n\"embodied AI\", where AI algorithms and agents no longer learn from datasets of\nimages, videos or text curated primarily from the internet. Instead, they learn\nthrough interactions with their environments from an egocentric perception\nsimilar to humans. Consequently, there has been substantial growth in the\ndemand for embodied AI simulators to support various embodied AI research\ntasks. This growing interest in embodied AI is beneficial to the greater\npursuit of Artificial General Intelligence (AGI), but there has not been a\ncontemporary and comprehensive survey of this field. This paper aims to provide\nan encyclopedic survey for the field of embodied AI, from its simulators to its\nresearch. By evaluating nine current embodied AI simulators with our proposed\nseven features, this paper aims to understand the simulators in their provision\nfor use in embodied AI research and their limitations. Lastly, this paper\nsurveys the three main research tasks in embodied AI -- visual exploration,\nvisual navigation and embodied question answering (QA), covering the\nstate-of-the-art approaches, evaluation metrics and datasets. Finally, with the\nnew insights revealed through surveying the field, the paper will provide\nsuggestions for simulator-for-task selections and recommendations for the\nfuture directions of the field.",
          "link": "http://arxiv.org/abs/2103.04918",
          "publishedOn": "2022-01-03T07:15:44.403Z",
          "wordCount": 716,
          "title": "A Survey of Embodied AI: From Simulators to Research Tasks. (arXiv:2103.04918v6 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karamouzas_I/0/1/0/all/0/1\">Ioannis Karamouzas</a>",
          "description": "We present a simple and intuitive approach for interactive control of\nphysically simulated characters. Our work builds upon generative adversarial\nnetworks (GAN) and reinforcement learning, and introduces an imitation learning\nframework where an ensemble of classifiers and an imitation policy are trained\nin tandem given pre-processed reference clips. The classifiers are trained to\ndiscriminate the reference motion from the motion generated by the imitation\npolicy, while the policy is rewarded for fooling the discriminators. Using our\nGAN-based approach, multiple motor control policies can be trained separately\nto imitate different behaviors. In runtime, our system can respond to external\ncontrol signal provided by the user and interactively switch between different\npolicies. Compared to existing methods, our proposed approach has the following\nattractive properties: 1) achieves state-of-the-art imitation performance\nwithout manually designing and fine tuning a reward function; 2) directly\ncontrols the character without having to track any target reference pose\nexplicitly or implicitly through a phase state; and 3) supports interactive\npolicy switching without requiring any motion generation or motion matching\nmechanism. We highlight the applicability of our approach in a range of\nimitation and interactive control tasks, while also demonstrating its ability\nto withstand external perturbations as well as to recover balance. Overall, our\napproach generates high-fidelity motion, has low runtime cost, and can be\neasily integrated into interactive applications and games.",
          "link": "http://arxiv.org/abs/2105.10066",
          "publishedOn": "2022-01-03T07:15:44.396Z",
          "wordCount": 722,
          "title": "A GAN-Like Approach for Physics-Based Imitation Learning and Interactive Character Control. (arXiv:2105.10066v4 [cs.GR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qi Ye</a>",
          "description": "This article presents a new way to study the theory of regularized learning\nfor generalized data in Banach spaces including representer theorems and\nconvergence theorems. The generalized data are composed of linear functionals\nand real scalars as the input and output elements to represent the discrete\ninformation of different local models. By the extension of the classical\nmachine learning, the empirical risks are computed by the generalized data and\nthe loss functions. According to the techniques of regularization, the exact\nsolutions are approximated globally by minimizing the regularized empirical\nrisks over the Banach spaces. The existence and convergence of the approximate\nsolutions are guaranteed by the relative compactness of the generalized input\ndata in the predual spaces of the Banach spaces.",
          "link": "http://arxiv.org/abs/2109.03159",
          "publishedOn": "2022-01-03T07:15:44.375Z",
          "wordCount": 597,
          "title": "Analysis of Regularized Learning in Banach Spaces. (arXiv:2109.03159v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yerushalmi_D/0/1/0/all/0/1\">Dvir Yerushalmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danon_D/0/1/0/all/0/1\">Dov Danon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1\">Amit H. Bermano</a>",
          "description": "Supervision for image-to-image translation (I2I) tasks is hard to come by,\nbut bears significant effect on the resulting quality. In this paper, we\nobserve that for many Unsupervised I2I (UI2I) scenarios, one domain is more\nfamiliar than the other, and offers in-domain prior knowledge, such as semantic\nsegmentation. We argue that for complex scenes, figuring out the semantic\nstructure of the domain is hard, especially with no supervision, but is an\nimportant part of a successful I2I operation. We hence introduce two techniques\nto incorporate this invaluable in-domain prior knowledge for the benefit of\ntranslation quality: through a novel Multi-Stream generator architecture, and\nthrough a semantic segmentation-based regularization loss term. In essence, we\npropose splitting the input data according to semantic masks, explicitly\nguiding the network to different behavior for the different regions of the\nimage. In addition, we propose training a semantic segmentation network along\nwith the translation task, and to leverage this output as a loss term that\nimproves robustness. We validate our approach on urban data, demonstrating\nsuperior quality in the challenging UI2I tasks of converting day images to\nnight ones. In addition, we also demonstrate how reinforcing the target dataset\nwith our augmented images improves the training of downstream tasks such as the\nclassical detection one.",
          "link": "http://arxiv.org/abs/2112.15091",
          "publishedOn": "2022-01-03T07:15:44.369Z",
          "wordCount": 645,
          "title": "Leveraging in-domain supervision for unsupervised image-to-image translation tasks via multi-stream generators. (arXiv:2112.15091v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maddox_W/0/1/0/all/0/1\">Wesley J. Maddox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_S/0/1/0/all/0/1\">Sanyam Kapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1\">Andrew Gordon Wilson</a>",
          "description": "While recent work on conjugate gradient methods and Lanczos decompositions\nhave achieved scalable Gaussian process inference with highly accurate point\npredictions, in several implementations these iterative methods appear to\nstruggle with numerical instabilities in learning kernel hyperparameters, and\npoor test likelihoods. By investigating CG tolerance, preconditioner rank, and\nLanczos decomposition rank, we provide a particularly simple prescription to\ncorrect these issues: we recommend that one should use a small CG tolerance\n($\\epsilon \\leq 0.01$) and a large root decomposition size ($r \\geq 5000$).\nMoreover, we show that L-BFGS-B is a compelling optimizer for Iterative GPs,\nachieving convergence with fewer gradient updates.",
          "link": "http://arxiv.org/abs/2112.15246",
          "publishedOn": "2022-01-03T07:15:44.362Z",
          "wordCount": 526,
          "title": "When are Iterative Gaussian Processes Reliably Accurate?. (arXiv:2112.15246v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15382",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Polyakov_S/0/1/0/all/0/1\">Stanislav Polyakov</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Demichev_A/0/1/0/all/0/1\">Andrey Demichev</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kryukov_A/0/1/0/all/0/1\">Alexander Kryukov</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Postnikov_E/0/1/0/all/0/1\">Evgeny Postnikov</a>",
          "description": "Extensive air showers created by high-energy particles interacting with the\nEarth atmosphere can be detected using imaging atmospheric Cherenkov telescopes\n(IACTs). The IACT images can be analyzed to distinguish between the events\ncaused by gamma rays and by hadrons and to infer the parameters of the event\nsuch as the energy of the primary particle. We use convolutional neural\nnetworks (CNNs) to analyze Monte Carlo-simulated images from the telescopes of\nthe TAIGA experiment. The analysis includes selection of the images\ncorresponding to the showers caused by gamma rays and estimating the energy of\nthe gamma rays. We compare performance of the CNNs using images from a single\ntelescope and the CNNs using images from two telescopes as inputs.",
          "link": "http://arxiv.org/abs/2112.15382",
          "publishedOn": "2022-01-03T07:15:44.356Z",
          "wordCount": 593,
          "title": "Processing Images from Multiple IACTs in the TAIGA Experiment with Convolutional Neural Networks. (arXiv:2112.15382v1 [astro-ph.IM])"
        },
        {
          "id": "http://arxiv.org/abs/2112.02719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anvari_Z/0/1/0/all/0/1\">Zahra Anvari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athitsos_V/0/1/0/all/0/1\">Vassilis Athitsos</a>",
          "description": "Digitized documents such as scientific articles, tax forms, invoices,\ncontract papers, historic texts are widely used nowadays. These document images\ncould be degraded or damaged due to various reasons including poor lighting\nconditions, shadow, distortions like noise and blur, aging, ink stain,\nbleed-through, watermark, stamp, etc. Document image enhancement plays a\ncrucial role as a pre-processing step in many automated document analysis and\nrecognition tasks such as character recognition. With recent advances in deep\nlearning, many methods are proposed to enhance the quality of these document\nimages. In this paper, we review deep learning-based methods, datasets, and\nmetrics for six main document image enhancement tasks, including binarization,\ndebluring, denoising, defading, watermark removal, and shadow removal. We\nsummarize the recent works for each task and discuss their features,\nchallenges, and limitations. We introduce multiple document image enhancement\ntasks that have received little to no attention, including over and under\nexposure correction, super resolution, and bleed-through removal. We identify\nseveral promising research directions and opportunities for future research.",
          "link": "http://arxiv.org/abs/2112.02719",
          "publishedOn": "2022-01-03T07:15:44.349Z",
          "wordCount": 626,
          "title": "A Survey on Deep learning based Document Image Enhancement. (arXiv:2112.02719v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.04870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Delahunt_C/0/1/0/all/0/1\">Charles B. Delahunt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutz_J/0/1/0/all/0/1\">J. Nathan Kutz</a>",
          "description": "We consider the data-driven discovery of governing equations from time-series\ndata in the limit of high noise. The algorithms developed describe an extensive\ntoolkit of methods for circumventing the deleterious effects of noise in the\ncontext of the sparse identification of nonlinear dynamics (SINDy) framework.\nWe offer two primary contributions, both focused on noisy data acquired from a\nsystem x' = f(x). First, we propose, for use in high-noise settings, an\nextensive toolkit of critically enabling extensions for the SINDy regression\nmethod, to progressively cull functionals from an over-complete library and\nyield a set of sparse equations that regress to the derivate x'. These\ninnovations can extract sparse governing equations and coefficients from\nhigh-noise time-series data (e.g. 300% added noise). For example, it discovers\nthe correct sparse libraries in the Lorenz system, with median coefficient\nestimate errors equal to 1% - 3% (for 50% noise), 6% - 8% (for 100% noise); and\n23% - 25% (for 300% noise). The enabling modules in the toolkit are combined\ninto a single method, but the individual modules can be tactically applied in\nother equation discovery methods (SINDy or not) to improve results on\nhigh-noise data. Second, we propose a technique, applicable to any model\ndiscovery method based on x' = f(x), to assess the accuracy of a discovered\nmodel in the context of non-unique solutions due to noisy data. Currently, this\nnon-uniqueness can obscure a discovered model's accuracy and thus a discovery\nmethod's effectiveness. We describe a technique that uses linear dependencies\namong functionals to transform a discovered model into an equivalent form that\nis closest to the true model, enabling more accurate assessment of a discovered\nmodel's accuracy.",
          "link": "http://arxiv.org/abs/2111.04870",
          "publishedOn": "2022-01-03T07:15:44.331Z",
          "wordCount": 747,
          "title": "A toolkit for data-driven discovery of governing equations in high-noise regimes. (arXiv:2111.04870v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15521",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neupartl_N/0/1/0/all/0/1\">Nils Neup&#xe4;rtl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rothkopf_C/0/1/0/all/0/1\">Constantin A. Rothkopf</a>",
          "description": "Bayesian models of behavior have provided computational level explanations in\na range of psychophysical tasks. One fundamental experimental paradigm is the\nproduction or reproduction task, in which subjects are instructed to generate\nan action that either reproduces a previously sensed stimulus magnitude or\nachieves a target response. This type of task therefore distinguishes itself\nfrom other psychophysical tasks in that the responses are on a continuum and\neffort plays an important role with increasing response magnitude. Based on\nBayesian decision theory we present an inference method to recover perceptual\nuncertainty, response variability, and the cost function underlying human\nresponses. Crucially, the cost function is parameterized such that effort is\nexplicitly included. We present a hybrid inference method employing MCMC\nsampling utilizing appropriate proposal distributions and an inner loop\nutilizing amortized inference with a neural network that approximates the mode\nof the optimal response distribution. We show how this model can be utilized to\navoid unidentifiability of experimental designs and that parameters can be\nrecovered through validation on synthetic and application to experimental data.\nOur approach will enable behavioral scientists to perform Bayesian inference of\ndecision making parameters in production and reproduction tasks.",
          "link": "http://arxiv.org/abs/2112.15521",
          "publishedOn": "2022-01-03T07:15:44.325Z",
          "wordCount": 625,
          "title": "Inferring perceptual decision making parameters from behavior in production and reproduction tasks. (arXiv:2112.15521v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10766",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Manjunath_G/0/1/0/all/0/1\">G Manjunath</a>",
          "description": "The celebrated Takens' embedding theorem concerns embedding an attractor of a\ndynamical system in a Euclidean space of appropriate dimension through a\ngeneric delay-observation map. The embedding also establishes a topological\nconjugacy. In this paper, we show how an arbitrary sequence can be mapped into\nanother space as an attractive solution of a nonautonomous dynamical system.\nSuch mapping also entails a topological conjugacy and an embedding between the\nsequence and the attractive solution spaces. This result is not a\ngeneralization of Takens embedding theorem but helps us understand what exactly\nis required by discrete-time state space models widely used in applications to\nembed an external stimulus onto its solution space. Our results settle another\nbasic problem concerning the perturbation of an autonomous dynamical system. We\ndescribe what exactly happens to the dynamics when exogenous noise perturbs\ncontinuously a local irreducible attracting set (such as a stable fixed point)\nof a discrete-time autonomous dynamical system.",
          "link": "http://arxiv.org/abs/2105.10766",
          "publishedOn": "2022-01-03T07:15:44.319Z",
          "wordCount": 592,
          "title": "Embedding Information onto a Dynamical System. (arXiv:2105.10766v3 [math.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hand_D/0/1/0/all/0/1\">D. J. Hand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anagnostopoulos_C/0/1/0/all/0/1\">C. Anagnostopoulos</a>",
          "description": "The H-measure is a classifier performance measure which takes into account\nthe context of application without requiring a rigid value of relative\nmisclassification costs to be set. Since its introduction in 2009 it has become\nwidely adopted. This paper answers various queries which users have raised\nsince its introduction, including questions about its interpretation, the\nchoice of a weighting function, whether it is strictly proper, and its\ncoherence, and relates the measure to other work.",
          "link": "http://arxiv.org/abs/2106.11888",
          "publishedOn": "2022-01-03T07:15:44.313Z",
          "wordCount": 517,
          "title": "Notes on the H-measure of classifier performance. (arXiv:2106.11888v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.14769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zafar_M/0/1/0/all/0/1\">Muhammad I. Zafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiequn Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xu-Hui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Heng Xiao</a>",
          "description": "Partial differential equations (PDEs) play a dominant role in the\nmathematical modeling of many complex dynamical processes. Solving these PDEs\noften requires prohibitively high computational costs, especially when multiple\nevaluations must be made for different parameters or conditions. After\ntraining, neural operators can provide PDEs solutions significantly faster than\ntraditional PDE solvers. In this work, invariance properties and computational\ncomplexity of two neural operators are examined for transport PDE of a scalar\nquantity. Neural operator based on graph kernel network (GKN) operates on\ngraph-structured data to incorporate nonlocal dependencies. Here we propose a\nmodified formulation of GKN to achieve frame invariance. Vector cloud neural\nnetwork (VCNN) is an alternate neural operator with embedded frame invariance\nwhich operates on point cloud data. GKN-based neural operator demonstrates\nslightly better predictive performance compared to VCNN. However, GKN requires\nan excessively high computational cost that increases quadratically with the\nincreasing number of discretized objects as compared to a linear increase for\nVCNN.",
          "link": "http://arxiv.org/abs/2112.14769",
          "publishedOn": "2022-01-03T07:15:44.307Z",
          "wordCount": 582,
          "title": "Frame invariance and scalability of neural operators for partial differential equations. (arXiv:2112.14769v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12678",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuchang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jiawei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuyi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jessie Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>",
          "description": "Federated edge learning (FEEL) has emerged as an effective approach to reduce\nthe large communication latency in Cloud-based machine learning solutions,\nwhile preserving data privacy. Unfortunately, the learning performance of FEEL\nmay be compromised due to limited training data in a single edge cluster. In\nthis paper, we investigate a novel framework of FEEL, namely semi-decentralized\nfederated edge learning (SD-FEEL). By allowing model aggregation across\ndifferent edge clusters, SD-FEEL enjoys the benefit of FEEL in reducing the\ntraining latency, while improving the learning performance by accessing richer\ntraining data from multiple edge clusters. A training algorithm for SD-FEEL\nwith three main procedures in each round is presented, including local model\nupdates, intra-cluster and inter-cluster model aggregations, which is proved to\nconverge on non-independent and identically distributed (non-IID) data. We also\ncharacterize the interplay between the network topology of the edge servers and\nthe communication overhead of inter-cluster model aggregation on the training\nperformance. Experiment results corroborate our analysis and demonstrate the\neffectiveness of SD-FFEL in achieving faster convergence than traditional\nfederated learning architectures. Besides, guidelines on choosing critical\nhyper-parameters of the training algorithm are also provided.",
          "link": "http://arxiv.org/abs/2104.12678",
          "publishedOn": "2022-01-03T07:15:44.283Z",
          "wordCount": null,
          "title": "Semi-Decentralized Federated Edge Learning for Fast Convergence on Non-IID Data. (arXiv:2104.12678v5 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.12134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qing-xin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian-wei Liu</a>",
          "description": "We present a unified analysis method that relies on the generalized cosine\nrule and $\\phi$-convex for online optimization in normed vector space using\ndynamic regret as the performance metric. In combing the update rules, we start\nwith strategy $S$ (a two-parameter variant strategy covering Optimistic-FTRL\nwith surrogate linearized losses), and obtain $S$-I (type-I relaxation variant\nform of $S$) and $S$-II (type-II relaxation variant form of $S$, which is\nOptimistic-MD) by relaxation. Regret bounds for $S$-I and $S$-II are the\ntightest possible. As instantiations, regret bounds of normalized exponentiated\nsubgradient and greedy/lazy projection are better than the currently known\noptimal results. By replacing losses of online game with monotone operators,\nand extending the definition of regret, namely regret$^n$, we extend online\nconvex optimization to online monotone optimization, which expands the\napplication scope of $S$-I and $S$-II.",
          "link": "http://arxiv.org/abs/2112.12134",
          "publishedOn": "2022-01-03T07:15:44.283Z",
          "wordCount": 580,
          "title": "A Unified Analysis Method for Online Optimization in Normed Vector Space. (arXiv:2112.12134v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shunqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurkoski_B/0/1/0/all/0/1\">Brian M. Kurkoski</a>",
          "description": "Approximate message passing (AMP) is a promising technique for unknown signal\nreconstruction of certain high-dimensional linear systems with non-Gaussian\nsignaling. A distinguished feature of the AMP-type algorithms is that their\ndynamics can be rigorously described by state evolution. However, state\nevolution does not necessarily guarantee the convergence of iterative\nalgorithms. To solve the convergence problem of AMP-type algorithms in\nprinciple, this paper proposes a memory AMP (MAMP) under a sufficient statistic\ncondition, named sufficient statistic MAMP (SS-MAMP). We show that the\ncovariance matrices of SS-MAMP are L-banded and convergent. Given an arbitrary\nMAMP, we can construct an SS-MAMP by damping, which not only ensures the\nconvergence of MAMP but also preserves the orthogonality of MAMP, i.e., its\ndynamics can be rigorously described by state evolution. As a byproduct, we\nprove that the Bayes-optimal orthogonal/vector AMP (BO-OAMP/VAMP) is an\nSS-MAMP. As a result, we reveal two interesting properties of BO-OAMP/VAMP for\nlarge systems: 1) the covariance matrices are L-banded and are convergent in\nBO-OAMP/VAMP, and 2) damping and memory are useless (i.e., do not bring\nperformance improvement) in BO-OAMP/VAMP. As an example, we construct a\nsufficient statistic Bayes-optimal MAMP (BO-MAMP), which is Bayes optimal if\nits state evolution has a unique fixed point and its MSE is not worse than the\noriginal BO-MAMP. Finally, simulations are provided to verify the validity and\naccuracy of the theoretical results.",
          "link": "http://arxiv.org/abs/2112.15327",
          "publishedOn": "2022-01-03T07:15:44.276Z",
          "wordCount": 660,
          "title": "Sufficient Statistic Memory AMP. (arXiv:2112.15327v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15265",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1\">Lang Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pal_S/0/1/0/all/0/1\">Soumik Pal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Harchaoui_Z/0/1/0/all/0/1\">Zaid Harchaoui</a>",
          "description": "Optimal transport (OT) and its entropy regularized offspring have recently\ngained a lot of attention in both machine learning and AI domains. In\nparticular, optimal transport has been used to develop probability metrics\nbetween probability distributions. We introduce in this paper an independence\ncriterion based on entropy regularized optimal transport. Our criterion can be\nused to test for independence between two samples. We establish non-asymptotic\nbounds for our test statistic, and study its statistical behavior under both\nthe null and alternative hypothesis. Our theoretical results involve tools from\nU-process theory and optimal transport theory. We present experimental results\non existing benchmarks, illustrating the interest of the proposed criterion.",
          "link": "http://arxiv.org/abs/2112.15265",
          "publishedOn": "2022-01-03T07:15:44.269Z",
          "wordCount": null,
          "title": "Entropy Regularized Optimal Transport Independence Criterion. (arXiv:2112.15265v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2112.07110",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bhattacharya_R/0/1/0/all/0/1\">Riddhiman Bhattacharya</a>",
          "description": "The gradient noise of Stochastic Gradient Descent (SGD) is considered to play\na key role in its properties (e.g. escaping low potential points and\nregularization). Past research has indicated that the covariance of the SGD\nerror done via minibatching plays a critical role in determining its\nregularization and escape from low potential points. It is however not much\nexplored how much the distribution of the error influences the behavior of the\nalgorithm. Motivated by some new research in this area, we prove universality\nresults by showing that noise classes that have the same mean and covariance\nstructure of SGD via minibatching have similar properties. We mainly consider\nthe Multiplicative Stochastic Gradient Descent (M-SGD) algorithm as introduced\nby Wu et al., which has a much more general noise class than the SGD algorithm\ndone via minibatching. We establish nonasymptotic bounds for the M-SGD\nalgorithm mainly with respect to the Stochastic Differential Equation\ncorresponding to SGD via minibatching. We also show that the M-SGD error is\napproximately a scaled Gaussian distribution with mean $0$ at any fixed point\nof the M-SGD algorithm. We also establish bounds for the convergence of the\nM-SGD algorithm in the strongly convex regime.",
          "link": "http://arxiv.org/abs/2112.07110",
          "publishedOn": "2022-01-03T07:15:44.268Z",
          "wordCount": 657,
          "title": "Non Asymptotic Bounds for Optimization via Online Multiplicative Stochastic Gradient Descent. (arXiv:2112.07110v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.14075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun-Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi-Jen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yun-Cheng Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Samuel Yen-Chi Chen</a>",
          "description": "The importance of deep learning data privacy has gained significant attention\nin recent years. It is probably to suffer data breaches when applying deep\nlearning to cryptocurrency that lacks supervision of financial regulatory\nagencies. However, there is little relative research in the financial area to\nour best knowledge. We apply two representative deep learning privacy-privacy\nframeworks proposed by Google to financial trading data. We designed the\nexperiments with several different parameters suggested from the original\nstudies. In addition, we refer the degree of privacy to Google and Apple\ncompanies to estimate the results more reasonably. The results show that DP-SGD\nperforms better than the PATE framework in financial trading data. The tradeoff\nbetween privacy and accuracy is low in DP-SGD. The degree of privacy also is in\nline with the actual case. Therefore, we can obtain a strong privacy guarantee\nwith precision to avoid potential financial loss.",
          "link": "http://arxiv.org/abs/2112.14075",
          "publishedOn": "2022-01-03T07:15:44.262Z",
          "wordCount": 582,
          "title": "Financial Vision Based Differential Privacy Applications. (arXiv:2112.14075v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.07068",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dockhorn_T/0/1/0/all/0/1\">Tim Dockhorn</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vahdat_A/0/1/0/all/0/1\">Arash Vahdat</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kreis_K/0/1/0/all/0/1\">Karsten Kreis</a>",
          "description": "Score-based generative models (SGMs) have demonstrated remarkable synthesis\nquality. SGMs rely on a diffusion process that gradually perturbs the data\ntowards a tractable distribution, while the generative model learns to denoise.\nThe complexity of this denoising task is, apart from the data distribution\nitself, uniquely determined by the diffusion process. We argue that current\nSGMs employ overly simplistic diffusions, leading to unnecessarily complex\ndenoising processes, which limit generative modeling performance. Based on\nconnections to statistical mechanics, we propose a novel critically-damped\nLangevin diffusion (CLD) and show that CLD-based SGMs achieve superior\nperformance. CLD can be interpreted as running a joint diffusion in an extended\nspace, where the auxiliary variables can be considered \"velocities\" that are\ncoupled to the data variables as in Hamiltonian dynamics. We derive a novel\nscore matching objective for CLD and show that the model only needs to learn\nthe score function of the conditional distribution of the velocity given data,\nan easier task than learning scores of the data directly. We also derive a new\nsampling scheme for efficient synthesis from CLD-based diffusion models. We\nfind that CLD outperforms previous SGMs in synthesis quality for similar\nnetwork architectures and sampling compute budgets. We show that our novel\nsampler for CLD significantly outperforms solvers such as Euler--Maruyama. Our\nframework provides new insights into score-based denoising diffusion models and\ncan be readily used for high-resolution image synthesis. Project page and code:\nhttps://nv-tlabs.github.io/CLD-SGM.",
          "link": "http://arxiv.org/abs/2112.07068",
          "publishedOn": "2022-01-03T07:15:44.255Z",
          "wordCount": 670,
          "title": "Score-Based Generative Modeling with Critically-Damped Langevin Diffusion. (arXiv:2112.07068v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mengin_E/0/1/0/all/0/1\">Elie Mengin</a> (SAMM), <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1\">Fabrice Rossi</a> (CEREMADE)",
          "description": "In this paper, we present a novel algorithm to address the Network Alignment\nproblem. It is inspired from a previous message passing framework of Bayati et\nal. [2] and includes several modifications designed to significantly speed up\nthe message updates as well as to enforce their convergence. Experiments show\nthat our proposed model outperforms other state-of-the-art solvers. Finally, we\npropose an application of our method in order to address the Binary Diffing\nproblem. We show that our solution provides better assignment than the\nreference differs in almost all submitted instances and outline the importance\nof leveraging the graphical structure of binary programs.",
          "link": "http://arxiv.org/abs/2112.15336",
          "publishedOn": "2022-01-03T07:15:44.248Z",
          "wordCount": null,
          "title": "Improved Algorithm for the Network Alignment Problem with Application to Binary Diffing. (arXiv:2112.15336v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arnob_S/0/1/0/all/0/1\">Samin Yeasar Arnob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_R/0/1/0/all/0/1\">Riashat Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>",
          "description": "We hypothesize that empirically studying the sample complexity of offline\nreinforcement learning (RL) is crucial for the practical applications of RL in\nthe real world. Several recent works have demonstrated the ability to learn\npolicies directly from offline data. In this work, we ask the question of the\ndependency on the number of samples for learning from offline data. Our\nobjective is to emphasize that studying sample complexity for offline RL is\nimportant, and is an indicator of the usefulness of existing offline\nalgorithms. We propose an evaluation approach for sample complexity analysis of\noffline RL.",
          "link": "http://arxiv.org/abs/2112.15578",
          "publishedOn": "2022-01-03T07:15:44.247Z",
          "wordCount": 521,
          "title": "Importance of Empirical Sample Complexity Analysis for Offline Reinforcement Learning. (arXiv:2112.15578v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.15010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhehao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Q/0/1/0/all/0/1\">Qinghua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>",
          "description": "Deep Neural Networks (DNNs) are acknowledged as vulnerable to adversarial\nattacks, while the existing black-box attacks require extensive queries on the\nvictim DNN to achieve high success rates. For query-efficiency, surrogate\nmodels of the victim are used to generate transferable Adversarial Examples\n(AEs) because of their Gradient Similarity (GS), i.e., surrogates' attack\ngradients are similar to the victim's ones. However, it is generally neglected\nto exploit their similarity on outputs, namely the Prediction Similarity (PS),\nto filter out inefficient queries by surrogates without querying the victim. To\njointly utilize and also optimize surrogates' GS and PS, we develop QueryNet, a\nunified attack framework that can significantly reduce queries. QueryNet\ncreatively attacks by multi-identity surrogates, i.e., crafts several AEs for\none sample by different surrogates, and also uses surrogates to decide on the\nmost promising AE for the query. After that, the victim's query feedback is\naccumulated to optimize not only surrogates' parameters but also their\narchitectures, enhancing both the GS and the PS. Although QueryNet has no\naccess to pre-trained surrogates' prior, it reduces queries by averagely about\nan order of magnitude compared to alternatives within an acceptable time,\naccording to our comprehensive experiments: 11 victims (including two\ncommercial models) on MNIST/CIFAR10/ImageNet, allowing only 8-bit image\nqueries, and no access to the victim's training data. The code is available at\nhttps://github.com/AllenChen1998/QueryNet.",
          "link": "http://arxiv.org/abs/2105.15010",
          "publishedOn": "2022-01-03T07:15:44.223Z",
          "wordCount": 689,
          "title": "QueryNet: Attack by Multi-Identity Surrogates. (arXiv:2105.15010v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12375",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Atz_K/0/1/0/all/0/1\">Kenneth Atz</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Grisoni_F/0/1/0/all/0/1\">Francesca Grisoni</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Schneider_G/0/1/0/all/0/1\">Gisbert Schneider</a>",
          "description": "Geometric deep learning (GDL), which is based on neural network architectures\nthat incorporate and process symmetry information, has emerged as a recent\nparadigm in artificial intelligence. GDL bears particular promise in molecular\nmodeling applications, in which various molecular representations with\ndifferent symmetry properties and levels of abstraction exist. This review\nprovides a structured and harmonized overview of molecular GDL, highlighting\nits applications in drug discovery, chemical synthesis prediction, and quantum\nchemistry. Emphasis is placed on the relevance of the learned molecular\nfeatures and their complementarity to well-established molecular descriptors.\nThis review provides an overview of current challenges and opportunities, and\npresents a forecast of the future of GDL for molecular sciences.",
          "link": "http://arxiv.org/abs/2107.12375",
          "publishedOn": "2022-01-03T07:15:44.215Z",
          "wordCount": 568,
          "title": "Geometric Deep Learning on Molecular Representations. (arXiv:2107.12375v4 [physics.chem-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.14278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fil_M/0/1/0/all/0/1\">Miroslav Fil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mesinovic_M/0/1/0/all/0/1\">Munib Mesinovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_M/0/1/0/all/0/1\">Matthew Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildberger_J/0/1/0/all/0/1\">Jonas Wildberger</a>",
          "description": "$\\beta$-VAE is a follow-up technique to variational autoencoders that\nproposes special weighting of the KL divergence term in the VAE loss to obtain\ndisentangled representations. Unsupervised learning is known to be brittle even\non toy datasets and a meaningful, mathematically precise definition of\ndisentanglement remains difficult to find. Here we investigate the original\n$\\beta$-VAE paper and add evidence to the results previously obtained\nindicating its lack of reproducibility. We also further expand the\nexperimentation of the models and include further more complex datasets in the\nanalysis. We also implement an FID scoring metric for the $\\beta$-VAE model and\nconclude a qualitative analysis of the results obtained. We end with a brief\ndiscussion on possible future investigations that can be conducted to add more\nrobustness to the claims.",
          "link": "http://arxiv.org/abs/2112.14278",
          "publishedOn": "2022-01-03T07:15:44.208Z",
          "wordCount": 558,
          "title": "Beta-VAE Reproducibility: Challenges and Extensions. (arXiv:2112.14278v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dongge Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_V/0/1/0/all/0/1\">Van-Thuan Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_G/0/1/0/all/0/1\">Gidon Ernst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_T/0/1/0/all/0/1\">Toby Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1\">Benjamin I.P. Rubinstein</a>",
          "description": "The statefulness property of network protocol implementations poses a unique\nchallenge for testing and verification techniques, including Fuzzing. Stateful\nfuzzers tackle this challenge by leveraging state models to partition the state\nspace and assist the test generation process. Since not all states are equally\nimportant and fuzzing campaigns have time limits, fuzzers need effective state\nselection algorithms to prioritize progressive states over others. Several\nstate selection algorithms have been proposed but they were implemented and\nevaluated separately on different platforms, making it hard to achieve\nconclusive findings. In this work, we evaluate an extensive set of state\nselection algorithms on the same fuzzing platform that is AFLNet, a\nstate-of-the-art fuzzer for network servers. The algorithm set includes\nexisting ones supported by AFLNet and our novel and principled algorithm called\nAFLNetLegion. The experimental results on the ProFuzzBench benchmark show that\n(i) the existing state selection algorithms of AFLNet achieve very similar code\ncoverage, (ii) AFLNetLegion clearly outperforms these algorithms in selected\ncase studies, but (iii) the overall improvement appears insignificant. These\nare unexpected yet interesting findings. We identify problems and share\ninsights that could open opportunities for future research on this topic.",
          "link": "http://arxiv.org/abs/2112.15498",
          "publishedOn": "2022-01-03T07:15:44.201Z",
          "wordCount": 639,
          "title": "State Selection Algorithms and Their Impact on The Performance of Stateful Network Protocol Fuzzing. (arXiv:2112.15498v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15516",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Shen_J/0/1/0/all/0/1\">Jianmin Shen</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Liu_F/0/1/0/all/0/1\">Feiyi Liu</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Chen_S/0/1/0/all/0/1\">Shiyang Chen</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Xu_D/0/1/0/all/0/1\">Dian Xu</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Chen_X/0/1/0/all/0/1\">Xiangna Chen</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Deng_S/0/1/0/all/0/1\">Shengfeng Deng</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Papp_G/0/1/0/all/0/1\">Gabor Papp</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Yang_C/0/1/0/all/0/1\">Chunbin Yang</a>",
          "description": "The latest advances of statistical physics have shown remarkable performance\nof machine learning in identifying phase transitions. In this paper, we apply\ndomain adversarial neural network (DANN) based on transfer learning to studying\nnon-equilibrium and equilibrium phase transition models, which are percolation\nmodel and directed percolation (DP) model, respectively. With the DANN, only a\nsmall fraction of input configurations (2d images) needs to be labeled, which\nis automatically chosen, in order to capture the critical point. To learn the\nDP model, the method is refined by an iterative procedure in determining the\ncritical point, which is a prerequisite for the data collapse in calculating\nthe critical exponent $\\nu_{\\perp}$. We then apply the DANN to a\ntwo-dimensional site percolation with configurations filtered to include only\nthe largest cluster which may contain the information related to the order\nparameter. The DANN learning of both models yields reliable results which are\ncomparable to the ones from Monte Carlo simulations. Our study also shows that\nthe DANN can achieve quite high accuracy at much lower cost, compared to the\nsupervised learning.",
          "link": "http://arxiv.org/abs/2112.15516",
          "publishedOn": "2022-01-03T07:15:44.182Z",
          "wordCount": 622,
          "title": "Transfer learning of phase transitions in percolation and directed percolation. (arXiv:2112.15516v1 [cond-mat.stat-mech])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quanziang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renzhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>",
          "description": "Continual learning requires models to learn new tasks while maintaining\npreviously learned knowledge. Various algorithms have been proposed to address\nthis real challenge. Till now, rehearsal-based methods, such as experience\nreplay, have achieved state-of-the-art performance. These approaches save a\nsmall part of the data of the past tasks as a memory buffer to prevent models\nfrom forgetting previously learned knowledge. However, most of them treat every\nnew task equally, i.e., fixed the hyperparameters of the framework while\nlearning different new tasks. Such a setting lacks the consideration of the\nrelationship/similarity between past and new tasks. For example, the previous\nknowledge/features learned from dogs are more beneficial for the identification\nof cats (new task), compared to those learned from buses. In this regard, we\npropose a meta learning algorithm based on bi-level optimization to adaptively\ntune the relationship between the knowledge extracted from the past and new\ntasks. Therefore, the model can find an appropriate direction of gradient\nduring continual learning and avoid the serious overfitting problem on memory\nbuffer. Extensive experiments are conducted on three publicly available\ndatasets (i.e., CIFAR-10, CIFAR-100, and Tiny ImageNet). The experimental\nresults demonstrate that the proposed method can consistently improve the\nperformance of all baselines.",
          "link": "http://arxiv.org/abs/2112.15402",
          "publishedOn": "2022-01-03T07:15:44.175Z",
          "wordCount": 634,
          "title": "Revisiting Experience Replay: Continual Learning by Adaptively Tuning Task-wise Relationship. (arXiv:2112.15402v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15532",
          "author": "<a href=\"http://arxiv.org/find/hep-lat/1/au:+Debbio_L/0/1/0/all/0/1\">Luigi Del Debbio</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Rossney_J/0/1/0/all/0/1\">Joe Marsh Rossney</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Wilson_M/0/1/0/all/0/1\">Michael Wilson</a>",
          "description": "A trivializing map is a field transformation whose Jacobian determinant\nexactly cancels the interaction terms in the action, providing a representation\nof the theory in terms of a deterministic transformation of a distribution from\nwhich sampling is trivial. Recently, a proof-of-principle study by Albergo,\nKanwar and Shanahan [arXiv:1904.12072] demonstrated that approximations of\ntrivializing maps can be `machine-learned' by a class of invertible,\ndifferentiable neural models called \\textit{normalizing flows}. By ensuring\nthat the Jacobian determinant can be computed efficiently, asymptotically exact\nsampling from the theory of interest can be performed by drawing samples from a\nsimple distribution and passing them through the network. From a theoretical\nperspective, this approach has the potential to become more efficient than\ntraditional Markov Chain Monte Carlo sampling techniques, where\nautocorrelations severely diminish the sampling efficiency as one approaches\nthe continuum limit. A major caveat is that it is not yet understood how the\nsize of models and the cost of training them is expected to scale. As a first\nstep, we have conducted an exploratory scaling study using two-dimensional\n$\\phi^4$ with up to $20^2$ lattice sites. Although the scope of our study is\nlimited to a particular model architecture and training algorithm, initial\nresults paint an interesting picture in which training costs grow very quickly\nindeed. We describe a candidate explanation for the poor scaling, and outline\nour intentions to clarify the situation in future work.",
          "link": "http://arxiv.org/abs/2112.15532",
          "publishedOn": "2022-01-03T07:15:44.163Z",
          "wordCount": null,
          "title": "Machine Learning Trivializing Maps: A First Step Towards Understanding How Flow-Based Samplers Scale Up. (arXiv:2112.15532v1 [hep-lat])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_X/0/1/0/all/0/1\">Xiaoqian Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gaoang Wang</a>",
          "description": "Large-scale datasets are important for the development of deep learning\nmodels. Such datasets usually require a heavy workload of annotations, which\nare extremely time-consuming and expensive. To accelerate the annotation\nprocedure, multiple annotators may be employed to label different subsets of\nthe data. However, the inconsistency and bias among different annotators are\nharmful to the model training, especially for qualitative and subjective\ntasks.To address this challenge, in this paper, we propose a novel contrastive\nregression framework to address the disjoint annotations problem, where each\nsample is labeled by only one annotator and multiple annotators work on\ndisjoint subsets of the data. To take account of both the intra-annotator\nconsistency and inter-annotator inconsistency, two strategies are\nemployed.Firstly, a contrastive-based loss is applied to learn the relative\nranking among different samples of the same annotator, with the assumption that\nthe ranking of samples from the same annotator is unanimous. Secondly, we apply\nthe gradient reversal layer to learn robust representations that are invariant\nto different annotators. Experiments on the facial expression prediction task,\nas well as the image quality assessment task, verify the effectiveness of our\nproposed framework.",
          "link": "http://arxiv.org/abs/2112.15411",
          "publishedOn": "2022-01-03T07:15:44.159Z",
          "wordCount": 605,
          "title": "Disjoint Contrastive Regression Learning for Multi-Sourced Annotations. (arXiv:2112.15411v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14798",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Fang_L/0/1/0/all/0/1\">Lidong Fang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ge_P/0/1/0/all/0/1\">Pei Ge</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lei_H/0/1/0/all/0/1\">Huan Lei</a>, <a href=\"http://arxiv.org/find/physics/1/au:+E_W/0/1/0/all/0/1\">Weinan E</a>",
          "description": "A long standing problem in the modeling of non-Newtonian hydrodynamics is the\navailability of reliable and interpretable hydrodynamic models that faithfully\nencode the underlying micro-scale polymer dynamics. The main complication\narises from the long polymer relaxation time, the complex molecular structure,\nand heterogeneous interaction. DeePN$^2$, a deep learning-based non-Newtonian\nhydrodynamic model, has been proposed and has shown some success in\nsystematically passing the micro-scale structural mechanics information to the\nmacro-scale hydrodynamics for suspensions with simple polymer conformation and\nbond potential. The model retains a multi-scaled nature by mapping the polymer\nconfigurations into a set of symmetry-preserving macro-scale features. The\nextended constitutive laws for these macro-scale features can be directly\nlearned from the kinetics of their micro-scale counterparts. In this paper, we\ncarry out further study of DeePN$^2$ using more complex micro-structural\nmodels. We show that DeePN$^2$ can faithfully capture the broadly overlooked\nviscoelastic differences arising from the specific molecular structural\nmechanics without human intervention.",
          "link": "http://arxiv.org/abs/2112.14798",
          "publishedOn": "2022-01-03T07:15:44.139Z",
          "wordCount": 578,
          "title": "DeePN$^2$: A deep learning-based non-Newtonian hydrodynamic model. (arXiv:2112.14798v1 [physics.comp-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiyang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuxiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>",
          "description": "Graph neural network (GNN) has shown convincing performance in learning\npowerful node representations that preserve both node attributes and graph\nstructural information. However, many GNNs encounter problems in effectiveness\nand efficiency when they are designed with a deeper network structure or handle\nlarge-sized graphs. Several sampling algorithms have been proposed for\nimproving and accelerating the training of GNNs, yet they ignore understanding\nthe source of GNN performance gain. The measurement of information within graph\ndata can help the sampling algorithms to keep high-value information while\nremoving redundant information and even noise. In this paper, we propose a\nMetric-Guided (MeGuide) subgraph learning framework for GNNs. MeGuide employs\ntwo novel metrics: Feature Smoothness and Connection Failure Distance to guide\nthe subgraph sampling and mini-batch based training. Feature Smoothness is\ndesigned for analyzing the feature of nodes in order to retain the most\nvaluable information, while Connection Failure Distance can measure the\nstructural information to control the size of subgraphs. We demonstrate the\neffectiveness and efficiency of MeGuide in training various GNNs on multiple\ndatasets.",
          "link": "http://arxiv.org/abs/2112.15015",
          "publishedOn": "2022-01-03T07:15:44.132Z",
          "wordCount": 599,
          "title": "Measuring and Sampling: A Metric-guided Subgraph Learning Framework for Graph Neural Network. (arXiv:2112.15015v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14877",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bui_Thanh_T/0/1/0/all/0/1\">Tan Bui-Thanh</a>",
          "description": "One of the reasons that many neural networks are capable of replicating\ncomplicated tasks or functions is their universality property. The past few\ndecades have seen many attempts in providing constructive proofs for single or\nclass of neural networks. This paper is an effort to provide a unified and\nconstructive framework for the universality of a large class of activations\nincluding most of existing activations and beyond. At the heart of the\nframework is the concept of neural network approximate identity. It turns out\nthat most of existing activations are neural network approximate identity, and\nthus universal in the space of continuous of functions on compacta. The\nframework induces several advantages. First, it is constructive with elementary\nmeans from functional analysis, probability theory, and numerical analysis.\nSecond, it is the first unified attempt that is valid for most of existing\nactivations. Third, as a by product, the framework provides the first\nuniversity proof for some of the existing activation functions including Mish,\nSiLU, ELU, GELU, and etc. Fourth, it discovers new activations with guaranteed\nuniversality property. Indeed, any activation\\textemdash whose $\\k$th\nderivative, with $\\k$ being an integer, is integrable and essentially\nbounded\\textemdash is universal. Fifth, for a given activation and error\ntolerance, the framework provides precisely the architecture of the\ncorresponding one-hidden neural network with predetermined number of neuron,\nand the values of weights/biases.",
          "link": "http://arxiv.org/abs/2112.14877",
          "publishedOn": "2022-01-03T07:15:44.125Z",
          "wordCount": 652,
          "title": "A Unified and Constructive Framework for the Universality of Neural Networks. (arXiv:2112.14877v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2111.15645",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Han_X/0/1/0/all/0/1\">X.Y. Han</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lewis_A/0/1/0/all/0/1\">Adrian S. Lewis</a>",
          "description": "For strongly convex objectives that are smooth, the classical theory of\ngradient descent ensures linear convergence relative to the number of gradient\nevaluations. An analogous nonsmooth theory is challenging: even when the\nobjective is smooth at every iterate, the corresponding local models are\nunstable, and traditional remedies need unpredictably many cutting planes. We\ninstead propose a multipoint generalization of the gradient descent iteration\nfor local optimization. While designed with general objectives in mind, we are\nmotivated by a \"max-of-smooth\" model that captures subdifferential dimension at\noptimality. We prove linear convergence when the objective is itself\nmax-of-smooth, and experiments suggest a more general phenomenon.",
          "link": "http://arxiv.org/abs/2111.15645",
          "publishedOn": "2022-01-03T07:15:44.106Z",
          "wordCount": 554,
          "title": "Survey Descent: A Multipoint Generalization of Gradient Descent for Nonsmooth Optimization. (arXiv:2111.15645v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.12545",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Bogyrbayeva_A/0/1/0/all/0/1\">Aigerim Bogyrbayeva</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yoon_T/0/1/0/all/0/1\">Taehyun Yoon</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ko_H/0/1/0/all/0/1\">Hanbum Ko</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lim_S/0/1/0/all/0/1\">Sungbin Lim</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yun_H/0/1/0/all/0/1\">Hyokun Yun</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kwon_C/0/1/0/all/0/1\">Changhyun Kwon</a>",
          "description": "Reinforcement learning has recently shown promise in learning quality\nsolutions in many combinatorial optimization problems. In particular, the\nattention-based encoder-decoder models show high effectiveness on various\nrouting problems, including the Traveling Salesman Problem (TSP).\nUnfortunately, they perform poorly for the TSP with Drone (TSP-D), requiring\nrouting a heterogeneous fleet of vehicles in coordination -- a truck and a\ndrone. In TSP-D, the two vehicles are moving in tandem and may need to wait at\na node for the other vehicle to join. State-less attention-based decoder fails\nto make such coordination between vehicles. We propose an attention\nencoder-LSTM decoder hybrid model, in which the decoder's hidden state can\nrepresent the sequence of actions made. We empirically demonstrate that such a\nhybrid model improves upon a purely attention-based model for both solution\nquality and computational efficiency. Our experiments on the min-max\nCapacitated Vehicle Routing Problem (mmCVRP) also confirm that the hybrid model\nis more suitable for coordinated routing of multiple vehicles than the\nattention-based model.",
          "link": "http://arxiv.org/abs/2112.12545",
          "publishedOn": "2022-01-03T07:15:43.983Z",
          "wordCount": null,
          "title": "A Deep Reinforcement Learning Approach for Solving the Traveling Salesman Problem with Drone. (arXiv:2112.12545v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.08888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1\">Ruiqi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1\">Rongxin Cui</a>",
          "description": "Unified understanding of neuro networks (NNs) gets the users into great\ntrouble because they have been puzzled by what kind of rules should be obeyed\nto optimize the internal structure of NNs. Considering the potential capability\nof random graphs to alter how computation is performed, we demonstrate that\nthey can serve as architecture generators to optimize the internal structure of\nNNs. To transform the random graph theory into an NN model with practical\nmeaning and based on clarifying the input-output relationship of each neuron,\nwe complete data feature mapping by calculating Fourier Random Features (FRFs).\nUnder the usage of this low-operation cost approach, neurons are assigned to\nseveral groups of which connection relationships can be regarded as uniform\nrepresentations of random graphs they belong to, and random arrangement fuses\nthose neurons to establish the pattern matrix, markedly reducing manual\nparticipation and computational cost without the fixed and deep architecture.\nLeveraging this single neuromorphic learning model termed random graph-based\nneuro network (RGNN) we develop a joint classification mechanism involving\ninformation interaction between multiple RGNNs and realize significant\nperformance improvements in supervised learning for three benchmark tasks,\nwhereby they effectively avoid the adverse impact of the interpretability of\nNNs on the structure design and engineering practice.",
          "link": "http://arxiv.org/abs/2111.08888",
          "publishedOn": "2022-01-03T07:15:43.972Z",
          "wordCount": null,
          "title": "Random Graph-Based Neuromorphic Learning with a Layer-Weaken Structure. (arXiv:2111.08888v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinghui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "\"Benign overfitting\", where classifiers memorize noisy training data yet\nstill achieve a good generalization performance, has drawn great attention in\nthe machine learning community. To explain this surprising phenomenon, a series\nof works have provided theoretical justification in over-parameterized linear\nregression, classification, and kernel methods. However, it is not clear if\nbenign overfitting still occurs in the presence of adversarial examples, i.e.,\nexamples with tiny and intentional perturbations to fool the classifiers. In\nthis paper, we show that benign overfitting indeed occurs in adversarial\ntraining, a principled approach to defend against adversarial examples. In\ndetail, we prove the risk bounds of the adversarially trained linear classifier\non the mixture of sub-Gaussian data under $\\ell_p$ adversarial perturbations.\nOur result suggests that under moderate perturbations, adversarially trained\nlinear classifiers can achieve the near-optimal standard and adversarial risks,\ndespite overfitting the noisy training data. Numerical experiments validate our\ntheoretical findings.",
          "link": "http://arxiv.org/abs/2112.15250",
          "publishedOn": "2022-01-03T07:15:43.964Z",
          "wordCount": null,
          "title": "Benign Overfitting in Adversarially Robust Linear Classification. (arXiv:2112.15250v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2110.09344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yongyi Mao</a>",
          "description": "We present a simple and yet effective interpolation-based regularization\ntechnique to improve the generalization of Graph Neural Networks (GNNs). Our\nmethod leverages the recent advances in Mixup regularizer for vision and text,\nwhere random sample pairs and their labels are interpolated to create synthetic\nsamples for training. Unlike images or natural sentences, graphs have arbitrary\nstructure and topology, and even simply deleting or adding one edge from a\ngraph can dramatically change its semantic meanings. This makes interpolating\ngraph inputs very challenging because mixing graph pairs may naturally create\ngraphs with identical structure but with conflict labels, causing the manifold\nintrusion issue. To cope with this obstacle, we propose a simple input mixing\nschema for Mixup on graph, coined ifMixup. We theoretically prove that, with a\nmild assumption, ifMixup guarantees that the mixed graphs are manifold\nintrusion free. We also empirically verify that ifMixup can effectively\nregularize the graph classification learning, resulting in superior predictive\naccuracy over popular graph augmentation baselines.",
          "link": "http://arxiv.org/abs/2110.09344",
          "publishedOn": "2022-01-03T07:15:43.940Z",
          "wordCount": null,
          "title": "ifMixup: Towards Intrusion-Free Graph Mixup for Graph Classification. (arXiv:2110.09344v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15199",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Kovalev_D/0/1/0/all/0/1\">Dmitry Kovalev</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gasnikov_A/0/1/0/all/0/1\">Alexander Gasnikov</a>, <a href=\"http://arxiv.org/find/math/1/au:+Richtarik_P/0/1/0/all/0/1\">Peter Richt&#xe1;rik</a>",
          "description": "In this paper we study a convex-concave saddle-point problem $\\min_x\\max_y\nf(x) + y^\\top\\mathbf{A} x - g(y)$, where $f(x)$ and $g(y)$ are smooth and\nconvex functions. We propose an Accelerated Primal-Dual Gradient Method for\nsolving this problem which (i) achieves an optimal linear convergence rate in\nthe strongly-convex-strongly-concave regime matching the lower complexity bound\n(Zhang et al., 2021) and (ii) achieves an accelerated linear convergence rate\nin the case when only one of the functions $f(x)$ and $g(y)$ is strongly convex\nor even none of them are. Finally, we obtain a linearly-convergent algorithm\nfor the general smooth and convex-concave saddle point problem $\\min_x\\max_y\nF(x,y)$ without requirement of strong convexity or strong concavity.",
          "link": "http://arxiv.org/abs/2112.15199",
          "publishedOn": "2022-01-03T07:15:43.937Z",
          "wordCount": null,
          "title": "Accelerated Primal-Dual Gradient Method for Smooth and Convex-Concave Saddle-Point Problems with Bilinear Coupling. (arXiv:2112.15199v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Sunny Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Roman Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Newman Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kevin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Leonard Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_E/0/1/0/all/0/1\">Elizabeth Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Nikhil Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patti_T/0/1/0/all/0/1\">Taylor L. Patti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lynch_J/0/1/0/all/0/1\">Jayson Lynch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shporer_A/0/1/0/all/0/1\">Avi Shporer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_N/0/1/0/all/0/1\">Nakul Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_E/0/1/0/all/0/1\">Eugene Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strang_G/0/1/0/all/0/1\">Gilbert Strang</a>",
          "description": "We demonstrate that a neural network pre-trained on text and fine-tuned on\ncode solves Mathematics problems by program synthesis. We turn questions into\nprogramming tasks, automatically generate programs, and then execute them,\nperfectly solving university-level problems from MIT's large Mathematics\ncourses (Single Variable Calculus 18.01, Multivariable Calculus 18.02,\nDifferential Equations 18.03, Introduction to Probability and Statistics 18.05,\nLinear Algebra 18.06, and Mathematics for Computer Science 6.042) as well as\nquestions from a MATH dataset (on Prealgebra, Algebra, Counting and\nProbability, Number Theory, and Precalculus), the latest benchmark of advanced\nmathematics problems specifically designed to assess mathematical reasoning. We\nexplore prompt generation methods that enable Transformers to generate question\nsolving programs for these subjects, including solutions with plots. We\ngenerate correct answers for a random sample of questions in each topic. We\nquantify the gap between the original and transformed questions and perform a\nsurvey to evaluate the quality and difficulty of generated questions. This is\nthe first work to automatically solve, grade, and generate university-level\nMathematics course questions at scale which represents a milestone for higher\neducation.",
          "link": "http://arxiv.org/abs/2112.15594",
          "publishedOn": "2022-01-03T07:15:43.925Z",
          "wordCount": null,
          "title": "A Neural Network Solves and Generates Mathematics Problems by Program Synthesis: Calculus, Differential Equations, Linear Algebra, and More. (arXiv:2112.15594v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2111.01201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raab_R/0/1/0/all/0/1\">Reilly Raab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "Realistically -- and equitably -- modeling the dynamics of group-level\ndisparities in machine learning remains an open problem. In particular, we\ndesire models that do not suppose inherent differences between artificial\ngroups of people -- but rather endogenize disparities by appeal to unequal\ninitial conditions of insular subpopulations. In this paper, agents each have a\nreal-valued feature $X$ (e.g., credit score) informed by a \"true\" binary label\n$Y$ representing qualification (e.g., for a loan). Each agent alternately (1)\nreceives a binary classification label $\\hat{Y}$ (e.g., loan approval) from a\nBayes-optimal machine learning classifier observing $X$ and (2) may update\ntheir qualification $Y$ by imitating successful strategies (e.g., seek a raise)\nwithin an isolated group $G$ of agents to which they belong. We consider the\ndisparity of qualification rates $\\Pr(Y=1)$ between different groups and how\nthis disparity changes subject to a sequence of Bayes-optimal classifiers\nrepeatedly retrained on the global population. We model the evolving\nqualification rates of each subpopulation (group) using the replicator\nequation, which derives from a class of imitation processes. We show that\ndifferences in qualification rates between subpopulations can persist\nindefinitely for a set of non-trivial equilibrium states due to uniformed\nclassifier deployments, even when groups are identical in all aspects except\ninitial qualification densities. We next simulate the effects of commonly\nproposed fairness interventions on this dynamical system along with a new\nfeedback control mechanism capable of permanently eliminating group-level\nqualification rate disparities. We conclude by discussing the limitations of\nour model and findings and by outlining potential future work.",
          "link": "http://arxiv.org/abs/2111.01201",
          "publishedOn": "2022-01-03T07:15:43.920Z",
          "wordCount": null,
          "title": "Unintended Selection: Persistent Qualification Rate Disparities and Interventions. (arXiv:2111.01201v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sevyeri_L/0/1/0/all/0/1\">Laya Rafiee Sevyeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevens_T/0/1/0/all/0/1\">Thomas Fevens</a>",
          "description": "Identifying anomalies refers to detecting samples that do not resemble the\ntraining data distribution. Many generative models have been used to find\nanomalies, and among them, generative adversarial network (GAN)-based\napproaches are currently very popular. GANs mainly rely on the rich contextual\ninformation of these models to identify the actual training distribution.\nFollowing this analogy, we suggested a new unsupervised model based on GANs --a\ncombination of an autoencoder and a GAN. Further, a new scoring function was\nintroduced to target anomalies where a linear combination of the internal\nrepresentation of the discriminator and the generator's visual representation,\nplus the encoded representation of the autoencoder, come together to define the\nproposed anomaly score. The model was further evaluated on benchmark datasets\nsuch as SVHN, CIFAR10, and MNIST, as well as a public medical dataset of\nleukemia images. In all the experiments, our model outperformed its existing\ncounterparts while slightly improving the inference time.",
          "link": "http://arxiv.org/abs/2112.15541",
          "publishedOn": "2022-01-03T07:15:43.908Z",
          "wordCount": null,
          "title": "on the effectiveness of generative adversarial network on anomaly detection. (arXiv:2112.15541v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2110.06394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weitong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We study the model-based reward-free reinforcement learning with linear\nfunction approximation for episodic Markov decision processes (MDPs). In this\nsetting, the agent works in two phases. In the exploration phase, the agent\ninteracts with the environment and collects samples without the reward. In the\nplanning phase, the agent is given a specific reward function and uses samples\ncollected from the exploration phase to learn a good policy. We propose a new\nprovably efficient algorithm, called UCRL-RFE under the Linear Mixture MDP\nassumption, where the transition probability kernel of the MDP can be\nparameterized by a linear function over certain feature mappings defined on the\ntriplet of state, action, and next state. We show that to obtain an\n$\\epsilon$-optimal policy for arbitrary reward function, UCRL-RFE needs to\nsample at most $\\tilde{\\mathcal{O}}(H^5d^2\\epsilon^{-2})$ episodes during the\nexploration phase. Here, $H$ is the length of the episode, $d$ is the dimension\nof the feature mapping. We also propose a variant of UCRL-RFE using\nBernstein-type bonus and show that it needs to sample at most\n$\\tilde{\\mathcal{O}}(H^4d(H + d)\\epsilon^{-2})$ to achieve an\n$\\epsilon$-optimal policy. By constructing a special class of linear Mixture\nMDPs, we also prove that for any reward-free algorithm, it needs to sample at\nleast $\\tilde \\Omega(H^2d\\epsilon^{-2})$ episodes to obtain an\n$\\epsilon$-optimal policy. Our upper bound matches the lower bound in terms of\nthe dependence on $\\epsilon$ and the dependence on $d$ if $H \\ge d$.",
          "link": "http://arxiv.org/abs/2110.06394",
          "publishedOn": "2022-01-03T07:15:43.908Z",
          "wordCount": null,
          "title": "Reward-Free Model-Based Reinforcement Learning with Linear Function Approximation. (arXiv:2110.06394v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.13208",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mayor_Torres_J/0/1/0/all/0/1\">Juan Manuel Mayor-Torres</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Medina_DeVilliers_S/0/1/0/all/0/1\">Sara Medina-DeVilliers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Clarkson_T/0/1/0/all/0/1\">Tessa Clarkson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lerner_M/0/1/0/all/0/1\">Matthew D. Lerner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riccardi_G/0/1/0/all/0/1\">Giuseppe Riccardi</a>",
          "description": "Current models on Explainable Artificial Intelligence (XAI) have shown an\nevident and quantified lack of reliability for measuring feature-relevance when\nstatistically entangled features are proposed for training deep classifiers.\nThere has been an increase in the application of Deep Learning in clinical\ntrials to predict early diagnosis of neuro-developmental disorders, such as\nAutism Spectrum Disorder (ASD). However, the inclusion of more reliable\nsaliency-maps to obtain more trustworthy and interpretable metrics using neural\nactivity features is still insufficiently mature for practical applications in\ndiagnostics or clinical trials. Moreover, in ASD research the inclusion of deep\nclassifiers that use neural measures to predict viewed facial emotions is\nrelatively unexplored. Therefore, in this study we propose the evaluation of a\nConvolutional Neural Network (CNN) for electroencephalography (EEG)-based\nfacial emotion recognition decoding complemented with a novel\nRemOve-And-Retrain (ROAR) methodology to recover highly relevant features used\nin the classifier. Specifically, we compare well-known relevance maps such as\nLayer-Wise Relevance Propagation (LRP), PatternNet, Pattern-Attribution, and\nSmooth-Grad Squared. This study is the first to consolidate a more transparent\nfeature-relevance calculation for a successful EEG-based facial emotion\nrecognition using a within-subject-trained CNN in typically-developed and ASD\nindividuals.",
          "link": "http://arxiv.org/abs/2111.13208",
          "publishedOn": "2022-01-03T07:15:43.906Z",
          "wordCount": null,
          "title": "Evaluation of Interpretability for Deep Learning algorithms in EEG Emotion Recognition: A case study in Autism. (arXiv:2111.13208v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.13901",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Irshad_F/0/1/0/all/0/1\">Faran Irshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karsch_S/0/1/0/all/0/1\">Stefan Karsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dopp_A/0/1/0/all/0/1\">Andreas D&#xf6;pp</a>",
          "description": "Bayesian optimization has proven to be an efficient method to optimize\nexpensive-to-evaluate systems. However, depending on the cost of single\nobservations, multi-dimensional optimizations of one or more objectives may\nstill be prohibitively expensive. Multi-fidelity optimization remedies this\nissue by including multiple, cheaper information sources such as low-resolution\napproximations in numerical simulations. Acquisition functions for\nmulti-fidelity optimization are typically based on exploration-heavy algorithms\nthat are difficult to combine with optimization towards multiple objectives.\n\nHere we show that the expected hypervolume improvement policy can act in many\nsituations as a suitable substitute. We incorporate the evaluation cost either\nvia a two-step evaluation or within a single acquisition function with an\nadditional fidelity-related objective. This permits simultaneous\nmulti-objective and multi-fidelity optimization, which allows to accurately\nestablish the Pareto set and front at fractional cost. Benchmarks show a cost\nreduction of an order of magnitude or more. Our method thus allows for Pareto\noptimization of extremely expansive black-box functions.\n\nThe presented methods are simple and straightforward to implement in\nexisting, optimized Bayesian optimization frameworks and can immediately be\nextended to batch optimization. The techniques can also be used to combine\ndifferent continuous and/or discrete fidelity dimensions, which makes them\nparticularly relevant for simulation problems in plasma physics, fluid dynamics\nand many other branches of scientific computing.",
          "link": "http://arxiv.org/abs/2112.13901",
          "publishedOn": "2022-01-03T07:15:43.897Z",
          "wordCount": null,
          "title": "Expected hypervolume improvement for simultaneous multi-objective and multi-fidelity optimization. (arXiv:2112.13901v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.12970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rongjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>",
          "description": "Scene Graph Generation (SGG) remains a challenging visual understanding task\ndue to its complex compositional property. Most previous works adopt a\nbottom-up two-stage or a point-based one-stage approach, which often suffers\nfrom overhead time complexity or sub-optimal design assumption. In this work,\nwe propose a novel SGG method to address the aforementioned issues, which\nformulates the task as a bipartite graph construction problem. To solve the\nproblem, we develop a transformer-based end-to-end framework that first\ngenerates the entity and predicate proposal set, followed by inferring directed\nedges to form the relation triplets. In particular, we develop a new\nentity-aware predicate representation based on a structural predicate generator\nto leverage the compositional property of relationships. Moreover, we design a\ngraph assembling module to infer the connectivity of the bipartite scene graph\nbased on our entity-aware structure, enabling us to generate the scene graph in\nan end-to-end manner. Extensive experimental results show that our design is\nable to achieve the state-of-the-art or comparable performance on two\nchallenging benchmarks, surpassing most of the existing approaches and enjoying\nhigher efficiency in inference. We hope our model can serve as a strong\nbaseline for the Transformer-based scene graph generation.",
          "link": "http://arxiv.org/abs/2112.12970",
          "publishedOn": "2022-01-03T07:15:43.896Z",
          "wordCount": null,
          "title": "SGTR: End-to-end Scene Graph Generation with Transformer. (arXiv:2112.12970v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weitong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lihong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "Thompson Sampling (TS) is one of the most effective algorithms for solving\ncontextual multi-armed bandit problems. In this paper, we propose a new\nalgorithm, called Neural Thompson Sampling, which adapts deep neural networks\nfor both exploration and exploitation. At the core of our algorithm is a novel\nposterior distribution of the reward, where its mean is the neural network\napproximator, and its variance is built upon the neural tangent features of the\ncorresponding neural network. We prove that, provided the underlying reward\nfunction is bounded, the proposed algorithm is guaranteed to achieve a\ncumulative regret of $\\mathcal{O}(T^{1/2})$, which matches the regret of other\ncontextual bandit algorithms in terms of total round number $T$. Experimental\ncomparisons with other benchmark bandit algorithms on various data sets\ncorroborate our theory.",
          "link": "http://arxiv.org/abs/2010.00827",
          "publishedOn": "2022-01-03T07:15:43.890Z",
          "wordCount": null,
          "title": "Neural Thompson Sampling. (arXiv:2010.00827v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heiss_J/0/1/0/all/0/1\">Jakob Heiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teichmann_J/0/1/0/all/0/1\">Josef Teichmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wutte_H/0/1/0/all/0/1\">Hanna Wutte</a>",
          "description": "We prove in this paper that wide ReLU neural networks (NNs) with at least one\nhidden layer optimized with l2-regularization on the parameters enforces\nmulti-task learning due to representation-learning - also in the limit width to\ninfinity. This is in contrast to multiple other idealized settings discussed in\nthe literature where wide (ReLU)-NNs loose their ability to benefit from\nmulti-task learning in the limit width to infinity. We deduce the multi-task\nlearning ability from proving an exact quantitative macroscopic\ncharacterization of the learned NN in function space.",
          "link": "http://arxiv.org/abs/2112.15577",
          "publishedOn": "2022-01-03T07:15:43.884Z",
          "wordCount": null,
          "title": "Infinite wide (finite depth) Neural Networks benefit from multi-task learning unlike shallow Gaussian Processes -- an exact quantitative macroscopic characterization. (arXiv:2112.15577v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.13811",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Guharoy_R/0/1/0/all/0/1\">Rabel Guharoy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jana_N/0/1/0/all/0/1\">Nanda Dulal Jana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biswas_S/0/1/0/all/0/1\">Suparna Biswas</a>",
          "description": "This paper presents an epilepsy detection method based on discrete wavelet\ntransform (DWT) and Machine learning classifiers. Here DWT has been used for\nfeature extraction as it provides a better decomposition of the signals in\ndifferent frequency bands. At first, DWT has been applied to the EEG signal to\nextract the detail and approximate coefficients or different sub-bands. After\nthe extraction of the coefficients, principal component analysis (PCA) has been\napplied on different sub-bands and then a feature level fusion technique is\nused to extract the important features in low dimensional feature space. Three\nclassifiers namely: Support Vector Machine (SVM) classifier, K-Nearest-Neighbor\n(KNN) classifier, and Naive Bayes (NB) Classifiers have been used in the\nproposed work for classifying the EEG signals. The proposed method is tested on\nBonn databases and provides a maximum of 100% recognition accuracy for KNN,\nSVM, NB classifiers.",
          "link": "http://arxiv.org/abs/2109.13811",
          "publishedOn": "2022-01-03T07:15:43.873Z",
          "wordCount": null,
          "title": "An Efficient Epileptic Seizure Detection Technique using Discrete Wavelet Transform and Machine Learning Classifiers. (arXiv:2109.13811v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.12271",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jia_Q/0/1/0/all/0/1\">Qiran Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shu_H/0/1/0/all/0/1\">Hai Shu</a>",
          "description": "Convolutional neural networks (CNNs) have achieved remarkable success in\nautomatically segmenting organs or lesions on 3D medical images. Recently,\nvision transformer networks have exhibited exceptional performance in 2D image\nclassification tasks. Compared with CNNs, transformer networks have an\nappealing advantage of extracting long-range features due to their\nself-attention algorithm. Therefore, we propose a CNN-Transformer combined\nmodel, called BiTr-Unet, with specific modifications for brain tumor\nsegmentation on multi-modal MRI scans. Our BiTr-Unet achieves good performance\non the BraTS2021 validation dataset with median Dice score 0.9335, 0.9304 and\n0.8899, and median Hausdorff distance 2.8284, 2.2361 and 1.4142 for the whole\ntumor, tumor core, and enhancing tumor, respectively. On the BraTS2021 testing\ndataset, the corresponding results are 0.9257, 0.9350 and 0.8874 for Dice\nscore, and 3, 2.2361 and 1.4142 for Hausdorff distance. The code is publicly\navailable at https://github.com/JustaTinyDot/BiTr-Unet.",
          "link": "http://arxiv.org/abs/2109.12271",
          "publishedOn": "2022-01-03T07:15:43.862Z",
          "wordCount": null,
          "title": "BiTr-Unet: a CNN-Transformer Combined Network for MRI Brain Tumor Segmentation. (arXiv:2109.12271v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arnob_S/0/1/0/all/0/1\">Samin Yeasar Arnob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohib_R/0/1/0/all/0/1\">Riyasat Ohib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plis_S/0/1/0/all/0/1\">Sergey Plis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>",
          "description": "Deep Reinforcement Learning (RL) is a powerful framework for solving complex\nreal-world problems. Large neural networks employed in the framework are\ntraditionally associated with better generalization capabilities, but their\nincreased size entails the drawbacks of extensive training duration,\nsubstantial hardware resources, and longer inference times. One way to tackle\nthis problem is to prune neural networks leaving only the necessary parameters.\nState-of-the-art concurrent pruning techniques for imposing sparsity perform\ndemonstrably well in applications where data distributions are fixed. However,\nthey have not yet been substantially explored in the context of RL. We close\nthe gap between RL and single-shot pruning techniques and present a general\npruning approach to the Offline RL. We leverage a fixed dataset to prune neural\nnetworks before the start of RL training. We then run experiments varying the\nnetwork sparsity level and evaluating the validity of pruning at initialization\ntechniques in continuous control tasks. Our results show that with 95% of the\nnetwork weights pruned, Offline-RL algorithms can still retain performance in\nthe majority of our experiments. To the best of our knowledge, no prior work\nutilizing pruning in RL retained performance at such high levels of sparsity.\n\nMoreover, pruning at initialization techniques can be easily integrated into\nany existing Offline-RL algorithms without changing the learning objective.",
          "link": "http://arxiv.org/abs/2112.15579",
          "publishedOn": "2022-01-03T07:15:43.849Z",
          "wordCount": null,
          "title": "Single-Shot Pruning for Offline Reinforcement Learning. (arXiv:2112.15579v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2111.02434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1\">Greg Ver Steeg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>",
          "description": "Sampling from an unnormalized probability distribution is a fundamental\nproblem in machine learning with applications including Bayesian modeling,\nlatent factor inference, and energy-based model training. After decades of\nresearch, variations of MCMC remain the default approach to sampling despite\nslow convergence. Auxiliary neural models can learn to speed up MCMC, but the\noverhead for training the extra model can be prohibitive. We propose a\nfundamentally different approach to this problem via a new Hamiltonian dynamics\nwith a non-Newtonian momentum. In contrast to MCMC approaches like Hamiltonian\nMonte Carlo, no stochastic step is required. Instead, the proposed\ndeterministic dynamics in an extended state space exactly sample the target\ndistribution, specified by an energy function, under an assumption of\nergodicity. Alternatively, the dynamics can be interpreted as a normalizing\nflow that samples a specified energy model without training. The proposed\nEnergy Sampling Hamiltonian (ESH) dynamics have a simple form that can be\nsolved with existing ODE solvers, but we derive a specialized solver that\nexhibits much better performance. ESH dynamics converge faster than their MCMC\ncompetitors enabling faster, more stable training of neural network energy\nmodels.",
          "link": "http://arxiv.org/abs/2111.02434",
          "publishedOn": "2022-01-03T07:15:43.848Z",
          "wordCount": null,
          "title": "Hamiltonian Dynamics with Non-Newtonian Momentum for Rapid Sampling. (arXiv:2111.02434v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silva_T/0/1/0/all/0/1\">Thalles Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivera_A/0/1/0/all/0/1\">Ad&#xed;n Ram&#xed;rez Rivera</a>",
          "description": "We introduce Consistent Assignment for Representation Learning (CARL), an\nunsupervised learning method to learn visual representations by combining ideas\nfrom self-supervised contrastive learning and deep clustering. By viewing\ncontrastive learning from a clustering perspective, CARL learns unsupervised\nrepresentations by learning a set of general prototypes that serve as energy\nanchors to enforce different views of a given image to be assigned to the same\nprototype. Unlike contemporary work on contrastive learning with deep\nclustering, CARL proposes to learn the set of general prototypes in an online\nfashion, using gradient descent without the necessity of using\nnon-differentiable algorithms or K-Means to solve the cluster assignment\nproblem. CARL surpasses its competitors in many representations learning\nbenchmarks, including linear evaluation, semi-supervised learning, and transfer\nlearning.",
          "link": "http://arxiv.org/abs/2112.15421",
          "publishedOn": "2022-01-03T07:15:43.847Z",
          "wordCount": null,
          "title": "Representation Learning via Consistent Assignment of Views to Clusters. (arXiv:2112.15421v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.13794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Weiran Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Sean Qian</a>",
          "description": "The effectiveness of traditional traffic prediction methods is often\nextremely limited when forecasting traffic dynamics in early morning. The\nreason is that traffic can break down drastically during the early morning\ncommute, and the time and duration of this break-down vary substantially from\nday to day. Early morning traffic forecast is crucial to inform morning-commute\ntraffic management, but they are generally challenging to predict in advance,\nparticularly by midnight. In this paper, we propose to mine Twitter messages as\na probing method to understand the impacts of people's work and rest patterns\nin the evening/midnight of the previous day to the next-day morning traffic.\nThe model is tested on freeway networks in Pittsburgh as experiments. The\nresulting relationship is surprisingly simple and powerful. We find that, in\ngeneral, the earlier people rest as indicated from Tweets, the more congested\nroads will be in the next morning. The occurrence of big events in the evening\nbefore, represented by higher or lower tweet sentiment than normal, often\nimplies lower travel demand in the next morning than normal days. Besides,\npeople's tweeting activities in the night before and early morning are\nstatistically associated with congestion in morning peak hours. We make use of\nsuch relationships to build a predictive framework which forecasts morning\ncommute congestion using people's tweeting profiles extracted by 5 am or as\nlate as the midnight prior to the morning. The Pittsburgh study supports that\nour framework can precisely predict morning congestion, particularly for some\nroad segments upstream of roadway bottlenecks with large day-to-day congestion\nvariation. Our approach considerably outperforms those existing methods without\nTwitter message features, and it can learn meaningful representation of demand\nfrom tweeting profiles that offer managerial insights.",
          "link": "http://arxiv.org/abs/2009.13794",
          "publishedOn": "2022-01-03T07:15:43.846Z",
          "wordCount": null,
          "title": "From Twitter to Traffic Predictor: Next-Day Morning Traffic Prediction Using Social Media Data. (arXiv:2009.13794v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Casacuberta_S/0/1/0/all/0/1\">S&#xed;lvia Casacuberta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suel_E/0/1/0/all/0/1\">Esra Suel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flaxman_S/0/1/0/all/0/1\">Seth Flaxman</a>",
          "description": "In this paper we introduce a new problem within the growing literature of\ninterpretability for convolution neural networks (CNNs). While previous work\nhas focused on the question of how to visually interpret CNNs, we ask what it\nis that we care to interpret, that is, which layers and neurons are worth our\nattention? Due to the vast size of modern deep learning network architectures,\nautomated, quantitative methods are needed to rank the relative importance of\nneurons so as to provide an answer to this question. We present a new\nstatistical method for ranking the hidden neurons in any convolutional layer of\na network. We define importance as the maximal correlation between the\nactivation maps and the class score. We provide different ways in which this\nmethod can be used for visualization purposes with MNIST and ImageNet, and show\na real-world application of our method to air pollution prediction with\nstreet-level images.",
          "link": "http://arxiv.org/abs/2112.15571",
          "publishedOn": "2022-01-03T07:15:43.843Z",
          "wordCount": null,
          "title": "PCACE: A Statistical Approach to Ranking Neurons for CNN Interpretability. (arXiv:2112.15571v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15538",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahsan_M/0/1/0/all/0/1\">Md Manjurul Ahsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddique_Z/0/1/0/all/0/1\">Zahed Siddique</a>",
          "description": "Globally, there is a substantial unmet need to diagnose various diseases\neffectively. The complexity of the different disease mechanisms and underlying\nsymptoms of the patient population presents massive challenges to developing\nthe early diagnosis tool and effective treatment. Machine Learning (ML), an\narea of Artificial Intelligence (AI), enables researchers, physicians, and\npatients to solve some of these issues. Based on relevant research, this review\nexplains how Machine Learning (ML) and Deep Learning (DL) are being used to\nhelp in the early identification of numerous diseases. To begin, a bibliometric\nstudy of the publication is given using data from the Scopus and Web of Science\n(WOS) databases. The bibliometric study of 1216 publications was undertaken to\ndetermine the most prolific authors, nations, organizations, and most cited\narticles. The review then summarizes the most recent trends and approaches in\nMachine Learning-based Disease Diagnosis (MLBDD), considering the following\nfactors: algorithm, disease types, data type, application, and evaluation\nmetrics. Finally, the paper highlights key results and provides insight into\nfuture trends and opportunities in the MLBDD area.",
          "link": "http://arxiv.org/abs/2112.15538",
          "publishedOn": "2022-01-03T07:15:43.842Z",
          "wordCount": null,
          "title": "Machine learning based disease diagnosis: A comprehensive review. (arXiv:2112.15538v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1907.00457",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ling_S/0/1/0/all/0/1\">Shaoshi Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salazar_J/0/1/0/all/0/1\">Julian Salazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuzong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1\">Katrin Kirchhoff</a>",
          "description": "We introduce BERTphone, a Transformer encoder trained on large speech corpora\nthat outputs phonetically-aware contextual representation vectors that can be\nused for both speaker and language recognition. This is accomplished by\ntraining on two objectives: the first, inspired by adapting BERT to the\ncontinuous domain, involves masking spans of input frames and reconstructing\nthe whole sequence for acoustic representation learning; the second, inspired\nby the success of bottleneck features from ASR, is a sequence-level CTC loss\napplied to phoneme labels for phonetic representation learning. We pretrain two\nBERTphone models (one on Fisher and one on TED-LIUM) and use them as feature\nextractors into x-vector-style DNNs for both tasks. We attain a\nstate-of-the-art $C_{\\text{avg}}$ of 6.16 on the challenging LRE07 3sec\nclosed-set language recognition task. On Fisher and VoxCeleb speaker\nrecognition tasks, we see an 18% relative reduction in speaker EER when\ntraining on BERTphone vectors instead of MFCCs. In general, BERTphone\noutperforms previous phonetic pretraining approaches on the same data. We\nrelease our code and models at\nhttps://github.com/awslabs/speech-representations.",
          "link": "http://arxiv.org/abs/1907.00457",
          "publishedOn": "2022-01-03T07:15:43.839Z",
          "wordCount": null,
          "title": "BERTphone: Phonetically-Aware Encoder Representations for Utterance-Level Speaker and Language Recognition. (arXiv:1907.00457v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06201",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Loecher_M/0/1/0/all/0/1\">Markus Loecher</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>",
          "description": "Tree-based algorithms such as random forests and gradient boosted trees\ncontinue to be among the most popular and powerful machine learning models used\nacross multiple disciplines. The conventional wisdom of estimating the impact\nof a feature in tree based models is to measure the \\textit{node-wise reduction\nof a loss function}, which (i) yields only global importance measures and (ii)\nis known to suffer from severe biases. Conditional feature contributions (CFCs)\nprovide \\textit{local}, case-by-case explanations of a prediction by following\nthe decision path and attributing changes in the expected output of the model\nto each feature along the path. However, Lundberg et al. pointed out a\npotential bias of CFCs which depends on the distance from the root of a tree.\nThe by now immensely popular alternative, SHapley Additive exPlanation (SHAP)\nvalues appear to mitigate this bias but are computationally much more\nexpensive. Here we contribute a thorough comparison of the explanations\ncomputed by both methods on a set of 164 publicly available classification\nproblems in order to provide data-driven algorithm recommendations to current\nresearchers. For random forests, we find extremely high similarities and\ncorrelations of both local and global SHAP values and CFC scores, leading to\nvery similar rankings and interpretations. Analogous conclusions hold for the\nfidelity of using global feature importance scores as a proxy for the\npredictive power associated with each feature.",
          "link": "http://arxiv.org/abs/2108.06201",
          "publishedOn": "2022-01-03T07:15:43.835Z",
          "wordCount": null,
          "title": "Data-driven advice for interpreting local and global model predictions in bioinformatics problems. (arXiv:2108.06201v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.11663",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shaocong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>",
          "description": "Alternating gradient-descent-ascent (AltGDA) is an optimization algorithm\nthat has been widely used for model training in various machine learning\napplications, which aim to solve a nonconvex minimax optimization problem.\nHowever, the existing studies show that it suffers from a high computation\ncomplexity in nonconvex minimax optimization. In this paper, we develop a\nsingle-loop and fast AltGDA-type algorithm that leverages proximal gradient\nupdates and momentum acceleration to solve regularized nonconvex minimax\noptimization problems. By identifying the intrinsic Lyapunov function of this\nalgorithm, we prove that it converges to a critical point of the nonconvex\nminimax optimization problem and achieves a computation complexity\n$\\mathcal{O}(\\kappa^{1.5}\\epsilon^{-2})$, where $\\epsilon$ is the desired level\nof accuracy and $\\kappa$ is the problem's condition number. Such a computation\ncomplexity improves the state-of-the-art complexities of single-loop GDA and\nAltGDA algorithms (see the summary of comparison in Table 1). We demonstrate\nthe effectiveness of our algorithm via an experiment on adversarial deep\nlearning.",
          "link": "http://arxiv.org/abs/2112.11663",
          "publishedOn": "2022-01-03T07:15:43.835Z",
          "wordCount": null,
          "title": "Accelerated Proximal Alternating Gradient-Descent-Ascent for Nonconvex Minimax Machine Learning. (arXiv:2112.11663v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samani_A/0/1/0/all/0/1\">Amir Samani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1\">Richard S. Sutton</a>",
          "description": "Learning continually and online from a continuous stream of data is\nchallenging, especially for a reinforcement learning agent with sequential\ndata. When the environment only provides observations giving partial\ninformation about the state of the environment, the agent must learn the agent\nstate based on the data stream of experience. We refer to the state learned\ndirectly from the data stream of experience as the agent state. Recurrent\nneural networks can learn the agent state, but the training methods are\ncomputationally expensive and sensitive to the hyper-parameters, making them\nunideal for online learning. This work introduces methods based on the\ngenerate-and-test approach to learn the agent state. A generate-and-test\nalgorithm searches for state features by generating features and testing their\nusefulness. In this process, features useful for the agent's performance on the\ntask are preserved, and the least useful features get replaced with newly\ngenerated features. We study the effectiveness of our methods on two online\nmulti-step prediction problems. The first problem, trace conditioning, focuses\non the agent's ability to remember a cue for a prediction multiple steps into\nthe future. In the second problem, trace patterning, the agent needs to learn\npatterns in the observation signals and remember them for future predictions.\nWe show that our proposed methods can effectively learn the agent state online\nand produce accurate predictions.",
          "link": "http://arxiv.org/abs/2112.15236",
          "publishedOn": "2022-01-03T07:15:43.832Z",
          "wordCount": null,
          "title": "Learning Agent State Online with Recurrent Generate-and-Test. (arXiv:2112.15236v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassanaly_M/0/1/0/all/0/1\">Malik Hassanaly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perry_B/0/1/0/all/0/1\">Bruce A. Perry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_M/0/1/0/all/0/1\">Michael E. Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yellapantula_S/0/1/0/all/0/1\">Shashank Yellapantula</a>",
          "description": "Improvements in computational and experimental capabilities are rapidly\nincreasing the amount of scientific data that is routinely generated. In\napplications that are constrained by memory and computational intensity,\nexcessively large datasets may hinder scientific discovery, making data\nreduction a critical component of data-driven methods. Datasets are growing in\ntwo directions: the number of data points and their dimensionality. Whereas\ndata compression techniques are concerned with reducing dimensionality, the\nfocus here is on reducing the number of data points. A strategy is proposed to\nselect data points such that they uniformly span the phase-space of the data.\nThe algorithm proposed relies on estimating the probability map of the data and\nusing it to construct an acceptance probability. An iterative method is used to\naccurately estimate the probability of the rare data points when only a small\nsubset of the dataset is used to construct the probability map. Instead of\nbinning the phase-space to estimate the probability map, its functional form is\napproximated with a normalizing flow. Therefore, the method naturally extends\nto high-dimensional datasets. The proposed framework is demonstrated as a\nviable pathway to enable data-efficient machine learning when abundant data is\navailable. An implementation of the method is available in a companion\nrepository (https://github.com/NREL/Phase-space-sampling).",
          "link": "http://arxiv.org/abs/2112.15446",
          "publishedOn": "2022-01-03T07:15:43.831Z",
          "wordCount": null,
          "title": "Uniform-in-Phase-Space Data Selection with Iterative Normalizing Flows. (arXiv:2112.15446v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15595",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Irons_N/0/1/0/all/0/1\">Nicholas J. Irons</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scetbon_M/0/1/0/all/0/1\">Meyer Scetbon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pal_S/0/1/0/all/0/1\">Soumik Pal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Harchaoui_Z/0/1/0/all/0/1\">Zaid Harchaoui</a>",
          "description": "Triangular flows, also known as Kn\\\"{o}the-Rosenblatt measure couplings,\ncomprise an important building block of normalizing flow models for generative\nmodeling and density estimation, including popular autoregressive flow models\nsuch as real-valued non-volume preserving transformation models (Real NVP). We\npresent statistical guarantees and sample complexity bounds for triangular flow\nstatistical models. In particular, we establish the statistical consistency and\nthe finite sample convergence rates of the Kullback-Leibler estimator of the\nKn\\\"{o}the-Rosenblatt measure coupling using tools from empirical process\ntheory. Our results highlight the anisotropic geometry of function classes at\nplay in triangular flows, shed light on optimal coordinate ordering, and lead\nto statistical guarantees for Jacobian flows. We conduct numerical experiments\non synthetic data to illustrate the practical implications of our theoretical\nfindings.",
          "link": "http://arxiv.org/abs/2112.15595",
          "publishedOn": "2022-01-03T07:15:43.831Z",
          "wordCount": null,
          "title": "Triangular Flows for Generative Modeling: Statistical Consistency, Smoothness Classes, and Fast Rates. (arXiv:2112.15595v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15550",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Irie_K/0/1/0/all/0/1\">Kazuki Irie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlag_I/0/1/0/all/0/1\">Imanol Schlag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Csordas_R/0/1/0/all/0/1\">R&#xf3;bert Csord&#xe1;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1\">J&#xfc;rgen Schmidhuber</a>",
          "description": "We share our experience with the recently released WILDS benchmark, a\ncollection of ten datasets dedicated to developing models and training\nstrategies which are robust to domain shifts. Several experiments yield a\ncouple of critical observations which we believe are of general interest for\nany future work on WILDS. Our study focuses on two datasets: iWildCam and FMoW.\nWe show that (1) Conducting separate cross-validation for each evaluation\nmetric is crucial for both datasets, (2) A weak correlation between validation\nand test performance might make model development difficult for iWildCam, (3)\nMinor changes in the training of hyper-parameters improve the baseline by a\nrelatively large margin (mainly on FMoW), (4) There is a strong correlation\nbetween certain domains and certain target labels (mainly on iWildCam). To the\nbest of our knowledge, no prior work on these datasets has reported these\nobservations despite their obvious importance. Our code is public.",
          "link": "http://arxiv.org/abs/2112.15550",
          "publishedOn": "2022-01-03T07:15:43.823Z",
          "wordCount": null,
          "title": "Improving Baselines in the Wild. (arXiv:2112.15550v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.07548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kocanaogullari_A/0/1/0/all/0/1\">Aziz Ko&#xe7;anao&#x11f;ullar&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ariyurek_C/0/1/0/all/0/1\">Cemre Ariyurek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afacan_O/0/1/0/all/0/1\">Onur Afacan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurugol_S/0/1/0/all/0/1\">Sila Kurugol</a>",
          "description": "Kidney DCE-MRI aims at both qualitative assessment of kidney anatomy and\nquantitative assessment of kidney function by estimating the tracer kinetic\n(TK) model parameters. Accurate estimation of TK model parameters requires an\naccurate measurement of the arterial input function (AIF) with high temporal\nresolution. Accelerated imaging is used to achieve high temporal resolution,\nwhich yields under-sampling artifacts in the reconstructed images. Compressed\nsensing (CS) methods offer a variety of reconstruction options. Most commonly,\nsparsity of temporal differences is encouraged for regularization to reduce\nartifacts. Increasing regularization in CS methods removes the ambient\nartifacts but also over-smooths the signal temporally which reduces the\nparameter estimation accuracy. In this work, we propose a single image trained\ndeep neural network to reduce MRI under-sampling artifacts without reducing the\naccuracy of functional imaging markers. Instead of regularizing with a penalty\nterm in optimization, we promote regularization by generating images from a\nlower dimensional representation. In this manuscript we motivate and explain\nthe lower dimensional input design. We compare our approach to CS\nreconstructions with multiple regularization weights. Proposed approach results\nin kidney biomarkers that are highly correlated with the ground truth markers\nestimated using the CS reconstruction which was optimized for functional\nanalysis. At the same time, the proposed approach reduces the artifacts in the\nreconstructed images.",
          "link": "http://arxiv.org/abs/2109.07548",
          "publishedOn": "2022-01-03T07:15:43.823Z",
          "wordCount": null,
          "title": "Learning the Regularization in DCE-MR Image Reconstruction for Functional Imaging of Kidneys. (arXiv:2109.07548v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.14793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Exarchakis_G/0/1/0/all/0/1\">Georgios Exarchakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oubari_O/0/1/0/all/0/1\">Omar Oubari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenz_G/0/1/0/all/0/1\">Gregor Lenz</a>",
          "description": "We propose a simple and efficient clustering method for high-dimensional data\nwith a large number of clusters. Our algorithm achieves high-performance by\nevaluating distances of datapoints with a subset of the cluster centres. Our\ncontribution is substantially more efficient than k-means as it does not\nrequire an all to all comparison of data points and clusters. We show that the\noptimal solutions of our approximation are the same as in the exact solution.\nHowever, our approach is considerably more efficient at extracting these\nclusters compared to the state-of-the-art. We compare our approximation with\nthe exact k-means and alternative approximation approaches on a series of\nstandardised clustering tasks. For the evaluation, we consider the algorithmic\ncomplexity, including number of operations to convergence, and the stability of\nthe results.",
          "link": "http://arxiv.org/abs/2112.14793",
          "publishedOn": "2022-01-03T07:15:43.813Z",
          "wordCount": null,
          "title": "A sampling-based approach for efficient clustering in large datasets. (arXiv:2112.14793v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Sungmin Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_N/0/1/0/all/0/1\">Naeun Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Youngjoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>",
          "description": "We propose a novel and effective purification based adversarial defense\nmethod against pre-processor blind white- and black-box attacks. Our method is\ncomputationally efficient and trained only with self-supervised learning on\ngeneral images, without requiring any adversarial training or retraining of the\nclassification model. We first show an empirical analysis on the adversarial\nnoise, defined to be the residual between an original image and its adversarial\nexample, has almost zero mean, symmetric distribution. Based on this\nobservation, we propose a very simple iterative Gaussian Smoothing (GS) which\ncan effectively smooth out adversarial noise and achieve substantially high\nrobust accuracy. To further improve it, we propose Neural Contextual Iterative\nSmoothing (NCIS), which trains a blind-spot network (BSN) in a self-supervised\nmanner to reconstruct the discriminative features of the original image that is\nalso smoothed out by GS. From our extensive experiments on the large-scale\nImageNet using four classification models, we show that our method achieves\nboth competitive standard accuracy and state-of-the-art robust accuracy against\nmost strong purifier-blind white- and black-box attacks. Also, we propose a new\nbenchmark for evaluating a purification method based on commercial image\nclassification APIs, such as AWS, Azure, Clarifai and Google. We generate\nadversarial examples by ensemble transfer-based black-box attack, which can\ninduce complete misclassification of APIs, and demonstrate that our method can\nbe used to increase adversarial robustness of APIs.",
          "link": "http://arxiv.org/abs/2106.11644",
          "publishedOn": "2022-01-03T07:15:43.813Z",
          "wordCount": null,
          "title": "NCIS: Neural Contextual Iterative Smoothing for Purifying Adversarial Perturbations. (arXiv:2106.11644v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bakkelund_D/0/1/0/all/0/1\">Daniel Bakkelund</a>",
          "description": "We present an objective function for similarity based hierarchical clustering\nof partially ordered data that preserves the partial order. That is, if $x \\le\ny$, and if $[x]$ and $[y]$ are the respective clusters of $x$ and $y$, then\nthere is an order relation $\\le'$ on the clusters for which $[x] \\le' |y]$. The\ntheory distinguishes itself from existing theories for clustering of ordered\ndata in that the order relation and the similarity are combined into a\nbi-objective optimisation problem to obtain a hierarchical clustering seeking\nto satisfy both. In particular, the order relation is weighted in the range\n$[0,1]$, and if the similarity and the order relation are not aligned, then\norder preservation may have to yield in favor of clustering. Finding an optimal\nsolution is NP-hard, so we provide a polynomial time approximation algorithm,\nwith a relative performance guarantee of $O\\!\\left(\\log^{3/2} \\!\\!\\, n\n\\right)$, based on successive applications of directed sparsest cut. We provide\na demonstration on a benchmark dataset, showing that our method outperforms\nexisting methods for order preserving hierarchical clustering with significant\nmargin. The theory is an extension of the Dasgupta cost function for divisive\nhierarchical clustering.",
          "link": "http://arxiv.org/abs/2109.04266",
          "publishedOn": "2022-01-03T07:15:43.812Z",
          "wordCount": null,
          "title": "An objective function for order preserving hierarchical clustering. (arXiv:2109.04266v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.12360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zixiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1\">Difan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "A recent line of research on deep learning focuses on the extremely\nover-parameterized setting, and shows that when the network width is larger\nthan a high degree polynomial of the training sample size $n$ and the inverse\nof the target error $\\epsilon^{-1}$, deep neural networks learned by\n(stochastic) gradient descent enjoy nice optimization and generalization\nguarantees. Very recently, it is shown that under certain margin assumptions on\nthe training data, a polylogarithmic width condition suffices for two-layer\nReLU networks to converge and generalize (Ji and Telgarsky, 2019). However,\nwhether deep neural networks can be learned with such a mild\nover-parameterization is still an open question. In this work, we answer this\nquestion affirmatively and establish sharper learning guarantees for deep ReLU\nnetworks trained by (stochastic) gradient descent. In specific, under certain\nassumptions made in previous work, our optimization and generalization\nguarantees hold with network width polylogarithmic in $n$ and $\\epsilon^{-1}$.\nOur results push the study of over-parameterized deep neural networks towards\nmore practical settings.",
          "link": "http://arxiv.org/abs/1911.12360",
          "publishedOn": "2022-01-03T07:15:43.810Z",
          "wordCount": null,
          "title": "How Much Over-parameterization Is Sufficient to Learn Deep ReLU Networks?. (arXiv:1911.12360v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15545",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Irie_K/0/1/0/all/0/1\">Kazuki Irie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1\">J&#xfc;rgen Schmidhuber</a>",
          "description": "The inputs and/or outputs of some neural nets are weight matrices of other\nneural nets. Indirect encodings or end-to-end compression of weight matrices\ncould help to scale such approaches. Our goal is to open a discussion on this\ntopic, starting with recurrent neural networks for character-level language\nmodelling whose weight matrices are encoded by the discrete cosine transform.\nOur fast weight version thereof uses a recurrent neural network to parameterise\nthe compressed weights. We present experimental results on the enwik8 dataset.",
          "link": "http://arxiv.org/abs/2112.15545",
          "publishedOn": "2022-01-03T07:15:43.802Z",
          "wordCount": null,
          "title": "Training and Generating Neural Networks in Compressed Weight Space. (arXiv:2112.15545v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15568",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lahire_T/0/1/0/all/0/1\">Thibault Lahire</a>",
          "description": "This technical report is devoted to explaining how the actor loss of soft\nactor critic is obtained, as well as the associated gradient estimate. It gives\nthe necessary mathematical background to derive all the presented equations,\nfrom the theoretical actor loss to the one implemented in practice. This\nnecessitates a comparison of the reparameterization trick used in soft actor\ncritic with the nabla log trick, which leads to open questions regarding the\nmost efficient method to use.",
          "link": "http://arxiv.org/abs/2112.15568",
          "publishedOn": "2022-01-03T07:15:43.801Z",
          "wordCount": null,
          "title": "Actor Loss of Soft Actor Critic Explained. (arXiv:2112.15568v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.08250",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Anderer_M/0/1/0/all/0/1\">Matthias Anderer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>",
          "description": "Hierarchical forecasting with intermittent time series is a challenge in both\nresearch and empirical studies. Extensive research focuses on improving the\naccuracy of each hierarchy, especially the intermittent time series at bottom\nlevels. Then hierarchical reconciliation could be used to improve the overall\nperformance further. In this paper, we present a\n\\emph{hierarchical-forecasting-with-alignment} approach that treats the bottom\nlevel forecasts as mutable to ensure higher forecasting accuracy on the upper\nlevels of the hierarchy. We employ a pure deep learning forecasting approach\nN-BEATS for continuous time series at the top levels and a widely used\ntree-based algorithm LightGBM for the intermittent time series at the bottom\nlevel. The \\emph{hierarchical-forecasting-with-alignment} approach is a simple\nyet effective variant of the bottom-up method, accounting for biases that are\ndifficult to observe at the bottom level. It allows suboptimal forecasts at the\nlower level to retain a higher overall performance. The approach in this\nempirical study was developed by the first author during the M5 Forecasting\nAccuracy competition, ranking second place. The method is also business\norientated and could benefit for business strategic planning.",
          "link": "http://arxiv.org/abs/2103.08250",
          "publishedOn": "2022-01-03T07:15:43.798Z",
          "wordCount": null,
          "title": "Hierarchical forecasting with a top-down alignment of independent level forecasts. (arXiv:2103.08250v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.05170",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hong_M/0/1/0/all/0/1\">Mingyi Hong</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wai_H/0/1/0/all/0/1\">Hoi-To Wai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoran Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoran Yang</a>",
          "description": "This paper analyzes a two-timescale stochastic algorithm framework for\nbilevel optimization. Bilevel optimization is a class of problems which exhibit\na two-level structure, and its goal is to minimize an outer objective function\nwith variables which are constrained to be the optimal solution to an (inner)\noptimization problem. We consider the case when the inner problem is\nunconstrained and strongly convex, while the outer problem is constrained and\nhas a smooth objective function. We propose a two-timescale stochastic\napproximation (TTSA) algorithm for tackling such a bilevel problem. In the\nalgorithm, a stochastic gradient update with a larger step size is used for the\ninner problem, while a projected stochastic gradient update with a smaller step\nsize is used for the outer problem. We analyze the convergence rates for the\nTTSA algorithm under various settings: when the outer problem is strongly\nconvex (resp.~weakly convex), the TTSA algorithm finds an\n$\\mathcal{O}(K^{-2/3})$-optimal (resp.~$\\mathcal{O}(K^{-2/5})$-stationary)\nsolution, where $K$ is the total iteration number. As an application, we show\nthat a two-timescale natural actor-critic proximal policy optimization\nalgorithm can be viewed as a special case of our TTSA framework. Importantly,\nthe natural actor-critic algorithm is shown to converge at a rate of\n$\\mathcal{O}(K^{-1/4})$ in terms of the gap in expected discounted reward\ncompared to a global optimal policy.",
          "link": "http://arxiv.org/abs/2007.05170",
          "publishedOn": "2022-01-03T07:15:43.797Z",
          "wordCount": null,
          "title": "A Two-Timescale Framework for Bilevel Optimization: Complexity Analysis and Application to Actor-Critic. (arXiv:2007.05170v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.01278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinghui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>",
          "description": "Adversarial training is so far the most effective strategy in defending\nagainst adversarial examples. However, it suffers from high computational costs\ndue to the iterative adversarial attacks in each training step. Recent studies\nshow that it is possible to achieve fast Adversarial Training by performing a\nsingle-step attack with random initialization. However, such an approach still\nlags behind state-of-the-art adversarial training algorithms on both stability\nand model robustness. In this work, we develop a new understanding towards Fast\nAdversarial Training, by viewing random initialization as performing randomized\nsmoothing for better optimization of the inner maximization problem. Following\nthis new perspective, we also propose a new initialization strategy, backward\nsmoothing, to further improve the stability and model robustness over\nsingle-step robust training methods. Experiments on multiple benchmarks\ndemonstrate that our method achieves similar model robustness as the original\nTRADES method while using much less training time ($\\sim$3x improvement with\nthe same training schedule).",
          "link": "http://arxiv.org/abs/2010.01278",
          "publishedOn": "2022-01-03T07:15:43.797Z",
          "wordCount": null,
          "title": "Efficient Robust Training via Backward Smoothing. (arXiv:2010.01278v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Huihan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinyuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xisen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>",
          "description": "Pre-trained language models have been successful on text classification\ntasks, but are prone to learning spurious correlations from biased datasets,\nand are thus vulnerable when making inferences in a new domain. Prior work\nreveals such spurious patterns via post-hoc explanation algorithms which\ncompute the importance of input features. Further, the model is regularized to\nalign the importance scores with human knowledge, so that the unintended model\nbehaviors are eliminated. However, such a regularization technique lacks\nflexibility and coverage, since only importance scores towards a pre-defined\nlist of features are adjusted, while more complex human knowledge such as\nfeature interaction and pattern generalization can hardly be incorporated. In\nthis work, we propose to refine a learned language model for a target domain by\ncollecting human-provided compositional explanations regarding observed biases.\nBy parsing these explanations into executable logic rules, the human-specified\nrefinement advice from a small set of explanations can be generalized to more\ntraining examples. We additionally introduce a regularization term allowing\nadjustments for both importance and interaction of features to better rectify\nmodel behavior. We demonstrate the effectiveness of the proposed approach on\ntwo text classification tasks by showing improved performance in target domain\nas well as improved model fairness after refinement.",
          "link": "http://arxiv.org/abs/2103.10415",
          "publishedOn": "2022-01-03T07:15:43.797Z",
          "wordCount": null,
          "title": "Refining Language Models with Compositional Explanations. (arXiv:2103.10415v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15383",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Seroussi_I/0/1/0/all/0/1\">Inbar Seroussi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ringel_Z/0/1/0/all/0/1\">Zohar Ringel</a>",
          "description": "Deep neural networks (DNNs) are powerful tools for compressing and distilling\ninformation. Due to their scale and complexity, often involving billions of\ninter-dependent internal degrees of freedom, exact analysis approaches often\nfall short. A common strategy in such cases is to identify slow degrees of\nfreedom that average out the erratic behavior of the underlying fast\nmicroscopic variables. Here, we identify such a separation of scales occurring\nin over-parameterized deep convolutional neural networks (CNNs) at the end of\ntraining. It implies that neuron pre-activations fluctuate in a nearly Gaussian\nmanner with a deterministic latent kernel. While for CNNs with infinitely many\nchannels these kernels are inert, for finite CNNs they adapt and learn from\ndata in an analytically tractable manner. The resulting thermodynamic theory of\ndeep learning yields accurate predictions on several deep non-linear CNN toy\nmodels. In addition, it provides new ways of analyzing and understanding CNNs.",
          "link": "http://arxiv.org/abs/2112.15383",
          "publishedOn": "2022-01-03T07:15:43.796Z",
          "wordCount": null,
          "title": "Separation of scales and a thermodynamic description of feature learning in some CNNs. (arXiv:2112.15383v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15555",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiju Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taejoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>",
          "description": "In this paper, we propose a dual-module network architecture that employs a\ndomain discriminative feature module to encourage the domain invariant feature\nmodule to learn more domain invariant features. The proposed architecture can\nbe applied to any model that utilizes domain invariant features for\nunsupervised domain adaptation to improve its ability to extract domain\ninvariant features. We conduct experiments with the Domain-Adversarial Training\nof Neural Networks (DANN) model as a representative algorithm. In the training\nprocess, we supply the same input to the two modules and then extract their\nfeature distribution and prediction results respectively. We propose a\ndiscrepancy loss to find the discrepancy of the prediction results and the\nfeature distribution between the two modules. Through the adversarial training\nby maximizing the loss of their feature distribution and minimizing the\ndiscrepancy of their prediction results, the two modules are encouraged to\nlearn more domain discriminative and domain invariant features respectively.\nExtensive comparative evaluations are conducted and the proposed approach\noutperforms the state-of-the-art in most unsupervised domain adaptation tasks.",
          "link": "http://arxiv.org/abs/2112.15555",
          "publishedOn": "2022-01-03T07:15:43.791Z",
          "wordCount": null,
          "title": "An Unsupervised Domain Adaptation Model based on Dual-module Adversarial Training. (arXiv:2112.15555v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_L/0/1/0/all/0/1\">Levent Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>",
          "description": "Despite recent advances in modern machine learning algorithms, the opaqueness\nof their underlying mechanisms continues to be an obstacle in adoption. To\ninstill confidence and trust in artificial intelligence systems, Explainable\nArtificial Intelligence has emerged as a response to improving modern machine\nlearning algorithms' explainability. Inductive Logic Programming (ILP), a\nsubfield of symbolic artificial intelligence, plays a promising role in\ngenerating interpretable explanations because of its intuitive logic-driven\nframework. ILP effectively leverages abductive reasoning to generate\nexplainable first-order clausal theories from examples and background\nknowledge. However, several challenges in developing methods inspired by ILP\nneed to be addressed for their successful application in practice. For example,\nexisting ILP systems often have a vast solution space, and the induced\nsolutions are very sensitive to noises and disturbances. This survey paper\nsummarizes the recent advances in ILP and a discussion of statistical\nrelational learning and neural-symbolic algorithms, which offer synergistic\nviews to ILP. Following a critical review of the recent advances, we delineate\nobserved challenges and highlight potential avenues of further ILP-motivated\nresearch toward developing self-explanatory artificial intelligence systems.",
          "link": "http://arxiv.org/abs/2112.15319",
          "publishedOn": "2022-01-03T07:15:43.790Z",
          "wordCount": null,
          "title": "A Critical Review of Inductive Logic Programming Techniques for Explainable AI. (arXiv:2112.15319v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shrikanth_N/0/1/0/all/0/1\">N.C. Shrikanth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menzies_T/0/1/0/all/0/1\">Tim Menzies</a>",
          "description": "Before researchers rush to reason across all available data or try complex\nmethods, perhaps it is prudent to first check for simpler alternatives.\nSpecifically, if the historical data has the most information in some small\nregion, then perhaps a model learned from that region would suffice for the\nrest of the project.\n\nTo support this claim, we offer a case study with 240 GitHub projects, where\nwe find that the information in those projects \"clumped\" towards the earliest\nparts of the project. A defect prediction model learned from just the first 150\ncommits works as well, or better than state-of-the-art alternatives. Using just\nthis early life cycle data, we can build models very quickly, very early in the\nsoftware project life cycle. Moreover, using this method, we have shown that a\nsimple model (with just two features) generalizes to hundreds of software\nprojects.\n\nBased on this experience, we doubt that prior work on generalizing software\nengineering defect prediction models may have needlessly complicated an\ninherently simple process. Further, prior work that focused on later-life cycle\ndata needs to be revisited since their conclusions were drawn from relatively\nuninformative regions.\n\nReplication note: all our data and scripts are online at\nhttps://github.com/snaraya7/simplifying-software-analytics",
          "link": "http://arxiv.org/abs/2105.11082",
          "publishedOn": "2022-01-03T07:15:43.786Z",
          "wordCount": null,
          "title": "Simplifying Software Defect Prediction (via the \"early bird\" Heuristic). (arXiv:2105.11082v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02454",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mingqi Yuan</a>",
          "description": "Extrapolating beyond-demonstrator (BD) performance through the imitation\nlearning (IL) algorithm aims to learn from and outperform the demonstrator.\nMost existing BDIL algorithms are performed in two stages by first inferring a\nreward function before learning a policy via reinforcement learning (RL).\nHowever, such two-stage BDIL algorithms suffer from high computational\ncomplexity, weak robustness, and large performance variations. In particular, a\npoor reward function derived in the first stage will inevitably incur severe\nperformance loss in the second stage. In this work, we propose a hybrid\nadversarial imitation learning (HAIL) algorithm that is one-stage, model-free,\ngenerative-adversarial (GA) fashion and curiosity-driven. Thanks to the\none-stage design, the HAIL can integrate both the reward function learning and\nthe policy optimization into one procedure, which leads to many advantages such\nas low computational complexity, high robustness, and strong adaptability. More\nspecifically, HAIL simultaneously imitates the demonstrator and explores BD\nperformance by utilizing hybrid rewards. Extensive simulation results confirm\nthat HAIL can achieve higher performance as compared to other similar BDIL\nalgorithms.",
          "link": "http://arxiv.org/abs/2102.02454",
          "publishedOn": "2022-01-03T07:15:43.778Z",
          "wordCount": null,
          "title": "Hybrid Adversarial Imitation Learning. (arXiv:2102.02454v10 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiaqi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingjian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_Q/0/1/0/all/0/1\">Qiaozhu Mei</a>",
          "description": "Multinomial Logit (MNL) is one of the most popular discrete choice models and\nhas been widely used to model ranking data. However, there is a long-standing\ntechnical challenge of learning MNL from many real-world ranking data: exact\ncalculation of the MNL likelihood of \\emph{partial rankings} is generally\nintractable. In this work, we develop a scalable method for approximating the\nMNL likelihood of general partial rankings in polynomial time complexity. We\nalso extend the proposed method to learn mixture of MNL. We demonstrate that\nthe proposed methods are particularly helpful for applications to choice-based\nnetwork formation modeling, where the formation of new edges in a network is\nviewed as individuals making choices of their friends over a candidate set. The\nproblem of learning mixture of MNL models from partial rankings naturally\narises in such applications. And the proposed methods can be used to learn MNL\nmodels from network data without the strong assumption that temporal orders of\nall the edge formation are available. We conduct experiments on both synthetic\nand real-world network data to demonstrate that the proposed methods achieve\nmore accurate parameter estimation and better fitness of data compared to\nconventional methods.",
          "link": "http://arxiv.org/abs/2112.15575",
          "publishedOn": "2022-01-03T07:15:43.774Z",
          "wordCount": null,
          "title": "Fast Learning of MNL Model from General Partial Rankings with Application to Network Formation Modeling. (arXiv:2112.15575v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15486",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yifan Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_K/0/1/0/all/0/1\">Kevin Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertozzi_A/0/1/0/all/0/1\">Andrea L. Bertozzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bao Wang</a>",
          "description": "We propose near-optimal overlay networks based on $d$-regular expander graphs\nto accelerate decentralized federated learning (DFL) and improve its\ngeneralization. In DFL a massive number of clients are connected by an overlay\nnetwork, and they solve machine learning problems collaboratively without\nsharing raw data. Our overlay network design integrates spectral graph theory\nand the theoretical convergence and generalization bounds for DFL. As such, our\nproposed overlay networks accelerate convergence, improve generalization, and\nenhance robustness to clients failures in DFL with theoretical guarantees.\nAlso, we present an efficient algorithm to convert a given graph to a practical\noverlay network and maintaining the network topology after potential client\nfailures. We numerically verify the advantages of DFL with our proposed\nnetworks on various benchmark tasks, ranging from image classification to\nlanguage modeling using hundreds of clients.",
          "link": "http://arxiv.org/abs/2112.15486",
          "publishedOn": "2022-01-03T07:15:43.773Z",
          "wordCount": null,
          "title": "Efficient and Reliable Overlay Networks for Decentralized Federated Learning. (arXiv:2112.15486v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Quan_N/0/1/0/all/0/1\">Nguyen Hoang Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dat_N/0/1/0/all/0/1\">Nguyen Thanh Dat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_N/0/1/0/all/0/1\">Nguyen Hoang Minh Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinh_N/0/1/0/all/0/1\">Nguyen Van Vinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinh_N/0/1/0/all/0/1\">Ngo Thi Vinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thai_N/0/1/0/all/0/1\">Nguyen Phuong Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viet_T/0/1/0/all/0/1\">Tran Hong Viet</a>",
          "description": "We present an open-source toolkit for neural machine translation (NMT). The\nnew toolkit is mainly based on vaulted Transformer (Vaswani et al., 2017) along\nwith many other improvements detailed below, in order to create a\nself-contained, simple to use, consistent and comprehensive framework for\nMachine Translation tasks of various domains. It is tooled to support both\nbilingual and multilingual translation tasks, starting from building the model\nfrom respective corpora, to inferring new predictions or packaging the model to\nserving-capable JIT format.",
          "link": "http://arxiv.org/abs/2112.15272",
          "publishedOn": "2022-01-03T07:15:43.731Z",
          "wordCount": null,
          "title": "ViNMT: Neural Machine Translation Tookit. (arXiv:2112.15272v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Costa_P/0/1/0/all/0/1\">Pedro Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerqueira_V/0/1/0/all/0/1\">Vitor Cerqueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinagre_J/0/1/0/all/0/1\">Jo&#xe3;o Vinagre</a>",
          "description": "A time series represents a set of observations collected over time.\nTypically, these observations are captured with a uniform sampling frequency\n(e.g. daily). When data points are observed in uneven time intervals the time\nseries is referred to as irregular or intermittent. In such scenarios, the most\ncommon solution is to reconstruct the time series to make it regular, thus\nremoving its intermittency. We hypothesise that, in irregular time series, the\ntime at which each observation is collected may be helpful to summarise the\ndynamics of the data and improve forecasting performance. We study this idea by\ndeveloping a novel automatic feature engineering framework, which focuses on\nextracting information from this point of view, i.e., when each instance is\ncollected. We study how valuable this information is by integrating it in a\ntime series forecasting workflow and investigate how it compares to or\ncomplements state-of-the-art methods for regular time series forecasting. In\nthe end, we contribute by providing a novel framework that tackles feature\nengineering for time series from an angle previously vastly ignored. We show\nthat our approach has the potential to further extract more information about\ntime series that significantly improves forecasting performance.",
          "link": "http://arxiv.org/abs/2112.14806",
          "publishedOn": "2022-01-03T07:15:43.727Z",
          "wordCount": null,
          "title": "AutoFITS: Automatic Feature Engineering for Irregular Time Series. (arXiv:2112.14806v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15012",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenguang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Shuyuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>",
          "description": "Predicting human motion from historical pose sequence is crucial for a\nmachine to succeed in intelligent interactions with humans. One aspect that has\nbeen obviated so far, is the fact that how we represent the skeletal pose has a\ncritical impact on the prediction results. Yet there is no effort that\ninvestigates across different pose representation schemes. We conduct an\nindepth study on various pose representations with a focus on their effects on\nthe motion prediction task. Moreover, recent approaches build upon\noff-the-shelf RNN units for motion prediction. These approaches process input\npose sequence sequentially and inherently have difficulties in capturing\nlong-term dependencies. In this paper, we propose a novel RNN architecture\ntermed AHMR (Attentive Hierarchical Motion Recurrent network) for motion\nprediction which simultaneously models local motion contexts and a global\ncontext. We further explore a geodesic loss and a forward kinematics loss for\nthe motion prediction task, which have more geometric significance than the\nwidely employed L2 loss. Interestingly, we applied our method to a range of\narticulate objects including human, fish, and mouse. Empirical results show\nthat our approach outperforms the state-of-the-art methods in short-term\nprediction and achieves much enhanced long-term prediction proficiency, such as\nretaining natural human-like motions over 50 seconds predictions. Our codes are\nreleased.",
          "link": "http://arxiv.org/abs/2112.15012",
          "publishedOn": "2022-01-03T07:15:43.726Z",
          "wordCount": null,
          "title": "Investigating Pose Representations and Motion Contexts Modeling for 3D Motion Prediction. (arXiv:2112.15012v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chenlin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huzhang_G/0/1/0/all/0/1\">Guangda Huzhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Da_Q/0/1/0/all/0/1\">Qing Da</a>",
          "description": "To approach different business objectives, online traffic shaping algorithms\naim at improving exposures of a target set of items, such as boosting the\ngrowth of new commodities. Generally, these algorithms assume that the utility\nof each user-item pair can be accessed via a well-trained conversion rate\nprediction model. However, for real E-Commerce platforms, there are unavoidable\nfactors preventing us from learning such an accurate model. In order to break\nthe heavy dependence on accurate inputs of the utility, we propose a general\nonline traffic shaping protocol for online E-Commerce applications. In our\nframework, we approximate the function mapping the bonus scores, which\ngenerally are the only method to influence the ranking result in the traffic\nshaping problem, to the numbers of exposures and purchases. Concretely, we\napproximate the above function by a class of the piece-wise linear function\nconstructed on the convex hull of the explored data points. Moreover, we\nreformulate the online traffic shaping problem as linear programming where\nthese piece-wise linear functions are embedded into both the objective and\nconstraints. Our algorithm can straightforwardly optimize the linear\nprogramming in the prime space, and its solution can be simply applied by a\nstochastic strategy to fulfill the optimized objective and the constraints in\nexpectation. Finally, the online A/B test shows our proposed algorithm steadily\noutperforms the previous industrial level traffic shaping algorithm.",
          "link": "http://arxiv.org/abs/2112.14941",
          "publishedOn": "2022-01-03T07:15:43.724Z",
          "wordCount": null,
          "title": "A General Traffic Shaping Protocol in E-Commerce. (arXiv:2112.14941v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zabihi_S/0/1/0/all/0/1\">Soheil Zabihi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimian_E/0/1/0/all/0/1\">Elahe Rahimian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marefat_F/0/1/0/all/0/1\">Fatemeh Marefat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_A/0/1/0/all/0/1\">Amir Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohseni_P/0/1/0/all/0/1\">Pedram Mohseni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_A/0/1/0/all/0/1\">Arash Mohammadi</a>",
          "description": "Objective: The paper focuses on development of robust and accurate processing\nsolutions for continuous and cuff-less blood pressure (BP) monitoring. In this\nregard, a robust deep learning-based framework is proposed for computation of\nlow latency, continuous, and calibration-free upper and lower bounds on the\nsystolic and diastolic BP. Method: Referred to as the BP-Net, the proposed\nframework is a novel convolutional architecture that provides longer effective\nmemory while achieving superior performance due to incorporation of casual\ndialated convolutions and residual connections. To utilize the real potential\nof deep learning in extraction of intrinsic features (deep features) and\nenhance the long-term robustness, the BP-Net uses raw Electrocardiograph (ECG)\nand Photoplethysmograph (PPG) signals without extraction of any form of\nhand-crafted features as it is common in existing solutions. Results: By\ncapitalizing on the fact that datasets used in recent literature are not\nunified and properly defined, a benchmark dataset is constructed from the\nMIMIC-I and MIMIC-III databases obtained from PhysioNet. The proposed BP-Net is\nevaluated based on this benchmark dataset demonstrating promising performance\nand shows superior generalizable capacity. Conclusion: The proposed BP-Net\narchitecture is more accurate than canonical recurrent networks and enhances\nthe long-term robustness of the BP estimation task. Significance: The proposed\nBP-Net architecture addresses key drawbacks of existing BP estimation\nsolutions, i.e., relying heavily on extraction of hand-crafted features, such\nas pulse arrival time (PAT), and; Lack of robustness. Finally, the constructed\nBP-Net dataset provides a unified base for evaluation and comparison of deep\nlearning-based BP estimation algorithms.",
          "link": "http://arxiv.org/abs/2112.15271",
          "publishedOn": "2022-01-03T07:15:43.724Z",
          "wordCount": null,
          "title": "BP-Net: Cuff-less, Calibration-free, and Non-invasive Blood Pressure Estimation via a Generic Deep Convolutional Architecture. (arXiv:2112.15271v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15362",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiamian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Ziyi Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_Z/0/1/0/all/0/1\">Zhiqiang Tao</a>",
          "description": "Recently, hyperspectral imaging (HSI) has attracted increasing research\nattention, especially for the ones based on a coded aperture snapshot spectral\nimaging (CASSI) system. Existing deep HSI reconstruction models are generally\ntrained on paired data to retrieve original signals upon 2D compressed\nmeasurements given by a particular optical hardware mask in CASSI, during which\nthe mask largely impacts the reconstruction performance and could work as a\n\"model hyperparameter\" governing on data augmentations. This mask-specific\ntraining style will lead to a hardware miscalibration issue, which sets up\nbarriers to deploying deep HSI models among different hardware and noisy\nenvironments. To address this challenge, we introduce mask uncertainty for HSI\nwith a complete variational Bayesian learning treatment and explicitly model it\nthrough a mask decomposition inspired by real hardware. Specifically, we\npropose a novel Graph-based Self-Tuning (GST) network to reason uncertainties\nadapting to varying spatial structures of masks among different hardware.\nMoreover, we develop a bilevel optimization framework to balance HSI\nreconstruction and uncertainty estimation, accounting for the hyperparameter\nproperty of masks. Extensive experimental results and model discussions\nvalidate the effectiveness (over 33/30 dB) of the proposed GST method under two\nmiscalibration scenarios and demonstrate a highly competitive performance\ncompared with the state-of-the-art well-calibrated methods. Our code and\npre-trained model are available at https://github.com/Jiamian\nWang/mask_uncertainty_spectral_SCI",
          "link": "http://arxiv.org/abs/2112.15362",
          "publishedOn": "2022-01-03T07:15:43.723Z",
          "wordCount": null,
          "title": "Calibrated Hyperspectral Image Reconstruction via Graph-based Self-Tuning Network. (arXiv:2112.15362v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salimibeni_M/0/1/0/all/0/1\">Mohammad Salimibeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_A/0/1/0/all/0/1\">Arash Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malekzadeh_P/0/1/0/all/0/1\">Parvin Malekzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>",
          "description": "Distributed Multi-Agent Reinforcement Learning (MARL) algorithms has\nattracted a surge of interest lately mainly due to the recent advancements of\nDeep Neural Networks (DNNs). Conventional Model-Based (MB) or Model-Free (MF)\nRL algorithms are not directly applicable to the MARL problems due to\nutilization of a fixed reward model for learning the underlying value function.\nWhile DNN-based solutions perform utterly well when a single agent is involved,\nsuch methods fail to fully generalize to the complexities of MARL problems. In\nother words, although recently developed approaches based on DNNs for\nmulti-agent environments have achieved superior performance, they are still\nprone to overfiting, high sensitivity to parameter selection, and sample\ninefficiency. The paper proposes the Multi-Agent Adaptive Kalman Temporal\nDifference (MAK-TD) framework and its Successor Representation-based variant,\nreferred to as the MAK-SR. Intuitively speaking, the main objective is to\ncapitalize on unique characteristics of Kalman Filtering (KF) such as\nuncertainty modeling and online second order learning. The proposed MAK-TD/SR\nframeworks consider the continuous nature of the action-space that is\nassociated with high dimensional multi-agent environments and exploit Kalman\nTemporal Difference (KTD) to address the parameter uncertainty. By leveraging\nthe KTD framework, SR learning procedure is modeled into a filtering problem,\nwhere Radial Basis Function (RBF) estimators are used to encode the continuous\nspace into feature vectors. On the other hand, for learning localized reward\nfunctions, we resort to Multiple Model Adaptive Estimation (MMAE), to deal with\nthe lack of prior knowledge on the observation noise covariance and observation\nmapping function. The proposed MAK-TD/SR frameworks are evaluated via several\nexperiments, which are implemented through the OpenAI Gym MARL benchmarks.",
          "link": "http://arxiv.org/abs/2112.15156",
          "publishedOn": "2022-01-03T07:15:43.722Z",
          "wordCount": null,
          "title": "Multi-Agent Reinforcement Learning via Adaptive Kalman Temporal Difference and Successor Representation. (arXiv:2112.15156v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Achab_M/0/1/0/all/0/1\">Mastane Achab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neu_G/0/1/0/all/0/1\">Gergely Neu</a>",
          "description": "In dynamic programming (DP) and reinforcement learning (RL), an agent learns\nto act optimally in terms of expected long-term return by sequentially\ninteracting with its environment modeled by a Markov decision process (MDP).\nMore generally in distributional reinforcement learning (DRL), the focus is on\nthe whole distribution of the return, not just its expectation. Although\nDRL-based methods produced state-of-the-art performance in RL with function\napproximation, they involve additional quantities (compared to the\nnon-distributional setting) that are still not well understood. As a first\ncontribution, we introduce a new class of distributional operators, together\nwith a practical DP algorithm for policy evaluation, that come with a robust\nMDP interpretation. Indeed, our approach reformulates through an augmented\nstate space where each state is split into a worst-case substate and a\nbest-case substate, whose values are maximized by safe and risky policies\nrespectively. Finally, we derive distributional operators and DP algorithms\nsolving a new control task: How to distinguish safe from risky optimal actions\nin order to break ties in the space of optimal policies?",
          "link": "http://arxiv.org/abs/2112.15430",
          "publishedOn": "2022-01-03T07:15:43.721Z",
          "wordCount": null,
          "title": "Robustness and risk management via distributional dynamic programming. (arXiv:2112.15430v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tjoa_E/0/1/0/all/0/1\">Erico Tjoa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuntai_G/0/1/0/all/0/1\">Guan Cuntai</a>",
          "description": "Transparency and fairness issues in Deep Reinforcement Learning may stem from\nthe black-box nature of deep neural networks used to learn its policy, value\nfunctions etc. This paper proposes a way to circumvent the issues through the\nbottom-up design of neural networks (NN) with detailed interpretability, where\neach neuron or layer has its own meaning and utility that corresponds to\nhumanly understandable concept. With deliberate design, we show that lavaland\nproblems can be solved using NN model with few parameters. Furthermore, we\nintroduce the Self Reward Design (SRD), inspired by the Inverse Reward Design,\nso that our interpretable design can (1) solve the problem by pure design\n(although imperfectly) (2) be optimized via SRD (3) perform avoidance of\nunknown states by recognizing the inactivations of neurons aggregated as the\nactivation in \\(w_{unknown}\\).",
          "link": "http://arxiv.org/abs/2112.15034",
          "publishedOn": "2022-01-03T07:15:43.719Z",
          "wordCount": null,
          "title": "Self Reward Design with Fine-grained Interpretability. (arXiv:2112.15034v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15287",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_X/0/1/0/all/0/1\">Xiao Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Milzarek_A/0/1/0/all/0/1\">Andre Milzarek</a>, <a href=\"http://arxiv.org/find/math/1/au:+Pu_S/0/1/0/all/0/1\">Shi Pu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Qiu_J/0/1/0/all/0/1\">Junwen Qiu</a>",
          "description": "In this paper, we consider the distributed optimization problem where $n$\nagents, each possessing a local cost function, collaboratively minimize the\naverage of the local cost functions over a connected network. To solve the\nproblem, we propose a distributed random reshuffling (D-RR) algorithm that\ncombines the classical distributed gradient descent (DGD) method and Random\nReshuffling (RR). We show that D-RR inherits the superiority of RR for both\nsmooth strongly convex and smooth nonconvex objective functions. In particular,\nfor smooth strongly convex objective functions, D-RR achieves\n$\\mathcal{O}(1/T^2)$ rate of convergence (here, $T$ counts the total number of\niterations) in terms of the squared distance between the iterate and the unique\nminimizer. When the objective function is assumed to be smooth nonconvex and\nhas Lipschitz continuous component functions, we show that D-RR drives the\nsquared norm of gradient to $0$ at a rate of $\\mathcal{O}(1/T^{2/3})$. These\nconvergence results match those of centralized RR (up to constant factors).",
          "link": "http://arxiv.org/abs/2112.15287",
          "publishedOn": "2022-01-03T07:15:43.714Z",
          "wordCount": null,
          "title": "Distributed Random Reshuffling over Networks. (arXiv:2112.15287v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Ruoming Jin</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_G/0/1/0/all/0/1\">Gagan Agrawal</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Ramnath_R/0/1/0/all/0/1\">Rajiv Ramnath</a> (4) ((1) Ohio State University, (2) Kent State University, (3) Augusta University)",
          "description": "Web-based interactions can be frequently represented by an attributed graph,\nand node clustering in such graphs has received much attention lately. Multiple\nefforts have successfully applied Graph Convolutional Networks (GCN), though\nwith some limits on accuracy as GCNs have been shown to suffer from\nover-smoothing issues. Though other methods (particularly those based on\nLaplacian Smoothing) have reported better accuracy, a fundamental limitation of\nall the work is a lack of scalability. This paper addresses this open problem\nby relating the Laplacian smoothing to the Generalized PageRank and applying a\nrandom-walk based algorithm as a scalable graph filter. This forms the basis\nfor our scalable deep clustering algorithm, RwSL, where through a\nself-supervised mini-batch training mechanism, we simultaneously optimize a\ndeep neural network for sample-cluster assignment distribution and an\nautoencoder for a clustering-oriented embedding. Using 6 real-world datasets\nand 6 clustering metrics, we show that RwSL achieved improved results over\nseveral recent baselines. Most notably, we show that RwSL, unlike all other\ndeep clustering frameworks, can continue to scale beyond graphs with more than\none million nodes, i.e., handle web-scale. We also demonstrate how RwSL could\nperform node clustering on a graph with 1.8 billion edges using only a single\nGPU.",
          "link": "http://arxiv.org/abs/2112.15530",
          "publishedOn": "2022-01-03T07:15:43.710Z",
          "wordCount": null,
          "title": "Scalable Deep Graph Clustering with Random-walk based Self-supervised Learning. (arXiv:2112.15530v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rachkovskij_D/0/1/0/all/0/1\">Dmitri A. Rachkovskij</a>",
          "description": "Hyperdimensional Computing (HDC), also known as Vector-Symbolic Architectures\n(VSA), is a promising framework for the development of cognitive architectures\nand artificial intelligence systems, as well as for technical applications and\nemerging neuromorphic and nanoscale hardware. HDC/VSA operate with\nhypervectors, i.e., distributed vector representations of large fixed dimension\n(usually > 1000). One of the key ingredients of HDC/VSA are the methods for\nencoding data of various types (from numeric scalars and vectors to graphs)\ninto hypervectors. In this paper, we propose an approach for the formation of\nhypervectors of sequences that provides both an equivariance with respect to\nthe shift of sequences and preserves the similarity of sequences with identical\nelements at nearby positions. Our methods represent the sequence elements by\ncompositional hypervectors and exploit permutations of hypervectors for\nrepresenting the order of sequence elements. We experimentally explored the\nproposed representations using a diverse set of tasks with data in the form of\nsymbolic strings. Although our approach is feature-free as it forms the\nhypervector of a sequence from the hypervectors of its symbols at their\npositions, it demonstrated the performance on a par with the methods that apply\nvarious features, such as subsequences. The proposed techniques were designed\nfor the HDC/VSA model known as Sparse Binary Distributed Representations.\nHowever, they can be adapted to hypervectors in formats of other HDC/VSA\nmodels, as well as for representing sequences of types other than symbolic\nstrings.",
          "link": "http://arxiv.org/abs/2112.15475",
          "publishedOn": "2022-01-03T07:15:43.703Z",
          "wordCount": null,
          "title": "Shift-Equivariant Similarity-Preserving Hypervector Representations of Sequences. (arXiv:2112.15475v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xi_D/0/1/0/all/0/1\">Dongbo Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jingjing Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>",
          "description": "Human mobility data accumulated from Point-of-Interest (POI) check-ins\nprovides great opportunity for user behavior understanding. However, data\nquality issues (e.g., geolocation information missing, unreal check-ins, data\nsparsity) in real-life mobility data limit the effectiveness of existing\nPOI-oriented studies, e.g., POI recommendation and location prediction, when\napplied to real applications. To this end, in this paper, we develop a model,\nnamed Bi-STDDP, which can integrate bi-directional spatio-temporal dependence\nand users' dynamic preferences, to identify the missing POI check-in where a\nuser has visited at a specific time. Specifically, we first utilize\nbi-directional global spatial and local temporal information of POIs to capture\nthe complex dependence relationships. Then, target temporal pattern in\ncombination with user and POI information are fed into a multi-layer network to\ncapture users' dynamic preferences. Moreover, the dynamic preferences are\ntransformed into the same space as the dependence relationships to form the\nfinal model. Finally, the proposed model is evaluated on three large-scale\nreal-world datasets and the results demonstrate significant improvements of our\nmodel compared with state-of-the-art methods. Also, it is worth noting that the\nproposed model can be naturally extended to address POI recommendation and\nlocation prediction tasks with competitive performances.",
          "link": "http://arxiv.org/abs/2112.15285",
          "publishedOn": "2022-01-03T07:15:43.686Z",
          "wordCount": null,
          "title": "Modelling of Bi-directional Spatio-Temporal Dependence and Users' Dynamic Preferences for Missing POI Check-in Identification. (arXiv:2112.15285v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+P_A/0/1/0/all/0/1\">Abirami S P</a>, <a href=\"http://arxiv.org/find/cs/1/au:+G_K/0/1/0/all/0/1\">Kousalya G</a>, <a href=\"http://arxiv.org/find/cs/1/au:+R_K/0/1/0/all/0/1\">Karthick R</a>",
          "description": "Autism Spectrum Disorder (ASD) is found to be a major concern among various\noccupational therapists. The foremost challenge of this neurodevelopmental\ndisorder lies in the fact of analyzing and exploring various symptoms of the\nchildren at their early stage of development. Such early identification could\nprop up the therapists and clinicians to provide proper assistive support to\nmake the children lead an independent life. Facial expressions and emotions\nperceived by the children could contribute to such early intervention of\nautism. In this regard, the paper implements in identifying basic facial\nexpression and exploring their emotions upon a time variant factor. The\nemotions are analyzed by incorporating the facial expression identified through\nCNN using 68 landmark points plotted on the frontal face with a prediction\nnetwork formed by RNN known as RCNN-FER system. The paper adopts R-CNN to take\nthe advantage of increased accuracy and performance with decreased time\ncomplexity in predicting emotion as a textual network analysis. The papers\nproves better accuracy in identifying the emotion in autistic children when\ncompared over simple machine learning models built for such identifications\ncontributing to autistic society.",
          "link": "http://arxiv.org/abs/2112.14983",
          "publishedOn": "2022-01-03T07:15:43.677Z",
          "wordCount": null,
          "title": "Exploring the pattern of Emotion in children with ASD as an early biomarker through Recurring-Convolution Neural Network (R-CNN). (arXiv:2112.14983v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.01041",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zhang_K/0/1/0/all/0/1\">Kaiqing Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyuan Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hu_B/0/1/0/all/0/1\">Bin Hu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Basar_T/0/1/0/all/0/1\">Tamer Ba&#x15f;ar</a>",
          "description": "Direct policy search serves as one of the workhorses in modern reinforcement\nlearning (RL), and its applications in continuous control tasks have recently\nattracted increasing attention. In this work, we investigate the convergence\ntheory of policy gradient (PG) methods for learning the linear risk-sensitive\nand robust controller. In particular, we develop PG methods that can be\nimplemented in a derivative-free fashion by sampling system trajectories, and\nestablish both global convergence and sample complexity results in the\nsolutions of two fundamental settings in risk-sensitive and robust control: the\nfinite-horizon linear exponential quadratic Gaussian, and the finite-horizon\nlinear-quadratic disturbance attenuation problems. As a by-product, our results\nalso provide the first sample complexity for the global convergence of PG\nmethods on solving zero-sum linear-quadratic dynamic games, a\nnonconvex-nonconcave minimax optimization problem that serves as a baseline\nsetting in multi-agent reinforcement learning (MARL) with continuous spaces.\nOne feature of our algorithms is that during the learning phase, a certain\nlevel of robustness/risk-sensitivity of the controller is preserved, which we\ntermed as the implicit regularization property, and is an essential requirement\nin safety-critical control systems.",
          "link": "http://arxiv.org/abs/2101.01041",
          "publishedOn": "2022-01-03T07:15:43.675Z",
          "wordCount": null,
          "title": "Derivative-Free Policy Optimization for Linear Risk-Sensitive and Robust Control Design: Implicit Regularization and Sample Complexity. (arXiv:2101.01041v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1\">Yue Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isac_A/0/1/0/all/0/1\">Alka Isac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yimin Nie</a>",
          "description": "The analysis of long sequence data remains challenging in many real-world\napplications. We propose a novel architecture, ChunkFormer, that improves the\nexisting Transformer framework to handle the challenges while dealing with long\ntime series. Original Transformer-based models adopt an attention mechanism to\ndiscover global information along a sequence to leverage the contextual data.\nLong sequential data traps local information such as seasonality and\nfluctuations in short data sequences. In addition, the original Transformer\nconsumes more resources by carrying the entire attention matrix during the\ntraining course. To overcome these challenges, ChunkFormer splits the long\nsequences into smaller sequence chunks for the attention calculation,\nprogressively applying different chunk sizes in each stage. In this way, the\nproposed model gradually learns both local and global information without\nchanging the total length of the input sequences. We have extensively tested\nthe effectiveness of this new architecture on different business domains and\nhave proved the advantage of such a model over the existing Transformer-based\nmodels.",
          "link": "http://arxiv.org/abs/2112.15087",
          "publishedOn": "2022-01-03T07:15:43.673Z",
          "wordCount": null,
          "title": "ChunkFormer: Learning Long Time Series with Multi-stage Chunked Transformer. (arXiv:2112.15087v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.01711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chengchun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shikai Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongtu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jieping Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>",
          "description": "A/B testing, or online experiment is a standard business strategy to compare\na new product with an old one in pharmaceutical, technological, and traditional\nindustries. Major challenges arise in online experiments of two-sided\nmarketplace platforms (e.g., Uber) where there is only one unit that receives a\nsequence of treatments over time. In those experiments, the treatment at a\ngiven time impacts current outcome as well as future outcomes. The aim of this\npaper is to introduce a reinforcement learning framework for carrying A/B\ntesting in these experiments, while characterizing the long-term treatment\neffects. Our proposed testing procedure allows for sequential monitoring and\nonline updating. It is generally applicable to a variety of treatment designs\nin different industries. In addition, we systematically investigate the\ntheoretical properties (e.g., size and power) of our testing procedure.\nFinally, we apply our framework to both simulated data and a real-world data\nexample obtained from a technological company to illustrate its advantage over\nthe current practice. A Python implementation of our test is available at\nhttps://github.com/callmespring/CausalRL.",
          "link": "http://arxiv.org/abs/2002.01711",
          "publishedOn": "2022-01-03T07:15:43.672Z",
          "wordCount": null,
          "title": "Dynamic Causal Effects Evaluation in A/B Testing with a Reinforcement Learning Framework. (arXiv:2002.01711v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuanying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhining Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Li Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1\">Lihong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiaodong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yize Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjie Gu</a>",
          "description": "Many payment platforms hold large-scale marketing campaigns, which allocate\nincentives to encourage users to pay through their applications. To maximize\nthe return on investment, incentive allocations are commonly solved in a\ntwo-stage procedure. After training a response estimation model to estimate the\nusers' mobile payment probabilities (MPP), a linear programming process is\napplied to obtain the optimal incentive allocation. However, the large amount\nof biased data in the training set, generated by the previous biased allocation\npolicy, causes a biased estimation. This bias deteriorates the performance of\nthe response model and misleads the linear programming process, dramatically\ndegrading the performance of the resulting allocation policy. To overcome this\nobstacle, we propose a bias correction adversarial network. Our method\nleverages the small set of unbiased data obtained under a full-randomized\nallocation policy to train an unbiased model and then uses it to reduce the\nbias with adversarial learning. Offline and online experimental results\ndemonstrate that our method outperforms state-of-the-art approaches and\nsignificantly improves the performance of the resulting allocation policy in a\nreal-world marketing campaign.",
          "link": "http://arxiv.org/abs/2112.15434",
          "publishedOn": "2022-01-03T07:15:43.671Z",
          "wordCount": null,
          "title": "Adversarial Learning for Incentive Optimization in Mobile Payment Marketing. (arXiv:2112.15434v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1\">Jorge F. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tobar_F/0/1/0/all/0/1\">Felipe Tobar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vicuna_M/0/1/0/all/0/1\">Mario Vicu&#xf1;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordova_F/0/1/0/all/0/1\">Felipe Cordova</a>",
          "description": "Information-theoretic measures have been widely adopted in the design of\nfeatures for learning and decision problems. Inspired by this, we look at the\nrelationship between i) a weak form of information loss in the Shannon sense\nand ii) the operation loss in the minimum probability of error (MPE) sense when\nconsidering a family of lossy continuous representations (features) of a\ncontinuous observation. We present several results that shed light on this\ninterplay. Our first result offers a lower bound on a weak form of information\nloss as a function of its respective operation loss when adopting a discrete\nlossy representation (quantization) instead of the original raw observation.\nFrom this, our main result shows that a specific form of vanishing information\nloss (a weak notion of asymptotic informational sufficiency) implies a\nvanishing MPE loss (or asymptotic operational sufficiency) when considering a\ngeneral family of lossy continuous representations. Our theoretical findings\nsupport the observation that the selection of feature representations that\nattempt to capture informational sufficiency is appropriate for learning, but\nthis selection is a rather conservative design principle if the intended goal\nis achieving MPE in classification. Supporting this last point, and under some\nstructural conditions, we show that it is possible to adopt an alternative\nnotion of informational sufficiency (strictly weaker than pure sufficiency in\nthe mutual information sense) to achieve operational sufficiency in learning.",
          "link": "http://arxiv.org/abs/2112.15238",
          "publishedOn": "2022-01-03T07:15:43.635Z",
          "wordCount": null,
          "title": "Studying the Interplay between Information Loss and Operation Loss in Representations for Classification. (arXiv:2112.15238v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14826",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Diab_W/0/1/0/all/0/1\">Waleed Diab</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kobaisi_M/0/1/0/all/0/1\">Mohammed Al Kobaisi</a>",
          "description": "The displacement of two immiscible fluids is a common problem in fluid flow\nin porous media. Such a problem can be posed as a partial differential equation\n(PDE) in what is commonly referred to as a Buckley-Leverett (B-L) problem. The\nB-L problem is a non-linear hyperbolic conservation law that is known to be\nnotoriously difficult to solve using traditional numerical methods. Here, we\naddress the forward hyperbolic B-L problem with a nonconvex flux function using\nphysics-informed neural networks (PINNs). The contributions of this paper are\ntwofold. First, we present a PINN approach to solve the hyperbolic B-L problem\nby embedding the Oleinik entropy condition into the neural network residual. We\ndo not use a diffusion term (artificial viscosity) in the residual-loss, but we\nrely on the strong form of the PDE. Second, we use the Adam optimizer with\nresidual-based adaptive refinement (RAR) algorithm to achieve an ultra-low loss\nwithout weighting. Our solution method can accurately capture the shock-front\nand produce an accurate overall solution. We report a L2 validation error of 2\nx 10-2 and a L2 loss of 1x 10-6. The proposed method does not require any\nadditional regularization or weighting of losses to obtain such accurate\nsolution.",
          "link": "http://arxiv.org/abs/2112.14826",
          "publishedOn": "2022-01-03T07:15:43.634Z",
          "wordCount": null,
          "title": "PINNs for the Solution of the Hyperbolic Buckley-Leverett Problem with a Non-convex Flux Function. (arXiv:2112.14826v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1\">Farley Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadav_A/0/1/0/all/0/1\">Asim Kadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruus_E/0/1/0/all/0/1\">Erik Kruus</a>",
          "description": "The recent success of deep learning applications has coincided with those\nwidely available powerful computational resources for training sophisticated\nmachine learning models with huge datasets. Nonetheless, training large models\nsuch as convolutional neural networks using model parallelism (as opposed to\ndata parallelism) is challenging because the complex nature of communication\nbetween model shards makes it difficult to partition the computation\nefficiently across multiple machines with an acceptable trade-off. This paper\npresents SplitBrain, a high performance distributed deep learning framework\nsupporting hybrid data and model parallelism. Specifically, SplitBrain provides\nlayer-specific partitioning that co-locates compute intensive convolutional\nlayers while sharding memory demanding layers. A novel scalable group\ncommunication is proposed to further improve the training throughput with\nreduced communication overhead. The results show that SplitBrain can achieve\nnearly linear speedup while saving up to 67\\% of memory consumption for data\nand model parallel VGG over CIFAR-10.",
          "link": "http://arxiv.org/abs/2112.15317",
          "publishedOn": "2022-01-03T07:15:43.634Z",
          "wordCount": null,
          "title": "SplitBrain: Hybrid Data and Model Parallel Deep Learning. (arXiv:2112.15317v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1\">Yongduo Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiancan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>",
          "description": "Learning powerful representations is one central theme of graph neural\nnetworks (GNNs). It requires refining the critical information from the input\ngraph, instead of the trivial patterns, to enrich the representations. Towards\nthis end, graph attention and pooling methods prevail. They mostly follow the\nparadigm of \"learning to attend\". It maximizes the mutual information between\nthe attended subgraph and the ground-truth label. However, this training\nparadigm is prone to capture the spurious correlations between the trivial\nsubgraph and the label. Such spurious correlations are beneficial to\nin-distribution (ID) test evaluations, but cause poor generalization in the\nout-of-distribution (OOD) test data. In this work, we revisit the GNN modeling\nfrom the causal perspective. On the top of our causal assumption, the trivial\ninformation serves as a confounder between the critical information and the\nlabel, which opens a backdoor path between them and makes them spuriously\ncorrelated. Hence, we present a new paradigm of deconfounded training (DTP)\nthat better mitigates the confounding effect and latches on the critical\ninformation, to enhance the representation and generalization ability.\nSpecifically, we adopt the attention modules to disentangle the critical\nsubgraph and trivial subgraph. Then we make each critical subgraph fairly\ninteract with diverse trivial subgraphs to achieve a stable prediction. It\nallows GNNs to capture a more reliable subgraph whose relation with the label\nis robust across different distributions. We conduct extensive experiments on\nsynthetic and real-world datasets to demonstrate the effectiveness.",
          "link": "http://arxiv.org/abs/2112.15089",
          "publishedOn": "2022-01-03T07:15:43.631Z",
          "wordCount": null,
          "title": "Deconfounded Training for Graph Neural Networks. (arXiv:2112.15089v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.01806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>",
          "description": "Dance choreography for a piece of music is a challenging task, having to be\ncreative in presenting distinctive stylistic dance elements while taking into\naccount the musical theme and rhythm. It has been tackled by different\napproaches such as similarity retrieval, sequence-to-sequence modeling and\ngenerative adversarial networks, but their generated dance sequences are often\nshort of motion realism, diversity and music consistency. In this paper, we\npropose a Music-to-Dance with Optimal Transport Network (MDOT-Net) for learning\nto generate 3D dance choreographs from music. We introduce an optimal transport\ndistance for evaluating the authenticity of the generated dance distribution\nand a Gromov-Wasserstein distance to measure the correspondence between the\ndance distribution and the input music. This gives a well defined and\nnon-divergent training objective that mitigates the limitation of standard GAN\ntraining which is frequently plagued with instability and divergent generator\nloss issues. Extensive experiments demonstrate that our MDOT-Net can synthesize\nrealistic and diverse dances which achieve an organic unity with the input\nmusic, reflecting the shared intentionality and matching the rhythmic\narticulation.",
          "link": "http://arxiv.org/abs/2112.01806",
          "publishedOn": "2022-01-03T07:15:43.629Z",
          "wordCount": null,
          "title": "Music-to-Dance Generation with Optimal Transport. (arXiv:2112.01806v1 [cs.SD] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Saidur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1\">Foutse Khomh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidi_A/0/1/0/all/0/1\">Alaleh Hamidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jinghui Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antoniol_G/0/1/0/all/0/1\">Giuliano Antoniol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washizaki_H/0/1/0/all/0/1\">Hironori Washizaki</a>",
          "description": "Nowadays, intelligent systems and services are getting increasingly popular\nas they provide data-driven solutions to diverse real-world problems, thanks to\nrecent breakthroughs in Artificial Intelligence (AI) and Machine Learning (ML).\nHowever, machine learning meets software engineering not only with promising\npotentials but also with some inherent challenges. Despite some recent research\nefforts, we still do not have a clear understanding of the challenges of\ndeveloping ML-based applications and the current industry practices. Moreover,\nit is unclear where software engineering researchers should focus their efforts\nto better support ML application developers. In this paper, we report about a\nsurvey that aimed to understand the challenges and best practices of ML\napplication development. We synthesize the results obtained from 80\npractitioners (with diverse skills, experience, and application domains) into\n17 findings; outlining challenges and best practices for ML application\ndevelopment. Practitioners involved in the development of ML-based software\nsystems can leverage the summarized best practices to improve the quality of\ntheir system. We hope that the reported challenges will inform the research\ncommunity about topics that need to be investigated to improve the engineering\nprocess and the quality of ML-based applications.",
          "link": "http://arxiv.org/abs/2112.15277",
          "publishedOn": "2022-01-03T07:15:43.625Z",
          "wordCount": null,
          "title": "Machine Learning Application Development: Practitioners' Insights. (arXiv:2112.15277v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dixian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianbao Yang</a>",
          "description": "Multi-class classification is one of the most common tasks in machine\nlearning applications, where data is labeled by one of many class labels. Many\nloss functions have been proposed for multi-class classification including two\nwell-known ones, namely the cross-entropy (CE) loss and the crammer-singer (CS)\nloss (aka. the SVM loss). While CS loss has been used widely for traditional\nmachine learning tasks, CE loss is usually a default choice for multi-class\ndeep learning tasks. There are also top-$k$ variants of CS loss and CE loss\nthat are proposed to promote the learning of a classifier for achieving better\ntop-$k$ accuracy. Nevertheless, it still remains unclear the relationship\nbetween these different losses, which hinders our understanding of their\nexpectations in different scenarios. In this paper, we present a unified view\nof the CS/CE losses and their smoothed top-$k$ variants by proposing a new\nfamily of loss functions, which are arguably better than the CS/CE losses when\nthe given label information is incomplete and noisy. The new family of smooth\nloss functions named {label-distributionally robust (LDR) loss} is defined by\nleveraging the distributionally robust optimization (DRO) framework to model\nthe uncertainty in the given label information, where the uncertainty over true\nclass labels is captured by using distributional weights for each label\nregularized by a function.",
          "link": "http://arxiv.org/abs/2112.14869",
          "publishedOn": "2022-01-03T07:15:43.624Z",
          "wordCount": null,
          "title": "A Unified DRO View of Multi-class Loss Functions with top-N Consistency. (arXiv:2112.14869v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lehmler_S/0/1/0/all/0/1\">Stephan Johann Lehmler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saif_ur_Rehman_M/0/1/0/all/0/1\">Muhammad Saif-ur-Rehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glasmachers_T/0/1/0/all/0/1\">Tobias Glasmachers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iossifidis_I/0/1/0/all/0/1\">Ioannis Iossifidis</a>",
          "description": "Accurate decoding of surface electromyography (sEMG) is pivotal for\nmuscle-to-machine-interfaces (MMI) and their application for e.g.\nrehabilitation therapy. sEMG signals have high inter-subject variability, due\nto various factors, including skin thickness, body fat percentage, and\nelectrode placement. Therefore, obtaining high generalization quality of a\ntrained sEMG decoder is quite challenging. Usually, machine learning based sEMG\ndecoders are either trained on subject-specific data, or at least recalibrated\nfor each user, individually. Even though, deep learning algorithms produced\nseveral state of the art results for sEMG decoding,however, due to the limited\namount of availability of sEMG data, the deep learning models are prone to\noverfitting. Recently, transfer learning for domain adaptation improved\ngeneralization quality with reduced training time on various machine learning\ntasks. In this study, we investigate the effectiveness of transfer learning\nusing weight initialization for recalibration of two different pretrained deep\nlearning models on a new subjects data, and compare their performance to\nsubject-specific models. To the best of our knowledge, this is the first study\nthat thoroughly investigated weight-initialization based transfer learning for\nsEMG classification and compared transfer learning to subject-specific\nmodeling. We tested our models on three publicly available databases under\nvarious settings. On average over all settings, our transfer learning approach\nimproves 5~\\%-points on the pretrained models without fine-tuning and\n12~\\%-points on the subject-specific models, while being trained on average\n22~\\% fewer epochs. Our results indicate that transfer learning enables faster\ntraining on fewer samples than user-specific models, and improves the\nperformance of pretrained models as long as enough data is available.",
          "link": "http://arxiv.org/abs/2112.15019",
          "publishedOn": "2022-01-03T07:15:43.621Z",
          "wordCount": null,
          "title": "Deep Transfer-Learning for patient specific model re-calibration: Application to sEMG-Classification. (arXiv:2112.15019v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xi_D/0/1/0/all/0/1\">Dongbo Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1\">Bowen Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yongchun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Dan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>",
          "description": "Many prediction tasks of real-world applications need to model multi-order\nfeature interactions in user's event sequence for better detection performance.\nHowever, existing popular solutions usually suffer two key issues: 1) only\nfocusing on feature interactions and failing to capture the sequence influence;\n2) only focusing on sequence information, but ignoring internal feature\nrelations of each event, thus failing to extract a better event representation.\nIn this paper, we consider a two-level structure for capturing the hierarchical\ninformation over user's event sequence: 1) learning effective feature\ninteractions based event representation; 2) modeling the sequence\nrepresentation of user's historical events. Experimental results on both\nindustrial and public datasets clearly demonstrate that our model achieves\nsignificantly better performance compared with state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2112.15292",
          "publishedOn": "2022-01-03T07:15:43.620Z",
          "wordCount": null,
          "title": "Neural Hierarchical Factorization Machines for User's Event Sequence Analysis. (arXiv:2112.15292v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2110.15557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianfu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jie Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yexin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu Zheng</a>",
          "description": "Illegal vehicle parking is a common urban problem faced by major cities in\nthe world, as it incurs traffic jams, which lead to air pollution and traffic\naccidents. The government highly relies on active human efforts to detect\nillegal parking events. However, such an approach is extremely ineffective to\ncover a large city since the police have to patrol over the entire city roads.\n\nThe massive and high-quality sharing bike trajectories from Mobike offer us a\nunique opportunity to design a ubiquitous illegal parking detection approach,\nas most of the illegal parking events happen at curbsides and have significant\nimpact on the bike users. The detection result can guide the patrol schedule,\ni.e. send the patrol policemen to the region with higher illegal parking risks,\nand further improve the patrol efficiency. Inspired by this idea, three main\ncomponents are employed in the proposed framework: 1)~{\\em trajectory\npre-processing}, which filters outlier GPS points, performs map-matching, and\nbuilds trajectory indexes; 2)~{\\em illegal parking detection}, which models the\nnormal trajectories, extracts features from the evaluation trajectories, and\nutilizes a distribution test-based method to discover the illegal parking\nevents; and 3)~{\\em patrol scheduling}, which leverages the detection result as\nreference context, and models the scheduling task as a multi-agent\nreinforcement learning problem to guide the patrol police. Finally, extensive\nexperiments are presented to validate the effectiveness of illegal parking\ndetection, as well as the improvement of patrol efficiency.",
          "link": "http://arxiv.org/abs/2110.15557",
          "publishedOn": "2022-01-03T07:15:43.620Z",
          "wordCount": null,
          "title": "Crowd-sensing Enhanced Parking Patrol using Trajectories of Sharing Bikes. (arXiv:2110.15557v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>",
          "description": "We introduce several new datasets namely ImageNet-A/O and ImageNet-R as well\nas a synthetic environment and testing suite we called CAOS. ImageNet-A/O allow\nresearchers to focus in on the blind spots remaining in ImageNet. ImageNet-R\nwas specifically created with the intention of tracking robust representation\nas the representations are no longer simply natural but include artistic, and\nother renditions. The CAOS suite is built off of CARLA simulator which allows\nfor the inclusion of anomalous objects and can create reproducible synthetic\nenvironment and scenes for testing robustness. All of the datasets were created\nfor testing robustness and measuring progress in robustness. The datasets have\nbeen used in various other works to measure their own progress in robustness\nand allowing for tangential progress that does not focus exclusively on natural\naccuracy.\n\nGiven these datasets, we created several novel methods that aim to advance\nrobustness research. We build off of simple baselines in the form of Maximum\nLogit, and Typicality Score as well as create a novel data augmentation method\nin the form of DeepAugment that improves on the aforementioned benchmarks.\nMaximum Logit considers the logit values instead of the values after the\nsoftmax operation, while a small change produces noticeable improvements. The\nTypicality Score compares the output distribution to a posterior distribution\nover classes. We show that this improves performance over the baseline in all\nbut the segmentation task. Speculating that perhaps at the pixel level the\nsemantic information of a pixel is less meaningful than that of class level\ninformation. Finally the new augmentation technique of DeepAugment utilizes\nneural networks to create augmentations on images that are radically different\nthan the traditional geometric and camera based transformations used\npreviously.",
          "link": "http://arxiv.org/abs/2112.15188",
          "publishedOn": "2022-01-03T07:15:43.613Z",
          "wordCount": null,
          "title": "Towards Robustness of Neural Networks. (arXiv:2112.15188v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1906.09338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yunhui Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuolin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunter_C/0/1/0/all/0/1\">Carl A. Gunter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Recent advances in machine learning have largely benefited from the massive\naccessible training data. However, large-scale data sharing has raised great\nprivacy concerns. In this work, we propose a novel privacy-preserving data\nGenerative model based on the PATE framework (G-PATE), aiming to train a\nscalable differentially private data generator that preserves high generated\ndata utility. Our approach leverages generative adversarial nets to generate\ndata, combined with private aggregation among different discriminators to\nensure strong privacy guarantees. Compared to existing approaches, G-PATE\nsignificantly improves the use of privacy budgets. In particular, we train a\nstudent data generator with an ensemble of teacher discriminators and propose a\nnovel private gradient aggregation mechanism to ensure differential privacy on\nall information that flows from teacher discriminators to the student\ngenerator. In addition, with random projection and gradient discretization, the\nproposed gradient aggregation mechanism is able to effectively deal with\nhigh-dimensional gradient vectors. Theoretically, we prove that G-PATE ensures\ndifferential privacy for the data generator. Empirically, we demonstrate the\nsuperiority of G-PATE over prior work through extensive experiments. We show\nthat G-PATE is the first work being able to generate high-dimensional image\ndata with high data utility under limited privacy budgets ($\\epsilon \\le 1$).\nOur code is available at https://github.com/AI-secure/G-PATE.",
          "link": "http://arxiv.org/abs/1906.09338",
          "publishedOn": "2022-01-03T07:15:43.590Z",
          "wordCount": null,
          "title": "G-PATE: Scalable Differentially Private Data Generator via Private Aggregation of Teacher Discriminators. (arXiv:1906.09338v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pietron_M/0/1/0/all/0/1\">Marcin Pietro&#x144;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zurek_D/0/1/0/all/0/1\">Dominik &#x17b;urek</a>",
          "description": "This work is focused on the pruning of some convolutional neural networks\n(CNNs) and improving theirs efficiency on graphic processing units (GPU) by\nusing a direct sparse algorithm. The Nvidia deep neural network (cuDnn) library\nis the most effective implementations of deep learning (DL) algorithms for\nGPUs. GPUs are the most commonly used accelerators for deep learning\ncomputations. One of the most common techniques for improving the efficiency of\nCNN models is weight pruning and quantization. There are two main types of\npruning: structural and non-structural. The first enables much easier\nacceleration on many type of accelerators, but with this type it is difficult\nto achieve a sparsity level and accuracy as high as that obtained with the\nsecond type. Non-structural pruning with retraining can generate a weight\ntensors up to 90% or more of sparsity in some deep CNN models. In this article\nthe pruning algorithm is presented which makes it possible to achieve high\nsparsity levels without accuracy drop. In the next stage the linear and\nnon-linear quantization is adapted for further time and footprint reduction.\nThis paper is an extended of previously published paper concerning effective\npruning techniques and present real models pruned with high sparsities and\nreduced precision which can achieve better performance than the CuDnn library.",
          "link": "http://arxiv.org/abs/2112.15445",
          "publishedOn": "2022-01-03T07:15:43.589Z",
          "wordCount": null,
          "title": "Speedup deep learning models on GPU by taking advantage of efficient unstructured pruning and bit-width reduction. (arXiv:2112.15445v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.10861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shunqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurkoski_B/0/1/0/all/0/1\">Brian M. Kurkoski</a>",
          "description": "Approximate message passing (AMP) is a low-cost iterative\nparameter-estimation technique for certain high-dimensional linear systems with\nnon-Gaussian distributions. However, AMP only applies to independent\nidentically distributed (IID) transform matrices, but may become unreliable\n(e.g., perform poorly or even diverge) for other matrix ensembles, especially\nfor ill-conditioned ones. Orthogonal/vector AMP (OAMP/VAMP) was proposed for\ngeneral right-unitarily-invariant matrices to handle this difficulty. However,\nthe Bayes-optimal OAMP/VAMP (BO-OAMP/VAMP) requires a high-complexity linear\nminimum mean square error (MMSE) estimator. This limits the application of\nOAMP/VAMP to large-scale systems.\n\nTo solve the disadvantages of AMP and BO-OAMP/VAMP, this paper proposes a\nmemory AMP (MAMP) framework under an orthogonality principle, which guarantees\nthe asymptotic IID Gaussianity of estimation errors in MAMP. We present an\northogonalization procedure for the local memory estimators to realize the\nrequired orthogonality for MAMP. Furthermore, we propose a Bayes-optimal MAMP\n(BO-MAMP), in which a long-memory matched filter is proposed for interference\nsuppression. The complexity of BO-MAMP is comparable to AMP. A state evolution\nis derived to asymptotically characterize the performance of BO-MAMP. Based on\nstate evolution, the relaxation parameters and damping vector in BO-MAMP are\noptimized. For all right-unitarily-invariant matrices, the state evolution of\nthe optimized BO-MAMP converges to the same fixed point as that of the\nhigh-complexity BO-OAMP/VAMP and is Bayes-optimal if its state evolution has a\nunique fixed point. Finally, simulations are provided to verify the validity\nand accuracy of the theoretical results.",
          "link": "http://arxiv.org/abs/2012.10861",
          "publishedOn": "2022-01-03T07:15:43.588Z",
          "wordCount": null,
          "title": "Memory AMP. (arXiv:2012.10861v5 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11612",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiafan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We study reinforcement learning (RL) with linear function approximation.\nExisting algorithms for this problem only have high-probability regret and/or\nProbably Approximately Correct (PAC) sample complexity guarantees, which cannot\nguarantee the convergence to the optimal policy. In this paper, in order to\novercome the limitation of existing algorithms, we propose a new algorithm\ncalled FLUTE, which enjoys uniform-PAC convergence to the optimal policy with\nhigh probability. The uniform-PAC guarantee is the strongest possible guarantee\nfor reinforcement learning in the literature, which can directly imply both PAC\nand high probability regret bounds, making our algorithm superior to all\nexisting algorithms with linear function approximation. At the core of our\nalgorithm is a novel minimax value function estimator and a multi-level\npartition scheme to select the training samples from historical observations.\nBoth of these techniques are new and of independent interest.",
          "link": "http://arxiv.org/abs/2106.11612",
          "publishedOn": "2022-01-03T07:15:43.561Z",
          "wordCount": null,
          "title": "Uniform-PAC Bounds for Reinforcement Learning with Linear Function Approximation. (arXiv:2106.11612v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.10523",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_D/0/1/0/all/0/1\">Ding Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pentland_A/0/1/0/all/0/1\">Alex Pentland</a>",
          "description": "Long ties, the social ties that bridge different communities, are widely\nbelieved to play crucial roles in spreading novel information in social\nnetworks. However, some existing network theories and prediction models\nindicate that long ties might dissolve quickly or eventually become redundant,\nthus putting into question the long-term value of long ties. Our empirical\nanalysis of real-world dynamic networks shows that contrary to such reasoning,\nlong ties are more likely to persist than other social ties, and that many of\nthem constantly function as social bridges without being embedded in local\nnetworks. Using a novel cost-benefit analysis model combined with machine\nlearning, we show that long ties are highly beneficial, which instinctively\nmotivates people to expend extra effort to maintain them. This partly explains\nwhy long ties are more persistent than what has been suggested by many existing\ntheories and models. Overall, our study suggests the need for social\ninterventions that can promote the formation of long ties, such as mixing\npeople with diverse backgrounds.",
          "link": "http://arxiv.org/abs/2109.10523",
          "publishedOn": "2022-01-03T07:15:43.561Z",
          "wordCount": null,
          "title": "Investigating and Modeling the Dynamics of Long Ties. (arXiv:2109.10523v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mingqi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pun_M/0/1/0/all/0/1\">Man-on Pun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi Chen</a>",
          "description": "In this work, we develop practical user scheduling algorithms for downlink\nbursty traffic with emphasis on user fairness. In contrast to the conventional\nscheduling algorithms that either equally divides the transmission time slots\namong users or maximizing some ratios without physcial meanings, we propose to\nuse the 5%-tile user data rate (5TUDR) as the metric to evaluate user fairness.\nSince it is difficult to directly optimize 5TUDR, we first cast the problem\ninto the stochastic game framework and subsequently propose a Multi-Agent\nReinforcement Learning (MARL)-based algorithm to perform distributed\noptimization on the resource block group (RBG) allocation. Furthermore, each\nMARL agent is designed to take information measured by network counters from\nmultiple network layers (e.g. Channel Quality Indicator, Buffer size) as the\ninput states while the RBG allocation as action with a proposed reward function\ndesigned to maximize 5TUDR. Extensive simulation is performed to show that the\nproposed MARL-based scheduler can achieve fair scheduling while maintaining\ngood average network throughput as compared to conventional schedulers.",
          "link": "http://arxiv.org/abs/2012.15081",
          "publishedOn": "2022-01-03T07:15:43.552Z",
          "wordCount": null,
          "title": "Fairness-Oriented User Scheduling for Bursty Downlink Transmission Using Multi-Agent Reinforcement Learning. (arXiv:2012.15081v13 [cs.OS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.14900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuexin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Ruichu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zijian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1\">Zhifeng Hao</a>",
          "description": "Graphs can model complicated interactions between entities, which naturally\nemerge in many important applications. These applications can often be cast\ninto standard graph learning tasks, in which a crucial step is to learn\nlow-dimensional graph representations. Graph neural networks (GNNs) are\ncurrently the most popular model in graph embedding approaches. However,\nstandard GNNs in the neighborhood aggregation paradigm suffer from limited\ndiscriminative power in distinguishing \\emph{high-order} graph structures as\nopposed to \\emph{low-order} structures. To capture high-order structures,\nresearchers have resorted to motifs and developed motif-based GNNs. However,\nexisting motif-based GNNs still often suffer from less discriminative power on\nhigh-order structures. To overcome the above limitations, we propose Motif\nGraph Neural Network (MGNN), a novel framework to better capture high-order\nstructures, hinging on our proposed motif redundancy minimization operator and\ninjective motif combination. First, MGNN produces a set of node representations\nw.r.t. each motif. The next phase is our proposed redundancy minimization among\nmotifs which compares the motifs with each other and distills the features\nunique to each motif. Finally, MGNN performs the updating of node\nrepresentations by combining multiple representations from different motifs. In\nparticular, to enhance the discriminative power, MGNN utilizes an injective\nfunction to combine the representations w.r.t. different motifs. We further\nshow that our proposed architecture increases the expressive power of GNNs with\na theoretical analysis. We demonstrate that MGNN outperforms state-of-the-art\nmethods on seven public benchmarks on both node classification and graph\nclassification tasks.",
          "link": "http://arxiv.org/abs/2112.14900",
          "publishedOn": "2022-01-03T07:15:43.545Z",
          "wordCount": null,
          "title": "Motif Graph Neural Network. (arXiv:2112.14900v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.13890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zhenglun Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1\">Peiyan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1\">Wei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mengshu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1\">Minghai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>",
          "description": "Recently, Vision Transformer (ViT) has continuously established new\nmilestones in the computer vision field, while the high computation and memory\ncost makes its propagation in industrial production difficult. Pruning, a\ntraditional model compression paradigm for hardware efficiency, has been widely\napplied in various DNN structures. Nevertheless, it stays ambiguous on how to\nperform exclusive pruning on the ViT structure. Considering three key points:\nthe structural characteristics, the internal data pattern of ViTs, and the\nrelated edge device deployment, we leverage the input token sparsity and\npropose a computation-aware soft pruning framework, which can be set up on\nvanilla Transformers of both flatten and CNN-type structures, such as\nPooling-based ViT (PiT). More concretely, we design a dynamic attention-based\nmulti-head token selector, which is a lightweight module for adaptive\ninstance-wise token selection. We further introduce a soft pruning technique,\nwhich integrates the less informative tokens generated by the selector module\ninto a package token that will participate in subsequent calculations rather\nthan being completely discarded. Our framework is bound to the trade-off\nbetween accuracy and computation constraints of specific edge devices through\nour proposed computation-aware training strategy. Experimental results show\nthat our framework significantly reduces the computation cost of ViTs while\nmaintaining comparable performance on image classification. Moreover, our\nframework can guarantee the identified model to meet resource specifications of\nmobile devices and FPGA, and even achieve the real-time execution of DeiT-T on\nmobile platforms. For example, our method reduces the latency of DeiT-T to 26\nms (26%$\\sim $41% superior to existing works) on the mobile device with\n0.25%$\\sim $4% higher top-1 accuracy on ImageNet. Our code will be released\nsoon.",
          "link": "http://arxiv.org/abs/2112.13890",
          "publishedOn": "2022-01-03T07:15:43.535Z",
          "wordCount": null,
          "title": "SPViT: Enabling Faster Vision Transformers via Soft Token Pruning. (arXiv:2112.13890v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15367",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Daudt_R/0/1/0/all/0/1\">Rodrigo Caye Daudt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saux_B/0/1/0/all/0/1\">Bertrand Le Saux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boulch_A/0/1/0/all/0/1\">Alexandre Boulch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gousseau_Y/0/1/0/all/0/1\">Yann Gousseau</a>",
          "description": "Large scale datasets created from crowdsourced labels or openly available\ndata have become crucial to provide training data for large scale learning\nalgorithms. While these datasets are easier to acquire, the data are frequently\nnoisy and unreliable, which is motivating research on weakly supervised\nlearning techniques. In this paper we propose original ideas that help us to\nleverage such datasets in the context of change detection. First, we propose\nthe guided anisotropic diffusion (GAD) algorithm, which improves semantic\nsegmentation results using the input images as guides to perform edge\npreserving filtering. We then show its potential in two weakly-supervised\nlearning strategies tailored for change detection. The first strategy is an\niterative learning method that combines model optimisation and data cleansing\nusing GAD to extract the useful information from a large scale change detection\ndataset generated from open vector data. The second one incorporates GAD within\na novel spatial attention layer that increases the accuracy of weakly\nsupervised networks trained to perform pixel-level predictions from image-level\nlabels. Improvements with respect to state-of-the-art are demonstrated on 4\ndifferent public datasets.",
          "link": "http://arxiv.org/abs/2112.15367",
          "publishedOn": "2022-01-03T07:15:43.531Z",
          "wordCount": null,
          "title": "Weakly Supervised Change Detection Using Guided Anisotropic Difusion. (arXiv:2112.15367v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chharia_A/0/1/0/all/0/1\">Aviral Chharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_N/0/1/0/all/0/1\">Nishi Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shivam Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prajapati_S/0/1/0/all/0/1\">Shivam Prajapati</a>",
          "description": "The rise of computation-based methods in thermal management has gained\nimmense attention in recent years due to the ability of deep learning to solve\ncomplex 'physics' problems, which are otherwise difficult to be approached\nusing conventional techniques. Thermal management is required in electronic\nsystems to keep them from overheating and burning, enhancing their efficiency\nand lifespan. For a long time, numerical techniques have been employed to aid\nin the thermal management of electronics. However, they come with some\nlimitations. To increase the effectiveness of traditional numerical approaches\nand address the drawbacks faced in conventional approaches, researchers have\nlooked at using artificial intelligence at various stages of the thermal\nmanagement process. The present study discusses in detail, the current uses of\ndeep learning in the domain of 'electronic' thermal management.",
          "link": "http://arxiv.org/abs/2112.14837",
          "publishedOn": "2022-01-03T07:15:43.530Z",
          "wordCount": null,
          "title": "Recent Trends in Artificial Intelligence-inspired Electronic Thermal Management. (arXiv:2112.14837v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15280",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1\">Saiping Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Long Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fujun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yutao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaolong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>",
          "description": "Besides entity-centric knowledge, usually organized as Knowledge Graph (KG),\nevents are also an essential kind of knowledge in the world, which trigger the\nspring up of event-centric knowledge representation form like Event KG (EKG).\nIt plays an increasingly important role in many machine learning and artificial\nintelligence applications, such as intelligent search, question-answering,\nrecommendation, and text generation. This paper provides a comprehensive survey\nof EKG from history, ontology, instance, and application views. Specifically,\nto characterize EKG thoroughly, we focus on its history, definitions, schema\ninduction, acquisition, related representative graphs/systems, and\napplications. The development processes and trends are studied therein. We\nfurther summarize perspective directions to facilitate future research on EKG.",
          "link": "http://arxiv.org/abs/2112.15280",
          "publishedOn": "2022-01-03T07:15:43.513Z",
          "wordCount": null,
          "title": "What is Event Knowledge Graph: A Survey. (arXiv:2112.15280v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shicheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaosen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_F/0/1/0/all/0/1\">Fangcheng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wentao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wen Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1\">Yangyu Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bin Cui</a>",
          "description": "K-core decomposition is a commonly used metric to analyze graph structure or\nstudy the relative importance of nodes in complex graphs. Recent years have\nseen rapid growth in the scale of the graph, especially in industrial settings.\nFor example, our industrial partner runs popular social applications with\nbillions of users and is able to gather a rich set of user data. As a result,\napplying K-core decomposition on large graphs has attracted more and more\nattention from academics and the industry. A simple but effective method to\ndeal with large graphs is to train them in the distributed settings, and some\ndistributed K-core decomposition algorithms are also proposed. Despite their\neffectiveness, we experimentally and theoretically observe that these\nalgorithms consume too many resources and become unstable on super-large-scale\ngraphs, especially when the given resources are limited. In this paper, we deal\nwith those super-large-scale graphs and propose a divide-and-conquer strategy\non top of the distributed K-core decomposition algorithm. We evaluate our\napproach on three large graphs. The experimental results show that the\nconsumption of resources can be significantly reduced, and the calculation on\nlarge-scale graphs becomes more stable than the existing methods. For example,\nthe distributed K-core decomposition algorithm can scale to a large graph with\n136 billion edges without losing correctness with our divide-and-conquer\ntechnique.",
          "link": "http://arxiv.org/abs/2112.14840",
          "publishedOn": "2022-01-03T07:15:43.511Z",
          "wordCount": null,
          "title": "K-Core Decomposition on Super Large Graphs with Limited Resources. (arXiv:2112.14840v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15210",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reinauer_R/0/1/0/all/0/1\">Raphael Reinauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caorsi_M/0/1/0/all/0/1\">Matteo Caorsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berkouk_N/0/1/0/all/0/1\">Nicolas Berkouk</a>",
          "description": "One of the main challenges of Topological Data Analysis (TDA) is to extract\nfeatures from persistent diagrams directly usable by machine learning\nalgorithms. Indeed, persistence diagrams are intrinsically (multi-)sets of\npoints in R2 and cannot be seen in a straightforward manner as vectors. In this\narticle, we introduce Persformer, the first Transformer neural network\narchitecture that accepts persistence diagrams as input. The Persformer\narchitecture significantly outperforms previous topological neural network\narchitectures on classical synthetic benchmark datasets. Moreover, it satisfies\na universal approximation theorem. This allows us to introduce the first\ninterpretability method for topological machine learning, which we explore in\ntwo examples.",
          "link": "http://arxiv.org/abs/2112.15210",
          "publishedOn": "2022-01-03T07:15:43.510Z",
          "wordCount": null,
          "title": "Persformer: A Transformer Architecture for Topological Machine Learning. (arXiv:2112.15210v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dejing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gus Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>",
          "description": "Could we automatically derive the score of a piano accompaniment based on the\naudio of a pop song? This is the audio-to-symbolic arrangement problem we\ntackle in this paper. A good arrangement model should not only consider the\naudio content but also have prior knowledge of piano composition (so that the\ngeneration \"sounds like\" the audio and meanwhile maintains musicality.) To this\nend, we contribute a cross-modal representation-learning model, which 1)\nextracts chord and melodic information from the audio, and 2) learns texture\nrepresentation from both audio and a corrupted ground truth arrangement. We\nfurther introduce a tailored training strategy that gradually shifts the source\nof texture information from corrupted score to audio. In the end, the\nscore-based texture posterior is reduced to a standard normal distribution, and\nonly audio is needed for inference. Experiments show that our model captures\nmajor audio information and outperforms baselines in generation quality.",
          "link": "http://arxiv.org/abs/2112.15110",
          "publishedOn": "2022-01-03T07:15:43.507Z",
          "wordCount": null,
          "title": "Audio-to-symbolic Arrangement via Cross-modal Music Representation Learning. (arXiv:2112.15110v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadiq_I/0/1/0/all/0/1\">Ismail Sadiq</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Alday_E/0/1/0/all/0/1\">Erick A. Perez-Alday</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Amit J. Shah</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Rad_A/0/1/0/all/0/1\">Ali Bahrami Rad</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Sameni_R/0/1/0/all/0/1\">Reza Sameni</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Clifford_G/0/1/0/all/0/1\">Gari D. Clifford</a> (1,2)",
          "description": "Objective: To determine if a realistic, but computationally efficient model\nof the electrocardiogram can be used to pre-train a deep neural network (DNN)\nwith a wide range of morphologies and abnormalities specific to a given\ncondition - T-wave Alternans (TWA) as a result of Post-Traumatic Stress\nDisorder, or PTSD - and significantly boost performance on a small database of\nrare individuals.\n\nApproach: Using a previously validated artificial ECG model, we generated\n180,000 artificial ECGs with or without significant TWA, with varying heart\nrate, breathing rate, TWA amplitude, and ECG morphology. A DNN, trained on over\n70,000 patients to classify 25 different rhythms, was modified the output layer\nto a binary class (TWA or no-TWA, or equivalently, PTSD or no-PTSD), and\ntransfer learning was performed on the artificial ECG. In a final transfer\nlearning step, the DNN was trained and cross-validated on ECG from 12 PTSD and\n24 controls for all combinations of using the three databases.\n\nMain results: The best performing approach (AUROC = 0.77, Accuracy = 0.72,\nF1-score = 0.64) was found by performing both transfer learning steps, using\nthe pre-trained arrhythmia DNN, the artificial data and the real PTSD-related\nECG data. Removing the artificial data from training led to the largest drop in\nperformance. Removing the arrhythmia data from training provided a modest, but\nsignificant, drop in performance. The final model showed no significant drop in\nperformance on the artificial data, indicating no overfitting.\n\nSignificance: In healthcare, it is common to only have a small collection of\nhigh-quality data and labels, or a larger database with much lower quality (and\nless relevant) labels. The paradigm presented here, involving model-based\nperformance boosting, provides a solution through transfer learning on a large\nrealistic artificial database, and a partially relevant real database.",
          "link": "http://arxiv.org/abs/2112.15442",
          "publishedOn": "2022-01-03T07:15:43.506Z",
          "wordCount": null,
          "title": "Mythological Medical Machine Learning: Boosting the Performance of a Deep Learning Medical Data Classifier Using Realistic Physiological Models. (arXiv:2112.15442v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassanaly_M/0/1/0/all/0/1\">Malik Hassanaly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaws_A/0/1/0/all/0/1\">Andrew Glaws</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_R/0/1/0/all/0/1\">Ryan N. King</a>",
          "description": "Designing manufacturing processes with high yield and strong reliability\nrelies on effective methods for rare event estimation. Genealogical importance\nsplitting reduces the variance of rare event probability estimators by\niteratively selecting and replicating realizations that are headed towards a\nrare event. The replication step is difficult when applied to deterministic\nsystems where the initial conditions of the offspring realizations need to be\nmodified. Typically, a random perturbation is applied to the offspring to\ndifferentiate their trajectory from the parent realization. However, this\nrandom perturbation strategy may be effective for some systems while failing\nfor others, preventing variance reduction in the probability estimate. This\nwork seeks to address this limitation using a generative model such as a\nGenerative Adversarial Network (GAN) to generate perturbations that are\nconsistent with the attractor of the dynamical system. The proposed\nGAN-assisted Importance SPlitting method (GANISP) improves the variance\nreduction for the system targeted. An implementation of the method is available\nin a companion repository (https://github.com/NREL/GANISP).",
          "link": "http://arxiv.org/abs/2112.15444",
          "publishedOn": "2022-01-03T07:15:43.506Z",
          "wordCount": null,
          "title": "GANISP: a GAN-assisted Importance SPlitting Probability Estimator. (arXiv:2112.15444v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyong Wang</a>",
          "description": "In the last decade, many deep learning models have been well trained and made\na great success in various fields of machine intelligence, especially for\ncomputer vision and natural language processing. To better leverage the\npotential of these well-trained models in intra-domain or cross-domain transfer\nlearning situations, knowledge distillation (KD) and domain adaptation (DA) are\nproposed and become research highlights. They both aim to transfer useful\ninformation from a well-trained model with original training data. However, the\noriginal data is not always available in many cases due to privacy, copyright\nor confidentiality. Recently, the data-free knowledge transfer paradigm has\nattracted appealing attention as it deals with distilling valuable knowledge\nfrom well-trained models without requiring to access to the training data. In\nparticular, it mainly consists of the data-free knowledge distillation (DFKD)\nand source data-free domain adaptation (SFDA). On the one hand, DFKD aims to\ntransfer the intra-domain knowledge of original data from a cumbersome teacher\nnetwork to a compact student network for model compression and efficient\ninference. On the other hand, the goal of SFDA is to reuse the cross-domain\nknowledge stored in a well-trained source model and adapt it to a target\ndomain. In this paper, we provide a comprehensive survey on data-free knowledge\ntransfer from the perspectives of knowledge distillation and unsupervised\ndomain adaptation, to help readers have a better understanding of the current\nresearch status and ideas. Applications and challenges of the two areas are\nbriefly reviewed, respectively. Furthermore, we provide some insights to the\nsubject of future research.",
          "link": "http://arxiv.org/abs/2112.15278",
          "publishedOn": "2022-01-03T07:15:43.490Z",
          "wordCount": null,
          "title": "Data-Free Knowledge Transfer: A Survey. (arXiv:2112.15278v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yifei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vannella_F/0/1/0/all/0/1\">Filippo Vannella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouton_M/0/1/0/all/0/1\">Maxime Bouton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jaeseong Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakim_E/0/1/0/all/0/1\">Ezeddin Al Hakim</a>",
          "description": "6G will move mobile networks towards increasing levels of complexity. To deal\nwith this complexity, optimization of network parameters is key to ensure high\nperformance and timely adaptivity to dynamic network environments. The\noptimization of the antenna tilt provides a practical and cost-efficient method\nto improve coverage and capacity in the network. Previous methods based on\nReinforcement Learning (RL) have shown great promise for tilt optimization by\nlearning adaptive policies outperforming traditional tilt optimization methods.\nHowever, most existing RL methods are based on single-cell features\nrepresentation, which fails to fully characterize the agent state, resulting in\nsuboptimal performance. Also, most of such methods lack scalability, due to\nstate-action explosion, and generalization ability. In this paper, we propose a\nGraph Attention Q-learning (GAQ) algorithm for tilt optimization. GAQ relies on\na graph attention mechanism to select relevant neighbors information, improve\nthe agent state representation, and update the tilt control policy based on a\nhistory of observations using a Deep Q-Network (DQN). We show that GAQ\nefficiently captures important network information and outperforms standard DQN\nwith local information by a large margin. In addition, we demonstrate its\nability to generalize to network deployments of different sizes and densities.",
          "link": "http://arxiv.org/abs/2112.14843",
          "publishedOn": "2022-01-03T07:15:43.488Z",
          "wordCount": null,
          "title": "A Graph Attention Learning Approach to Antenna Tilt Optimization. (arXiv:2112.14843v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2004.00184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Besserve_M/0/1/0/all/0/1\">Michel Besserve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">R&#xe9;my Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janzing_D/0/1/0/all/0/1\">Dominik Janzing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "Generative models can be trained to emulate complex empirical data, but are\nthey useful to make predictions in the context of previously unobserved\nenvironments? An intuitive idea to promote such extrapolation capabilities is\nto have the architecture of such model reflect a causal graph of the true data\ngenerating process, such that one can intervene on each node independently of\nthe others. However, the nodes of this graph are usually unobserved, leading to\noverparameterization and lack of identifiability of the causal structure. We\ndevelop a theoretical framework to address this challenging situation by\ndefining a weaker form of identifiability, based on the principle of\nindependence of mechanisms. We demonstrate on toy examples that classical\nstochastic gradient descent can hinder the model's extrapolation capabilities,\nsuggesting independence of mechanisms should be enforced explicitly during\ntraining. Experiments on deep generative models trained on real world data\nsupport these insights and illustrate how the extrapolation capabilities of\nsuch models can be leveraged.",
          "link": "http://arxiv.org/abs/2004.00184",
          "publishedOn": "2022-01-03T07:15:43.454Z",
          "wordCount": null,
          "title": "A theory of independent mechanisms for extrapolation in generative models. (arXiv:2004.00184v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Astudillo_R/0/1/0/all/0/1\">Raul Astudillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frazier_P/0/1/0/all/0/1\">Peter I. Frazier</a>",
          "description": "We consider Bayesian optimization of the output of a network of functions,\nwhere each function takes as input the output of its parent nodes, and where\nthe network takes significant time to evaluate. Such problems arise, for\nexample, in reinforcement learning, engineering design, and manufacturing.\nWhile the standard Bayesian optimization approach observes only the final\noutput, our approach delivers greater query efficiency by leveraging\ninformation that the former ignores: intermediate output within the network.\nThis is achieved by modeling the nodes of the network using Gaussian processes\nand choosing the points to evaluate using, as our acquisition function, the\nexpected improvement computed with respect to the implied posterior on the\nobjective. Although the non-Gaussian nature of this posterior prevents\ncomputing our acquisition function in closed form, we show that it can be\nefficiently maximized via sample average approximation. In addition, we prove\nthat our method is asymptotically consistent, meaning that it finds a globally\noptimal solution as the number of evaluations grows to infinity, thus\ngeneralizing previously known convergence results for the expected improvement.\nNotably, this holds even though our method might not evaluate the domain\ndensely, instead leveraging problem structure to leave regions unexplored.\nFinally, we show that our approach dramatically outperforms standard Bayesian\noptimization methods in several synthetic and real-world problems.",
          "link": "http://arxiv.org/abs/2112.15311",
          "publishedOn": "2022-01-03T07:15:43.437Z",
          "wordCount": 640,
          "title": "Bayesian Optimization of Function Networks. (arXiv:2112.15311v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xidong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haifeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaodong Yang</a>",
          "description": "In recent years, gradient based Meta-RL (GMRL) methods have achieved\nremarkable successes in either discovering effective online hyperparameter for\none single task (Xu et al., 2018) or learning good initialisation for\nmulti-task transfer learning (Finn et al., 2017). Despite the empirical\nsuccesses, it is often neglected that computing meta gradients via vanilla\nbackpropagation is ill-defined. In this paper, we argue that the stochastic\nmeta-gradient estimation adopted by many existing MGRL methods are in fact\nbiased; the bias comes from two sources: 1) the compositional bias that is\ninborn in the structure of compositional optimisation problems and 2) the bias\nof multi-step Hessian estimation caused by direct automatic differentiation. To\nbetter understand the meta gradient biases, we perform the first of its kind\nstudy to quantify the amount for each of them. We start by providing a unifying\nderivation for existing GMRL algorithms, and then theoretically analyse both\nthe bias and the variance of existing gradient estimation methods. On\nunderstanding the underlying principles of bias, we propose two mitigation\nsolutions based on off-policy correction and multi-step Hessian estimation\ntechniques. Comprehensive ablation studies have been conducted and results\nreveals: (1) The existence of these two biases and how they influence the\nmeta-gradient estimation when combined with different estimator/sample\nsize/step and learning rate. (2) The effectiveness of these mitigation\napproaches for meta-gradient estimation and thereby the final return on two\npractical Meta-RL algorithms: LOLA-DiCE and Meta-gradient Reinforcement\nLearning.",
          "link": "http://arxiv.org/abs/2112.15400",
          "publishedOn": "2022-01-03T07:15:43.419Z",
          "wordCount": null,
          "title": "Settling the Bias and Variance of Meta-Gradient Estimation for Meta-Reinforcement Learning. (arXiv:2112.15400v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15121",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Galanti_T/0/1/0/all/0/1\">Tomer Galanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyorgy_A/0/1/0/all/0/1\">Andr&#xe1;s Gy&#xf6;rgy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1\">Marcus Hutter</a>",
          "description": "We study the ability of foundation models to learn representations for\nclassification that are transferable to new, unseen classes. Recent results in\nthe literature show that representations learned by a single classifier over\nmany classes are competitive on few-shot learning problems with representations\nlearned by special-purpose algorithms designed for such problems. In this paper\nwe provide an explanation for this behavior based on the recently observed\nphenomenon that the features learned by overparameterized classification\nnetworks show an interesting clustering property, called neural collapse. We\ndemonstrate both theoretically and empirically that neural collapse generalizes\nto new samples from the training classes, and -- more importantly -- to new\nclasses as well, allowing foundation models to provide feature maps that work\nwell in transfer learning and, specifically, in the few-shot setting.",
          "link": "http://arxiv.org/abs/2112.15121",
          "publishedOn": "2022-01-03T07:15:43.404Z",
          "wordCount": 545,
          "title": "On the Role of Neural Collapse in Transfer Learning. (arXiv:2112.15121v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarsa_S/0/1/0/all/0/1\">Sami Sarsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leinonen_J/0/1/0/all/0/1\">Juho Leinonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellas_A/0/1/0/all/0/1\">Arto Hellas</a>",
          "description": "In this work, we review and evaluate a body of deep learning knowledge\ntracing (DLKT) models with openly available and widely-used data sets, and with\na novel data set of students learning to program. The evaluated DLKT models\nhave been reimplemented for assessing reproducibility and replicability of\npreviously reported results. We test different input and output layer\nvariations found in the compared models that are independent of the main\narchitectures of the models, and different maximum attempt count options that\nhave been implicitly and explicitly used in some studies. Several metrics are\nused to reflect on the quality of the evaluated knowledge tracing models. The\nevaluated knowledge tracing models include Vanilla-DKT, two Long Short-Term\nMemory Deep Knowledge Tracing (LSTM-DKT) variants, two Dynamic Key-Value Memory\nNetwork (DKVMN) variants, and Self-Attentive Knowledge Tracing (SAKT). We\nevaluate logistic regression, Bayesian Knowledge Tracing (BKT) and simple\nnon-learning models as baselines. Our results suggest that the DLKT models in\ngeneral outperform non-DLKT models, and the relative differences between the\nDLKT models are subtle and often vary between datasets. Our results also show\nthat naive models such as mean prediction can yield better performance than\nmore sophisticated knowledge tracing models, especially in terms of accuracy.\nFurther, our metric and hyperparameter analysis shows that the metric used to\nselect the best model hyperparameters has a noticeable effect on the\nperformance of the models, and that metric choice can affect model ranking. We\nalso study the impact of input and output layer variations, filtering out long\nattempt sequences, and non-model properties such as randomness and hardware.\nFinally, we discuss model performance replicability and related issues. Our\nmodel implementations, evaluation code, and data are published as a part of\nthis work.",
          "link": "http://arxiv.org/abs/2112.15072",
          "publishedOn": "2022-01-03T07:15:43.392Z",
          "wordCount": 713,
          "title": "Deep Learning Models for Knowledge Tracing: Review and Empirical Evaluation. (arXiv:2112.15072v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14893",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chong_B/0/1/0/all/0/1\">Bin Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yingguang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zi-Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_H/0/1/0/all/0/1\">Hang Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhirong Liu</a>",
          "description": "Most algorithms for the multi-armed bandit problem in reinforcement learning\naimed to maximize the expected reward, which are thus useful in searching the\noptimized candidate with the highest reward (function value) for diverse\napplications (e.g., AlphaGo). However, in some typical application scenaios\nsuch as drug discovery, the aim is to search a diverse set of candidates with\nhigh reward. Here we propose a reversible upper confidence bound (rUCB)\nalgorithm for such a purpose, and demonstrate its application in virtual\nscreening upon intrinsically disordered proteins (IDPs). It is shown that rUCB\ngreatly reduces the query times while achieving both high accuracy and low\nperformance loss.The rUCB may have potential application in multipoint\noptimization and other reinforcement-learning cases.",
          "link": "http://arxiv.org/abs/2112.14893",
          "publishedOn": "2022-01-03T07:15:43.383Z",
          "wordCount": 554,
          "title": "Reversible Upper Confidence Bound Algorithm to Generate Diverse Optimized Candidates. (arXiv:2112.14893v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15275",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Stachenfeld_K/0/1/0/all/0/1\">Kimberly Stachenfeld</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Fielding_D/0/1/0/all/0/1\">Drummond B. Fielding</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kochkov_D/0/1/0/all/0/1\">Dmitrii Kochkov</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Cranmer_M/0/1/0/all/0/1\">Miles Cranmer</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pfaff_T/0/1/0/all/0/1\">Tobias Pfaff</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Godwin_J/0/1/0/all/0/1\">Jonathan Godwin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ho_S/0/1/0/all/0/1\">Shirley Ho</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Battaglia_P/0/1/0/all/0/1\">Peter Battaglia</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sanchez_Gonzalez_A/0/1/0/all/0/1\">Alvaro Sanchez-Gonzalez</a>",
          "description": "Turbulence simulation with classical numerical solvers requires very\nhigh-resolution grids to accurately resolve dynamics. Here we train learned\nsimulators at low spatial and temporal resolutions to capture turbulent\ndynamics generated at high resolution. We show that our proposed model can\nsimulate turbulent dynamics more accurately than classical numerical solvers at\nthe same low resolutions across various scientifically relevant metrics. Our\nmodel is trained end-to-end from data and is capable of learning a range of\nchallenging chaotic and turbulent dynamics at low resolution, including\ntrajectories generated by the state-of-the-art Athena++ engine. We show that\nour simpler, general-purpose architecture outperforms various more specialized,\nturbulence-specific architectures from the learned turbulence simulation\nliterature. In general, we see that learned simulators yield unstable\ntrajectories; however, we show that tuning training noise and temporal\ndownsampling solves this problem. We also find that while generalization beyond\nthe training distribution is a challenge for learned models, training noise,\nconvolutional architectures, and added loss constraints can help. Broadly, we\nconclude that our learned simulator outperforms traditional solvers run on\ncoarser grids, and emphasize that simple design choices can offer stability and\nrobust generalization.",
          "link": "http://arxiv.org/abs/2112.15275",
          "publishedOn": "2022-01-03T07:15:43.351Z",
          "wordCount": null,
          "title": "Learned Coarse Models for Efficient Turbulence Simulation. (arXiv:2112.15275v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1\">Wenxuan Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sihang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linxuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xihong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1\">En Zhu</a>",
          "description": "Deep graph clustering, which aims to reveal the underlying graph structure\nand divide the nodes into different groups, has attracted intensive attention\nin recent years. However, we observe that, in the process of node encoding,\nexisting methods suffer from representation collapse which tends to map all\ndata into the same representation. Consequently, the discriminative capability\nof the node representation is limited, leading to unsatisfied clustering\nperformance. To address this issue, we propose a novel self-supervised deep\ngraph clustering method termed Dual Correlation Reduction Network (DCRN) by\nreducing information correlation in a dual manner. Specifically, in our method,\nwe first design a siamese network to encode samples. Then by forcing the\ncross-view sample correlation matrix and cross-view feature correlation matrix\nto approximate two identity matrices, respectively, we reduce the information\ncorrelation in the dual-level, thus improving the discriminative capability of\nthe resulting features. Moreover, in order to alleviate representation collapse\ncaused by over-smoothing in GCN, we introduce a propagation regularization term\nto enable the network to gain long-distance information with the shallow\nnetwork structure. Extensive experimental results on six benchmark datasets\ndemonstrate the effectiveness of the proposed DCRN against the existing\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2112.14772",
          "publishedOn": "2022-01-03T07:15:43.327Z",
          "wordCount": null,
          "title": "Deep Graph Clustering via Dual Correlation Reduction. (arXiv:2112.14772v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruoyu Wang</a>",
          "description": "Active learning (AL) is a machine learning algorithm that can achieve greater\naccuracy with fewer labeled training instances, for having the ability to ask\noracles to label the most valuable unlabeled data chosen iteratively and\nheuristically by query strategies. Scientific experiments nowadays, though\nbecoming increasingly automated, are still suffering from human involvement in\nthe designing process and the exhaustive search in the experimental space. This\narticle performs a retrospective study on a drug response dataset using the\nproposed AL scheme comprised of the matrix factorization method of alternating\nleast square (ALS) and deep neural networks (DNN). This article also proposes\nan AL query strategy based on expected loss minimization. As a result, the\nretrospective study demonstrates that scientific experimental design, instead\nof being manually set, can be optimized by AL, and the proposed query strategy\nELM sampling shows better experimental performance than other ones such as\nrandom sampling and uncertainty sampling.",
          "link": "http://arxiv.org/abs/2112.14811",
          "publishedOn": "2022-01-03T07:15:43.321Z",
          "wordCount": null,
          "title": "Active Learning-Based Optimization of Scientific Experimental Design. (arXiv:2112.14811v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alfarisi_O/0/1/0/all/0/1\">Omar Alfarisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouzzane_D/0/1/0/all/0/1\">Djamel Ouzzane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sassi_M/0/1/0/all/0/1\">Mohamed Sassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tiejun Zhang</a>",
          "description": "Each grid block in a 3D geological model requires a rock type that represents\nall physical and chemical properties of that block. The properties that\nclassify rock types are lithology, permeability, and capillary pressure.\nScientists and engineers determined these properties using conventional\nlaboratory measurements, which embedded destructive methods to the sample or\naltered some of its properties (i.e., wettability, permeability, and porosity)\nbecause the measurements process includes sample crushing, fluid flow, or fluid\nsaturation. Lately, Digital Rock Physics (DRT) has emerged to quantify these\nproperties from micro-Computerized Tomography (uCT) and Magnetic Resonance\nImaging (MRI) images. However, the literature did not attempt rock typing in a\nwholly digital context. We propose performing Digital Rock Typing (DRT) by: (1)\nintegrating the latest DRP advances in a novel process that honors digital rock\nproperties determination, while; (2) digitalizing the latest rock typing\napproaches in carbonate, and (3) introducing a novel carbonate rock typing\nprocess that utilizes computer vision capabilities to provide more insight\nabout the heterogeneous carbonate rock texture.",
          "link": "http://arxiv.org/abs/2112.15068",
          "publishedOn": "2022-01-03T07:15:43.318Z",
          "wordCount": null,
          "title": "Digital Rock Typing DRT Algorithm Formulation with Optimal Supervised Semantic Segmentation. (arXiv:2112.15068v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sato_R/0/1/0/all/0/1\">Ryoma Sato</a>",
          "description": "Suppose we have a black-box function (e.g., deep neural network) that takes\nan image as input and outputs a value that indicates preference. How can we\nretrieve optimal images with respect to this function from an external database\non the Internet? Standard retrieval problems in the literature (e.g., item\nrecommendations) assume that an algorithm has full access to the set of items.\nIn other words, such algorithms are designed for service providers. In this\npaper, we consider the retrieval problem under different assumptions.\nSpecifically, we consider how users with limited access to an image database\ncan retrieve images using their own black-box functions. This formulation\nenables a flexible and finer-grained image search defined by each user. We\nassume the user can access the database through a search query with tight API\nlimits. Therefore, a user needs to efficiently retrieve optimal images in terms\nof the number of queries. We propose an efficient retrieval algorithm Tiara for\nthis problem. In the experiments, we confirm that our proposed method performs\nbetter than several baselines under various settings.",
          "link": "http://arxiv.org/abs/2112.14921",
          "publishedOn": "2022-01-03T07:15:43.291Z",
          "wordCount": 601,
          "title": "Retrieving Black-box Optimal Images from External Databases. (arXiv:2112.14921v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14872",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Radhakrishnan_A/0/1/0/all/0/1\">Adityanarayanan Radhakrishnan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Belkin_M/0/1/0/all/0/1\">Mikhail Belkin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Uhler_C/0/1/0/all/0/1\">Caroline Uhler</a>",
          "description": "Establishing a fast rate of convergence for optimization methods is crucial\nto their applicability in practice. With the increasing popularity of deep\nlearning over the past decade, stochastic gradient descent and its adaptive\nvariants (e.g. Adagrad, Adam, etc.) have become prominent methods of choice for\nmachine learning practitioners. While a large number of works have demonstrated\nthat these first order optimization methods can achieve sub-linear or linear\nconvergence, we establish local quadratic convergence for stochastic gradient\ndescent with adaptive step size for problems such as matrix inversion.",
          "link": "http://arxiv.org/abs/2112.14872",
          "publishedOn": "2022-01-03T07:15:43.281Z",
          "wordCount": null,
          "title": "Local Quadratic Convergence of Stochastic Gradient Descent with Adaptive Step Size. (arXiv:2112.14872v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Saurav Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loustaunau_P/0/1/0/all/0/1\">Philippe Loustaunau</a>",
          "description": "Rhetorical Frames in AI can be thought of as expressions that describe AI\ndevelopment as a competition between two or more actors, such as governments or\ncompanies. Examples of such Frames include robotic arms race, AI rivalry,\ntechnological supremacy, cyberwarfare dominance and 5G race. Detection of\nRhetorical Frames from open sources can help us track the attitudes of\ngovernments or companies towards AI, specifically whether attitudes are\nbecoming more cooperative or competitive over time. Given the rapidly\nincreasing volumes of open sources (online news media, twitter, blogs), it is\ndifficult for subject matter experts to identify Rhetorical Frames in (near)\nreal-time. Moreover, these sources are in general unstructured (noisy) and\ntherefore, detecting Frames from these sources will require state-of-the-art\ntext classification techniques. In this paper, we develop RheFrameDetect, a\ntext classification system for (near) real-time capture of Rhetorical Frames\nfrom open sources. Given an input document, RheFrameDetect employs text\nclassification techniques at multiple levels (document level and paragraph\nlevel) to identify all occurrences of Frames used in the discussion of AI. We\nperformed extensive evaluation of the text classification techniques used in\nRheFrameDetect against human annotated Frames from multiple news sources. To\nfurther demonstrate the effectiveness of RheFrameDetect, we show multiple case\nstudies depicting the Frames identified by RheFrameDetect compared against\nhuman annotated Frames.",
          "link": "http://arxiv.org/abs/2112.14933",
          "publishedOn": "2022-01-03T07:15:43.281Z",
          "wordCount": null,
          "title": "RheFrameDetect: A Text Classification System for Automatic Detection of Rhetorical Frames in AI from Open Sources. (arXiv:2112.14933v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15348",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bemporad_A/0/1/0/all/0/1\">Alberto Bemporad</a>",
          "description": "For training recurrent neural network models of nonlinear dynamical systems\nfrom an input/output training dataset based on rather arbitrary convex and\ntwice-differentiable loss functions and regularization terms, we propose the\nuse of sequential least squares for determining the optimal network parameters\nand hidden states. In addition, to handle non-smooth regularization terms such\nas L1, L0, and group-Lasso regularizers, as well as to impose possibly\nnon-convex constraints such as integer and mixed-integer constraints, we\ncombine sequential least squares with the alternating direction method of\nmultipliers (ADMM). The performance of the resulting algorithm, that we call\nNAILS (Nonconvex ADMM Iterations and Least Squares), is tested in a nonlinear\nsystem identification benchmark.",
          "link": "http://arxiv.org/abs/2112.15348",
          "publishedOn": "2022-01-03T07:15:43.281Z",
          "wordCount": null,
          "title": "Training Recurrent Neural Networks by Sequential Least Squares and the Alternating Direction Method of Multipliers. (arXiv:2112.15348v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sommana_B/0/1/0/all/0/1\">Benjaphan Sommana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watchareeruetai_U/0/1/0/all/0/1\">Ukrit Watchareeruetai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1\">Ankush Ganguly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Earp_S/0/1/0/all/0/1\">Samuel W.F. Earp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitiyakara_T/0/1/0/all/0/1\">Taya Kitiyakara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boonmanunt_S/0/1/0/all/0/1\">Suparee Boonmanunt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thammasudjarit_R/0/1/0/all/0/1\">Ratchainant Thammasudjarit</a>",
          "description": "During the SARS-Cov-2 pandemic, mask-wearing became an effective tool to\nprevent spreading and contracting the virus. The ability to monitor the\nmask-wearing rate in the population would be useful for determining public\nhealth strategies against the virus. However, artificial intelligence\ntechnologies for detecting face masks have not been deployed at a large scale\nin real-life to measure the mask-wearing rate in public. In this paper, we\npresent a two-step face mask detection approach consisting of two separate\nmodules: 1) face detection and alignment and 2) face mask classification. This\napproach allowed us to experiment with different combinations of face detection\nand face mask classification modules. More specifically, we experimented with\nPyramidKey and RetinaFace as face detectors while maintaining a lightweight\nbackbone for the face mask classification module. Moreover, we also provide a\nrelabeled annotation of the test set of the AIZOO dataset, where we rectified\nthe incorrect labels for some face images. The evaluation results on the AIZOO\nand Moxa 3K datasets showed that the proposed face mask detection pipeline\nsurpassed the state-of-the-art methods. The proposed pipeline also yielded a\nhigher mAP on the relabeled test set of the AIZOO dataset than the original\ntest set. Since we trained the proposed model using in-the-wild face images, we\ncan successfully deploy our model to monitor the mask-wearing rate using public\nCCTV images.",
          "link": "http://arxiv.org/abs/2112.15031",
          "publishedOn": "2022-01-03T07:15:43.277Z",
          "wordCount": 724,
          "title": "Development of a face mask detection pipeline for mask-wearing monitoring in the era of the COVID-19 pandemic: A modular approach. (arXiv:2112.15031v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xi_D/0/1/0/all/0/1\">Dongbo Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Ganbin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiaohu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>",
          "description": "Domain adaptation tasks such as cross-domain sentiment classification aim to\nutilize existing labeled data in the source domain and unlabeled or few labeled\ndata in the target domain to improve the performance in the target domain via\nreducing the shift between the data distributions. Existing cross-domain\nsentiment classification methods need to distinguish pivots, i.e., the\ndomain-shared sentiment words, and non-pivots, i.e., the domain-specific\nsentiment words, for excellent adaptation performance. In this paper, we first\ndesign a Category Attention Network (CAN), and then propose a model named\nCAN-CNN to integrate CAN and a Convolutional Neural Network (CNN). On the one\nhand, the model regards pivots and non-pivots as unified category attribute\nwords and can automatically capture them to improve the domain adaptation\nperformance; on the other hand, the model makes an attempt at interpretability\nto learn the transferred category attribute words. Specifically, the\noptimization objective of our model has three different components: 1) the\nsupervised classification loss; 2) the distributions loss of category feature\nweights; 3) the domain invariance loss. Finally, the proposed model is\nevaluated on three public sentiment analysis datasets and the results\ndemonstrate that CAN-CNN can outperform other various baseline methods.",
          "link": "http://arxiv.org/abs/2112.15290",
          "publishedOn": "2022-01-03T07:15:43.269Z",
          "wordCount": null,
          "title": "Domain Adaptation with Category Attention Network for Deep Sentiment Analysis. (arXiv:2112.15290v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suarez_Varela_J/0/1/0/all/0/1\">Jos&#xe9; Su&#xe1;rez-Varela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almasan_P/0/1/0/all/0/1\">Paul Almasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferriol_Galmes_M/0/1/0/all/0/1\">Miquel Ferriol-Galm&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusek_K/0/1/0/all/0/1\">Krzysztof Rusek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geyer_F/0/1/0/all/0/1\">Fabien Geyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiangle Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shihan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarselli_F/0/1/0/all/0/1\">Franco Scarselli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabellos_Aparicio_A/0/1/0/all/0/1\">Albert Cabellos-Aparicio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barlet_Ros_P/0/1/0/all/0/1\">Pere Barlet-Ros</a>",
          "description": "Graph neural networks (GNN) have shown outstanding applications in many\nfields where data is fundamentally represented as graphs (e.g., chemistry,\nbiology, recommendation systems). In this vein, communication networks comprise\nmany fundamental components that are naturally represented in a\ngraph-structured manner (e.g., topology, configurations, traffic flows). This\nposition article presents GNNs as a fundamental tool for modeling, control and\nmanagement of communication networks. GNNs represent a new generation of\ndata-driven models that can accurately learn and reproduce the complex\nbehaviors behind real networks. As a result, such models can be applied to a\nwide variety of networking use cases, such as planning, online optimization, or\ntroubleshooting. The main advantage of GNNs over traditional neural networks\nlies in its unprecedented generalization capabilities when applied to other\nnetworks and configurations unseen during training, which is a critical feature\nfor achieving practical data-driven solutions for networking. This article\ncomprises a brief tutorial on GNNs and their possible applications to\ncommunication networks. To showcase the potential of this technology, we\npresent two use cases with state-of-the-art GNN models respectively applied to\nwired and wireless networks. Lastly, we delve into the key open challenges and\nopportunities yet to be explored in this novel research area.",
          "link": "http://arxiv.org/abs/2112.14792",
          "publishedOn": "2022-01-03T07:15:43.257Z",
          "wordCount": 649,
          "title": "Graph Neural Networks for Communication Networks: Context, Use Cases and Opportunities. (arXiv:2112.14792v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15523",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Aneja_S/0/1/0/all/0/1\">Sandhya Aneja</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aneja_N/0/1/0/all/0/1\">Nagender Aneja</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abas_P/0/1/0/all/0/1\">Pg Emeroylariffion Abas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naim_A/0/1/0/all/0/1\">Abdul Ghani Naim</a>",
          "description": "Transfer learning allows us to exploit knowledge gained from one task to\nassist in solving another but relevant task. In modern computer vision\nresearch, the question is which architecture performs better for a given\ndataset. In this paper, we compare the performance of 14 pre-trained ImageNet\nmodels on the histopathologic cancer detection dataset, where each model has\nbeen configured as a naive model, feature extractor model, or fine-tuned model.\nDensenet161 has been shown to have high precision whilst Resnet101 has a high\nrecall. A high precision model is suitable to be used when follow-up\nexamination cost is high, whilst low precision but a high recall/sensitivity\nmodel can be used when the cost of follow-up examination is low. Results also\nshow that transfer learning helps to converge a model faster.",
          "link": "http://arxiv.org/abs/2112.15523",
          "publishedOn": "2022-01-03T07:15:43.219Z",
          "wordCount": null,
          "title": "Transfer learning for cancer diagnosis in histopathological images. (arXiv:2112.15523v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wali_S/0/1/0/all/0/1\">Syed Wali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_I/0/1/0/all/0/1\">Irfan Khan</a>",
          "description": "The transformation of conventional power networks into smart grids with the\nheavy penetration level of renewable energy resources, particularly\ngrid-connected Photovoltaic (PV) systems, has increased the need for efficient\nfault identification systems. Malfunctioning any single component in\ngrid-connected PV systems may lead to grid instability and other serious\nconsequences, showing that a reliable fault identification system is the utmost\nrequirement for ensuring operational integrity. Therefore, this paper presents\na novel fault identification approach based on statistical signatures of PV\noperational states. These signatures are unique because each fault has a\ndifferent nature and distinctive impact on the electrical system. Thus, the\nRandom Forest Classifier trained on these extracted signatures showed 100%\naccuracy in identifying all types of faults. Furthermore, the performance\ncomparison of the proposed framework with other Machine Learning classifiers\ndepicts its credibility. Moreover, to elevate user trust in the predicted\noutcomes, SHAP (Shapley Additive Explanation) was utilized during the training\nphase to extract a complete model response (global explanation). This extracted\nglobal explanation can help in the assessment of predicted outcomes credibility\nby decoding each prediction in terms of features contribution. Hence, the\nproposed explainable signature-based fault identification technique is highly\ncredible and fulfills all the requirements of smart grids.",
          "link": "http://arxiv.org/abs/2112.14842",
          "publishedOn": "2022-01-03T07:15:43.204Z",
          "wordCount": null,
          "title": "Explainable Signature-based Machine Learning Approach for Identification of Faults in Grid-Connected Photovoltaic Systems. (arXiv:2112.14842v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mai_T/0/1/0/all/0/1\">Tien Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaillet_P/0/1/0/all/0/1\">Patrick Jaillet</a>",
          "description": "Stochastic and soft optimal policies resulting from entropy-regularized\nMarkov decision processes (ER-MDP) are desirable for exploration and imitation\nlearning applications. Motivated by the fact that such policies are sensitive\nwith respect to the state transition probabilities, and the estimation of these\nprobabilities may be inaccurate, we study a robust version of the ER-MDP model,\nwhere the stochastic optimal policies are required to be robust with respect to\nthe ambiguity in the underlying transition probabilities. Our work is at the\ncrossroads of two important schemes in reinforcement learning (RL), namely,\nrobust MDP and entropy regularized MDP. We show that essential properties that\nhold for the non-robust ER-MDP and robust unregularized MDP models also hold in\nour settings, making the robust ER-MDP problem tractable. We show how our\nframework and results can be integrated into different algorithmic schemes\nincluding value or (modified) policy iteration, which would lead to new robust\nRL and inverse RL algorithms to handle uncertainties. Analyses on computational\ncomplexity and error propagation under conventional uncertainty settings are\nalso provided.",
          "link": "http://arxiv.org/abs/2112.15364",
          "publishedOn": "2022-01-03T07:15:43.194Z",
          "wordCount": null,
          "title": "Robust Entropy-regularized Markov Decision Processes. (arXiv:2112.15364v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Donghwan Lee</a>",
          "description": "The goal of this paper is to investigate a control theoretic analysis of\nlinear stochastic iterative algorithm and temporal difference (TD) learning.\nTD-learning is a linear stochastic iterative algorithm to estimate the value\nfunction of a given policy for a Markov decision process, which is one of the\nmost popular and fundamental reinforcement learning algorithms. While there has\nbeen a series of successful works in theoretical analysis of TD-learning, it\nwas not until recently that researchers found some guarantees on its\nstatistical efficiency. In this paper, we propose a control theoretic\nfinite-time analysis TD-learning, which exploits standard notions in linear\nsystem control communities. Therefore, the proposed work provides additional\ninsights on TD-learning and reinforcement learning with simple concepts and\nanalysis tools in control theory.",
          "link": "http://arxiv.org/abs/2112.14417",
          "publishedOn": "2022-01-03T07:15:43.111Z",
          "wordCount": null,
          "title": "Control Theoretic Analysis of Temporal Difference Learning. (arXiv:2112.14417v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alver_S/0/1/0/all/0/1\">Safa Alver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>",
          "description": "We study the problem of learning a good set of policies, so that when\ncombined together, they can solve a wide variety of unseen reinforcement\nlearning tasks with no or very little new data. Specifically, we consider the\nframework of generalized policy evaluation and improvement, in which the\nrewards for all tasks of interest are assumed to be expressible as a linear\ncombination of a fixed set of features. We show theoretically that, under\ncertain assumptions, having access to a specific set of diverse policies, which\nwe call a set of independent policies, can allow for instantaneously achieving\nhigh-level performance on all possible downstream tasks which are typically\nmore complex than the ones on which the agent was trained. Based on this\ntheoretical analysis, we propose a simple algorithm that iteratively constructs\nthis set of policies. In addition to empirically validating our theoretical\nresults, we compare our approach with recently proposed diverse policy set\nconstruction methods and show that, while others fail, our approach is able to\nbuild a behavior basis that enables instantaneous transfer to all possible\ndownstream tasks. We also show empirically that having access to a set of\nindependent policies can better bootstrap the learning process on downstream\ntasks where the new reward function cannot be described as a linear combination\nof the features. Finally, we demonstrate that this policy set can be useful in\na realistic lifelong reinforcement learning setting.",
          "link": "http://arxiv.org/abs/2112.15025",
          "publishedOn": "2022-01-03T07:15:43.050Z",
          "wordCount": null,
          "title": "Constructing a Good Behavior Basis for Transfer using Generalized Policy Updates. (arXiv:2112.15025v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.12961",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yuan_C/0/1/0/all/0/1\">Chaoxia Yuan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ying_C/0/1/0/all/0/1\">Chao Ying</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fang_F/0/1/0/all/0/1\">Fang Fang</a>",
          "description": "Support vector machine (SVM) is a powerful classification method that has\nachieved great success in many fields. Since its performance can be seriously\nimpaired by redundant covariates, model selection techniques are widely used\nfor SVM with high dimensional covariates. As an alternative to model selection,\nsignificant progress has been made in the area of model averaging in the past\ndecades. Yet no frequentist model averaging method was considered for SVM. This\nwork aims to fill the gap and to propose a frequentist model averaging\nprocedure for SVM which selects the optimal weight by cross validation. Even\nwhen the number of covariates diverges at an exponential rate of the sample\nsize, we show asymptotic optimality of the proposed method in the sense that\nthe ratio of its hinge loss to the lowest possible loss converges to one. We\nalso derive the convergence rate which provides more insights to model\naveraging. Compared to model selection methods of SVM which require a tedious\nbut critical task of tuning parameter selection, the model averaging method\navoids the task and shows promising performances in the empirical studies.",
          "link": "http://arxiv.org/abs/2112.12961",
          "publishedOn": "2022-01-03T07:15:42.864Z",
          "wordCount": null,
          "title": "Optimal Model Averaging of Support Vector Machines in Diverging Model Spaces. (arXiv:2112.12961v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.14949",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>",
          "description": "In this paper, we focus on the decentralized optimization problem over the\nStiefel manifold, which is defined on a connected network of $d$ agents. The\nobjective is an average of $d$ local functions, and each function is privately\nheld by an agent and encodes its data. The agents can only communicate with\ntheir neighbors in a collaborative effort to solve this problem. In existing\nmethods, multiple rounds of communications are required to guarantee the\nconvergence, giving rise to high communication costs. In contrast, this paper\nproposes a decentralized algorithm, called DESTINY, which only invokes a single\nround of communications per iteration. DESTINY combines gradient tracking\ntechniques with a novel approximate augmented Lagrangian function. The global\nconvergence to stationary points is rigorously established. Comprehensive\nnumerical experiments demonstrate that DESTINY has a strong potential to\ndeliver a cutting-edge performance in solving a variety of testing problems.",
          "link": "http://arxiv.org/abs/2112.14949",
          "publishedOn": "2022-01-03T07:15:42.535Z",
          "wordCount": 588,
          "title": "Decentralized Optimization Over the Stiefel Manifold by an Approximate Augmented Lagrangian Function. (arXiv:2112.14949v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1\">Deven Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghate_P/0/1/0/all/0/1\">Pinak Ghate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paranjape_M/0/1/0/all/0/1\">Manali Paranjape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Amit Kumar</a>",
          "description": "The current work intends to study the performance of the Hierarchical\nTemporal Memory(HTM) theory for automated classification of text as well as\ndocuments. HTM is a biologically inspired theory based on the working\nprinciples of the human neocortex. The current study intends to provide an\nalternative framework for document categorization using the Spatial Pooler\nlearning algorithm in the HTM Theory. As HTM accepts only a stream of binary\ndata as input, Latent Semantic Indexing(LSI) technique is used for extracting\nthe top features from the input and converting them into binary format. The\nSpatial Pooler algorithm converts the binary input into sparse patterns with\nsimilar input text having overlapping spatial patterns making it easy for\nclassifying the patterns into categories. The results obtained prove that HTM\ntheory, although is in its nascent stages, performs at par with most of the\npopular machine learning based classifiers.",
          "link": "http://arxiv.org/abs/2112.14820",
          "publishedOn": "2022-01-03T07:15:42.401Z",
          "wordCount": 587,
          "title": "Application of Hierarchical Temporal Memory Theory for Document Categorization. (arXiv:2112.14820v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14868",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+So_B/0/1/0/all/0/1\">Banghee So</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Valdez_E/0/1/0/all/0/1\">Emiliano A. Valdez</a>",
          "description": "Classification predictive modeling involves the accurate assignment of\nobservations in a dataset to target classes or categories. There is an\nincreasing growth of real-world classification problems with severely\nimbalanced class distributions. In this case, minority classes have much fewer\nobservations to learn from than those from majority classes. Despite this\nsparsity, a minority class is often considered the more interesting class yet\ndeveloping a scientific learning algorithm suitable for the observations\npresents countless challenges. In this article, we suggest a novel multi-class\nclassification algorithm specialized to handle severely imbalanced classes\nbased on the method we refer to as SAMME.C2. It blends the flexible mechanics\nof the boosting techniques from SAMME algorithm, a multi-class classifier, and\nAda.C2 algorithm, a cost-sensitive binary classifier designed to address highly\nclass imbalances. Not only do we provide the resulting algorithm but we also\nestablish scientific and statistical formulation of our proposed SAMME.C2\nalgorithm. Through numerical experiments examining various degrees of\nclassifier difficulty, we demonstrate consistent superior performance of our\nproposed model.",
          "link": "http://arxiv.org/abs/2112.14868",
          "publishedOn": "2022-01-03T07:15:42.394Z",
          "wordCount": 593,
          "title": "The SAMME.C2 algorithm for severely imbalanced multi-class classification. (arXiv:2112.14868v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15095",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Csoka_A/0/1/0/all/0/1\">&#xc1;d&#xe1;m Cs&#xf3;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovacs_G/0/1/0/all/0/1\">Gy&#xf6;rgy Kov&#xe1;cs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acs_V/0/1/0/all/0/1\">Vir&#xe1;g &#xc1;cs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matics_Z/0/1/0/all/0/1\">Zsolt Matics</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerencser_Z/0/1/0/all/0/1\">Zsolt Gerencs&#xe9;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szendro_Z/0/1/0/all/0/1\">Zsolt Szendr&#x151;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagy_I/0/1/0/all/0/1\">Istv&#xe1;n Nagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petnehazy_O/0/1/0/all/0/1\">&#xd6;rs Petneh&#xe1;zy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Repa_I/0/1/0/all/0/1\">Imre Repa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moizs_M/0/1/0/all/0/1\">Mariann Moizs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donko_T/0/1/0/all/0/1\">Tam&#xe1;s Donk&#xf3;</a>",
          "description": "Various applications of farm animal imaging are based on the estimation of\nweights of certain body parts and cuts from the CT images of animals. In many\ncases, the complexity of the problem is increased by the enormous variability\nof postures in CT images due to the scanning of non-sedated, living animals. In\nthis paper, we propose a general and robust approach for the estimation of the\nweights of cuts and body parts from the CT images of (possibly) living animals.\nWe adapt multi-atlas based segmentation driven by elastic registration and\njoint feature and model selection for the regression component to cape with the\nlarge number of features and low number of samples. The proposed technique is\nevaluated and illustrated through real applications in rabbit breeding\nprograms, showing r^2 scores 12% higher than previous techniques and methods\nthat used to drive the selection so far. The proposed technique is easily\nadaptable to similar problems, consequently, it is shared in an open source\nsoftware package for the benefit of the community.",
          "link": "http://arxiv.org/abs/2112.15095",
          "publishedOn": "2022-01-03T07:15:42.367Z",
          "wordCount": 640,
          "title": "A general technique for the estimation of farm animal body part weights from CT scans and its applications in a rabbit breeding program. (arXiv:2112.15095v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15036",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Inzirillo_H/0/1/0/all/0/1\">Hugo Inzirillo</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Mat_B/0/1/0/all/0/1\">Benjamin Mat</a>",
          "description": "The objective of this paper is to assess the performances of dimensionality\nreduction techniques to establish a link between cryptocurrencies. We have\nfocused our analysis on the two most traded cryptocurrencies: Bitcoin and\nEthereum. To perform our analysis, we took log returns and added some\ncovariates to build our data set. We first introduced the pearson correlation\ncoefficient in order to have a preliminary assessment of the link between\nBitcoin and Ethereum. We then reduced the dimension of our data set using\ncanonical correlation analysis and principal component analysis. After\nperforming an analysis of the links between Bitcoin and Ethereum with both\nstatistical techniques, we measured their performance on forecasting Ethereum\nreturns with Bitcoin s features.",
          "link": "http://arxiv.org/abs/2112.15036",
          "publishedOn": "2022-01-03T07:15:42.361Z",
          "wordCount": 535,
          "title": "Dimensionality reduction for prediction: Application to Bitcoin and Ethereum. (arXiv:2112.15036v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Torres_P/0/1/0/all/0/1\">Pablo Torres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sirmacek_B/0/1/0/all/0/1\">Beril Sirmacek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoyas_S/0/1/0/all/0/1\">Sergio Hoyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinuesa_R/0/1/0/all/0/1\">Ricardo Vinuesa</a>",
          "description": "The sustainability of urban environments is an increasingly relevant problem.\nAir pollution plays a key role in the degradation of the environment as well as\nthe health of the citizens exposed to it. In this chapter we provide a review\nof the methods available to model air pollution, focusing on the application of\nmachine-learning methods. In fact, machine-learning methods have proved to\nimportantly increase the accuracy of traditional air-pollution approaches while\nlimiting the development cost of the models. Machine-learning tools have opened\nnew approaches to study air pollution, such as flow-dynamics modelling or\nremote-sensing methodologies.",
          "link": "http://arxiv.org/abs/2112.15115",
          "publishedOn": "2022-01-03T07:15:42.347Z",
          "wordCount": 523,
          "title": "Aim in Climate Change and City Pollution. (arXiv:2112.15115v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1\">Qingsong Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wenzheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Siming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jianguo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>",
          "description": "Heterogeneous graph neural networks (HGNNs) have been blossoming in recent\nyears, but the unique data processing and evaluation setups used by each work\nobstruct a full understanding of their advancements. In this work, we present a\nsystematical reproduction of 12 recent HGNNs by using their official codes,\ndatasets, settings, and hyperparameters, revealing surprising findings about\nthe progress of HGNNs. We find that the simple homogeneous GNNs, e.g., GCN and\nGAT, are largely underestimated due to improper settings. GAT with proper\ninputs can generally match or outperform all existing HGNNs across various\nscenarios. To facilitate robust and reproducible HGNN research, we construct\nthe Heterogeneous Graph Benchmark (HGB), consisting of 11 diverse datasets with\nthree tasks. HGB standardizes the process of heterogeneous graph data splits,\nfeature processing, and performance evaluation. Finally, we introduce a simple\nbut very strong baseline Simple-HGN--which significantly outperforms all\nprevious models on HGB--to accelerate the advancement of HGNNs in the future.",
          "link": "http://arxiv.org/abs/2112.14936",
          "publishedOn": "2022-01-03T07:15:42.338Z",
          "wordCount": 605,
          "title": "Are we really making much progress? Revisiting, benchmarking, and refining heterogeneous graph neural networks. (arXiv:2112.14936v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tjoa_E/0/1/0/all/0/1\">Erico Tjoa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuntai_G/0/1/0/all/0/1\">Guan Cuntai</a>",
          "description": "This paper proposes two bottom-up interpretable neural network (NN)\nconstructions for universal approximation, namely Triangularly-constructed NN\n(TNN) and Semi-Quantized Activation NN (SQANN). The notable properties are (1)\nresistance to catastrophic forgetting (2) existence of proof for arbitrarily\nhigh accuracies on training dataset (3) for an input \\(x\\), users can identify\nspecific samples of training data whose activation ``fingerprints\" are similar\nto that of \\(x\\)'s activations. Users can also identify samples that are out of\ndistribution.",
          "link": "http://arxiv.org/abs/2112.15026",
          "publishedOn": "2022-01-03T07:15:42.311Z",
          "wordCount": 493,
          "title": "Two Instances of Interpretable Neural Network for Universal Approximations. (arXiv:2112.15026v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mengin_E/0/1/0/all/0/1\">Elie Mengin</a> (SAMM), <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1\">Fabrice Rossi</a> (CEREMADE)",
          "description": "In this paper, we address the problem of finding a correspondence, or\nmatching, between the functions of two programs in binary form, which is one of\nthe most common task in binary diffing. We introduce a new formulation of this\nproblem as a particular instance of a graph edit problem over the call graphs\nof the programs. In this formulation, the quality of a mapping is evaluated\nsimultaneously with respect to both function content and call graph\nsimilarities. We show that this formulation is equivalent to a network\nalignment problem. We propose a solving strategy for this problem based on\nmax-product belief propagation. Finally, we implement a prototype of our\nmethod, called QBinDiff, and propose an extensive evaluation which shows that\nour approach outperforms state of the art diffing tools.",
          "link": "http://arxiv.org/abs/2112.15337",
          "publishedOn": "2022-01-03T07:15:42.299Z",
          "wordCount": 579,
          "title": "Binary Diffing as a Network Alignment Problem via Belief Propagation. (arXiv:2112.15337v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_F/0/1/0/all/0/1\">Fu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengcai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>",
          "description": "Quantizing deep neural networks (DNNs) has been a promising solution for\ndeploying deep neural networks on embedded devices. However, most of the\nexisting methods do not quantize gradients, and the process of quantizing DNNs\nstill has a lot of floating-point operations, which hinders the further\napplications of quantized DNNs. To solve this problem, we propose a new\nheuristic method based on cooperative coevolution for quantizing DNNs. Under\nthe framework of cooperative coevolution, we use the estimation of distribution\nalgorithm to search for the low-bits weights. Specifically, we first construct\nan initial quantized network from a pre-trained network instead of random\ninitialization and then start searching from it by restricting the search\nspace. So far, the problem is the largest discrete problem known to be solved\nby evolutionary algorithms. Experiments show that our method can train 4 bit\nResNet-20 on the Cifar-10 dataset without sacrificing accuracy.",
          "link": "http://arxiv.org/abs/2112.14834",
          "publishedOn": "2022-01-03T07:15:42.246Z",
          "wordCount": 573,
          "title": "Training Quantized Deep Neural Networks via Cooperative Coevolution. (arXiv:2112.14834v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chin-Tung Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mu Yang</a>",
          "description": "Many social media users prefer consuming content in the form of videos rather\nthan text. However, in order for content creators to produce videos with a high\nclick-through rate, much editing is needed to match the footage to the music.\nThis posts additional challenges for more amateur video makers. Therefore, we\npropose a novel attention-based model VMT (Video-Music Transformer) that\nautomatically generates piano scores from video frames. Using music generated\nfrom models also prevent potential copyright infringements that often come with\nusing existing music. To the best of our knowledge, there is no work besides\nthe proposed VMT that aims to compose music for video. Additionally, there\nlacks a dataset with aligned video and symbolic music. We release a new dataset\ncomposed of over 7 hours of piano scores with fine alignment between pop music\nvideos and MIDI files. We conduct experiments with human evaluation on VMT,\nSeqSeq model (our baseline), and the original piano version soundtrack. VMT\nachieves consistent improvements over the baseline on music smoothness and\nvideo relevance. In particular, with the relevance scores and our case study,\nour model has shown the capability of multimodality on frame-level actors'\nmovement for music generation. Our VMT model, along with the new dataset,\npresents a promising research direction toward composing the matching\nsoundtrack for videos. We have released our code at\nhttps://github.com/linchintung/VMT",
          "link": "http://arxiv.org/abs/2112.15320",
          "publishedOn": "2022-01-03T07:15:42.240Z",
          "wordCount": 657,
          "title": "InverseMV: Composing Piano Scores with a Convolutional Video-Music Transformer. (arXiv:2112.15320v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15303",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zang_H/0/1/0/all/0/1\">Hongyu Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingzhong Wang</a>",
          "description": "This work explores how to learn robust and generalizable state representation\nfrom image-based observations with deep reinforcement learning methods.\nAddressing the computational complexity, stringent assumptions, and\nrepresentation collapse challenges in the existing work of bisimulation metric,\nwe devise Simple State Representation (SimSR) operator, which achieves\nequivalent functionality while reducing the complexity by an order in\ncomparison with bisimulation metric. SimSR enables us to design a\nstochastic-approximation-based method that can practically learn the mapping\nfunctions (encoders) from observations to latent representation space. Besides\nthe theoretical analysis, we experimented and compared our work with recent\nstate-of-the-art solutions in visual MuJoCo tasks. The results show that our\nmodel generally achieves better performance and has better robustness and good\ngeneralization.",
          "link": "http://arxiv.org/abs/2112.15303",
          "publishedOn": "2022-01-03T07:15:42.234Z",
          "wordCount": 546,
          "title": "SimSR: Simple Distance-based State Representation for Deep Reinforcement Learning. (arXiv:2112.15303v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianghao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianfu Wu</a>",
          "description": "Image synthesis and image recognition have witnessed remarkable progress, but\noften at the expense of computationally expensive training and inference.\nLearning lightweight yet expressive deep model has emerged as an important and\ninteresting direction. Inspired by the well-known split-transform-aggregate\ndesign heuristic in the Inception building block, this paper proposes a\nSkip-Layer Inception Module (SLIM) that facilitates efficient learning of image\nsynthesis models, and a same-layer variant (dubbed as SLIM too) as a stronger\nalternative to the well-known ResNeXts for image recognition. In SLIM, the\ninput feature map is first split into a number of groups (e.g., 4).Each group\nis then transformed to a latent style vector(via channel-wise attention) and a\nlatent spatial mask (via spatial attention). The learned latent masks and\nlatent style vectors are aggregated to modulate the target feature map. For\ngenerative learning, SLIM is built on a recently proposed lightweight\nGenerative Adversarial Networks (i.e., FastGANs) which present a skip-layer\nexcitation(SLE) module. For few-shot image synthesis tasks, the proposed SLIM\nachieves better performance than the SLE work and other related methods. For\none-shot image synthesis tasks, it shows stronger capability of preserving\nimages structures than prior arts such as the SinGANs. For image classification\ntasks, the proposed SLIM is used as a drop-in replacement for convolution\nlayers in ResNets (resulting in ResNeXt-like models) and achieves better\naccuracy in theImageNet-1000 dataset, with significantly smaller model\ncomplexity",
          "link": "http://arxiv.org/abs/2112.14804",
          "publishedOn": "2022-01-03T07:15:42.192Z",
          "wordCount": 651,
          "title": "Learning Inception Attention for Image Synthesis and Image Recognition. (arXiv:2112.14804v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">JunKyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhanov_L/0/1/0/all/0/1\">Lev Mukhanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molahosseini_A/0/1/0/all/0/1\">Amir Sabbagh Molahosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minhas_U/0/1/0/all/0/1\">Umar Minhas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rincon_J/0/1/0/all/0/1\">Jesus Martinez del Rincon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dichev_K/0/1/0/all/0/1\">Kiril Dichev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1\">Cheol-Ho Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandierendonck_H/0/1/0/all/0/1\">Hans Vandierendonck</a>",
          "description": "Deep learning is pervasive in our daily life, including self-driving cars,\nvirtual assistants, social network services, healthcare services, face\nrecognition, etc. However, deep neural networks demand substantial compute\nresources during training and inference. The machine learning community has\nmainly focused on model-level optimizations such as architectural compression\nof deep learning models, while the system community has focused on\nimplementation-level optimization. In between, various arithmetic-level\noptimization techniques have been proposed in the arithmetic community. This\narticle provides a survey on resource-efficient deep learning techniques in\nterms of model-, arithmetic-, and implementation-level techniques and\nidentifies the research gaps for resource-efficient deep learning techniques\nacross the three different level techniques. Our survey clarifies the influence\nfrom higher to lower-level techniques based on our resource-efficiency metric\ndefinition and discusses the future trend for resource-efficient deep learning\nresearch.",
          "link": "http://arxiv.org/abs/2112.15131",
          "publishedOn": "2022-01-03T07:15:42.184Z",
          "wordCount": 575,
          "title": "Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques. (arXiv:2112.15131v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sung Min Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Kuo-An Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1\">Kai Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jerry Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madry_A/0/1/0/all/0/1\">Aleksander Madry</a>",
          "description": "We identify properties of universal adversarial perturbations (UAPs) that\ndistinguish them from standard adversarial perturbations. Specifically, we show\nthat targeted UAPs generated by projected gradient descent exhibit two\nhuman-aligned properties: semantic locality and spatial invariance, which\nstandard targeted adversarial perturbations lack. We also demonstrate that UAPs\ncontain significantly less signal for generalization than standard adversarial\nperturbations -- that is, UAPs leverage non-robust features to a smaller extent\nthan standard adversarial perturbations.",
          "link": "http://arxiv.org/abs/2112.15329",
          "publishedOn": "2022-01-03T07:15:42.163Z",
          "wordCount": 495,
          "title": "On Distinctive Properties of Universal Perturbations. (arXiv:2112.15329v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hodan_T/0/1/0/all/0/1\">Tomas Hodan</a>",
          "description": "In this thesis, we address the problem of estimating the 6D pose of rigid\nobjects from a single RGB or RGB-D input image, assuming that 3D models of the\nobjects are available. This problem is of great importance to many application\nfields such as robotic manipulation, augmented reality, and autonomous driving.\nFirst, we propose EPOS, a method for 6D object pose estimation from an RGB\nimage. The key idea is to represent an object by compact surface fragments and\npredict the probability distribution of corresponding fragments at each pixel\nof the input image by a neural network. Each pixel is linked with a\ndata-dependent number of fragments, which allows systematic handling of\nsymmetries, and the 6D poses are estimated from the links by a RANSAC-based\nfitting method. EPOS outperformed all RGB and most RGB-D and D methods on\nseveral standard datasets. Second, we present HashMatch, an RGB-D method that\nslides a window over the input image and searches for a match against\ntemplates, which are pre-generated by rendering 3D object models in different\norientations. The method applies a cascade of evaluation stages to each window\nlocation, which avoids exhaustive matching against all templates. Third, we\npropose ObjectSynth, an approach to synthesize photorealistic images of 3D\nobject models for training methods based on neural networks. The images yield\nsubstantial improvements compared to commonly used images of objects rendered\non top of random photographs. Fourth, we introduce T-LESS, the first dataset\nfor 6D object pose estimation that includes 3D models and RGB-D images of\nindustry-relevant objects. Fifth, we define BOP, a benchmark that captures the\nstatus quo in the field. BOP comprises eleven datasets in a unified format, an\nevaluation methodology, an online evaluation system, and public challenges held\nat international workshops organized at the ICCV and ECCV conferences.",
          "link": "http://arxiv.org/abs/2112.15075",
          "publishedOn": "2022-01-03T07:15:42.157Z",
          "wordCount": 759,
          "title": "Pose Estimation of Specific Rigid Objects. (arXiv:2112.15075v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joseph_R/0/1/0/all/0/1\">Rinu Joseph</a>",
          "description": "Branch prediction is an architectural feature that speeds up the execution of\nbranch instruction on pipeline processors and reduces the cost of branching.\nRecent advancements of Deep Learning (DL) in the post Moore's Law era is\naccelerating areas of automated chip design, low-power computer architectures,\nand much more. Traditional computer architecture design and algorithms could\nbenefit from dynamic predictors based on deep learning algorithms which learns\nfrom experience by optimizing its parameters on large number of data. In this\nsurvey paper, we focus on traditional branch prediction algorithms, analyzes\nits limitations, and presents a literature survey of how deep learning\ntechniques can be applied to create dynamic branch predictors capable of\npredicting conditional branch instructions. Prior surveys in this field focus\non dynamic branch prediction techniques based on neural network perceptrons. We\nplan to improve the survey based on latest research in DL and advanced Machine\nLearning (ML) based branch predictors.",
          "link": "http://arxiv.org/abs/2112.14911",
          "publishedOn": "2022-01-03T07:15:42.150Z",
          "wordCount": 577,
          "title": "A Survey of Deep Learning Techniques for Dynamic Branch Prediction. (arXiv:2112.14911v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15094",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Faradonbeh_M/0/1/0/all/0/1\">Mohamad Kazem Shirani Faradonbeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Faradonbeh_M/0/1/0/all/0/1\">Mohamad Sadegh Shirani Faradonbeh</a>",
          "description": "Linear dynamical systems are canonical models for learning-based control of\nplants with uncertain dynamics. The setting consists of a stochastic\ndifferential equation that captures the state evolution of the plant\nunderstudy, while the true dynamics matrices are unknown and need to be learned\nfrom the observed data of state trajectory. An important issue is to ensure\nthat the system is stabilized and destabilizing control actions due to model\nuncertainties are precluded as soon as possible. A reliable stabilization\nprocedure for this purpose that can effectively learn from unstable data to\nstabilize the system in a finite time is not currently available. In this work,\nwe propose a novel Bayesian learning algorithm that stabilizes unknown\ncontinuous-time stochastic linear systems. The presented algorithm is flexible\nand exposes effective stabilization performance after a remarkably short time\nperiod of interacting with the system.",
          "link": "http://arxiv.org/abs/2112.15094",
          "publishedOn": "2022-01-03T07:15:42.143Z",
          "wordCount": 577,
          "title": "Bayesian Algorithms Learn to Stabilize Unknown Continuous-Time Systems. (arXiv:2112.15094v1 [eess.SY])"
        }
      ]
    },
    {
      "title": "stat.ML updates on arXiv.org",
      "feedUrl": "http://arxiv.org/rss/stat.ML",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2110.08205",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Romano_G/0/1/0/all/0/1\">Gaetano Romano</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Eckley_I/0/1/0/all/0/1\">Idris Eckley</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fearnhead_P/0/1/0/all/0/1\">Paul Fearnhead</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rigaill_G/0/1/0/all/0/1\">Guillem Rigaill</a>",
          "description": "Many modern applications of online changepoint detection require the ability\nto process high-frequency observations, sometimes with limited available\ncomputational resources. Online algorithms for detecting a change in mean often\ninvolve using a moving window, or specifying the expected size of change. Such\nchoices affect which changes the algorithms have most power to detect. We\nintroduce an algorithm, Functional Online CuSUM (FOCuS), which is equivalent to\nrunning these earlier methods simultaneously for all sizes of window, or all\npossible values for the size of change. Our theoretical results give tight\nbounds on the expected computational cost per iteration of FOCuS, with this\nbeing logarithmic in the number of observations. We show how FOCuS can be\napplied to a number of different change in mean scenarios, and demonstrate its\npractical utility through its state-of-the art performance at detecting\nanomalous behaviour in computer server data.",
          "link": "http://arxiv.org/abs/2110.08205",
          "publishedOn": "2022-01-14T00:38:52.234Z",
          "wordCount": 581,
          "title": "Fast Online Changepoint Detection via Functional Pruning CUSUM statistics. (arXiv:2110.08205v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaoyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhongwang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhi-Qin John Xu</a>",
          "description": "Understanding the structure of loss landscape of deep neural networks\n(DNNs)is obviously important. In this work, we prove an embedding principle\nthat the loss landscape of a DNN \"contains\" all the critical points of all the\nnarrower DNNs. More precisely, we propose a critical embedding such that any\ncritical point, e.g., local or global minima, of a narrower DNN can be embedded\nto a critical point/hyperplane of the target DNN with higher degeneracy and\npreserving the DNN output function. The embedding structure of critical points\nis independent of loss function and training data, showing a stark difference\nfrom other nonconvex problems such as protein-folding. Empirically, we find\nthat a wide DNN is often attracted by highly-degenerate critical points that\nare embedded from narrow DNNs. The embedding principle provides an explanation\nfor the general easy optimization of wide DNNs and unravels a potential\nimplicit low-complexity regularization during the training. Overall, our work\nprovides a skeleton for the study of loss landscape of DNNs and its\nimplication, by which a more exact and comprehensive understanding can be\nanticipated in the near",
          "link": "http://arxiv.org/abs/2105.14573",
          "publishedOn": "2022-01-14T00:38:52.183Z",
          "wordCount": 642,
          "title": "Embedding Principle of Loss Landscape of Deep Neural Networks. (arXiv:2105.14573v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.11161",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Kumar_A/0/1/0/all/0/1\">Akshat Kumar</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Sarovar_M/0/1/0/all/0/1\">Mohan Sarovar</a>",
          "description": "We introduce an algorithm for computing geodesics on sampled manifolds that\nrelies on simulation of quantum dynamics on a graph embedding of the sampled\ndata. Our approach exploits classic results in semiclassical analysis and the\nquantum-classical correspondence, and forms a basis for techniques to learn the\nmanifold from which a dataset is sampled, and subsequently for nonlinear\ndimensionality reduction of high-dimensional datasets. We illustrate the new\nalgorithm with data sampled from model manifolds and also by a clustering\ndemonstration based on COVID-19 mobility data. Finally, our method reveals\ninteresting connections between the discretization provided by data sampling\nand quantization.",
          "link": "http://arxiv.org/abs/2112.11161",
          "publishedOn": "2022-01-14T00:38:52.172Z",
          "wordCount": 592,
          "title": "Manifold learning via quantum dynamics. (arXiv:2112.11161v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.12348",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Seddik_M/0/1/0/all/0/1\">Mohamed El Amine Seddik</a>, <a href=\"http://arxiv.org/find/math/1/au:+Guillaud_M/0/1/0/all/0/1\">Maxime Guillaud</a>, <a href=\"http://arxiv.org/find/math/1/au:+Couillet_R/0/1/0/all/0/1\">Romain Couillet</a>",
          "description": "Relying on random matrix theory (RMT), this paper studies asymmetric\norder-$d$ spiked tensor models with Gaussian noise. Using the variational\ndefinition of the singular vectors and values of (Lim, 2005), we show that the\nanalysis of the considered model boils down to the analysis of an equivalent\nspiked symmetric block-wise random matrix, that is constructed from\ncontractions of the studied tensor with the singular vectors associated to its\nbest rank-1 approximation. Our approach allows the exact characterization of\nthe almost sure asymptotic singular value and alignments of the corresponding\nsingular vectors with the true spike components, when $\\frac{n_i}{\\sum_{j=1}^d\nn_j}\\to c_i\\in [0, 1]$ with $n_i$'s the tensor dimensions. In contrast to other\nworks that rely mostly on tools from statistical physics to study random\ntensors, our results rely solely on classical RMT tools such as Stein's lemma.\nFinally, classical RMT results concerning spiked random matrices are recovered\nas a particular case.",
          "link": "http://arxiv.org/abs/2112.12348",
          "publishedOn": "2022-01-14T00:38:52.155Z",
          "wordCount": 587,
          "title": "When Random Tensors meet Random Matrices. (arXiv:2112.12348v2 [math.PR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02990",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kamulete_V/0/1/0/all/0/1\">Vathy M. Kamulete</a>",
          "description": "Statistical tests for dataset shift are susceptible to false alarms: they are\nsensitive to minor differences when there is in fact adequate sample coverage\nand predictive performance. We propose instead a framework to detect adverse\ndataset shifts based on outlier scores, $\\texttt{D-SOS}$ for short.\n$\\texttt{D-SOS}$ holds that the new (test) sample is not substantively worse\nthan the reference (training) sample, and not that the two are equal. The key\nidea is to reduce observations to outlier scores and compare contamination\nrates at varying weighted thresholds. Users can define what $\\it{worse}$ means\nin terms of relevant notions of outlyingness, including proxies for predictive\nperformance. Compared to tests of equal distribution, our approach is uniquely\ntailored to serve as a robust metric for model monitoring and data validation.\nWe show how versatile and practical $\\texttt{D-SOS}$ is on a wide range of real\nand simulated data.",
          "link": "http://arxiv.org/abs/2107.02990",
          "publishedOn": "2022-01-14T00:38:52.148Z",
          "wordCount": 594,
          "title": "Test for non-negligible adverse shifts. (arXiv:2107.02990v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.04116",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Daly_C/0/1/0/all/0/1\">Colin Daly</a>",
          "description": "An algorithm for non-stationary spatial modelling using multiple secondary\nvariables is developed. It combines Geostatistics with Quantile Random Forests\nto give a new interpolation and stochastic simulation algorithm. This paper\nintroduces the method and shows that it has consistency results that are\nsimilar in nature to those applying to geostatistical modelling and to Quantile\nRandom Forests. The method allows for embedding of simpler interpolation\ntechniques, such as Kriging, to further condition the model. The algorithm\nworks by estimating a conditional distribution for the target variable at each\ntarget location. The family of such distributions is called the envelope of the\ntarget variable. From this, it is possible to obtain spatial estimates,\nquantiles and uncertainty. An algorithm to produce conditional simulations from\nthe envelope is also developed. As they sample from the envelope, realizations\nare therefore locally influenced by relative changes of importance of secondary\nvariables, trends and variability.",
          "link": "http://arxiv.org/abs/2011.04116",
          "publishedOn": "2022-01-14T00:38:52.142Z",
          "wordCount": 609,
          "title": "An Embedded Model Estimator for Non-Stationary Random Functions using Multiple Secondary Variables. (arXiv:2011.04116v4 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.11067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1\">Claudiu Musat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>",
          "description": "Using personalized explanations to support recommendations has been shown to\nincrease trust and perceived quality. However, to actually obtain better\nrecommendations, there needs to be a means for users to modify the\nrecommendation criteria by interacting with the explanation. We present a novel\ntechnique using aspect markers that learns to generate personalized\nexplanations of recommendations from review texts, and we show that human users\nsignificantly prefer these explanations over those produced by state-of-the-art\ntechniques. Our work's most important innovation is that it allows users to\nreact to a recommendation by critiquing the textual explanation: removing\n(symmetrically adding) certain aspects they dislike or that are no longer\nrelevant (symmetrically that are of interest). The system updates its user\nmodel and the resulting recommendations according to the critique. This is\nbased on a novel unsupervised critiquing method for single- and multi-step\ncritiquing with textual explanations. Experiments on two real-world datasets\nshow that our system is the first to achieve good performance in adapting to\nthe preferences expressed in multi-step critiquing.",
          "link": "http://arxiv.org/abs/2005.11067",
          "publishedOn": "2022-01-14T00:38:52.119Z",
          "wordCount": 645,
          "title": "Interacting with Explanations through Critiquing. (arXiv:2005.11067v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06712",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Bilkis_M/0/1/0/all/0/1\">M. Bilkis</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Cerezo_M/0/1/0/all/0/1\">M. Cerezo</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Verdon_G/0/1/0/all/0/1\">Guillaume Verdon</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Coles_P/0/1/0/all/0/1\">Patrick J. Coles</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Cincio_L/0/1/0/all/0/1\">Lukasz Cincio</a>",
          "description": "Quantum machine learning (QML) offers a powerful, flexible paradigm for\nprogramming near-term quantum computers, with applications in chemistry,\nmetrology, materials science, data science, and mathematics. Here, one trains\nan ansatz, in the form of a parameterized quantum circuit, to accomplish a task\nof interest. However, challenges have recently emerged suggesting that deep\nansatzes are difficult to train, due to flat training landscapes caused by\nrandomness or by hardware noise. This motivates our work, where we present a\nvariable structure approach to build ansatzes for QML. Our approach, called\nVAns (Variable Ansatz), applies a set of rules to both grow and (crucially)\nremove quantum gates in an informed manner during the optimization.\nConsequently, VAns is ideally suited to mitigate trainability and noise-related\nissues by keeping the ansatz shallow. We employ VAns in the variational quantum\neigensolver for condensed matter and quantum chemistry applications and also in\nthe quantum autoencoder for data compression, showing successful results in all\ncases.",
          "link": "http://arxiv.org/abs/2103.06712",
          "publishedOn": "2022-01-14T00:38:52.112Z",
          "wordCount": 615,
          "title": "A semi-agnostic ansatz with variable structure for quantum machine learning. (arXiv:2103.06712v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.04543",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pastur_L/0/1/0/all/0/1\">Leonid Pastur</a>",
          "description": "The paper deals with the distribution of singular values of the input-output\nJacobian of deep untrained neural networks in the limit of their infinite\nwidth. The Jacobian is the product of random matrices where the independent\nrectangular weight matrices alternate with diagonal matrices whose entries\ndepend on the corresponding column of the nearest neighbor weight matrix. The\nproblem was considered in \\cite{Pe-Co:18} for the Gaussian weights and biases\nand also for the weights that are Haar distributed orthogonal matrices and\nGaussian biases. Basing on a free probability argument, it was claimed that in\nthese cases the singular value distribution of the Jacobian in the limit of\ninfinite width (matrix size) coincides with that of the analog of the Jacobian\nwith special random but weight independent diagonal matrices, the case well\nknown in random matrix theory. The claim was rigorously proved in\n\\cite{Pa-Sl:21} for a quite general class of weights and biases with i.i.d.\n(including Gaussian) entries by using a version of the techniques of random\nmatrix theory. In this paper we use another version of the techniques to\njustify the claim for random Haar distributed weight matrices and Gaussian\nbiases.",
          "link": "http://arxiv.org/abs/2201.04543",
          "publishedOn": "2022-01-14T00:38:52.104Z",
          "wordCount": 627,
          "title": "Eigenvalue Distribution of Large Random Matrices Arising in Deep Neural Networks: Orthogonal Case. (arXiv:2201.04543v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2111.10189",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Bialas_P/0/1/0/all/0/1\">Piotr Bia&#x142;as</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Korcyl_P/0/1/0/all/0/1\">Piotr Korcyl</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Stebel_T/0/1/0/all/0/1\">Tomasz Stebel</a>",
          "description": "We provide a deepened study of autocorrelations in Neural Markov Chain Monte\nCarlo simulations, a version of the traditional Metropolis algorithm which\nemploys neural networks to provide independent proposals. We illustrate our\nideas using the two-dimensional Ising model. We propose several estimates of\nautocorrelation times, some inspired by analytical results derived for the\nMetropolized Independent Sampler, which we compare and study as a function of\ninverse temperature $\\beta$. Based on that we propose an alternative loss\nfunction and study its impact on the autocorelation times. Furthermore, we\ninvestigate the impact of imposing system symmetries ($Z_2$ and/or\ntranslational) in the neural network training process on the autocorrelation\ntimes. Eventually, we propose a scheme which incorporates partial heat-bath\nupdates. The impact of the above enhancements is discussed for a $16 \\times 16$\nspin system. The summary of our findings may serve as a guide to the\nimplementation of Neural Markov Chain Monte Carlo simulations of more\ncomplicated models.",
          "link": "http://arxiv.org/abs/2111.10189",
          "publishedOn": "2022-01-14T00:38:52.098Z",
          "wordCount": 617,
          "title": "Analysis of autocorrelation times in Neural Markov Chain Monte Carlo simulations. (arXiv:2111.10189v2 [cond-mat.stat-mech] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.07844",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Buehler_H/0/1/0/all/0/1\">Hans Buehler</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Murray_P/0/1/0/all/0/1\">Phillip Murray</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Pakkanen_M/0/1/0/all/0/1\">Mikko S. Pakkanen</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Wood_B/0/1/0/all/0/1\">Ben Wood</a>",
          "description": "We present a machine learning approach for finding minimal equivalent\nmartingale measures for markets simulators of tradable instruments, e.g. for a\nspot price and options written on the same underlying. We extend our results to\nmarkets with frictions, in which case we find \"near-martingale measures\" under\nwhich the prices of hedging instruments are martingales within their bid/ask\nspread.\n\nBy removing the drift, we are then able to learn using Deep Hedging a \"clean\"\nhedge for an exotic payoff which is not polluted by the trading strategy trying\nto make money from statistical arbitrage opportunities. We correspondingly\nhighlight the robustness of this hedge vs estimation error of the original\nmarket simulator. We discuss applications to two market simulators.",
          "link": "http://arxiv.org/abs/2111.07844",
          "publishedOn": "2022-01-14T00:38:52.091Z",
          "wordCount": 585,
          "title": "Deep Hedging: Learning to Remove the Drift under Trading Frictions with Minimal Equivalent Near-Martingale Measures. (arXiv:2111.07844v3 [q-fin.CP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.09271",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Duarte_E/0/1/0/all/0/1\">Eliana Duarte</a>, <a href=\"http://arxiv.org/find/math/1/au:+Solus_L/0/1/0/all/0/1\">Liam Solus</a>",
          "description": "We consider the problem of representing causal models that encode\ncontext-specific information for discrete data using a proper subclass of\nstaged tree models which we call CStrees. We show that the context-specific\ninformation encoded by a CStree can be equivalently expressed via a collection\nof DAGs. As not all staged tree models admit this property, CStrees are a\nsubclass that provides a transparent, intuitive and compact representation of\ncontext-specific causal information. We prove that CStrees admit a global\nMarkov property which yields a graphical criterion for model equivalence\ngeneralizing that of Verma and Pearl for DAG models. These results extend to\nthe general interventional model setting, making CStrees the first family of\ncontext-specific models admitting a characterization of interventional model\nequivalence. We also provide a closed-form formula for the maximum likelihood\nestimator of a CStree and use it to show that the Bayesian information\ncriterion is a locally consistent score function for this model class. The\nperformance of CStrees is analyzed on both simulated and real data, where we\nsee that modeling with CStrees instead of general staged trees does not result\nin a significant loss of predictive accuracy, while affording DAG\nrepresentations of context-specific causal information.",
          "link": "http://arxiv.org/abs/2101.09271",
          "publishedOn": "2022-01-14T00:38:52.069Z",
          "wordCount": 665,
          "title": "Representation of Context-Specific Causal Models with Observational and Interventional Data. (arXiv:2101.09271v3 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.04545",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Imaizumi_M/0/1/0/all/0/1\">Masaaki Imaizumi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schmidt_Hieber_J/0/1/0/all/0/1\">Johannes Schmidt-Hieber</a>",
          "description": "The classical statistical learning theory says that fitting too many\nparameters leads to overfitting and poor performance. That modern deep neural\nnetworks generalize well despite a large number of parameters contradicts this\nfinding and constitutes a major unsolved problem towards explaining the success\nof deep learning. The implicit regularization induced by stochastic gradient\ndescent (SGD) has been regarded to be important, but its specific principle is\nstill unknown. In this work, we study how the local geometry of the energy\nlandscape around local minima affects the statistical properties of SGD with\nGaussian gradient noise. We argue that under reasonable assumptions, the local\ngeometry forces SGD to stay close to a low dimensional subspace and that this\ninduces implicit regularization and results in tighter bounds on the\ngeneralization error for deep neural networks. To derive generalization error\nbounds for neural networks, we first introduce a notion of stagnation sets\naround the local minima and impose a local essential convexity property of the\npopulation risk. Under these conditions, lower bounds for SGD to remain in\nthese stagnation sets are derived. If stagnation occurs, we derive a bound on\nthe generalization error of deep neural networks involving the spectral norms\nof the weight matrices but not the number of network parameters. Technically,\nour proofs are based on controlling the change of parameter values in the SGD\niterates and local uniform convergence of the empirical loss functions based on\nthe entropy of suitable neighborhoods around local minima. Our work attempts to\nbetter connect non-convex optimization and generalization analysis with uniform\nconvergence.",
          "link": "http://arxiv.org/abs/2201.04545",
          "publishedOn": "2022-01-14T00:38:52.061Z",
          "wordCount": 684,
          "title": "On generalization bounds for deep networks based on loss surface implicit regularization. (arXiv:2201.04545v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04207",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jensen_R/0/1/0/all/0/1\">Rasmus Jensen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>",
          "description": "Money laundering is a profound, global problem. Nonetheless, there is little\nstatistical and machine learning research on the topic. In this paper, we focus\non anti-money laundering in banks. To help organize existing research in the\nfield, we propose a unifying terminology and provide a review of the\nliterature. This is structured around two central tasks: (i) client risk\nprofiling and (ii) suspicious behavior flagging. We find that client risk\nprofiling is characterized by diagnostics, i.e., efforts to find and explain\nrisk factors. Suspicious behavior flagging, on the other hand, is characterized\nby non-disclosed features and hand-crafted risk indices. Finally, we discuss\ndirections for future research. One major challenge is the lack of public data\nsets. This may, potentially, be addressed by synthetic data generation. Other\npossible research directions include semi-supervised and deep learning,\ninterpretability and fairness of the results.",
          "link": "http://arxiv.org/abs/2201.04207",
          "publishedOn": "2022-01-14T00:38:52.055Z",
          "wordCount": 563,
          "title": "Fighting Money-Laundering with Statistics and Machine Learning: An Introduction and Review. (arXiv:2201.04207v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2006.00693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pengyu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_M/0/1/0/all/0/1\">Martin Renqiang Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinghan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malon_C/0/1/0/all/0/1\">Christopher Malon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>",
          "description": "Learning disentangled representations of natural language is essential for\nmany NLP tasks, e.g., conditional text generation, style transfer, personalized\ndialogue systems, etc. Similar problems have been studied extensively for other\nforms of data, such as images and videos. However, the discrete nature of\nnatural language makes the disentangling of textual representations more\nchallenging (e.g., the manipulation over the data space cannot be easily\nachieved). Inspired by information theory, we propose a novel method that\neffectively manifests disentangled representations of text, without any\nsupervision on semantics. A new mutual information upper bound is derived and\nleveraged to measure dependence between style and content. By minimizing this\nupper bound, the proposed method induces style and content embeddings into two\nindependent low-dimensional spaces. Experiments on both conditional text\ngeneration and text-style transfer demonstrate the high quality of our\ndisentangled representation in terms of content and style preservation.",
          "link": "http://arxiv.org/abs/2006.00693",
          "publishedOn": "2022-01-14T00:38:52.048Z",
          "wordCount": 628,
          "title": "Improving Disentangled Text Representation Learning with Information-Theoretic Guidance. (arXiv:2006.00693v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.04234",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Saurabh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_S/0/1/0/all/0/1\">Sivaraman Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1\">Behnam Neyshabur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedghi_H/0/1/0/all/0/1\">Hanie Sedghi</a>",
          "description": "Real-world machine learning deployments are characterized by mismatches\nbetween the source (training) and target (test) distributions that may cause\nperformance drops. In this work, we investigate methods for predicting the\ntarget domain accuracy using only labeled source data and unlabeled target\ndata. We propose Average Thresholded Confidence (ATC), a practical method that\nlearns a threshold on the model's confidence, predicting accuracy as the\nfraction of unlabeled examples for which model confidence exceeds that\nthreshold. ATC outperforms previous methods across several model architectures,\ntypes of distribution shifts (e.g., due to synthetic corruptions, dataset\nreproduction, or novel subpopulations), and datasets (Wilds, ImageNet, Breeds,\nCIFAR, and MNIST). In our experiments, ATC estimates target performance\n$2$-$4\\times$ more accurately than prior methods. We also explore the\ntheoretical foundations of the problem, proving that, in general, identifying\nthe accuracy is just as hard as identifying the optimal predictor and thus, the\nefficacy of any method rests upon (perhaps unstated) assumptions on the nature\nof the shift. Finally, analyzing our method on some toy distributions, we\nprovide insights concerning when it works.",
          "link": "http://arxiv.org/abs/2201.04234",
          "publishedOn": "2022-01-14T00:38:52.041Z",
          "wordCount": 597,
          "title": "Leveraging Unlabeled Data to Predict Out-of-Distribution Performance. (arXiv:2201.04234v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.04469",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kato_M/0/1/0/all/0/1\">Masahiro Kato</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ariu_K/0/1/0/all/0/1\">Kaito Ariu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Imaizumi_M/0/1/0/all/0/1\">Masaaki Imaizumi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Uehara_M/0/1/0/all/0/1\">Masatoshi Uehara</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nomura_M/0/1/0/all/0/1\">Masahiro Nomura</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Qin_a/0/1/0/all/0/1\">and Chao Qin</a>",
          "description": "We consider the fixed-budget best arm identification problem in two-armed\nGaussian bandits with unknown variances. The tightest lower bound on the\ncomplexity and an algorithm whose performance guarantee matches the lower bound\nhave long been open problems when the variances are unknown and when the\nalgorithm is agnostic to the optimal proportion of the arm draws. In this\npaper, we propose a strategy comprising a sampling rule with randomized\nsampling (RS) following the estimated target allocation probabilities of arm\ndraws and a recommendation rule using the augmented inverse probability\nweighting (AIPW) estimator, which is often used in the causal inference\nliterature. We refer to our strategy as the RS-AIPW strategy. In the\ntheoretical analysis, we first derive a large deviation principle for\nmartingales, which can be used when the second moment converges in mean, and\napply it to our proposed strategy. Then, we show that the proposed strategy is\nasymptotically optimal in the sense that the probability of misidentification\nachieves the lower bound by Kaufmann et al. (2016) when the sample size becomes\ninfinitely large and the gap between the two arms goes to zero.",
          "link": "http://arxiv.org/abs/2201.04469",
          "publishedOn": "2022-01-14T00:38:52.021Z",
          "wordCount": 641,
          "title": "Optimal Fixed-Budget Best Arm Identification using the Augmented Inverse Probability Estimator in Two-Armed Gaussian Bandits with Unknown Variances. (arXiv:2201.04469v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rothfuss_J/0/1/0/all/0/1\">Jonas Rothfuss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heyn_D/0/1/0/all/0/1\">Dominique Heyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinfan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1\">Andreas Krause</a>",
          "description": "When data are scarce meta-learning can improve a learner's accuracy by\nharnessing previous experience from related learning tasks. However, existing\nmethods have unreliable uncertainty estimates which are often overconfident.\nAddressing these shortcomings, we introduce a novel meta-learning framework,\ncalled F-PACOH, that treats meta-learned priors as stochastic processes and\nperforms meta-level regularization directly in the function space. This allows\nus to directly steer the probabilistic predictions of the meta-learner towards\nhigh epistemic uncertainty in regions of insufficient meta-training data and,\nthus, obtain well-calibrated uncertainty estimates. Finally, we showcase how\nour approach can be integrated with sequential decision making, where reliable\nuncertainty quantification is imperative. In our benchmark study on\nmeta-learning for Bayesian Optimization (BO), F-PACOH significantly outperforms\nall other meta-learners and standard baselines.",
          "link": "http://arxiv.org/abs/2106.03195",
          "publishedOn": "2022-01-13T00:40:20.130Z",
          "wordCount": 582,
          "title": "Meta-Learning Reliable Priors in the Function Space. (arXiv:2106.03195v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08903",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Klicpera_J/0/1/0/all/0/1\">Johannes Klicpera</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Becker_F/0/1/0/all/0/1\">Florian Becker</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>",
          "description": "Effectively predicting molecular interactions has the potential to accelerate\nmolecular dynamics by multiple orders of magnitude and thus revolutionize\nchemical simulations. Graph neural networks (GNNs) have recently shown great\nsuccesses for this task, overtaking classical methods based on fixed molecular\nkernels. However, they still appear very limited from a theoretical\nperspective, since regular GNNs cannot distinguish certain types of graphs. In\nthis work we close this gap between theory and practice. We show that GNNs with\ndirected edge embeddings and two-hop message passing are indeed universal\napproximators for predictions that are invariant to translation, and\nequivariant to permutation and rotation. We then leverage these insights and\nmultiple structural improvements to propose the geometric message passing\nneural network (GemNet). We demonstrate the benefits of the proposed changes in\nmultiple ablation studies. GemNet outperforms previous models on the COLL,\nMD17, and OC20 datasets by 34%, 41%, and 20%, respectively, and performs\nespecially well on the most challenging molecules. Our implementation is\navailable online.",
          "link": "http://arxiv.org/abs/2106.08903",
          "publishedOn": "2022-01-13T00:40:20.123Z",
          "wordCount": 656,
          "title": "GemNet: Universal Directional Graph Neural Networks for Molecules. (arXiv:2106.08903v6 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07359",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Matsubara_T/0/1/0/all/0/1\">Takuo Matsubara</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Knoblauch_J/0/1/0/all/0/1\">Jeremias Knoblauch</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Briol_F/0/1/0/all/0/1\">Fran&#xe7;ois-Xavier Briol</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Oates_C/0/1/0/all/0/1\">Chris. J. Oates</a>",
          "description": "Generalised Bayesian inference updates prior beliefs using a loss function,\nrather than a likelihood, and can therefore be used to confer robustness\nagainst possible mis-specification of the likelihood. Here we consider\ngeneralised Bayesian inference with a Stein discrepancy as a loss function,\nmotivated by applications in which the likelihood contains an intractable\nnormalisation constant. In this context, the Stein discrepancy circumvents\nevaluation of the normalisation constant and produces generalised posteriors\nthat are either closed form or accessible using standard Markov chain Monte\nCarlo. On a theoretical level, we show consistency, asymptotic normality, and\nbias-robustness of the generalised posterior, highlighting how these properties\nare impacted by the choice of Stein discrepancy. Then, we provide numerical\nexperiments on a range of intractable distributions, including applications to\nkernel-based exponential family models and non-Gaussian graphical models.",
          "link": "http://arxiv.org/abs/2104.07359",
          "publishedOn": "2022-01-13T00:40:20.115Z",
          "wordCount": 583,
          "title": "Robust Generalised Bayesian Inference for Intractable Likelihoods. (arXiv:2104.07359v3 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04715",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Laumont_R/0/1/0/all/0/1\">R&#xe9;mi Laumont</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bortoli_V/0/1/0/all/0/1\">Valentin de Bortoli</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Almansa_A/0/1/0/all/0/1\">Andr&#xe9;s Almansa</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Delon_J/0/1/0/all/0/1\">Julie Delon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1\">Alain Durmus</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pereyra_M/0/1/0/all/0/1\">Marcelo Pereyra</a>",
          "description": "Since the seminal work of Venkatakrishnan et al. (2013), Plug & Play (PnP)\nmethods have become ubiquitous in Bayesian imaging. These methods derive\nMinimum Mean Square Error (MMSE) or Maximum A Posteriori (MAP) estimators for\ninverse problems in imaging by combining an explicit likelihood function with a\nprior that is implicitly defined by an image denoising algorithm. The PnP\nalgorithms proposed in the literature mainly differ in the iterative schemes\nthey use for optimisation or for sampling. In the case of optimisation schemes,\nsome recent works guarantee the convergence to a fixed point, albeit not\nnecessarily a MAP estimate. In the case of sampling schemes, to the best of our\nknowledge, there is no known proof of convergence. There also remain important\nopen questions regarding whether the underlying Bayesian models and estimators\nare well defined, well-posed, and have the basic regularity properties required\nto support these numerical schemes. To address these limitations, this paper\ndevelops theory, methods, and provably convergent algorithms for performing\nBayesian inference with PnP priors. We introduce two algorithms: 1) PnP-ULA\n(Unadjusted Langevin Algorithm) for Monte Carlo sampling and MMSE inference;\nand 2) PnP-SGD (Stochastic Gradient Descent) for MAP inference. Using recent\nresults on the quantitative convergence of Markov chains, we establish detailed\nconvergence guarantees for these two algorithms under realistic assumptions on\nthe denoising operators used, with special attention to denoisers based on deep\nneural networks. We also show that these algorithms approximately target a\ndecision-theoretically optimal Bayesian model that is well-posed. The proposed\nalgorithms are demonstrated on several canonical problems such as image\ndeblurring, inpainting, and denoising, where they are used for point estimation\nas well as for uncertainty visualisation and quantification.",
          "link": "http://arxiv.org/abs/2103.04715",
          "publishedOn": "2022-01-13T00:40:20.092Z",
          "wordCount": 781,
          "title": "Bayesian imaging using Plug & Play priors: when Langevin meets Tweedie. (arXiv:2103.04715v5 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.03952",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Riabiz_M/0/1/0/all/0/1\">Marina Riabiz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1\">Wilson Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cockayne_J/0/1/0/all/0/1\">Jon Cockayne</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Swietach_P/0/1/0/all/0/1\">Pawel Swietach</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Niederer_S/0/1/0/all/0/1\">Steven A. Niederer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1\">Lester Mackey</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Oates_C/0/1/0/all/0/1\">Chris. J. Oates</a>",
          "description": "The use of heuristics to assess the convergence and compress the output of\nMarkov chain Monte Carlo can be sub-optimal in terms of the empirical\napproximations that are produced. Typically a number of the initial states are\nattributed to \"burn in\" and removed, whilst the remainder of the chain is\n\"thinned\" if compression is also required. In this paper we consider the\nproblem of retrospectively selecting a subset of states, of fixed cardinality,\nfrom the sample path such that the approximation provided by their empirical\ndistribution is close to optimal. A novel method is proposed, based on greedy\nminimisation of a kernel Stein discrepancy, that is suitable for problems where\nheavy compression is required. Theoretical results guarantee consistency of the\nmethod and its effectiveness is demonstrated in the challenging context of\nparameter inference for ordinary differential equations. Software is available\nin the Stein Thinning package in Python, R and MATLAB.",
          "link": "http://arxiv.org/abs/2005.03952",
          "publishedOn": "2022-01-13T00:40:20.085Z",
          "wordCount": 636,
          "title": "Optimal Thinning of MCMC Output. (arXiv:2005.03952v5 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.12908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1\">Joshua T. Vogelstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_J/0/1/0/all/0/1\">Jayanta Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helm_H/0/1/0/all/0/1\">Hayden S. Helm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1\">Will LeVine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1\">Ronak D. Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geisa_A/0/1/0/all/0/1\">Ali Geisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoyin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ven_G/0/1/0/all/0/1\">Gido M. van de Ven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Emily Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chenyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Weiwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tower_B/0/1/0/all/0/1\">Bryan Tower</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larson_J/0/1/0/all/0/1\">Jonathan Larson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1\">Christopher M. White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1\">Carey E. Priebe</a>",
          "description": "In biological learning, data are used to improve performance not only on the\ncurrent task, but also on previously encountered, and as yet unencountered\ntasks. In contrast, classical machine learning starts from a blank slate, or\ntabula rasa, using data only for the single task at hand. While typical\ntransfer learning algorithms can improve performance on future tasks, their\nperformance on prior tasks degrades upon learning new tasks (called\ncatastrophic forgetting). Many recent approaches for continual or lifelong\nlearning have attempted to maintain performance given new tasks. But striving\nto avoid forgetting sets the goal unnecessarily low: the goal of lifelong\nlearning, whether biological or artificial, should be to improve performance on\nboth past and future tasks with any new data. Our key insight is that we can\nensemble representations learned independently across tasks to transfer\nomnidirectionally, that is, jointly improve performance on both future\nunforeseen tasks (forward transfer) and past tasks (backward transfer).\nSpecifically, we propose two lifelong learners: one ensembling trees and the\nother ensembling networks. In both cases, the algorithms are able to learn\nsynergistically, improving performance on both past and future tasks in a\nvariety of simulated and real data scenarios, including tabular data, image\ndata, spoken data, and adversarial tasks. Moreover, they can do so with\nquasilinear space and time complexity, a requirement for any bona fide lifelong\nlearning system.",
          "link": "http://arxiv.org/abs/2004.12908",
          "publishedOn": "2022-01-13T00:40:20.078Z",
          "wordCount": 806,
          "title": "Representation Ensembling for Synergistic Lifelong Learning with Quasilinear Complexity. (arXiv:2004.12908v12 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_R/0/1/0/all/0/1\">Robi Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_G/0/1/0/all/0/1\">Gaurav Mahajan</a>",
          "description": "We consider a lifelong learning scenario in which a learner faces a\nneverending and arbitrary stream of facts and has to decide which ones to\nretain in its limited memory. We introduce a mathematical model based on the\nonline learning framework, in which the learner measures itself against a\ncollection of experts that are also memory-constrained and that reflect\ndifferent policies for what to remember. Interspersed with the stream of facts\nare occasional questions, and on each of these the learner incurs a loss if it\nhas not remembered the corresponding fact. Its goal is to do almost as well as\nthe best expert in hindsight, while using roughly the same amount of memory. We\nidentify difficulties with using the multiplicative weights update algorithm in\nthis memory-constrained scenario, and design an alternative scheme whose regret\nguarantees are close to the best possible.",
          "link": "http://arxiv.org/abs/2201.03806",
          "publishedOn": "2022-01-13T00:40:20.070Z",
          "wordCount": 553,
          "title": "Learning what to remember. (arXiv:2201.03806v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.03996",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Fan_J/0/1/0/all/0/1\">Jianqing Fan</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Masini_R/0/1/0/all/0/1\">Ricardo P. Masini</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Medeiros_M/0/1/0/all/0/1\">Marcelo C. Medeiros</a>",
          "description": "Optimal pricing, i.e., determining the price level that maximizes profit or\nrevenue of a given product, is a vital task for the retail industry. To select\nsuch a quantity, one needs first to estimate the price elasticity from the\nproduct demand. Regression methods usually fail to recover such elasticities\ndue to confounding effects and price endogeneity. Therefore, randomized\nexperiments are typically required. However, elasticities can be highly\nheterogeneous depending on the location of stores, for example. As the\nrandomization frequently occurs at the municipal level, standard\ndifference-in-differences methods may also fail. Possible solutions are based\non methodologies to measure the effects of treatments on a single (or just a\nfew) treated unit(s) based on counterfactuals constructed from artificial\ncontrols. For example, for each city in the treatment group, a counterfactual\nmay be constructed from the untreated locations. In this paper, we apply a\nnovel high-dimensional statistical method to measure the effects of price\nchanges on daily sales from a major retailer in Brazil. The proposed\nmethodology combines principal components (factors) and sparse regressions,\nresulting in a method called Factor-Adjusted Regularized Method for Treatment\nevaluation (\\texttt{FarmTreat}). The data consist of daily sales and prices of\nfive different products over more than 400 municipalities. The products\nconsidered belong to the \\emph{sweet and candies} category and experiments have\nbeen conducted over the years of 2016 and 2017. Our results confirm the\nhypothesis of a high degree of heterogeneity yielding very different pricing\nstrategies over distinct municipalities.",
          "link": "http://arxiv.org/abs/2011.03996",
          "publishedOn": "2022-01-13T00:40:20.064Z",
          "wordCount": 713,
          "title": "Do We Exploit all Information for Counterfactual Analysis? Benefits of Factor Models and Idiosyncratic Correction. (arXiv:2011.03996v3 [econ.EM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.08488",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Matsubara_T/0/1/0/all/0/1\">Takuo Matsubara</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Oates_C/0/1/0/all/0/1\">Chris J. Oates</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Briol_F/0/1/0/all/0/1\">Fran&#xe7;ois-Xavier Briol</a>",
          "description": "Bayesian neural networks attempt to combine the strong predictive performance\nof neural networks with formal quantification of uncertainty associated with\nthe predictive output in the Bayesian framework. However, it remains unclear\nhow to endow the parameters of the network with a prior distribution that is\nmeaningful when lifted into the output space of the network. A possible\nsolution is proposed that enables the user to posit an appropriate Gaussian\nprocess covariance function for the task at hand. Our approach constructs a\nprior distribution for the parameters of the network, called a ridgelet prior,\nthat approximates the posited Gaussian process in the output space of the\nnetwork. In contrast to existing work on the connection between neural networks\nand Gaussian processes, our analysis is non-asymptotic, with finite sample-size\nerror bounds provided. This establishes the universality property that a\nBayesian neural network can approximate any Gaussian process whose covariance\nfunction is sufficiently regular. Our experimental assessment is limited to a\nproof-of-concept, where we demonstrate that the ridgelet prior can out-perform\nan unstructured prior on regression problems for which a suitable Gaussian\nprocess prior can be provided.",
          "link": "http://arxiv.org/abs/2010.08488",
          "publishedOn": "2022-01-13T00:40:20.056Z",
          "wordCount": 650,
          "title": "The Ridgelet Prior: A Covariance Function Approach to Prior Specification for Bayesian Neural Networks. (arXiv:2010.08488v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.01646",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Bo Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhiyun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1\">Srinivasan Parthasarathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1\">Xia Ning</a>",
          "description": "Next-basket recommendation considers the problem of recommending a set of\nitems into the next basket that users will purchase as a whole. In this paper,\nwe develop a novel mixed model with preferences, popularities and transitions\n(M2) for the next-basket recommendation. This method models three important\nfactors in next-basket generation process: 1) users' general preferences, 2)\nitems' global popularities and 3) transition patterns among items. Unlike\nexisting recurrent neural network-based approaches, M2 does not use the\ncomplicated networks to model the transitions among items, or generate\nembeddings for users. Instead, it has a simple encoder-decoder based approach\n(ed-Trans) to better model the transition patterns among items. We compared M2\nwith different combinations of the factors with 5 state-of-the-art next-basket\nrecommendation methods on 4 public benchmark datasets in recommending the\nfirst, second and third next basket. Our experimental results demonstrate that\nM2 significantly outperforms the state-of-the-art methods on all the datasets\nin all the tasks, with an improvement of up to 22.1%. In addition, our ablation\nstudy demonstrates that the ed-Trans is more effective than recurrent neural\nnetworks in terms of the recommendation performance. We also have a thorough\ndiscussion on various experimental protocols and evaluation metrics for\nnext-basket recommendation evaluation.",
          "link": "http://arxiv.org/abs/2004.01646",
          "publishedOn": "2022-01-13T00:40:20.048Z",
          "wordCount": 681,
          "title": "M2: Mixed Models with Preferences, Popularities and Transitions for Next-Basket Recommendation. (arXiv:2004.01646v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vaswani_S/0/1/0/all/0/1\">Sharan Vaswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachem_O/0/1/0/all/0/1\">Olivier Bachem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Totaro_S/0/1/0/all/0/1\">Simone Totaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_R/0/1/0/all/0/1\">Robert Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Shivam Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1\">Matthieu Geist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_M/0/1/0/all/0/1\">Marlos C. Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1\">Pablo Samuel Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_N/0/1/0/all/0/1\">Nicolas Le Roux</a>",
          "description": "Common policy gradient methods rely on the maximization of a sequence of\nsurrogate functions. In recent years, many such surrogate functions have been\nproposed, most without strong theoretical guarantees, leading to algorithms\nsuch as TRPO, PPO or MPO. Rather than design yet another surrogate function, we\ninstead propose a general framework (FMA-PG) based on functional mirror ascent\nthat gives rise to an entire family of surrogate functions. We construct\nsurrogate functions that enable policy improvement guarantees, a property not\nshared by most existing surrogate functions. Crucially, these guarantees hold\nregardless of the choice of policy parameterization. Moreover, a particular\ninstantiation of FMA-PG recovers important implementation heuristics (e.g.,\nusing forward vs reverse KL divergence) resulting in a variant of TRPO with\nadditional desirable properties. Via experiments on simple bandit problems, we\nevaluate the algorithms instantiated by FMA-PG. The proposed framework also\nsuggests an improved variant of PPO, whose robustness and efficiency we\nempirically demonstrate on the MuJoCo suite.",
          "link": "http://arxiv.org/abs/2108.05828",
          "publishedOn": "2022-01-13T00:40:20.039Z",
          "wordCount": 663,
          "title": "A general class of surrogate functions for stable and efficient reinforcement learning. (arXiv:2108.05828v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03624",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Panousis_K/0/1/0/all/0/1\">Konstantinos P. Panousis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antoniadis_A/0/1/0/all/0/1\">Anastasios Antoniadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzis_S/0/1/0/all/0/1\">Sotirios Chatzis</a>",
          "description": "This work aims to address the long-established problem of learning\ndiversified representations. To this end, we combine information-theoretic\narguments with stochastic competition-based activations, namely Stochastic\nLocal Winner-Takes-All (LWTA) units. In this context, we ditch the conventional\ndeep architectures commonly used in Representation Learning, that rely on\nnon-linear activations; instead, we replace them with sets of locally and\nstochastically competing linear units. In this setting, each network layer\nyields sparse outputs, determined by the outcome of the competition between\nunits that are organized into blocks of competitors. We adopt stochastic\narguments for the competition mechanism, which perform posterior sampling to\ndetermine the winner of each block. We further endow the considered networks\nwith the ability to infer the sub-part of the network that is essential for\nmodeling the data at hand; we impose appropriate stick-breaking priors to this\nend. To further enrich the information of the emerging representations, we\nresort to information-theoretic principles, namely the Information Competing\nProcess (ICP). Then, all the components are tied together under the stochastic\nVariational Bayes framework for inference. We perform a thorough experimental\ninvestigation for our approach using benchmark datasets on image\nclassification. As we experimentally show, the resulting networks yield\nsignificant discriminative representation learning abilities. In addition, the\nintroduced paradigm allows for a principled investigation mechanism of the\nemerging intermediate network representations.",
          "link": "http://arxiv.org/abs/2201.03624",
          "publishedOn": "2022-01-13T00:40:20.015Z",
          "wordCount": 650,
          "title": "Competing Mutual Information Constraints with Stochastic Competition-based Activations for Learning Diversified Representations. (arXiv:2201.03624v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03668",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lokhande_V/0/1/0/all/0/1\">Vishnu Suresh Lokhande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kihyuk Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jinsung Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udell_M/0/1/0/all/0/1\">Madeleine Udell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>",
          "description": "Learning invariant representations is an important requirement when training\nmachine learning models that are driven by spurious correlations in the\ndatasets. These spurious correlations, between input samples and the target\nlabels, wrongly direct the neural network predictions resulting in poor\nperformance on certain groups, especially the minority groups. Robust training\nagainst these spurious correlations requires the knowledge of group membership\nfor every sample. Such a requirement is impractical in situations where the\ndata labeling efforts for minority or rare groups are significantly laborious\nor where the individuals comprising the dataset choose to conceal sensitive\ninformation. On the other hand, the presence of such data collection efforts\nresults in datasets that contain partially labeled group information. Recent\nworks have tackled the fully unsupervised scenario where no labels for groups\nare available. Thus, we aim to fill the missing gap in the literature by\ntackling a more realistic setting that can leverage partially available\nsensitive or group information during training. First, we construct a\nconstraint set and derive a high probability bound for the group assignment to\nbelong to the set. Second, we propose an algorithm that optimizes for the\nworst-off group assignments from the constraint set. Through experiments on\nimage and tabular datasets, we show improvements in the minority group's\nperformance while preserving overall aggregate accuracy across groups.",
          "link": "http://arxiv.org/abs/2201.03668",
          "publishedOn": "2022-01-13T00:40:19.979Z",
          "wordCount": 659,
          "title": "Towards Group Robustness in the presence of Partial Group Labels. (arXiv:2201.03668v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03949",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Keriven_N/0/1/0/all/0/1\">Nicolas Keriven</a>",
          "description": "In graph analysis, a classic task consists in computing similarity measures\nbetween (groups of) nodes. In latent space random graphs, nodes are associated\nto unknown latent variables. One may then seek to compute distances directly in\nthe latent space, using only the graph structure. In this paper, we show that\nit is possible to consistently estimate entropic-regularized Optimal Transport\n(OT) distances between groups of nodes in the latent space. We provide a\ngeneral stability result for entropic OT with respect to perturbations of the\ncost matrix. We then apply it to several examples of random graphs, such as\ngraphons or $\\epsilon$-graphs on manifolds. Along the way, we prove new\nconcentration results for the so-called Universal Singular Value Thresholding\nestimator, and for the estimation of geodesic distances on a manifold.",
          "link": "http://arxiv.org/abs/2201.03949",
          "publishedOn": "2022-01-13T00:40:19.935Z",
          "wordCount": 544,
          "title": "Entropic Optimal Transport in Random Graphs. (arXiv:2201.03949v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sunwoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_A/0/1/0/all/0/1\">Anit Kumar Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chaoyang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1\">Salman Avestimehr</a>",
          "description": "Local Stochastic Gradient Descent (SGD) with periodic model averaging\n(FedAvg) is a foundational algorithm in Federated Learning. The algorithm\nindependently runs SGD on multiple workers and periodically averages the model\nacross all the workers. When local SGD runs with many workers, however, the\nperiodic averaging causes a significant model discrepancy across the workers\nmaking the global loss converge slowly. While recent advanced optimization\nmethods tackle the issue focused on non-IID settings, there still exists the\nmodel discrepancy issue due to the underlying periodic model averaging. We\npropose a partial model averaging framework that mitigates the model\ndiscrepancy issue in Federated Learning. The partial averaging encourages the\nlocal models to stay close to each other on parameter space, and it enables to\nmore effectively minimize the global loss. Given a fixed number of iterations\nand a large number of workers (128), the partial averaging achieves up to 2.2%\nhigher validation accuracy than the periodic full averaging.",
          "link": "http://arxiv.org/abs/2201.03789",
          "publishedOn": "2022-01-13T00:40:19.912Z",
          "wordCount": 581,
          "title": "Partial Model Averaging in Federated Learning: Performance Guarantees and Benefits. (arXiv:2201.03789v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1907.07330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Finocchiaro_J/0/1/0/all/0/1\">Jessie Finocchiaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frongillo_R/0/1/0/all/0/1\">Rafael Frongillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waggoner_B/0/1/0/all/0/1\">Bo Waggoner</a>",
          "description": "We formalize and study the natural approach of designing convex surrogate\nloss functions via embeddings, for problems such as classification, ranking, or\nstructured prediction. In this approach, one embeds each of the finitely many\npredictions (e.g.\\ rankings) as a point in $\\mathbb{R}^d$, assigns the original\nloss values to these points, and \"convexifies\" the loss in some way to obtain a\nsurrogate. We establish a strong connection between this approach and\npolyhedral (piecewise-linear convex) surrogate losses. Given any polyhedral\nloss $L$, we give a construction of a link function through which $L$ is a\nconsistent surrogate for the loss it embeds. Conversely, we show how to\nconstruct a consistent polyhedral surrogate for any given discrete loss. Our\nframework yields succinct proofs of consistency or inconsistency of various\npolyhedral surrogates in the literature, and for inconsistent surrogates, it\nfurther reveals the discrete losses for which these surrogates are consistent.\nWe show some additional structure of embeddings, such as the equivalence of\nembedding and matching Bayes risks, and the equivalence of various notions of\nnon-redudancy. Using these results, we establish that indirect elicitation, a\nnecessary condition for consistency, is also sufficient when working with\npolyhedral surrogates.",
          "link": "http://arxiv.org/abs/1907.07330",
          "publishedOn": "2022-01-13T00:40:19.890Z",
          "wordCount": 646,
          "title": "An Embedding Framework for Consistent Polyhedral Surrogates. (arXiv:1907.07330v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03522",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1\">Qiwen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Simon S. Du</a>",
          "description": "We study what dataset assumption permits solving offline two-player zero-sum\nMarkov game. In stark contrast to the offline single-agent Markov decision\nprocess, we show that the single strategy concentration assumption is\ninsufficient for learning the Nash equilibrium (NE) strategy in offline\ntwo-player zero-sum Markov games. On the other hand, we propose a new\nassumption named unilateral concentration and design a pessimism-type algorithm\nthat is provably efficient under this assumption. In addition, we show that the\nunilateral concentration assumption is necessary for learning an NE strategy.\nFurthermore, our algorithm can achieve minimax sample complexity without any\nmodification for two widely studied settings: dataset with uniform\nconcentration assumption and turn-based Markov game. Our work serves as an\nimportant initial step towards understanding offline multi-agent reinforcement\nlearning.",
          "link": "http://arxiv.org/abs/2201.03522",
          "publishedOn": "2022-01-12T00:38:42.888Z",
          "wordCount": 557,
          "title": "When is Offline Two-Player Zero-Sum Markov Game Solvable?. (arXiv:2201.03522v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2111.03972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dandi_Y/0/1/0/all/0/1\">Yatin Dandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacot_A/0/1/0/all/0/1\">Arthur Jacot</a>",
          "description": "Spectral analysis is a powerful tool, decomposing any function into simpler\nparts. In machine learning, Mercer's theorem generalizes this idea, providing\nfor any kernel and input distribution a natural basis of functions of\nincreasing frequency. More recently, several works have extended this analysis\nto deep neural networks through the framework of Neural Tangent Kernel. In this\nwork, we analyze the layer-wise spectral bias of Deep Neural Networks and\nrelate it to the contributions of different layers in the reduction of\ngeneralization error for a given target function. We utilize the properties of\nHermite polynomials and Spherical Harmonics to prove that initial layers\nexhibit a larger bias towards high-frequency functions defined on the unit\nsphere. We further provide empirical results validating our theory in high\ndimensional datasets for Deep Neural Networks.",
          "link": "http://arxiv.org/abs/2111.03972",
          "publishedOn": "2022-01-12T00:38:42.735Z",
          "wordCount": 587,
          "title": "Understanding Layer-wise Contributions in Deep Neural Networks through Spectral Analysis. (arXiv:2111.03972v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.08527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ng_I/0/1/0/all/0/1\">Ignavier Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shengyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhuangyan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhitang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>",
          "description": "This paper studies the problem of learning causal structures from\nobservational data. We reformulate the Structural Equation Model (SEM) with\nadditive noises in a form parameterized by binary graph adjacency matrix and\nshow that, if the original SEM is identifiable, then the binary adjacency\nmatrix can be identified up to super-graphs of the true causal graph under mild\nconditions. We then utilize the reformulated SEM to develop a causal structure\nlearning method that can be efficiently trained using gradient-based\noptimization, by leveraging a smooth characterization on acyclicity and the\nGumbel-Softmax approach to approximate the binary adjacency matrix. It is found\nthat the obtained entries are typically near zero or one and can be easily\nthresholded to identify the edges. We conduct experiments on synthetic and real\ndatasets to validate the effectiveness of the proposed method, and show that it\nreadily includes different smooth model functions and achieves a much improved\nperformance on most datasets considered.",
          "link": "http://arxiv.org/abs/1910.08527",
          "publishedOn": "2022-01-12T00:38:42.726Z",
          "wordCount": 619,
          "title": "Masked Gradient-Based Causal Structure Learning. (arXiv:1910.08527v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.06910",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_F/0/1/0/all/0/1\">Fanghui Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Suykens_J/0/1/0/all/0/1\">Johan A.K. Suykens</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cevher_V/0/1/0/all/0/1\">Volkan Cevher</a>",
          "description": "This paper studies generalization properties of random features (RF)\nregression in high dimensions optimized by stochastic gradient descent (SGD).\nIn this regime, we derive precise non-asymptotic error bounds of RF regression\nunder both constant and adaptive step-size SGD setting, and observe the double\ndescent phenomenon both theoretically and empirically. Our analysis shows how\nto cope with multiple randomness sources of initialization, label noise, and\ndata sampling (as well as stochastic gradients) with no closed-form solution,\nand also goes beyond the commonly-used Gaussian/spherical data assumption. Our\ntheoretical results demonstrate that, with SGD training, RF regression still\ngeneralizes well for interpolation learning, and is able to characterize the\ndouble descent behavior by the unimodality of variance and monotonic decrease\nof bias. Besides, we also prove that the constant step-size SGD setting incurs\nno loss in convergence rate when compared to the exact minimal-norm\ninterpolator, as a theoretical justification of using SGD in practice.",
          "link": "http://arxiv.org/abs/2110.06910",
          "publishedOn": "2022-01-12T00:38:42.719Z",
          "wordCount": 629,
          "title": "On the Double Descent of Random Features Models Trained with SGD. (arXiv:2110.06910v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.03764",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Xiong_R/0/1/0/all/0/1\">Ruoxuan Xiong</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Athey_S/0/1/0/all/0/1\">Susan Athey</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Bayati_M/0/1/0/all/0/1\">Mohsen Bayati</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Imbens_G/0/1/0/all/0/1\">Guido Imbens</a>",
          "description": "In this paper, we study the problem of designing experiments that are\nconducted on a set of units such as users or groups of users in an online\nmarketplace, for multiple time periods such as weeks or months. These\nexperiments are particularly useful to study the treatments that have causal\neffects on both current and future outcomes (instantaneous and lagged effects).\nThe design problem involves selecting a treatment time for each unit, before or\nduring the experiment, in order to most precisely estimate the instantaneous\nand lagged effects, post experimentation. This optimization of the treatment\ndecisions can directly minimize the opportunity cost of the experiment by\nreducing its sample size requirement. The optimization is an NP-hard integer\nprogram for which we provide a near-optimal solution, when the design decisions\nare performed all at the beginning (fixed-sample-size designs). Next, we study\nsequential experiments that allow adaptive decisions during the experiments,\nand also potentially early stop the experiments, further reducing their cost.\nHowever, the sequential nature of these experiments complicates both the design\nphase and the estimation phase. We propose a new algorithm, PGAE, that\naddresses these challenges by adaptively making treatment decisions, estimating\nthe treatment effects, and drawing valid post-experimentation inference. PGAE\ncombines ideas from Bayesian statistics, dynamic programming, and sample\nsplitting. Using synthetic experiments on real data sets from multiple domains,\nwe demonstrate that our proposed solutions for fixed-sample-size and sequential\nexperiments reduce the opportunity cost of the experiments by over 50% and 70%,\nrespectively, compared to benchmarks.",
          "link": "http://arxiv.org/abs/1911.03764",
          "publishedOn": "2022-01-12T00:38:42.695Z",
          "wordCount": 692,
          "title": "Optimal Experimental Design for Staggered Rollouts. (arXiv:1911.03764v3 [econ.EM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.00115",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Choe_Y/0/1/0/all/0/1\">Yo Joong Choe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1\">Aaditya Ramdas</a>",
          "description": "Consider two or more forecasters, each making a sequence of predictions for\ndifferent events over time. We ask a relatively basic question: how might we\ncompare these forecasters, either online or post-hoc, while avoiding\nunverifiable assumptions on how the forecasts or outcomes were generated? This\nwork presents a novel and rigorous answer to this question. We design a\nsequential inference procedure for estimating the time-varying difference in\nforecast quality as measured by any scoring rule. The resulting confidence\nintervals are nonasymptotically valid and can be continuously monitored to\nyield statistically valid comparisons at arbitrary data-dependent stopping\ntimes (\"anytime-valid\"); this is enabled by adapting variance-adaptive\nsupermartingales, confidence sequences, and e-processes to our setting.\nMotivated by Shafer and Vovk's game-theoretic probability, our coverage\nguarantees are also distribution-free, in the sense that they make no\ndistributional assumptions on the forecasts or outcomes. In contrast to a\nrecent work by Henzi and Ziegel, our tools can sequentially test a weak null\nhypothesis about whether one forecaster outperforms another on average over\ntime. We demonstrate their effectiveness by comparing probability forecasts on\nMajor League Baseball (MLB) games and statistical postprocessing methods for\nensemble weather forecasts.",
          "link": "http://arxiv.org/abs/2110.00115",
          "publishedOn": "2022-01-12T00:38:42.686Z",
          "wordCount": 645,
          "title": "Comparing Sequential Forecasters. (arXiv:2110.00115v3 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03447",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Ho_N/0/1/0/all/0/1\">Nhat Ho</a>, <a href=\"http://arxiv.org/find/math/1/au:+Walker_S/0/1/0/all/0/1\">Stephen G. Walker</a>",
          "description": "We present simple conditions for Bayesian consistency in the supremum metric.\nThe key to the technique is a triangle inequality which allows us to explicitly\nuse weak convergence, a consequence of the standard Kullback--Leibler support\ncondition for the prior. A further condition is to ensure that smoothed\nversions of densities are not too far from the original density, thus dealing\nwith densities which could track the data too closely. A key result of the\npaper is that we demonstrate supremum consistency using weaker conditions\ncompared to those currently used to secure $\\mathbb{L}_1$ consistency.",
          "link": "http://arxiv.org/abs/2201.03447",
          "publishedOn": "2022-01-12T00:38:42.679Z",
          "wordCount": 512,
          "title": "Bayesian Consistency with the Supremum Metric. (arXiv:2201.03447v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2111.08922",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shaojie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaughan_J/0/1/0/all/0/1\">Joel Vaughan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudjianto_A/0/1/0/all/0/1\">Agus Sudjianto</a>",
          "description": "Although neural networks (NNs) with ReLU activation functions have found\nsuccess in a wide range of applications, their adoption in risk-sensitive\nsettings has been limited by the concerns on robustness and interpretability.\nPrevious works to examine robustness and to improve interpretability partially\nexploited the piecewise linear function form of ReLU NNs. In this paper, we\nexplore the unique topological structure that ReLU NNs create in the input\nspace, identifying the adjacency among the partitioned local polytopes and\ndeveloping a traversing algorithm based on this adjacency. Our polytope\ntraversing algorithm can be adapted to verify a wide range of network\nproperties related to robustness and interpretability, providing an unified\napproach to examine the network behavior. As the traversing algorithm\nexplicitly visits all local polytopes, it returns a clear and full picture of\nthe network behavior within the traversed region. The time and space complexity\nof the traversing algorithm is determined by the number of a ReLU NN's\npartitioning hyperplanes passing through the traversing region.",
          "link": "http://arxiv.org/abs/2111.08922",
          "publishedOn": "2022-01-12T00:38:42.630Z",
          "wordCount": 634,
          "title": "Traversing the Local Polytopes of ReLU Neural Networks: A Unified Approach for Network Verification. (arXiv:2111.08922v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04831",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pfitzinger_J/0/1/0/all/0/1\">Johann Pfitzinger</a>",
          "description": "This paper proposes a novel graph-based regularized regression estimator -\nthe hierarchical feature regression (HFR) -, which mobilizes insights from the\ndomains of machine learning and graph theory to estimate robust parameters for\na linear regression. The estimator constructs a supervised feature graph that\ndecomposes parameters along its edges, adjusting first for common variation and\nsuccessively incorporating idiosyncratic patterns into the fitting process. The\ngraph structure has the effect of shrinking parameters towards group targets,\nwhere the extent of shrinkage is governed by a hyperparamter, and group\ncompositions as well as shrinkage targets are determined endogenously. The\nmethod offers rich resources for the visual exploration of the latent effect\nstructure in the data, and demonstrates good predictive accuracy and\nversatility when compared to a panel of commonly used regularization techniques\nacross a range of empirical and simulated regression tasks.",
          "link": "http://arxiv.org/abs/2107.04831",
          "publishedOn": "2022-01-12T00:38:42.587Z",
          "wordCount": 576,
          "title": "Cluster Regularization via a Hierarchical Feature Regression. (arXiv:2107.04831v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Arindam Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tiancong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingxue Zhou</a>",
          "description": "We study generalization bounds for noisy stochastic mini-batch iterative\nalgorithms based on the notion of stability. Recent years have seen key\nadvances in data-dependent generalization bounds for noisy iterative learning\nalgorithms such as stochastic gradient Langevin dynamics (SGLD) based on\nstability (Mou et al., 2018; Li et al., 2020) and information theoretic\napproaches (Xu and Raginsky, 2017; Negrea et al., 2019; Steinke and\nZakynthinou, 2020; Haghifam et al., 2020). In this paper, we unify and\nsubstantially generalize stability based generalization bounds and make three\ntechnical advances. First, we bound the generalization error of general noisy\nstochastic iterative algorithms (not necessarily gradient descent) in terms of\nexpected (not uniform) stability. The expected stability can in turn be bounded\nby a Le Cam Style Divergence. Such bounds have a O(1/n) sample dependence\nunlike many existing bounds with O(1/\\sqrt{n}) dependence. Second, we introduce\nExponential Family Langevin Dynamics(EFLD) which is a substantial\ngeneralization of SGLD and which allows exponential family noise to be used\nwith stochastic gradient descent (SGD). We establish data-dependent expected\nstability based generalization bounds for general EFLD algorithms. Third, we\nconsider an important special case of EFLD: noisy sign-SGD, which extends\nsign-SGD using Bernoulli noise over {-1,+1}. Generalization bounds for noisy\nsign-SGD are implied by that of EFLD and we also establish optimization\nguarantees for the algorithm. Further, we present empirical results on\nbenchmark datasets to illustrate that our bounds are non-vacuous and\nquantitatively much sharper than existing bounds.",
          "link": "http://arxiv.org/abs/2201.03064",
          "publishedOn": "2022-01-12T00:38:42.343Z",
          "wordCount": 662,
          "title": "Stability Based Generalization Bounds for Exponential Family Langevin Dynamics. (arXiv:2201.03064v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03528",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Slawski_M/0/1/0/all/0/1\">Martin Slawski</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sen_B/0/1/0/all/0/1\">Bodhisattva Sen</a>",
          "description": "Suppose that we have a regression problem with response variable Y in\n$\\mathbb{R}^d$ and predictor X in $\\mathbb{R}^d$, for $d \\geq 1$. In permuted\nor unlinked regression we have access to separate unordered data on X and Y, as\nopposed to data on (X,Y)-pairs in usual regression. So far in the literature\nthe case $d=1$ has received attention, see e.g., the recent papers by Rigollet\nand Weed [Information & Inference, 8, 619--717] and Balabdaoui et al. [J. Mach.\nLearn. Res., 22(172), 1--60]. In this paper, we consider the general\nmultivariate setting with $d \\geq 1$. We show that the notion of cyclical\nmonotonicity of the regression function is sufficient for identification and\nestimation in the permuted/unlinked regression model. We study permutation\nrecovery in the permuted regression setting and develop a computationally\nefficient and easy-to-use algorithm for denoising based on the Kiefer-Wolfowitz\n[Ann. Math. Statist., 27, 887--906] nonparametric maximum likelihood estimator\nand techniques from the theory of optimal transport. We provide explicit upper\nbounds on the associated mean squared denoising error for Gaussian noise. As in\nprevious work on the case $d = 1$, the permuted/unlinked setting involves slow\n(logarithmic) rates of convergence rooting in the underlying deconvolution\nproblem. Numerical studies corroborate our theoretical analysis and show that\nthe proposed approach performs at least on par with the methods in the\naforementioned prior work in the case $d = 1$ while achieving substantial\nreductions in terms of computational complexity.",
          "link": "http://arxiv.org/abs/2201.03528",
          "publishedOn": "2022-01-12T00:38:42.316Z",
          "wordCount": 682,
          "title": "Permuted and Unlinked Monotone Regression in $\\mathbb{R}^d$: an approach based on mixture modeling and optimal transport. (arXiv:2201.03528v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiyang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Beibei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tian Lu</a>",
          "description": "We develop a structural econometric model to capture the decision dynamics of\nhuman evaluators on an online micro-lending platform, and estimate the model\nparameters using a real-world dataset. We find two types of biases in gender,\npreference-based bias and belief-based bias, are present in human evaluators'\ndecisions. Both types of biases are in favor of female applicants. Through\ncounterfactual simulations, we quantify the effect of gender bias on loan\ngranting outcomes and the welfare of the company and the borrowers. Our results\nimply that both the existence of the preference-based bias and that of the\nbelief-based bias reduce the company's profits. When the preference-based bias\nis removed, the company earns more profits. When the belief-based bias is\nremoved, the company's profits also increase. Both increases result from\nraising the approval probability for borrowers, especially male borrowers, who\neventually pay back loans. For borrowers, the elimination of either bias\ndecreases the gender gap of the true positive rates in the credit risk\nevaluation. We also train machine learning algorithms on both the real-world\ndata and the data from the counterfactual simulations. We compare the decisions\nmade by those algorithms to see how evaluators' biases are inherited by the\nalgorithms and reflected in machine-based decisions. We find that machine\nlearning algorithms can mitigate both the preference-based bias and the\nbelief-based bias.",
          "link": "http://arxiv.org/abs/2201.03092",
          "publishedOn": "2022-01-12T00:38:42.307Z",
          "wordCount": 654,
          "title": "Uncovering the Source of Machine Bias. (arXiv:2201.03092v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1904.07272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Slivkins_A/0/1/0/all/0/1\">Aleksandrs Slivkins</a>",
          "description": "Multi-armed bandits a simple but very powerful framework for algorithms that\nmake decisions over time under uncertainty. An enormous body of work has\naccumulated over the years, covered in several books and surveys. This book\nprovides a more introductory, textbook-like treatment of the subject. Each\nchapter tackles a particular line of work, providing a self-contained,\nteachable technical introduction and a brief review of the further\ndevelopments; many of the chapters conclude with exercises.\n\nThe book is structured as follows. The first four chapters are on IID\nrewards, from the basic model to impossibility results to Bayesian priors to\nLipschitz rewards. The next three chapters cover adversarial rewards, from the\nfull-feedback version to adversarial bandits to extensions with linear rewards\nand combinatorially structured actions. Chapter 8 is on contextual bandits, a\nmiddle ground between IID and adversarial bandits in which the change in reward\ndistributions is completely explained by observable contexts. The last three\nchapters cover connections to economics, from learning in repeated games to\nbandits with supply/budget constraints to exploration in the presence of\nincentives. The appendix provides sufficient background on concentration and\nKL-divergence.\n\nThe chapters on \"bandits with similarity information\", \"bandits with\nknapsacks\" and \"bandits and agents\" can also be consumed as standalone surveys\non the respective topics.",
          "link": "http://arxiv.org/abs/1904.07272",
          "publishedOn": "2022-01-12T00:38:42.297Z",
          "wordCount": 744,
          "title": "Introduction to Multi-Armed Bandits. (arXiv:1904.07272v7 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.01123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Funkner_A/0/1/0/all/0/1\">Anastasia A. Funkner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yakovlev_A/0/1/0/all/0/1\">Aleksey N. Yakovlev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovalchuk_S/0/1/0/all/0/1\">Sergey V. Kovalchuk</a>",
          "description": "The paper proposes and investigates an approach for surrogate-assisted\nperformance prediction of data-driven knowledge discovery algorithms. The\napproach is based on the identification of surrogate models for prediction of\nthe target algorithm's quality and performance. The proposed approach was\nimplemented and investigated as applied to an evolutionary algorithm for\ndiscovering clusters of interpretable clinical pathways in electronic health\nrecords of patients with acute coronary syndrome. Several clustering metrics\nand execution time were used as the target quality and performance metrics\nrespectively. An analytical software prototype based on the proposed approach\nfor the prediction of algorithm characteristics and feature analysis was\ndeveloped to provide a more interpretable prediction of the target algorithm's\nperformance and quality that can be further used for parameter tuning.",
          "link": "http://arxiv.org/abs/2004.01123",
          "publishedOn": "2022-01-12T00:38:42.290Z",
          "wordCount": 589,
          "title": "Surrogate-assisted performance prediction for data-driven knowledge discovery algorithms: application to evolutionary modeling of clinical pathways. (arXiv:2004.01123v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03128",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Morais_M/0/1/0/all/0/1\">Michael J. Morais</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pillow_J/0/1/0/all/0/1\">Jonathan W. Pillow</a>",
          "description": "Approximate Bayesian inference methods provide a powerful suite of tools for\nfinding approximations to intractable posterior distributions. However, machine\nlearning applications typically involve selecting actions, which -- in a\nBayesian setting -- depend on the posterior distribution only via its\ncontribution to expected utility. A growing body of work on loss-calibrated\napproximate inference methods has therefore sought to develop posterior\napproximations sensitive to the influence of the utility function. Here we\nintroduce loss-calibrated expectation propagation (Loss-EP), a loss-calibrated\nvariant of expectation propagation. This method resembles standard EP with an\nadditional factor that \"tilts\" the posterior towards higher-utility decisions.\nWe show applications to Gaussian process classification under binary utility\nfunctions with asymmetric penalties on False Negative and False Positive\nerrors, and show how this asymmetry can have dramatic consequences on what\ninformation is \"useful\" to capture in an approximation.",
          "link": "http://arxiv.org/abs/2201.03128",
          "publishedOn": "2022-01-12T00:38:42.282Z",
          "wordCount": 555,
          "title": "Loss-calibrated expectation propagation for approximate Bayesian decision-making. (arXiv:2201.03128v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2008.11193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feldman_V/0/1/0/all/0/1\">Vitaly Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zrnic_T/0/1/0/all/0/1\">Tijana Zrnic</a>",
          "description": "We consider a sequential setting in which a single dataset of individuals is\nused to perform adaptively-chosen analyses, while ensuring that the\ndifferential privacy loss of each participant does not exceed a pre-specified\nprivacy budget. The standard approach to this problem relies on bounding a\nworst-case estimate of the privacy loss over all individuals and all possible\nvalues of their data, for every single analysis. Yet, in many scenarios this\napproach is overly conservative, especially for \"typical\" data points which\nincur little privacy loss by participation in most of the analyses. In this\nwork, we give a method for tighter privacy loss accounting based on the value\nof a personalized privacy loss estimate for each individual in each analysis.\nTo implement the accounting method we design a filter for R\\'enyi differential\nprivacy. A filter is a tool that ensures that the privacy parameter of a\ncomposed sequence of algorithms with adaptively-chosen privacy parameters does\nnot exceed a pre-specified budget. Our filter is simpler and tighter than the\nknown filter for $(\\epsilon,\\delta)$-differential privacy by Rogers et al. We\napply our results to the analysis of noisy gradient descent and show that\npersonalized accounting can be practical, easy to implement, and can only make\nthe privacy-utility tradeoff tighter.",
          "link": "http://arxiv.org/abs/2008.11193",
          "publishedOn": "2022-01-12T00:38:42.257Z",
          "wordCount": 673,
          "title": "Individual Privacy Accounting via a Renyi Filter. (arXiv:2008.11193v4 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02654",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Chaudhuri_A/0/1/0/all/0/1\">Anamitra Chaudhuri</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chatterjee_S/0/1/0/all/0/1\">Sabyasachi Chatterjee</a>",
          "description": "This paper formulates a general cross validation framework for signal\ndenoising. The general framework is then applied to nonparametric regression\nmethods such as Trend Filtering and Dyadic CART. The resulting cross validated\nversions are then shown to attain nearly the same rates of convergence as are\nknown for the optimally tuned analogues. There did not exist any previous\ntheoretical analyses of cross validated versions of Trend Filtering or Dyadic\nCART. To illustrate the generality of the framework we also propose and study\ncross validated versions of two fundamental estimators; lasso for high\ndimensional linear regression and singular value thresholding for matrix\nestimation. Our general framework is inspired by the ideas in Chatterjee and\nJafarov (2015) and is potentially applicable to a wide range of estimation\nmethods which use tuning parameters.",
          "link": "http://arxiv.org/abs/2201.02654",
          "publishedOn": "2022-01-12T00:38:42.245Z",
          "wordCount": 568,
          "title": "A Cross Validation framework for Signal Denoising with Applications to Trend Filtering, Dyadic CART and Beyond. (arXiv:2201.02654v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02958",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_W/0/1/0/all/0/1\">Wenjia Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1\">Yanyuan Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaowei Zhang</a>",
          "description": "Nested simulation concerns estimating functionals of a conditional\nexpectation via simulation. In this paper, we propose a new method based on\nkernel ridge regression to exploit the smoothness of the conditional\nexpectation as a function of the multidimensional conditioning variable.\nAsymptotic analysis shows that the proposed method can effectively alleviate\nthe curse of dimensionality on the convergence rate as the simulation budget\nincreases, provided that the conditional expectation is sufficiently smooth.\nThe smoothness bridges the gap between the cubic root convergence rate (that\nis, the optimal rate for the standard nested simulation) and the square root\nconvergence rate (that is, the canonical rate for the standard Monte Carlo\nsimulation). We demonstrate the performance of the proposed method via\nnumerical examples from portfolio risk management and input uncertainty\nquantification.",
          "link": "http://arxiv.org/abs/2201.02958",
          "publishedOn": "2022-01-12T00:38:42.236Z",
          "wordCount": 575,
          "title": "Smooth Nested Simulation: Bridging Cubic and Square Root Convergence Rates in High Dimensions. (arXiv:2201.02958v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02664",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_N/0/1/0/all/0/1\">Nicole Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balle_J/0/1/0/all/0/1\">Johannes Ball&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charles_Z/0/1/0/all/0/1\">Zachary Charles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konecny_J/0/1/0/all/0/1\">Jakub Kone&#x10d;n&#xfd;</a>",
          "description": "A significant bottleneck in federated learning is the network communication\ncost of sending model updates from client devices to the central server. We\npropose a method to reduce this cost. Our method encodes quantized updates with\nan appropriate universal code, taking into account their empirical\ndistribution. Because quantization introduces error, we select quantization\nlevels by optimizing for the desired trade-off in average total bitrate and\ngradient distortion. We demonstrate empirically that in spite of the non-i.i.d.\nnature of federated learning, the rate-distortion frontier is consistent across\ndatasets, optimizers, clients and training rounds, and within each setting,\ndistortion reliably predicts model performance. This allows for a remarkably\nsimple compression scheme that is near-optimal in many use cases, and\noutperforms Top-K, DRIVE, 3LC and QSGD on the Stack Overflow next-word\nprediction benchmark.",
          "link": "http://arxiv.org/abs/2201.02664",
          "publishedOn": "2022-01-12T00:38:42.227Z",
          "wordCount": 569,
          "title": "Optimizing the Communication-Accuracy Trade-off in Federated Learning with Rate-Distortion Theory. (arXiv:2201.02664v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Utkin_L/0/1/0/all/0/1\">Lev V. Utkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konstantinov_A/0/1/0/all/0/1\">Andrei V. Konstantinov</a>",
          "description": "A new approach called ABRF (the attention-based random forest) and its\nmodifications for applying the attention mechanism to the random forest (RF)\nfor regression and classification are proposed. The main idea behind the\nproposed ABRF models is to assign attention weights with trainable parameters\nto decision trees in a specific way. The weights depend on the distance between\nan instance, which falls into a corresponding leaf of a tree, and instances,\nwhich fall in the same leaf. This idea stems from representation of the\nNadaraya-Watson kernel regression in the form of a RF. Three modifications of\nthe general approach are proposed. The first one is based on applying the\nHuber's contamination model and on computing the attention weights by solving\nquadratic or linear optimization problems. The second and the third\nmodifications use the gradient-based algorithms for computing trainable\nparameters. Numerical experiments with various regression and classification\ndatasets illustrate the proposed method.",
          "link": "http://arxiv.org/abs/2201.02880",
          "publishedOn": "2022-01-12T00:38:42.216Z",
          "wordCount": 571,
          "title": "Attention-based Random Forest and Contamination Model. (arXiv:2201.02880v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02824",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Stephanovitch_A/0/1/0/all/0/1\">Arthur St&#xe9;phanovitch</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tanielian_U/0/1/0/all/0/1\">Ugo Tanielian</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cadre_B/0/1/0/all/0/1\">Beno&#xee;t Cadre</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Klutchnikoff_N/0/1/0/all/0/1\">Nicolas Klutchnikoff</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Biau_G/0/1/0/all/0/1\">G&#xe9;rard Biau</a>",
          "description": "The mathematical forces at work behind Generative Adversarial Networks raise\nchallenging theoretical issues. Motivated by the important question of\ncharacterizing the geometrical properties of the generated distributions, we\nprovide a thorough analysis of Wasserstein GANs (WGANs) in both the finite\nsample and asymptotic regimes. We study the specific case where the latent\nspace is univariate and derive results valid regardless of the dimension of the\noutput space. We show in particular that for a fixed sample size, the optimal\nWGANs are closely linked with connected paths minimizing the sum of the squared\nEuclidean distances between the sample points. We also highlight the fact that\nWGANs are able to approach (for the 1-Wasserstein distance) the target\ndistribution as the sample size tends to infinity, at a given convergence rate\nand provided the family of generative Lipschitz functions grows appropriately.\nWe derive in passing new results on optimal transport theory in the\nsemi-discrete setting.",
          "link": "http://arxiv.org/abs/2201.02824",
          "publishedOn": "2022-01-12T00:38:42.194Z",
          "wordCount": 575,
          "title": "Optimal 1-Wasserstein Distance for WGANs. (arXiv:2201.02824v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2112.13366",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Podusenko_A/0/1/0/all/0/1\">Albert Podusenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Erp_B/0/1/0/all/0/1\">Bart van Erp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koudahl_M/0/1/0/all/0/1\">Magnus Koudahl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vries_B/0/1/0/all/0/1\">Bert de Vries</a>",
          "description": "In this paper we present AIDA, which is an active inference-based agent that\niteratively designs a personalized audio processing algorithm through situated\ninteractions with a human client. The target application of AIDA is to propose\non-the-spot the most interesting alternative values for the tuning parameters\nof a hearing aid (HA) algorithm, whenever a HA client is not satisfied with\ntheir HA performance. AIDA interprets searching for the \"most interesting\nalternative\" as an issue of optimal (acoustic) context-aware Bayesian trial\ndesign. In computational terms, AIDA is realized as an active inference-based\nagent with an Expected Free Energy criterion for trial design. This type of\narchitecture is inspired by neuro-economic models on efficient (Bayesian) trial\ndesign in brains and implies that AIDA comprises generative probabilistic\nmodels for acoustic signals and user responses. We propose a novel generative\nmodel for acoustic signals as a sum of time-varying auto-regressive filters and\na user response model based on a Gaussian Process Classifier. The full AIDA\nagent has been implemented in a factor graph for the generative model and all\ntasks (parameter learning, acoustic context classification, trial design, etc.)\nare realized by variational message passing on the factor graph. All\nverification and validation experiments and demonstrations are freely\naccessible at our GitHub repository.",
          "link": "http://arxiv.org/abs/2112.13366",
          "publishedOn": "2022-01-12T00:38:42.187Z",
          "wordCount": 666,
          "title": "AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms. (arXiv:2112.13366v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anderson_D/0/1/0/all/0/1\">Daron Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_G/0/1/0/all/0/1\">George Iosifidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leith_D/0/1/0/all/0/1\">Douglas J. Leith</a>",
          "description": "We consider the general problem of online convex optimization with\ntime-varying additive constraints in the presence of predictions for the next\ncost and constraint functions. A novel primal-dual algorithm is designed by\ncombining a Follow-The-Regularized-Leader iteration with prediction-adaptive\ndynamic steps. The algorithm achieves $\\mathcal O(T^{\\frac{3-\\beta}{4}})$\nregret and $\\mathcal O(T^{\\frac{1+\\beta}{2}})$ constraint violation bounds that\nare tunable via parameter $\\beta\\!\\in\\![1/2,1)$ and have constant factors that\nshrink with the predictions quality, achieving eventually $\\mathcal O(1)$\nregret for perfect predictions. Our work extends the FTRL framework for this\nconstrained OCO setting and outperforms the respective state-of-the-art\ngreedy-based solutions, without imposing conditions on the quality of\npredictions, the cost functions or the geometry of constraints, beyond\nconvexity.",
          "link": "http://arxiv.org/abs/2201.02890",
          "publishedOn": "2022-01-12T00:38:42.180Z",
          "wordCount": 537,
          "title": "Lazy Lagrangians with Predictions for Online Learning. (arXiv:2201.02890v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.03892",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hu_Y/0/1/0/all/0/1\">Yihao Hu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhao_T/0/1/0/all/0/1\">Tong Zhao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Xu_S/0/1/0/all/0/1\">Shixin Xu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiliang Xu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lin_L/0/1/0/all/0/1\">Lizhen Lin</a>",
          "description": "Partial differential equations (PDEs) play a crucial role in studying a vast\nnumber of problems in science and engineering. Numerically solving nonlinear\nand/or high-dimensional PDEs is often a challenging task. Inspired by the\ntraditional finite difference and finite elements methods and emerging\nadvancements in machine learning, we propose a sequence deep learning framework\ncalled Neural-PDE, which allows to automatically learn governing rules of any\ntime-dependent PDE system from existing data by using a bidirectional LSTM\nencoder, and predict the next n time steps data. One critical feature of our\nproposed framework is that the Neural-PDE is able to simultaneously learn and\nsimulate the multiscale variables.We test the Neural-PDE by a range of examples\nfrom one-dimensional PDEs to a high-dimensional and nonlinear complex fluids\nmodel. The results show that the Neural-PDE is capable of learning the initial\nconditions, boundary conditions and differential operators without the\nknowledge of the specific form of a PDE system.In our experiments the\nNeural-PDE can efficiently extract the dynamics within 20 epochs training, and\nproduces accurate predictions. Furthermore, unlike the traditional machine\nlearning approaches in learning PDE such as CNN and MLP which require vast\nparameters for model precision, Neural-PDE shares parameters across all time\nsteps, thus considerably reduces the computational complexity and leads to a\nfast learning algorithm.",
          "link": "http://arxiv.org/abs/2009.03892",
          "publishedOn": "2022-01-12T00:38:42.173Z",
          "wordCount": 675,
          "title": "Neural-PDE: A RNN based neural network for solving time dependent PDEs. (arXiv:2009.03892v3 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shaham_U/0/1/0/all/0/1\">Uri Shaham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segal_E/0/1/0/all/0/1\">Elad Segal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivgi_M/0/1/0/all/0/1\">Maor Ivgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efrat_A/0/1/0/all/0/1\">Avia Efrat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoran_O/0/1/0/all/0/1\">Ori Yoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haviv_A/0/1/0/all/0/1\">Adi Haviv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Wenhan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>",
          "description": "NLP benchmarks have largely focused on short texts, such as sentences and\nparagraphs, even though long texts comprise a considerable amount of natural\nlanguage in the wild. We introduce SCROLLS, a suite of tasks that require\nreasoning over long texts. We examine existing long-text datasets, and handpick\nones where the text is naturally long, while prioritizing tasks that involve\nsynthesizing information across the input. SCROLLS contains summarization,\nquestion answering, and natural language inference tasks, covering multiple\ndomains, including literature, science, business, and entertainment. Initial\nbaselines, including Longformer Encoder-Decoder, indicate that there is ample\nroom for improvement on SCROLLS. We make all datasets available in a unified\ntext-to-text format and host a live leaderboard to facilitate research on model\narchitecture and pretraining methods.",
          "link": "http://arxiv.org/abs/2201.03533",
          "publishedOn": "2022-01-12T00:38:42.166Z",
          "wordCount": 566,
          "title": "SCROLLS: Standardized CompaRison Over Long Language Sequences. (arXiv:2201.03533v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.10629",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Yang_K/0/1/0/all/0/1\">Kai Yang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Asgharian_M/0/1/0/all/0/1\">Masoud Asgharian</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bhatnagar_S/0/1/0/all/0/1\">Sahir Bhatnagar</a>",
          "description": "Nesterov's accelerated gradient (AG) is a popular technique to optimize\nobjective functions comprising two components: a convex loss and a penalty\nfunction. While AG methods perform well for convex penalties, such as the\nLASSO, convergence issues may arise when it is applied to nonconvex penalties,\nsuch as SCAD. A recent proposal generalizes Nesterov's AG method to the\nnonconvex setting but has never been applied to sparse statistical learning\nproblems. There are several hyperparameters to be set before running the\nproposed algorithm. However, there is no explicit rule as to how the\nhyperparameters should be selected. In this article, we consider the\napplication of this nonconvex AG algorithm to high-dimensional linear and\nlogistic sparse learning problems, and propose a hyperparameter setting based\non the complexity upper bound to accelerate convergence. We further establish\nthe rate of convergence and present a simple and useful bound for the damping\nsequence. Simulation studies show that convergence can be made, on average,\nconsiderably faster than that of the conventional ISTA algorithm. Our\nexperiments also show that the proposed method generally outperforms the\ncurrent state-of-the-art method in terms of signal recovery.",
          "link": "http://arxiv.org/abs/2009.10629",
          "publishedOn": "2022-01-12T00:38:42.159Z",
          "wordCount": 645,
          "title": "Accelerated Gradient Methods for Sparse Statistical Learning with Nonconvex Penalties. (arXiv:2009.10629v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02967",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Houdouin_P/0/1/0/all/0/1\">Pierre Houdouin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pascal_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Pascal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jonckheere_M/0/1/0/all/0/1\">Matthieu Jonckheere</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_A/0/1/0/all/0/1\">Andrew Wang</a>",
          "description": "Linear and Quadratic Discriminant Analysis are well-known classical methods\nbut can heavily suffer from non-Gaussian distributions and/or contaminated\ndatasets, mainly because of the underlying Gaussian assumption that is not\nrobust. To fill this gap, this paper presents a new robust discriminant\nanalysis where each data point is drawn by its own arbitrary Elliptically\nSymmetrical (ES) distribution and its own arbitrary scale parameter. Such a\nmodel allows for possibly very heterogeneous, independent but non-identically\ndistributed samples. After deriving a new decision rule, it is shown that\nmaximum-likelihood parameter estimation and classification are very simple,\nfast and robust compared to state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2201.02967",
          "publishedOn": "2022-01-12T00:38:41.813Z",
          "wordCount": 532,
          "title": "Robust classification with flexible discriminant analysis in heterogeneous data. (arXiv:2201.02967v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03283",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Crisan_D/0/1/0/all/0/1\">Dan Crisan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lobbe_A/0/1/0/all/0/1\">Alexander Lobbe</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ortiz_Latorre_S/0/1/0/all/0/1\">Salvador Ortiz-Latorre</a>",
          "description": "The filtering equations govern the evolution of the conditional distribution\nof a signal process given partial, and possibly noisy, observations arriving\nsequentially in time. Their numerical approximation plays a central role in\nmany real-life applications, including numerical weather prediction, finance\nand engineering. One of the classical approaches to approximate the solution of\nthe filtering equations is to use a PDE inspired method, called the\nsplitting-up method, initiated by Gyongy, Krylov, LeGland, among other\ncontributors. This method, and other PDE based approaches, have particular\napplicability for solving low-dimensional problems. In this work we combine\nthis method with a neural network representation. The new methodology is used\nto produce an approximation of the unnormalised conditional distribution of the\nsignal process. We further develop a recursive normalisation procedure to\nrecover the normalised conditional distribution of the signal process. The new\nscheme can be iterated over multiple time steps whilst keeping its asymptotic\nunbiasedness property intact.\n\nWe test the neural network approximations with numerical approximation\nresults for the Kalman and Benes filter.",
          "link": "http://arxiv.org/abs/2201.03283",
          "publishedOn": "2022-01-12T00:38:41.797Z",
          "wordCount": 645,
          "title": "An application of the splitting-up method for the computation of a neural network representation for the solution for the filtering equations. (arXiv:2201.03283v1 [math.PR])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03065",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Si_N/0/1/0/all/0/1\">Nian Si</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zheng_Z/0/1/0/all/0/1\">Zeyu Zheng</a>",
          "description": "We formulate selecting the best optimizing system (SBOS) problems and provide\nsolutions for those problems. In an SBOS problem, a finite number of systems\nare contenders. Inside each system, a continuous decision variable affects the\nsystem's expected performance. An SBOS problem compares different systems based\non their expected performances under their own optimally chosen decision to\nselect the best, without advance knowledge of expected performances of the\nsystems nor the optimizing decision inside each system. We design\neasy-to-implement algorithms that adaptively chooses a system and a choice of\ndecision to evaluate the noisy system performance, sequentially eliminates\ninferior systems, and eventually recommends a system as the best after spending\na user-specified budget. The proposed algorithms integrate the stochastic\ngradient descent method and the sequential elimination method to simultaneously\nexploit the structure inside each system and make comparisons across systems.\nFor the proposed algorithms, we prove exponential rates of convergence to zero\nfor the probability of false selection, as the budget grows to infinity. We\nconduct three numerical examples that represent three practical cases of SBOS\nproblems. Our proposed algorithms demonstrate consistent and stronger\nperformances in terms of the probability of false selection over benchmark\nalgorithms under a range of problem settings and sampling budgets.",
          "link": "http://arxiv.org/abs/2201.03065",
          "publishedOn": "2022-01-12T00:38:41.790Z",
          "wordCount": 629,
          "title": "Selecting the Best Optimizing System. (arXiv:2201.03065v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1\">Frances Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardt_M/0/1/0/all/0/1\">Moritz Hardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1\">John Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>",
          "description": "Although the fairness community has recognized the importance of data,\nresearchers in the area primarily rely on UCI Adult when it comes to tabular\ndata. Derived from a 1994 US Census survey, this dataset has appeared in\nhundreds of research papers where it served as the basis for the development\nand comparison of many algorithmic fairness interventions. We reconstruct a\nsuperset of the UCI Adult data from available US Census sources and reveal\nidiosyncrasies of the UCI Adult dataset that limit its external validity. Our\nprimary contribution is a suite of new datasets derived from US Census surveys\nthat extend the existing data ecosystem for research on fair machine learning.\nWe create prediction tasks relating to income, employment, health,\ntransportation, and housing. The data span multiple years and all states of the\nUnited States, allowing researchers to study temporal shift and geographic\nvariation. We highlight a broad initial sweep of new empirical insights\nrelating to trade-offs between fairness criteria, performance of algorithmic\ninterventions, and the role of distribution shift based on our new datasets.\nOur findings inform ongoing debates, challenge some existing narratives, and\npoint to future research directions. Our datasets are available at\nhttps://github.com/zykls/folktables.",
          "link": "http://arxiv.org/abs/2108.04884",
          "publishedOn": "2022-01-12T00:38:41.638Z",
          "wordCount": 656,
          "title": "Retiring Adult: New Datasets for Fair Machine Learning. (arXiv:2108.04884v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinhui Xu</a>",
          "description": "We study the problem of Differentially Private Stochastic Convex Optimization\n(DP-SCO) with heavy-tailed data. Specifically, we focus on the $\\ell_1$-norm\nlinear regression in the $\\epsilon$-DP model. While most of the previous work\nfocuses on the case where the loss function is Lipschitz, here we only need to\nassume the variates has bounded moments. Firstly, we study the case where the\n$\\ell_2$ norm of data has bounded second order moment. We propose an algorithm\nwhich is based on the exponential mechanism and show that it is possible to\nachieve an upper bound of $\\tilde{O}(\\sqrt{\\frac{d}{n\\epsilon}})$ (with high\nprobability). Next, we relax the assumption to bounded $\\theta$-th order moment\nwith some $\\theta\\in (1, 2)$ and show that it is possible to achieve an upper\nbound of $\\tilde{O}(({\\frac{d}{n\\epsilon}})^\\frac{\\theta-1}{\\theta})$. Our\nalgorithms can also be extended to more relaxed cases where only each\ncoordinate of the data has bounded moments, and we can get an upper bound of\n$\\tilde{O}({\\frac{d}{\\sqrt{n\\epsilon}}})$ and\n$\\tilde{O}({\\frac{d}{({n\\epsilon})^\\frac{\\theta-1}{\\theta}}})$ in the second\nand $\\theta$-th moment case respectively.",
          "link": "http://arxiv.org/abs/2201.03204",
          "publishedOn": "2022-01-12T00:38:41.616Z",
          "wordCount": 586,
          "title": "Differentially Private $\\ell_1$-norm Linear Regression with Heavy-tailed Data. (arXiv:2201.03204v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.03419",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Dostert_M/0/1/0/all/0/1\">Maria Dostert</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jochemko_K/0/1/0/all/0/1\">Katharina Jochemko</a>",
          "description": "We consider the task of reconstructing polytopes with fixed facet directions\nfrom finitely many support function evaluations. We show that for fixed\nsimplicial normal fan the least-squares estimate is given by a convex quadratic\nprogram. We study the geometry of the solution set and give a combinatorial\ncharacterization for the uniqueness of the reconstruction in this case. We\nprovide an algorithm that, under mild assumptions, converges to the unknown\ninput shape as the number of noisy support function evaluations increases. We\nalso discuss limitations of our results if the restriction on the normal fan is\nremoved.",
          "link": "http://arxiv.org/abs/2201.03419",
          "publishedOn": "2022-01-12T00:38:41.376Z",
          "wordCount": 525,
          "title": "Learning polytopes with fixed facet directions. (arXiv:2201.03419v1 [math.MG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.01184",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Polo_F/0/1/0/all/0/1\">Felipe Maia Polo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vicente_R/0/1/0/all/0/1\">Renato Vicente</a>",
          "description": "In supervised learning, training and test datasets are often sampled from\ndistinct distributions. Domain adaptation techniques are thus required.\nCovariate shift adaptation yields good generalization performance when domains\ndiffer only by the marginal distribution of features. Covariate shift\nadaptation is usually implemented using importance weighting, which may fail,\naccording to common wisdom, due to small effective sample sizes (ESS). Previous\nresearch argues this scenario is more common in high-dimensional settings.\nHowever, how effective sample size, dimensionality, and model\nperformance/generalization are formally related in supervised learning,\nconsidering the context of covariate shift adaptation, is still somewhat\nobscure in the literature. Thus, a main challenge is presenting a unified\ntheory connecting those points. Hence, in this paper, we focus on building a\nunified view connecting the ESS, data dimensionality, and generalization in the\ncontext of covariate shift adaptation. Moreover, we also demonstrate how\ndimensionality reduction or feature selection can increase the ESS, and argue\nthat our results support dimensionality reduction before covariate shift\nadaptation as a good practice.",
          "link": "http://arxiv.org/abs/2010.01184",
          "publishedOn": "2022-01-12T00:38:41.368Z",
          "wordCount": 647,
          "title": "Effective Sample Size, Dimensionality, and Generalization in Covariate Shift Adaptation. (arXiv:2010.01184v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03544",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_A/0/1/0/all/0/1\">Alexander Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_K/0/1/0/all/0/1\">Kush Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>",
          "description": "Reward hacking -- where RL agents exploit gaps in misspecified reward\nfunctions -- has been widely observed, but not yet systematically studied. To\nunderstand how reward hacking arises, we construct four RL environments with\nmisspecified rewards. We investigate reward hacking as a function of agent\ncapabilities: model capacity, action space resolution, observation space noise,\nand training time. More capable agents often exploit reward misspecifications,\nachieving higher proxy reward and lower true reward than less capable agents.\nMoreover, we find instances of phase transitions: capability thresholds at\nwhich the agent's behavior qualitatively shifts, leading to a sharp decrease in\nthe true reward. Such phase transitions pose challenges to monitoring the\nsafety of ML systems. To address this, we propose an anomaly detection task for\naberrant policies and offer several baseline detectors.",
          "link": "http://arxiv.org/abs/2201.03544",
          "publishedOn": "2022-01-12T00:38:41.359Z",
          "wordCount": 561,
          "title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models. (arXiv:2201.03544v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gawlikowski_J/0/1/0/all/0/1\">Jakob Gawlikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tassi_C/0/1/0/all/0/1\">Cedrique Rovile Njieutcheu Tassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mohsin Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jongseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Humt_M/0/1/0/all/0/1\">Matthias Humt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianxiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruspe_A/0/1/0/all/0/1\">Anna Kruspe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triebel_R/0/1/0/all/0/1\">Rudolph Triebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_P/0/1/0/all/0/1\">Peter Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1\">Ribana Roscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahzad_M/0/1/0/all/0/1\">Muhammad Shahzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamler_R/0/1/0/all/0/1\">Richard Bamler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>",
          "description": "Due to their increasing spread, confidence in neural network predictions\nbecame more and more important. However, basic neural networks do not deliver\ncertainty estimates or suffer from over or under confidence. Many researchers\nhave been working on understanding and quantifying uncertainty in a neural\nnetwork's prediction. As a result, different types and sources of uncertainty\nhave been identified and a variety of approaches to measure and quantify\nuncertainty in neural networks have been proposed. This work gives a\ncomprehensive overview of uncertainty estimation in neural networks, reviews\nrecent advances in the field, highlights current challenges, and identifies\npotential research opportunities. It is intended to give anyone interested in\nuncertainty estimation in neural networks a broad overview and introduction,\nwithout presupposing prior knowledge in this field. A comprehensive\nintroduction to the most crucial sources of uncertainty is given and their\nseparation into reducible model uncertainty and not reducible data uncertainty\nis presented. The modeling of these uncertainties based on deterministic neural\nnetworks, Bayesian neural networks, ensemble of neural networks, and test-time\ndata augmentation approaches is introduced and different branches of these\nfields as well as the latest developments are discussed. For a practical\napplication, we discuss different measures of uncertainty, approaches for the\ncalibration of neural networks and give an overview of existing baselines and\nimplementations. Different examples from the wide spectrum of challenges in\ndifferent fields give an idea of the needs and challenges regarding\nuncertainties in practical applications. Additionally, the practical\nlimitations of current methods for mission- and safety-critical real world\napplications are discussed and an outlook on the next steps towards a broader\nusage of such methods is given.",
          "link": "http://arxiv.org/abs/2107.03342",
          "publishedOn": "2022-01-12T00:38:41.308Z",
          "wordCount": 747,
          "title": "A Survey of Uncertainty in Deep Neural Networks. (arXiv:2107.03342v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08717",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Goscinski_A/0/1/0/all/0/1\">Alexander Goscinski</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Musil_F/0/1/0/all/0/1\">F&#xe9;lix Musil</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pozdnyakov_S/0/1/0/all/0/1\">Sergey Pozdnyakov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ceriotti_M/0/1/0/all/0/1\">Michele Ceriotti</a>",
          "description": "The input of almost every machine learning algorithm targeting the properties\nof matter at the atomic scale involves a transformation of the list of\nCartesian atomic coordinates into a more symmetric representation. Many of the\nmost popular representations can be seen as an expansion of the symmetrized\ncorrelations of the atom density, and differ mainly by the choice of basis.\nConsiderable effort has been dedicated to the optimization of the basis set,\ntypically driven by heuristic considerations on the behavior of the regression\ntarget. Here we take a different, unsupervised viewpoint, aiming to determine\nthe basis that encodes in the most compact way possible the structural\ninformation that is relevant for the dataset at hand. For each training dataset\nand number of basis functions, one can determine a unique basis that is optimal\nin this sense, and can be computed at no additional cost with respect to the\nprimitive basis by approximating it with splines. We demonstrate that this\nconstruction yields representations that are accurate and computationally\nefficient, particularly when constructing representations that correspond to\nhigh-body order correlations. We present examples that involve both molecular\nand condensed-phase machine-learning models.",
          "link": "http://arxiv.org/abs/2105.08717",
          "publishedOn": "2022-01-12T00:38:41.301Z",
          "wordCount": 642,
          "title": "Optimal radial basis for density-based atomic representations. (arXiv:2105.08717v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.00269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1\">David Heckerman</a>",
          "description": "A Bayesian network is a graphical model that encodes probabilistic\nrelationships among variables of interest. When used in conjunction with\nstatistical techniques, the graphical model has several advantages for data\nanalysis. One, because the model encodes dependencies among all variables, it\nreadily handles situations where some data entries are missing. Two, a Bayesian\nnetwork can be used to learn causal relationships, and hence can be used to\ngain understanding about a problem domain and to predict the consequences of\nintervention. Three, because the model has both a causal and probabilistic\nsemantics, it is an ideal representation for combining prior knowledge (which\noften comes in causal form) and data. Four, Bayesian statistical methods in\nconjunction with Bayesian networks offer an efficient and principled approach\nfor avoiding the overfitting of data. In this paper, we discuss methods for\nconstructing Bayesian networks from prior knowledge and summarize Bayesian\nstatistical methods for using data to improve these models. With regard to the\nlatter task, we describe methods for learning both the parameters and structure\nof a Bayesian network, including techniques for learning with incomplete data.\nIn addition, we relate Bayesian-network methods for learning to techniques for\nsupervised and unsupervised learning. We illustrate the graphical-modeling\napproach using a real-world case study.",
          "link": "http://arxiv.org/abs/2002.00269",
          "publishedOn": "2022-01-12T00:38:41.016Z",
          "wordCount": 689,
          "title": "A Tutorial on Learning With Bayesian Networks. (arXiv:2002.00269v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02745",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_M/0/1/0/all/0/1\">Mostafa Rahmani</a>",
          "description": "This paper focuses on the Matrix Factorization based Clustering (MFC) method\nwhich is one of the few closed form algorithms for the subspace clustering\nproblem. Despite being simple, closed-form, and computation-efficient, MFC can\noutperform the other sophisticated subspace clustering methods in many\nchallenging scenarios. We reveal the connection between MFC and the Innovation\nPursuit (iPursuit) algorithm which was shown to be able to outperform the other\nspectral clustering based methods with a notable margin especially when the\nspan of clusters are close. A novel theoretical study is presented which sheds\nlight on the key performance factors of both algorithms (MFC/iPursuit) and it\nis shown that both algorithms can be robust to notable intersections between\nthe span of clusters. Importantly, in contrast to the theoretical guarantees of\nother algorithms which emphasized on the distance between the subspaces as the\nkey performance factor and without making the innovation assumption, it is\nshown that the performance of MFC/iPursuit mainly depends on the distance\nbetween the innovative components of the clusters.",
          "link": "http://arxiv.org/abs/2201.02745",
          "publishedOn": "2022-01-12T00:38:40.992Z",
          "wordCount": 596,
          "title": "Provable Clustering of a Union of Linear Manifolds Using Optimal Directions. (arXiv:2201.02745v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.04622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hayes_J/0/1/0/all/0/1\">Jamie Hayes</a>",
          "description": "Historically, machine learning methods have not been designed with security\nin mind. In turn, this has given rise to adversarial examples, carefully\nperturbed input samples aimed to mislead detection at test time, which have\nbeen applied to attack spam and malware classification, and more recently to\nattack image classification. Consequently, an abundance of research has been\ndevoted to designing machine learning methods that are robust to adversarial\nexamples. Unfortunately, there are desiderata besides robustness that a secure\nand safe machine learning model must satisfy, such as fairness and privacy.\nRecent work by Song et al. (2019) has shown, empirically, that there exists a\ntrade-off between robust and private machine learning models. Models designed\nto be robust to adversarial examples often overfit on training data to a larger\nextent than standard (non-robust) models. If a dataset contains private\ninformation, then any statistical test that separates training and test data by\nobserving a model's outputs can represent a privacy breach, and if a model\noverfits on training data, these statistical tests become easier.\n\nIn this work, we identify settings where standard models will overfit to a\nlarger extent in comparison to robust models, and as empirically observed in\nprevious works, settings where the opposite behavior occurs. Thus, it is not\nnecessarily the case that privacy must be sacrificed to achieve robustness. The\ndegree of overfitting naturally depends on the amount of data available for\ntraining. We go on to characterize how the training set size factors into the\nprivacy risks exposed by training a robust model on a simple Gaussian data\ntask, and show empirically that our findings hold on image classification\nbenchmark datasets, such as CIFAR-10 and CIFAR-100.",
          "link": "http://arxiv.org/abs/2006.04622",
          "publishedOn": "2022-01-12T00:38:40.985Z",
          "wordCount": 724,
          "title": "Trade-offs between membership privacy & adversarially robust learning. (arXiv:2006.04622v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.02901",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mernyei_P/0/1/0/all/0/1\">P&#xe9;ter Mernyei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cangea_C/0/1/0/all/0/1\">C&#x103;t&#x103;lina Cangea</a>",
          "description": "We present Wiki-CS, a novel dataset derived from Wikipedia for benchmarking\nGraph Neural Networks. The dataset consists of nodes corresponding to Computer\nScience articles, with edges based on hyperlinks and 10 classes representing\ndifferent branches of the field. We use the dataset to evaluate semi-supervised\nnode classification and single-relation link prediction models. Our experiments\nshow that these methods perform well on a new domain, with structural\nproperties different from earlier benchmarks. The dataset is publicly\navailable, along with the implementation of the data pipeline and the benchmark\nexperiments, at https://github.com/pmernyei/wiki-cs-dataset .",
          "link": "http://arxiv.org/abs/2007.02901",
          "publishedOn": "2022-01-12T00:38:40.977Z",
          "wordCount": 561,
          "title": "Wiki-CS: A Wikipedia-Based Benchmark for Graph Neural Networks. (arXiv:2007.02901v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.13164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Luxuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Ting Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yubin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jinqiao Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tao Liu</a>",
          "description": "With the fast development of modern deep learning techniques, the study of\ndynamic systems and neural networks is increasingly benefiting each other in a\nlot of different ways. Since uncertainties often arise in real world\nobservations, SDEs (stochastic differential equations) come to play an\nimportant role. To be more specific, in this paper, we use a collection of SDEs\nequipped with neural networks to predict long-term trend of noisy time series\nwhich has big jump properties and high probability distribution shift. Our\ncontributions are, first, we explored SDEs driven by $\\alpha$-stable L\\'evy\nmotion to model the time series data and solved the problem through neural\nnetwork approximation. Second, we theoretically proved the convergence of the\nmodel and obtained the convergence rate. Finally, we illustrated our method by\napplying it to stock marketing time series prediction and found the convergence\norder of error.",
          "link": "http://arxiv.org/abs/2111.13164",
          "publishedOn": "2022-01-12T00:38:40.969Z",
          "wordCount": 612,
          "title": "L\\'evy Induced Stochastic Differential Equation Equipped with Neural Network for Time Series Forecasting. (arXiv:2111.13164v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1708.03288",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mazumder_R/0/1/0/all/0/1\">Rahul Mazumder</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Radchenko_P/0/1/0/all/0/1\">Peter Radchenko</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dedieu_A/0/1/0/all/0/1\">Antoine Dedieu</a>",
          "description": "We study a seemingly unexpected and relatively less understood overfitting\naspect of a fundamental tool in sparse linear modeling - best subset selection,\nwhich minimizes the residual sum of squares subject to a constraint on the\nnumber of nonzero coefficients. While the best subset selection procedure is\noften perceived as the \"gold standard\" in sparse learning when the signal to\nnoise ratio (SNR) is high, its predictive performance deteriorates when the SNR\nis low. In particular, it is outperformed by continuous shrinkage methods, such\nas ridge regression and the Lasso. We investigate the behavior of best subset\nselection in the high-noise regimes and propose an alternative approach based\non a regularized version of the least-squares criterion. Our proposed\nestimators (a) mitigate, to a large extent, the poor predictive performance of\nbest subset selection in the high-noise regimes; and (b) perform favorably,\nwhile generally delivering substantially sparser models, relative to the best\npredictive models available via ridge regression and the Lasso. We conduct an\nextensive theoretical analysis of the predictive properties of the proposed\napproach and provide justification for its superior predictive performance\nrelative to best subset selection when the noise-level is high. Our estimators\ncan be expressed as solutions to mixed integer second order conic optimization\nproblems and, hence, are amenable to modern computational tools from\nmathematical optimization.",
          "link": "http://arxiv.org/abs/1708.03288",
          "publishedOn": "2022-01-12T00:38:40.959Z",
          "wordCount": 691,
          "title": "Subset Selection with Shrinkage: Sparse Linear Modeling when the SNR is low. (arXiv:1708.03288v4 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.02002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+K_P/0/1/0/all/0/1\">Priyadarshini K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Subhasis Chaudhuri</a>",
          "description": "Discriminative features are crucial for several learning applications, such\nas object detection and classification. Neural networks are extensively used\nfor extracting discriminative features of images and speech signals. However,\nthe lack of large datasets in the haptics domain often limits the applicability\nof such techniques. This paper presents a general framework for the analysis of\nthe discriminative properties of haptic signals. We demonstrate the\neffectiveness of spectral features and a boosted embedding technique in\nenhancing the distinguishability of haptic signals. Experiments indicate our\nframework needs less training data, generalizes well for different predictors,\nand outperforms the related state-of-the-art.",
          "link": "http://arxiv.org/abs/2010.02002",
          "publishedOn": "2022-01-12T00:38:40.921Z",
          "wordCount": 573,
          "title": "Enhancing Haptic Distinguishability of Surface Materials with Boosting Technique. (arXiv:2010.02002v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.03182",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Xu_L/0/1/0/all/0/1\">Lihu Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yao_F/0/1/0/all/0/1\">Fang Yao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yao_Q/0/1/0/all/0/1\">Qiuran Yao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1\">Huiming Zhang</a>",
          "description": "There has been a surge of interest in developing robust estimators for models\nwith heavy-tailed data in statistics and machine learning. This paper proposes\na log-truncated M-estimator for a large family of statistical regressions and\nestablishes its excess risk bound under the condition that the data have\n$(1+\\varepsilon)$-th moment with $\\varepsilon \\in (0,1]$. With an additional\nassumption on the associated risk function, we obtain an $\\ell_2$-error bound\nfor the estimation. Our theorems are applied to establish robust M-estimators\nfor concrete regressions. Besides convex regressions such as quantile\nregression and generalized linear models, many non-convex regressions can also\nbe fit into our theorems, we focus on robust deep neural network regressions,\nwhich can be solved by the stochastic gradient descent algorithms. Simulations\nand real data analysis demonstrate the superiority of log-truncated estimations\nover standard estimations.",
          "link": "http://arxiv.org/abs/2201.03182",
          "publishedOn": "2022-01-12T00:38:40.913Z",
          "wordCount": 568,
          "title": "Non-Asymptotic Guarantees for Robust Statistical Learning under $(1+\\varepsilon)$-th Moment Assumption. (arXiv:2201.03182v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03531",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shaker_A/0/1/0/all/0/1\">Ammar Shaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shujian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onoro_Rubio_D/0/1/0/all/0/1\">Daniel O&#xf1;oro-Rubio</a>",
          "description": "The similarity of feature representations plays a pivotal role in the success\nof problems related to domain adaptation. Feature similarity includes both the\ninvariance of marginal distributions and the closeness of conditional\ndistributions given the desired response $y$ (e.g., class labels).\nUnfortunately, traditional methods always learn such features without fully\ntaking into consideration the information in $y$, which in turn may lead to a\nmismatch of the conditional distributions or the mix-up of discriminative\nstructures underlying data distributions. In this work, we introduce the\nrecently proposed von Neumann conditional divergence to improve the\ntransferability across multiple domains. We show that this new divergence is\ndifferentiable and eligible to easily quantify the functional dependence\nbetween features and $y$. Given multiple source tasks, we integrate this\ndivergence to capture discriminative information in $y$ and design novel\nlearning objectives assuming those source tasks are observed either\nsimultaneously or sequentially. In both scenarios, we obtain favorable\nperformance against state-of-the-art methods in terms of smaller generalization\nerror on new tasks and less catastrophic forgetting on source tasks (in the\nsequential setup).",
          "link": "http://arxiv.org/abs/2108.03531",
          "publishedOn": "2022-01-11T00:39:35.466Z",
          "wordCount": null,
          "title": "Learning to Transfer with von Neumann Conditional Divergence. (arXiv:2108.03531v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.05589",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Segadlo_K/0/1/0/all/0/1\">Kai Segadlo</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Epping_B/0/1/0/all/0/1\">Bastian Epping</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Meegen_A/0/1/0/all/0/1\">Alexander van Meegen</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Dahmen_D/0/1/0/all/0/1\">David Dahmen</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kramer_M/0/1/0/all/0/1\">Michael Kr&#xe4;mer</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Helias_M/0/1/0/all/0/1\">Moritz Helias</a>",
          "description": "Understanding capabilities and limitations of different network architectures\nis of fundamental importance to machine learning. Bayesian inference on\nGaussian processes has proven to be a viable approach for studying recurrent\nand deep networks in the limit of infinite layer width, $n\\to\\infty$. Here we\npresent a unified and systematic derivation of the mean-field theory for both\narchitectures that starts from first principles by employing established\nmethods from statistical physics of disordered systems. The theory elucidates\nthat while the mean-field equations are different with regard to their temporal\nstructure, they yet yield identical Gaussian kernels when readouts are taken at\na single time point or layer, respectively. Bayesian inference applied to\nclassification then predicts identical performance and capabilities for the two\narchitectures. Numerically, we find that convergence towards the mean-field\ntheory is typically slower for recurrent networks than for deep networks and\nthe convergence speed depends non-trivially on the parameters of the weight\nprior as well as the depth or number of time steps, respectively. Our method\nexposes that Gaussian processes are but the lowest order of a systematic\nexpansion in $1/n$. The formalism thus paves the way to investigate the\nfundamental differences between recurrent and deep architectures at finite\nwidths $n$.",
          "link": "http://arxiv.org/abs/2112.05589",
          "publishedOn": "2022-01-11T00:39:30.077Z",
          "wordCount": 652,
          "title": "Unified Field Theory for Deep and Recurrent Neural Networks. (arXiv:2112.05589v2 [cond-mat.dis-nn] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hertrich_C/0/1/0/all/0/1\">Christoph Hertrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1\">Amitabh Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Summa_M/0/1/0/all/0/1\">Marco Di Summa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skutella_M/0/1/0/all/0/1\">Martin Skutella</a>",
          "description": "We contribute to a better understanding of the class of functions that is\nrepresented by a neural network with ReLU activations and a given architecture.\nUsing techniques from mixed-integer optimization, polyhedral theory, and\ntropical geometry, we provide a mathematical counterbalance to the universal\napproximation theorems which suggest that a single hidden layer is sufficient\nfor learning tasks. In particular, we investigate whether the class of exactly\nrepresentable functions strictly increases by adding more layers (with no\nrestrictions on size). This problem has potential impact on algorithmic and\nstatistical aspects because of the insight it provides into the class of\nfunctions represented by neural hypothesis classes. However, to the best of our\nknowledge, this question has not been investigated in the neural network\nliterature. We also present upper bounds on the sizes of neural networks\nrequired to represent functions in these neural hypothesis classes.",
          "link": "http://arxiv.org/abs/2105.14835",
          "publishedOn": "2022-01-11T00:39:30.070Z",
          "wordCount": 631,
          "title": "Towards Lower Bounds on the Depth of ReLU Neural Networks. (arXiv:2105.14835v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02441",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Akyildirim_E/0/1/0/all/0/1\">Erdinc Akyildirim</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Gambara_M/0/1/0/all/0/1\">Matteo Gambara</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Teichmann_J/0/1/0/all/0/1\">Josef Teichmann</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zhou_S/0/1/0/all/0/1\">Syang Zhou</a>",
          "description": "Anomaly detection is the process of identifying abnormal instances or events\nin data sets which deviate from the norm significantly. In this study, we\npropose a signatures based machine learning algorithm to detect rare or\nunexpected items in a given data set of time series type. We present\napplications of signature or randomized signature as feature extractors for\nanomaly detection algorithms; additionally we provide an easy, representation\ntheoretic justification for the construction of randomized signatures. Our\nfirst application is based on synthetic data and aims at distinguishing between\nreal and fake trajectories of stock prices, which are indistinguishable by\nvisual inspection. We also show a real life application by using transaction\ndata from the cryptocurrency market. In this case, we are able to identify pump\nand dump attempts organized on social networks with F1 scores up to 88% by\nmeans of our unsupervised learning algorithm, thus achieving results that are\nclose to the state-of-the-art in the field based on supervised learning.",
          "link": "http://arxiv.org/abs/2201.02441",
          "publishedOn": "2022-01-11T00:39:30.062Z",
          "wordCount": 591,
          "title": "Applications of Signature Methods to Market Anomaly Detection. (arXiv:2201.02441v1 [q-fin.CP])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02300",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Miyaguchi_K/0/1/0/all/0/1\">Kohei Miyaguchi</a>",
          "description": "We are concerned with the problem of hyperparameter selection of offline\npolicy evaluation (OPE). OPE is a key component of offline reinforcement\nlearning, which is a core technology for data-driven decision optimization\nwithout environment simulators. However, the current state-of-the-art OPE\nmethods are not hyperparameter-free, which undermines their utility in\nreal-life applications. We address this issue by introducing a new approximate\nhyperparameter selection (AHS) framework for OPE, which defines a notion of\noptimality (called selection criteria) in a quantitative and interpretable\nmanner without hyperparameters. We then derive four AHS methods each of which\nhas different characteristics such as convergence rate and time complexity.\nFinally, we verify effectiveness and limitation of these methods with a\npreliminary experiment.",
          "link": "http://arxiv.org/abs/2201.02300",
          "publishedOn": "2022-01-11T00:39:30.053Z",
          "wordCount": 543,
          "title": "A Theoretical Framework of Almost Hyperparameter-free Hyperparameter Selection Methods for Offline Policy Evaluation. (arXiv:2201.02300v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2112.03898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zadik_I/0/1/0/all/0/1\">Ilias Zadik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Min Jae Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wein_A/0/1/0/all/0/1\">Alexander S. Wein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>",
          "description": "Clustering is a fundamental primitive in unsupervised learning which gives\nrise to a rich class of computationally-challenging inference tasks. In this\nwork, we focus on the canonical task of clustering d-dimensional Gaussian\nmixtures with unknown (and possibly degenerate) covariance. Recent works (Ghosh\net al. '20; Mao, Wein '21; Davis, Diaz, Wang '21) have established lower bounds\nagainst the class of low-degree polynomial methods and the sum-of-squares (SoS)\nhierarchy for recovering certain hidden structures planted in Gaussian\nclustering instances. Prior work on many similar inference tasks portends that\nsuch lower bounds strongly suggest the presence of an inherent\nstatistical-to-computational gap for clustering, that is, a parameter regime\nwhere the clustering task is statistically possible but no polynomial-time\nalgorithm succeeds.\n\nOne special case of the clustering task we consider is equivalent to the\nproblem of finding a planted hypercube vector in an otherwise random subspace.\nWe show that, perhaps surprisingly, this particular clustering model does not\nexhibit a statistical-to-computational gap, even though the aforementioned\nlow-degree and SoS lower bounds continue to apply in this case. To achieve\nthis, we give a polynomial-time algorithm based on the Lenstra--Lenstra--Lovasz\nlattice basis reduction method which achieves the statistically-optimal sample\ncomplexity of d+1 samples. This result extends the class of problems whose\nconjectured statistical-to-computational gaps can be \"closed\" by \"brittle\"\npolynomial-time algorithms, highlighting the crucial but subtle role of noise\nin the onset of statistical-to-computational gaps.",
          "link": "http://arxiv.org/abs/2112.03898",
          "publishedOn": "2022-01-11T00:39:30.047Z",
          "wordCount": 709,
          "title": "Lattice-Based Methods Surpass Sum-of-Squares in Clustering. (arXiv:2112.03898v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11724",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Cheng_L/0/1/0/all/0/1\">Lu Cheng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Guo_R/0/1/0/all/0/1\">Ruocheng Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>",
          "description": "An important problem in causal inference is to break down the total effect of\na treatment on an outcome into different causal pathways and to quantify the\ncausal effect in each pathway. For instance, in causal fairness, the total\neffect of being a male employee (i.e., treatment) constitutes its direct effect\non annual income (i.e., outcome) and the indirect effect via the employee's\noccupation (i.e., mediator). Causal mediation analysis (CMA) is a formal\nstatistical framework commonly used to reveal such underlying causal\nmechanisms. One major challenge of CMA in observational studies is handling\nconfounders, variables that cause spurious causal relationships among\ntreatment, mediator, and outcome. Conventional methods assume sequential\nignorability that implies all confounders can be measured, which is often\nunverifiable in practice. This work aims to circumvent the stringent sequential\nignorability assumptions and consider hidden confounders. Drawing upon proxy\nstrategies and recent advances in deep learning, we propose to simultaneously\nuncover the latent variables that characterize hidden confounders and estimate\nthe causal effects. Empirical evaluations using both synthetic and\nsemi-synthetic datasets validate the effectiveness of the proposed method. We\nfurther show the potentials of our approach for causal fairness analysis.",
          "link": "http://arxiv.org/abs/2102.11724",
          "publishedOn": "2022-01-11T00:39:30.029Z",
          "wordCount": 648,
          "title": "Causal Mediation Analysis with Hidden Confounders. (arXiv:2102.11724v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1\">Huaiqian You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DElia_M/0/1/0/all/0/1\">Marta D&#x27;Elia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tian Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silling_S/0/1/0/all/0/1\">Stewart Silling</a>",
          "description": "Neural operators have recently become popular tools for designing solution\nmaps between function spaces in the form of neural networks. Differently from\nclassical scientific machine learning approaches that learn parameters of a\nknown partial differential equation (PDE) for a single instance of the input\nparameters at a fixed resolution, neural operators approximate the solution map\nof a family of PDEs. Despite their success, the uses of neural operators are so\nfar restricted to relatively shallow neural networks and confined to learning\nhidden governing laws. In this work, we propose a novel nonlocal neural\noperator, which we refer to as nonlocal kernel network (NKN), that is\nresolution independent, characterized by deep neural networks, and capable of\nhandling a variety of tasks such as learning governing equations and\nclassifying images. Our NKN stems from the interpretation of the neural network\nas a discrete nonlocal diffusion reaction equation that, in the limit of\ninfinite layers, is equivalent to a parabolic nonlocal equation, whose\nstability is analyzed via nonlocal vector calculus. The resemblance with\nintegral forms of neural operators allows NKNs to capture long-range\ndependencies in the feature space, while the continuous treatment of\nnode-to-node interactions makes NKNs resolution independent. The resemblance\nwith neural ODEs, reinterpreted in a nonlocal sense, and the stable network\ndynamics between layers allow for generalization of NKN's optimal parameters\nfrom shallow to deep networks. This fact enables the use of shallow-to-deep\ninitialization techniques. Our tests show that NKNs outperform baseline methods\nin both learning governing equations and image classification tasks and\ngeneralize well to different resolutions and depths.",
          "link": "http://arxiv.org/abs/2201.02217",
          "publishedOn": "2022-01-11T00:39:30.022Z",
          "wordCount": 688,
          "title": "Nonlocal Kernel Network (NKN): a Stable and Resolution-Independent Deep Neural Network. (arXiv:2201.02217v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02283",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhao_W/0/1/0/all/0/1\">Weijie Zhao</a>",
          "description": "We develop the \"generalized consistent weighted sampling\" (GCWS) for hashing\nthe \"powered-GMM\" (pGMM) kernel (with a tuning parameter $p$). It turns out\nthat GCWS provides a numerically stable scheme for applying power\ntransformation on the original data, regardless of the magnitude of $p$ and the\ndata. The power transformation is often effective for boosting the performance,\nin many cases considerably so. We feed the hashed data to neural networks on a\nvariety of public classification datasets and name our method ``GCWSNet''. Our\nextensive experiments show that GCWSNet often improves the classification\naccuracy. Furthermore, it is evident from the experiments that GCWSNet\nconverges substantially faster. In fact, GCWS often reaches a reasonable\naccuracy with merely (less than) one epoch of the training process. This\nproperty is much desired because many applications, such as advertisement\nclick-through rate (CTR) prediction models, or data streams (i.e., data seen\nonly once), often train just one epoch. Another beneficial side effect is that\nthe computations of the first layer of the neural networks become additions\ninstead of multiplications because the input data become binary (and highly\nsparse).\n\nEmpirical comparisons with (normalized) random Fourier features (NRFF) are\nprovided. We also propose to reduce the model size of GCWSNet by count-sketch\nand develop the theory for analyzing the impact of using count-sketch on the\naccuracy of GCWS. Our analysis shows that an ``8-bit'' strategy should work\nwell in that we can always apply an 8-bit count-sketch hashing on the output of\nGCWS hashing without hurting the accuracy much. There are many other ways to\ntake advantage of GCWS when training deep neural networks. For example, one can\napply GCWS on the outputs of the last layer to boost the accuracy of trained\ndeep neural networks.",
          "link": "http://arxiv.org/abs/2201.02283",
          "publishedOn": "2022-01-11T00:39:30.016Z",
          "wordCount": 714,
          "title": "GCWSNet: Generalized Consistent Weighted Sampling for Scalable and Accurate Training of Neural Networks. (arXiv:2201.02283v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02555",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hughes_A/0/1/0/all/0/1\">Aidan J. Hughes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bull_L/0/1/0/all/0/1\">Lawrence A. Bull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_P/0/1/0/all/0/1\">Paul Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dervilis_N/0/1/0/all/0/1\">Nikolaos Dervilis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worden_K/0/1/0/all/0/1\">Keith Worden</a>",
          "description": "Classification models are a fundamental component of physical-asset\nmanagement technologies such as structural health monitoring (SHM) systems and\ndigital twins. Previous work introduced \\textit{risk-based active learning}, an\nonline approach for the development of statistical classifiers that takes into\naccount the decision-support context in which they are applied. Decision-making\nis considered by preferentially querying data labels according to\n\\textit{expected value of perfect information} (EVPI). Although several\nbenefits are gained by adopting a risk-based active learning approach,\nincluding improved decision-making performance, the algorithms suffer from\nissues relating to sampling bias as a result of the guided querying process.\nThis sampling bias ultimately manifests as a decline in decision-making\nperformance during the later stages of active learning, which in turn\ncorresponds to lost resource/utility.\n\nThe current paper proposes two novel approaches to counteract the effects of\nsampling bias: \\textit{semi-supervised learning}, and \\textit{discriminative\nclassification models}. These approaches are first visualised using a synthetic\ndataset, then subsequently applied to an experimental case study, specifically,\nthe Z24 Bridge dataset. The semi-supervised learning approach is shown to have\nvariable performance; with robustness to sampling bias dependent on the\nsuitability of the generative distributions selected for the model with respect\nto each dataset. In contrast, the discriminative classifiers are shown to have\nexcellent robustness to the effects of sampling bias. Moreover, it was found\nthat the number of inspections made during a monitoring campaign, and therefore\nresource expenditure, could be reduced with the careful selection of the\nstatistical classifiers used within a decision-supporting monitoring system.",
          "link": "http://arxiv.org/abs/2201.02555",
          "publishedOn": "2022-01-11T00:39:30.009Z",
          "wordCount": 686,
          "title": "On robust risk-based active-learning algorithms for enhanced decision support. (arXiv:2201.02555v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02539",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pearce_M/0/1/0/all/0/1\">Michael Pearce</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Erosheva_E/0/1/0/all/0/1\">Elena A. Erosheva</a>",
          "description": "Rankings and scores are two common data types used by judges to express\npreferences and/or perceptions of quality in a collection of objects. Numerous\nmodels exist to study data of each type separately, but no unified statistical\nmodel captures both data types simultaneously without first performing data\nconversion. We propose the Mallows-Binomial model to close this gap, which\ncombines a Mallows' $\\phi$ ranking model with Binomial score models through\nshared parameters that quantify object quality, a consensus ranking, and the\nlevel of consensus between judges. We propose an efficient tree-search\nalgorithm to calculate the exact MLE of model parameters, study statistical\nproperties of the model both analytically and through simulation, and apply our\nmodel to real data from an instance of grant panel review that collected both\nscores and partial rankings. Furthermore, we demonstrate how model outputs can\nbe used to rank objects with confidence. The proposed model is shown to\nsensibly combine information from both scores and rankings to quantify object\nquality and measure consensus with appropriate levels of statistical\nuncertainty.",
          "link": "http://arxiv.org/abs/2201.02539",
          "publishedOn": "2022-01-11T00:39:30.003Z",
          "wordCount": 600,
          "title": "A Unified Statistical Learning Model for Rankings and Scores with Application to Grant Panel Review. (arXiv:2201.02539v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/1808.09222",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Ayeni_B/0/1/0/all/0/1\">Babatunde M. Ayeni</a>",
          "description": "A set of introductory notes on the subject of data classification using a\nlinear classifier and least-squares cost function, and the negative effect of\nthe presence of outliers on the decision boundary of the linear discriminant.\nWe also show how a simple scaling could make the outlier less significant,\nthereby obtaining a much better decision boundary. We present some numerical\nresults.",
          "link": "http://arxiv.org/abs/1808.09222",
          "publishedOn": "2022-01-11T00:39:29.987Z",
          "wordCount": 511,
          "title": "Linear classifier, least-squares cost function, and outliers. (arXiv:1808.09222v2 [physics.data-an] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08191",
          "author": "The <a href=\"http://arxiv.org/find/stat/1/au:+Mai_T/0/1/0/all/0/1\">Tien Mai</a>",
          "description": "We study the problem of matrix completion in this paper. A spectral scaled\nStudent prior is exploited to favour the underlying low-rank structure of the\ndata matrix. We provide a thorough theoretical investigation for our approach\nthrough PAC-Bayesian bounds. More precisely, our PAC-Bayesian approach enjoys a\nminimax-optimal oracle inequality which guarantees that our method works well\nunder model misspecification and under general sampling distribution.\nInterestingly, we also provide efficient gradient-based sampling\nimplementations for our approach by using Langevin Monte Carlo. More\nspecifically, we show that our algorithms are significantly faster than Gibbs\nsampler in this problem. To illustrate the attractive features of our inference\nstrategy, some numerical simulations are conducted and an application to image\ninpainting is demonstrated.",
          "link": "http://arxiv.org/abs/2104.08191",
          "publishedOn": "2022-01-11T00:39:29.981Z",
          "wordCount": 554,
          "title": "PAC-Bayesian Matrix Completion with a Spectral Scaled Student Prior. (arXiv:2104.08191v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.13408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duffy_K/0/1/0/all/0/1\">Kate Duffy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandal_T/0/1/0/all/0/1\">Thomas Vandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weile Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemani_R/0/1/0/all/0/1\">Ramakrishna Nemani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1\">Auroop R. Ganguly</a>",
          "description": "Numerical models based on physics represent the state-of-the-art in earth\nsystem modeling and comprise our best tools for generating insights and\npredictions. Despite rapid growth in computational power, the perceived need\nfor higher model resolutions overwhelms the latest-generation computers,\nreducing the ability of modelers to generate simulations for understanding\nparameter sensitivities and characterizing variability and uncertainty. Thus,\nsurrogate models are often developed to capture the essential attributes of the\nfull-blown numerical models. Recent successes of machine learning methods,\nespecially deep learning, across many disciplines offer the possibility that\ncomplex nonlinear connectionist representations may be able to capture the\nunderlying complex structures and nonlinear processes in earth systems. A\ndifficult test for deep learning-based emulation, which refers to function\napproximation of numerical models, is to understand whether they can be\ncomparable to traditional forms of surrogate models in terms of computational\nefficiency while simultaneously reproducing model results in a credible manner.\nA deep learning emulation that passes this test may be expected to perform even\nbetter than simple models with respect to capturing complex processes and\nspatiotemporal dependencies. Here we examine, with a case study in\nsatellite-based remote sensing, the hypothesis that deep learning approaches\ncan credibly represent the simulations from a surrogate model with comparable\ncomputational efficiency. Our results are encouraging in that the deep learning\nemulation reproduces the results with acceptable accuracy and often even faster\nperformance. We discuss the broader implications of our results in light of the\npace of improvements in high-performance implementations of deep learning as\nwell as the growing desire for higher-resolution simulations in the earth\nsciences.",
          "link": "http://arxiv.org/abs/1910.13408",
          "publishedOn": "2022-01-11T00:39:29.975Z",
          "wordCount": 750,
          "title": "A framework for deep learning emulation of numerical models with a case study in satellite remote sensing. (arXiv:1910.13408v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02547",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Carson_W/0/1/0/all/0/1\">William E. Carson IV</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Talbot_A/0/1/0/all/0/1\">Austin Talbot</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carlson_D/0/1/0/all/0/1\">David Carlson</a>",
          "description": "Deep autoencoders are often extended with a supervised or adversarial loss to\nlearn latent representations with desirable properties, such as greater\npredictivity of labels and outcomes or fairness with respects to a sensitive\nvariable. Despite the ubiquity of supervised and adversarial deep latent factor\nmodels, these methods should demonstrate improvement over simpler linear\napproaches to be preferred in practice. This necessitates a reproducible linear\nanalog that still adheres to an augmenting supervised or adversarial objective.\nWe address this methodological gap by presenting methods that augment the\nprincipal component analysis (PCA) objective with either a supervised or an\nadversarial objective and provide analytic and reproducible solutions. We\nimplement these methods in an open-source Python package, AugmentedPCA, that\ncan produce excellent real-world baselines. We demonstrate the utility of these\nfactor models on an open-source, RNA-seq cancer gene expression dataset,\nshowing that augmenting with a supervised objective results in improved\ndownstream classification performance, produces principal components with\ngreater class fidelity, and facilitates identification of genes aligned with\nthe principal axes of data variance with implications to development of\nspecific types of cancer.",
          "link": "http://arxiv.org/abs/2201.02547",
          "publishedOn": "2022-01-11T00:39:29.967Z",
          "wordCount": 619,
          "title": "AugmentedPCA: A Python Package of Supervised and Adversarial Linear Factor Models. (arXiv:2201.02547v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03090",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bartolomaeus_W/0/1/0/all/0/1\">Wiebke Bartolomaeus</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Boutaib_Y/0/1/0/all/0/1\">Youness Boutaib</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nestler_S/0/1/0/all/0/1\">Sandra Nestler</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rauhut_H/0/1/0/all/0/1\">Holger Rauhut</a>",
          "description": "We investigate the functioning of a classifying biological neural network\nfrom the perspective of statistical learning theory, modelled, in a simplified\nsetting, as a continuous-time stochastic recurrent neural network (RNN) with\nidentity activation function. In the purely stochastic (robust) regime, we give\na generalisation error bound that holds with high probability, thus showing\nthat the empirical risk minimiser is the best-in-class hypothesis. We show that\nRNNs retain a partial signature of the paths they are fed as the unique\ninformation exploited for training and classification tasks. We argue that\nthese RNNs are easy to train and robust and back these observations with\nnumerical experiments on both synthetic and real data. We also exhibit a\ntrade-off phenomenon between accuracy and robustness.",
          "link": "http://arxiv.org/abs/2108.03090",
          "publishedOn": "2022-01-11T00:39:29.960Z",
          "wordCount": 574,
          "title": "Path classification by stochastic linear recurrent neural networks. (arXiv:2108.03090v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.04973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Paras Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ajay Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1\">Ion Stoica</a>",
          "description": "Recent work learns contextual representations of source code by\nreconstructing tokens from their context. For downstream semantic understanding\ntasks like summarizing code in English, these representations should ideally\ncapture program functionality. However, we show that the popular\nreconstruction-based BERT model is sensitive to source code edits, even when\nthe edits preserve semantics. We propose ContraCode: a contrastive pre-training\ntask that learns code functionality, not form. ContraCode pre-trains a neural\nnetwork to identify functionally similar variants of a program among many\nnon-equivalent distractors. We scalably generate these variants using an\nautomated source-to-source compiler as a form of data augmentation. Contrastive\npre-training improves JavaScript summarization and TypeScript type inference\naccuracy by 2% to 13%. We also propose a new zero-shot JavaScript code clone\ndetection dataset, showing that ContraCode is both more robust and semantically\nmeaningful. On it, we outperform RoBERTa by 39% AUROC in an adversarial setting\nand up to 5% on natural code.",
          "link": "http://arxiv.org/abs/2007.04973",
          "publishedOn": "2022-01-11T00:39:29.953Z",
          "wordCount": 652,
          "title": "Contrastive Code Representation Learning. (arXiv:2007.04973v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Cheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_P/0/1/0/all/0/1\">Pengwei Tian</a>",
          "description": "Recent advances in AIoT technologies have led to an increasing popularity of\nutilizing machine learning algorithms to detect operational failures for\ncyber-physical systems (CPS). In its basic form, an anomaly detection module\nmonitors the sensor measurements and actuator states from the physical plant,\nand detects anomalies in these measurements to identify abnormal operation\nstatus. Nevertheless, building effective anomaly detection models for CPS is\nrather challenging as the model has to accurately detect anomalies in presence\nof highly complicated system dynamics and unknown amount of sensor noise. In\nthis work, we propose a novel time series anomaly detection method called\nNeural System Identification and Bayesian Filtering (NSIBF) in which a\nspecially crafted neural network architecture is posed for system\nidentification, i.e., capturing the dynamics of CPS in a dynamical state-space\nmodel; then a Bayesian filtering algorithm is naturally applied on top of the\n\"identified\" state-space model for robust anomaly detection by tracking the\nuncertainty of the hidden state of the system recursively over time. We provide\nqualitative as well as quantitative experiments with the proposed method on a\nsynthetic and three real-world CPS datasets, showing that NSIBF compares\nfavorably to the state-of-the-art methods with considerable improvements on\nanomaly detection in CPS.",
          "link": "http://arxiv.org/abs/2106.07992",
          "publishedOn": "2022-01-11T00:39:29.932Z",
          "wordCount": 671,
          "title": "Time Series Anomaly Detection for Cyber-Physical Systems via Neural System Identification and Bayesian Filtering. (arXiv:2106.07992v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.10329",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jeon_W/0/1/0/all/0/1\">Woojay Jeon</a>",
          "description": "This paper proposes a novel acoustic word embedding called Acoustic Neighbor\nEmbeddings where speech or text of arbitrary length are mapped to a vector\nspace of fixed, reduced dimensions by adapting stochastic neighbor embedding\n(SNE) to sequential inputs. The Euclidean distance between coordinates in the\nembedding space reflects the phonetic confusability between their corresponding\nsequences. Two encoder neural networks are trained: an acoustic encoder that\naccepts speech signals in the form of frame-wise subword posterior\nprobabilities obtained from an acoustic model and a text encoder that accepts\ntext in the form of subword transcriptions. Compared to a triplet loss\ncriterion, the proposed method is shown to have more effective gradients for\nneural network training. Experimentally, it also gives more accurate results\nwith low-dimensional embeddings when the two encoder networks are used in\ntandem in a word (name) recognition task, and when the text encoder network is\nused standalone in an approximate phonetic matching task. In particular, in an\nisolated name recognition task depending solely on Euclidean nearest-neighbor\nsearch between the proposed embedding vectors, the recognition accuracy is\nidentical to that of conventional finite state transducer(FST)-based decoding\nusing test data with up to 1 million names in the vocabulary and 40 dimensions\nin the embeddings.",
          "link": "http://arxiv.org/abs/2007.10329",
          "publishedOn": "2022-01-11T00:39:29.923Z",
          "wordCount": 677,
          "title": "Acoustic Neighbor Embeddings. (arXiv:2007.10329v5 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02310",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Radha_S/0/1/0/all/0/1\">Santosh Kumar Radha</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Jao_C/0/1/0/all/0/1\">Casey Jao</a>",
          "description": "The similarity between objects is significant in a broad range of areas.\nWhile similarity can be measured using off-the-shelf distance functions, they\nmay fail to capture the inherent meaning of similarity, which tends to depend\non the underlying data and task. Moreover, conventional distance functions\nlimit the space of similarity measures to be symmetric and do not directly\nallow comparing objects from different spaces. We propose using quantum\nnetworks (GQSim) for learning task-dependent (a)symmetric similarity between\ndata that need not have the same dimensionality. We analyze the properties of\nsuch similarity function analytically (for a simple case) and numerically (for\na complex case) and showthat these similarity measures can extract salient\nfeatures of the data. We also demonstrate that the similarity measure derived\nusing this technique is $(\\epsilon,\\gamma,\\tau)$-good, resulting in\ntheoretically guaranteed performance. Finally, we conclude by applying this\ntechnique for three relevant applications - Classification, Graph Completion,\nGenerative modeling.",
          "link": "http://arxiv.org/abs/2201.02310",
          "publishedOn": "2022-01-11T00:39:29.917Z",
          "wordCount": 564,
          "title": "Generalized quantum similarity learning. (arXiv:2201.02310v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02325",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yoshizawa_G/0/1/0/all/0/1\">Ginga Yoshizawa</a>",
          "description": "In time series data analysis, detecting change points on a real-time basis\n(online) is of great interest in many areas, such as finance, environmental\nmonitoring, and medicine. One promising means to achieve this is the Bayesian\nonline change point detection (BOCPD) algorithm, which has been successfully\nadopted in particular cases in which the time series of interest has a fixed\nbaseline. However, we have found that the algorithm struggles when the baseline\nirreversibly shifts from its initial state. This is because with the original\nBOCPD algorithm, the sensitivity with which a change point can be detected is\ndegraded if the data points are fluctuating at locations relatively far from\nthe original baseline. In this paper, we not only extend the original BOCPD\nalgorithm to be applicable to a time series whose baseline is constantly\nshifting toward unknown values but also visualize why the proposed extension\nworks. To demonstrate the efficacy of the proposed algorithm compared to the\noriginal one, we examine these algorithms on two real-world data sets and six\nsynthetic data sets.",
          "link": "http://arxiv.org/abs/2201.02325",
          "publishedOn": "2022-01-11T00:39:29.898Z",
          "wordCount": 609,
          "title": "Bayesian Online Change Point Detection for Baseline Shifts. (arXiv:2201.02325v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bennett_M/0/1/0/all/0/1\">Michele Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_K/0/1/0/all/0/1\">Karin Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleczyk_E/0/1/0/all/0/1\">Ewa J. Kleczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1\">Rajesh Mehta</a>",
          "description": "Data scientists and statisticians are often at odds when determining the best\napproach, machine learning or statistical modeling, to solve an analytics\nchallenge. However, machine learning and statistical modeling are more cousins\nthan adversaries on different sides of an analysis battleground. Choosing\nbetween the two approaches or in some cases using both is based on the problem\nto be solved and outcomes required as well as the data available for use and\ncircumstances of the analysis. Machine learning and statistical modeling are\ncomplementary, based on similar mathematical principles, but simply using\ndifferent tools in an overall analytics knowledge base. Determining the\npredominant approach should be based on the problem to be solved as well as\nempirical evidence, such as size and completeness of the data, number of\nvariables, assumptions or lack thereof, and expected outcomes such as\npredictions or causality. Good analysts and data scientists should be well\nversed in both techniques and their proper application, thereby using the right\ntool for the right project to achieve the desired results.",
          "link": "http://arxiv.org/abs/2201.02469",
          "publishedOn": "2022-01-11T00:39:29.889Z",
          "wordCount": 608,
          "title": "Similarities and Differences between Machine Learning and Traditional Advanced Statistical Modeling in Healthcare Analytics. (arXiv:2201.02469v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1909.09528",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Christensen_S/0/1/0/all/0/1\">S&#xf6;ren Christensen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Strauch_C/0/1/0/all/0/1\">Claudia Strauch</a>",
          "description": "One of the fundamental assumptions in stochastic control of continuous time\nprocesses is that the dynamics of the underlying (diffusion) process is known.\nThis is, however, usually obviously not fulfilled in practice. On the other\nhand, over the last decades, a rich theory for nonparametric estimation of the\ndrift (and volatility) for continuous time processes has been developed. The\naim of this paper is bringing together techniques from stochastic control with\nmethods from statistics for stochastic processes to find a way to both learn\nthe dynamics of the underlying process and control in a reasonable way at the\nsame time. More precisely, we study a long-term average impulse control\nproblem, a stochastic version of the classical Faustmann timber harvesting\nproblem. One of the problems that immediately arises is an\nexploration-exploitation dilemma as is well known for problems in machine\nlearning. We propose a way to deal with this issue by combining exploration and\nexploitation periods in a suitable way. Our main finding is that this\nconstruction can be based on the rates of convergence of estimators for the\ninvariant density. Using this, we obtain that the average cumulated regret is\nof uniform order $O({T^{-1/3}})$.",
          "link": "http://arxiv.org/abs/1909.09528",
          "publishedOn": "2022-01-11T00:39:29.881Z",
          "wordCount": 649,
          "title": "Nonparametric learning for impulse control problems. (arXiv:1909.09528v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02432",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Llorente_F/0/1/0/all/0/1\">Fernando Llorente</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Martino_L/0/1/0/all/0/1\">Luca Martino</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Read_J/0/1/0/all/0/1\">Jesse Read</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Delgado_Gomez_D/0/1/0/all/0/1\">David Delgado-G&#xf3;mez</a>",
          "description": "In this work, we analyze the noisy importance sampling (IS), i.e., IS working\nwith noisy evaluations of the target density. We present the general framework\nand derive optimal proposal densities for noisy IS estimators. The optimal\nproposals incorporate the information of the variance of the noisy\nrealizations, proposing points in regions where the noise power is higher. We\nalso compare the use of the optimal proposals with previous optimality\napproaches considered in a noisy IS framework.",
          "link": "http://arxiv.org/abs/2201.02432",
          "publishedOn": "2022-01-11T00:39:29.872Z",
          "wordCount": 497,
          "title": "Optimality in Noisy Importance Sampling. (arXiv:2201.02432v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07263",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Guo_Y/0/1/0/all/0/1\">Yongyi Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Coey_D/0/1/0/all/0/1\">Dominic Coey</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Konutgan_M/0/1/0/all/0/1\">Mikael Konutgan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_W/0/1/0/all/0/1\">Wenting Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schoener_C/0/1/0/all/0/1\">Chris Schoener</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Goldman_M/0/1/0/all/0/1\">Matt Goldman</a>",
          "description": "We consider the problem of variance reduction in randomized controlled\ntrials, through the use of covariates correlated with the outcome but\nindependent of the treatment. We propose a machine learning regression-adjusted\ntreatment effect estimator, which we call MLRATE. MLRATE uses machine learning\npredictors of the outcome to reduce estimator variance. It employs\ncross-fitting to avoid overfitting biases, and we prove consistency and\nasymptotic normality under general conditions. MLRATE is robust to poor\npredictions from the machine learning step: if the predictions are uncorrelated\nwith the outcomes, the estimator performs asymptotically no worse than the\nstandard difference-in-means estimator, while if predictions are highly\ncorrelated with outcomes, the efficiency gains are large. In A/A tests, for a\nset of 48 outcome metrics commonly monitored in Facebook experiments the\nestimator has over 70% lower variance than the simple difference-in-means\nestimator, and about 19% lower variance than the common univariate procedure\nwhich adjusts only for pre-experiment values of the outcome.",
          "link": "http://arxiv.org/abs/2106.07263",
          "publishedOn": "2022-01-08T00:37:47.812Z",
          "wordCount": null,
          "title": "Machine Learning for Variance Reduction in Online Experiments. (arXiv:2106.07263v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.05815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ting_K/0/1/0/all/0/1\">Kai Ming Ting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wells_J/0/1/0/all/0/1\">Jonathan R. Wells</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Ye Zhu</a>",
          "description": "Measuring similarity between two objects is the core operation in existing\nclustering algorithms in grouping similar objects into clusters. This paper\nintroduces a new similarity measure called point-set kernel which computes the\nsimilarity between an object and a set of objects. The proposed clustering\nprocedure utilizes this new measure to characterize every cluster grown from a\nseed object. We show that the new clustering procedure is both effective and\nefficient that enables it to deal with large scale datasets. In contrast,\nexisting clustering algorithms are either efficient or effective. In comparison\nwith the state-of-the-art density-peak clustering and scalable kernel k-means\nclustering, we show that the proposed algorithm is more effective and runs\norders of magnitude faster when applying to datasets of millions of data\npoints, on a commonly used computing machine.",
          "link": "http://arxiv.org/abs/2002.05815",
          "publishedOn": "2022-01-08T00:37:47.404Z",
          "wordCount": 577,
          "title": "Point-Set Kernel Clustering. (arXiv:2002.05815v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Orosz_G/0/1/0/all/0/1\">Gy&#xf6;rgy Orosz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szanto_Z/0/1/0/all/0/1\">Zsolt Sz&#xe1;nt&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berkecz_P/0/1/0/all/0/1\">P&#xe9;ter Berkecz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szabo_G/0/1/0/all/0/1\">Gerg&#x151; Szab&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farkas_R/0/1/0/all/0/1\">Rich&#xe1;rd Farkas</a>",
          "description": "Although there are a couple of open-source language processing pipelines\navailable for Hungarian, none of them satisfies the requirements of today's NLP\napplications. A language processing pipeline should consist of close to\nstate-of-the-art lemmatization, morphosyntactic analysis, entity recognition\nand word embeddings. Industrial text processing applications have to satisfy\nnon-functional software quality requirements, what is more, frameworks\nsupporting multiple languages are more and more favored. This paper introduces\nHuSpaCy, an industryready Hungarian language processing pipeline. The presented\ntool provides components for the most important basic linguistic analysis\ntasks. It is open-source and is available under a permissive license. Our\nsystem is built upon spaCy's NLP components which means that it is fast, has a\nrich ecosystem of NLP applications and extensions, comes with extensive\ndocumentation and a well-known API. Besides the overview of the underlying\nmodels, we also present rigorous evaluation on common benchmark datasets. Our\nexperiments confirm that HuSpaCy has high accuracy in all subtasks while\nmaintaining resource-efficient prediction capabilities.",
          "link": "http://arxiv.org/abs/2201.01956",
          "publishedOn": "2022-01-08T00:37:47.380Z",
          "wordCount": 591,
          "title": "HuSpaCy: an industrial-strength Hungarian natural language processing toolkit. (arXiv:2201.01956v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2002.00839",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiangyu Chang</a>",
          "description": "Spectral clustering has been one of the widely used methods for community\ndetection in networks. However, large-scale networks bring computational\nchallenges to the eigenvalue decomposition therein. In this paper, we study the\nspectral clustering using randomized sketching algorithms from a statistical\nperspective, where we typically assume the network data are generated from a\nstochastic block model that is not necessarily of full rank. To do this, we\nfirst use the recently developed sketching algorithms to obtain two randomized\nspectral clustering algorithms, namely, the random projection-based and the\nrandom sampling-based spectral clustering. Then we study the theoretical bounds\nof the resulting algorithms in terms of the approximation error for the\npopulation adjacency matrix, the misclassification error, and the estimation\nerror for the link probability matrix. It turns out that, under mild\nconditions, the randomized spectral clustering algorithms lead to the same\ntheoretical bounds as those of the original spectral clustering algorithm. We\nalso extend the results to degree-corrected stochastic block models. Numerical\nexperiments support our theoretical findings and show the efficiency of\nrandomized methods. A new R package called Rclust is developed and made\navailable to the public.",
          "link": "http://arxiv.org/abs/2002.00839",
          "publishedOn": "2022-01-08T00:37:46.456Z",
          "wordCount": 655,
          "title": "Randomized Spectral Clustering in Large-Scale Stochastic Block Models. (arXiv:2002.00839v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02082",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lacombe_T/0/1/0/all/0/1\">Th&#xe9;o Lacombe</a>",
          "description": "This work studies how the introduction of the entropic regularization term in\nunbalanced Optimal Transport (OT) models may alter their homogeneity with\nrespect to the input measures. We observe that in common settings (including\nbalanced OT and unbalanced OT with Kullback-Leibler divergence to the\nmarginals), although the optimal transport cost itself is not homogeneous,\noptimal transport plans and the so-called Sinkhorn divergences are indeed\nhomogeneous. However, homogeneity does not hold in more general Unbalanced\nRegularized Optimal Transport (UROT) models, for instance those using the Total\nVariation as divergence to the marginals. We propose to modify the entropic\nregularization term to retrieve an UROT model that is homogeneous while\npreserving most properties of the standard UROT model. We showcase the\nimportance of using our Homogeneous UROT (HUROT) model when it comes to\nregularize Optimal Transport with Boundary, a transportation model involving a\nspatially varying divergence to the marginals for which the standard\n(inhomogeneous) UROT model would yield inappropriate behavior.",
          "link": "http://arxiv.org/abs/2201.02082",
          "publishedOn": "2022-01-08T00:37:46.448Z",
          "wordCount": 590,
          "title": "An Homogeneous Unbalanced Regularized Optimal Transport model with applications to Optimal Transport with Boundary. (arXiv:2201.02082v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2103.05277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramanath_R/0/1/0/all/0/1\">Rohan Ramanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keerthi_S/0/1/0/all/0/1\">S. Sathiya Keerthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salomatin_K/0/1/0/all/0/1\">Konstantin Salomatin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_K/0/1/0/all/0/1\">Kinjal Basu</a>",
          "description": "We consider applications involving a large set of instances of projecting\npoints to polytopes. We develop an intuition guided by theoretical and\nempirical analysis to show that when these instances follow certain structures,\na large majority of the projections lie on vertices of the polytopes. To do\nthese projections efficiently we derive a vertex-oriented incremental algorithm\nto project a point onto any arbitrary polytope, as well as give specific\nalgorithms to cater to simplex projection and polytopes where the unit box is\ncut by planes. Such settings are especially useful in web-scale applications\nsuch as optimal matching or allocation problems. Several such problems in\ninternet marketplaces (e-commerce, ride-sharing, food delivery, professional\nservices, advertising, etc.), can be formulated as Linear Programs (LP) with\nsuch polytope constraints that require a projection step in the overall\noptimization process. We show that in the very recent work, the polytopic\nprojection is the most expensive step and our efficient projection algorithms\nhelp in gaining massive improvements in performance.",
          "link": "http://arxiv.org/abs/2103.05277",
          "publishedOn": "2022-01-08T00:37:46.442Z",
          "wordCount": 630,
          "title": "Efficient Vertex-Oriented Polytopic Projection for Web-scale Applications. (arXiv:2103.05277v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dar_Y/0/1/0/all/0/1\">Yehuda Dar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard G. Baraniuk</a>",
          "description": "We study the transfer learning process between two linear regression\nproblems. An important and timely special case is when the regressors are\noverparameterized and perfectly interpolate their training data. We examine a\nparameter transfer mechanism whereby a subset of the parameters of the target\ntask solution are constrained to the values learned for a related source task.\nWe analytically characterize the generalization error of the target task in\nterms of the salient factors in the transfer learning architecture, i.e., the\nnumber of examples available, the number of (free) parameters in each of the\ntasks, the number of parameters transferred from the source to target task, and\nthe relation between the two tasks. Our non-asymptotic analysis shows that the\ngeneralization error of the target task follows a two-dimensional double\ndescent trend (with respect to the number of free parameters in each of the\ntasks) that is controlled by the transfer learning factors. Our analysis points\nto specific cases where the transfer of parameters is beneficial as a\nsubstitute for extra overparameterization (i.e., additional free parameters in\nthe target task). Specifically, we show that the usefulness of a transfer\nlearning setting is fragile and depends on a delicate interplay among the set\nof transferred parameters, the relation between the tasks, and the true\nsolution. We also demonstrate that overparameterized transfer learning is not\nnecessarily more beneficial when the source task is closer or identical to the\ntarget task.",
          "link": "http://arxiv.org/abs/2006.07002",
          "publishedOn": "2022-01-08T00:37:46.422Z",
          "wordCount": 738,
          "title": "Double Double Descent: On Generalization Errors in Transfer Learning between Linear Regression Tasks. (arXiv:2006.07002v7 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16239",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Piotrowski_T/0/1/0/all/0/1\">Tomasz Piotrowski</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cavalcante_R/0/1/0/all/0/1\">Renato L. G. Cavalcante</a>",
          "description": "We derive conditions for the existence of fixed points of nonnegative neural\nnetworks, an important research objective to understand the behavior of neural\nnetworks in modern applications involving autoencoders and loop unrolling\ntechniques, among others. In particular, we show that neural networks with\nnonnegative inputs and nonnegative parameters can be recognized as monotonic\nand (weakly) scalable functions within the framework of nonlinear\nPerron-Frobenius theory. This fact enables us to derive conditions for the\nexistence of a nonempty fixed point set of the nonnegative neural networks, and\nthese conditions are weaker than those obtained recently using arguments in\nconvex analysis, which are typically based on the assumption of nonexpansivity\nof the activation functions. Furthermore, we prove that the shape of the fixed\npoint set of monotonic and weakly scalable neural networks is often an\ninterval, which degenerates to a point for the case of scalable networks. The\nchief results of this paper are verified in numerical simulations, where we\nconsider an autoencoder-type network that first compresses angular power\nspectra in massive MIMO systems, and, second, reconstruct the input spectra\nfrom the compressed signals.",
          "link": "http://arxiv.org/abs/2106.16239",
          "publishedOn": "2022-01-08T00:37:46.415Z",
          "wordCount": 634,
          "title": "Fixed points of nonnegative neural networks. (arXiv:2106.16239v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02510",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peixoto_T/0/1/0/all/0/1\">Tiago P. Peixoto</a>",
          "description": "Network homophily, the tendency of similar nodes to be connected, and\ntransitivity, the tendency of two nodes being connected if they share a common\nneighbor, are conflated properties in network analysis, since one mechanism can\ndrive the other. Here we present a generative model and corresponding inference\nprocedure that are capable of distinguishing between both mechanisms. Our\napproach is based on a variation of the stochastic block model (SBM) with the\naddition of triadic closure edges, and its inference can identify the most\nplausible mechanism responsible for the existence of every edge in the network,\nin addition to the underlying community structure itself. We show how the\nmethod can evade the detection of spurious communities caused solely by the\nformation of triangles in the network, and how it can improve the performance\nof edge prediction when compared to the pure version of the SBM without triadic\nclosure.",
          "link": "http://arxiv.org/abs/2101.02510",
          "publishedOn": "2022-01-08T00:37:46.409Z",
          "wordCount": 635,
          "title": "Disentangling homophily, community structure and triadic closure in networks. (arXiv:2101.02510v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.12337",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ding_D/0/1/0/all/0/1\">Daisy Yi Ding</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Narasimhan_B/0/1/0/all/0/1\">Balasubramanian Narasimhan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tibshirani_R/0/1/0/all/0/1\">Robert Tibshirani</a>",
          "description": "We propose a new method for supervised learning with multiple sets of\nfeatures (\"views\"). Cooperative learning combines the usual squared error loss\nof predictions with an \"agreement\" penalty to encourage the predictions from\ndifferent data views to agree. By varying the weight of the agreement penalty,\nwe get a continuum of solutions that include the well-known early and late\nfusion approaches. Cooperative learning chooses the degree of agreement (or\nfusion) in an adaptive manner, using a validation set or cross-validation to\nestimate test set prediction error. One version of our fitting procedure is\nmodular, where one can choose different fitting mechanisms (e.g. lasso, random\nforests, boosting, neural networks) appropriate for different data views. In\nthe setting of cooperative regularized linear regression, the method combines\nthe lasso penalty with the agreement penalty. The method can be especially\npowerful when the different data views share some underlying relationship in\ntheir signals that we aim to strengthen, while each view has its idiosyncratic\nnoise that we aim to reduce. We illustrate the effectiveness of our proposed\nmethod on simulated and real data examples.",
          "link": "http://arxiv.org/abs/2112.12337",
          "publishedOn": "2022-01-08T00:37:46.403Z",
          "wordCount": 623,
          "title": "Cooperative learning for multi-view analysis. (arXiv:2112.12337v3 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.02172",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dhulipala_S/0/1/0/all/0/1\">Somayajulu L. N. Dhulipala</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shields_M/0/1/0/all/0/1\">Michael D. Shields</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chakroborty_P/0/1/0/all/0/1\">Promit Chakroborty</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jiang_W/0/1/0/all/0/1\">Wen Jiang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Spencer_B/0/1/0/all/0/1\">Benjamin W. Spencer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hales_J/0/1/0/all/0/1\">Jason D. Hales</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Laboure_V/0/1/0/all/0/1\">Vincent M. Laboure</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Prince_Z/0/1/0/all/0/1\">Zachary M. Prince</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bolisetti_C/0/1/0/all/0/1\">Chandrakanth Bolisetti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Che_Y/0/1/0/all/0/1\">Yifeng Che</a>",
          "description": "Tristructural isotropic (TRISO)-coated particle fuel is a robust nuclear fuel\nand determining its reliability is critical for the success of advanced nuclear\ntechnologies. However, TRISO failure probabilities are small and the associated\ncomputational models are expensive. We used coupled active learning,\nmultifidelity modeling, and subset simulation to estimate the failure\nprobabilities of TRISO fuels using several 1D and 2D models. With multifidelity\nmodeling, we replaced expensive high-fidelity (HF) model evaluations with\ninformation fusion from two low-fidelity (LF) models. For the 1D TRISO models,\nwe considered three multifidelity modeling strategies: only Kriging, Kriging LF\nprediction plus Kriging correction, and deep neural network (DNN) LF prediction\nplus Kriging correction. While the results across these multifidelity modeling\nstrategies compared satisfactorily, strategies employing information fusion\nfrom two LF models consistently called the HF model least often. Next, for the\n2D TRISO model, we considered two multifidelity modeling strategies: DNN LF\nprediction plus Kriging correction (data-driven) and 1D TRISO LF prediction\nplus Kriging correction (physics-based). The physics-based strategy, as\nexpected, consistently required the fewest calls to the HF model. However, the\ndata-driven strategy had a lower overall simulation time since the DNN\npredictions are instantaneous, and the 1D TRISO model requires a non-negligible\nsimulation time.",
          "link": "http://arxiv.org/abs/2201.02172",
          "publishedOn": "2022-01-08T00:37:46.396Z",
          "wordCount": 655,
          "title": "Reliability Estimation of an Advanced Nuclear Fuel using Coupled Active Learning, Multifidelity Modeling, and Subset Simulation. (arXiv:2201.02172v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2006.01738",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bolland_A/0/1/0/all/0/1\">Adrien Bolland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boukas_I/0/1/0/all/0/1\">Ioannis Boukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_M/0/1/0/all/0/1\">Mathias Berger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_D/0/1/0/all/0/1\">Damien Ernst</a>",
          "description": "We consider the joint design and control of discrete-time stochastic\ndynamical systems over a finite time horizon. We formulate the problem as a\nmulti-step optimization problem under uncertainty seeking to identify a system\ndesign and a control policy that jointly maximize the expected sum of rewards\ncollected over the time horizon considered. The transition function, the reward\nfunction and the policy are all parametrized, assumed known and differentiable\nwith respect to their parameters. We then introduce a deep reinforcement\nlearning algorithm combining policy gradient methods with model-based\noptimization techniques to solve this problem. In essence, our algorithm\niteratively approximates the gradient of the expected return via Monte-Carlo\nsampling and automatic differentiation and takes projected gradient ascent\nsteps in the space of environment and policy parameters. This algorithm is\nreferred to as Direct Environment and Policy Search (DEPS). We assess the\nperformance of our algorithm in three environments concerned with the design\nand control of a mass-spring-damper system, a small-scale off-grid power system\nand a drone, respectively. In addition, our algorithm is benchmarked against a\nstate-of-the-art deep reinforcement learning algorithm used to tackle joint\ndesign and control problems. We show that DEPS performs at least as well or\nbetter in all three environments, consistently yielding solutions with higher\nreturns in fewer iterations. Finally, solutions produced by our algorithm are\nalso compared with solutions produced by an algorithm that does not jointly\noptimize environment and policy parameters, highlighting the fact that higher\nreturns can be achieved when joint optimization is performed.",
          "link": "http://arxiv.org/abs/2006.01738",
          "publishedOn": "2022-01-08T00:37:46.374Z",
          "wordCount": 732,
          "title": "Jointly Learning Environments and Control Policies with Projected Stochastic Gradient Ascent. (arXiv:2006.01738v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devraj_A/0/1/0/all/0/1\">Adithya M. Devraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1\">Benjamin Van Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kuang Xu</a>",
          "description": "Assuming distributions are Gaussian often facilitates computations that are\notherwise intractable. We consider an agent who is designed to attain a low\ninformation ratio with respect to a bandit environment with a Gaussian prior\ndistribution and a Gaussian likelihood function, but study the agent's\nperformance when applied instead to a Bernoulli bandit. We establish a bound on\nthe increase in Bayesian regret when an agent interacts with the Bernoulli\nbandit, relative to an information-theoretic bound satisfied with the Gaussian\nbandit. If the Gaussian prior distribution and likelihood function are\nsufficiently diffuse, this increase grows with the square-root of the time\nhorizon, and thus the per-timestep increase vanishes. Our results formalize the\nfolklore that so-called Bayesian agents remain effective when instantiated with\ndiffuse misspecified distributions.",
          "link": "http://arxiv.org/abs/2201.01902",
          "publishedOn": "2022-01-08T00:37:46.368Z",
          "wordCount": 542,
          "title": "Gaussian Imagination in Bandit Learning. (arXiv:2201.01902v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.14501",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Geisa_A/0/1/0/all/0/1\">Ali Geisa</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mehta_R/0/1/0/all/0/1\">Ronak Mehta</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Helm_H/0/1/0/all/0/1\">Hayden S. Helm</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dey_J/0/1/0/all/0/1\">Jayanta Dey</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Eaton_E/0/1/0/all/0/1\">Eric Eaton</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dick_J/0/1/0/all/0/1\">Jeffery Dick</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Priebe_C/0/1/0/all/0/1\">Carey E. Priebe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vogelstein_J/0/1/0/all/0/1\">Joshua T. Vogelstein</a>",
          "description": "What is learning? 20$^{st}$ century formalizations of learning theory --\nwhich precipitated revolutions in artificial intelligence -- focus primarily on\n$\\mathit{in-distribution}$ learning, that is, learning under the assumption\nthat the training data are sampled from the same distribution as the evaluation\ndistribution. This assumption renders these theories inadequate for\ncharacterizing 21$^{st}$ century real world data problems, which are typically\ncharacterized by evaluation distributions that differ from the training data\ndistributions (referred to as out-of-distribution learning). We therefore make\na small change to existing formal definitions of learnability by relaxing that\nassumption. We then introduce $\\mathbf{learning\\ efficiency}$ (LE) to quantify\nthe amount a learner is able to leverage data for a given problem, regardless\nof whether it is an in- or out-of-distribution problem. We then define and\nprove the relationship between generalized notions of learnability, and show\nhow this framework is sufficiently general to characterize transfer, multitask,\nmeta, continual, and lifelong learning. We hope this unification helps bridge\nthe gap between empirical practice and theoretical guidance in real world\nproblems. Finally, because biological learning continues to outperform machine\nlearning algorithms on certain OOD challenges, we discuss the limitations of\nthis framework vis-\\'a-vis its ability to formalize biological learning,\nsuggesting multiple avenues for future research.",
          "link": "http://arxiv.org/abs/2109.14501",
          "publishedOn": "2022-01-08T00:37:46.362Z",
          "wordCount": 670,
          "title": "Towards a theory of out-of-distribution learning. (arXiv:2109.14501v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.03685",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Runge_J/0/1/0/all/0/1\">Jakob Runge</a>",
          "description": "The paper introduces a novel conditional independence (CI) based method for\nlinear and nonlinear, lagged and contemporaneous causal discovery from\nobservational time series in the causally sufficient case. Existing CI-based\nmethods such as the PC algorithm and also common methods from other frameworks\nsuffer from low recall and partially inflated false positives for strong\nautocorrelation which is an ubiquitous challenge in time series. The novel\nmethod, PCMCI$^+$, extends PCMCI [Runge et al., 2019b] to include discovery of\ncontemporaneous links. PCMCI$^+$ improves the reliability of CI tests by\noptimizing the choice of conditioning sets and even benefits from\nautocorrelation. The method is order-independent and consistent in the oracle\ncase. A broad range of numerical experiments demonstrates that PCMCI$^+$ has\nhigher adjacency detection power and especially more contemporaneous\norientation recall compared to other methods while better controlling false\npositives. Optimized conditioning sets also lead to much shorter runtimes than\nthe PC algorithm. PCMCI$^+$ can be of considerable use in many real world\napplication scenarios where often time resolutions are too coarse to resolve\ntime delays and strong autocorrelation is present.",
          "link": "http://arxiv.org/abs/2003.03685",
          "publishedOn": "2022-01-08T00:37:46.355Z",
          "wordCount": 641,
          "title": "Discovering contemporaneous and lagged causal relations in autocorrelated nonlinear time series datasets. (arXiv:2003.03685v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.07612",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lyu_H/0/1/0/all/0/1\">Hanbaek Lyu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Strohmeier_C/0/1/0/all/0/1\">Christopher Strohmeier</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Needell_D/0/1/0/all/0/1\">Deanna Needell</a>",
          "description": "Online Tensor Factorization (OTF) is a fundamental tool in learning\nlow-dimensional interpretable features from streaming multi-modal data. While\nvarious algorithmic and theoretical aspects of OTF have been investigated\nrecently, a general convergence guarantee to stationary points of the objective\nfunction without any incoherence or sparsity assumptions is still lacking even\nfor the i.i.d. case. In this work, we introduce a novel algorithm that learns a\nCANDECOMP/PARAFAC (CP) basis from a given stream of tensor-valued data under\ngeneral constraints, including nonnegativity constraints that induce\ninterpretability of the learned CP basis. We prove that our algorithm converges\nalmost surely to the set of stationary points of the objective function under\nthe hypothesis that the sequence of data tensors is generated by an underlying\nMarkov chain. Our setting covers the classical i.i.d. case as well as a wide\nrange of application contexts including data streams generated by independent\nor MCMC sampling. Our result closes a gap between OTF and Online Matrix\nFactorization in global convergence analysis \\commHL{for CP-decompositions}.\nExperimentally, we show that our algorithm converges much faster than standard\nalgorithms for nonnegative tensor factorization tasks on both synthetic and\nreal-world data. Also, we demonstrate the utility of our algorithm on a diverse\nset of examples from an image, video, and time-series data, illustrating how\none may learn qualitatively different CP-dictionaries from the same tensor data\nby exploiting the tensor structure in multiple ways.",
          "link": "http://arxiv.org/abs/2009.07612",
          "publishedOn": "2022-01-08T00:37:46.349Z",
          "wordCount": 683,
          "title": "Online nonnegative CP-dictionary learning for Markovian data. (arXiv:2009.07612v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jadbabaie_A/0/1/0/all/0/1\">Ali Jadbabaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makur_A/0/1/0/all/0/1\">Anuran Makur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1\">Devavrat Shah</a>",
          "description": "In this work, we study empirical risk minimization (ERM) within a federated\nlearning framework, where a central server minimizes an ERM objective function\nusing training data that is stored across $m$ clients. In this setting, the\nFederated Averaging (FedAve) algorithm is the staple for determining\n$\\epsilon$-approximate solutions to the ERM problem. Similar to standard\noptimization algorithms, the convergence analysis of FedAve only relies on\nsmoothness of the loss function in the optimization parameter. However, loss\nfunctions are often very smooth in the training data too. To exploit this\nadditional smoothness, we propose the Federated Low Rank Gradient Descent\n(FedLRGD) algorithm. Since smoothness in data induces an approximate low rank\nstructure on the loss function, our method first performs a few rounds of\ncommunication between the server and clients to learn weights that the server\ncan use to approximate clients' gradients. Then, our method solves the ERM\nproblem at the server using inexact gradient descent. To show that FedLRGD can\nhave superior performance to FedAve, we present a notion of federated oracle\ncomplexity as a counterpart to canonical oracle complexity. Under some\nassumptions on the loss function, e.g., strong convexity in parameter,\n$\\eta$-H\\\"older smoothness in data, etc., we prove that the federated oracle\ncomplexity of FedLRGD scales like $\\phi m(p/\\epsilon)^{\\Theta(d/\\eta)}$ and\nthat of FedAve scales like $\\phi m(p/\\epsilon)^{3/4}$ (neglecting sub-dominant\nfactors), where $\\phi\\gg 1$ is a \"communication-to-computation ratio,\" $p$ is\nthe parameter dimension, and $d$ is the data dimension. Then, we show that when\n$d$ is small and the loss function is sufficiently smooth in the data, FedLRGD\nbeats FedAve in federated oracle complexity. Finally, in the course of\nanalyzing FedLRGD, we also establish a result on low rank approximation of\nlatent variable models.",
          "link": "http://arxiv.org/abs/2201.01954",
          "publishedOn": "2022-01-08T00:37:46.328Z",
          "wordCount": 713,
          "title": "Federated Optimization of Smooth Loss Functions. (arXiv:2201.01954v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01973",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chakraborty_S/0/1/0/all/0/1\">Saptarshi Chakraborty</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Paul_D/0/1/0/all/0/1\">Debolina Paul</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Das_S/0/1/0/all/0/1\">Swagatam Das</a>",
          "description": "The problem of linear predictions has been extensively studied for the past\ncentury under pretty generalized frameworks. Recent advances in the robust\nstatistics literature allow us to analyze robust versions of classical linear\nmodels through the prism of Median of Means (MoM). Combining these approaches\nin a piecemeal way might lead to ad-hoc procedures, and the restricted\ntheoretical conclusions that underpin each individual contribution may no\nlonger be valid. To meet these challenges coherently, in this study, we offer a\nunified robust framework that includes a broad variety of linear prediction\nproblems on a Hilbert space, coupled with a generic class of loss functions.\nNotably, we do not require any assumptions on the distribution of the outlying\ndata points ($\\mathcal{O}$) nor the compactness of the support of the inlying\nones ($\\mathcal{I}$). Under mild conditions on the dual norm, we show that for\nmisspecification level $\\epsilon$, these estimators achieve an error rate of\n$O(\\max\\left\\{|\\mathcal{O}|^{1/2}n^{-1/2}, |\\mathcal{I}|^{1/2}n^{-1}\n\\right\\}+\\epsilon)$, matching the best-known rates in literature. This rate is\nslightly slower than the classical rates of $O(n^{-1/2})$, indicating that we\nneed to pay a price in terms of error rates to obtain robust estimates.\nAdditionally, we show that this rate can be improved to achieve so-called\n``fast rates\" under additional assumptions.",
          "link": "http://arxiv.org/abs/2201.01973",
          "publishedOn": "2022-01-08T00:37:46.322Z",
          "wordCount": 640,
          "title": "Robust Linear Predictions: Analyses of Uniform Concentration, Fast Rates and Model Misspecification. (arXiv:2201.01973v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02115",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Refinetti_M/0/1/0/all/0/1\">Maria Refinetti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Goldt_S/0/1/0/all/0/1\">Sebastian Goldt</a>",
          "description": "Autoencoders are the simplest neural network for unsupervised learning, and\nthus an ideal framework for studying feature learning. While a detailed\nunderstanding of the dynamics of linear autoencoders has recently been\nobtained, the study of non-linear autoencoders has been hindered by the\ntechnical difficulty of handling training data with non-trivial correlations -\na fundamental prerequisite for feature extraction. Here, we study the dynamics\nof feature learning in non-linear, shallow autoencoders. We derive a set of\nasymptotically exact equations that describe the generalisation dynamics of\nautoencoders trained with stochastic gradient descent (SGD) in the limit of\nhigh-dimensional inputs. These equations reveal that autoencoders learn the\nleading principal components of their inputs sequentially. An analysis of the\nlong-time dynamics explains the failure of sigmoidal autoencoders to learn with\ntied weights, and highlights the importance of training the bias in ReLU\nautoencoders. Building on previous results for linear networks, we analyse a\nmodification of the vanilla SGD algorithm which allows learning of the exact\nprincipal components. Finally, we show that our equations accurately describe\nthe generalisation dynamics of non-linear autoencoders on realistic datasets\nsuch as CIFAR10.",
          "link": "http://arxiv.org/abs/2201.02115",
          "publishedOn": "2022-01-08T00:37:46.181Z",
          "wordCount": 614,
          "title": "The dynamics of representation learning in shallow, non-linear autoencoders. (arXiv:2201.02115v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hestness_J/0/1/0/all/0/1\">Joel Hestness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Church_K/0/1/0/all/0/1\">Kenneth Church</a>",
          "description": "This paper proposes an efficient approach to learning disentangled\nrepresentations with causal mechanisms based on the difference of conditional\nprobabilities in original and new distributions. We approximate the difference\nwith models' generalization abilities so that it fits in the standard machine\nlearning framework and can be efficiently computed. In contrast to the\nstate-of-the-art approach, which relies on the learner's adaptation speed to\nnew distribution, the proposed approach only requires evaluating the model's\ngeneralization ability. We provide a theoretical explanation for the advantage\nof the proposed method, and our experiments show that the proposed technique is\n1.9--11.0$\\times$ more sample efficient and 9.4--32.4 times quicker than the\nprevious method on various tasks. The source code is available at\n\\url{https://github.com/yuanpeng16/EDCR}.",
          "link": "http://arxiv.org/abs/2201.01942",
          "publishedOn": "2022-01-08T00:37:46.050Z",
          "wordCount": 542,
          "title": "Efficiently Disentangle Causal Representations. (arXiv:2201.01942v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.09394",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Choi_J/0/1/0/all/0/1\">Jaehyuk Choi</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Ge_D/0/1/0/all/0/1\">Desheng Ge</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Kang_K/0/1/0/all/0/1\">Kyu Ho Kang</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Sohn_S/0/1/0/all/0/1\">Sungbin Sohn</a>",
          "description": "The literature on using yield curves to forecast recessions customarily uses\n10-year--three-month Treasury yield spread without verification on the pair\nselection. This study investigates whether the predictive ability of spread can\nbe improved by letting a machine learning algorithm identify the best maturity\npair and coefficients. Our comprehensive analysis shows that, despite the\nlikelihood gain, the machine learning approach does not significantly improve\nprediction, owing to the estimation error. This is robust to the forecasting\nhorizon, control variable, sample period, and oversampling of the recession\nobservations. Our finding supports the use of the 10-year--three-month spread.",
          "link": "http://arxiv.org/abs/2101.09394",
          "publishedOn": "2022-01-08T00:37:45.996Z",
          "wordCount": 538,
          "title": "Yield Spread Selection in Predicting Recession Probabilities: A Machine Learning Approach. (arXiv:2101.09394v2 [econ.EM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.04652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uehara_M/0/1/0/all/0/1\">Masatoshi Uehara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuezhou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wen Sun</a>",
          "description": "This work studies the question of Representation Learning in RL: how can we\nlearn a compact low-dimensional representation such that on top of the\nrepresentation we can perform RL procedures such as exploration and\nexploitation, in a sample efficient manner. We focus on the low-rank Markov\nDecision Processes (MDPs) where the transition dynamics correspond to a\nlow-rank transition matrix. Unlike prior works that assume the representation\nis known (e.g., linear MDPs), here we need to learn the representation for the\nlow-rank MDP. We study both the online RL and offline RL settings. For the\nonline setting, operating with the same computational oracles used in FLAMBE\n(Agarwal et.al), the state-of-art algorithm for learning representations in\nlow-rank MDPs, we propose an algorithm REP-UCB Upper Confidence Bound driven\nRepresentation learning for RL), which significantly improves the sample\ncomplexity from $\\widetilde{O}( A^9 d^7 / (\\epsilon^{10} (1-\\gamma)^{22}))$ for\nFLAMBE to $\\widetilde{O}( A^2 d^4 / (\\epsilon^2 (1-\\gamma)^{5}) )$ with $d$\nbeing the rank of the transition matrix (or dimension of the ground truth\nrepresentation), $A$ being the number of actions, and $\\gamma$ being the\ndiscounted factor. Notably, REP-UCB is simpler than FLAMBE, as it directly\nbalances the interplay between representation learning, exploration, and\nexploitation, while FLAMBE is an explore-then-commit style approach and has to\nperform reward-free exploration step-by-step forward in time. For the offline\nRL setting, we develop an algorithm that leverages pessimism to learn under a\npartial coverage condition: our algorithm is able to compete against any policy\nas long as it is covered by the offline distribution.",
          "link": "http://arxiv.org/abs/2110.04652",
          "publishedOn": "2022-01-08T00:37:45.942Z",
          "wordCount": 719,
          "title": "Representation Learning for Online and Offline RL in Low-rank MDPs. (arXiv:2110.04652v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alomar_A/0/1/0/all/0/1\">Abdullah Alomar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamadanian_P/0/1/0/all/0/1\">Pouya Hamadanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasr_Esfahany_A/0/1/0/all/0/1\">Arash Nasr-Esfahany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Anish Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alizadeh_M/0/1/0/all/0/1\">Mohammad Alizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1\">Devavrat Shah</a>",
          "description": "Evaluating the real-world performance of network protocols is challenging.\nRandomized control trials (RCT) are expensive and inaccessible to most\nresearchers, while expert-designed simulators fail to capture complex behaviors\nin real networks. We present CausalSim, a data-driven simulator for network\nprotocols that addresses this challenge. Learning network behavior from\nobservational data is complicated due to the bias introduced by the protocols\nused during data collection. CausalSim uses traces from an initial RCT under a\nset of protocols to learn a causal network model, effectively removing the\nbiases present in the data. Using this model, CausalSim can then simulate any\nprotocol over the same traces (i.e., for counterfactual predictions). Key to\nCausalSim is the novel use of adversarial neural network training that exploits\ndistributional invariances that are present due to the training data coming\nfrom an RCT. Our extensive evaluation of CausalSim on both real and synthetic\ndatasets and two use cases, including more than nine months of real data from\nthe Puffer video streaming system, shows that it provides accurate\ncounterfactual predictions, reducing prediction error by 44% and 53% on average\ncompared to expert-designed and standard supervised learning baselines.",
          "link": "http://arxiv.org/abs/2201.01811",
          "publishedOn": "2022-01-08T00:37:45.919Z",
          "wordCount": 626,
          "title": "CausalSim: Toward a Causal Data-Driven Simulator for Network Protocols. (arXiv:2201.01811v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.02037",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Smucler_E/0/1/0/all/0/1\">Ezequiel Smucler</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rotnitzky_A/0/1/0/all/0/1\">Andrea Rotnitzky</a>",
          "description": "We study the selection of adjustment sets for estimating the interventional\nmean under an individualized treatment rule. We assume a non-parametric causal\ngraphical model with, possibly, hidden variables and at least one adjustment\nset comprised of observable variables. Moreover, we assume that observable\nvariables have positive costs associated with them. We define the cost of an\nobservable adjustment set as the sum of the costs of the variables that\ncomprise it. We show that in this setting there exist adjustment sets that are\nminimum cost optimal, in the sense that they yield non-parametric estimators of\nthe interventional mean with the smallest asymptotic variance among those that\ncontrol for observable adjustment sets that have minimum cost. Our results are\nbased on the construction of a special flow network associated with the\noriginal causal graph. We show that a minimum cost optimal adjustment set can\nbe found by computing a maximum flow on the network, and then finding the set\nof vertices that are reachable from the source by augmenting paths. The\noptimaladj Python package implements the algorithms introduced in this paper.",
          "link": "http://arxiv.org/abs/2201.02037",
          "publishedOn": "2022-01-08T00:37:45.912Z",
          "wordCount": 612,
          "title": "A note on efficient minimum cost adjustment sets in causal graphical models. (arXiv:2201.02037v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01793",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yu_L/0/1/0/all/0/1\">Lu Yu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gu_J/0/1/0/all/0/1\">Jiaying Gu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Volgushev_S/0/1/0/all/0/1\">Stanislav Volgushev</a>",
          "description": "Consider a panel data setting where repeated observations on individuals are\navailable. Often it is reasonable to assume that there exist groups of\nindividuals that share similar effects of observed characteristics, but the\ngrouping is typically unknown in advance. We propose a novel approach to\nestimate such unobserved groupings for general panel data models. Our method\nexplicitly accounts for the uncertainty in individual parameter estimates and\nremains computationally feasible with a large number of individuals and/or\nrepeated measurements on each individual. The developed ideas can be applied\neven when individual-level data are not available and only parameter estimates\ntogether with some quantification of uncertainty are given to the researcher.",
          "link": "http://arxiv.org/abs/2201.01793",
          "publishedOn": "2022-01-08T00:37:45.904Z",
          "wordCount": 529,
          "title": "Group structure estimation for panel data -- a general approach. (arXiv:2201.01793v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2110.07557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhemin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongxia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bao Wang</a>",
          "description": "The explicit low-rank regularization, e.g., nuclear norm regularization, has\nbeen widely used in imaging sciences. However, it has been found that implicit\nregularization outperforms explicit ones in various image processing tasks.\nAnother issue is that the fixed explicit regularization limits the\napplicability to broad kinds of images since different images favor different\nfeatures captured by using different explicit regularizations. As such, this\npaper proposes a new adaptive and implicit low-rank regularization that\ncaptures the low-rank prior dynamically from the training data. At the core of\nour new adaptive and implicit low-rank regularization is parameterizing the\nLaplacian matrix in the Dirichlet energy-based regularization with a neural\nnetwork, and we call the proposed model \\textit{AIR-Net}. Theoretically, we\nshow that the adaptive regularization of AIR-Net enhances the implicit\nregularization and vanishes at the end of training. We validate AIR-Net's\neffectiveness on various benchmark tasks, indicating that the AIR-Net is\nparticularly favorable for the scenarios when the missing entries are\nnon-uniform. The code can be found at\n\\href{https://github.com/lizhemin15/AIR-Net}{https://github.com/lizhemin15/AIR-Net}.",
          "link": "http://arxiv.org/abs/2110.07557",
          "publishedOn": "2022-01-08T00:37:45.896Z",
          "wordCount": 628,
          "title": "AIR-Net: Adaptive and Implicit Regularization Neural Network for Matrix Completion. (arXiv:2110.07557v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.12435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rahul Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shroff_N/0/1/0/all/0/1\">Ness B. Shroff</a>",
          "description": "We consider reinforcement learning (RL) in Markov Decision Processes in which\nan agent repeatedly interacts with an environment that is modeled by a\ncontrolled Markov process. At each time step $t$, it earns a reward, and also\nincurs a cost-vector consisting of $M$ costs. We design model-based RL\nalgorithms that maximize the cumulative reward earned over a time horizon of\n$T$ time-steps, while simultaneously ensuring that the average values of the\n$M$ cost expenditures are bounded by agent-specified thresholds\n$c^{ub}_i,i=1,2,\\ldots,M$.\n\nIn order to measure the performance of a reinforcement learning algorithm\nthat satisfies the average cost constraints, we define an $M+1$ dimensional\nregret vector that is composed of its reward regret, and $M$ cost regrets. The\nreward regret measures the sub-optimality in the cumulative reward, while the\n$i$-th component of the cost regret vector is the difference between its $i$-th\ncumulative cost expense and the expected cost expenditures $Tc^{ub}_i$.\n\nWe prove that the expected value of the regret vector of UCRL-CMDP, is\nupper-bounded as $\\tilde{O}\\left(T^{2\\slash 3}\\right)$, where $T$ is the time\nhorizon. We further show how to reduce the regret of a desired subset of the\n$M$ costs, at the expense of increasing the regrets of rewards and the\nremaining costs. To the best of our knowledge, ours is the only work that\nconsiders non-episodic RL under average cost constraints, and derive algorithms\nthat can~\\emph{tune the regret vector} according to the agent's requirements on\nits cost regrets.",
          "link": "http://arxiv.org/abs/2002.12435",
          "publishedOn": "2022-01-08T00:37:45.889Z",
          "wordCount": 719,
          "title": "Learning in Markov Decision Processes under Constraints. (arXiv:2002.12435v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.13838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suk_J/0/1/0/all/0/1\">Joe Suk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kpotufe_S/0/1/0/all/0/1\">Samory Kpotufe</a>",
          "description": "In bandits with distribution shifts, one aims to automatically detect an\nunknown number $L$ of changes in reward distribution, and restart exploration\nwhen necessary. While this problem remained open for many years, a recent\nbreakthrough of Auer et al. (2018, 2019) provide the first adaptive procedure\nto guarantee an optimal (dynamic) regret $\\sqrt{LT}$, for $T$ rounds, with no\nknowledge of $L$. However, not all distributional shifts are equally severe,\ne.g., suppose no best arm switches occur, then we cannot rule out that a regret\n$O(\\sqrt{T})$ may remain possible; in other words, is it possible to achieve\ndynamic regret that optimally scales only with an unknown number of severe\nshifts? This unfortunately has remained elusive, despite various attempts (Auer\net al., 2019, Foster et al., 2020).\n\nWe resolve this problem in the case of two-armed bandits: we derive an\nadaptive procedure that guarantees a dynamic regret of order\n$\\tilde{O}(\\sqrt{\\tilde{L} T})$, where $\\tilde L \\ll L$ captures an unknown\nnumber of severe best arm changes, i.e., with significant switches in rewards,\nand which last sufficiently long to actually require a restart. As a\nconsequence, for any number $L$ of distributional shifts outside of these\nsevere shifts, our procedure achieves regret just $\\tilde{O}(\\sqrt{T})\\ll\n\\tilde{O}(\\sqrt{LT})$.\n\nFinally, we note that our notion of severe shift applies in both classical\nsettings of stochastic switching bandits and of adversarial bandits.",
          "link": "http://arxiv.org/abs/2112.13838",
          "publishedOn": "2022-01-07T00:40:42.967Z",
          "wordCount": 665,
          "title": "Tracking Most Severe Arm Changes in Bandits. (arXiv:2112.13838v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.13264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1\">Rishabh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarzer_M/0/1/0/all/0/1\">Max Schwarzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1\">Pablo Samuel Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellemare_M/0/1/0/all/0/1\">Marc G. Bellemare</a>",
          "description": "Deep reinforcement learning (RL) algorithms are predominantly evaluated by\ncomparing their relative performance on a large suite of tasks. Most published\nresults on deep RL benchmarks compare point estimates of aggregate performance\nsuch as mean and median scores across tasks, ignoring the statistical\nuncertainty implied by the use of a finite number of training runs. Beginning\nwith the Arcade Learning Environment (ALE), the shift towards\ncomputationally-demanding benchmarks has led to the practice of evaluating only\na small number of runs per task, exacerbating the statistical uncertainty in\npoint estimates. In this paper, we argue that reliable evaluation in the few\nrun deep RL regime cannot ignore the uncertainty in results without running the\nrisk of slowing down progress in the field. We illustrate this point using a\ncase study on the Atari 100k benchmark, where we find substantial discrepancies\nbetween conclusions drawn from point estimates alone versus a more thorough\nstatistical analysis. With the aim of increasing the field's confidence in\nreported results with a handful of runs, we advocate for reporting interval\nestimates of aggregate performance and propose performance profiles to account\nfor the variability in results, as well as present more robust and efficient\naggregate metrics, such as interquartile mean scores, to achieve small\nuncertainty in results. Using such statistical tools, we scrutinize performance\nevaluations of existing algorithms on other widely used RL benchmarks including\nthe ALE, Procgen, and the DeepMind Control Suite, again revealing discrepancies\nin prior comparisons. Our findings call for a change in how we evaluate\nperformance in deep RL, for which we present a more rigorous evaluation\nmethodology, accompanied with an open-source library rliable, to prevent\nunreliable results from stagnating the field.",
          "link": "http://arxiv.org/abs/2108.13264",
          "publishedOn": "2022-01-07T00:40:42.922Z",
          "wordCount": 780,
          "title": "Deep Reinforcement Learning at the Edge of the Statistical Precipice. (arXiv:2108.13264v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01658",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Vaca_Ramirez_F/0/1/0/all/0/1\">Felipe Vaca-Ram&#xed;rez</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Peixoto_T/0/1/0/all/0/1\">Tiago P. Peixoto</a>",
          "description": "We perform a systematic analysis of the quality of fit of the stochastic\nblock model (SBM) for 275 empirical networks spanning a wide range of domains\nand orders of size magnitude. We employ posterior predictive model checking as\na criterion to assess the quality of fit, which involves comparing networks\ngenerated by the inferred model with the empirical network, according to a set\nof network descriptors. We observe that the SBM is capable of providing an\naccurate description for the majority of networks considered, but falls short\nof saturating all modeling requirements. In particular, networks possessing a\nlarge diameter and slow-mixing random walks tend to be badly described by the\nSBM. However, contrary to what is often assumed, networks with a high abundance\nof triangles can be well described by the SBM in many cases. We demonstrate\nthat simple network descriptors can be used to evaluate whether or not the SBM\ncan provide a sufficiently accurate representation, potentially pointing to\npossible model extensions that can systematically improve the expressiveness of\nthis class of models.",
          "link": "http://arxiv.org/abs/2201.01658",
          "publishedOn": "2022-01-07T00:40:42.843Z",
          "wordCount": 624,
          "title": "Systematic assessment of the quality of fit of the stochastic block model for empirical networks. (arXiv:2201.01658v1 [physics.soc-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2111.03306",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mojiri_A/0/1/0/all/0/1\">Arezou Mojiri</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Khalili_A/0/1/0/all/0/1\">Abbas Khalili</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hamadani_A/0/1/0/all/0/1\">Ali Zeinal Hamadani</a>",
          "description": "In binary classification, imbalance refers to situations in which one class\nis heavily under-represented. This issue is due to either a data collection\nprocess or because one class is indeed rare in a population. Imbalanced\nclassification frequently arises in applications such as biology, medicine,\nengineering, and social sciences. In this paper, for the first time, we\ntheoretically study the impact of imbalance class sizes on the linear\ndiscriminant analysis (LDA) in high dimensions. We show that due to data\nscarcity in one class, referred to as the minority class, and\nhigh-dimensionality of the feature space, the LDA ignores the minority class\nyielding a maximum misclassification rate. We then propose a new construction\nof hard-thresholding rules based on a data splitting technique that reduces the\nlarge difference between the misclassification rates. We show that the proposed\nmethod is asymptotically optimal. We further study two well-known sparse\nversions of the LDA in imbalanced cases. We evaluate the finite-sample\nperformance of different methods using simulations and by analyzing two real\ndata sets. The results show that our method either outperforms its competitors\nor has comparable performance based on a much smaller subset of selected\nfeatures, while being computationally more efficient.",
          "link": "http://arxiv.org/abs/2111.03306",
          "publishedOn": "2022-01-07T00:40:42.836Z",
          "wordCount": 650,
          "title": "New Hard-thresholding Rules based on Data Splitting in High-dimensional Imbalanced Classification. (arXiv:2111.03306v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14539",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chatzimparmpas_A/0/1/0/all/0/1\">Angelos Chatzimparmpas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_R/0/1/0/all/0/1\">Rafael M. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kucher_K/0/1/0/all/0/1\">Kostiantyn Kucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerren_A/0/1/0/all/0/1\">Andreas Kerren</a>",
          "description": "The machine learning (ML) life cycle involves a series of iterative steps,\nfrom the effective gathering and preparation of the data, including complex\nfeature engineering processes, to the presentation and improvement of results,\nwith various algorithms to choose from in every step. Feature engineering in\nparticular can be very beneficial for ML, leading to numerous improvements such\nas boosting the predictive results, decreasing computational times, reducing\nexcessive noise, and increasing the transparency behind the decisions taken\nduring the training. Despite that, while several visual analytics tools exist\nto monitor and control the different stages of the ML life cycle (especially\nthose related to data and algorithms), feature engineering support remains\ninadequate. In this paper, we present FeatureEnVi, a visual analytics system\nspecifically designed to assist with the feature engineering process. Our\nproposed system helps users to choose the most important feature, to transform\nthe original features into powerful alternatives, and to experiment with\ndifferent feature generation combinations. Additionally, data space slicing\nallows users to explore the impact of features on both local and global scales.\nFeatureEnVi utilizes multiple automatic feature selection techniques;\nfurthermore, it visually guides users with statistical evidence about the\ninfluence of each feature (or subsets of features). The final outcome is the\nextraction of heavily engineered features, evaluated by multiple validation\nmetrics. The usefulness and applicability of FeatureEnVi are demonstrated with\ntwo use cases and a case study. We also report feedback from interviews with\ntwo ML experts and a visualization researcher who assessed the effectiveness of\nour system.",
          "link": "http://arxiv.org/abs/2103.14539",
          "publishedOn": "2022-01-07T00:40:38.437Z",
          "wordCount": 743,
          "title": "FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise Selection and Semi-Automatic Extraction Approaches. (arXiv:2103.14539v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07636",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_W/0/1/0/all/0/1\">Wenkai Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lu_J/0/1/0/all/0/1\">Jie Lu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sutherland_D/0/1/0/all/0/1\">Danica J. Sutherland</a>",
          "description": "Modern kernel-based two-sample tests have shown great success in\ndistinguishing complex, high-dimensional distributions with appropriate learned\nkernels. Previous work has demonstrated that this kernel learning procedure\nsucceeds, assuming a considerable number of observed samples from each\ndistribution. In realistic scenarios with very limited numbers of data samples,\nhowever, it can be challenging to identify a kernel powerful enough to\ndistinguish complex distributions. We address this issue by introducing the\nproblem of meta two-sample testing (M2ST), which aims to exploit (abundant)\nauxiliary data on related tasks to find an algorithm that can quickly identify\na powerful test on new target tasks. We propose two specific algorithms for\nthis task: a generic scheme which improves over baselines and a more tailored\napproach which performs even better. We provide both theoretical justification\nand empirical evidence that our proposed meta-testing schemes out-perform\nlearning kernel-based tests directly from scarce observations, and identify\nwhen such schemes will be successful.",
          "link": "http://arxiv.org/abs/2106.07636",
          "publishedOn": "2022-01-07T00:40:38.429Z",
          "wordCount": 631,
          "title": "Meta Two-Sample Testing: Learning Kernels for Testing with Limited Data. (arXiv:2106.07636v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04520",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruihan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1\">Kilian Q. Weinberger</a>",
          "description": "Machine learning models often encounter distribution shifts when deployed in\nthe real world. In this paper, we focus on adaptation to label distribution\nshift in the online setting, where the test-time label distribution is\ncontinually changing and the model must dynamically adapt to it without\nobserving the true label. Leveraging a novel analysis, we show that the lack of\ntrue label does not hinder estimation of the expected test loss, which enables\nthe reduction of online label shift adaptation to conventional online learning.\nInformed by this observation, we propose adaptation algorithms inspired by\nclassical online learning techniques such as Follow The Leader (FTL) and Online\nGradient Descent (OGD) and derive their regret bounds. We empirically verify\nour findings under both simulated and real world label distribution shifts and\nshow that OGD is particularly effective and robust to a variety of challenging\nlabel shift scenarios.",
          "link": "http://arxiv.org/abs/2107.04520",
          "publishedOn": "2022-01-07T00:40:38.422Z",
          "wordCount": 592,
          "title": "Online Adaptation to Label Distribution Shift. (arXiv:2107.04520v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.09416",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinxing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qiang Cheng</a>",
          "description": "Feature selection, as a vital dimension reduction technique, reduces data\ndimension by identifying an essential subset of input features, which can\nfacilitate interpretable insights into learning and inference processes.\nAlgorithmic stability is a key characteristic of an algorithm regarding its\nsensitivity to perturbations of input samples. In this paper, we propose an\ninnovative unsupervised feature selection algorithm attaining this stability\nwith provable guarantees. The architecture of our algorithm consists of a\nfeature scorer and a feature selector. The scorer trains a neural network (NN)\nto globally score all the features, and the selector adopts a dependent sub-NN\nto locally evaluate the representation abilities for selecting features.\nFurther, we present algorithmic stability analysis and show that our algorithm\nhas a performance guarantee via a generalization error bound. Extensive\nexperimental results on real-world datasets demonstrate superior generalization\nperformance of our proposed algorithm to strong baseline methods. Also, the\nproperties revealed by our theoretical analysis and the stability of our\nalgorithm-selected features are empirically confirmed.",
          "link": "http://arxiv.org/abs/2010.09416",
          "publishedOn": "2022-01-07T00:40:38.407Z",
          "wordCount": 619,
          "title": "Algorithmic Stability and Generalization of an Unsupervised Feature Selection Algorithm. (arXiv:2010.09416v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Do_V/0/1/0/all/0/1\">Virginie Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corbett_Davies_S/0/1/0/all/0/1\">Sam Corbett-Davies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atif_J/0/1/0/all/0/1\">Jamal Atif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usunier_N/0/1/0/all/0/1\">Nicolas Usunier</a>",
          "description": "Recommender systems are facing scrutiny because of their growing impact on\nthe opportunities we have access to. Current audits for fairness are limited to\ncoarse-grained parity assessments at the level of sensitive groups. We propose\nto audit for envy-freeness, a more granular criterion aligned with individual\npreferences: every user should prefer their recommendations to those of other\nusers. Since auditing for envy requires to estimate the preferences of users\nbeyond their existing recommendations, we cast the audit as a new pure\nexploration problem in multi-armed bandits. We propose a sample-efficient\nalgorithm with theoretical guarantees that it does not deteriorate user\nexperience. We also study the trade-offs achieved on real-world recommendation\ndatasets.",
          "link": "http://arxiv.org/abs/2104.14527",
          "publishedOn": "2022-01-07T00:40:38.241Z",
          "wordCount": 592,
          "title": "Online certification of preference-based fairness for personalized recommender systems. (arXiv:2104.14527v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Sixu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qinbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhaomin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bingsheng He</a>",
          "description": "This paper presents and characterizes an Open Application Repository for\nFederated Learning (OARF), a benchmark suite for federated machine learning\nsystems. Previously available benchmarks for federated learning have focused\nmainly on synthetic datasets and use a limited number of applications. OARF\nmimics more realistic application scenarios with publicly available data sets\nas different data silos in image, text and structured data. Our\ncharacterization shows that the benchmark suite is diverse in data size,\ndistribution, feature distribution and learning task complexity. The extensive\nevaluations with reference implementations show the future research\nopportunities for important aspects of federated learning systems. We have\ndeveloped reference implementations, and evaluated the important aspects of\nfederated learning, including model accuracy, communication cost, throughput\nand convergence time. Through these evaluations, we discovered some interesting\nfindings such as federated learning can effectively increase end-to-end\nthroughput.",
          "link": "http://arxiv.org/abs/2006.07856",
          "publishedOn": "2022-01-07T00:40:38.190Z",
          "wordCount": 607,
          "title": "The OARF Benchmark Suite: Characterization and Implications for Federated Learning Systems. (arXiv:2006.07856v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_U/0/1/0/all/0/1\">Uiwon Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heeseung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1\">Dahuin Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1\">Hyemi Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyungyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>",
          "description": "Generative adversarial networks (GANs) with clustered latent spaces can\nperform conditional generation in a completely unsupervised manner. In the real\nworld, the salient attributes of unlabeled data can be imbalanced. However,\nmost of existing unsupervised conditional GANs cannot cluster attributes of\nthese data in their latent spaces properly because they assume uniform\ndistributions of the attributes. To address this problem, we theoretically\nderive Stein latent optimization that provides reparameterizable gradient\nestimations of the latent distribution parameters assuming a Gaussian mixture\nprior in a continuous latent space. Structurally, we introduce an encoder\nnetwork and novel unsupervised conditional contrastive loss to ensure that data\ngenerated from a single mixture component represent a single attribute. We\nconfirm that the proposed method, named Stein Latent Optimization for GANs\n(SLOGAN), successfully learns balanced or imbalanced attributes and achieves\nstate-of-the-art unsupervised conditional generation performance even in the\nabsence of attribute information (e.g., the imbalance ratio). Moreover, we\ndemonstrate that the attributes to be learned can be manipulated using a small\namount of probe data.",
          "link": "http://arxiv.org/abs/2106.05319",
          "publishedOn": "2022-01-07T00:40:38.101Z",
          "wordCount": 639,
          "title": "Stein Latent Optimization for Generative Adversarial Networks. (arXiv:2106.05319v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01728",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Elmahdy_A/0/1/0/all/0/1\">Adel Elmahdy</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ahn_J/0/1/0/all/0/1\">Junhyung Ahn</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Suh_C/0/1/0/all/0/1\">Changho Suh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mohajer_S/0/1/0/all/0/1\">Soheil Mohajer</a>",
          "description": "We consider a matrix completion problem that exploits social or item\nsimilarity graphs as side information. We develop a universal, parameter-free,\nand computationally efficient algorithm that starts with hierarchical graph\nclustering and then iteratively refines estimates both on graph clustering and\nmatrix ratings. Under a hierarchical stochastic block model that well respects\npractically-relevant social graphs and a low-rank rating matrix model (to be\ndetailed), we demonstrate that our algorithm achieves the information-theoretic\nlimit on the number of observed matrix entries (i.e., optimal sample\ncomplexity) that is derived by maximum likelihood estimation together with a\nlower-bound impossibility result. One consequence of this result is that\nexploiting the hierarchical structure of social graphs yields a substantial\ngain in sample complexity relative to the one that simply identifies different\ngroups without resorting to the relational structure across them. We conduct\nextensive experiments both on synthetic and real-world datasets to corroborate\nour theoretical results as well as to demonstrate significant performance\nimprovements over other matrix completion algorithms that leverage graph side\ninformation.",
          "link": "http://arxiv.org/abs/2201.01728",
          "publishedOn": "2022-01-07T00:40:37.923Z",
          "wordCount": 644,
          "title": "Matrix Completion with Hierarchical Graph Side Information. (arXiv:2201.01728v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2007.12098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_N/0/1/0/all/0/1\">Neha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Karren Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uhler_C/0/1/0/all/0/1\">Caroline Uhler</a>",
          "description": "In this paper, we present Super-OT, a novel approach to computational lineage\ntracing that combines a supervised learning framework with optimal transport\nbased on Generative Adversarial Networks (GANs). Unlike previous approaches to\nlineage tracing, Super-OT has the flexibility to integrate paired data. We\nbenchmark Super-OT based on single-cell RNA-seq data against Waddington-OT, a\npopular approach for lineage tracing that also employs optimal transport. We\nshow that Super-OT achieves gains over Waddington-OT in predicting the class\noutcome of cells during differentiation, since it allows the integration of\nadditional information during training.",
          "link": "http://arxiv.org/abs/2007.12098",
          "publishedOn": "2022-01-07T00:40:37.915Z",
          "wordCount": 573,
          "title": "Optimal Transport using GANs for Lineage Tracing. (arXiv:2007.12098v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01689",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Davison_A/0/1/0/all/0/1\">Andrew Davison</a>",
          "description": "A common approach to solving tasks, such as node classification or link\nprediction, on a large network begins by learning a Euclidean embedding of the\nnodes of the network, from which regular machine learning methods can be\napplied. For unsupervised random walk methods such as DeepWalk and node2vec,\nadding a $\\ell_2$ penalty on the embedding vectors to the loss leads to\nimproved downstream task performance. In this paper we study the effects of\nthis regularization and prove that, under exchangeability assumptions on the\ngraph, it asymptotically leads to learning a nuclear-norm-type penalized\ngraphon. In particular, the exact form of the penalty depends on the choice of\nsubsampling method used within stochastic gradient descent to learn the\nembeddings. We also illustrate empirically that concatenating node covariates\nto $\\ell_2$ regularized node2vec embeddings leads to comparable, if not\nsuperior, performance to methods which incorporate node covariates and the\nnetwork structure in a non-linear manner.",
          "link": "http://arxiv.org/abs/2201.01689",
          "publishedOn": "2022-01-07T00:40:37.893Z",
          "wordCount": 575,
          "title": "Asymptotics of $\\ell_2$ Regularized Network Embeddings. (arXiv:2201.01689v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1910.03134",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zapata_J/0/1/0/all/0/1\">Javier Zapata</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Oh_S/0/1/0/all/0/1\">Sang-Yun Oh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Petersen_A/0/1/0/all/0/1\">Alexander Petersen</a>",
          "description": "The covariance structure of multivariate functional data can be highly\ncomplex, especially if the multivariate dimension is large, making extensions\nof statistical methods for standard multivariate data to the functional data\nsetting challenging. For example, Gaussian graphical models have recently been\nextended to the setting of multivariate functional data by applying\nmultivariate methods to the coefficients of truncated basis expansions.\nHowever, a key difficulty compared to multivariate data is that the covariance\noperator is compact, and thus not invertible. The methodology in this paper\naddresses the general problem of covariance modeling for multivariate\nfunctional data, and functional Gaussian graphical models in particular. As a\nfirst step, a new notion of separability for the covariance operator of\nmultivariate functional data is proposed, termed partial separability, leading\nto a novel Karhunen-Lo\\`eve-type expansion for such data. Next, the partial\nseparability structure is shown to be particularly useful in order to provide a\nwell-defined functional Gaussian graphical model that can be identified with a\nsequence of finite-dimensional graphical models, each of identical fixed\ndimension. This motivates a simple and efficient estimation procedure through\napplication of the joint graphical lasso. Empirical performance of the method\nfor graphical model estimation is assessed through simulation and analysis of\nfunctional brain connectivity during a motor task. %Empirical performance of\nthe method for graphical model estimation is assessed through simulation and\nanalysis of functional brain connectivity during a motor task.",
          "link": "http://arxiv.org/abs/1910.03134",
          "publishedOn": "2022-01-07T00:40:37.886Z",
          "wordCount": 699,
          "title": "Partial Separability and Functional Graphical Models for Multivariate Gaussian Processes. (arXiv:1910.03134v4 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Ningyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuoguang Yang</a>",
          "description": "In the multi-armed bandit framework, there are two formulations that are\ncommonly employed to handle time-varying reward distributions: adversarial\nbandit and nonstationary bandit. Although their oracles, algorithms, and regret\nanalysis differ significantly, we provide a unified formulation in this paper\nthat smoothly bridges the two as special cases. The formulation uses an oracle\nthat takes the best-fixed arm within time windows. Depending on the window\nsize, it turns into the oracle in hindsight in the adversarial bandit and\ndynamic oracle in the nonstationary bandit. We provide algorithms that attain\nthe optimal regret with the matching lower bound.",
          "link": "http://arxiv.org/abs/2201.01628",
          "publishedOn": "2022-01-07T00:40:37.878Z",
          "wordCount": 511,
          "title": "Bridging Adversarial and Nonstationary Multi-armed Bandit. (arXiv:2201.01628v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.11598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khiari_J/0/1/0/all/0/1\">Jihed Khiari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olaverri_Monreal_C/0/1/0/all/0/1\">Cristina Olaverri-Monreal</a>",
          "description": "Travel time is a crucial measure in transportation. Accurate travel time\nprediction is also fundamental for operation and advanced information systems.\nA variety of solutions exist for short-term travel time predictions such as\nsolutions that utilize real-time GPS data and optimization methods to track the\npath of a vehicle. However, reliable long-term predictions remain challenging.\nWe show in this paper the applicability and usefulness of travel time i.e.\ndelivery time prediction for postal services. We investigate several methods\nsuch as linear regression models and tree based ensembles such as random\nforest, bagging, and boosting, that allow to predict delivery time by\nconducting extensive experiments and considering many usability scenarios.\nResults reveal that travel time prediction can help mitigate high delays in\npostal services. We show that some boosting algorithms, such as light gradient\nboosting and catboost, have a higher performance in terms of accuracy and\nruntime efficiency than other baselines such as linear regression models,\nbagging regressor and random forest.",
          "link": "http://arxiv.org/abs/2009.11598",
          "publishedOn": "2022-01-07T00:40:37.867Z",
          "wordCount": 608,
          "title": "Boosting Algorithms for Delivery Time Prediction in Transportation Logistics. (arXiv:2009.11598v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bashivan_P/0/1/0/all/0/1\">Pouya Bashivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayat_R/0/1/0/all/0/1\">Reza Bayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_A/0/1/0/all/0/1\">Adam Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1\">Kartik Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faramarzi_M/0/1/0/all/0/1\">Mojtaba Faramarzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laleh_T/0/1/0/all/0/1\">Touraj Laleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richards_B/0/1/0/all/0/1\">Blake Aaron Richards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1\">Irina Rish</a>",
          "description": "Neural networks are known to be vulnerable to adversarial attacks -- slight\nbut carefully constructed perturbations of the inputs which can drastically\nimpair the network's performance. Many defense methods have been proposed for\nimproving robustness of deep networks by training them on adversarially\nperturbed inputs. However, these models often remain vulnerable to new types of\nattacks not seen during training, and even to slightly stronger versions of\npreviously seen attacks. In this work, we propose a novel approach to\nadversarial robustness, which builds upon the insights from the domain\nadaptation field. Our method, called Adversarial Feature Desensitization (AFD),\naims at learning features that are invariant towards adversarial perturbations\nof the inputs. This is achieved through a game where we learn features that are\nboth predictive and robust (insensitive to adversarial attacks), i.e. cannot be\nused to discriminate between natural and adversarial data. Empirical results on\nseveral benchmarks demonstrate the effectiveness of the proposed approach\nagainst a wide range of attack types and attack strengths. Our code is\navailable at https://github.com/BashivanLab/afd.",
          "link": "http://arxiv.org/abs/2006.04621",
          "publishedOn": "2022-01-07T00:40:37.859Z",
          "wordCount": 634,
          "title": "Adversarial Feature Desensitization. (arXiv:2006.04621v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ziemann_I/0/1/0/all/0/1\">Ingvar Ziemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandberg_H/0/1/0/all/0/1\">Henrik Sandberg</a>",
          "description": "This paper presents local minimax regret lower bounds for adaptively\ncontrolling linear-quadratic-Gaussian (LQG) systems. We consider smoothly\nparametrized instances and provide an understanding of when logarithmic regret\nis impossible which is both instance specific and flexible enough to take\nproblem structure into account. This understanding relies on two key notions:\nThat of local-uninformativeness; when the optimal policy does not provide\nsufficient excitation for identification of the optimal policy, and yields a\ndegenerate Fisher information matrix; and that of\ninformation-regret-boundedness, when the small eigenvalues of a\npolicy-dependent information matrix are boundable in terms of the regret of\nthat policy. Combined with a reduction to Bayesian estimation and application\nof Van Trees' inequality, these two conditions are sufficient for proving\nregret bounds on order of magnitude $\\sqrt{T}$ in the time horizon, $T$. This\nmethod yields lower bounds that exhibit tight dimensional dependencies and\nscale naturally with control-theoretic problem constants. For instance, we are\nable to prove that systems operating near marginal stability are fundamentally\nhard to learn to control. We further show that large classes of systems satisfy\nthese conditions, among them any state-feedback system with both $A$- and\n$B$-matrices unknown. Most importantly, we also establish that a nontrivial\nclass of partially observable systems, essentially those that are\nover-actuated, satisfy these conditions, thus providing a $\\sqrt{T}$ lower\nbound also valid for partially observable systems. Finally, we turn to two\nsimple examples which demonstrate that our lower bound captures classical\ncontrol-theoretic intuition: our lower bounds diverge for systems operating\nnear marginal stability or with large filter gain -- these can be arbitrarily\nhard to (learn to) control.",
          "link": "http://arxiv.org/abs/2201.01680",
          "publishedOn": "2022-01-07T00:40:37.834Z",
          "wordCount": 690,
          "title": "Regret Lower Bounds for Learning Linear Quadratic Gaussian Systems. (arXiv:2201.01680v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01741",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bamler_R/0/1/0/all/0/1\">Robert Bamler</a>",
          "description": "Entropy coding is the backbone data compression. Novel machine-learning based\ncompression methods often use a new entropy coder called Asymmetric Numeral\nSystems (ANS) [Duda et al., 2015], which provides very close to optimal\nbitrates and simplifies [Townsend et al., 2019] advanced compression techniques\nsuch as bits-back coding. However, researchers with a background in machine\nlearning often struggle to understand how ANS works, which prevents them from\nexploiting its full versatility. This paper is meant as an educational resource\nto make ANS more approachable by presenting it from a new perspective of latent\nvariable models and the so-called bits-back trick. We guide the reader step by\nstep to a complete implementation of ANS in the Python programming language,\nwhich we then generalize for more advanced use cases. We also present and\nempirically evaluate an open-source library of various entropy coders designed\nfor both research and production use. Related teaching videos and problem sets\nare available online.",
          "link": "http://arxiv.org/abs/2201.01741",
          "publishedOn": "2022-01-07T00:40:37.825Z",
          "wordCount": 598,
          "title": "Understanding Entropy Coding With Asymmetric Numeral Systems (ANS): a Statistician's Perspective. (arXiv:2201.01741v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01539",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Singh_H/0/1/0/all/0/1\">Himali Singh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chattopadhyay_A/0/1/0/all/0/1\">Arpan Chattopadhyay</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mishra_K/0/1/0/all/0/1\">Kumar Vijay Mishra</a>",
          "description": "Recent advances in counter-adversarial systems have garnered significant\nresearch interest in inverse filtering from a Bayesian perspective. For\nexample, interest in estimating the adversary's Kalman filter tracked estimate\nwith the purpose of predicting the adversary's future steps has led to recent\nformulations of inverse Kalman filter (I-KF). In this context of inverse\nfiltering, we address the key challenges of nonlinear process dynamics and\nunknown input to the forward filter by proposing inverse extended Kalman filter\n(I-EKF). We derive I-EKF with and without an unknown input by considering\nnonlinearity in both forward and inverse state-space models. In the process,\nI-KF-with-unknown-input is also obtained. We then provide theoretical stability\nguarantees using both bounded nonlinearity and unknown matrix approaches. We\nfurther generalize these formulations and results to the case of higher-order,\nGaussian-sum, and dithered I-EKFs. Numerical experiments validate our methods\nfor various proposed inverse filters using the recursive Cram\\'er-Rao lower\nbound as a benchmark.",
          "link": "http://arxiv.org/abs/2201.01539",
          "publishedOn": "2022-01-07T00:40:37.817Z",
          "wordCount": 583,
          "title": "Inverse Extended Kalman Filter. (arXiv:2201.01539v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01652",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lyu_H/0/1/0/all/0/1\">Hanbaek Lyu</a>",
          "description": "Stochastic majorization-minimization (SMM) is an online extension of the\nclassical principle of majorization-minimization, which consists of sampling\ni.i.d. data points from a fixed data distribution and minimizing a recursively\ndefined majorizing surrogate of an objective function. In this paper, we\nintroduce stochastic block majorization-minimization, where the surrogates can\nnow be only block multi-convex and a single block is optimized at a time within\na diminishing radius. Relaxing the standard strong convexity requirements for\nsurrogates in SMM, our framework gives wider applicability including online\nCANDECOMP/PARAFAC (CP) dictionary learning and yields greater computational\nefficiency especially when the problem dimension is large. We provide an\nextensive convergence analysis on the proposed algorithm, which we derive under\npossibly dependent data streams, relaxing the standard i.i.d. assumption on\ndata samples. We show that the proposed algorithm converges almost surely to\nthe set of stationary points of a nonconvex objective under constraints at a\nrate $O((\\log n)^{1+\\eps}/n^{1/2})$ for the empirical loss function and\n$O((\\log n)^{1+\\eps}/n^{1/4})$ for the expected loss function, where $n$\ndenotes the number of data samples processed. Under some additional assumption,\nthe latter convergence rate can be improved to $O((\\log n)^{1+\\eps}/n^{1/2})$.\nOur results provide first convergence rate bounds for various online matrix and\ntensor decomposition algorithms under a general Markovian data setting.",
          "link": "http://arxiv.org/abs/2201.01652",
          "publishedOn": "2022-01-07T00:40:37.809Z",
          "wordCount": 632,
          "title": "Convergence and Complexity of Stochastic Block Majorization-Minimization. (arXiv:2201.01652v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2110.03825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hanxun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1\">Sarah Monazam Erfani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1\">James Bailey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xingjun Ma</a>",
          "description": "Deep neural networks (DNNs) are known to be vulnerable to adversarial\nattacks. A range of defense methods have been proposed to train adversarially\nrobust DNNs, among which adversarial training has demonstrated promising\nresults. However, despite preliminary understandings developed for adversarial\ntraining, it is still not clear, from the architectural perspective, what\nconfigurations can lead to more robust DNNs. In this paper, we address this gap\nvia a comprehensive investigation on the impact of network width and depth on\nthe robustness of adversarially trained DNNs. Specifically, we make the\nfollowing key observations: 1) more parameters (higher model capacity) does not\nnecessarily help adversarial robustness; 2) reducing capacity at the last stage\n(the last group of blocks) of the network can actually improve adversarial\nrobustness; and 3) under the same parameter budget, there exists an optimal\narchitectural configuration for adversarial robustness. We also provide a\ntheoretical analysis explaning why such network configuration can help\nrobustness. These architectural insights can help design adversarially robust\nDNNs. Code is available at \\url{https://github.com/HanxunH/RobustWRN}.",
          "link": "http://arxiv.org/abs/2110.03825",
          "publishedOn": "2022-01-06T00:40:22.981Z",
          "wordCount": 656,
          "title": "Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks. (arXiv:2110.03825v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09276",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Koehler_F/0/1/0/all/0/1\">Frederic Koehler</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_L/0/1/0/all/0/1\">Lijia Zhou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sutherland_D/0/1/0/all/0/1\">Danica J. Sutherland</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Srebro_N/0/1/0/all/0/1\">Nathan Srebro</a>",
          "description": "We consider interpolation learning in high-dimensional linear regression with\nGaussian data, and prove a generic uniform convergence guarantee on the\ngeneralization error of interpolators in an arbitrary hypothesis class in terms\nof the class's Gaussian width. Applying the generic bound to Euclidean norm\nballs recovers the consistency result of Bartlett et al. (2020) for\nminimum-norm interpolators, and confirms a prediction of Zhou et al. (2020) for\nnear-minimal-norm interpolators in the special case of Gaussian data. We\ndemonstrate the generality of the bound by applying it to the simplex,\nobtaining a novel consistency result for minimum l1-norm interpolators (basis\npursuit). Our results show how norm-based generalization bounds can explain and\nbe used to analyze benign overfitting, at least in some settings.",
          "link": "http://arxiv.org/abs/2106.09276",
          "publishedOn": "2022-01-06T00:40:22.861Z",
          "wordCount": 585,
          "title": "Uniform Convergence of Interpolators: Gaussian Width, Norm Bounds, and Benign Overfitting. (arXiv:2106.09276v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15427",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nadjahi_K/0/1/0/all/0/1\">Kimia Nadjahi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1\">Alain Durmus</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jacob_P/0/1/0/all/0/1\">Pierre E. Jacob</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Badeau_R/0/1/0/all/0/1\">Roland Badeau</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Simsekli_U/0/1/0/all/0/1\">Umut &#x15e;im&#x15f;ekli</a>",
          "description": "The Sliced-Wasserstein distance (SW) is being increasingly used in machine\nlearning applications as an alternative to the Wasserstein distance and offers\nsignificant computational and statistical benefits. Since it is defined as an\nexpectation over random projections, SW is commonly approximated by Monte\nCarlo. We adopt a new perspective to approximate SW by making use of the\nconcentration of measure phenomenon: under mild assumptions, one-dimensional\nprojections of a high-dimensional random vector are approximately Gaussian.\nBased on this observation, we develop a simple deterministic approximation for\nSW. Our method does not require sampling a number of random projections, and is\ntherefore both accurate and easy to use compared to the usual Monte Carlo\napproximation. We derive nonasymptotical guarantees for our approach, and show\nthat the approximation error goes to zero as the dimension increases, under a\nweak dependence condition on the data distribution. We validate our theoretical\nfindings on synthetic datasets, and illustrate the proposed approximation on a\ngenerative modeling problem.",
          "link": "http://arxiv.org/abs/2106.15427",
          "publishedOn": "2022-01-06T00:40:22.853Z",
          "wordCount": 611,
          "title": "Fast Approximation of the Sliced-Wasserstein Distance Using Concentration of Random Projections. (arXiv:2106.15427v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zahavy_T/0/1/0/all/0/1\">Tom Zahavy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODonoghue_B/0/1/0/all/0/1\">Brendan O&#x27;Donoghue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barreto_A/0/1/0/all/0/1\">Andre Barreto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mnih_V/0/1/0/all/0/1\">Volodymyr Mnih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flennerhag_S/0/1/0/all/0/1\">Sebastian Flennerhag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satinder Singh</a>",
          "description": "Finding different solutions to the same problem is a key aspect of\nintelligence associated with creativity and adaptation to novel situations. In\nreinforcement learning, a set of diverse policies can be useful for\nexploration, transfer, hierarchy, and robustness. We propose Diverse Successive\nPolicies, a method for discovering policies that are diverse in the space of\nSuccessor Features, while assuring that they are near optimal. We formalize the\nproblem as a Constrained Markov Decision Process (CMDP) where the goal is to\nfind policies that maximize diversity, characterized by an intrinsic diversity\nreward, while remaining near-optimal with respect to the extrinsic reward of\nthe MDP. We also analyze how recently proposed robustness and discrimination\nrewards perform and find that they are sensitive to the initialization of the\nprocedure and may converge to sub-optimal solutions. To alleviate this, we\npropose new explicit diversity rewards that aim to minimize the correlation\nbetween the Successor Features of the policies in the set. We compare the\ndifferent diversity mechanisms in the DeepMind Control Suite and find that the\ntype of explicit diversity we are proposing is important to discover distinct\nbehavior, like for example different locomotion patterns.",
          "link": "http://arxiv.org/abs/2106.00669",
          "publishedOn": "2022-01-06T00:40:22.737Z",
          "wordCount": 652,
          "title": "Discovering Diverse Nearly Optimal Policies with Successor Features. (arXiv:2106.00669v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1\">Yifei Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We study the off-policy evaluation (OPE) problem in reinforcement learning\nwith linear function approximation, which aims to estimate the value function\nof a target policy based on the offline data collected by a behavior policy. We\npropose to incorporate the variance information of the value function to\nimprove the sample efficiency of OPE. More specifically, for time-inhomogeneous\nepisodic linear Markov decision processes (MDPs), we propose an algorithm,\nVA-OPE, which uses the estimated variance of the value function to reweight the\nBellman residual in Fitted Q-Iteration. We show that our algorithm achieves a\ntighter error bound than the best-known result. We also provide a fine-grained\ncharacterization of the distribution shift between the behavior policy and the\ntarget policy. Extensive numerical experiments corroborate our theory.",
          "link": "http://arxiv.org/abs/2106.11960",
          "publishedOn": "2022-01-06T00:40:22.711Z",
          "wordCount": 585,
          "title": "Variance-Aware Off-Policy Evaluation with Linear Function Approximation. (arXiv:2106.11960v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01036",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wu_S/0/1/0/all/0/1\">Shihao Wu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhu_Z/0/1/0/all/0/1\">Ziwei Zhu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_L/0/1/0/all/0/1\">Ling Zhou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Song_P/0/1/0/all/0/1\">Peter X.-K. Song</a>",
          "description": "Fusing regression coefficients into homogenous groups can unveil those\ncoefficients that share a common value within each group. Such groupwise\nhomogeneity reduces the intrinsic dimension of the parameter space and\nunleashes sharper statistical accuracy. We propose and investigate a new\ncombinatorial grouping approach called $L_0$-Fusion that is amenable to mixed\ninteger optimization (MIO). On the statistical aspect, we identify a\nfundamental quantity called grouping sensitivity that underpins the difficulty\nof recovering the true groups. We show that $L_0$-Fusion achieves grouping\nconsistency under the weakest possible requirement of the grouping sensitivity:\nif this requirement is violated, then the minimax risk of group\nmisspecification will fail to converge to zero. Moreover, we show that in the\nhigh-dimensional regime, one can apply $L_0$-Fusion coupled with a sure\nscreening set of features without any essential loss of statistical efficiency,\nwhile reducing the computational cost substantially. On the algorithmic aspect,\nwe provide a MIO formulation for $L_0$-Fusion along with a warm start strategy.\nSimulation and real data analysis demonstrate that $L_0$-Fusion exhibits\nsuperiority over its competitors in terms of grouping accuracy.",
          "link": "http://arxiv.org/abs/2201.01036",
          "publishedOn": "2022-01-06T00:40:22.701Z",
          "wordCount": 602,
          "title": "Supervised Homogeneity Fusion: a Combinatorial Approach. (arXiv:2201.01036v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2103.12676",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mehari_T/0/1/0/all/0/1\">Temesgen Mehari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strodthoff_N/0/1/0/all/0/1\">Nils Strodthoff</a>",
          "description": "Clinical 12-lead electrocardiography (ECG) is one of the most widely\nencountered kinds of biosignals. Despite the increased availability of public\nECG datasets, label scarcity remains a central challenge in the field.\nSelf-supervised learning represents a promising way to alleviate this issue. In\nthis work, we put forward the first comprehensive assessment of self-supervised\nrepresentation learning from clinical 12-lead ECG data. To this end, we adapt\nstate-of-the-art self-supervised methods based on instance discrimination and\nlatent forecasting to the ECG domain. In a first step, we learn contrastive\nrepresentations and evaluate their quality based on linear evaluation\nperformance on a recently established, comprehensive, clinical ECG\nclassification task. In a second step, we analyze the impact of self-supervised\npretraining on finetuned ECG classifiers as compared to purely supervised\nperformance. For the best-performing method, an adaptation of contrastive\npredictive coding, we find a linear evaluation performance only 0.5% below\nsupervised performance. For the finetuned models, we find improvements in\ndownstream performance of roughly 1% compared to supervised performance, label\nefficiency, as well as robustness against physiological noise. This work\nclearly establishes the feasibility of extracting discriminative\nrepresentations from ECG data via self-supervised learning and the numerous\nadvantages when finetuning such representations on downstream tasks as compared\nto purely supervised training. As first comprehensive assessment of its kind in\nthe ECG domain carried out exclusively on publicly available datasets, we hope\nto establish a first step towards reproducible progress in the rapidly evolving\nfield of representation learning for biosignals.",
          "link": "http://arxiv.org/abs/2103.12676",
          "publishedOn": "2022-01-06T00:40:22.694Z",
          "wordCount": 709,
          "title": "Self-supervised representation learning from 12-lead ECG data. (arXiv:2103.12676v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.15991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kursa_M/0/1/0/all/0/1\">Miron Bartosz Kursa</a>",
          "description": "Kendall transformation is a conversion of an ordered feature into a vector of\npairwise order relations between individual values. This way, it preserves\nranking of observations and represents it in a categorical form.\n\nSuch transformation allows for generalisation of methods requiring strictly\ncategorical input, especially in the limit of small number of observations,\nwhen discretisation becomes problematic. In particular, many approaches of\ninformation theory can be directly applied to Kendall-transformed continuous\ndata without relying on differential entropy or any additional parameters.\nMoreover, by filtering information to this contained in ranking, Kendall\ntransformation leads to a better robustness at a reasonable cost of dropping\nsophisticated interactions which are anyhow unlikely to be correctly estimated.\n\nIn bivariate analysis, Kendall transformation can be related to popular\nnon-parametric methods, showing the soundness of the approach. The paper also\ndemonstrates its efficiency in multivariate problems, as well as provides an\nexample analysis of a real-world data.",
          "link": "http://arxiv.org/abs/2006.15991",
          "publishedOn": "2022-01-06T00:40:22.687Z",
          "wordCount": 601,
          "title": "Kendall transformation: a robust representation of continuous data for information theory. (arXiv:2006.15991v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00844",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Azeraf_E/0/1/0/all/0/1\">Elie Azeraf</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Monfrini_E/0/1/0/all/0/1\">Emmanuel Monfrini</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pieczynski_W/0/1/0/all/0/1\">Wojciech Pieczynski</a>",
          "description": "We deal with Bayesian generative and discriminative classifiers. Given a\nmodel distribution $p(x, y)$, with the observation $y$ and the target $x$, one\ncomputes generative classifiers by firstly considering $p(x, y)$ and then using\nthe Bayes rule to calculate $p(x | y)$. A discriminative model is directly\ngiven by $p(x | y)$, which is used to compute discriminative classifiers.\nHowever, recent works showed that the Bayesian Maximum Posterior classifier\ndefined from the Naive Bayes (NB) or Hidden Markov Chain (HMC), both generative\nmodels, can also match the discriminative classifier definition. Thus, there\nare situations in which dividing classifiers into \"generative\" and\n\"discriminative\" is somewhat misleading. Indeed, such a distinction is rather\nrelated to the way of computing classifiers, not to the classifiers themselves.\nWe present a general theoretical result specifying how a generative classifier\ninduced from a generative model can also be computed in a discriminative way\nfrom the same model. Examples of NB and HMC are found again as particular\ncases, and we apply the general result to two original extensions of NB, and\ntwo extensions of HMC, one of which being original. Finally, we shortly\nillustrate the interest of the new discriminative way of computing classifiers\nin the Natural Language Processing (NLP) framework.",
          "link": "http://arxiv.org/abs/2201.00844",
          "publishedOn": "2022-01-06T00:40:22.667Z",
          "wordCount": 624,
          "title": "Deriving discriminative classifiers from generative models. (arXiv:2201.00844v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.01051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pradhan_A/0/1/0/all/0/1\">Ashirbad Pradhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiayuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Ning Jiang</a>",
          "description": "Recently, surface electromyogram (EMG) has been proposed as a novel biometric\ntrait for addressing some key limitations of current biometrics, such as\nspoofing and liveness. The EMG signals possess a unique characteristic: they\nare inherently different for individuals (biometrics), and they can be\ncustomized to realize multi-length codes or passwords (for example, by\nperforming different gestures). However, current EMG-based biometric research\nhas two critical limitations: 1) a small subject pool, compared to other more\nestablished biometric traits, and 2) limited to single-session or single-day\ndata sets. In this study, forearm and wrist EMG data were collected from 43\nparticipants over three different days with long separation while they\nperformed static hand and wrist gestures. The multi-day biometric\nauthentication resulted in a median EER of 0.017 for the forearm setup and\n0.025 for the wrist setup, comparable to well-established biometric traits\nsuggesting consistent performance over multiple days. The presented\nlarge-sample multi-day data set and findings could facilitate further research\non EMG-based biometrics and other gesture recognition-based applications.",
          "link": "http://arxiv.org/abs/2201.01051",
          "publishedOn": "2022-01-06T00:40:22.633Z",
          "wordCount": 599,
          "title": "Open Access Dataset for Electromyography based Multi-code Biometric Authentication. (arXiv:2201.01051v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2110.03652",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Sreekumar_S/0/1/0/all/0/1\">Sreejith Sreekumar</a>, <a href=\"http://arxiv.org/find/math/1/au:+Goldfeld_Z/0/1/0/all/0/1\">Ziv Goldfeld</a>",
          "description": "Statistical divergences (SDs), which quantify the dissimilarity between\nprobability distributions, are a basic constituent of statistical inference and\nmachine learning. A modern method for estimating those divergences relies on\nparametrizing an empirical variational form by a neural network (NN) and\noptimizing over parameter space. Such neural estimators are abundantly used in\npractice, but corresponding performance guarantees are partial and call for\nfurther exploration. In particular, there is a fundamental tradeoff between the\ntwo sources of error involved: approximation and empirical estimation. While\nthe former needs the NN class to be rich and expressive, the latter relies on\ncontrolling complexity. We explore this tradeoff for an estimator based on a\nshallow NN by means of non-asymptotic error bounds, focusing on four popular\n$\\mathsf{f}$-divergences -- Kullback-Leibler, chi-squared, squared Hellinger,\nand total variation. Our analysis relies on non-asymptotic function\napproximation theorems and tools from empirical process theory. The bounds\nreveal the tension between the NN size and the number of samples, and enable to\ncharacterize scaling rates thereof that ensure consistency. For compactly\nsupported distributions, we further show that neural estimators of the first\nthree divergences above with appropriate NN growth-rate are near minimax\nrate-optimal, achieving the parametric rate up to logarithmic factors.",
          "link": "http://arxiv.org/abs/2110.03652",
          "publishedOn": "2022-01-06T00:40:22.626Z",
          "wordCount": 631,
          "title": "Neural Estimation of Statistical Divergences. (arXiv:2110.03652v2 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.01123",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Vogrinc_J/0/1/0/all/0/1\">Jure Vogrinc</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Livingstone_S/0/1/0/all/0/1\">Samuel Livingstone</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zanella_G/0/1/0/all/0/1\">Giacomo Zanella</a>",
          "description": "We study the class of first-order locally-balanced Metropolis--Hastings\nalgorithms introduced in Livingstone & Zanella (2021). To choose a specific\nalgorithm within the class the user must select a balancing function\n$g:\\mathbb{R} \\to \\mathbb{R}$ satisfying $g(t) = tg(1/t)$, and a noise\ndistribution for the proposal increment. Popular choices within the class are\nthe Metropolis-adjusted Langevin algorithm and the recently introduced Barker\nproposal. We first establish a universal limiting optimal acceptance rate of\n57% and scaling of $n^{-1/3}$ as the dimension $n$ tends to infinity among all\nmembers of the class under mild smoothness assumptions on $g$ and when the\ntarget distribution for the algorithm is of the product form. In particular we\nobtain an explicit expression for the asymptotic efficiency of an arbitrary\nalgorithm in the class, as measured by expected squared jumping distance. We\nthen consider how to optimise this expression under various constraints. We\nderive an optimal choice of noise distribution for the Barker proposal, optimal\nchoice of balancing function under a Gaussian noise distribution, and optimal\nchoice of first-order locally-balanced algorithm among the entire class, which\nturns out to depend on the specific target distribution. Numerical simulations\nconfirm our theoretical findings and in particular show that a bi-modal choice\nof noise distribution in the Barker proposal gives rise to a practical\nalgorithm that is consistently more efficient than the original Gaussian\nversion.",
          "link": "http://arxiv.org/abs/2201.01123",
          "publishedOn": "2022-01-06T00:40:22.612Z",
          "wordCount": 656,
          "title": "Optimal design of the Barker proposal and other locally-balanced Metropolis-Hastings algorithms. (arXiv:2201.01123v1 [stat.CO])"
        },
        {
          "id": "http://arxiv.org/abs/2006.04635",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anthony_T/0/1/0/all/0/1\">Thomas Anthony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eccles_T/0/1/0/all/0/1\">Tom Eccles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tacchetti_A/0/1/0/all/0/1\">Andrea Tacchetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kramar_J/0/1/0/all/0/1\">J&#xe1;nos Kram&#xe1;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemp_I/0/1/0/all/0/1\">Ian Gemp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudson_T/0/1/0/all/0/1\">Thomas C. Hudson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porcel_N/0/1/0/all/0/1\">Nicolas Porcel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanctot_M/0/1/0/all/0/1\">Marc Lanctot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perolat_J/0/1/0/all/0/1\">Julien P&#xe9;rolat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Everett_R/0/1/0/all/0/1\">Richard Everett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werpachowski_R/0/1/0/all/0/1\">Roman Werpachowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satinder Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graepel_T/0/1/0/all/0/1\">Thore Graepel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachrach_Y/0/1/0/all/0/1\">Yoram Bachrach</a>",
          "description": "Recent advances in deep reinforcement learning (RL) have led to considerable\nprogress in many 2-player zero-sum games, such as Go, Poker and Starcraft. The\npurely adversarial nature of such games allows for conceptually simple and\nprincipled application of RL methods. However real-world settings are\nmany-agent, and agent interactions are complex mixtures of common-interest and\ncompetitive aspects. We consider Diplomacy, a 7-player board game designed to\naccentuate dilemmas resulting from many-agent interactions. It also features a\nlarge combinatorial action space and simultaneous moves, which are challenging\nfor RL algorithms. We propose a simple yet effective approximate best response\noperator, designed to handle large combinatorial action spaces and simultaneous\nmoves. We also introduce a family of policy iteration methods that approximate\nfictitious play. With these methods, we successfully apply RL to Diplomacy: we\nshow that our agents convincingly outperform the previous state-of-the-art, and\ngame theoretic equilibrium analysis shows that the new process yields\nconsistent improvements.",
          "link": "http://arxiv.org/abs/2006.04635",
          "publishedOn": "2022-01-06T00:40:22.594Z",
          "wordCount": 665,
          "title": "Learning to Play No-Press Diplomacy with Best Response Policy Iteration. (arXiv:2006.04635v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.00183",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Peixoto_T/0/1/0/all/0/1\">Tiago P. Peixoto</a>",
          "description": "Community detection is one of the most important methodological fields of\nnetwork science, and one which has attracted a significant amount of attention\nover the past decades. This area deals with the automated division of a network\ninto fundamental building blocks, with the objective of providing a summary of\nits large-scale structure. Despite its importance and widespread adoption,\nthere is a noticeable gap between what is considered the state-of-the-art and\nthe methods that are actually used in practice in a variety of fields. Here we\nattempt to address this discrepancy by dividing existing methods according to\nwhether they have a \"descriptive\" or an \"inferential\" goal. While descriptive\nmethods find patterns in networks based on intuitive notions of community\nstructure, inferential methods articulate a precise generative model, and\nattempt to fit it to data. In this way, they are able to provide insights into\nthe mechanisms of network formation, and separate structure from randomness in\na manner supported by statistical evidence. We review how employing descriptive\nmethods with inferential aims is riddled with pitfalls and misleading answers,\nand thus should be in general avoided. We argue that inferential methods are\nmore typically aligned with clearer scientific questions, yield more robust\nresults, and should be in many cases preferred. We attempt to dispel some myths\nand half-truths often believed when community detection is employed in\npractice, in an effort to improve both the use of such methods as well as the\ninterpretation of their results.",
          "link": "http://arxiv.org/abs/2112.00183",
          "publishedOn": "2022-01-06T00:40:22.585Z",
          "wordCount": 712,
          "title": "Descriptive vs. inferential community detection: pitfalls, myths and half-truths. (arXiv:2112.00183v3 [physics.soc-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kireev_K/0/1/0/all/0/1\">Klim Kireev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andriushchenko_M/0/1/0/all/0/1\">Maksym Andriushchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flammarion_N/0/1/0/all/0/1\">Nicolas Flammarion</a>",
          "description": "The literature on robustness towards common corruptions shows no consensus on\nwhether adversarial training can improve the performance in this setting.\nFirst, we show that, when used with an appropriately selected perturbation\nradius, $\\ell_p$ adversarial training can serve as a strong baseline against\ncommon corruptions improving both accuracy and calibration. Then we explain why\nadversarial training performs better than data augmentation with simple\nGaussian noise which has been observed to be a meaningful baseline on common\ncorruptions. Related to this, we identify the $\\sigma$-overfitting phenomenon\nwhen Gaussian augmentation overfits to a particular standard deviation used for\ntraining which has a significant detrimental effect on common corruption\naccuracy. We discuss how to alleviate this problem and then how to further\nenhance $\\ell_p$ adversarial training by introducing an efficient relaxation of\nadversarial training with learned perceptual image patch similarity as the\ndistance metric. Through experiments on CIFAR-10 and ImageNet-100, we show that\nour approach does not only improve the $\\ell_p$ adversarial training baseline\nbut also has cumulative gains with data augmentation methods such as AugMix,\nDeepAugment, ANT, and SIN, leading to state-of-the-art performance on common\ncorruptions.\n\nThe code of our experiments is publicly available at\nhttps://github.com/tml-epfl/adv-training-corruptions.",
          "link": "http://arxiv.org/abs/2103.02325",
          "publishedOn": "2022-01-06T00:40:22.575Z",
          "wordCount": 670,
          "title": "On the effectiveness of adversarial training against common corruptions. (arXiv:2103.02325v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patterson_J/0/1/0/all/0/1\">John Patterson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avery_C/0/1/0/all/0/1\">Chris Avery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grear_T/0/1/0/all/0/1\">Tyler Grear</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1\">Donald J. Jacobs</a>",
          "description": "The effect of bias on hypothesis formation is characterized for an automated\ndata-driven projection pursuit neural network to extract and select features\nfor binary classification of data streams. This intelligent exploratory process\npartitions a complete vector state space into disjoint subspaces to create\nworking hypotheses quantified by similarities and differences observed between\ntwo groups of labeled data streams. Data streams are typically time sequenced,\nand may exhibit complex spatio-temporal patterns. For example, given atomic\ntrajectories from molecular dynamics simulation, the machine's task is to\nquantify dynamical mechanisms that promote function by comparing protein\nmutants, some known to function while others are nonfunctional. Utilizing\nsynthetic two-dimensional molecules that mimic the dynamics of functional and\nnonfunctional proteins, biases are identified and controlled in both the\nmachine learning model and selected training data under different contexts. The\nrefinement of a working hypothesis converges to a statistically robust\nmultivariate perception of the data based on a context-dependent perspective.\nIncluding diverse perspectives during data exploration enhances\ninterpretability of the multivariate characterization of similarities and\ndifferences.",
          "link": "http://arxiv.org/abs/2201.00889",
          "publishedOn": "2022-01-06T00:40:22.564Z",
          "wordCount": 612,
          "title": "Biased Hypothesis Formation From Projection Pursuit. (arXiv:2201.00889v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1\">Ibrahim Alabdulmohsin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1\">Mario Lucic</a>",
          "description": "We present a scalable post-processing algorithm for debiasing trained models,\nincluding deep neural networks (DNNs), which we prove to be near-optimal by\nbounding its excess Bayes risk. We empirically validate its advantages on\nstandard benchmark datasets across both classical algorithms as well as modern\nDNN architectures and demonstrate that it outperforms previous post-processing\nmethods while performing on par with in-processing. In addition, we show that\nthe proposed algorithm is particularly effective for models trained at scale\nwhere post-processing is a natural and practical choice.",
          "link": "http://arxiv.org/abs/2106.12887",
          "publishedOn": "2022-01-05T00:39:34.072Z",
          "wordCount": 563,
          "title": "A Near-Optimal Algorithm for Debiasing Trained Machine Learning Models. (arXiv:2106.12887v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smelko_A/0/1/0/all/0/1\">Adam &#x160;melko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molnarova_S/0/1/0/all/0/1\">So&#x148;a Moln&#xe1;rov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kratochvil_M/0/1/0/all/0/1\">Miroslav Kratochv&#xed;l</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koladiya_A/0/1/0/all/0/1\">Abhishek Koladiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musil_J/0/1/0/all/0/1\">Jan Musil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krulis_M/0/1/0/all/0/1\">Martin Kruli&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrasek_J/0/1/0/all/0/1\">Ji&#x159;&#xed; Vondr&#xe1;&#x161;ek</a>",
          "description": "Dimensionality reduction methods have found vast application as visualization\ntools in diverse areas of science. Although many different methods exist, their\nperformance is often insufficient for providing quick insight into many\ncontemporary datasets, and the unsupervised mode of use prevents the users from\nutilizing the methods for dataset exploration and fine-tuning the details for\nimproved visualization quality. We present BlosSOM, a high-performance\nsemi-supervised dimensionality reduction software for interactive\nuser-steerable visualization of high-dimensional datasets with millions of\nindividual data points. BlosSOM builds on a GPU-accelerated implementation of\nthe EmbedSOM algorithm, complemented by several landmark-based algorithms for\ninterfacing the unsupervised model learning algorithms with the user\nsupervision. We show the application of BlosSOM on realistic datasets, where it\nhelps to produce high-quality visualizations that incorporate user-specified\nlayout and focus on certain features. We believe the semi-supervised\ndimensionality reduction will improve the data visualization possibilities for\nscience areas such as single-cell cytometry, and provide a fast and efficient\nbase methodology for new directions in dataset exploration and annotation.",
          "link": "http://arxiv.org/abs/2201.00701",
          "publishedOn": "2022-01-05T00:39:34.066Z",
          "wordCount": 608,
          "title": "Scalable semi-supervised dimensionality reduction with GPU-accelerated EmbedSOM. (arXiv:2201.00701v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.01750",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shaojie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jing Yuan</a>",
          "description": "Running machine learning algorithms on large and rapidly growing volumes of\ndata is often computationally expensive, one common trick to reduce the size of\na data set, and thus reduce the computational cost of machine learning\nalgorithms, is \\emph{probability sampling}. It creates a sampled data set by\nincluding each data point from the original data set with a known probability.\nAlthough the benefit of running machine learning algorithms on the reduced data\nset is obvious, one major concern is that the performance of the solution\nobtained from samples might be much worse than that of the optimal solution\nwhen using the full data set. In this paper, we examine the performance loss\ncaused by probability sampling in the context of adaptive submodular\nmaximization. We consider a simple probability sampling method which selects\neach data point with probability $r\\in[0,1]$. If we set the sampling rate\n$r=1$, our problem reduces to finding a solution based on the original full\ndata set. We define sampling gap as the largest ratio between the optimal\nsolution obtained from the full data set and the optimal solution obtained from\nthe samples, over independence systems. %It captures the performance loss of\nthe optimal solution caused by the probability sampling. Our main contribution\nis to show that if the utility function is policywise submodular, then for a\ngiven sampling rate $r$, the sampling gap is both upper bounded and lower\nbounded by $1/r$. One immediate implication of our result is that if we can\nfind an $\\alpha$-approximation solution based on a sampled data set (which is\nsampled at sampling rate $r$), then this solution achieves an $\\alpha r$\napproximation ratio against the optimal solution when using the full data set.",
          "link": "http://arxiv.org/abs/2104.01750",
          "publishedOn": "2022-01-05T00:39:34.059Z",
          "wordCount": 748,
          "title": "Optimal Sampling Gaps for Adaptive Submodular Maximization. (arXiv:2104.01750v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.14142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lifu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bo Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xing Cao</a>",
          "description": "Recurrent Neural Network (RNN) is a fundamental structure in deep learning.\nRecently, some works study the training process of over-parameterized neural\nnetworks, and show that over-parameterized networks can learn functions in some\nnotable concept classes with a provable generalization error bound. In this\npaper, we analyze the training and generalization for RNNs with random\ninitialization, and provide the following improvements over recent works:\n\n1) For a RNN with input sequence $x=(X_1,X_2,...,X_L)$, previous works study\nto learn functions that are summation of $f(\\beta^T_lX_l)$ and require\nnormalized conditions that $||X_l||\\leq\\epsilon$ with some very small\n$\\epsilon$ depending on the complexity of $f$. In this paper, using detailed\nanalysis about the neural tangent kernel matrix, we prove a generalization\nerror bound to learn such functions without normalized conditions and show that\nsome notable concept classes are learnable with the numbers of iterations and\nsamples scaling almost-polynomially in the input length $L$.\n\n2) Moreover, we prove a novel result to learn N-variables functions of input\nsequence with the form $f(\\beta^T[X_{l_1},...,X_{l_N}])$, which do not belong\nto the \"additive\" concept class, i,e., the summation of function $f(X_l)$. And\nwe show that when either $N$ or $l_0=\\max(l_1,..,l_N)-\\min(l_1,..,l_N)$ is\nsmall, $f(\\beta^T[X_{l_1},...,X_{l_N}])$ will be learnable with the number\niterations and samples scaling almost-polynomially in the input length $L$.",
          "link": "http://arxiv.org/abs/2109.14142",
          "publishedOn": "2022-01-05T00:39:34.028Z",
          "wordCount": 680,
          "title": "On the Provable Generalization of Recurrent Neural Networks. (arXiv:2109.14142v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.02470",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_C/0/1/0/all/0/1\">Chi-Hua Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1\">Zhanyu Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sun_W/0/1/0/all/0/1\">Will Wei Sun</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cheng_G/0/1/0/all/0/1\">Guang Cheng</a>",
          "description": "Devising dynamic pricing policy with always valid online statistical learning\nprocedure is an important and as yet unresolved problem. Most existing dynamic\npricing policy, which focus on the faithfulness of adopted customer choice\nmodels, exhibit a limited capability for adapting the online uncertainty of\nlearned statistical model during pricing process. In this paper, we propose a\nnovel approach for designing dynamic pricing policy based regularized online\nstatistical learning with theoretical guarantees. The new approach overcomes\nthe challenge of continuous monitoring of online Lasso procedure and possesses\nseveral appealing properties. In particular, we make the decisive observation\nthat the always-validity of pricing decisions builds and thrives on the online\nregularization scheme. Our proposed online regularization scheme equips the\nproposed optimistic online regularized maximum likelihood pricing (OORMLP)\npricing policy with three major advantages: encode market noise knowledge into\npricing process optimism; empower online statistical learning with\nalways-validity over all decision points; envelop prediction error process with\ntime-uniform non-asymptotic oracle inequalities. This type of non-asymptotic\ninference results allows us to design more sample-efficient and robust dynamic\npricing algorithms in practice. In theory, the proposed OORMLP algorithm\nexploits the sparsity structure of high-dimensional models and secures a\nlogarithmic regret in a decision horizon. These theoretical advances are made\npossible by proposing an optimistic online Lasso procedure that resolves\ndynamic pricing problems at the process level, based on a novel use of\nnon-asymptotic martingale concentration. In experiments, we evaluate OORMLP in\ndifferent synthetic and real pricing problem settings, and demonstrate that\nOORMLP advances the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2007.02470",
          "publishedOn": "2022-01-05T00:39:34.020Z",
          "wordCount": 693,
          "title": "Online Regularization towards Always-Valid High-Dimensional Dynamic Pricing. (arXiv:2007.02470v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10840",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tsukurimichi_T/0/1/0/all/0/1\">Toshiaki Tsukurimichi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Inatsu_Y/0/1/0/all/0/1\">Yu Inatsu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Duy_V/0/1/0/all/0/1\">Vo Nguyen Le Duy</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Takeuchi_I/0/1/0/all/0/1\">Ichiro Takeuchi</a>",
          "description": "In practical data analysis under noisy environment, it is common to first use\nrobust methods to identify outliers, and then to conduct further analysis after\nremoving the outliers. In this paper, we consider statistical inference of the\nmodel estimated after outliers are removed, which can be interpreted as a\nselective inference (SI) problem. To use conditional SI framework, it is\nnecessary to characterize the events of how the robust method identifies\noutliers. Unfortunately, the existing methods cannot be directly used here\nbecause they are applicable to the case where the selection events can be\nrepresented by linear/quadratic constraints. In this paper, we propose a\nconditional SI method for popular robust regressions by using homotopy method.\nWe show that the proposed conditional SI method is applicable to a wide class\nof robust regression and outlier detection methods and has good empirical\nperformance on both synthetic data and real data experiments.",
          "link": "http://arxiv.org/abs/2104.10840",
          "publishedOn": "2022-01-05T00:39:33.954Z",
          "wordCount": 600,
          "title": "Conditional Selective Inference for Robust Regression and Outlier Detection using Piecewise-Linear Homotopy Continuation. (arXiv:2104.10840v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00726",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moghaddam_M/0/1/0/all/0/1\">Mohammad A. Moghaddam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferre_T/0/1/0/all/0/1\">Ty P. A. Ferre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kewei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsani_M/0/1/0/all/0/1\">Mohammad Reza Ehsani</a>",
          "description": "We examine the ability of machine learning (ML) and deep learning (DL)\nalgorithms to infer surface/ground exchange flux based on subsurface\ntemperature observations. The observations and fluxes are produced from a\nhigh-resolution numerical model representing conditions in the Columbia River\nnear the Department of Energy Hanford site located in southeastern Washington\nState. Random measurement error, of varying magnitude, is added to the\nsynthetic temperature observations. The results indicate that both ML and DL\nmethods can be used to infer the surface/ground exchange flux. DL methods,\nespecially convolutional neural networks, outperform the ML methods when used\nto interpret noisy temperature data with a smoothing filter applied. However,\nthe ML methods also performed well and they are can better identify a reduced\nnumber of important observations, which could be useful for measurement network\noptimization. Surprisingly, the ML and DL methods better inferred upward flux\nthan downward flux. This is in direct contrast to previous findings using\nnumerical models to infer flux from temperature observations and it may suggest\nthat combined use of ML or DL inference with numerical inference could improve\nflux estimation beneath river systems.",
          "link": "http://arxiv.org/abs/2201.00726",
          "publishedOn": "2022-01-05T00:39:33.948Z",
          "wordCount": 642,
          "title": "Application of Machine Learning Methods in Inferring Surface Water Groundwater Exchanges using High Temporal Resolution Temperature Measurements. (arXiv:2201.00726v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.11684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tripuraneni_N/0/1/0/all/0/1\">Nilesh Tripuraneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "Meta-learning, or learning-to-learn, seeks to design algorithms that can\nutilize previous experience to rapidly learn new skills or adapt to new\nenvironments. Representation learning -- a key tool for performing\nmeta-learning -- learns a data representation that can transfer knowledge\nacross multiple tasks, which is essential in regimes where data is scarce.\nDespite a recent surge of interest in the practice of meta-learning, the\ntheoretical underpinnings of meta-learning algorithms are lacking, especially\nin the context of learning transferable representations. In this paper, we\nfocus on the problem of multi-task linear regression -- in which multiple\nlinear regression models share a common, low-dimensional linear representation.\nHere, we provide provably fast, sample-efficient algorithms to address the dual\nchallenges of (1) learning a common set of features from multiple, related\ntasks, and (2) transferring this knowledge to new, unseen tasks. Both are\ncentral to the general problem of meta-learning. Finally, we complement these\nresults by providing information-theoretic lower bounds on the sample\ncomplexity of learning these linear features.",
          "link": "http://arxiv.org/abs/2002.11684",
          "publishedOn": "2022-01-05T00:39:30.797Z",
          "wordCount": 649,
          "title": "Provable Meta-Learning of Linear Representations. (arXiv:2002.11684v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1807.04431",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Chi Chen</a>",
          "description": "We study the statistical properties of an estimator derived by applying a\ngradient ascent method with multiple initializations to a multi-modal\nlikelihood function. We derive the population quantity that is the target of\nthis estimator and study the properties of confidence intervals (CIs)\nconstructed from asymptotic normality and the bootstrap approach. In\nparticular, we analyze the coverage deficiency due to finite number of random\ninitializations. We also investigate the CIs by inverting the likelihood ratio\ntest, the score test, and the Wald test, and we show that the resulting CIs may\nbe very different. We propose a two-sample test procedure even when the MLE is\nintractable. In addition, we analyze the performance of the EM algorithm under\nrandom initializations and derive the coverage of a CI with a finite number of\ninitializations.",
          "link": "http://arxiv.org/abs/1807.04431",
          "publishedOn": "2022-01-05T00:39:30.790Z",
          "wordCount": 577,
          "title": "Statistical Inference with Local Optima. (arXiv:1807.04431v2 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00073",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Yan_J/0/1/0/all/0/1\">Jian Yan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_X/0/1/0/all/0/1\">Xianyang Zhang</a>",
          "description": "Motivated by the increasing use of kernel-based metrics for high-dimensional\nand large-scale data, we study the asymptotic behavior of kernel two-sample\ntests when the dimension and sample sizes both diverge to infinity. We focus on\nthe maximum mean discrepancy (MMD) with the kernel of the form\n$k(x,y)=f(\\|x-y\\|_{2}^{2}/\\gamma)$, including MMD with the Gaussian kernel and\nthe Laplacian kernel, and the energy distance as special cases. We derive\nasymptotic expansions of the kernel two-sample statistics, based on which we\nestablish the central limit theorem (CLT) under both the null hypothesis and\nthe local and fixed alternatives. The new non-null CLT results allow us to\nperform asymptotic exact power analysis, which reveals a delicate interplay\nbetween the moment discrepancy that can be detected by the kernel two-sample\ntests and the dimension-and-sample orders. The asymptotic theory is further\ncorroborated through numerical studies. Our findings complement those in the\nrecent literature and shed new light on the use of kernel two-sample tests for\nhigh-dimensional and large-scale data.",
          "link": "http://arxiv.org/abs/2201.00073",
          "publishedOn": "2022-01-05T00:39:30.783Z",
          "wordCount": 590,
          "title": "Kernel Two-Sample Tests in High Dimension: Interplay Between Moment Discrepancy and Dimension-and-Sample Orders. (arXiv:2201.00073v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2010.10786",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Guo_Y/0/1/0/all/0/1\">Yiping Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bondell_H/0/1/0/all/0/1\">Howard D. Bondell</a>",
          "description": "Probabilistic principal component analysis (PPCA) is a probabilistic\nreformulation of principal component analysis (PCA), under the framework of a\nGaussian latent variable model. To improve the robustness of PPCA, it has been\nproposed to change the underlying Gaussian distributions to multivariate\n$t$-distributions. Based on the representation of $t$-distribution as a scale\nmixture of Gaussian distributions, a hierarchical model is used for\nimplementation. However, in the existing literature, the hierarchical model\nimplemented does not yield the equivalent interpretation.\n\nIn this paper, we present two sets of equivalent relationships between the\nhigh-level multivariate $t$-PPCA framework and the hierarchical model used for\nimplementation. In doing so, we clarify a current misrepresentation in the\nliterature, by specifying the correct correspondence. In addition, we discuss\nthe performance of different multivariate $t$ robust PPCA methods both in\ntheory and simulation studies, and propose a novel Monte Carlo\nexpectation-maximization (MCEM) algorithm to implement one general type of such\nmodels.",
          "link": "http://arxiv.org/abs/2010.10786",
          "publishedOn": "2022-01-05T00:39:30.777Z",
          "wordCount": 600,
          "title": "On Robust Probabilistic Principal Component Analysis using Multivariate $t$-Distributions. (arXiv:2010.10786v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00217",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_M/0/1/0/all/0/1\">Minshuo Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liao_W/0/1/0/all/0/1\">Wenjing Liao</a>",
          "description": "Learning operators between infinitely dimensional spaces is an important\nlearning task arising in wide applications in machine learning, imaging\nscience, mathematical modeling and simulations, etc. This paper studies the\nnonparametric estimation of Lipschitz operators using deep neural networks.\nNon-asymptotic upper bounds are derived for the generalization error of the\nempirical risk minimizer over a properly chosen network class. Under the\nassumption that the target operator exhibits a low dimensional structure, our\nerror bounds decay as the training sample size increases, with an attractive\nfast rate depending on the intrinsic dimension in our estimation. Our\nassumptions cover most scenarios in real applications and our results give rise\nto fast rates by exploiting low dimensional structures of data in operator\nestimation. We also investigate the influence of network structures (e.g.,\nnetwork width, depth, and sparsity) on the generalization error of the neural\nnetwork estimator and propose a general suggestion on the choice of network\nstructures to maximize the learning efficiency quantitatively.",
          "link": "http://arxiv.org/abs/2201.00217",
          "publishedOn": "2022-01-05T00:39:30.771Z",
          "wordCount": 584,
          "title": "Deep Nonparametric Estimation of Operators between Infinite Dimensional Spaces. (arXiv:2201.00217v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00409",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Akyildiz_O/0/1/0/all/0/1\">&#xd6;mer Deniz Akyildiz</a>",
          "description": "We analyze the optimized adaptive importance sampler (OAIS) for performing\nMonte Carlo integration with general proposals. We leverage a classical result\nwhich shows that the bias and the mean-squared error (MSE) of the importance\nsampling scales with the $\\chi^2$-divergence between the target and the\nproposal and develop a scheme which performs global optimization of\n$\\chi^2$-divergence. While it is known that this quantity is convex for\nexponential family proposals, the case of the general proposals has been an\nopen problem. We close this gap by utilizing stochastic gradient Langevin\ndynamics (SGLD) and its underdamped counterpart for the global optimization of\n$\\chi^2$-divergence and derive nonasymptotic bounds for the MSE by leveraging\nrecent results from non-convex optimization literature. The resulting AIS\nschemes have explicit theoretical guarantees uniform in the number of\niterations.",
          "link": "http://arxiv.org/abs/2201.00409",
          "publishedOn": "2022-01-05T00:39:30.750Z",
          "wordCount": 545,
          "title": "Global convergence of optimized adaptive importance samplers. (arXiv:2201.00409v1 [stat.CO])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiyang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botta_N/0/1/0/all/0/1\">Nicola Botta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_C/0/1/0/all/0/1\">Cezar Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">George H. Chen</a>",
          "description": "Outlier detection refers to the identification of data points that deviate\nfrom a general data distribution. Existing unsupervised approaches often suffer\nfrom high computational cost, complex hyperparameter tuning, and limited\ninterpretability, especially when working with large, high-dimensional\ndatasets. To address these issues, we present a simple yet effective algorithm\ncalled ECOD (Empirical-Cumulative-distribution-based Outlier Detection), which\nis inspired by the fact that outliers are often the \"rare events\" that appear\nin the tails of a distribution. In a nutshell, ECOD first estimates the\nunderlying distribution of the input data in a nonparametric fashion by\ncomputing the empirical cumulative distribution per dimension of the data. ECOD\nthen uses these empirical distributions to estimate tail probabilities per\ndimension for each data point. Finally, ECOD computes an outlier score of each\ndata point by aggregating estimated tail probabilities across dimensions. Our\ncontributions are as follows: (1) we propose a novel outlier detection method\ncalled ECOD, which is both parameter-free and easy to interpret; (2) we perform\nextensive experiments on 30 benchmark datasets, where we find that ECOD\noutperforms 11 state-of-the-art baselines in terms of accuracy, efficiency, and\nscalability; and (3) we release an easy-to-use and scalable (with distributed\nsupport) Python implementation for accessibility and reproducibility.",
          "link": "http://arxiv.org/abs/2201.00382",
          "publishedOn": "2022-01-05T00:39:30.742Z",
          "wordCount": 647,
          "title": "ECOD: Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions. (arXiv:2201.00382v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00494",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Faletto_G/0/1/0/all/0/1\">Gregory Faletto</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bien_J/0/1/0/all/0/1\">Jacob Bien</a>",
          "description": "Stability selection (Meinshausen and Buhlmann, 2010) makes any feature\nselection method more stable by returning only those features that are\nconsistently selected across many subsamples. We prove (in what is, to our\nknowledge, the first result of its kind) that for data containing highly\ncorrelated proxies for an important latent variable, the lasso typically\nselects one proxy, yet stability selection with the lasso can fail to select\nany proxy, leading to worse predictive performance than the lasso alone.\n\nWe introduce cluster stability selection, which exploits the practitioner's\nknowledge that highly correlated clusters exist in the data, resulting in\nbetter feature rankings than stability selection in this setting. We consider\nseveral feature-combination approaches, including taking a weighted average of\nthe features in each important cluster where weights are determined by the\nfrequency with which cluster members are selected, which we show leads to\nbetter predictive models than previous proposals.\n\nWe present generalizations of theoretical guarantees from Meinshausen and\nBuhlmann (2010) and Shah and Samworth (2012) to show that cluster stability\nselection retains the same guarantees. In summary, cluster stability selection\nenjoys the best of both worlds, yielding a sparse selected set that is both\nstable and has good predictive performance.",
          "link": "http://arxiv.org/abs/2201.00494",
          "publishedOn": "2022-01-05T00:39:30.735Z",
          "wordCount": 617,
          "title": "Cluster Stability Selection. (arXiv:2201.00494v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2104.13628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belkin_M/0/1/0/all/0/1\">Mikhail Belkin</a>",
          "description": "Modern machine learning systems such as deep neural networks are often highly\nover-parameterized so that they can fit the noisy training data exactly, yet\nthey can still achieve small test errors in practice. In this paper, we study\nthis \"benign overfitting\" phenomenon of the maximum margin classifier for\nlinear classification problems. Specifically, we consider data generated from\nsub-Gaussian mixtures, and provide a tight risk bound for the maximum margin\nlinear classifier in the over-parameterized setting. Our results precisely\ncharacterize the condition under which benign overfitting can occur in linear\nclassification problems, and improve on previous work. They also have direct\nimplications for over-parameterized logistic regression.",
          "link": "http://arxiv.org/abs/2104.13628",
          "publishedOn": "2022-01-05T00:39:30.729Z",
          "wordCount": 571,
          "title": "Risk Bounds for Over-parameterized Maximum Margin Classification on Sub-Gaussian Mixtures. (arXiv:2104.13628v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.13236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demirkiran_F/0/1/0/all/0/1\">Ferhat Demirk&#x131;ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cayir_A/0/1/0/all/0/1\">Aykut &#xc7;ay&#x131;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_U/0/1/0/all/0/1\">U&#x11f;ur &#xdc;nal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dag_H/0/1/0/all/0/1\">Hasan Da&#x11f;</a>",
          "description": "Classification of malware families is crucial for a comprehensive\nunderstanding of how they can infect devices, computers, or systems. Thus,\nmalware identification enables security researchers and incident responders to\ntake precautions against malware and accelerate mitigation. API call sequences\nmade by malware are widely utilized features by machine and deep learning\nmodels for malware classification as these sequences represent the behavior of\nmalware. However, traditional machine and deep learning models remain incapable\nof capturing sequence relationships between API calls. On the other hand, the\ntransformer-based models process sequences as a whole and learn relationships\nbetween API calls due to multi-head attention mechanisms and positional\nembeddings. Our experiments demonstrate that the transformer model with one\ntransformer block layer surpassed the widely used base architecture, LSTM.\nMoreover, BERT or CANINE, pre-trained transformer models, outperformed in\nclassifying highly imbalanced malware families according to evaluation metrics,\nF1-score, and AUC score. Furthermore, the proposed bagging-based random\ntransformer forest (RTF), an ensemble of BERT or CANINE, has reached the\nstate-of-the-art evaluation scores on three out of four datasets, particularly\nstate-of-the-art F1-score of 0.6149 on one of the commonly used benchmark\ndataset.",
          "link": "http://arxiv.org/abs/2112.13236",
          "publishedOn": "2022-01-05T00:39:30.723Z",
          "wordCount": 646,
          "title": "An Ensemble of Pre-trained Transformer Models For Imbalanced Multiclass Malware Classification. (arXiv:2112.13236v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06428",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ibriga_H/0/1/0/all/0/1\">Hilda S Ibriga</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sun_W/0/1/0/all/0/1\">Will Wei Sun</a>",
          "description": "We aim to provably complete a sparse and highly-missing tensor in the\npresence of covariate information along tensor modes. Our motivation comes from\nonline advertising where users click-through-rates (CTR) on ads over various\ndevices form a CTR tensor that has about 96% missing entries and has many zeros\non non-missing entries, which makes the standalone tensor completion method\nunsatisfactory. Beside the CTR tensor, additional ad features or user\ncharacteristics are often available. In this paper, we propose\nCovariate-assisted Sparse Tensor Completion (COSTCO) to incorporate covariate\ninformation for the recovery of the sparse tensor. The key idea is to jointly\nextract latent components from both the tensor and the covariate matrix to\nlearn a synthetic representation. Theoretically, we derive the error bound for\nthe recovered tensor components and explicitly quantify the improvements on\nboth the reveal probability condition and the tensor recovery accuracy due to\ncovariates. Finally, we apply COSTCO to an advertisement dataset consisting of\na CTR tensor and ad covariate matrix, leading to 23% accuracy improvement over\nthe baseline. An important by-product is that ad latent components from COSTCO\nreveal interesting ad clusters, which are useful for better ad targeting.",
          "link": "http://arxiv.org/abs/2103.06428",
          "publishedOn": "2022-01-05T00:39:30.716Z",
          "wordCount": 629,
          "title": "Covariate-assisted Sparse Tensor Completion. (arXiv:2103.06428v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hazimeh_H/0/1/0/all/0/1\">Hussein Hazimeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1\">Aakanksha Chowdhery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sathiamoorthy_M/0/1/0/all/0/1\">Maheswaran Sathiamoorthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yihua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazumder_R/0/1/0/all/0/1\">Rahul Mazumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lichan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed H. Chi</a>",
          "description": "The Mixture-of-Experts (MoE) architecture is showing promising results in\nimproving parameter sharing in multi-task learning (MTL) and in scaling\nhigh-capacity neural networks. State-of-the-art MoE models use a trainable\nsparse gate to select a subset of the experts for each input example. While\nconceptually appealing, existing sparse gates, such as Top-k, are not smooth.\nThe lack of smoothness can lead to convergence and statistical performance\nissues when training with gradient-based methods. In this paper, we develop\nDSelect-k: a continuously differentiable and sparse gate for MoE, based on a\nnovel binary encoding formulation. The gate can be trained using first-order\nmethods, such as stochastic gradient descent, and offers explicit control over\nthe number of experts to select. We demonstrate the effectiveness of DSelect-k\non both synthetic and real MTL datasets with up to $128$ tasks. Our experiments\nindicate that DSelect-k can achieve statistically significant improvements in\nprediction and expert selection over popular MoE gates. Notably, on a\nreal-world, large-scale recommender system, DSelect-k achieves over $22\\%$\nimprovement in predictive performance compared to Top-k. We provide an\nopen-source implementation of DSelect-k.",
          "link": "http://arxiv.org/abs/2106.03760",
          "publishedOn": "2022-01-05T00:39:30.698Z",
          "wordCount": 669,
          "title": "DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning. (arXiv:2106.03760v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00766",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boschini_M/0/1/0/all/0/1\">Matteo Boschini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonicelli_L/0/1/0/all/0/1\">Lorenzo Bonicelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buzzega_P/0/1/0/all/0/1\">Pietro Buzzega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porrello_A/0/1/0/all/0/1\">Angelo Porrello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calderara_S/0/1/0/all/0/1\">Simone Calderara</a>",
          "description": "The staple of human intelligence is the capability of acquiring knowledge in\na continuous fashion. In stark contrast, Deep Networks forget catastrophically\nand, for this reason, the sub-field of Class-Incremental Continual Learning\nfosters methods that learn a sequence of tasks incrementally, blending\nsequentially-gained knowledge into a comprehensive prediction.\n\nThis work aims at assessing and overcoming the pitfalls of our previous\nproposal Dark Experience Replay (DER), a simple and effective approach that\ncombines rehearsal and Knowledge Distillation. Inspired by the way our minds\nconstantly rewrite past recollections and set expectations for the future, we\nendow our model with the abilities to i) revise its replay memory to welcome\nnovel information regarding past data ii) pave the way for learning yet unseen\nclasses.\n\nWe show that the application of these strategies leads to remarkable\nimprovements; indeed, the resulting method - termed eXtended-DER (X-DER) -\noutperforms the state of the art on both standard benchmarks (such as CIFAR-100\nand miniImagenet) and a novel one here introduced. To gain a better\nunderstanding, we further provide extensive ablation studies that corroborate\nand extend the findings of our previous research (e.g. the value of Knowledge\nDistillation and flatter minima in continual learning setups).",
          "link": "http://arxiv.org/abs/2201.00766",
          "publishedOn": "2022-01-05T00:39:30.691Z",
          "wordCount": 624,
          "title": "Class-Incremental Continual Learning into the eXtended DER-verse. (arXiv:2201.00766v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.16388",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Azzone_M/0/1/0/all/0/1\">Michele Azzone</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Baviera_R/0/1/0/all/0/1\">Roberto Baviera</a>",
          "description": "Middle-term horizon (months to a year) power consumption prediction is a main\nchallenge in the energy sector, in particular when probabilistic forecasting is\nconsidered. We propose a new modelling approach that incorporates trend,\nseasonality and weather conditions, as explicative variables in a shallow\nNeural Network with an autoregressive feature. We obtain excellent results for\ndensity forecast on the one-year test set applying it to the daily power\nconsumption in New England U.S.A.. The quality of the achieved power\nconsumption probabilistic forecasting has been verified, on the one hand,\ncomparing the results to other standard models for density forecasting and, on\nthe other hand, considering measures that are frequently used in the energy\nsector as pinball loss and CI backtesting.",
          "link": "http://arxiv.org/abs/2006.16388",
          "publishedOn": "2022-01-05T00:39:30.683Z",
          "wordCount": 571,
          "title": "Neural Network Middle-Term Probabilistic Forecasting of Daily Power Consumption. (arXiv:2006.16388v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.13769",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1\">Luoluo Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Simhon_E/0/1/0/all/0/1\">Eran Simhon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kulkarni_C/0/1/0/all/0/1\">Chaitanya Kulkarni</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Noren_D/0/1/0/all/0/1\">David Noren</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mans_R/0/1/0/all/0/1\">Ronny Mans</a>",
          "description": "In the hospital setting, a small percentage of recurrent frequent patients\ncontribute to a disproportional amount of healthcare resource usage. Moreover,\nin many of these cases, patient outcomes can be greatly improved by reducing\nreoccurring visits, especially when they are associated with substance abuse,\nmental health, and medical factors that could be improved by social-behavioral\ninterventions, outpatient or preventative care. Additionally, health care costs\ncan be reduced significantly with fewer preventable recurrent visits.\n\nTo address this, we developed a computationally efficient and interpretable\nframework that both identifies recurrent patients with high utilization and\ndetermines which comorbidities contribute most to their recurrent visits.\nSpecifically, we present a novel algorithm, called the minimum similarity\nassociation rules (MSAR), balancing confidence-support trade-off, to determine\nthe conditions most associated with reoccurring Emergency department (ED) and\ninpatient visits. We validate MSAR on a large Electric Health Record (EHR)\ndataset.",
          "link": "http://arxiv.org/abs/2110.13769",
          "publishedOn": "2022-01-05T00:39:30.672Z",
          "wordCount": 585,
          "title": "Identify comorbidities associated with recurrent ED and in-patient visits. (arXiv:2110.13769v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tabaghi_P/0/1/0/all/0/1\">Puoya Tabaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_E/0/1/0/all/0/1\">Eli Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jianhao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1\">Olgica Milenkovic</a>",
          "description": "Embedding methods for product spaces are powerful techniques for\nlow-distortion and low-dimensional representation of complex data structures.\nHere, we address the new problem of linear classification in product space\nforms -- products of Euclidean, spherical, and hyperbolic spaces. First, we\ndescribe novel formulations for linear classifiers on a Riemannian manifold\nusing geodesics and Riemannian metrics which generalize straight lines and\ninner products in vector spaces. Second, we prove that linear classifiers in\n$d$-dimensional space forms of any curvature have the same expressive power,\ni.e., they can shatter exactly $d+1$ points. Third, we formalize linear\nclassifiers in product space forms, describe the first known perceptron and\nsupport vector machine classifiers for such spaces and establish rigorous\nconvergence results for perceptrons. Moreover, we prove that the\nVapnik-Chervonenkis dimension of linear classifiers in a product space form of\ndimension $d$ is \\emph{at least} $d+1$. We support our theoretical findings\nwith simulations on several datasets, including synthetic data, image data, and\nsingle-cell RNA sequencing (scRNA-seq) data. The results show that\nclassification in low-dimensional product space forms for scRNA-seq data\noffers, on average, a performance improvement of $\\sim15\\%$ when compared to\nthat in Euclidean spaces of the same dimension.",
          "link": "http://arxiv.org/abs/2102.10204",
          "publishedOn": "2022-01-05T00:39:30.324Z",
          "wordCount": 650,
          "title": "Linear Classifiers in Product Space Forms. (arXiv:2102.10204v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We study reinforcement learning (RL) with linear function approximation under\nthe adaptivity constraint. We consider two popular limited adaptivity models:\nthe batch learning model and the rare policy switch model, and propose two\nefficient online RL algorithms for episodic linear Markov decision processes,\nwhere the transition probability and the reward function can be represented as\na linear function of some known feature mapping. In specific, for the batch\nlearning model, our proposed LSVI-UCB-Batch algorithm achieves an $\\tilde\nO(\\sqrt{d^3H^3T} + dHT/B)$ regret, where $d$ is the dimension of the feature\nmapping, $H$ is the episode length, $T$ is the number of interactions and $B$\nis the number of batches. Our result suggests that it suffices to use only\n$\\sqrt{T/dH}$ batches to obtain $\\tilde O(\\sqrt{d^3H^3T})$ regret. For the rare\npolicy switch model, our proposed LSVI-UCB-RareSwitch algorithm enjoys an\n$\\tilde O(\\sqrt{d^3H^3T[1+T/(dH)]^{dH/B}})$ regret, which implies that $dH\\log\nT$ policy switches suffice to obtain the $\\tilde O(\\sqrt{d^3H^3T})$ regret. Our\nalgorithms achieve the same regret as the LSVI-UCB algorithm (Jin et al.,\n2019), yet with a substantially smaller amount of adaptivity. We also establish\na lower bound for the batch learning model, which suggests that the dependency\non $B$ in our regret bound is tight.",
          "link": "http://arxiv.org/abs/2101.02195",
          "publishedOn": "2022-01-05T00:39:30.316Z",
          "wordCount": 667,
          "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation Under Adaptivity Constraints. (arXiv:2101.02195v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.06850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bohang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Du Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>",
          "description": "Recently, Zhang et al.(2021) developed a new neural network architecture\nbased on $\\ell_\\infty$-distance functions, which naturally possesses certified\n$\\ell_\\infty$ robustness by its construction. Despite rigorous theoretical\nguarantees, the model so far can only achieve comparable performance to\nconventional networks. In this paper, we make the following two contributions:\n$\\mathrm{(i)}$ We demonstrate that $\\ell_\\infty$-distance nets enjoy a\nfundamental advantage in certified robustness over conventional networks (under\ntypical certification approaches); $\\mathrm{(ii)}$ With an improved training\nprocess we are able to significantly boost the certified accuracy of\n$\\ell_\\infty$-distance nets. Our training approach largely alleviates the\noptimization problem that arose in the previous training scheme, in particular,\nthe unexpected large Lipschitz constant due to the use of a crucial trick\ncalled $\\ell_p$-relaxation. The core of our training approach is a novel\nobjective function that combines scaled cross-entropy loss and clipped hinge\nloss with a decaying mixing coefficient. Experiments show that using the\nproposed training strategy, the certified accuracy of $\\ell_\\infty$-distance\nnet can be dramatically improved from 33.30% to 40.06% on CIFAR-10\n($\\epsilon=8/255$), meanwhile outperforming other approaches in this area by a\nlarge margin. Our results clearly demonstrate the effectiveness and potential\nof $\\ell_\\infty$-distance net for certified robustness. Codes are available at\nhttps://github.com/zbh2047/L_inf-dist-net-v2.",
          "link": "http://arxiv.org/abs/2110.06850",
          "publishedOn": "2022-01-05T00:39:30.176Z",
          "wordCount": 672,
          "title": "Boosting the Certified Robustness of L-infinity Distance Nets. (arXiv:2110.06850v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00468",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chakrabortty_A/0/1/0/all/0/1\">Abhishek Chakrabortty</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dai_G/0/1/0/all/0/1\">Guorong Dai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tchetgen_E/0/1/0/all/0/1\">Eric Tchetgen Tchetgen</a>",
          "description": "In this article, we aim to provide a general and complete understanding of\nsemi-supervised (SS) causal inference for treatment effects. Specifically, we\nconsider two such estimands: (a) the average treatment effect and (b) the\nquantile treatment effect, as prototype cases, in an SS setting, characterized\nby two available data sets: (i) a labeled data set of size $n$, providing\nobservations for a response and a set of high dimensional covariates, as well\nas a binary treatment indicator; and (ii) an unlabeled data set of size $N$,\nmuch larger than $n$, but without the response observed. Using these two data\nsets, we develop a family of SS estimators which are ensured to be: (1) more\nrobust and (2) more efficient than their supervised counterparts based on the\nlabeled data set only. Beyond the 'standard' double robustness results (in\nterms of consistency) that can be achieved by supervised methods as well, we\nfurther establish root-n consistency and asymptotic normality of our SS\nestimators whenever the propensity score in the model is correctly specified,\nwithout requiring specific forms of the nuisance functions involved. Such an\nimprovement of robustness arises from the use of the massive unlabeled data, so\nit is generally not attainable in a purely supervised setting. In addition, our\nestimators are shown to be semi-parametrically efficient as long as all the\nnuisance functions are correctly specified. Moreover, as an illustration of the\nnuisance estimators, we consider inverse-probability-weighting type kernel\nsmoothing estimators involving unknown covariate transformation mechanisms, and\nestablish in high dimensional scenarios novel results on their uniform\nconvergence rates, which should be of independent interest. Numerical results\non both simulated and real data validate the advantage of our methods over\ntheir supervised counterparts with respect to both robustness and efficiency.",
          "link": "http://arxiv.org/abs/2201.00468",
          "publishedOn": "2022-01-05T00:39:30.026Z",
          "wordCount": 731,
          "title": "A General Framework for Treatment Effect Estimation in Semi-Supervised and High Dimensional Settings. (arXiv:2201.00468v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2102.02976",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gao_R/0/1/0/all/0/1\">Rui Gao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Calmon_F/0/1/0/all/0/1\">Flavio P. Calmon</a>",
          "description": "Machine learning models trained by different optimization algorithms under\ndifferent data distributions can exhibit distinct generalization behaviors. In\nthis paper, we analyze the generalization of models trained by noisy iterative\nalgorithms. We derive distribution-dependent generalization bounds by\nconnecting noisy iterative algorithms to additive noise channels found in\ncommunication and information theory. Our generalization bounds shed light on\nseveral applications, including differentially private stochastic gradient\ndescent (DP-SGD), federated learning, and stochastic gradient Langevin dynamics\n(SGLD). We demonstrate our bounds through numerical experiments, showing that\nthey can help understand recent empirical observations of the generalization\nphenomena of neural networks.",
          "link": "http://arxiv.org/abs/2102.02976",
          "publishedOn": "2022-01-05T00:39:30.019Z",
          "wordCount": 556,
          "title": "Generalization Bounds for Noisy Iterative Algorithms Using Properties of Additive Noise Channels. (arXiv:2102.02976v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_A/0/1/0/all/0/1\">Aolin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raginsky_M/0/1/0/all/0/1\">Maxim Raginsky</a>",
          "description": "We analyze the best achievable performance of Bayesian learning under\ngenerative models by defining and upper-bounding the minimum excess risk (MER):\nthe gap between the minimum expected loss attainable by learning from data and\nthe minimum expected loss that could be achieved if the model realization were\nknown. The definition of MER provides a principled way to define different\nnotions of uncertainties in Bayesian learning, including the aleatoric\nuncertainty and the minimum epistemic uncertainty. Two methods for deriving\nupper bounds for the MER are presented. The first method, generally suitable\nfor Bayesian learning with a parametric generative model, upper-bounds the MER\nby the conditional mutual information between the model parameters and the\nquantity being predicted given the observed data. It allows us to quantify the\nrate at which the MER decays to zero as more data becomes available. Under\nrealizable models, this method also relates the MER to the richness of the\ngenerative function class, notably the VC dimension in binary classification.\nThe second method, particularly suitable for Bayesian learning with a\nparametric predictive model, relates the MER to the minimum estimation error of\nthe model parameters from data. It explicitly shows how the uncertainty in\nmodel parameter estimation translates to the MER and to the final prediction\nuncertainty. We also extend the definition and analysis of MER to the setting\nwith multiple model families and the setting with nonparametric models. Along\nthe discussions we draw some comparisons between the MER in Bayesian learning\nand the excess risk in frequentist learning.",
          "link": "http://arxiv.org/abs/2012.14868",
          "publishedOn": "2022-01-05T00:39:30.001Z",
          "wordCount": 728,
          "title": "Minimum Excess Risk in Bayesian Learning. (arXiv:2012.14868v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiafan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We study the reinforcement learning problem for discounted Markov Decision\nProcesses (MDPs) under the tabular setting. We propose a model-based algorithm\nnamed UCBVI-$\\gamma$, which is based on the \\emph{optimism in the face of\nuncertainty principle} and the Bernstein-type bonus. We show that\nUCBVI-$\\gamma$ achieves an $\\tilde{O}\\big({\\sqrt{SAT}}/{(1-\\gamma)^{1.5}}\\big)$\nregret, where $S$ is the number of states, $A$ is the number of actions,\n$\\gamma$ is the discount factor and $T$ is the number of steps. In addition, we\nconstruct a class of hard MDPs and show that for any algorithm, the expected\nregret is at least $\\tilde{\\Omega}\\big({\\sqrt{SAT}}/{(1-\\gamma)^{1.5}}\\big)$.\nOur upper bound matches the minimax lower bound up to logarithmic factors,\nwhich suggests that UCBVI-$\\gamma$ is nearly minimax optimal for discounted\nMDPs.",
          "link": "http://arxiv.org/abs/2010.00587",
          "publishedOn": "2022-01-05T00:39:29.994Z",
          "wordCount": 590,
          "title": "Nearly Minimax Optimal Reinforcement Learning for Discounted MDPs. (arXiv:2010.00587v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+LeJeune_D/0/1/0/all/0/1\">Daniel LeJeune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javadi_H/0/1/0/all/0/1\">Hamid Javadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard G. Baraniuk</a>",
          "description": "Among the most successful methods for sparsifying deep (neural) networks are\nthose that adaptively mask the network weights throughout training. By\nexamining this masking, or dropout, in the linear case, we uncover a duality\nbetween such adaptive methods and regularization through the so-called\n\"$\\eta$-trick\" that casts both as iteratively reweighted optimizations. We show\nthat any dropout strategy that adapts to the weights in a monotonic way\ncorresponds to an effective subquadratic regularization penalty, and therefore\nleads to sparse solutions. We obtain the effective penalties for several\npopular sparsification strategies, which are remarkably similar to classical\npenalties commonly used in sparse optimization. Considering variational dropout\nas a case study, we demonstrate similar empirical behavior between the adaptive\ndropout method and classical methods on the task of deep network\nsparsification, validating our theory.",
          "link": "http://arxiv.org/abs/2106.07769",
          "publishedOn": "2022-01-05T00:39:29.988Z",
          "wordCount": 611,
          "title": "The Flip Side of the Reweighted Coin: Duality of Adaptive Dropout and Regularization. (arXiv:2106.07769v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.06823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jospin_L/0/1/0/all/0/1\">Laurent Valentin Jospin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1\">Wray Buntine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boussaid_F/0/1/0/all/0/1\">Farid Boussaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laga_H/0/1/0/all/0/1\">Hamid Laga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>",
          "description": "Modern deep learning methods constitute incredibly powerful tools to tackle a\nmyriad of challenging problems. However, since deep learning methods operate as\nblack boxes, the uncertainty associated with their predictions is often\nchallenging to quantify. Bayesian statistics offer a formalism to understand\nand quantify the uncertainty associated with deep neural network predictions.\nThis tutorial provides an overview of the relevant literature and a complete\ntoolset to design, implement, train, use and evaluate Bayesian Neural Networks,\ni.e. Stochastic Artificial Neural Networks trained using Bayesian methods.",
          "link": "http://arxiv.org/abs/2007.06823",
          "publishedOn": "2022-01-05T00:39:29.977Z",
          "wordCount": 570,
          "title": "Hands-on Bayesian Neural Networks -- a Tutorial for Deep Learning Users. (arXiv:2007.06823v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00230",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Duan_J/0/1/0/all/0/1\">Juntao Duan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Popescu_I/0/1/0/all/0/1\">Ionel Popescu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Matzinger_H/0/1/0/all/0/1\">Heinrich Matzinger</a>",
          "description": "It is well known the sample covariance has a consistent bias in the spectrum,\nfor example spectrum of Wishart matrix follows the Marchenko-Pastur law. We in\nthis work introduce an iterative algorithm 'Concent' that actively eliminate\nthis bias and recover the true spectrum for small and moderate dimensions.",
          "link": "http://arxiv.org/abs/2201.00230",
          "publishedOn": "2022-01-05T00:39:29.971Z",
          "wordCount": 474,
          "title": "Recover the spectrum of covariance matrix: a non-asymptotic iterative method. (arXiv:2201.00230v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2003.05783",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nadjahi_K/0/1/0/all/0/1\">Kimia Nadjahi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1\">Alain Durmus</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chizat_L/0/1/0/all/0/1\">L&#xe9;na&#xef;c Chizat</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kolouri_S/0/1/0/all/0/1\">Soheil Kolouri</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shahrampour_S/0/1/0/all/0/1\">Shahin Shahrampour</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Simsekli_U/0/1/0/all/0/1\">Umut &#x15e;im&#x15f;ekli</a>",
          "description": "The idea of slicing divergences has been proven to be successful when\ncomparing two probability measures in various machine learning applications\nincluding generative modeling, and consists in computing the expected value of\na `base divergence' between one-dimensional random projections of the two\nmeasures. However, the topological, statistical, and computational consequences\nof this technique have not yet been well-established. In this paper, we aim at\nbridging this gap and derive various theoretical properties of sliced\nprobability divergences. First, we show that slicing preserves the metric\naxioms and the weak continuity of the divergence, implying that the sliced\ndivergence will share similar topological properties. We then precise the\nresults in the case where the base divergence belongs to the class of integral\nprobability metrics. On the other hand, we establish that, under mild\nconditions, the sample complexity of a sliced divergence does not depend on the\nproblem dimension. We finally apply our general results to several base\ndivergences, and illustrate our theory on both synthetic and real data\nexperiments.",
          "link": "http://arxiv.org/abs/2003.05783",
          "publishedOn": "2022-01-05T00:39:29.955Z",
          "wordCount": 615,
          "title": "Statistical and Topological Properties of Sliced Probability Divergences. (arXiv:2003.05783v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00292",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Xu_S/0/1/0/all/0/1\">Shizhou Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Strohmer_T/0/1/0/all/0/1\">Thomas Strohmer</a>",
          "description": "As machine learning powered decision making is playing an increasingly\nimportant role in our daily lives, it is imperative to strive for fairness of\nthe underlying data processing and algorithms. We propose a pre-processing\nalgorithm for fair data representation via which L2- objective supervised\nlearning algorithms result in an estimation of the Pareto frontier between\nprediction error and statistical disparity. In particular, the present work\napplies the optimal positive definite affine transport maps to approach the\npost-processing Wasserstein barycenter characterization of the optimal fair\nL2-objective supervised learning via a pre-processing data deformation. We call\nthe resulting data Wasserstein pseudo-barycenter. Furthermore, we show that the\nWasserstein geodesics from the learning outcome marginals to the barycenter\ncharacterizes the Pareto frontier between L2-loss and total Wasserstein\ndistance among learning outcome marginals. Thereby, an application of McCann\ninterpolation generalizes the pseudo-barycenter to a family of data\nrepresentations via which L2-objective supervised learning algorithms result in\nthe Pareto frontier. Numerical simulations underscore the advantages of the\nproposed data representation: (1) the pre-processing step is compositive with\narbitrary L2-objective supervised learning methods and unseen data; (2) the\nfair representation protects data privacy by preventing the training machine\nfrom direct or indirect access to the sensitive information of the data; (3)\nthe optimal affine map results in efficient computation of fair supervised\nlearning on high-dimensional data; (4) experimental results shed light on the\nfairness of L2-objective unsupervised learning via the proposed fair data\nrepresentation.",
          "link": "http://arxiv.org/abs/2201.00292",
          "publishedOn": "2022-01-05T00:39:29.948Z",
          "wordCount": 667,
          "title": "Fair Data Representation for Machine Learning at the Pareto Frontier. (arXiv:2201.00292v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01734",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Passino_F/0/1/0/all/0/1\">Francesco Sanna Passino</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Heard_N/0/1/0/all/0/1\">Nicholas A. Heard</a>",
          "description": "Spectral embedding of network adjacency matrices often produces node\nrepresentations living approximately around low-dimensional submanifold\nstructures. In particular, hidden substructure is expected to arise when the\ngraph is generated from a latent position model. Furthermore, the presence of\ncommunities within the network might generate community-specific submanifold\nstructures in the embedding, but this is not explicitly accounted for in most\nstatistical models for networks. In this article, a class of models called\nlatent structure block models (LSBM) is proposed to address such scenarios,\nallowing for graph clustering when community-specific one dimensional manifold\nstructure is present. LSBMs focus on a specific class of latent space model,\nthe random dot product graph (RDPG), and assign a latent submanifold to the\nlatent positions of each community. A Bayesian model for the embeddings arising\nfrom LSBMs is discussed, and shown to have a good performance on simulated and\nreal world network data. The model is able to correctly recover the underlying\ncommunities living in a one-dimensional manifold, even when the parametric form\nof the underlying curves is unknown, achieving remarkable results on a variety\nof real data.",
          "link": "http://arxiv.org/abs/2107.01734",
          "publishedOn": "2022-01-05T00:39:29.942Z",
          "wordCount": 619,
          "title": "Latent structure blockmodels for Bayesian spectral graph clustering. (arXiv:2107.01734v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1811.09469",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Akyildiz_O/0/1/0/all/0/1\">&#xd6;mer Deniz Akyildiz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Crisan_D/0/1/0/all/0/1\">Dan Crisan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Miguez_J/0/1/0/all/0/1\">Joaqu&#xed;n M&#xed;guez</a>",
          "description": "We introduce and analyze a parallel sequential Monte Carlo methodology for\nthe numerical solution of optimization problems that involve the minimization\nof a cost function that consists of the sum of many individual components. The\nproposed scheme is a stochastic zeroth order optimization algorithm which\ndemands only the capability to evaluate small subsets of components of the cost\nfunction. It can be depicted as a bank of samplers that generate particle\napproximations of several sequences of probability measures. These measures are\nconstructed in such a way that they have associated probability density\nfunctions whose global maxima coincide with the global minima of the original\ncost function. The algorithm selects the best performing sampler and uses it to\napproximate a global minimum of the cost function. We prove analytically that\nthe resulting estimator converges to a global minimum of the cost function\nalmost surely and provide explicit convergence rates in terms of the number of\ngenerated Monte Carlo samples and the dimension of the search space. We show,\nby way of numerical examples, that the algorithm can tackle cost functions with\nmultiple minima or with broad \"flat\" regions which are hard to minimize using\ngradient-based techniques.",
          "link": "http://arxiv.org/abs/1811.09469",
          "publishedOn": "2022-01-05T00:39:29.936Z",
          "wordCount": 669,
          "title": "Parallel sequential Monte Carlo for stochastic gradient-free nonconvex optimization. (arXiv:1811.09469v4 [stat.CO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pauli_P/0/1/0/all/0/1\">Patricia Pauli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funcke_N/0/1/0/all/0/1\">Niklas Funcke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gramlich_D/0/1/0/all/0/1\">Dennis Gramlich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Msalmi_M/0/1/0/all/0/1\">Mohamed Amine Msalmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allgower_F/0/1/0/all/0/1\">Frank Allg&#xf6;wer</a>",
          "description": "This paper is concerned with the training of neural networks (NNs) under\nsemidefinite constraints. This type of training problems has recently gained\npopularity since semidefinite constraints can be used to verify interesting\nproperties for NNs that include, e.g., the estimation of an upper bound on the\nLipschitz constant, which relates to the robustness of an NN, or the stability\nof dynamic systems with NN controllers. The utilized semidefinite constraints\nare based on sector constraints satisfied by the underlying activation\nfunctions. Unfortunately, one of the biggest bottlenecks of these new results\nis the required computational effort for incorporating the semidefinite\nconstraints into the training of NNs which is limiting their scalability to\nlarge NNs. We address this challenge by developing interior point methods for\nNN training that we implement using barrier functions for semidefinite\nconstraints. In order to efficiently compute the gradients of the barrier\nterms, we exploit the structure of the semidefinite constraints. In\nexperiments, we demonstrate the superior efficiency of our training method over\nprevious approaches, which allows us, e.g., to use semidefinite constraints in\nthe training of Wasserstein generative adversarial networks, where the\ndiscriminator must satisfy a Lipschitz condition.",
          "link": "http://arxiv.org/abs/2201.00632",
          "publishedOn": "2022-01-05T00:39:29.929Z",
          "wordCount": 625,
          "title": "Neural network training under semidefinite constraints. (arXiv:2201.00632v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.03781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yancey_R/0/1/0/all/0/1\">Robin Elizabeth Yancey</a>",
          "description": "Each woman living in the United States has about 1 in 8 chance of developing\ninvasive breast cancer. The mitotic cell count is one of the most common tests\nto assess the aggressiveness or grade of breast cancer. In this prognosis,\nhistopathology images must be examined by a pathologist using high-resolution\nmicroscopes to count the cells. Unfortunately, can be an exhaustive task with\npoor reproducibility, especially for non-experts. Deep learning networks have\nrecently been adapted to medical applications which are able to automatically\nlocalize these regions of interest. However, these region-based networks lack\nthe ability to take advantage of the segmentation features produced by a full\nimage CNN which are often used as a sole method of detection. Therefore, the\nproposed method leverages Faster RCNN for object detection while fusing\nsegmentation features generated by a UNet with RGB image features to achieve an\nF-score of 0.508 on the MITOS-ATYPIA 2014 mitosis counting challenge dataset,\noutperforming state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2002.03781",
          "publishedOn": "2022-01-05T00:39:29.911Z",
          "wordCount": 607,
          "title": "Deep Feature Fusion for Mitosis Counting. (arXiv:2002.03781v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Duo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fekri_F/0/1/0/all/0/1\">Faramarz Fekri</a>",
          "description": "The actor-critic RL is widely used in various robotic control tasks. By\nviewing the actor-critic RL from the perspective of variational inference (VI),\nthe policy network is trained to obtain the approximate posterior of actions\ngiven the optimality criteria. However, in practice, the actor-critic RL may\nyield suboptimal policy estimates due to the amortization gap and insufficient\nexploration. In this work, inspired by the previous use of Hamiltonian Monte\nCarlo (HMC) in VI, we propose to integrate the policy network of actor-critic\nRL with HMC, which is termed as {\\it Hamiltonian Policy}. As such we propose to\nevolve actions from the base policy according to HMC, and our proposed method\nhas many benefits. First, HMC can improve the policy distribution to better\napproximate the posterior and hence reduce the amortization gap. Second, HMC\ncan also guide the exploration more to the regions of action spaces with higher\nQ values, enhancing the exploration efficiency. Further, instead of directly\napplying HMC into RL, we propose a new leapfrog operator to simulate the\nHamiltonian dynamics. Finally, in safe RL problems, we find that the proposed\nmethod can not only improve the achieved return, but also reduce safety\nconstraint violations by discarding potentially unsafe actions. With\ncomprehensive empirical experiments on continuous control baselines, including\nMuJoCo and PyBullet Roboschool, we show that the proposed approach is a\ndata-efficient and easy-to-implement improvement over previous actor-critic\nmethods.",
          "link": "http://arxiv.org/abs/2103.12020",
          "publishedOn": "2022-01-05T00:39:29.905Z",
          "wordCount": 685,
          "title": "Improving Actor-Critic Reinforcement Learning via Hamiltonian Monte Carlo Method. (arXiv:2103.12020v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08966",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sigrist_F/0/1/0/all/0/1\">Fabio Sigrist</a>",
          "description": "Latent Gaussian models and boosting are widely used techniques in statistics\nand machine learning. Tree-boosting shows excellent prediction accuracy on many\ndata sets, but potential drawbacks are that it assumes conditional independence\nof samples, produces discontinuous predictions for, e.g., spatial data, and it\ncan have difficulty with high-cardinality categorical variables. Latent\nGaussian models, such as Gaussian process and grouped random effects models,\nare flexible prior models which explicitly model dependence among samples and\nwhich allow for efficient learning of predictor functions and for making\nprobabilistic predictions. However, existing latent Gaussian models usually\nassume either a zero or a linear prior mean function which can be an\nunrealistic assumption. This article introduces a novel approach that combines\nboosting and latent Gaussian models to remedy the above-mentioned drawbacks and\nto leverage the advantages of both techniques. We obtain increased prediction\naccuracy compared to existing approaches in both simulated and real-world data\nexperiments.",
          "link": "http://arxiv.org/abs/2105.08966",
          "publishedOn": "2022-01-05T00:39:29.898Z",
          "wordCount": 603,
          "title": "Latent Gaussian Model Boosting. (arXiv:2105.08966v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2201.00272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Astudillo_R/0/1/0/all/0/1\">Raul Astudillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frazier_P/0/1/0/all/0/1\">Peter I. Frazier</a>",
          "description": "Bayesian optimization (BO) is a framework for global optimization of\nexpensive-to-evaluate objective functions. Classical BO methods assume that the\nobjective function is a black box. However, internal information about\nobjective function computation is often available. For example, when optimizing\na manufacturing line's throughput with simulation, we observe the number of\nparts waiting at each workstation, in addition to the overall throughput.\nRecent BO methods leverage such internal information to dramatically improve\nperformance. We call these \"grey-box\" BO methods because they treat objective\ncomputation as partially observable and even modifiable, blending the black-box\napproach with so-called \"white-box\" first-principles knowledge of objective\nfunction computation. This tutorial describes these methods, focusing on BO of\ncomposite objective functions, where one can observe and selectively evaluate\nindividual constituents that feed into the overall objective; and\nmulti-fidelity BO, where one can evaluate cheaper approximations of the\nobjective function by varying parameters of the evaluation oracle.",
          "link": "http://arxiv.org/abs/2201.00272",
          "publishedOn": "2022-01-05T00:39:29.892Z",
          "wordCount": 592,
          "title": "Thinking inside the box: A tutorial on grey-box Bayesian optimization. (arXiv:2201.00272v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00698",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nanzhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Qinzhuo Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Haibin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongxiao Zhang</a>",
          "description": "Large-scale or high-resolution geologic models usually comprise a huge number\nof grid blocks, which can be computationally demanding and time-consuming to\nsolve with numerical simulators. Therefore, it is advantageous to upscale\ngeologic models (e.g., hydraulic conductivity) from fine-scale (high-resolution\ngrids) to coarse-scale systems. Numerical upscaling methods have been proven to\nbe effective and robust for coarsening geologic models, but their efficiency\nremains to be improved. In this work, a deep-learning-based method is proposed\nto upscale the fine-scale geologic models, which can assist to improve\nupscaling efficiency significantly. In the deep learning method, a deep\nconvolutional neural network (CNN) is trained to approximate the relationship\nbetween the coarse grid of hydraulic conductivity fields and the hydraulic\nheads, which can then be utilized to replace the numerical solvers while\nsolving the flow equations for each coarse block. In addition, physical laws\n(e.g., governing equations and periodic boundary conditions) can also be\nincorporated into the training process of the deep CNN model, which is termed\nthe theory-guided convolutional neural network (TgCNN). With the physical\ninformation considered, dependence on the data volume of training the deep\nlearning models can be reduced greatly. Several subsurface flow cases are\nintroduced to test the performance of the proposed deep-learning-based\nupscaling method, including 2D and 3D cases, and isotropic and anisotropic\ncases. The results show that the deep learning method can provide equivalent\nupscaling accuracy to the numerical method, and efficiency can be improved\nsignificantly compared to numerical upscaling.",
          "link": "http://arxiv.org/abs/2201.00698",
          "publishedOn": "2022-01-05T00:39:29.886Z",
          "wordCount": 676,
          "title": "Deep-learning-based upscaling method for geologic models via theory-guided convolutional neural network. (arXiv:2201.00698v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Ziyang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yihao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>",
          "description": "Reinforcement learning (RL) has drawn increasing interests in recent years\ndue to its tremendous success in various applications. However, standard RL\nalgorithms can only be applied for single reward function, and cannot adapt to\nan unseen reward function quickly. In this paper, we advocate a general\noperator view of reinforcement learning, which enables us to directly\napproximate the operator that maps from reward function to value function. The\nbenefit of learning the operator is that we can incorporate any new reward\nfunction as input and attain its corresponding value function in a zero-shot\nmanner. To approximate this special type of operator, we design a number of\nnovel operator neural network architectures based on its theoretical\nproperties. Our design of operator networks outperform the existing methods and\nthe standard design of general purpose operator network, and we demonstrate the\nbenefit of our operator deep Q-learning framework in several tasks including\nreward transferring for offline policy evaluation (OPE) and reward transferring\nfor offline policy optimization in a range of tasks.",
          "link": "http://arxiv.org/abs/2201.00236",
          "publishedOn": "2022-01-05T00:39:29.878Z",
          "wordCount": 593,
          "title": "Operator Deep Q-Learning: Zero-Shot Reward Transferring in Reinforcement Learning. (arXiv:2201.00236v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00001",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Maddix_D/0/1/0/all/0/1\">Danielle C Maddix</a>, <a href=\"http://arxiv.org/find/math/1/au:+Saad_N/0/1/0/all/0/1\">Nadim Saad</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wang_Y/0/1/0/all/0/1\">Yuyang Wang</a>",
          "description": "The transport of traffic flow can be modeled by the advection equation.\nFinite difference and finite volumes methods have been used to numerically\nsolve this hyperbolic equation on a mesh. Advection has also been modeled\ndiscretely on directed graphs using the graph advection operator [4, 18]. In\nthis paper, we first show that we can reformulate this graph advection operator\nas a finite difference scheme. We then propose the Directed Graph Advection\nMat\\'ern Gaussian Process (DGAMGP) model that incorporates the dynamics of this\ngraph advection operator into the kernel of a trainable Mat\\'ern Gaussian\nProcess to effectively model traffic flow and its uncertainty as an advective\nprocess on a directed graph.",
          "link": "http://arxiv.org/abs/2201.00001",
          "publishedOn": "2022-01-05T00:39:28.413Z",
          "wordCount": 555,
          "title": "Modeling Advection on Directed Graphs using Mat\\'ern Gaussian Processes for Traffic Flow. (arXiv:2201.00001v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2201.00057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Y/0/1/0/all/0/1\">Yangjun Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1\">Yann Dubois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maddison_C/0/1/0/all/0/1\">Chris J. Maddison</a>",
          "description": "Machine learning systems often experience a distribution shift between\ntraining and testing. In this paper, we introduce a simple variational\nobjective whose optima are exactly the set of all representations on which risk\nminimizers are guaranteed to be robust to any distribution shift that preserves\nthe Bayes predictor, e.g., covariate shifts. Our objective has two components.\nFirst, a representation must remain discriminative for the task, i.e., some\npredictor must be able to simultaneously minimize the source and target risk.\nSecond, the representation's marginal support needs to be the same across\nsource and target. We make this practical by designing self-supervised learning\nmethods that only use unlabelled data and augmentations to train robust\nrepresentations. Our objectives achieve state-of-the-art results on DomainBed,\nand give insights into the robustness of recent methods, such as CLIP.",
          "link": "http://arxiv.org/abs/2201.00057",
          "publishedOn": "2022-01-05T00:39:28.406Z",
          "wordCount": 556,
          "title": "Optimal Representations for Covariate Shift. (arXiv:2201.00057v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11612",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiafan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We study reinforcement learning (RL) with linear function approximation.\nExisting algorithms for this problem only have high-probability regret and/or\nProbably Approximately Correct (PAC) sample complexity guarantees, which cannot\nguarantee the convergence to the optimal policy. In this paper, in order to\novercome the limitation of existing algorithms, we propose a new algorithm\ncalled FLUTE, which enjoys uniform-PAC convergence to the optimal policy with\nhigh probability. The uniform-PAC guarantee is the strongest possible guarantee\nfor reinforcement learning in the literature, which can directly imply both PAC\nand high probability regret bounds, making our algorithm superior to all\nexisting algorithms with linear function approximation. At the core of our\nalgorithm is a novel minimax value function estimator and a multi-level\npartition scheme to select the training samples from historical observations.\nBoth of these techniques are new and of independent interest.",
          "link": "http://arxiv.org/abs/2106.11612",
          "publishedOn": "2022-01-03T07:15:42.387Z",
          "wordCount": 600,
          "title": "Uniform-PAC Bounds for Reinforcement Learning with Linear Function Approximation. (arXiv:2106.11612v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.03152",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Biswas_N/0/1/0/all/0/1\">Niloy Biswas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1\">Lester Mackey</a>",
          "description": "Markov chain Monte Carlo (MCMC) provides asymptotically consistent estimates\nof intractable posterior expectations as the number of iterations tends to\ninfinity. However, in large data applications, MCMC can be computationally\nexpensive per iteration. This has catalyzed interest in sampling methods such\nas approximate MCMC, which trade off asymptotic consistency for improved\ncomputational speed. In this article, we propose estimators based on couplings\nof Markov chains to assess the quality of such asymptotically biased sampling\nmethods. The estimators give empirical upper bounds of the Wassertein distance\nbetween the limiting distribution of the asymptotically biased sampling method\nand the original target distribution of interest. We establish theoretical\nguarantees for our upper bounds and show that our estimators can remain\neffective in high dimensions. We apply our quality measures to stochastic\ngradient MCMC, variational Bayes, and Laplace approximations for tall data and\nto approximate MCMC for Bayesian logistic regression in 4500 dimensions and\nBayesian linear regression in 50000 dimensions.",
          "link": "http://arxiv.org/abs/2112.03152",
          "publishedOn": "2022-01-03T07:15:42.353Z",
          "wordCount": 593,
          "title": "Bounding Wasserstein distance with couplings. (arXiv:2112.03152v2 [stat.CO] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02864",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Reuvers_H/0/1/0/all/0/1\">Hanno Reuvers</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Wijler_E/0/1/0/all/0/1\">Etienne Wijler</a>",
          "description": "We consider a high-dimensional model in which variables are observed over\ntime and space. The model consists of a spatio-temporal regression containing a\ntime lag and a spatial lag of the dependent variable. Unlike classical spatial\nautoregressive models, we do not rely on a predetermined spatial interaction\nmatrix, but infer all spatial interactions from the data. Assuming sparsity, we\nestimate the spatial and temporal dependence fully data-driven by penalizing a\nset of Yule-Walker equations. This regularization can be left unstructured, but\nwe also propose customized shrinkage procedures when observations originate\nfrom spatial grids (e.g. satellite images). Finite sample error bounds are\nderived and estimation consistency is established in an asymptotic framework\nwherein the sample size and the number of spatial units diverge jointly.\nExogenous variables can be included as well. A simulation exercise shows strong\nfinite sample performance compared to competing procedures. As an empirical\napplication, we model satellite measured NO2 concentrations in London. Our\napproach delivers forecast improvements over a competitive benchmark and we\ndiscover evidence for strong spatial interactions.",
          "link": "http://arxiv.org/abs/2108.02864",
          "publishedOn": "2022-01-03T07:15:42.306Z",
          "wordCount": 616,
          "title": "Sparse Generalized Yule-Walker Estimation for Large Spatio-temporal Autoregressions with an Application to NO2 Satellite Data. (arXiv:2108.02864v2 [econ.EM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15401",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Kycia_R/0/1/0/all/0/1\">Rados&#x142;aw A. Kycia</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Niemczynowicz_A/0/1/0/all/0/1\">Agnieszka Niemczynowicz</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Niezurawska_Zajac_J/0/1/0/all/0/1\">Joanna Nie&#x17c;urawska-Zaj&#x105;c</a>",
          "description": "Correlation and cluster analyses (k-Means, Gaussian Mixture Models) were\nperformed on Generation Z engagement surveys at the workplace. The clustering\nindicates relations between various factors that describe the engagement of\nemployees. The most noticeable factors are a clear statement about the\nresponsibilities at work, and challenging work. These factors are essential in\npractice. The results of this paper can be used in preparing better\nmotivational systems aimed at Generation Z employees.",
          "link": "http://arxiv.org/abs/2112.15401",
          "publishedOn": "2022-01-03T07:15:42.291Z",
          "wordCount": 529,
          "title": "Towards the global vision of engagement of Generation Z at the workplace: Mathematical modeling. (arXiv:2112.15401v1 [econ.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2004.00184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Besserve_M/0/1/0/all/0/1\">Michel Besserve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">R&#xe9;my Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janzing_D/0/1/0/all/0/1\">Dominik Janzing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "Generative models can be trained to emulate complex empirical data, but are\nthey useful to make predictions in the context of previously unobserved\nenvironments? An intuitive idea to promote such extrapolation capabilities is\nto have the architecture of such model reflect a causal graph of the true data\ngenerating process, such that one can intervene on each node independently of\nthe others. However, the nodes of this graph are usually unobserved, leading to\noverparameterization and lack of identifiability of the causal structure. We\ndevelop a theoretical framework to address this challenging situation by\ndefining a weaker form of identifiability, based on the principle of\nindependence of mechanisms. We demonstrate on toy examples that classical\nstochastic gradient descent can hinder the model's extrapolation capabilities,\nsuggesting independence of mechanisms should be enforced explicitly during\ntraining. Experiments on deep generative models trained on real world data\nsupport these insights and illustrate how the extrapolation capabilities of\nsuch models can be leveraged.",
          "link": "http://arxiv.org/abs/2004.00184",
          "publishedOn": "2022-01-03T07:15:42.285Z",
          "wordCount": 623,
          "title": "A theory of independent mechanisms for extrapolation in generative models. (arXiv:2004.00184v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.07068",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dockhorn_T/0/1/0/all/0/1\">Tim Dockhorn</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vahdat_A/0/1/0/all/0/1\">Arash Vahdat</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kreis_K/0/1/0/all/0/1\">Karsten Kreis</a>",
          "description": "Score-based generative models (SGMs) have demonstrated remarkable synthesis\nquality. SGMs rely on a diffusion process that gradually perturbs the data\ntowards a tractable distribution, while the generative model learns to denoise.\nThe complexity of this denoising task is, apart from the data distribution\nitself, uniquely determined by the diffusion process. We argue that current\nSGMs employ overly simplistic diffusions, leading to unnecessarily complex\ndenoising processes, which limit generative modeling performance. Based on\nconnections to statistical mechanics, we propose a novel critically-damped\nLangevin diffusion (CLD) and show that CLD-based SGMs achieve superior\nperformance. CLD can be interpreted as running a joint diffusion in an extended\nspace, where the auxiliary variables can be considered \"velocities\" that are\ncoupled to the data variables as in Hamiltonian dynamics. We derive a novel\nscore matching objective for CLD and show that the model only needs to learn\nthe score function of the conditional distribution of the velocity given data,\nan easier task than learning scores of the data directly. We also derive a new\nsampling scheme for efficient synthesis from CLD-based diffusion models. We\nfind that CLD outperforms previous SGMs in synthesis quality for similar\nnetwork architectures and sampling compute budgets. We show that our novel\nsampler for CLD significantly outperforms solvers such as Euler--Maruyama. Our\nframework provides new insights into score-based denoising diffusion models and\ncan be readily used for high-resolution image synthesis. Project page and code:\nhttps://nv-tlabs.github.io/CLD-SGM.",
          "link": "http://arxiv.org/abs/2112.07068",
          "publishedOn": "2022-01-03T07:15:42.278Z",
          "wordCount": 670,
          "title": "Score-Based Generative Modeling with Critically-Damped Langevin Diffusion. (arXiv:2112.07068v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maddox_W/0/1/0/all/0/1\">Wesley J. Maddox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_S/0/1/0/all/0/1\">Sanyam Kapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1\">Andrew Gordon Wilson</a>",
          "description": "While recent work on conjugate gradient methods and Lanczos decompositions\nhave achieved scalable Gaussian process inference with highly accurate point\npredictions, in several implementations these iterative methods appear to\nstruggle with numerical instabilities in learning kernel hyperparameters, and\npoor test likelihoods. By investigating CG tolerance, preconditioner rank, and\nLanczos decomposition rank, we provide a particularly simple prescription to\ncorrect these issues: we recommend that one should use a small CG tolerance\n($\\epsilon \\leq 0.01$) and a large root decomposition size ($r \\geq 5000$).\nMoreover, we show that L-BFGS-B is a compelling optimizer for Iterative GPs,\nachieving convergence with fewer gradient updates.",
          "link": "http://arxiv.org/abs/2112.15246",
          "publishedOn": "2022-01-03T07:15:42.253Z",
          "wordCount": 526,
          "title": "When are Iterative Gaussian Processes Reliably Accurate?. (arXiv:2112.15246v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15392",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Benning_F/0/1/0/all/0/1\">Felix Benning</a>",
          "description": "This thesis reviews numerical optimization methods with machine learning\nproblems in mind. Since machine learning models are highly parametrized, we\nfocus on methods suited for high dimensional optimization. We build intuition\non quadratic models to figure out which methods are suited for non-convex\noptimization, and develop convergence proofs on convex functions for this\nselection of methods. With this theoretical foundation for stochastic gradient\ndescent and momentum methods, we try to explain why the methods used commonly\nin the machine learning field are so successful. Besides explaining successful\nheuristics, the last chapter also provides a less extensive review of more\ntheoretical methods, which are not quite as popular in practice. So in some\nsense this work attempts to answer the question: Why are the default Tensorflow\noptimizers included in the defaults?",
          "link": "http://arxiv.org/abs/2112.15392",
          "publishedOn": "2022-01-03T07:15:42.136Z",
          "wordCount": 559,
          "title": "High Dimensional Optimization through the Lens of Machine Learning. (arXiv:2112.15392v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2110.06394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weitong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We study the model-based reward-free reinforcement learning with linear\nfunction approximation for episodic Markov decision processes (MDPs). In this\nsetting, the agent works in two phases. In the exploration phase, the agent\ninteracts with the environment and collects samples without the reward. In the\nplanning phase, the agent is given a specific reward function and uses samples\ncollected from the exploration phase to learn a good policy. We propose a new\nprovably efficient algorithm, called UCRL-RFE under the Linear Mixture MDP\nassumption, where the transition probability kernel of the MDP can be\nparameterized by a linear function over certain feature mappings defined on the\ntriplet of state, action, and next state. We show that to obtain an\n$\\epsilon$-optimal policy for arbitrary reward function, UCRL-RFE needs to\nsample at most $\\tilde{\\mathcal{O}}(H^5d^2\\epsilon^{-2})$ episodes during the\nexploration phase. Here, $H$ is the length of the episode, $d$ is the dimension\nof the feature mapping. We also propose a variant of UCRL-RFE using\nBernstein-type bonus and show that it needs to sample at most\n$\\tilde{\\mathcal{O}}(H^4d(H + d)\\epsilon^{-2})$ to achieve an\n$\\epsilon$-optimal policy. By constructing a special class of linear Mixture\nMDPs, we also prove that for any reward-free algorithm, it needs to sample at\nleast $\\tilde \\Omega(H^2d\\epsilon^{-2})$ episodes to obtain an\n$\\epsilon$-optimal policy. Our upper bound matches the lower bound in terms of\nthe dependence on $\\epsilon$ and the dependence on $d$ if $H \\ge d$.",
          "link": "http://arxiv.org/abs/2110.06394",
          "publishedOn": "2022-01-03T07:15:42.111Z",
          "wordCount": 700,
          "title": "Reward-Free Model-Based Reinforcement Learning with Linear Function Approximation. (arXiv:2110.06394v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heiss_J/0/1/0/all/0/1\">Jakob Heiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teichmann_J/0/1/0/all/0/1\">Josef Teichmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wutte_H/0/1/0/all/0/1\">Hanna Wutte</a>",
          "description": "We prove in this paper that wide ReLU neural networks (NNs) with at least one\nhidden layer optimized with l2-regularization on the parameters enforces\nmulti-task learning due to representation-learning - also in the limit width to\ninfinity. This is in contrast to multiple other idealized settings discussed in\nthe literature where wide (ReLU)-NNs loose their ability to benefit from\nmulti-task learning in the limit width to infinity. We deduce the multi-task\nlearning ability from proving an exact quantitative macroscopic\ncharacterization of the learned NN in function space.",
          "link": "http://arxiv.org/abs/2112.15577",
          "publishedOn": "2022-01-03T07:15:42.103Z",
          "wordCount": 543,
          "title": "Infinite wide (finite depth) Neural Networks benefit from multi-task learning unlike shallow Gaussian Processes -- an exact quantitative macroscopic characterization. (arXiv:2112.15577v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.01711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chengchun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shikai Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongtu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jieping Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>",
          "description": "A/B testing, or online experiment is a standard business strategy to compare\na new product with an old one in pharmaceutical, technological, and traditional\nindustries. Major challenges arise in online experiments of two-sided\nmarketplace platforms (e.g., Uber) where there is only one unit that receives a\nsequence of treatments over time. In those experiments, the treatment at a\ngiven time impacts current outcome as well as future outcomes. The aim of this\npaper is to introduce a reinforcement learning framework for carrying A/B\ntesting in these experiments, while characterizing the long-term treatment\neffects. Our proposed testing procedure allows for sequential monitoring and\nonline updating. It is generally applicable to a variety of treatment designs\nin different industries. In addition, we systematically investigate the\ntheoretical properties (e.g., size and power) of our testing procedure.\nFinally, we apply our framework to both simulated data and a real-world data\nexample obtained from a technological company to illustrate its advantage over\nthe current practice. A Python implementation of our test is available at\nhttps://github.com/callmespring/CausalRL.",
          "link": "http://arxiv.org/abs/2002.01711",
          "publishedOn": "2022-01-03T07:15:42.097Z",
          "wordCount": 663,
          "title": "Dynamic Causal Effects Evaluation in A/B Testing with a Reinforcement Learning Framework. (arXiv:2002.01711v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.09338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yunhui Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuolin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunter_C/0/1/0/all/0/1\">Carl A. Gunter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Recent advances in machine learning have largely benefited from the massive\naccessible training data. However, large-scale data sharing has raised great\nprivacy concerns. In this work, we propose a novel privacy-preserving data\nGenerative model based on the PATE framework (G-PATE), aiming to train a\nscalable differentially private data generator that preserves high generated\ndata utility. Our approach leverages generative adversarial nets to generate\ndata, combined with private aggregation among different discriminators to\nensure strong privacy guarantees. Compared to existing approaches, G-PATE\nsignificantly improves the use of privacy budgets. In particular, we train a\nstudent data generator with an ensemble of teacher discriminators and propose a\nnovel private gradient aggregation mechanism to ensure differential privacy on\nall information that flows from teacher discriminators to the student\ngenerator. In addition, with random projection and gradient discretization, the\nproposed gradient aggregation mechanism is able to effectively deal with\nhigh-dimensional gradient vectors. Theoretically, we prove that G-PATE ensures\ndifferential privacy for the data generator. Empirically, we demonstrate the\nsuperiority of G-PATE over prior work through extensive experiments. We show\nthat G-PATE is the first work being able to generate high-dimensional image\ndata with high data utility under limited privacy budgets ($\\epsilon \\le 1$).\nOur code is available at https://github.com/AI-secure/G-PATE.",
          "link": "http://arxiv.org/abs/1906.09338",
          "publishedOn": "2022-01-03T07:15:41.872Z",
          "wordCount": 675,
          "title": "G-PATE: Scalable Differentially Private Data Generator via Private Aggregation of Teacher Discriminators. (arXiv:1906.09338v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15423",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chang_J/0/1/0/all/0/1\">Jinyuan Chang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+He_J/0/1/0/all/0/1\">Jing He</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_L/0/1/0/all/0/1\">Lin Yang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yao_Q/0/1/0/all/0/1\">Qiwei Yao</a>",
          "description": "We propose to model matrix time series based on a tensor CP-decomposition.\nInstead of using an iterative algorithm which is the standard practice for\nestimating CP-decompositions, we propose a new and one-pass estimation\nprocedure based on a generalized eigenanalysis constructed from the serial\ndependence structure of the underlying process. A key idea of the new procedure\nis to project a generalized eigenequation defined in terms of rank-reduced\nmatrices to a lower-dimensional one with full-ranked matrices, to avoid the\nintricacy of the former of which the number of eigenvalues can be zero, finite\nand infinity. The asymptotic theory has been established under a general\nsetting without the stationarity. It shows, for example, that all the component\ncoefficient vectors in the CP-decomposition are estimated consistently with the\ndifferent error rates, depending on the relative sizes between the dimensions\nof time series and the sample size. The proposed model and the estimation\nmethod are further illustrated with both simulated and real data; showing\neffective dimension-reduction in modelling and forecasting matrix time series.",
          "link": "http://arxiv.org/abs/2112.15423",
          "publishedOn": "2022-01-03T07:15:41.613Z",
          "wordCount": 587,
          "title": "Modelling matrix time series via a tensor CP-decomposition. (arXiv:2112.15423v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2010.01278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinghui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>",
          "description": "Adversarial training is so far the most effective strategy in defending\nagainst adversarial examples. However, it suffers from high computational costs\ndue to the iterative adversarial attacks in each training step. Recent studies\nshow that it is possible to achieve fast Adversarial Training by performing a\nsingle-step attack with random initialization. However, such an approach still\nlags behind state-of-the-art adversarial training algorithms on both stability\nand model robustness. In this work, we develop a new understanding towards Fast\nAdversarial Training, by viewing random initialization as performing randomized\nsmoothing for better optimization of the inner maximization problem. Following\nthis new perspective, we also propose a new initialization strategy, backward\nsmoothing, to further improve the stability and model robustness over\nsingle-step robust training methods. Experiments on multiple benchmarks\ndemonstrate that our method achieves similar model robustness as the original\nTRADES method while using much less training time ($\\sim$3x improvement with\nthe same training schedule).",
          "link": "http://arxiv.org/abs/2010.01278",
          "publishedOn": "2022-01-03T07:15:41.607Z",
          "wordCount": 616,
          "title": "Efficient Robust Training via Backward Smoothing. (arXiv:2010.01278v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15094",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Faradonbeh_M/0/1/0/all/0/1\">Mohamad Kazem Shirani Faradonbeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Faradonbeh_M/0/1/0/all/0/1\">Mohamad Sadegh Shirani Faradonbeh</a>",
          "description": "Linear dynamical systems are canonical models for learning-based control of\nplants with uncertain dynamics. The setting consists of a stochastic\ndifferential equation that captures the state evolution of the plant\nunderstudy, while the true dynamics matrices are unknown and need to be learned\nfrom the observed data of state trajectory. An important issue is to ensure\nthat the system is stabilized and destabilizing control actions due to model\nuncertainties are precluded as soon as possible. A reliable stabilization\nprocedure for this purpose that can effectively learn from unstable data to\nstabilize the system in a finite time is not currently available. In this work,\nwe propose a novel Bayesian learning algorithm that stabilizes unknown\ncontinuous-time stochastic linear systems. The presented algorithm is flexible\nand exposes effective stabilization performance after a remarkably short time\nperiod of interacting with the system.",
          "link": "http://arxiv.org/abs/2112.15094",
          "publishedOn": "2022-01-03T07:15:41.599Z",
          "wordCount": 577,
          "title": "Bayesian Algorithms Learn to Stabilize Unknown Continuous-Time Systems. (arXiv:2112.15094v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14949",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>",
          "description": "In this paper, we focus on the decentralized optimization problem over the\nStiefel manifold, which is defined on a connected network of $d$ agents. The\nobjective is an average of $d$ local functions, and each function is privately\nheld by an agent and encodes its data. The agents can only communicate with\ntheir neighbors in a collaborative effort to solve this problem. In existing\nmethods, multiple rounds of communications are required to guarantee the\nconvergence, giving rise to high communication costs. In contrast, this paper\nproposes a decentralized algorithm, called DESTINY, which only invokes a single\nround of communications per iteration. DESTINY combines gradient tracking\ntechniques with a novel approximate augmented Lagrangian function. The global\nconvergence to stationary points is rigorously established. Comprehensive\nnumerical experiments demonstrate that DESTINY has a strong potential to\ndeliver a cutting-edge performance in solving a variety of testing problems.",
          "link": "http://arxiv.org/abs/2112.14949",
          "publishedOn": "2022-01-03T07:15:41.574Z",
          "wordCount": 588,
          "title": "Decentralized Optimization Over the Stiefel Manifold by an Approximate Augmented Lagrangian Function. (arXiv:2112.14949v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinghui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "\"Benign overfitting\", where classifiers memorize noisy training data yet\nstill achieve a good generalization performance, has drawn great attention in\nthe machine learning community. To explain this surprising phenomenon, a series\nof works have provided theoretical justification in over-parameterized linear\nregression, classification, and kernel methods. However, it is not clear if\nbenign overfitting still occurs in the presence of adversarial examples, i.e.,\nexamples with tiny and intentional perturbations to fool the classifiers. In\nthis paper, we show that benign overfitting indeed occurs in adversarial\ntraining, a principled approach to defend against adversarial examples. In\ndetail, we prove the risk bounds of the adversarially trained linear classifier\non the mixture of sub-Gaussian data under $\\ell_p$ adversarial perturbations.\nOur result suggests that under moderate perturbations, adversarially trained\nlinear classifiers can achieve the near-optimal standard and adversarial risks,\ndespite overfitting the noisy training data. Numerical experiments validate our\ntheoretical findings.",
          "link": "http://arxiv.org/abs/2112.15250",
          "publishedOn": "2022-01-03T07:15:41.549Z",
          "wordCount": 576,
          "title": "Benign Overfitting in Adversarially Robust Linear Classification. (arXiv:2112.15250v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06201",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Loecher_M/0/1/0/all/0/1\">Markus Loecher</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>",
          "description": "Tree-based algorithms such as random forests and gradient boosted trees\ncontinue to be among the most popular and powerful machine learning models used\nacross multiple disciplines. The conventional wisdom of estimating the impact\nof a feature in tree based models is to measure the \\textit{node-wise reduction\nof a loss function}, which (i) yields only global importance measures and (ii)\nis known to suffer from severe biases. Conditional feature contributions (CFCs)\nprovide \\textit{local}, case-by-case explanations of a prediction by following\nthe decision path and attributing changes in the expected output of the model\nto each feature along the path. However, Lundberg et al. pointed out a\npotential bias of CFCs which depends on the distance from the root of a tree.\nThe by now immensely popular alternative, SHapley Additive exPlanation (SHAP)\nvalues appear to mitigate this bias but are computationally much more\nexpensive. Here we contribute a thorough comparison of the explanations\ncomputed by both methods on a set of 164 publicly available classification\nproblems in order to provide data-driven algorithm recommendations to current\nresearchers. For random forests, we find extremely high similarities and\ncorrelations of both local and global SHAP values and CFC scores, leading to\nvery similar rankings and interpretations. Analogous conclusions hold for the\nfidelity of using global feature importance scores as a proxy for the\npredictive power associated with each feature.",
          "link": "http://arxiv.org/abs/2108.06201",
          "publishedOn": "2022-01-03T07:15:41.541Z",
          "wordCount": 684,
          "title": "Data-driven advice for interpreting local and global model predictions in bioinformatics problems. (arXiv:2108.06201v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.13794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Weiran Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Sean Qian</a>",
          "description": "The effectiveness of traditional traffic prediction methods is often\nextremely limited when forecasting traffic dynamics in early morning. The\nreason is that traffic can break down drastically during the early morning\ncommute, and the time and duration of this break-down vary substantially from\nday to day. Early morning traffic forecast is crucial to inform morning-commute\ntraffic management, but they are generally challenging to predict in advance,\nparticularly by midnight. In this paper, we propose to mine Twitter messages as\na probing method to understand the impacts of people's work and rest patterns\nin the evening/midnight of the previous day to the next-day morning traffic.\nThe model is tested on freeway networks in Pittsburgh as experiments. The\nresulting relationship is surprisingly simple and powerful. We find that, in\ngeneral, the earlier people rest as indicated from Tweets, the more congested\nroads will be in the next morning. The occurrence of big events in the evening\nbefore, represented by higher or lower tweet sentiment than normal, often\nimplies lower travel demand in the next morning than normal days. Besides,\npeople's tweeting activities in the night before and early morning are\nstatistically associated with congestion in morning peak hours. We make use of\nsuch relationships to build a predictive framework which forecasts morning\ncommute congestion using people's tweeting profiles extracted by 5 am or as\nlate as the midnight prior to the morning. The Pittsburgh study supports that\nour framework can precisely predict morning congestion, particularly for some\nroad segments upstream of roadway bottlenecks with large day-to-day congestion\nvariation. Our approach considerably outperforms those existing methods without\nTwitter message features, and it can learn meaningful representation of demand\nfrom tweeting profiles that offer managerial insights.",
          "link": "http://arxiv.org/abs/2009.13794",
          "publishedOn": "2022-01-03T07:15:41.525Z",
          "wordCount": 765,
          "title": "From Twitter to Traffic Predictor: Next-Day Morning Traffic Prediction Using Social Media Data. (arXiv:2009.13794v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15258",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lam_K/0/1/0/all/0/1\">Ka Kin Lam</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>",
          "description": "There have been significant efforts devoted to solving the longevity risk\ngiven that a continuous growth in population ageing has become a severe issue\nfor many developed countries over the past few decades. The Cairns-Blake-Dowd\n(CBD) model, which incorporates cohort effects parameters in its parsimonious\ndesign, is one of the most well-known approaches for mortality modelling at\nhigher ages and longevity risk. This article proposes a novel mixed-effects\ntime-series approach for mortality modelling and forecasting with\nconsiderations of age groups dependence and random cohort effects parameters.\nThe proposed model can disclose more mortality data information and provide a\nnatural quantification of the model parameters uncertainties with no\npre-specified constraint required for estimating the cohort effects parameters.\nThe abilities of the proposed approach are demonstrated through two\napplications with empirical male and female mortality data. The proposed\napproach shows remarkable improvements in terms of forecast accuracy compared\nto the CBD model in the short-, mid-and long-term forecasting using mortality\ndata of several developed countries in the numerical examples.",
          "link": "http://arxiv.org/abs/2112.15258",
          "publishedOn": "2022-01-03T07:15:41.517Z",
          "wordCount": 608,
          "title": "Random cohort effects and age groups dependency structure for mortality modelling and forecasting: Mixed-effects time-series model approach. (arXiv:2112.15258v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mengin_E/0/1/0/all/0/1\">Elie Mengin</a> (SAMM), <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1\">Fabrice Rossi</a> (CEREMADE)",
          "description": "In this paper, we address the problem of finding a correspondence, or\nmatching, between the functions of two programs in binary form, which is one of\nthe most common task in binary diffing. We introduce a new formulation of this\nproblem as a particular instance of a graph edit problem over the call graphs\nof the programs. In this formulation, the quality of a mapping is evaluated\nsimultaneously with respect to both function content and call graph\nsimilarities. We show that this formulation is equivalent to a network\nalignment problem. We propose a solving strategy for this problem based on\nmax-product belief propagation. Finally, we implement a prototype of our\nmethod, called QBinDiff, and propose an extensive evaluation which shows that\nour approach outperforms state of the art diffing tools.",
          "link": "http://arxiv.org/abs/2112.15337",
          "publishedOn": "2022-01-03T07:15:41.510Z",
          "wordCount": 579,
          "title": "Binary Diffing as a Network Alignment Problem via Belief Propagation. (arXiv:2112.15337v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1906.12072",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Azais_J/0/1/0/all/0/1\">J.-M. Aza&#xef;s</a>, <a href=\"http://arxiv.org/find/math/1/au:+Castro_Y/0/1/0/all/0/1\">Y. De Castro</a>",
          "description": "We investigate multiple testing and variable selection using the Least Angle\nRegression (LARS) algorithm in high dimensions under the assumption of Gaussian\nnoise. LARS is known to produce a piecewise affine solution path with change\npoints referred to as the knots of the LARS path. The key to our results is an\nexpression in closed form of the exact joint law of a $K$-tuple of knots\nconditional on the variables selected by LARS, namely the so-called\npost-selection joint law of the LARS knots. Numerical experiments demonstrate\nthe perfect fit of our findings.\n\nThis paper makes three main contributions. First, we build testing procedures\non variables entering the model along the LARS path in the general design case\nwhen the noise level can be unknown. These testing procedures are referred to\nas the Generalized $t$-Spacing tests (GtSt) and we prove that they have an\nexact non-asymptotic level (i.e., the Type I error is exactly controlled). This\nextends work of (Taylor et al., 2014) where the spacing test works for\nconsecutive knots and known variance. Second, we introduce a new exact multiple\nfalse negatives test after model selection in the general design case when the\nnoise level may be unknown. We prove that this testing procedure has exact\nnon-asymptotic level for general design and unknown noise level. Third, we give\nan exact control of the false discovery rate under orthogonal design\nassumption. Monte Carlo simulations and a real data experiment are provided to\nillustrate our results in this case. Of independent interest, we introduce an\nequivalent formulation of the LARS algorithm based on a recursive function.",
          "link": "http://arxiv.org/abs/1906.12072",
          "publishedOn": "2022-01-03T07:15:41.490Z",
          "wordCount": 790,
          "title": "Multiple Testing and Variable Selection along the path of the Least Angle Regression. (arXiv:1906.12072v4 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15595",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Irons_N/0/1/0/all/0/1\">Nicholas J. Irons</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scetbon_M/0/1/0/all/0/1\">Meyer Scetbon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pal_S/0/1/0/all/0/1\">Soumik Pal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Harchaoui_Z/0/1/0/all/0/1\">Zaid Harchaoui</a>",
          "description": "Triangular flows, also known as Kn\\\"{o}the-Rosenblatt measure couplings,\ncomprise an important building block of normalizing flow models for generative\nmodeling and density estimation, including popular autoregressive flow models\nsuch as real-valued non-volume preserving transformation models (Real NVP). We\npresent statistical guarantees and sample complexity bounds for triangular flow\nstatistical models. In particular, we establish the statistical consistency and\nthe finite sample convergence rates of the Kullback-Leibler estimator of the\nKn\\\"{o}the-Rosenblatt measure coupling using tools from empirical process\ntheory. Our results highlight the anisotropic geometry of function classes at\nplay in triangular flows, shed light on optimal coordinate ordering, and lead\nto statistical guarantees for Jacobian flows. We conduct numerical experiments\non synthetic data to illustrate the practical implications of our theoretical\nfindings.",
          "link": "http://arxiv.org/abs/2112.15595",
          "publishedOn": "2022-01-03T07:15:41.482Z",
          "wordCount": 555,
          "title": "Triangular Flows for Generative Modeling: Statistical Consistency, Smoothness Classes, and Fast Rates. (arXiv:2112.15595v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2112.07110",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bhattacharya_R/0/1/0/all/0/1\">Riddhiman Bhattacharya</a>",
          "description": "The gradient noise of Stochastic Gradient Descent (SGD) is considered to play\na key role in its properties (e.g. escaping low potential points and\nregularization). Past research has indicated that the covariance of the SGD\nerror done via minibatching plays a critical role in determining its\nregularization and escape from low potential points. It is however not much\nexplored how much the distribution of the error influences the behavior of the\nalgorithm. Motivated by some new research in this area, we prove universality\nresults by showing that noise classes that have the same mean and covariance\nstructure of SGD via minibatching have similar properties. We mainly consider\nthe Multiplicative Stochastic Gradient Descent (M-SGD) algorithm as introduced\nby Wu et al., which has a much more general noise class than the SGD algorithm\ndone via minibatching. We establish nonasymptotic bounds for the M-SGD\nalgorithm mainly with respect to the Stochastic Differential Equation\ncorresponding to SGD via minibatching. We also show that the M-SGD error is\napproximately a scaled Gaussian distribution with mean $0$ at any fixed point\nof the M-SGD algorithm. We also establish bounds for the convergence of the\nM-SGD algorithm in the strongly convex regime.",
          "link": "http://arxiv.org/abs/2112.07110",
          "publishedOn": "2022-01-03T07:15:41.469Z",
          "wordCount": 657,
          "title": "Non Asymptotic Bounds for Optimization via Online Multiplicative Stochastic Gradient Descent. (arXiv:2112.07110v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shunqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurkoski_B/0/1/0/all/0/1\">Brian M. Kurkoski</a>",
          "description": "Approximate message passing (AMP) is a promising technique for unknown signal\nreconstruction of certain high-dimensional linear systems with non-Gaussian\nsignaling. A distinguished feature of the AMP-type algorithms is that their\ndynamics can be rigorously described by state evolution. However, state\nevolution does not necessarily guarantee the convergence of iterative\nalgorithms. To solve the convergence problem of AMP-type algorithms in\nprinciple, this paper proposes a memory AMP (MAMP) under a sufficient statistic\ncondition, named sufficient statistic MAMP (SS-MAMP). We show that the\ncovariance matrices of SS-MAMP are L-banded and convergent. Given an arbitrary\nMAMP, we can construct an SS-MAMP by damping, which not only ensures the\nconvergence of MAMP but also preserves the orthogonality of MAMP, i.e., its\ndynamics can be rigorously described by state evolution. As a byproduct, we\nprove that the Bayes-optimal orthogonal/vector AMP (BO-OAMP/VAMP) is an\nSS-MAMP. As a result, we reveal two interesting properties of BO-OAMP/VAMP for\nlarge systems: 1) the covariance matrices are L-banded and are convergent in\nBO-OAMP/VAMP, and 2) damping and memory are useless (i.e., do not bring\nperformance improvement) in BO-OAMP/VAMP. As an example, we construct a\nsufficient statistic Bayes-optimal MAMP (BO-MAMP), which is Bayes optimal if\nits state evolution has a unique fixed point and its MSE is not worse than the\noriginal BO-MAMP. Finally, simulations are provided to verify the validity and\naccuracy of the theoretical results.",
          "link": "http://arxiv.org/abs/2112.15327",
          "publishedOn": "2022-01-03T07:15:41.461Z",
          "wordCount": 660,
          "title": "Sufficient Statistic Memory AMP. (arXiv:2112.15327v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14868",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+So_B/0/1/0/all/0/1\">Banghee So</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Valdez_E/0/1/0/all/0/1\">Emiliano A. Valdez</a>",
          "description": "Classification predictive modeling involves the accurate assignment of\nobservations in a dataset to target classes or categories. There is an\nincreasing growth of real-world classification problems with severely\nimbalanced class distributions. In this case, minority classes have much fewer\nobservations to learn from than those from majority classes. Despite this\nsparsity, a minority class is often considered the more interesting class yet\ndeveloping a scientific learning algorithm suitable for the observations\npresents countless challenges. In this article, we suggest a novel multi-class\nclassification algorithm specialized to handle severely imbalanced classes\nbased on the method we refer to as SAMME.C2. It blends the flexible mechanics\nof the boosting techniques from SAMME algorithm, a multi-class classifier, and\nAda.C2 algorithm, a cost-sensitive binary classifier designed to address highly\nclass imbalances. Not only do we provide the resulting algorithm but we also\nestablish scientific and statistical formulation of our proposed SAMME.C2\nalgorithm. Through numerical experiments examining various degrees of\nclassifier difficulty, we demonstrate consistent superior performance of our\nproposed model.",
          "link": "http://arxiv.org/abs/2112.14868",
          "publishedOn": "2022-01-03T07:15:41.438Z",
          "wordCount": 593,
          "title": "The SAMME.C2 algorithm for severely imbalanced multi-class classification. (arXiv:2112.14868v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1911.12360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zixiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1\">Difan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "A recent line of research on deep learning focuses on the extremely\nover-parameterized setting, and shows that when the network width is larger\nthan a high degree polynomial of the training sample size $n$ and the inverse\nof the target error $\\epsilon^{-1}$, deep neural networks learned by\n(stochastic) gradient descent enjoy nice optimization and generalization\nguarantees. Very recently, it is shown that under certain margin assumptions on\nthe training data, a polylogarithmic width condition suffices for two-layer\nReLU networks to converge and generalize (Ji and Telgarsky, 2019). However,\nwhether deep neural networks can be learned with such a mild\nover-parameterization is still an open question. In this work, we answer this\nquestion affirmatively and establish sharper learning guarantees for deep ReLU\nnetworks trained by (stochastic) gradient descent. In specific, under certain\nassumptions made in previous work, our optimization and generalization\nguarantees hold with network width polylogarithmic in $n$ and $\\epsilon^{-1}$.\nOur results push the study of over-parameterized deep neural networks towards\nmore practical settings.",
          "link": "http://arxiv.org/abs/1911.12360",
          "publishedOn": "2022-01-03T07:15:41.431Z",
          "wordCount": 653,
          "title": "How Much Over-parameterization Is Sufficient to Learn Deep ReLU Networks?. (arXiv:1911.12360v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1\">Jorge F. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tobar_F/0/1/0/all/0/1\">Felipe Tobar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vicuna_M/0/1/0/all/0/1\">Mario Vicu&#xf1;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordova_F/0/1/0/all/0/1\">Felipe Cordova</a>",
          "description": "Information-theoretic measures have been widely adopted in the design of\nfeatures for learning and decision problems. Inspired by this, we look at the\nrelationship between i) a weak form of information loss in the Shannon sense\nand ii) the operation loss in the minimum probability of error (MPE) sense when\nconsidering a family of lossy continuous representations (features) of a\ncontinuous observation. We present several results that shed light on this\ninterplay. Our first result offers a lower bound on a weak form of information\nloss as a function of its respective operation loss when adopting a discrete\nlossy representation (quantization) instead of the original raw observation.\nFrom this, our main result shows that a specific form of vanishing information\nloss (a weak notion of asymptotic informational sufficiency) implies a\nvanishing MPE loss (or asymptotic operational sufficiency) when considering a\ngeneral family of lossy continuous representations. Our theoretical findings\nsupport the observation that the selection of feature representations that\nattempt to capture informational sufficiency is appropriate for learning, but\nthis selection is a rather conservative design principle if the intended goal\nis achieving MPE in classification. Supporting this last point, and under some\nstructural conditions, we show that it is possible to adopt an alternative\nnotion of informational sufficiency (strictly weaker than pure sufficiency in\nthe mutual information sense) to achieve operational sufficiency in learning.",
          "link": "http://arxiv.org/abs/2112.15238",
          "publishedOn": "2022-01-03T07:15:41.424Z",
          "wordCount": 668,
          "title": "Studying the Interplay between Information Loss and Operation Loss in Representations for Classification. (arXiv:2112.15238v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.00827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weitong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lihong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "Thompson Sampling (TS) is one of the most effective algorithms for solving\ncontextual multi-armed bandit problems. In this paper, we propose a new\nalgorithm, called Neural Thompson Sampling, which adapts deep neural networks\nfor both exploration and exploitation. At the core of our algorithm is a novel\nposterior distribution of the reward, where its mean is the neural network\napproximator, and its variance is built upon the neural tangent features of the\ncorresponding neural network. We prove that, provided the underlying reward\nfunction is bounded, the proposed algorithm is guaranteed to achieve a\ncumulative regret of $\\mathcal{O}(T^{1/2})$, which matches the regret of other\ncontextual bandit algorithms in terms of total round number $T$. Experimental\ncomparisons with other benchmark bandit algorithms on various data sets\ncorroborate our theory.",
          "link": "http://arxiv.org/abs/2010.00827",
          "publishedOn": "2022-01-03T07:15:41.417Z",
          "wordCount": 578,
          "title": "Neural Thompson Sampling. (arXiv:2010.00827v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.12961",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yuan_C/0/1/0/all/0/1\">Chaoxia Yuan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ying_C/0/1/0/all/0/1\">Chao Ying</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fang_F/0/1/0/all/0/1\">Fang Fang</a>",
          "description": "Support vector machine (SVM) is a powerful classification method that has\nachieved great success in many fields. Since its performance can be seriously\nimpaired by redundant covariates, model selection techniques are widely used\nfor SVM with high dimensional covariates. As an alternative to model selection,\nsignificant progress has been made in the area of model averaging in the past\ndecades. Yet no frequentist model averaging method was considered for SVM. This\nwork aims to fill the gap and to propose a frequentist model averaging\nprocedure for SVM which selects the optimal weight by cross validation. Even\nwhen the number of covariates diverges at an exponential rate of the sample\nsize, we show asymptotic optimality of the proposed method in the sense that\nthe ratio of its hinge loss to the lowest possible loss converges to one. We\nalso derive the convergence rate which provides more insights to model\naveraging. Compared to model selection methods of SVM which require a tedious\nbut critical task of tuning parameter selection, the model averaging method\navoids the task and shows promising performances in the empirical studies.",
          "link": "http://arxiv.org/abs/2112.12961",
          "publishedOn": "2022-01-03T07:15:41.410Z",
          "wordCount": 647,
          "title": "Optimal Model Averaging of Support Vector Machines in Diverging Model Spaces. (arXiv:2112.12961v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08250",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Anderer_M/0/1/0/all/0/1\">Matthias Anderer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>",
          "description": "Hierarchical forecasting with intermittent time series is a challenge in both\nresearch and empirical studies. Extensive research focuses on improving the\naccuracy of each hierarchy, especially the intermittent time series at bottom\nlevels. Then hierarchical reconciliation could be used to improve the overall\nperformance further. In this paper, we present a\n\\emph{hierarchical-forecasting-with-alignment} approach that treats the bottom\nlevel forecasts as mutable to ensure higher forecasting accuracy on the upper\nlevels of the hierarchy. We employ a pure deep learning forecasting approach\nN-BEATS for continuous time series at the top levels and a widely used\ntree-based algorithm LightGBM for the intermittent time series at the bottom\nlevel. The \\emph{hierarchical-forecasting-with-alignment} approach is a simple\nyet effective variant of the bottom-up method, accounting for biases that are\ndifficult to observe at the bottom level. It allows suboptimal forecasts at the\nlower level to retain a higher overall performance. The approach in this\nempirical study was developed by the first author during the M5 Forecasting\nAccuracy competition, ranking second place. The method is also business\norientated and could benefit for business strategic planning.",
          "link": "http://arxiv.org/abs/2103.08250",
          "publishedOn": "2022-01-03T07:15:41.334Z",
          "wordCount": 638,
          "title": "Hierarchical forecasting with a top-down alignment of independent level forecasts. (arXiv:2103.08250v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15383",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Seroussi_I/0/1/0/all/0/1\">Inbar Seroussi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ringel_Z/0/1/0/all/0/1\">Zohar Ringel</a>",
          "description": "Deep neural networks (DNNs) are powerful tools for compressing and distilling\ninformation. Due to their scale and complexity, often involving billions of\ninter-dependent internal degrees of freedom, exact analysis approaches often\nfall short. A common strategy in such cases is to identify slow degrees of\nfreedom that average out the erratic behavior of the underlying fast\nmicroscopic variables. Here, we identify such a separation of scales occurring\nin over-parameterized deep convolutional neural networks (CNNs) at the end of\ntraining. It implies that neuron pre-activations fluctuate in a nearly Gaussian\nmanner with a deterministic latent kernel. While for CNNs with infinitely many\nchannels these kernels are inert, for finite CNNs they adapt and learn from\ndata in an analytically tractable manner. The resulting thermodynamic theory of\ndeep learning yields accurate predictions on several deep non-linear CNN toy\nmodels. In addition, it provides new ways of analyzing and understanding CNNs.",
          "link": "http://arxiv.org/abs/2112.15383",
          "publishedOn": "2022-01-03T07:15:41.326Z",
          "wordCount": 584,
          "title": "Separation of scales and a thermodynamic description of feature learning in some CNNs. (arXiv:2112.15383v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mengin_E/0/1/0/all/0/1\">Elie Mengin</a> (SAMM), <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1\">Fabrice Rossi</a> (CEREMADE)",
          "description": "In this paper, we present a novel algorithm to address the Network Alignment\nproblem. It is inspired from a previous message passing framework of Bayati et\nal. [2] and includes several modifications designed to significantly speed up\nthe message updates as well as to enforce their convergence. Experiments show\nthat our proposed model outperforms other state-of-the-art solvers. Finally, we\npropose an application of our method in order to address the Binary Diffing\nproblem. We show that our solution provides better assignment than the\nreference differs in almost all submitted instances and outline the importance\nof leveraging the graphical structure of binary programs.",
          "link": "http://arxiv.org/abs/2112.15336",
          "publishedOn": "2022-01-03T07:15:41.265Z",
          "wordCount": 562,
          "title": "Improved Algorithm for the Network Alignment Problem with Application to Binary Diffing. (arXiv:2112.15336v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Astudillo_R/0/1/0/all/0/1\">Raul Astudillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frazier_P/0/1/0/all/0/1\">Peter I. Frazier</a>",
          "description": "We consider Bayesian optimization of the output of a network of functions,\nwhere each function takes as input the output of its parent nodes, and where\nthe network takes significant time to evaluate. Such problems arise, for\nexample, in reinforcement learning, engineering design, and manufacturing.\nWhile the standard Bayesian optimization approach observes only the final\noutput, our approach delivers greater query efficiency by leveraging\ninformation that the former ignores: intermediate output within the network.\nThis is achieved by modeling the nodes of the network using Gaussian processes\nand choosing the points to evaluate using, as our acquisition function, the\nexpected improvement computed with respect to the implied posterior on the\nobjective. Although the non-Gaussian nature of this posterior prevents\ncomputing our acquisition function in closed form, we show that it can be\nefficiently maximized via sample average approximation. In addition, we prove\nthat our method is asymptotically consistent, meaning that it finds a globally\noptimal solution as the number of evaluations grows to infinity, thus\ngeneralizing previously known convergence results for the expected improvement.\nNotably, this holds even though our method might not evaluate the domain\ndensely, instead leveraging problem structure to leave regions unexplored.\nFinally, we show that our approach dramatically outperforms standard Bayesian\noptimization methods in several synthetic and real-world problems.",
          "link": "http://arxiv.org/abs/2112.15311",
          "publishedOn": "2022-01-03T07:15:41.025Z",
          "wordCount": 640,
          "title": "Bayesian Optimization of Function Networks. (arXiv:2112.15311v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14877",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bui_Thanh_T/0/1/0/all/0/1\">Tan Bui-Thanh</a>",
          "description": "One of the reasons that many neural networks are capable of replicating\ncomplicated tasks or functions is their universality property. The past few\ndecades have seen many attempts in providing constructive proofs for single or\nclass of neural networks. This paper is an effort to provide a unified and\nconstructive framework for the universality of a large class of activations\nincluding most of existing activations and beyond. At the heart of the\nframework is the concept of neural network approximate identity. It turns out\nthat most of existing activations are neural network approximate identity, and\nthus universal in the space of continuous of functions on compacta. The\nframework induces several advantages. First, it is constructive with elementary\nmeans from functional analysis, probability theory, and numerical analysis.\nSecond, it is the first unified attempt that is valid for most of existing\nactivations. Third, as a by product, the framework provides the first\nuniversity proof for some of the existing activation functions including Mish,\nSiLU, ELU, GELU, and etc. Fourth, it discovers new activations with guaranteed\nuniversality property. Indeed, any activation\\textemdash whose $\\k$th\nderivative, with $\\k$ being an integer, is integrable and essentially\nbounded\\textemdash is universal. Fifth, for a given activation and error\ntolerance, the framework provides precisely the architecture of the\ncorresponding one-hidden neural network with predetermined number of neuron,\nand the values of weights/biases.",
          "link": "http://arxiv.org/abs/2112.14877",
          "publishedOn": "2022-01-03T07:15:40.990Z",
          "wordCount": 652,
          "title": "A Unified and Constructive Framework for the Universality of Neural Networks. (arXiv:2112.14877v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Exarchakis_G/0/1/0/all/0/1\">Georgios Exarchakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oubari_O/0/1/0/all/0/1\">Omar Oubari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenz_G/0/1/0/all/0/1\">Gregor Lenz</a>",
          "description": "We propose a simple and efficient clustering method for high-dimensional data\nwith a large number of clusters. Our algorithm achieves high-performance by\nevaluating distances of datapoints with a subset of the cluster centres. Our\ncontribution is substantially more efficient than k-means as it does not\nrequire an all to all comparison of data points and clusters. We show that the\noptimal solutions of our approximation are the same as in the exact solution.\nHowever, our approach is considerably more efficient at extracting these\nclusters compared to the state-of-the-art. We compare our approximation with\nthe exact k-means and alternative approximation approaches on a series of\nstandardised clustering tasks. For the evaluation, we consider the algorithmic\ncomplexity, including number of operations to convergence, and the stability of\nthe results.",
          "link": "http://arxiv.org/abs/2112.14793",
          "publishedOn": "2022-01-03T07:15:40.939Z",
          "wordCount": 574,
          "title": "A sampling-based approach for efficient clustering in large datasets. (arXiv:2112.14793v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15265",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1\">Lang Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pal_S/0/1/0/all/0/1\">Soumik Pal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Harchaoui_Z/0/1/0/all/0/1\">Zaid Harchaoui</a>",
          "description": "Optimal transport (OT) and its entropy regularized offspring have recently\ngained a lot of attention in both machine learning and AI domains. In\nparticular, optimal transport has been used to develop probability metrics\nbetween probability distributions. We introduce in this paper an independence\ncriterion based on entropy regularized optimal transport. Our criterion can be\nused to test for independence between two samples. We establish non-asymptotic\nbounds for our test statistic, and study its statistical behavior under both\nthe null and alternative hypothesis. Our theoretical results involve tools from\nU-process theory and optimal transport theory. We present experimental results\non existing benchmarks, illustrating the interest of the proposed criterion.",
          "link": "http://arxiv.org/abs/2112.15265",
          "publishedOn": "2022-01-03T07:15:40.909Z",
          "wordCount": 523,
          "title": "Entropy Regularized Optimal Transport Independence Criterion. (arXiv:2112.15265v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14862",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Jadbabaie_A/0/1/0/all/0/1\">Ali Jadbabaie</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mania_H/0/1/0/all/0/1\">Horia Mania</a>, <a href=\"http://arxiv.org/find/math/1/au:+Shah_D/0/1/0/all/0/1\">Devavrat Shah</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sra_S/0/1/0/all/0/1\">Suvrit Sra</a>",
          "description": "We revisit a model for time-varying linear regression that assumes the\nunknown parameters evolve according to a linear dynamical system.\nCounterintuitively, we show that when the underlying dynamics are stable the\nparameters of this model can be estimated from data by combining just two\nordinary least squares estimates. We offer a finite sample guarantee on the\nestimation error of our method and discuss certain advantages it has over\nExpectation-Maximization (EM), which is the main approach proposed by prior\nwork.",
          "link": "http://arxiv.org/abs/2112.14862",
          "publishedOn": "2022-01-03T07:15:40.892Z",
          "wordCount": 508,
          "title": "Time varying regression with hidden linear dynamics. (arXiv:2112.14862v1 [math.ST])"
        }
      ]
    },
    {
      "title": "Machine Learning",
      "feedUrl": "https://www.reddit.com/r/MachineLearning/.rss",
      "siteUrl": "https://www.reddit.com/r/MachineLearning/",
      "articles": [
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s3e08b/r_hypertransformer_model_generation_for/",
          "author": null,
          "description": "submitted by    /u/hardmaru  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s3e08b/r_hypertransformer_model_generation_for/",
          "publishedOn": "2022-01-14T00:18:40.000Z",
          "wordCount": 103,
          "title": "[R] HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s3bebw/n_if_you_are_interested_in_graph_representation/",
          "author": null,
          "description": "Hi r/machinelearning,\n For the past 1.5 years I have been organizing an online journal club on the topic of Graph Representation Learning. We meet either weekly or fortnightly via Zoom to discuss a relevant paper.\n We are a small and friendly group and we would like to invite others who have similar interests to join us.\n We meet on Thursdays, 6:00pm-7:30pm, Canada/Pacific timezone, and out next meeting is on January 20, 2022.\n You are welcome to join us here.\n Cheers!\n    submitted by    /u/YodaML  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s3bebw/n_if_you_are_interested_in_graph_representation/",
          "publishedOn": "2022-01-13T22:21:53.000Z",
          "wordCount": 224,
          "title": "[N] If you are interested in graph representation learning with Graph Neural Networks, then read on"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s391xo/p_colab_notebook_to_create_aigenerated_pokémon_in/",
          "author": null,
          "description": "https://colab.research.google.com/drive/1A3t2gQofQGeXo5z1BAr1zqYaqVg3czKd?usp=sharing\n Last month I did an experiment with AI-Generated Pokémon on a fine trained ruDALL-E that went unexpectedly viral. Today, I've open-sourced the finetuned model and released a Colab Notebook that lets you generate Pokemon with just two clicks. Also, I added an infinite generation feature which works surprisingly well!\n    submitted by    /u/minimaxir  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s391xo/p_colab_notebook_to_create_aigenerated_pokémon_in/",
          "publishedOn": "2022-01-13T20:37:53.000Z",
          "wordCount": 154,
          "title": "[P] Colab Notebook to create AI-Generated Pokémon in 2 clicks"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s38afp/trouble_modelling_heavytailed_distributions/",
          "author": null,
          "description": "I'm currently working a problem where I'm attempting to reconstruct 1x600 long Fourier vectors using an Autoencoder, where each element in the vector is a frequency (increasing sequentially). I have a set of 17,000 spectra which I apply Numpy's rfft to obtain the corresponding Fourier vector. After transforming all of my data, I use an sklearn StandardScaler() to Standardize each column (frequency). From here, I am training my NN on this Standardized data. I have created plots of two samples from my data for illustration purposes.\n ​\n https://preview.redd.it/w357enqdhib81.png?width=1526&format=png&auto=webp&s=b34d65ada9d241614b22944b2648cc60845f801c\n ​\n https://preview.redd.it/96d3i8cfhib81.png?width=1526&format=png&auto=webp&s=1f89c75902685cb500271b2235fc382e584e0d09\n You can see that in …",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s38afp/trouble_modelling_heavytailed_distributions/",
          "publishedOn": "2022-01-13T20:04:56.000Z",
          "wordCount": 554,
          "title": "Trouble Modelling Heavy-Tailed Distributions [Research]"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s36dqk/d_solo_machine_learning_engineer_woes/",
          "author": null,
          "description": "I work at a start-up as the sole machine learning engineer, and I have been tasked to develop a state-of-the-art model on a domain specific problem.\n Some information on the task:\n  \nComputer vision problem - complex keypoint localisation / 3D mesh prediction\n No training data - I'm proposing that I build a synthetic data pipeline supplemented with real-world data.\n  \nI feel like there is too much for me to be doing at once: reading papers, writing plans and documentation, implementing code and training models, reporting to management, etc.\n My boss understands that research takes a long time. Nevertheless, I feel like management does not fully grasp that results on this are not certain or even likely.\n Even though the work environment is good, I am not handling the uncertainty around results well mentally.\n I wanted to ask, in your experience, what is a typical team size for this kind of job? And do you have any advice?\n Thanks\n    submitted by    /u/Hgat  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s36dqk/d_solo_machine_learning_engineer_woes/",
          "publishedOn": "2022-01-13T18:41:28.000Z",
          "wordCount": 574,
          "title": "[D] Solo machine learning engineer woes"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s33hs8/d_portals_for_outsourcing_preliminary_data/",
          "author": null,
          "description": "I am currently working on building a machine learning model, which will be used to automatically label tweets with regards to some prespecified labels (a total of four).\n Together with a team we are looking for an online tool which we could use to outsource preliminary labeling for later training of the model. So far we have been using a program built specifically for this task by one of the co-researchers, but now we want to switch to something more mainstream.\n The task we want to outsource goes as follows: participants will be given the text of the tweet and they will have to label it according to prespecified labels. Simple as that.\n Since the tweets are in Polish we are not interested in any additional features, which would be specific to the English population.\n We are currently considering the two following platforms:\n https://prodi.gy/demo\n https://universaldatatool.com/app/\n Did any of you take part in a similar study and/or have some other experience with using the above tools?\n Are there any other tools that could work for the task above that you could recommend?\n    submitted by    /u/Hub_Pli  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s33hs8/d_portals_for_outsourcing_preliminary_data/",
          "publishedOn": "2022-01-13T16:35:49.000Z",
          "wordCount": 324,
          "title": "[D] Portals for outsourcing preliminary data labeling"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s32lin/d_what_has_been_your_experience_as_a_machine/",
          "author": null,
          "description": "As the title says, I am curious to learn more about what this sub’s experience has been working with/as management. \n As context, the company I work for (series b startup) is asking me to take over as the machine learning team manager. While I’m flattered that they see me as a “rising star”, I am a bit concerned about moving away from the actual building, testing, and deployment of machine learning models. I have been an ML engineer for about a year and prior to that I worked as a DS at large tech firm for 2 years and have managed multiple teams prior to that as well. My questions for the sub are as follows:\n  \nDoes moving into a management role take me out of contention for more IC roles in the broader job market? Or do recruiters not care as much about that?\n  \nI recognize that I will be doing less code development as manager, but just how much of a drop off should I expect?\n ML Managers of Reddit, how do you keep pace with the technical developments in the field without actually writing and submitting code?\n \n  \nThanks to everyone in advance!\n    submitted by    /u/frank_sobotka_  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s32lin/d_what_has_been_your_experience_as_a_machine/",
          "publishedOn": "2022-01-13T15:56:23.000Z",
          "wordCount": 883,
          "title": "[D] What has been your experience as a machine learning manager?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s2yvyk/n_easysynth_unreal_engine_plugin_for_easy/",
          "author": null,
          "description": "Hello Community!\n We needed a user-friendly image dataset creation tool for Machine Learning and Computer Vision purposes, but since all we could find were advanced simulators, we decided to create an open source one in case anybody else found it useful.\n EasySynth is an easy-to-use UnrealEngine plugin which enables simple generation of ground truth depth images, normal images, optical flow images and semantic images.\n EasySynth does not require knowledge of either C++ or Blueprints. It utilizes a LevelSequence (checkout the video using the link below) to define the movement of the camera and provides a simple interface for semantic labeling of actors present in the scene. It supports exporting camera positions and rotations at each frame, as well as the following output formats:\n  \nColor images rendered by default\n Depth grayscale images\n Pixel normal images\n Optical flow between frames\n Images with actor semantic labels\n  \nAs an example, the following output can be created in 20 minutes, from the project setup to the output rendering (using a third-party level). Check out the workflow timelapse.\n Provides GT data for training depth estimation, normal estimation, semantic or optical flow models\n For more details check out the GitHub repo. We are working on making this plugin available for free in the Unreal Engine plugin marketplace. The current version works with UE 4.27.\n We hope somebody will find it useful!\n    submitted by    /u/Ok_Compote_3050  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s2yvyk/n_easysynth_unreal_engine_plugin_for_easy/",
          "publishedOn": "2022-01-13T12:59:20.000Z",
          "wordCount": 747,
          "title": "[N] EasySynth - Unreal Engine plugin for easy creation of synthetic images (depth maps, optical flow, semantics, ...)"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s2yre7/d_is_there_value_in_adding_comments_to_large_ml/",
          "author": null,
          "description": "So I've started working with yolov5 which has literally 0 comments. If I went and added comments to the code, would it\n  \nBe appreciated\n \nActually be accepted into the main branch\n \n If it would never be accepted I don't really want to spend the time tbh.\n    submitted by    /u/a_slay_nub  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s2yre7/d_is_there_value_in_adding_comments_to_large_ml/",
          "publishedOn": "2022-01-13T12:52:17.000Z",
          "wordCount": 410,
          "title": "[D] Is there value in adding comments to large ML projects?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s2yl7i/d_how_to_do_selfresearch/",
          "author": null,
          "description": "Hai!\n I want to gain research experience in Deep Learning (preferably NLP). \n How should I go about doing self-research? Working as an RA is one way but most of the professors I've contacted are either not accepting people outside their universities or want to me do full-time which I cannot. \n I want to build my research profile which will be a valuable asset my grad school application also. \n Please help me figuring out ways to do research and build top notch projects.\n    submitted by    /u/ICantStopMe-  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s2yl7i/d_how_to_do_selfresearch/",
          "publishedOn": "2022-01-13T12:42:29.000Z",
          "wordCount": 971,
          "title": "[D] How to do self-research?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s2ws0b/p_audio_data_augmentation_techniques/",
          "author": null,
          "description": "Audio data augmentation is a great solution to make your audio AI models more robust. \n In my new video in the \"Audio Data Augmentation\" series, you can learn about augmentation techniques both in the raw audio and spectrogram domains.\n Specifically, you’ll learn about:\n 📌 Time shifting\n 📌 Time stretching\n 📌 Pitch scaling\n 📌 Noise addition\n 📌 Impulse response addition\n 📌 Filters\n 📌 Polarity Inversion\n 📌 Random gain\n 📌 Time masking \n 📌 Frequency masking\n Check out the video: https://www.youtube.com/watch?v=bm1cQfb_pLA&list=PL-wATfeyAMNoR4aqS-Fv0GRmS6bx5RtTW&index=3\n    submitted by    /u/diabulusInMusica  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s2ws0b/p_audio_data_augmentation_techniques/",
          "publishedOn": "2022-01-13T10:51:36.000Z",
          "wordCount": 156,
          "title": "[P] Audio data augmentation techniques"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s2vksv/research_talks_about_bayesian_optimization/",
          "author": null,
          "description": "[Research] Given the previous post I had (https://www.reddit.com/r/MachineLearning/comments/rhppgq/research_new_library_for_bayesian_optimisation/hot7x74/?context=3), I realised a lot are not sure what BO is. I put out a new playlist with two (short) videos for now about BO and its importance as promised: \n Video 1: https://www.youtube.com/watch?v=YwFiB7vOQD8&t=4s \n Video 2: https://www.youtube.com/watch?v=85dslFi7IB8\n ​\n I will increase the videos over time and detail how BO actually works. The first two-three videos are there to serve as examples conveying the class of problems this field covers. \n ​\n If you are interested, feel free to check them out!\n View Poll\n    submitted by    /u/Ok_Can2425  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s2vksv/research_talks_about_bayesian_optimization/",
          "publishedOn": "2022-01-13T09:32:31.000Z",
          "wordCount": 192,
          "title": "[Research] Talks about Bayesian Optimization"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s2vjyt/d_thoughts_on_ieee_access_as_a_journal/",
          "author": null,
          "description": "I’m doing my PhD and tempted to publish my latest work there due to the faster review process when compared to other journals. My supervisor is encouraging me to publish there and get started on the next piece of work as soon as possible. He likes to get his PhD students finished quickly via article based phds and says that Access counts as much as any of the other mainstream journals (towards achieving the goal of article based phd). \n I want to get involved in industry research as soon as possible once graduating and am wondering if publishing in access will diminish my prospects vs publishing in the likes of NN or something like that.\n    submitted by    /u/DaBeastGeek  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s2vjyt/d_thoughts_on_ieee_access_as_a_journal/",
          "publishedOn": "2022-01-13T09:31:01.000Z",
          "wordCount": 905,
          "title": "[D] Thoughts on IEEE Access as a journal?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s2veux/d_does_pytorch_credit_contributors/",
          "author": null,
          "description": "Asked them on two occasions, no response.\n My PRs were closed and \"landed\" by a dev, so no standard Github contributor credit. If I were to contribute an optimizer, they'd merge it without attribution?\n    submitted by    /u/OverLordGoldDragon  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s2veux/d_does_pytorch_credit_contributors/",
          "publishedOn": "2022-01-13T09:21:34.000Z",
          "wordCount": 152,
          "title": "[D] Does PyTorch credit contributors?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s2rtw5/d_what_is_state_of_the_art_for_audio_generation/",
          "author": null,
          "description": "I’m looking for suggestions for short sound effects, music, and / or speech but primarily concerned about sound effects. Is WaveGAN still dominant?\n    submitted by    /u/gameml  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s2rtw5/d_what_is_state_of_the_art_for_audio_generation/",
          "publishedOn": "2022-01-13T05:42:34.000Z",
          "wordCount": 275,
          "title": "[D] What is state of the art for audio generation?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s2g1rb/d_is_ai_research_reaching_saturation/",
          "author": null,
          "description": "It feels like every topic is heavily researched. There are 100 new papers everyday mentioning small improvements to previous architectures/models with some tweaking. Every task has a near 100% accuracy. Is AI research reaching saturation? What do you guys work on, and how competitive is it in your area? How do you keep up with the several papers cpming in everyday?\n    submitted by    /u/Bibbidi_Babbidi_Boo  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s2g1rb/d_is_ai_research_reaching_saturation/",
          "publishedOn": "2022-01-12T20:28:41.000Z",
          "wordCount": 888,
          "title": "[D] Is AI research reaching saturation?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s2bies/d_classical_clustering_benchmarks/",
          "author": null,
          "description": "Besides for MNIST, what datasets are generally standard to test a method out on? I am looking to test out a non-deep clustering method on various benchmarks.\n When I look at papers accepted into top ML conferences like NeurIPS, ICML and ICLR, there doesn't appear to be a consistent set of benchmarks. Is there a standard set of clustering benchmarks one is supposed to deploy new methods on?\n ( I would not expect it this method I'm testing to perform well on e.g. CIFAR-10. My impression is that almost all non-deep methods that aren't custom-tuned to images perform poorly on CIFAR-10)\n    submitted by    /u/Grand_Distribution83  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s2bies/d_classical_clustering_benchmarks/",
          "publishedOn": "2022-01-12T17:17:56.000Z",
          "wordCount": 431,
          "title": "[D] Classical Clustering Benchmarks"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s29h96/d_r_large_query_set_few_shot_learning/",
          "author": null,
          "description": "Is there a research field specific to image classification/ embedding learning where we have a large number of classes +2k and only few samples per class ~1-5 ? What i noticed in few shot learning papers is the query set is always very small 20-way 1-shot or something similar. Is there any paper you have seen that i can check ?\n Thank you!\n    submitted by    /u/flow_smith  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s29h96/d_r_large_query_set_few_shot_learning/",
          "publishedOn": "2022-01-12T15:52:30.000Z",
          "wordCount": 303,
          "title": "[D] [R] large query set few shot learning"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s252wb/r_convnets_vs_transformers/",
          "author": null,
          "description": "A ConvNet for the 2020s - nice read to start 2022. The authors explore modernizations of Resnets and adopt some tricks from transformers training design to make ConvNets great again.\n There is a lot to reflect and thing about.\n Code is here. \n https://preview.redd.it/kqnqe86729b81.png?width=2696&format=png&auto=webp&s=ac0a4f045c61c34756cfcce3073792ace8f64301\n    submitted by    /u/AdelSexy  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s252wb/r_convnets_vs_transformers/",
          "publishedOn": "2022-01-12T12:24:53.000Z",
          "wordCount": 1332,
          "title": "[R] ConvNets vs Transformers"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s24vj1/d_how_good_are_australian_universities_for_deep/",
          "author": null,
          "description": "Hey. I was thinking of pursuing my Masters from Australia but am still confused about upto what extent they pursue deep learning. Like I was going through University of Adelaide and found they have their dedicated Computer Vision lab among others which has also produced some good SOTA results in the past. But apart from that, what other universities would the community recommend? Or are there any certain professors which could be of interest. Any suggestions are welcome :)\n    submitted by    /u/Unitrix247  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s24vj1/d_how_good_are_australian_universities_for_deep/",
          "publishedOn": "2022-01-12T12:12:43.000Z",
          "wordCount": 238,
          "title": "[D] How good are Australian Universities for Deep Learning/AI?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s23mpx/d_gan_training_initially_degrades_results_of/",
          "author": null,
          "description": "I have an issue with the training of a GAN, which consists of a generator and two discriminators. The generator is used to generate waveforms.\n 1-The generator is independently pre-trained by regression, up to 400k steps.\n 2-The two randomly initialized discriminators are then activated, and GANs training takes place.\n The orange curve, represents the l1-norm between input and output: the l1-norm steeply rises for the first few validation steps. In the blue curve, I tried freezing the generator for 80k steps, to allow pre-training of the discriminators, but the problems persists.\n However, step 2 inititially degrades the output of the generator, by removing too much information. This is also reflected in all the validation losses (see image above), whose values get worse.\n The GAN is based or a variation of the code below:\n https://github.com/rishikksh20/hifigan-denoiser/blob/master/train.py\n Any suggestions?\n    submitted by    /u/alf_Lafleur  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s23mpx/d_gan_training_initially_degrades_results_of/",
          "publishedOn": "2022-01-12T10:57:53.000Z",
          "wordCount": 396,
          "title": "[D] GAN training initially degrades results of pre-trained generator"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s21vuf/p_tutorial_time_series_crossvalidation/",
          "author": null,
          "description": "[Tutorial] - Time Series Cross Validation\n  \nTL;DR: The code is here.\n  \nTime Series Forecasting can be overwhelming. Especially if you are just getting started. There are many different types of Time Series tasks each differs by the number of input or output sequences, the number of steps to predict, whether the input and/or the output sequence length is static or changing, and so on. In this notebook, we will experiment with different types of Time Series Cross-Validation Strategies in order to become familiar with them and understand which works best for what case. \n As written before, Time Series problems can be of different variations, so in order to get a deeper understanding, we should explore each. \n  \nVariation I: Number of Input / Output sequences\n  \n(1.0) Single input and single…",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s21vuf/p_tutorial_time_series_crossvalidation/",
          "publishedOn": "2022-01-12T09:04:22.000Z",
          "wordCount": 467,
          "title": "[P] Tutorial: Time Series Cross-Validation"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s21pqf/d_faster_optimization_using_gpu_multiprocessing/",
          "author": null,
          "description": "GPU Multiprocessing for Parameter Optimization\n  \nTL;DR: The code is here\n  \nHi Guys, \n I published a notebook using a trick for running multiprocessing on GPU devices. Might be interesting for some here, so have fun:\n As we know, GPU makes everything faster. Moreover, oftentimes the GPU device we use is so powerful that our code doesn't utilize the GPU code to 100% potential. In such cases, running multiple GPU instances in parallel can come in handy and save us some extra time.\n I've made a notebook that introduces a method for running parallel jobs on GPU devices. It is a bit tricky since most packages that support GPU usage aren't built from the ground up for such use cases.\n The \"trick\" is simple: We simply do not call the GPU in the main process, the first time we call for any GPU utilization should be from the child processes. \n ![](https://i.ibb.co/VDpCKT4/gpu-multiprocessing.png)\n To demonstrate the power of this concept, I made a notebook that runs Parallel Hyperparameters Optimization on the GPU for LightGBM. As it is assumed, the GPU enables us to train our models faster (much faster) and by leveraging that in combination with the parallel execution we get to search a large space of parameter configurations for fully maximizing our model's performance (Since we can now try many combinations). The Optimization framework is optuna, the popular framework for optimizing the hyperparameters. Again, this is just an example of the approach. You can use the same approach for many other GPU-enabled use cases. \n Personally, This concept helped me over and over as it is easily transferable to many different GPU-based models including Neural Networks (Tensorflow, Pytorch..), Other GBM models (Catboost, XGBoost..), Feature Engineering (Pandas, CuPy), and more.\n    submitted by    /u/yamqwe  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s21pqf/d_faster_optimization_using_gpu_multiprocessing/",
          "publishedOn": "2022-01-12T08:53:59.000Z",
          "wordCount": 369,
          "title": "[D] Faster Optimization using GPU Multiprocessing"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s1zj44/r_julia_developers_discuss_the_current_state_of/",
          "author": null,
          "description": "https://discourse.julialang.org/t/state-of-machine-learning-in-julia/74385/18\n The developers of some of the largest Julia language packages discuss the current state of ML in Julia, and compare and contrast its status with the Python ML ecosystem.\n    submitted by    /u/kdfn  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s1zj44/r_julia_developers_discuss_the_current_state_of/",
          "publishedOn": "2022-01-12T06:34:26.000Z",
          "wordCount": 443,
          "title": "[R] Julia developers discuss the current state of ML tools in Julia compared to Python"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s1z04l/d_question_to_ml_developers_do_you_split_your/",
          "author": null,
          "description": "Like you do the theory (maths and…) and give it to a programmer to deploy your thoughts.\n Is it worth your time?\n    submitted by    /u/Secure_Pomegranate10  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s1z04l/d_question_to_ml_developers_do_you_split_your/",
          "publishedOn": "2022-01-12T06:03:18.000Z",
          "wordCount": 426,
          "title": "[D] Question to ML developers: Do you split your work with a programmer?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s1x8w4/r_automated_reinforcement_learning_autorl_a/",
          "author": null,
          "description": "Paper: https://arxiv.org/abs/2201.03916. First survey on the recently developing field of AutoRL.\n Abstract: The combination of Reinforcement Learning (RL) with deep learning has led to a series of impressive feats, with many believing (deep) RL provides a path towards generally capable agents. However, the success of RL agents is often highly sensitive to design choices in the training process, which may require tedious and error-prone manual tuning. This makes it challenging to use RL for new problems, while also limits its full potential. In many other areas of machine learning, AutoML has shown it is possible to automate such design choices and has also yielded promising initial results when applied to RL. However, Automated Reinforcement Learning (AutoRL) involves not only standard applications of AutoML but also includes additional challenges unique to RL, that naturally produce a different set of methods. As such, AutoRL has been emerging as an important area of research in RL, providing promise in a variety of applications from RNA design to playing games such as Go. Given the diversity of methods and environments considered in RL, much of the research has been conducted in distinct subfields, ranging from meta-learning to evolution. In this survey we seek to unify the field of AutoRL, we provide a common taxonomy, discuss each area in detail and pose open problems which would be of interest to researchers going forward.\n    submitted by    /u/machinelearner5000  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s1x8w4/r_automated_reinforcement_learning_autorl_a/",
          "publishedOn": "2022-01-12T04:28:06.000Z",
          "wordCount": 332,
          "title": "[R] Automated Reinforcement Learning (AutoRL): A Survey and Open Problems"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s1v92d/d_edit_videos_with_clip_styleganv_a_continuous/",
          "author": null,
          "description": "While we have seen several new SOTA image generation models pop up over the last year, video generation still remains lackluster, to say the least. But does it have to be? The authors of StyleGAN-V certainly don’t think so! By adapting the generator from StyleGAN2 to work with motion conditions, developing a hypernetwork-based discriminator, and designing a clever acyclic positional encoding, Ivan Skorohodov and the team at KAUST and Snap Inc. deliver a model that generates videos of arbitrary length with arbitrary framerate, is just 5% more expensive to train than a vanilla StyleGAN2, and beats multiple baseline models on 256 and 1024 resolution. Oh, and it only needs to see about 2 frames from a video during training to do so!\n And if that wasn’t impressive enough, StyleGAN-V is CLIP-compatible for first-ever text-based consistent video editing\n Full summary: https://t.me/casual_gan/238\n Blog post: https://www.casualganpapers.com/text_guided_video_editing_hd_video_generation/StyleGAN-V-explained.html\n StyleGAN-V: generate hd videos and edit them with CLIP\n arxiv / code (coming soon)\n Subscribe to Casual GAN Papers and follow me on Twitter for weekly AI paper summaries!\n    submitted by    /u/KirillTheMunchKing  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s1v92d/d_edit_videos_with_clip_styleganv_a_continuous/",
          "publishedOn": "2022-01-12T02:49:10.000Z",
          "wordCount": 356,
          "title": "[D] Edit Videos With CLIP - StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2 by Ivan Skorokhodov et al. explained in 5 minutes (by Casual GAN Papers)"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s1rtox/d_realtime_public_data_api/",
          "author": null,
          "description": "I am testing a featured related with data pipeline. To produce a more realistic demo I started searching for a dynamic/real-time public data that I can build a end-to-end project.\n Best if it is a finance related data. Stock limited order book etc\n    submitted by    /u/Late-trip-to-cabo  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s1rtox/d_realtime_public_data_api/",
          "publishedOn": "2022-01-12T00:09:40.000Z",
          "wordCount": 132,
          "title": "[D] Real-time Public data API"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s1r3yu/time_series_with_high_seasonal_period_discussion/",
          "author": null,
          "description": "What might be the best model(s) for daily time series with annual seasonality (period = 365) ? I used auto_arima and it doesn’t support high seasonal periods. It takes forever to fit and then throws a memory error. I read a little about Fourier but not sure how to use that in Python.\n    submitted by    /u/CheeseBurgersx  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s1r3yu/time_series_with_high_seasonal_period_discussion/",
          "publishedOn": "2022-01-11T23:36:22.000Z",
          "wordCount": 149,
          "title": "Time Series with high seasonal period [Discussion]"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s1oroa/d_interview_this_team_won_the_minecraft_rl_basalt/",
          "author": null,
          "description": "https://youtu.be/a4P8v8lGFPw\n The MineRL BASALT challenge has no reward functions or technical descriptions of what's to be achieved. Instead, the goal of each task is given as a short natural language string, and the agent is evaluated by a team of human judges who rate both how well the goal has been fulfilled, as well as how human-like the agent behaved. In this video, I interview KAIROS, the winning team of the 2021 challenge, and discuss how they used a combination of machine learning, efficient data collection, hand engineering, and a bit of knowledge about Minecraft to beat all other teams.\n ​\n OUTLINE:\n 0:00 - Introduction\n 4:10 - Paper Overview\n 11:15 - Start of Interview\n 17:05 - First Approach\n 20:30 - State Machine\n 26:45 - Efficient Label Collection\n 30:00 - Navigation Policy\n 38:15 - Odometry Estimation\n 46:00 - Pain Points & Learnings\n 50:40 - Live Run Commentary\n 58:50 - What other tasks can be solved?\n 1:01:55 - What made the difference?\n 1:07:30 - Recommendations & Conclusion\n 1:11:10 - Full Runs: Waterfall\n 1:12:40 - Full Runs: Build House\n 1:17:45 - Full Runs: Animal Pen\n 1:20:50 - Full Runs: Find Cave\n ​\n Paper: https://arxiv.org/abs/2112.03482\n Code: https://github.com/viniciusguigo/kairos_minerl_basalt\n Challenge Website: https://minerl.io/basalt/\n ​\n Paper Title: Combining Learning from Human Feedback and Knowledge Engineering to Solve Hierarchical Tasks in Minecraft\n    submitted by    /u/ykilcher  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s1oroa/d_interview_this_team_won_the_minecraft_rl_basalt/",
          "publishedOn": "2022-01-11T21:55:34.000Z",
          "wordCount": 348,
          "title": "[D] Interview - This Team won the Minecraft RL BASALT Challenge! (Paper Explanation & Interview with the authors)"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s1nyf1/d_build_system_for_machine_learning/",
          "author": null,
          "description": "Hey all,\n I'm setting up a new machine learning project and I know I'm going to be collecting data over time (and making model improvements). Is there a best way to set up a build system to generate models using my training script, eval on test data, all relatively automatically?\n Curious what other people have tried and liked (or disliked). Thanks!\n    submitted by    /u/smp2005throwaway  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s1nyf1/d_build_system_for_machine_learning/",
          "publishedOn": "2022-01-11T21:21:02.000Z",
          "wordCount": 152,
          "title": "[D] Build system for machine learning?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s1fc07/d_significance_of_mlm_loss_when_pretraining/",
          "author": null,
          "description": "What significance does the MLM loss have when I'm pre-training Transformers for language modeling from scratch or continue the training of a pre-trained model on a different dataset?\n Apart from initial spikes, I don't really see any significant movement in the loss curves. Most papers just evaluate on downstream tasks such as NER or NLI. Is the MLM loss really not that well interpretable?\n    submitted by    /u/optimized-adam  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s1fc07/d_significance_of_mlm_loss_when_pretraining/",
          "publishedOn": "2022-01-11T15:20:47.000Z",
          "wordCount": 226,
          "title": "[D] Significance of MLM loss when pre-training Transformers for language modeling"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s1erkb/d_how_is_confidence_bound_derived_for_linucb/",
          "author": null,
          "description": "Hi! I am reading the paper A Contextual-Bandit Approach to Personalized News Article Recommendation. I struggle to understand how the confidence bound is derived: \n In particular, I do not understand the term under the square root. Why is this a standard deviation? Also, what values does alpha take or correspond to. Would appreciate any hints/advice. I did read the paper, but could not get the interpretation for the derivation of the upper confidence bound. Thanks\n https://i.redd.it/2bkd1rkro2b81.gif\n \n Latex code for the picture \n |x_{t,a}^T\\hat{\\theta_a} - E[r_{t,a}|x_{t,a}]| \\leq \\alpha \\sqrt{x_{t,a}^T(D_a^TD_a + I_d)^{-1}x_{t,a}} \\\\~\\\\ \\text{For any }\\delta > 0 \\text{ and } x_{t,a} \\in R^d \\text{, where } \\alpha = 1 + \\sqrt{ln(2/\\delta)/2} \\\\~\\\\ a_t = \\arg \\max_{a \\ in A_t}(x_{t,a}^T\\hat{\\theta_a} + \\alpha \\sqrt{x_{t,a}^T(D_a^TD_a + I_d)^{-1}x_{t,a}}) \n    submitted by    /u/denis56  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s1erkb/d_how_is_confidence_bound_derived_for_linucb/",
          "publishedOn": "2022-01-11T14:56:32.000Z",
          "wordCount": 522,
          "title": "[D] How is confidence bound derived for LinUCB"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s1ejwz/p_txtai_40_released_semantic_search_with_sql/",
          "author": null,
          "description": "txtai 4.0 has been released with a number of new features.\n https://preview.redd.it/qm9tgjkum2b81.png?width=1061&format=png&auto=webp&s=22c90382d1dee780530e69635242078a26b8305c\n  \nContent Storage - Content can now be stored alongside embeddings vectors. No longer required to have external data storage.\n Query with SQL - txtai supports both natural language queries and SQL queries\n embeddings.search(\"feel good story\")\n SELECT id, text, score FROM txtai WHERE similar('feel good story') AND score >= 0.15\n Object Storage - Store binary objects alongside embeddings vectors\n Reindex - Indexes can be rebuilt using stored content, no need to resend data to txtai\n Index Compression - Indexes can be compressed using GZ/XZ/BZ2/ZIP\n External Vectors - Use external vector models from an API or an external library. Centralize building vectors on GPU servers leaving index servers to be powered by more modest hardware.\n  \nMore information can be found in following links.\n  \nGitHub Project\n 4.0 Release Notes\n What's new in txtai 4.0\n Documentation\n Examples\n  \n   submitted by    /u/davidmezzetti  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s1ejwz/p_txtai_40_released_semantic_search_with_sql/",
          "publishedOn": "2022-01-11T14:47:00.000Z",
          "wordCount": 253,
          "title": "[P] txtai 4.0 released - semantic search with SQL, content storage, object storage, reindexing and more"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s1e35a/project_a_dataset_of_parse_trees_generated_from/",
          "author": null,
          "description": "(Admittedly cross-posting from r/LanguageTechnology)\n https://github.com/l74d/scholarly-trees\n I have put up some (not so few) parse trees online as a dataset. Not something as substantial as Penn Treebank, since the trees have not been human-edited. But it is still a lot more parse trees, than those from Penn, to feed into your later-stage NLP algorithms, free of charge or hassle.\n The current format is straight from where they were generated. Suggestions of alternative formats based on ease of use would be heavily appreciated!\n    submitted by    /u/l74d  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s1e35a/project_a_dataset_of_parse_trees_generated_from/",
          "publishedOn": "2022-01-11T14:26:01.000Z",
          "wordCount": 189,
          "title": "[Project] A dataset of parse trees generated from abstracts of arXiv articles"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s1cxel/r_hot_under_the_collar_a_latent_measure_of/",
          "author": null,
          "description": "Tl; dr: A Bayesian ideal-point model for modeling crises in international relations.\n Abstract: \"The majority of studies on international conflict escalation use a variety of measures of hostility including the use of force, reciprocity, and the number of fatalities. The use of different measures, however, leads to different empirical results and creates difficulties when testing existing theories of interstate conflict. Furthermore, hostility measures currently used in the conflict literature are ill suited to the task of identifying consistent predictors of international conflict escalation. This article presents a new dyadic latent measure of interstate hostility, created using a Bayesian item-response theory model and conflict data from the Militarized Interstate Dispute (MID) and Phoenix political event datasets. This model (1) provides a more granular, conceptually precise, and validated measure of hostility, which incorporates the uncertainty inherent in the latent variable; and (2) solves the problem of temporal variation in event data using a varying-intercept structure and human-coded data as a benchmark against which biases in machine-coded data are corrected. In addition, this measurement model allows for the systematic evaluation of how existing measures relate to the construct of hostility. The presented model will therefore enhance the ability of researchers to understand factors affecting conflict dynamics, including escalation and de-escalation processes.\"\n Paper: https://journals.sagepub.com/doi/pdf/10.1177/0022343320962546\n Non-paywalled: http://zterechshenko.com/assets/ZTerechshenko_MA.pdf\n    submitted by    /u/bikeskata  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s1cxel/r_hot_under_the_collar_a_latent_measure_of/",
          "publishedOn": "2022-01-11T13:29:51.000Z",
          "wordCount": 412,
          "title": "[R] \"Hot under the collar: A latent measure of interstate hostility\""
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s1bxxx/d_large_query_set_few_shot_learning/",
          "author": null,
          "description": "Is there a research field specific to image classification/ embedding learning where we have a large number of classes +2k and only few samples per class ~1-5 ? What i noticed in few shot learning papers is the query set is always very small 20-way 1-shot or something similar. Is there any paper you have seen that i can check ?\n Thank you!\n    submitted by    /u/flow_smith  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s1bxxx/d_large_query_set_few_shot_learning/",
          "publishedOn": "2022-01-11T12:36:57.000Z",
          "wordCount": 158,
          "title": "[D] large query set few shot learning"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s1bwo3/p_tiny_video_search_engine_using_openais_clip/",
          "author": null,
          "description": "Tiny Video Search Engine Using OpenAI's CLIP\n A fun project that I did to try out OpenAI's CLIP model. In this article, I describe a tiny video search engine and indexer that will let you search through a video with descriptive \"natural language\" queries and find matching frames of video. All the code is included in a Google Colab Notebook. So even if you don't have your own cuda-capable GPU, you can easily run the code yourself without setting up anything on your own computer.\n    submitted by    /u/CakeStandard3577  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s1bwo3/p_tiny_video_search_engine_using_openais_clip/",
          "publishedOn": "2022-01-11T12:34:40.000Z",
          "wordCount": 183,
          "title": "[P] Tiny Video Search Engine Using OpenAI's CLIP"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s127z6/p_tensorflow_similarity_now_selfsupervised/",
          "author": null,
          "description": "Happy new year :)\n Very happy to announce that as part of the 0.15 release, TensorFlow Similarity now support self-supervised learning using STOA algorithms. To help you get started we included in the release a detailed getting started notebook that you can run in Colab. This notebook shows you how to use SimSiam self-supervised pre-training to almost double the accuracy compared to a model trained from scratch on CIFAR 10.\n Hope you will find this release useful to your research and experimentations :)\n ​\n https://preview.redd.it/ulodxbq43za81.jpg?width=1600&format=pjpg&auto=webp&s=188a51095ea122573458856db9a524491743a587\n    submitted by    /u/ebursztein  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s127z6/p_tensorflow_similarity_now_selfsupervised/",
          "publishedOn": "2022-01-11T02:50:31.000Z",
          "wordCount": 356,
          "title": "[P] TensorFlow Similarity now self-supervised training"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s0ztwr/rtwitter_crawl_data_for_research/",
          "author": null,
          "description": "Hello All I am looking to crawl data for academic research (most likely need to release/open-source the dataset). conclude you guys know the license? (I have already read their webpage, terms and condition), however, I don't find too many open source twitter data set, wondering if there ‏‏‎ is any hidden terms that I am not awared off?\n    submitted by    /u/hushedBobolink7  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s0ztwr/rtwitter_crawl_data_for_research/",
          "publishedOn": "2022-01-11T00:54:13.000Z",
          "wordCount": 213,
          "title": "[R]:Twitter Crawl Data for Research."
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s0y5og/dynamic_time_series_chunk_regression_problem_d/",
          "author": null,
          "description": "Hello all, I currently have a issue and I'd like to hear some advice on how to tackle the given problem. I'm running independent trials, where for each trial a series of variables are recorded at each timestep, where each trial lasts for different periods of time. The outcome of each trial is a single value. I'd like to predict this single output value given the dynamic timesteps for time series variables. To give a visual example, here is what some data would look like:\n ​\n https://preview.redd.it/ssz9dlb74ya81.png?width=704&format=png&auto=webp&s=da8cd49520e5570e45bdc85ff5e7cf9f7ff75ef6\n Each of the first 8 columns represents a variable, where each row represents a different timestep, and each blank row separates the timesteps into chunks, where each chunk is a trial; and the last column is the single value resulting from each trial that I'd like to predict. Any advice on how to tackle this problem of predicting the last column given a dynamic amount of timesteps for a set of timeseries variables???\n    submitted by    /u/MyActualUserName99  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s0y5og/dynamic_time_series_chunk_regression_problem_d/",
          "publishedOn": "2022-01-10T23:37:45.000Z",
          "wordCount": 383,
          "title": "Dynamic Time Series Chunk Regression Problem [D]"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s0uz2u/r_model_selection_in_batch_policy_optimization/",
          "author": null,
          "description": "submitted by    /u/hardmaru  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s0uz2u/r_model_selection_in_batch_policy_optimization/",
          "publishedOn": "2022-01-10T21:23:51.000Z",
          "wordCount": 97,
          "title": "[R] Model Selection in Batch Policy Optimization"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s0sfm9/d_is_there_a_solid_aka_non_euristic_reason_for/",
          "author": null,
          "description": "I mean a mathematical argument. Thanks everyone\n    submitted by    /u/alesaso2000  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s0sfm9/d_is_there_a_solid_aka_non_euristic_reason_for/",
          "publishedOn": "2022-01-10T19:36:27.000Z",
          "wordCount": 532,
          "title": "[D] Is there a solid (aka non euristic) reason for why smaller batch sizes lead to better generalization?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s0rzps/d_why_is_one_loss_positive_and_the_other_loss/",
          "author": null,
          "description": "I was reading section 3.1.D of this paper on re-id and I had a question about this part:\n image of relevant part\n Specifically, both parts are minibatch losses. So why is the 1/Nb loss negative, and why would the 1/Ns loss be positive. Intuitively, the losses are summed, so it should be (-1/Nb)+(-1/Ns) = -1/Nb - 1/Ns, right?\n    submitted by    /u/asuprem  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s0rzps/d_why_is_one_loss_positive_and_the_other_loss/",
          "publishedOn": "2022-01-10T19:17:46.000Z",
          "wordCount": 194,
          "title": "[D] Why is one loss positive and the other loss negative in a multi-output branch?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s0rpl0/p_a_library_for_visualizing_cnn_architectures_and/",
          "author": null,
          "description": "Hi!\n In my PhD, I studied the design of neural architectures and how to design them without tremendous amounts of trial and error.This lib is my most significant insight compiled into a lightweight package.Simply speaking, I figured out how to predict useless / unproductive layers in CNN architectures.\n This library allows you to easily visualize neural architectures from PyTorch, with unproductive layers highlighted within in the topology. This makes it possible for you to spot inefficiencies within your CNN architecture reliably, without the need for a single training step!\n GitHub\n PyPi\n Doc\n    submitted by    /u/KrakenInAJar  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s0rpl0/p_a_library_for_visualizing_cnn_architectures_and/",
          "publishedOn": "2022-01-10T19:05:49.000Z",
          "wordCount": 384,
          "title": "[P] A library for visualizing (CNN) architectures and receptive field analysis"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s0p8rc/d_lessons_from_the_field_in_building_your_mlops/",
          "author": null,
          "description": "Hi r/MachineLearning!\n I wanted to share a webinar coming up in January 2022 at Enterprise Data & AI. I put the info from the website below along with the link if you're interested. I'm really interested in the real life case studies they mention with Uber.\n ---------\n Featured Speaker: Harpreet Sahota, Data Scientist at Comet and host of \"The Artists of Data Science\" Podcast\n As machine learning expands and larger organizations begin deploying across bigger teams, the need to efficiently operationalize becomes critical for enterprises. In our discussions with leading organizations utilizing ML like The RealReal and Uber, we have compiled real-world case studies and organizational best practices for MLOps in the enterprise.\n Join us for a discussion where we'll explore the benefits of MLOps and discuss when and how to deploy MLOps in your ML. We'll review three real world case studies that will answer key questions:\n  \n When to start implementing in MLOps?\n  How to start implementing in MLOps?\n How to measure the value of your MLOps strategy?\n  \nAgenda:\n  \n12:00pm-12:30: Featured Presentation\n 12:30-13:00pm: Your Q&A and interaction\n  \nLink to the website: https://events.cognilytica.com/CLNjE3MHwyNA\n    submitted by    /u/DataGeek0  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s0p8rc/d_lessons_from_the_field_in_building_your_mlops/",
          "publishedOn": "2022-01-10T17:22:06.000Z",
          "wordCount": 337,
          "title": "[D] Lessons From the Field in Building Your MLOps Strategy with Harpreet Sahota, Data Scientist at Comet at Enterprise Data & AI - Jan 27 @ 12 PM ET"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s0ozzc/p_dissecting_and_implementing_research_papers_in/",
          "author": null,
          "description": "Hi, I wrote a 2 part article on creating interaction networks between characters in novels and other bodies of text. The first part is a detailed literature review outlining and dissecting research papers relevant to the topic and the second part is the implementation of the thoughts and ideas presented in the first part (in python). Check it out if you're interested. \n Part 1 : https://towardsdatascience.com/mining-modelling-character-networks-part-i-e37e4878c467\n Part 2 : https://towardsdatascience.com/mining-modelling-character-networks-part-ii-a3d77de89638\n    submitted by    /u/spidermon97  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s0ozzc/p_dissecting_and_implementing_research_papers_in/",
          "publishedOn": "2022-01-10T17:12:10.000Z",
          "wordCount": 167,
          "title": "[P] Dissecting and implementing research papers in python"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s0lgqb/p_library_for_endtoend_neural_search_pipelines/",
          "author": null,
          "description": "Hello everyone ! \n While working on my PhD, I developed a Python package that I am pretty proud of. \n It allows you to easily create diverse neural search pipelines with retrievers and pre-trained language models as rankers, and it works flawlessly with middle-sized corpus. \n It is also currently trending #15 on HackerNews ! Check-it out ! \n Github link\n Documentation\n Hackernews link\n    submitted by    /u/RaphaelYt  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s0lgqb/p_library_for_endtoend_neural_search_pipelines/",
          "publishedOn": "2022-01-10T14:37:07.000Z",
          "wordCount": 585,
          "title": "[P] Library for end-to-end neural search pipelines"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s0huxv/d_what_are_your_favorite_tools_to/",
          "author": null,
          "description": "Oftentimes I encounter operations that I do not understand as a whole - which can be fixed by simply using dummy arrays and printing them out.\n However, that quickly becomes cumbersome. What do y'all use to visualize such complex tensor operations quickly and accurately to understand what exactly it accomplishes?\n    submitted by    /u/Competitive-Rub-1958  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s0huxv/d_what_are_your_favorite_tools_to/",
          "publishedOn": "2022-01-10T11:23:25.000Z",
          "wordCount": 165,
          "title": "[D] What are your favorite tools to visualize/explain tensor operations?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s0h4gu/d_how_to_speed_up_inference_of_your/",
          "author": null,
          "description": "Hello all,\n Many of us are having a hard time speeding up our Transformer based NLP models for inference in production.\n So I thought it would be worth writing an article that summarizes the options one should consider (GPU, batch inference, export to ONNX or Torchscript, using TensorRT, Deepspeed, Triton Inference Server... etc.):\n https://nlpcloud.io/how-to-speed-up-deep-learning-nlp-transformers-inference.html\n I hope you'll find it useful. If you can think of additional options, please let me know and I'll add them to the article!\n Julien\n    submitted by    /u/juliensalinas  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s0h4gu/d_how_to_speed_up_inference_of_your/",
          "publishedOn": "2022-01-10T10:37:26.000Z",
          "wordCount": 639,
          "title": "[D] How to speed up inference of your Transformer-based NLP models?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s0gzaa/dyoure_working_against_a_tight_deadline_and_need/",
          "author": null,
          "description": "View Poll\n    submitted by    /u/rrpelgrim  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s0gzaa/dyoure_working_against_a_tight_deadline_and_need/",
          "publishedOn": "2022-01-10T10:28:04.000Z",
          "wordCount": 1170,
          "title": "[D]You're working against a tight deadline and need to deliver a project quickly. Would you consider speeding up your model training by using only a subset of the available training data?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s0cdrg/dwhat_would_happen_if_i_removed_the_feedforward/",
          "author": null,
          "description": "I think it would still work albeit less efficient?\n    submitted by    /u/Sudden-Lingonberry80  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s0cdrg/dwhat_would_happen_if_i_removed_the_feedforward/",
          "publishedOn": "2022-01-10T05:40:28.000Z",
          "wordCount": 261,
          "title": "[D]What would happen if I removed the feed-forward layer in the transformer architecture?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s03uwv/r_rudolph_one_hypermodal_transformer_can_be/",
          "author": null,
          "description": "submitted by    /u/Illustrious_Row_9971  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s03uwv/r_rudolph_one_hypermodal_transformer_can_be/",
          "publishedOn": "2022-01-09T22:47:58.000Z",
          "wordCount": 116,
          "title": "[R] RuDOLPH: One Hyper-Modal Transformer can be creative as DALL-E and smart as CLIP"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s02l2f/d_are_there_any_ml_groups_in_boston/",
          "author": null,
          "description": "Hi, I am a graduate student and I recently moved to Boston. I was wondering if there are any ML/DL/RL or related groups in Boston that host meetups in and around the city. Thank you. \n ​\n P.S: If you are from Boston and interested in any of these fields, then I would love to connect with you. Please send a DM or text me in reddit chat :)\n    submitted by    /u/frankhart98  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s02l2f/d_are_there_any_ml_groups_in_boston/",
          "publishedOn": "2022-01-09T21:50:32.000Z",
          "wordCount": 294,
          "title": "[D] Are there any ML groups in Boston?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/s01us1/r_sensing_depth_with_3d_computer_vision_link_to_a/",
          "author": null,
          "description": "submitted by    /u/pinter69  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/s01us1/r_sensing_depth_with_3d_computer_vision_link_to_a/",
          "publishedOn": "2022-01-09T21:19:15.000Z",
          "wordCount": 722,
          "title": "[R] Sensing Depth with 3D Computer Vision - Link to a free online lecture by the author in comments"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rzy588/d_does_anyone_else_think_open_source_codeexamples/",
          "author": null,
          "description": "Admittedly, I am not an expert in machine learning or different libraries but the code I see as an example is not really beginner friendly. Even for an expert, I am not sure, they know all libraries and quircks of different datasets.\n Let me elaborate. The main problem I see is the use of magic numbers. For example, in below hypothetical code\n x = dataset[1] \n there is no indication of why 1 is used instead of 0 or what does it mean. May be 0th elemnt contains metadata/some useless data. Or in other cases, some axis is chosen without specifying why that is used and what are other axis to put in context.\n My only suggestion would be to not ever use a magic number unless it is immediately obvious. Can we not use an appropriately named constant in that case?\n MY_DATA_INDEX=1 x = dataset[MY_DATA_INDEX] \n I believe this is a very simple and helpful convention to follow. If such conventions are already there, can someone point me to then? May be people aren't just using them too often.\n    submitted by    /u/junovac  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rzy588/d_does_anyone_else_think_open_source_codeexamples/",
          "publishedOn": "2022-01-09T18:36:22.000Z",
          "wordCount": 953,
          "title": "[D] Does anyone else think open source code/examples in machine learning domain usually are not as readable as they could be? Specifically use of magic numbers."
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rzs255/d_the_future_of_machine_learning_and_why_it_looks/",
          "author": null,
          "description": "I published an article on Towards Data Science a few weeks ago about why Julia is the future of ML: https://towardsdatascience.com/the-future-of-machine-learning-and-why-it-looks-a-lot-like-julia-a0e26b51f6a6\n ​\n Following up on this post, I will be hosting a Twitter Spaces tomorrow (Monday). If you are interested in joining the discussion, you can find out more in this tweet: https://twitter.com/JuliaLanguage/status/1479899084261572609?s=20\n    submitted by    /u/LoganKilpatrick1  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rzs255/d_the_future_of_machine_learning_and_why_it_looks/",
          "publishedOn": "2022-01-09T13:47:24.000Z",
          "wordCount": 1128,
          "title": "[D] The Future of Machine Learning and why it looks a lot like Julia 🤖"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rzpfzr/d_best_tools_for_multigpu_model_training/",
          "author": null,
          "description": "Hi everyone, until recently I only had to work on problems for which a single GPU training setup would suffice. But i am working on a problem with a large dataset and I have access to multiple GPUs so I was wondering what's the best way to set this up. Going through the PyTorch documentation, they seem to suggest using torch.distributed.run along with DistributedDataParallel for distributed training although I also came across libraries in which the boilerplate stuff is abstracted out such as PyTorch Lightening or DeepSpeed. Since I'm new to this, I just wanted an opinion on the route I should choose for training my Transformer models in a distributed way. Thanks in advance!\n    submitted by    /u/Areyy_Yaar  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rzpfzr/d_best_tools_for_multigpu_model_training/",
          "publishedOn": "2022-01-09T11:10:09.000Z",
          "wordCount": 656,
          "title": "[D] Best tools for Multi-GPU model training?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rzmx95/d_looking_for_open_source_projects_to_contribute/",
          "author": null,
          "description": "Hi all,\n For the last 6 months I have immersed in deep learning problem domain, and have been spending a lot of time catching up on the literature, few courses and doing some personal projects as well.\n But now, I'm at the point where I'd like to start contributing in a more meaningful way to the community. Does anyone have idea of good open source projects related to DL (maybe even classic machine learning) that are looking for contributors?\n Thanks for any suggestions!\n    submitted by    /u/wreemde  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rzmx95/d_looking_for_open_source_projects_to_contribute/",
          "publishedOn": "2022-01-09T08:14:26.000Z",
          "wordCount": 976,
          "title": "[D] Looking for open source projects to contribute"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rz6kbq/d_eric_jang_on_robots_learning_at_google_and/",
          "author": null,
          "description": "Hi there, just want to share out latest Gradient interview with Eric Jang, a research scientist on Google AI's Robotics team. He has worked a good deal with they arm farm, and more recently on imitation learning and reinforcement learning for enabling robots to generalize to many tasks. Here's the link:\n Eric Jang on Robots Learning at Google and Generalization via Language\n As usual, we get pretty technical and most of it going over his research, as well as his recent blog posts.\n Sections:\n (00:00) Intro\n (00:50) Start in AI / Research\n (03:58) Joining Google Robotics\n (10:08) End to End Learning of Semantic Grasping\n (19:11) Off Policy RL for Robotic Grasping\n (29:33) Grasp2Vec\n (40:50) Watch, Try, Learn Meta-Learning from Demonstrations and Rewards\n (50:12) BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning\n (59:41) Just Ask for Generalization\n (01:09:02) Data for Robotics\n (01:22:10) To Understand Language is to Understand Generalization\n (01:32:38) Outro\n Papers discussed:\n  \nGrasp2Vec: Learning Object Representations from Self-Supervised Grasping\n End-to-End Learning of Semantic Grasping\n Deep reinforcement learning for vision-based robotic grasping: A simulated comparative evaluation of off-policy methods\n Watch, Try, Learn Meta-Learning from Demonstrations and Rewards\n BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning\n Just Ask for Generalization\n To Understand Language is to Understand Generalization\n Robots Must Be Ephemeralized\n  \n   submitted by    /u/regalalgorithm  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rz6kbq/d_eric_jang_on_robots_learning_at_google_and/",
          "publishedOn": "2022-01-08T18:36:41.000Z",
          "wordCount": 312,
          "title": "[D] Eric Jang on Robots Learning at Google and Generalization via Language"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rz62v8/hypothetical_testing_a_spiked_neural_network_in/",
          "author": null,
          "description": "I am working on a hard sci fi project set several hundred years in the future. I'd like to show a data scientist running realistic (but interesting) tests on an android to see if it's core SNN is intact.\n Ideally, this would be some kind of behavioral simulation, a series of tests that the android has to \"pass.\"\n Thought I should come to the experts for suggestions!\n    submitted by    /u/Sensitive_Necessary7  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rz62v8/hypothetical_testing_a_spiked_neural_network_in/",
          "publishedOn": "2022-01-08T18:15:07.000Z",
          "wordCount": 209,
          "title": "Hypothetical: testing a spiked neural network in an android [D] [R] [P]"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rz5prc/d_how_does_10shot_work/",
          "author": null,
          "description": "I am trying to get into contrastive learning, and I have stumbled upon benchmarks like this: https://paperswithcode.com/sota/few-shot-image-classification-on-imagenet-10\n What exactly is meant by 10-shot? Is it just training on 10 random examples (selected beforehand for fairness)? How does it work?\n    submitted by    /u/AICoderGamer  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rz5prc/d_how_does_10shot_work/",
          "publishedOn": "2022-01-08T18:00:10.000Z",
          "wordCount": 138,
          "title": "[D] How does 10-shot work?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rz3q9s/p_weekly_updated_open_sourced_model/",
          "author": null,
          "description": "Even though the popularity of JAX is increasing everyday, open sourced code implementations are still hard to find. To counter this, I have created a repository with the goal to achieve something similar to timm for torch (except for the pretrained models because I don't have the funds currently).\n Some features of jax_models: 1. Pip installable 2. Weekly / fortnightly model additions 3. Comprehensive tests 4. Model architectures are cross checked by checking original implementations if available or otherwise miscellaneous aspects of the paper such as model parameters\n The repository is currently in its pre-alpha stage with only around 6 model and layer implementations. If you are interested in this, you can contribute by either asking for specific paper implementations or by helping me train and open source weights of these models.\n If you like my work then please consider starring the repository and giving feedback. Thank you!\n Link: https://github.com/DarshanDeshpande/jax-models\n    submitted by    /u/Megixist  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rz3q9s/p_weekly_updated_open_sourced_model/",
          "publishedOn": "2022-01-08T16:32:15.000Z",
          "wordCount": 240,
          "title": "[P] Weekly updated open sourced model implementations in Flax"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rz3ig9/r_a_5_million_source_code_file_dataset/",
          "author": null,
          "description": "I published a dataset of 5m source code files from 15k open source files. Its long term goal is to enable identifying causality in software engineering. \n Describing paper: End to End Software Engineering Research\n People from ML, NLP, causality, and software engineering, might find it interesting.\n The dataset enables investigating code similarity (and also text similarity in English), program difficulty, defect predictions, etc.\n The code is extracted every two months in order to investigate the difference. By the difference one can investigate if a change in a possible cause (e.g., smell removal) leads to influence (less bugs)and in which context.\n I plan to keep extending the dataset and would like to get feedback on it - ease of use, new use cases, etc.\n If you have related dataset that can be merged with, related data that you would like to obtain or ideas for research directions, please contact me.\n    submitted by    /u/idan_huji  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rz3ig9/r_a_5_million_source_code_file_dataset/",
          "publishedOn": "2022-01-08T16:22:35.000Z",
          "wordCount": 243,
          "title": "[R] A 5 million source code file dataset"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rz1t11/r_the_pymc_devs_have_made_their_books_available/",
          "author": null,
          "description": "Forward: \"Bayesian modeling provides an elegant approach to many data science and decision-making problems. However, it can be hard to make it work well in practice. In particular, although there are many software packages that make it easy to specify complex hierarchical models such as Stan, PyMC3, TensorFlow Probability (TFP), and Pyro, users still need additional tools to diagnose whether the results of their computations are correct or not. They may also need advice on what to do when things do go wrong.\n This book focuses on the ArviZ library, which enables users to perform exploratory analysis of Bayesian models, for example, diagnostics of posterior samples generated by any inference method. This can be used to diagnose a variety of failure modes in Bayesian inference. The book also discusses various modeling strategies (such as centering) that can be employed to eliminate many of the most common problems. Most of the examples in the book use PyMC3, although some also use TFP; a brief comparison of other probabilistic programming languages is also included.\n The authors are all experts in the area of Bayesian software and are major contributors to the PyMC3, ArviZ, and TFP libraries. They also have significant experience applying Bayesian data analysis in practice, and this is reflected in the practical approach adopted in this book. Overall, I think this is a valuable addition to the literature, which should hopefully further the adoption of Bayesian methods.\"\n Link: https://bayesiancomputationbook.com/welcome.html\n    submitted by    /u/bikeskata  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rz1t11/r_the_pymc_devs_have_made_their_books_available/",
          "publishedOn": "2022-01-08T15:04:47.000Z",
          "wordCount": 446,
          "title": "[R] The PyMC devs have made their books available free online!"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rz00k0/stylegan_2_and_psp_running_on_apple_neural_engine/",
          "author": null,
          "description": "submitted by    /u/surelyouarejoking  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rz00k0/stylegan_2_and_psp_running_on_apple_neural_engine/",
          "publishedOn": "2022-01-08T13:36:34.000Z",
          "wordCount": 245,
          "title": "‎StyleGAN 2 and PSP running on Apple Neural Engine [P]"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ryw53x/d_fourier_transform_vs_nns_as_function/",
          "author": null,
          "description": "So this is probably a basic question. If the main premise of neural networks is that they are global function approximators, what advantage do they have against other approximators such Fourier transform, which is also proven to be able to approximate any function. Why does not the whole supervised learning field become one of calculating Fourier coefficients\n    submitted by    /u/Hazalem  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ryw53x/d_fourier_transform_vs_nns_as_function/",
          "publishedOn": "2022-01-08T09:33:30.000Z",
          "wordCount": 1875,
          "title": "[D] Fourier transform vs NNs as function approximators"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ryv05p/d_how_often_is_the_case_that_a_conference/",
          "author": null,
          "description": "My understanding is that if your research paper is denied from a conference, you would take the reviewer's feedback, make some changes, then submit to the next big ML conference. The process repeats until the paper is eventually accepted somewhere. But how often is it the case where the paper isn't accepted anywhere? How many papers are accepted on the first (or second) try? \n I'm an undergrad who's really new with the concept of publishing papers, so I thought it's worth asking around here. :)\n    submitted by    /u/akardashian  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ryv05p/d_how_often_is_the_case_that_a_conference/",
          "publishedOn": "2022-01-08T08:13:45.000Z",
          "wordCount": 777,
          "title": "[D] How often is the case that a conference submission is rejected everywhere?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ryqw1a/d_is_there_a_complete_list_of_modern_different_ai/",
          "author": null,
          "description": "Like DNN for all domains, Reinforcement learning for? Genetic algorithm for ? Symbolic AI for ? ...\n    submitted by    /u/ghosthamlet  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ryqw1a/d_is_there_a_complete_list_of_modern_different_ai/",
          "publishedOn": "2022-01-08T04:04:47.000Z",
          "wordCount": 282,
          "title": "[D] Is there a complete list of modern different AI algorithms and its main application?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ryohri/d_questions_about_paper_pay_attention_to_mlps/",
          "author": null,
          "description": "In Pay Attention to MLPs, \n  \nthe authors said that it is necessary for the layer s(') to contain a contraction operation over the spatial dimension. It's hard to understand why s(') should be a contraction operation. I think s(') should maintain the spatial dimension for proper calculation. Why should s(') be a contraction operation? \n Can I say transformer dynamically parameterize spatial interactions with positional encoder? I am confused whether spatial interaction already contains the meaning of positional encoding. \n  \nWhat do you think? Thank you in advance.\n    submitted by    /u/Spiritual_Fig3632  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ryohri/d_questions_about_paper_pay_attention_to_mlps/",
          "publishedOn": "2022-01-08T02:01:57.000Z",
          "wordCount": 182,
          "title": "[D] Questions about paper \"Pay Attention to MLPs\""
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ryngxc/can_we_use_data_available_for_noncommercial_use/",
          "author": null,
          "description": "I am having a lot of difficulties getting the right data for my task since most of the datasets out there including ImageNet restrict the usage of the data to only educational and non-commercial research. In this case, should I reach out to them to get usage permission? What are the chances that they will allow/respond and would there be a significant cost associated with this? Thanks!\n    submitted by    /u/PinPitiful  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ryngxc/can_we_use_data_available_for_noncommercial_use/",
          "publishedOn": "2022-01-08T01:11:55.000Z",
          "wordCount": 932,
          "title": "Can we use data available for \"non-commercial use\" for training a DL model which will be deployed as a subsystem in a commercial product? [D]"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ryl6pv/neural_architecture_search_nas_d/",
          "author": null,
          "description": "The goal of NAS is to find an optimal architecture of a neural network for a given problem. In my case, I am interested in creating an algorithm for finding the architecture of a Convolutional Neural Network for an Image dataset.\n The problem with any NAS algorithm is that the only true way to know if a created architecture is good is by running the model to convergence, thus increasing computational resources spent.\n There have been many possible solutions to gauge the relative trained accuracy of model. The most popular is to simply reduce the dataset, reuse training weights, and/or train for only a couple of iterations.\n I was wanting to hear some thoughts...\n I can save computational resources by only training each possible model for only 3 iterations and gauge its success using its training accuracy. The problem however with using training accuracy is that it does not gauge how well the model will generalize, only if it is capable of learning anything from the training data, if not overfitting. The problem with using the validation accuracy after 3 iterations is that most models have extremely poor validation accuracy scores early on during training, only catching up around iteration 10, therefore in order to fully gauged the validation accuracy the models would have to be trained until iteration 10, 3x more than using the training accuracy after 3 iterations.\n Would you rather train the models for only 3 iterations, using training accuracy as the metric to compare models, risking the models overfitting when trained until convergence; or, would you rather play the safe game and train models for 10 iterations using validation accuracy as a metric to compare models, but spend 3x the amount of computational resources?\n    submitted by    /u/MyActualUserName99  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ryl6pv/neural_architecture_search_nas_d/",
          "publishedOn": "2022-01-07T23:27:13.000Z",
          "wordCount": 744,
          "title": "Neural Architecture Search (NAS) [D]"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ryj8pw/r_grokking_generalization_beyond_overfitting_on/",
          "author": null,
          "description": "submitted by    /u/hardmaru  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ryj8pw/r_grokking_generalization_beyond_overfitting_on/",
          "publishedOn": "2022-01-07T22:01:44.000Z",
          "wordCount": 103,
          "title": "[R] Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ryh9e1/d_is_model_calibration_important_for_industry/",
          "author": null,
          "description": "There has been a lot of recent work on model calibration — ensuring a match between a model’s confidence and correctness.\n This seems critical in applications where confidence/safety matters (e.g. perception related parts of self-driving), but I’m curious whether model calibration is something important for industry practitioners.\n    submitted by    /u/Zestyclose-Orange468  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ryh9e1/d_is_model_calibration_important_for_industry/",
          "publishedOn": "2022-01-07T20:37:33.000Z",
          "wordCount": 479,
          "title": "[D] Is model calibration important for industry practitioners?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rybj70/p_ml_contests_competitive_ml_community_website/",
          "author": null,
          "description": "I posted here a while ago about a website I created that aggregates competitions across Kaggle and other websites: https://mlcontests.com (open source and community maintained - GitHub link at the bottom of the page)\n https://preview.redd.it/7poem4aqmaa81.png?width=1100&format=png&auto=webp&s=362666ed2bec33d88cdb2c03b84371891e29a928\n Based on some feedback I implemented a few extra features, and today I launched a Discord community to go along with it - this allows people to find teammates to collaborate with on a competition, and to get real-time alerts when new competitions are launched. There are also a few team members from the competition platforms in the Discord already. \n https://preview.redd.it/5a0tzdjomaa81.png?width=673&format=png&auto=webp&s=c78df3326d6a8574eba2e1cd94245d6563f29df3\n Also welcome is discussion about ongoing competitions, and post-mortems/debriefs after competitions end. If this sounds interesting to you, you can join the Discord here: https://discord.com/invite/nM482gc9h8\n Hope to see you there!\n    submitted by    /u/hcarlens  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rybj70/p_ml_contests_competitive_ml_community_website/",
          "publishedOn": "2022-01-07T16:36:51.000Z",
          "wordCount": 223,
          "title": "[P] ML Contests: Competitive ML Community Website + Discord"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ry61q5/d_mc_dropout_is_not_bayesian_so_why_are_so_many/",
          "author": null,
          "description": "In the domain of uncertainty estimation in places like segmentation, MC Dropout is used as an approximation of Bayesian computations. I've seen a thread from a few years ago talking about its issues and I've just read this recent paper called Is MC Dropout Bayesian?, and they conclude it isn't.\n They propose a new method for parametric VI based on the reparametrization trick and stochastic backpropagation, but my maths isn't good enough to fully understand it. \n Does anyone have any thoughts on this? I find it a little strange that every uncertainty estimation paper in segmentation just uses MC Dropout as a default, with just a throwaway line that it is \"approximately Bayesian\", even though it isn't!\n    submitted by    /u/shellyturnwarm  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ry61q5/d_mc_dropout_is_not_bayesian_so_why_are_so_many/",
          "publishedOn": "2022-01-07T12:11:25.000Z",
          "wordCount": 892,
          "title": "[D] MC Dropout is not Bayesian, so why are so many papers still using it for that?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ry4wxp/how_to_implement_an_efficient_layernorm_cuda/",
          "author": null,
          "description": "Article:https://oneflow2020.medium.com/how-to-implement-an-efficient-layernorm-cuda-kernel-oneflow-performance-optimization-731e91a285b8\n Code:https://github.com/Oneflow-Inc/oneflow/\n In a previous article, we discussed OneFlow’s techniques for optimizing the Softmax CUDA Kernel. The performance of the OneFlow-optimized Softmax greatly exceeds that of the Softmax of CuDNN, and OneFlow also fully optimizes half types that many frameworks do not take into account.\n Here, we share OneFlow’s approach for optimizing the performance of another important operator, LayerNorm.\n    submitted by    /u/Just0by  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ry4wxp/how_to_implement_an_efficient_layernorm_cuda/",
          "publishedOn": "2022-01-07T10:59:40.000Z",
          "wordCount": 160,
          "title": "How to Implement an Efficient LayerNorm CUDA Kernel[R]"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ry3urr/d_neurosymbolic_ai/",
          "author": null,
          "description": "Hello,\n I recently came across neuro-symbolic ai, and the concept looked really cool. However, except for a 2-3 papers from IBM-MIT research and a couple of blogs, I couldn’t find any work in this direction. \n  \nApart from VQA, what kind of potential domains can NS be applied to?\n Are there any caveats I am not aware of as to why not many from the community are not working in this direction?\n  \nThank you!\n    submitted by    /u/NightlessBaron  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ry3urr/d_neurosymbolic_ai/",
          "publishedOn": "2022-01-07T09:47:39.000Z",
          "wordCount": 348,
          "title": "[D] Neuro-Symbolic AI"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ry3qw2/d_gan_training_initially_degrades_results_of/",
          "author": null,
          "description": "Hello, I have an issue with the training of a GAN, which consists of a generator and two discriminators, each discriminator working on one specific domain (i.e. time domain and time-frequency domain).\n The generator is used to generate waveforms, provided other waveforms (i.e. noisy waveform to de-noised waveform).\n ​\n The whole GAN is trained in the following way:\n 1-The generator is independently pre-trained by regression, until validation losses reach a plateau.\n 2-The two randomly initialized discriminators are then activated, and GANs training takes place.\n ​\n The output of step 1. is a raw version of the desired output.\n However, step 2 inititially degrades the output of the generator, by removing too much information. This is also reflected in all the validation losses, whose values…",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ry3qw2/d_gan_training_initially_degrades_results_of/",
          "publishedOn": "2022-01-07T09:40:24.000Z",
          "wordCount": 764,
          "title": "[D] GAN training initially degrades results of pre-trained generator"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ry2aw9/ddatacamp_data_leakage_practices_does_datacamp/",
          "author": null,
          "description": "I must be going crazy.\n Anyone take Datacamp courses and do projects?\n So quite a lot of projects and courses actually apply data preprocessing techniques before splitting data that encourage data leakage. \n I can't believe I paid for their services......\n Anyone catch other bad habits that Datacamp encourages? Aside from being a rather ineffective platform to learn?\n    submitted by    /u/THE_REAL_ODB  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ry2aw9/ddatacamp_data_leakage_practices_does_datacamp/",
          "publishedOn": "2022-01-07T08:03:33.000Z",
          "wordCount": 640,
          "title": "[D]DATACAMP & Data Leakage Practices. Does Datacamp need a quality audit?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ry1znw/d_how_to_choose_the_appropriate_network/",
          "author": null,
          "description": "Hi!\n I am currently involved in a project that needs me to classify multimodal data - In which one modality is image. I went through many papers on the topic and most of them just seem to choose a random pretrained architecture for the same - VGG19 being the most common. I found little to no reason for the same except for the performance being better than other models.\n So basically, I want to know whether there is a way to choose the right model? And if not, which one out of Inception, VGG and ResNet are the best for binary image classification on a large scale? \n Thanks!\n    submitted by    /u/prabhav55221  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ry1znw/d_how_to_choose_the_appropriate_network/",
          "publishedOn": "2022-01-07T07:42:32.000Z",
          "wordCount": 494,
          "title": "[D] How to choose the appropriate network: Pretrained Image Recognition Models"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rxnpa4/d_turning_petabytescale_video_data_into_great/",
          "author": null,
          "description": "Imagine 10 cameras, running 24/7 at 30 FPS - that's 27 million frames generated in a single day. Knowing what's in that data or finding the 1% of things that are actually interesting is hard. I genuinely think there's way too little attention put forth to how people should use their raw data effectively even though so many people choose to store petabytes of it, just in case.\n I wrote a little article about taking raw video and turning that into an actionable computer vision model. Would love to have a technical discussion about this so comment away :)\n https://towardsdatascience.com/curating-a-dataset-from-raw-images-and-videos-c8b962eca9ba\n P.S. Yes, I'm also the OP of the post showcasing Sieve from a few days ago!\n    submitted by    /u/happybirthday290  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rxnpa4/d_turning_petabytescale_video_data_into_great/",
          "publishedOn": "2022-01-06T20:09:45.000Z",
          "wordCount": 344,
          "title": "[D] Turning petabyte-scale video data into great datasets"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rxmyw8/project_guidance_on_key_information_extraction/",
          "author": null,
          "description": "Hello all,\n I'm currently working on a project where I'm trying to perform key information extraction (KIE) and classification on rental property operating statements. My plan is to pre-train a model on a similar dataset (e.g. [IBM FinTabNet](https://developer.ibm.com/exchanges/data/all/fintabnet/), etc.) and then fine-tune on a custom dataset I'm creating. My task is to:\n  \nIdentify the key:value pairs associated with each line item for each period in the statement. So for example, for the first line item, we'd have a 3-tuple of ('Rent income', 'Nov 2020', 21,428.03)\n Identify the key:value pairs associated with the total for each line item in the statement. So for example, for the first line item, we'd have a 3-tuple of ('Rent income', 'Total', 322,872.36)\n Automatically classify line it…",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rxmyw8/project_guidance_on_key_information_extraction/",
          "publishedOn": "2022-01-06T19:38:23.000Z",
          "wordCount": 718,
          "title": "[Project] Guidance on Key Information Extraction for financial statements"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rxlaxe/d_sparsest_cut_in_practice/",
          "author": null,
          "description": "Sparsest cut, and its close variant \"balanced cut\", have been studied for ages in the computer science theory community, and there are algorithms to compute this which run fast. Sparsest cut is often billed as a \"graph clustering\" algorithm. \n Sparsest cut gives rise to an obvious data clustering algorithm: build a graph on your data (like kNN or threshold graph), and run sparsest cut to get a 2-way partition. To do a k-way partition, you could recursively do this. From what I know, sparsest cut intuition is one thing that guided the creation of spectral clustering, a widely used clustering method. \n HHas anyone tried implementing this in practice? If so, how does it do on data? If not, why not? I have searched extensively on Google for any implementations of this sparsest-cut style clustering, but I haven't found any. Given that sparsest cut is one of the most well-studied problems in computer theory, I was surprised that no implementation of this clustering method exists.\n    submitted by    /u/Grand_Distribution83  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rxlaxe/d_sparsest_cut_in_practice/",
          "publishedOn": "2022-01-06T18:26:52.000Z",
          "wordCount": 290,
          "title": "[D] Sparsest Cut in practice?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rxker7/d_any_measurement_studies_about_mlaas_monetisation/",
          "author": null,
          "description": "ML as a Service appears to be the new hype, but I cant quite figure out how such technologies are monetised. GPT-3 appears to have attracted quite a lot of clients, but is it profitable at all? What do the incentives look like it this market? \n Are model extraction attacks a problem here at all? How does one protect their IP? I heard of people watermarking their models, but I am not sure how such techniques in their current form would fare in court.\n    submitted by    /u/SuchOccasion457  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rxker7/d_any_measurement_studies_about_mlaas_monetisation/",
          "publishedOn": "2022-01-06T17:49:13.000Z",
          "wordCount": 326,
          "title": "[D] Any measurement studies about MLaaS monetisation"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rxjept/d_yolov5_tta_slower_on_smaller_image/",
          "author": null,
          "description": "Hi all,\n I'm using a Yolov5 model to make inferences and noticed that it was faster at inferring 320x320 images (~0.25s/image) compared to 288x288 images (~0.45s/image). When I disabled test time augmentation, the gap closed, and both image sizes are inferred at ~0.084s/image. Any ideas why augmenting a smaller image size is slower than augmenting a faster image size?\n    submitted by    /u/queue_learning  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rxjept/d_yolov5_tta_slower_on_smaller_image/",
          "publishedOn": "2022-01-06T17:05:49.000Z",
          "wordCount": 341,
          "title": "[D] Yolov5 TTA slower on smaller image"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rxf7wq/n_mdli_ops_a_free_conference_to_help_you_make/",
          "author": null,
          "description": "Disclaimer: Not my conference, but a good friend's work. He's doing an awesome job community building and I thought this might interest the community here as well (I am not a sponsor).\n Hey r/ML, some of you might have heard about MDLI – short for Machine & Deep Learning Israel. It's an independent Israeli community (over 25K members) for professionals in data science and ML, and they are having a conference for everyone (in English), in just 14 days.\n You can register here https://machinelearning.co.il/mdli-ops-2022/\n I know that sometimes these free events tend to feel like commercials, but you should really check out the agenda. Some of the talks are going to be by ML teams at companies like AppsFlyer and BigPanda, explaining how they built their internal stacks and ML systems.\n There's also a super interesting talk by the NVIDIA team, where they will talk about building supercomputers to train ML models on them. This one seems crazy awesome to me.\n It's also a good opportunity to listen to offerings by some cool ML startups, which might give you a better understanding of how they compare, and what you actually care about when choosing MLOps tools.\n The organizers will probably be here in the comments if you have any questions, but in my opinion, every event this community organizes is really awesome, and I get to learn a lot, so I really recommend this.\n    submitted by    /u/PhYsIcS-GUY227  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rxf7wq/n_mdli_ops_a_free_conference_to_help_you_make/",
          "publishedOn": "2022-01-06T13:59:22.000Z",
          "wordCount": 467,
          "title": "[N] MDLI Ops – A free conference to help you make sense of the MLOps landscape"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rxe28v/p_machine_learning_engineering_conferences/",
          "author": null,
          "description": "I was wondering if anyone could recommend and conferences suitable for MLE’s, maybe with a focus on deploying models. I’ve been to traditional data science focused one, such as the Anaconda one. Though I wanted to see if any existed that were more closely connected to deployment.\n    submitted by    /u/MenArePigs69  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rxe28v/p_machine_learning_engineering_conferences/",
          "publishedOn": "2022-01-06T13:00:20.000Z",
          "wordCount": 316,
          "title": "[P] Machine Learning Engineering Conferences"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rxczzl/p_deepchecks_an_opensource_tool_for_high/",
          "author": null,
          "description": "Hey everyone!\n I wanted to share with you an open-source tool we've been building for a while. Deepchecks is an open-source tool for validating & testing models and data efficiently:\n https://github.com/deepchecks/deepchecks\n Deepchecks is a python package, implementing validations and tests needed in order to trust an ML pipeline. It contains many built-in checks, such as verifying the data integrity, inspecting its distributions, validating data splits, evaluating your model, and comparing between different models.\n In addition, it contains test suites, similar to the test suites in software programs, that can accompany you through all building blocks of the ML pipeline development. Each test suite contains checks necessary for the specific part in the pipeline.\n The suite result looks something like this:\n ​\n Suite result\n The suites and checks have a simple syntax and are highly customizable.\n If you want to jump right in, you can try it out in the quick start notebook:\n https://docs.deepchecks.com/en/stable/examples/guides/quickstart_in_5_minutes.html \n What do you think? I’ll be happy to hear your thoughts and feedback.\n    submitted by    /u/EuphoricMeal8344  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rxczzl/p_deepchecks_an_opensource_tool_for_high/",
          "publishedOn": "2022-01-06T12:02:52.000Z",
          "wordCount": 658,
          "title": "[P] Deepchecks: an open-source tool for high standards validations for ML models and data."
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rx9exf/relationship_extraction_for_knowledge_graph/",
          "author": null,
          "description": "submitted by    /u/dreadknight011  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rx9exf/relationship_extraction_for_knowledge_graph/",
          "publishedOn": "2022-01-06T08:15:19.000Z",
          "wordCount": 108,
          "title": "Relationship extraction for knowledge graph creation from biomedical literature"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rx2vov/d_retrieval_transformers_for_other_domains/",
          "author": null,
          "description": "I am just going through the research behind this: http://jalammar.github.io/illustrated-retrieval-transformer/ and was wondering if there has been work like this for other domains say computer vision for example?\n    submitted by    /u/data-drone  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rx2vov/d_retrieval_transformers_for_other_domains/",
          "publishedOn": "2022-01-06T01:46:08.000Z",
          "wordCount": 201,
          "title": "[D] Retrieval transformers for other domains?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rx1c4c/d_how_to_correctly_use_batch_norm_in_preloaded/",
          "author": null,
          "description": "I have tried using a few pre-loaded architectures in Tensorflow including tf.keras.applications.efficientnet.EfficientNetB3\n and tf.keras.applications.MobileNetV3Large\n . I am working on medical images so I am not any pretrained ImageNet weights. I manage to reach a decent accuracy during the training epoch (eg 75%), but my validation accuracy doesn't cross random performance (eg 3%). Upon closer inspection I have identified the cause as being the batch normalization layers - If I run the evaluation batches w/ training=True\n on the BatchNorm layers, I am able to reproduce the training set accuracy. I have tried playing with the momentum parameter and changing it from its default value of .999 to .75 or even .5 but with no effect.\n My question is what may be causing this and how can I fix it? \n On a larger note, do most people find that they can use pre-loaded architectures out-of-the-box or are there various parameters that they need to modify to get it to work properly?\n    submitted by    /u/rsandler  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rx1c4c/d_how_to_correctly_use_batch_norm_in_preloaded/",
          "publishedOn": "2022-01-06T00:27:33.000Z",
          "wordCount": 259,
          "title": "[D] How to correctly use batch norm in pre-loaded architectures?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rwxt8r/d_legal_use_of_functions_in_pytorch_or_tensorflow/",
          "author": null,
          "description": "Does anybody know, is it legal to use Dropout or BatchNorm from Pytorch and Tensorflow due to Google's patents of these two functions? Did some library avoided patent infringement in its implementation of those functions?\n    submitted by    /u/Nos_per  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rwxt8r/d_legal_use_of_functions_in_pytorch_or_tensorflow/",
          "publishedOn": "2022-01-05T21:55:02.000Z",
          "wordCount": 262,
          "title": "[D] Legal use of functions in Pytorch or Tensorflow"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rwxmw4/d_search_engine_for_time_series/",
          "author": null,
          "description": "Hi,\n Last year I developed a passenger flow forecasting model. Passenger flows are heavily influenced by lockdown measures and the weather, so I wanted to incorporate features relating to that into my model. Doing so, I encountered various frustrations:\n ​\n  \nData providers generally focus on a specific domain (e.g. covid, weather, ecommerce). Forecasts however can be influenced by data from many domains. Finding these providers, signing up, and reading documentation is very time consuming.\n It takes a lot of code just to get the data you want. For example, to get covid data for my province, I first had to call a /list endpoint, then retrieve the province identifier, and then loop over the data endpoint because the maximum range was 2 months. I think the core problem is that API’s (as the …",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rwxmw4/d_search_engine_for_time_series/",
          "publishedOn": "2022-01-05T21:46:11.000Z",
          "wordCount": 373,
          "title": "[D] Search engine for time series"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rwwjal/d_a_paper_suggests_most_time_series_anomaly/",
          "author": null,
          "description": "I just stumbled on this very nice paper [a], which will appear in AAAI-22. \n The title seems much too modest, they show that a random algorithm can achieve apparent SOTA results in this domain. This seems to be a stunning result, that casts doubt on the contribution of dozens of papers. \n For some reason, the area of Time Series Anomaly Detection seems to be the wild west of dubious papers and sloppy thinking. \n As an aside, there is a benchmark set of 250 datasets here [b] that can be evaluated in a way that is free of the flaw.\n (my post title reflects my understanding of the paper, the authors may have a different preferred claim).\n [a] Towards a Rigorous Evaluation of Time-series Anomaly Detection https://arxiv.org/pdf/2109.05257.pdf\n [b] www.cs.ucr.edu/\\~eamonn/time\\_series\\_data\\_2018/UCR\\_TimeSeriesAnomalyDatasets2021.zip\n    submitted by    /u/eamonnkeogh  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rwwjal/d_a_paper_suggests_most_time_series_anomaly/",
          "publishedOn": "2022-01-05T20:49:37.000Z",
          "wordCount": 292,
          "title": "[D] (A paper suggests) Most Time Series Anomaly Detection Papers are Wrong"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rwwdh0/d_multiagent_deep_reinforcement_learning/",
          "author": null,
          "description": "Hello!\n I hope you´re doing well.\n I am working on a multi-agent system with MADDPG.At time t when an agent asks for a task, the other agents are busy (i.e., the busy agents are those that are still processing a task, they didn´t finish it yet).So with this configuration, in the learning phase, I don´t know how to mask the state of the busy agents when injecting the state and action pair to the critic network.\n Thank you.\n    submitted by    /u/GuavaAgreeable208  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rwwdh0/d_multiagent_deep_reinforcement_learning/",
          "publishedOn": "2022-01-05T20:40:20.000Z",
          "wordCount": 165,
          "title": "[D] Multi-agent Deep Reinforcement learning"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rwumjj/d_vqvae_are_there_heuristics_for_the_number_of/",
          "author": null,
          "description": "Hi r/MachineLearning, does anyone with experience training VQ-VAEs know if there are good rules of thumb for the embedding size?\n E.g. given data of dimension N, use M embeddings of size P\n Thanks for any help!\n    submitted by    /u/Natural_Profession_8  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rwumjj/d_vqvae_are_there_heuristics_for_the_number_of/",
          "publishedOn": "2022-01-05T19:15:26.000Z",
          "wordCount": 193,
          "title": "[D] VQ-VAE: Are there heuristics for the number of embeddings and the embedding dimension?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rwtpxv/d_preparing_for_a_comprehensive_exam_in_ml/",
          "author": null,
          "description": "I am a PhD student based in Canada and have a comprehensive exam coming up in 4-6 months. This is an exam I have been nervous about since I began my PhD. I am fairly confident about the actual proposal and answering questions related to my field. What concerns me more is fundamental/background question as ML and statistics is so broad. Plus, I am a little on the older side and my memory is a little poor.\n Have any students here taken a comprehensive exam? If so, what was your experience and how did you prepare? Is reading/making notes from a textbook a good idea? Or is preparing a list of topics and reading extensively about them a better option?\n    submitted by    /u/ConfusedNoobie  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rwtpxv/d_preparing_for_a_comprehensive_exam_in_ml/",
          "publishedOn": "2022-01-05T18:37:16.000Z",
          "wordCount": 399,
          "title": "[D] Preparing for a Comprehensive Exam in ML"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rwsh49/d_features_of_features_of_samples_to_features_of/",
          "author": null,
          "description": "I have a mxn matrix, m observations and n variables. Along with that, I have another dataset which are the features of the variables, a nxp matrix.\n What could be a way to get an mxp matrix (without naive matrix multiplication)? I wish to relate the observations to the features of variable.\n    submitted by    /u/l34df4rm3r  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rwsh49/d_features_of_features_of_samples_to_features_of/",
          "publishedOn": "2022-01-05T17:44:08.000Z",
          "wordCount": 156,
          "title": "[D] Features of features of samples to features of samples?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rwseha/d_what_is_the_format_of_full_paper_presentations/",
          "author": null,
          "description": "Hi all,\n This is my first conference season, so I am curious about how do author of full papers (not extended abstracts or student-track, not invited speeches) present in conferences like ICML, AAAI and IJCAI?\n I mean, for instance: are presentations performed in a panel with 3-5 presenters using slides, or are they all presented as posters where authors stay available for a duration of time for the interested readers to show up and discuss? Or something else?\n    submitted by    /u/briannaszvenska  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rwseha/d_what_is_the_format_of_full_paper_presentations/",
          "publishedOn": "2022-01-05T17:40:54.000Z",
          "wordCount": 202,
          "title": "[D] What is the format of full paper presentations in general ML conferences like IJCAI and AAAI?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rwrnk2/d_can_you_recommend_funny_papper_like_single/",
          "author": null,
          "description": "I really enjoyed reading this for a change to the textbook papers.\n  \nAbstract The leading approaches in language modeling are all obsessed with TV shows of my youth - namely Transformers and Sesame Street. Transformers this, Transformers that, and over here a bonfire worth of GPU-TPU-neuromorphic wafer scale sil- icon. We opt for the lazy path of old and proven techniques with a fancy crypto1 inspired acronym: the Single Headed Attention RNN (SHA-RNN). The author’s lone goal is to show that the entire field might have evolved a different direction if we had instead been obsessed with a slightly differ- ent acronym and slightly different result. We take a previously strong language model based only on boring LSTMs and get it to within a stone’s throw of a stone’s throw of state-of-the-art byte level language model results on enwik8. This work has undergone no intensive hyperparameter optimization and lived entirely on a commodity desktop machine that made the author’s small stu- dio apartment far too warm in the midst of a San Franciscan summer2 . The final results are achiev- able in plus or minus 24 hours on a single GPU as the author is impatient. The attention mechanism is also readily extended to large contexts with minimal computation. Take that Sesame Street.\n  \nhttps://arxiv.org/abs/1911.11423\n    submitted by    /u/Puzzled-Bite-8467  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rwrnk2/d_can_you_recommend_funny_papper_like_single/",
          "publishedOn": "2022-01-05T17:07:29.000Z",
          "wordCount": 791,
          "title": "[D] Can you recommend funny papper like \"Single Headed Attention RNN: Stop Thinking With Your Head\""
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rwq3tb/d_normalizing_flows_for_distributions_with_finit/",
          "author": null,
          "description": "I need to learn a map from Gaussain distribution to Gamma distribution with some custom parameters. So for both distributions, I can sample and evaluate probability density. The first thing, that came to my mind is using normalizing flow. \n Most approaches include log target probability density evaluation in the loss function. Obviously, normalizing flow sometimes returns negative values, and this term equals to infinity. \"Positivation\" functions on top of the NF break bijection properties for some regions of space (if not theoretically, but numerically - defenetely). \n Does NF approach is inapplicable from the box for such a simple problem or I'm missing something?\n    submitted by    /u/likan_blk  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rwq3tb/d_normalizing_flows_for_distributions_with_finit/",
          "publishedOn": "2022-01-05T15:59:33.000Z",
          "wordCount": 347,
          "title": "[D] Normalizing flows for distributions with finit support"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rwots2/d_methods_to_create_monolingual_language_model/",
          "author": null,
          "description": "Apart from just fine-tuning the pretrained multilingual language model on the target language, is there anything more sophisticated that people are doing?\n    submitted by    /u/learning-machinist  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rwots2/d_methods_to_create_monolingual_language_model/",
          "publishedOn": "2022-01-05T14:59:47.000Z",
          "wordCount": 159,
          "title": "[D] Methods to create monolingual language model from pretrained multilingual model"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rwmvn7/p_blogs_on_fundamentals_of_scorebased_and/",
          "author": null,
          "description": "This two-part blog describes the theoretical fundamentals of Score based models, Diffusion Probabilstic models and their relationship. It is written to be a coherent documentation of the theoretical developements in this new class of generative model. Rigorous mathematical proofs are excluded in order to make it more readable. Sharing it in case anyone finds it useful.\n  \nPart 1: Score-base models\n Part 2: Diffusion Probabilistic Models\n  \n   submitted by    /u/dasayan05  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rwmvn7/p_blogs_on_fundamentals_of_scorebased_and/",
          "publishedOn": "2022-01-05T13:26:31.000Z",
          "wordCount": 168,
          "title": "[P] Blogs on fundamentals of Score-based and Diffusion Probabilistic Models"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rwl4xl/d_pretraining_the_discriminator_of_a_least/",
          "author": null,
          "description": "I am trying to train a GAN to generate human poses in 3D space using the Humans 3.6M dataset. The output of the GAN is thus the 3D coordinates of the human joints. I have been experimenting with vanilla GANs but the output is quite noisy.\n I am now looking into Least Squares GAN but was wondering if it is a good idea to pretrain the discriminator of a Least Squares GAN since LSGANs address the problem of vanishing gradients and loss saturation?\n    submitted by    /u/I_am_a_robot_  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rwl4xl/d_pretraining_the_discriminator_of_a_least/",
          "publishedOn": "2022-01-05T11:49:55.000Z",
          "wordCount": 642,
          "title": "[D] Pretraining the discriminator of a Least Squares GAN"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rwju9t/d_minimum_corpus_size_for_word_embedding/",
          "author": null,
          "description": "Dear all,\n I have a smallish ~100MB corpus (of historical text in a non-mainstream language), on which I want to apply word embedding.\n - Is that enough? Shall I only consider \"frequent\" words? How frequent? does it help if I do some preprocessing such as stemming etc...?\n - How do I choose the parameters, especially the embedding dimensionality?\n - Any libraries recommended?\n - Are there some language-agnostic, unsupervised ways to evaluate the embeddings?\n ​\n Thanks\n    submitted by    /u/ihatebeinganonymous  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rwju9t/d_minimum_corpus_size_for_word_embedding/",
          "publishedOn": "2022-01-05T10:29:13.000Z",
          "wordCount": 420,
          "title": "[D] Minimum Corpus Size for Word Embedding Extraction"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rwj813/d_ideal_deep_learning_library/",
          "author": null,
          "description": "From researcher perspective: - What do you miss in libraries like PyTorch or TensorFlow? - What could be improved?\n Some possible examples: - The way how autodiff works - Debugging features - Working with axes, einops - Something that just feel awkward, inconvenient or incomplete\n I would very much appreciate it if you could share your thoughts on this.\n    submitted by    /u/u6785  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rwj813/d_ideal_deep_learning_library/",
          "publishedOn": "2022-01-05T09:48:44.000Z",
          "wordCount": 949,
          "title": "[D] Ideal deep learning library"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rwcikx/predicting_future_labels_where_the_future_values/",
          "author": null,
          "description": "I've done a fair bit of work with RNNs and Time series data, but I now realize there may be a fundamental gap in my knowledge. I was reading through this and it raised some questions about the different kinds of time forecasting problems. \n So, this is my summary of how it all works. Please correct me if I’m wrong. \n Scenario 1: You want to predict the value of some stocks into the future. Let’s say you have k stocks and n days of data. You don’t have features/labels stocks, rather the input is each stocks’ current and previous values, and the output is each stocks’ future values. \n Your two main options are the ‘auto-regressive’ approach where you predict the values one step at a time and feed them back in, or ‘single shot’ approach where you predict all values a fixed amount of time step…",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rwcikx/predicting_future_labels_where_the_future_values/",
          "publishedOn": "2022-01-05T03:09:46.000Z",
          "wordCount": 781,
          "title": "Predicting future labels where the future values of only some features are known (RNNs, time series)[D]"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rwc7zy/p_classification_with_imbalanced_datasets_question/",
          "author": null,
          "description": "I've been working on a medical classification project with an imbalanced tabular dataset. I have 3 classes, and each class has 44, 16, and 14 rows of data respectively. When I train a random forest classifier, I see that my model is only predicting the dominant class for all test instances most of the time. How can I get around to this? Also, are there any recommendations you can give me for dealing with imbalanced datasets? Thank you\n    submitted by    /u/chazy07  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rwc7zy/p_classification_with_imbalanced_datasets_question/",
          "publishedOn": "2022-01-05T02:55:25.000Z",
          "wordCount": 954,
          "title": "[P] Classification with imbalanced datasets question"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rwc10x/p_i_implemented_conformer_convolutionaugmented/",
          "author": null,
          "description": "I implemented Google AI's \"Conformer: Convolution-augmented Transformer for Speech Recognition\" paper, it achieves the best of both worlds by combining CNNs and transformers to model both local and global dependencies and improves the local inductive bias in Transformers.\n https://github.com/Rishit-dagli/Conformer\n    submitted by    /u/Rishit-dagli  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rwc10x/p_i_implemented_conformer_convolutionaugmented/",
          "publishedOn": "2022-01-05T02:45:59.000Z",
          "wordCount": 179,
          "title": "[P] I implemented Conformer: Convolution-augmented Transformer"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rw8jxd/p_rayskorch_distributed_pytorch_on_ray_with/",
          "author": null,
          "description": "tl;dr: train PyTorch models on large tabular datasets with a scikit-learn (skorch) API\n Hi r/MachineLearning,\n I'm the principal author of ray-skorch, a library that lets you run distributed PyTorch training on large-scale datasets while providing a familiar, scikit-learn compatible skorch API, integrating well with the rest of the scikit-learn ecosystem.\n Under the hood, ray-skorch uses Ray Train for distributed PyTorch training and Ray Data for handling and shuffling large datasets.\n ray-skorch works only with tabular data. Currently, it can use numpy arrays, pandas dataframes and Ray Data Datasets.\n pip install ray-skorch\n You can switch your skorch code to ray-skorch just by changing a few lines:\n import numpy as np from sklearn.datasets import make_classification from torch import nn # pip install pytorch_tabnet from pytorch_tabnet.tab_network import TabNet from ray_skorch import RayTrainNeuralNet X, y = make_classification(1000, 20, n_informative=10, random_state=0) X = X.astype(np.float32) y = y.astype(np.int64) net = RayTrainNeuralNet( TabNet, num_workers=2, # the only new mandatory argument criterion=nn.CrossEntropyLoss, max_epochs=10, lr=0.1, # TabNet specific arguments module__input_dim=20, module__output_dim=2, # required for classification loss funcs iterator_train__unsqueeze_label_tensor=False, iterator_valid__unsqueeze_label_tensor=False, ) net.fit(X, y) # predict_proba returns a ray.data.Dataset y_proba = net.predict_proba(X).to_pandas() \n More examples, including ones on bigger datasets, can be found here - https://github.com/Yard1/ray-skorch/tree/main/examples\n The package is experimental, and I’d love to hear your feedback - both on the package itself and on the concept of distributed training on tabular data with simple, familiar APIs. Any comments, suggestions or bug reports are hugely appreciated!\n    submitted by    /u/Yard1PL  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rw8jxd/p_rayskorch_distributed_pytorch_on_ray_with/",
          "publishedOn": "2022-01-04T23:59:25.000Z",
          "wordCount": 331,
          "title": "[P] ray-skorch - distributed PyTorch on Ray with sklearn API"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rw50hg/d_deep_learning_is_the_future_of_gaming/",
          "author": null,
          "description": "Hey everybody --- I know this isn't hard core AI research but I have been thinking a lot about deep learning and gaming recently and put together a little presentation on how I see things unfolding. Lots of cool research featured in the video.\n https://www.youtube.com/watch?v=JDL8rZzYVwQ\n I go over:\n  \nPhotorealistic neural rendering\n Deepfakes for gaming (https://www.youtube.com/watch?v=RR7u11ANDWE is a better example than the obama one I used)\n GAN theft auto and dreaming up game engines with neural networks\n Large language models for building realistic NPCs and storytelling\n Using OpenAI Codex to automatically program games.\n  \nIt's really clear that deep learning is the most important technology to impact gaming since the advent of 3D graphics. Would love to talk with anybody who is working on stuff in this space.\n    submitted by    /u/sabalaba  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rw50hg/d_deep_learning_is_the_future_of_gaming/",
          "publishedOn": "2022-01-04T21:18:30.000Z",
          "wordCount": 440,
          "title": "[D] Deep Learning is the future of gaming."
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rw2uac/d_style_transfer_with_noise_vector/",
          "author": null,
          "description": "Hi everyone, I'm looking for a model which can perform style transfer, but also takes an auxiliary noise vector similar to that for StyleGAN to generate many stylized images for a single input image. Is anyone aware of any model meeting these requirements? My best idea so far is to first embed the image into the StyleGAN latent space with this paper, and then add noise to that vector.\n    submitted by    /u/Sebass13  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rw2uac/d_style_transfer_with_noise_vector/",
          "publishedOn": "2022-01-04T19:43:09.000Z",
          "wordCount": 246,
          "title": "[D] Style Transfer with Noise Vector"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rvzhnh/d_interpolation_extrapolation_and_linearisation/",
          "author": null,
          "description": "Special machine learning street talk episode! Yann LeCun thinks that it's specious to say neural network models are interpolating because in high dimensions, everything is extrapolation. Recently Dr. Randall Balestriero, Dr. Jerome Pesente and prof. Yann LeCun released their paper learning in high dimensions always amounts to extrapolation. This discussion has completely changed how we think about neural networks and their behaviour. \n In the intro we talk about the spline theory of NNs, interpolation in NNs and the curse of dimensionality. \n YT: https://youtu.be/86ib0sfdFtw\n Pod: https://anchor.fm/machinelearningstreettalk/episodes/061-Interpolation--Extrapolation-and-Linearisation-Prof--Yann-LeCun--Dr--Randall-Balestriero-e1cgdr0\n References: \n Learning in High Dimension Always Amounts to Extrapolation [Randall Balestriero, Jerome Pesenti, Yann LeCun]\n https://arxiv.org/abs/2110.09485 \n A Spline Theory of Deep Learning [Dr. Balestriero, baraniuk] https://proceedings.mlr.press/v80/balestriero18b.html \n Neural Decision Trees [Dr. Balestriero]\n https://arxiv.org/pdf/1702.07360.pdf \n Interpolation of Sparse High-Dimensional Data [Dr. Thomas Lux] https://tchlux.github.io/papers/tchlux-2020-NUMA.pdf\n    submitted by    /u/timscarfe  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rvzhnh/d_interpolation_extrapolation_and_linearisation/",
          "publishedOn": "2022-01-04T17:17:46.000Z",
          "wordCount": 401,
          "title": "[D] Interpolation, Extrapolation and Linearisation (Prof. Yann LeCun, Dr. Randall Balestriero)"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rvz50d/d_neural_networks_using_a_generic_gpu_framework/",
          "author": null,
          "description": "I have a (personal) ML project that uses CNNs but I have two little problems: 1. not everyone has a NVidia GPU at home (myself included, sadly); 2. The CNN needs to be trained every time it is used (it's photo to photo style transfer).\n So, what would be a good framework to implement the CNN for training (targeting desktop only)? I thought about using OpenGL, but I don't know if using GLSL shaders would be a good fit for it.\n    submitted by    /u/crimsom_king  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rvz50d/d_neural_networks_using_a_generic_gpu_framework/",
          "publishedOn": "2022-01-04T17:02:38.000Z",
          "wordCount": 497,
          "title": "[D] Neural Networks using a generic GPU framework"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rvyuhb/d_what_are_interviews_usually_like_for_ml/",
          "author": null,
          "description": "For context, I'm applying for PhD level positions. Should I expect technical interviews including coding challenges similar to SWE?\n Any advice on prepping?\n    submitted by    /u/_Dark_Forest  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rvyuhb/d_what_are_interviews_usually_like_for_ml/",
          "publishedOn": "2022-01-04T16:50:10.000Z",
          "wordCount": 195,
          "title": "[D] What are interviews usually like for ML positions?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rvwehk/deep_learning_interviews_hundreds_of_fully_solved/",
          "author": null,
          "description": "submitted by    /u/pit_station  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rvwehk/deep_learning_interviews_hundreds_of_fully_solved/",
          "publishedOn": "2022-01-04T15:04:16.000Z",
          "wordCount": 138,
          "title": "Deep Learning Interviews: Hundreds of fully solved job interview questions from a wide range of key topics in AI"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rvw0rv/n_launching_dagshub_20_gitintegrated_data/",
          "author": null,
          "description": "TL;DR – DagsHub is integrated with Label Studio, and you can now open datasets from Git and DVC remotes, label them and commit labels back, without doing any DevOps. You can also comment on labels, bounding boxes, or any file. Check out the example project, or try out the tutorial.\n Comparing annotations\n Hi r/ML! I'm one of the creators of DagsHub (https://www.dagshub.com). We help ML practitioners create a central repository for their projects, where they can leverage open-source tools to version datasets and models, track experiments, and starting today – label data, and comment on anything. Like GitHub for machine learning (you probably heard that before, but we mean it).\n Our vision is that anyone could jump into an open-source data science project and contribute code, data, labeling,…",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rvw0rv/n_launching_dagshub_20_gitintegrated_data/",
          "publishedOn": "2022-01-04T14:46:59.000Z",
          "wordCount": 678,
          "title": "[N] Launching DagsHub 2.0 – Git-integrated data labeling and smart ML discussions"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rvvfoa/d_why_is_vae_used_instead_of_autoencoder_in_the/",
          "author": null,
          "description": "Hi All,\n I was just reading this paper and was wondering if we just want to achieve a compact version of the original representation we could just use a traditional AutoEncoder. Is there any specific reason the VAE is used?\n Thanks!\n    submitted by    /u/StageTraditional636  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rvvfoa/d_why_is_vae_used_instead_of_autoencoder_in_the/",
          "publishedOn": "2022-01-04T14:19:29.000Z",
          "wordCount": 647,
          "title": "[D] Why is VAE used instead of AutoEncoder in the World Models paper (https://arxiv.org/pdf/1803.10122.pdf)?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rvqr95/r_play_against_an_ai_to_detect_fake_audio/",
          "author": null,
          "description": "Hi everybody,\n i'm a PhD student interested in audio spoofs (voice recordings faked with the help of AI), and have developed an online game: You play against an artificial intelligence and try to distinguish spoofed from real audio recordings.\n It's fun, and very much supports my research. All partificpation (i.e. playing the game), comments or suggestions are welcome!\n https://deepfake-demo.aisec.fraunhofer.de/\n    submitted by    /u/mummni  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rvqr95/r_play_against_an_ai_to_detect_fake_audio/",
          "publishedOn": "2022-01-04T09:51:57.000Z",
          "wordCount": 328,
          "title": "[R] Play against an AI to detect fake audio"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rvol2l/d_what_are_the_reviewers_score_of_the_submissions/",
          "author": null,
          "description": "I submitted a paper to AISTATS 2022 that can be a breakthrough with outstanding contributions. The paper received 876 scores from reviewers that could only improve to 877 after the rebuttals. What are the chances that our submission enters the short list for best paper recognition? What are the average reviewers' score of the ones getting nominated in these top conferences?\n    submitted by    /u/Cyrus_the-great  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rvol2l/d_what_are_the_reviewers_score_of_the_submissions/",
          "publishedOn": "2022-01-04T07:21:23.000Z",
          "wordCount": 354,
          "title": "[D] What are the reviewers' score of the submissions nominated for best paper award in top ML conferences such as NeurIPS, ICML, AISTATS, etc.?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rvn3vp/dwho_knows_the_paper_address_of_the_code/",
          "author": null,
          "description": "https://github.com/yuzisheng/trajectory-compress\n especially the Spatio-Temporal Curvature Streaming\n    submitted by    /u/choayue  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rvn3vp/dwho_knows_the_paper_address_of_the_code/",
          "publishedOn": "2022-01-04T05:53:22.000Z",
          "wordCount": 96,
          "title": "[D]who knows the paper address of the code?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rvn3dh/p_sieve_we_processed_24_hours_of_security_footage/",
          "author": null,
          "description": "Hey everyone! I’m one of the creators of Sieve, and I’m excited to be sharing it!\n Sieve is an API that helps you store, process, and automatically search your video data–instantly and efficiently. Just think 10 cameras recording footage at 30 FPS, 24/7. That would be 27 million frames generated in a single day. The videos might be searchable by timestamp, but finding moments of interest is like searching for a needle in a haystack.\n We built this visual demo (link here) a little while back which we’d love to get feedback on. It’s ~24 hours of security footage that our API processed in <10 mins and has simple querying and export functionality enabled. We see applications in better understanding what data you have, figuring out which data to send to labeling, sampling datasets for training, and building multiple test sets for models by scenario.\n To try it on your videos: https://github.com/Sieve-Data/automatic-video-processing\n Visual dashboard walkthrough: https://youtu.be/_uyjp_HGZl4\n https://preview.redd.it/bn8hoqoa1m981.png?width=2540&format=png&auto=webp&s=25fb08037438593291fecf7e50ca58ec1f9bea72\n https://preview.redd.it/jwkd7uoa1m981.png?width=2540&format=png&auto=webp&s=e25382b4b09855e5934608754a8b74bdbaf93204\n https://preview.redd.it/0dd74toa1m981.png?width=2540&format=png&auto=webp&s=05b7625195947b8f15891a9019070efa3730b336\n https://preview.redd.it/alg4ruoa1m981.png?width=2540&format=png&auto=webp&s=f5caad143b0d23f3add08f431d0ada322ae4e84d\n https://preview.redd.it/8c2pw0pa1m981.png?width=2540&format=png&auto=webp&s=e6438f03e3fc7a00ccdf01c9b7075b9e8752affd\n    submitted by    /u/happybirthday290  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rvn3dh/p_sieve_we_processed_24_hours_of_security_footage/",
          "publishedOn": "2022-01-04T05:52:33.000Z",
          "wordCount": 1785,
          "title": "[P] Sieve: We processed ~24 hours of security footage in <10 mins (now semantically searchable per-frame!)"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rvmiyr/d_paper_summary_rethinking_segmentation_from_a/",
          "author": null,
          "description": "Hi, I have just published my latest medium article. It is a summary of a scientific paper that aims to eliminate the effect of locality which is one of the limitations of CNNs. In this attempt, researchers tried to reform the image semantic segmentation problem then operate a proposed transformer, and finally, introduce three different decoder architectures.\n Please read it and give me your feedback. If you find it interesting, you can share it with others who are interested in ML as well. Also, if you find it helpful, you can follow me on medium to be updated on my forthcoming articles.🙂\n https://rezayazdanfar.medium.com/26868efacc52\n    submitted by    /u/rezayazdanfar  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rvmiyr/d_paper_summary_rethinking_segmentation_from_a/",
          "publishedOn": "2022-01-04T05:20:56.000Z",
          "wordCount": 209,
          "title": "[D] Paper Summary [Rethinking Segmentation from a Sequence-to-Sequence Perspective with Transfromers]"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rvmeo6/d_which_tools_can_be_helpful_for_annotation_of/",
          "author": null,
          "description": "There is a team in my university who work on ergonomics. They want to do action recognition on some videos. They approached me for help. I work on images. I don't have idea about videos. I have dataset. I want to annotate key points in each frame. Please tell me which tools can be helpful for annotation of videos?\n    submitted by    /u/SAbdusSamad  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rvmeo6/d_which_tools_can_be_helpful_for_annotation_of/",
          "publishedOn": "2022-01-04T05:14:15.000Z",
          "wordCount": 252,
          "title": "[D] Which tools can be helpful for annotation of videos for action recognition?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rvadz4/r_yourtts_towards_zeroshot_multispeaker_tts_and/",
          "author": null,
          "description": "YourTTS brings the power of a multilingual approach to the task of zero-shot multi-speaker TTS... it is possible to fine-tune the model with less than 1 minute of speech and achieve state-of-the-art results in voice similarity and with reasonable quality.\n 🤖 Demo: https://coqui.ai\n 👩‍💻 Code: https://github.com/coqui-ai/tts\n 🚀 Blogpost: https://coqui.ai/blog/tts/yourtts-zero-shot-text-synthesis-low-resource-languages\n 📎 Paper: https://arxiv.org/abs/2112.02418\n    submitted by    /u/josh-r-meyer  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rvadz4/r_yourtts_towards_zeroshot_multispeaker_tts_and/",
          "publishedOn": "2022-01-03T19:54:47.000Z",
          "wordCount": 179,
          "title": "[R] 🐸YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rva1dk/d_why_academia_tends_to_underinvest_in/",
          "author": null,
          "description": "Tweet from @jackclarkSF asks an interesting question:\n  \nIs there a good paper that explains how/why academia tends to under-invest in engineering infrastructure?\n  \nhttps://twitter.com/jackclarkSF/status/1478077579110207489\n    submitted by    /u/MassivePellfish  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rva1dk/d_why_academia_tends_to_underinvest_in/",
          "publishedOn": "2022-01-03T19:39:29.000Z",
          "wordCount": 236,
          "title": "[D] \"why academia tends to under-invest in engineering infrastructure?\""
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rv7yos/d_how_to_measure_accuracy_of_knn_imputation/",
          "author": null,
          "description": "I have a dataset in which the the best way to impute missing values is to use kNN but before I go ahead and do that I'd like to check what kind of accuracy I have with that form of imputation in this specific dataset and which k should be used. My original solution was as follows:\n  \nFrom my original dataset, remove all rows with missing values\n From this dataset, impute NaNs randomly throughout the dataset with the same frequency that they were missing originally and store the values that were replaced with NaN in a new dataset as the ground truth\n Impute using kNN\n Check the accuracy of the imputed values against the ground truth values stored in step 2 using MAE for different k values\n  \nIs there an easier way to do this? If not, should I be using MAE or another accuracy score?\n    submitted by    /u/Ok-Culture-9123  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rv7yos/d_how_to_measure_accuracy_of_knn_imputation/",
          "publishedOn": "2022-01-03T18:10:28.000Z",
          "wordCount": 253,
          "title": "[D] How to measure accuracy of kNN Imputation?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rv4u2w/d_paper_that_mathematically_proves_that_gradient/",
          "author": null,
          "description": "I think this is a well-known paper, but I have not been able to find it. I am interested in the paper that mathematically proves neural nets can fit any set of datapoints. So far what I have found mostly is papers that show empirically that or something related to that, like this one. I'd appreciate any help.\n Edit: u/the_new_scientist shared this paper which is what I was looking for: https://arxiv.org/pdf/1810.02054.pdf Also, I apologize for my vague description. Now that the paper is shown, I hope it is more clear to future readers what kind of results I meant, but in case that is not case, I was wondering about this question: under what conditions can a neural network achieve zero training error? and in particular, I am interested in papers with mathematical (even without empirical) results.\n    submitted by    /u/carlml  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rv4u2w/d_paper_that_mathematically_proves_that_gradient/",
          "publishedOn": "2022-01-03T15:56:40.000Z",
          "wordCount": 529,
          "title": "[D] Paper that mathematically proves that gradient descent can achieve zero training error."
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rv4nah/d_nlp_hybridization_of_statistical_approach_and/",
          "author": null,
          "description": "Hi everyone!\n I have a question for you. For context, we aggregate on a platform the various AI APIs on the market (GCP, Azure, etc.) and including NLP APIs (keyword extraction, sentiment analysis, NER, etc.). The idea is that a developer doesn't have to create accounts with different providers and can have them all on one API to test, compare and change whenever he wants.\n However, many customers ask us how to mix the \"statistical\" approach behind these APIs with expert systems and how to achieve hybridization.\n Do you have any idea how to do this?\n Thanks,\n    submitted by    /u/tah_zem  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rv4nah/d_nlp_hybridization_of_statistical_approach_and/",
          "publishedOn": "2022-01-03T15:48:20.000Z",
          "wordCount": 198,
          "title": "[D] NLP: Hybridization of statistical approach and expert system ?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rv3yty/p_bringing_serverless_to_ml_stateful_arbitrary/",
          "author": null,
          "description": "Serverless infrastructure is yet practical to use for ML but we think it could bring lots of benefits. So, with a friend, we decided to make serverless easy for ML and we are building a platform to solve the main issues we find in serverless for ML: \n - Stateful: We don´t want to reload a whole model every time a user calls model.predict\n - Arbritary dependecies: Normal python code with any package dependencies you use and love, just many many times in parallel\n - Scale-up and scale-down: scale up with ease and auto shutdown to keep resources consumptionYou can visit our webpage, try the demo, and request early access to use our platform!\n Webpage: https://telekinesis.cloud \n Happy to receive questions and comments on what we are building!\n    submitted by    /u/snuns90  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rv3yty/p_bringing_serverless_to_ml_stateful_arbitrary/",
          "publishedOn": "2022-01-03T15:18:08.000Z",
          "wordCount": 281,
          "title": "[P] Bringing serverless to ML - stateful, arbitrary dependency, serverless for ML"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rv37yq/d_what_are_your_hopes_for_machine_learning_in_2022/",
          "author": null,
          "description": "Hi r/MachineLearning!\n I was just wondering what some of you are hoping ML can accomplish or overcome in this new year - interested in hearing your thoughts!\n    submitted by    /u/DataGeek0  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rv37yq/d_what_are_your_hopes_for_machine_learning_in_2022/",
          "publishedOn": "2022-01-03T14:43:33.000Z",
          "wordCount": 287,
          "title": "[D] What are your hopes for Machine Learning in 2022?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rv2j9k/r_the_illustrated_retrieval_transformer_gpt3/",
          "author": null,
          "description": "Hi r/MachineLearning,\n I spent some time wrapping my head around DeepMind's Retro Transformer and visualizing how it works. Hope you find it useful. All feedback is welcome!\n http://jalammar.github.io/illustrated-retrieval-transformer/\n    submitted by    /u/jayalammar  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rv2j9k/r_the_illustrated_retrieval_transformer_gpt3/",
          "publishedOn": "2022-01-03T14:10:10.000Z",
          "wordCount": 1337,
          "title": "[R] The Illustrated Retrieval Transformer (GPT3 performance at 4% the size)"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ruz0nc/d_what_causes_feature_collapse/",
          "author": null,
          "description": "For those of you unfamiliar feature collapse is when you train a model for classification and the model ends up mapping out-of-distribution data or data of different classes in very close proximity in multi-dimensional space. So for example once your model learns a cluster so to speak for cat, during test it projects a dog into the center of that cluster and classifies it as cat. Some ways to sort of deal with this in CV is double gradient penalty and spectral norm of resnet blocks, but what causes feature collapse?\n    submitted by    /u/DolantheMFWizard  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ruz0nc/d_what_causes_feature_collapse/",
          "publishedOn": "2022-01-03T10:50:11.000Z",
          "wordCount": 239,
          "title": "[D] What causes feature collapse?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ruyk5i/r_new_paper_a_relational_tsetlin_machine_with/",
          "author": null,
          "description": "​\n Relational Tsetlin Machine\n The paper introduces the first Relational #TsetlinMachine, which reasons with relations, variables, and constants. The approach is based on first-order logic and Herbrand semantics, taking the first steps toward the computing power of a universal Turing machine. The approach can take advantage of logical structures appearing in natural language, to learn rules that represent how actions and consequences are related in the real world. The outcome is a logic program of Horn clauses, bringing in a structured view of unstructured data. In closed-domain question-answering, the first-order representation produces 10× more compact knowledge bases, along with an increase in answering accuracy from 94.83% to 99.48%. The approach is further robust towards erroneous, missing, and superfluous information, distilling the aspects of a text that are important for real-world understanding. https://link.springer.com/article/10.1007/s10844-021-00682-5 #ML #AI #NLP #MachineLearning #Logic #Relational\n    submitted by    /u/olegranmo  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ruyk5i/r_new_paper_a_relational_tsetlin_machine_with/",
          "publishedOn": "2022-01-03T10:19:54.000Z",
          "wordCount": 567,
          "title": "[R] New paper: \"A relational Tsetlin machine with applications to natural language understanding\""
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ruwidq/d_how_to_deal_with_huge_categorical_data/",
          "author": null,
          "description": "I have a dataset that already contains about 55 columns and out of this, around 10 Columns or so have categorical data in it. If I were to OneHotEncode them, I will end up having a column count of more than 300. Is this something advisable? How do you people deal with such huge number of columns? I mean 300 columns is not a big deal, but I would like to know your opinion and thoughts on this.\n    submitted by    /u/CaterpillarPrevious2  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ruwidq/d_how_to_deal_with_huge_categorical_data/",
          "publishedOn": "2022-01-03T08:00:25.000Z",
          "wordCount": 516,
          "title": "[D] How to deal with huge Categorical data"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ruwchh/d_spotted_this_post_in_lesswrong_can_anyone/",
          "author": null,
          "description": "https://www.lesswrong.com/posts/rCP5iTYLtfcoC8NXd/self-organised-neural-networks-a-simple-natural-and#Roadmap\n The writing has some red flags, but it looks interesting enough. Having some trouble with my gpu drivers so I can't run it right now.\n    submitted by    /u/Their_bad_spellers  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ruwchh/d_spotted_this_post_in_lesswrong_can_anyone/",
          "publishedOn": "2022-01-03T07:49:34.000Z",
          "wordCount": 1115,
          "title": "[D] Spotted this post in LessWrong. Can anyone verify the rather fantastic claims being made here?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rutbpv/r_a_neural_network_solves_and_generates/",
          "author": null,
          "description": "submitted by    /u/shitboots  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rutbpv/r_a_neural_network_solves_and_generates/",
          "publishedOn": "2022-01-03T04:50:29.000Z",
          "wordCount": 138,
          "title": "[R] A Neural Network Solves and Generates Mathematics Problems by Program Synthesis: Calculus, Differential Equations, Linear Algebra, and More"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rut9hs/d_is_there_flowbased_method_which_treats_input/",
          "author": null,
          "description": "Hello. I am searching the researches that different size of data are generated through the flow-based network, not super-resolution task such as continuous mapping.\n I want to generate output as time-aligned scalar data, for example\n ​\n Input: noise sampling (B x T x C)\n Output: scalar data (B x T, C=1)\n ​\n with introducing the variational data augmentation technique (in vFlow, which can output high-dimensionality as concatenate noise vector for input and output both) for output.\n But there's a problem time dimension T is different for each of all data input. How can I treat this problem?\n ​\n p.s. I am very appreciate if I can read the flow-based research in NLP task.\n    submitted by    /u/RedCuraceo  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rut9hs/d_is_there_flowbased_method_which_treats_input/",
          "publishedOn": "2022-01-03T04:47:08.000Z",
          "wordCount": 279,
          "title": "[D] Is there flow-based method which treats input data as different lengths each?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rur95m/d_anyone_switched_from_vision_to_robotics/",
          "author": null,
          "description": "I’m about to finish my PhD and the whole field of robotics looks so exciting right now, especially applications like farming and recycling. Has anyone switched from more pure deep learning (vision / NLP) to robotics and how did it happen?\n Did you just get a robotics related job focusing on the vision side of things or is it key to have more experience on the robotics side before getting a job?\n Also I’m curious what’s the best location for robotics? Like how you go to Hong Kong / New York for finance, SF for software or Shenzhen for hardware.\n    submitted by    /u/temporary_ml_guy  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rur95m/d_anyone_switched_from_vision_to_robotics/",
          "publishedOn": "2022-01-03T03:03:06.000Z",
          "wordCount": 259,
          "title": "[D] Anyone switched from vision to robotics?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rur2j3/p_i_like_yolov5_but_the_code_complexity_is/",
          "author": null,
          "description": "I like YOLOv5 but the code complexity is...\n I can't deny that YOLOv5 is a practical open-source object detection pipeline. However, the pain begins when adding new features or new experimental methods. Code dependencies are hard to follow which makes the code difficult to maintain. We wanted to try various experimental methods but hate to write one-time code that is never re-used.\n So we worked on making an object detection pipeline to have a better code structure so that we could continuously improve and add new features while easy to maintain.\n https://github.com/j-marple-dev/AYolov2\n And we applied CI(Formating, Linting, Unittest) to ensure code quality with Docker support for development and inference. Our Docker supports the development environment with VIM.\n Our code design from the…",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rur2j3/p_i_like_yolov5_but_the_code_complexity_is/",
          "publishedOn": "2022-01-03T02:54:14.000Z",
          "wordCount": 595,
          "title": "[P] I like YOLOv5 but the code complexity is..."
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ruofql/d_guibased_machine_learning_applications/",
          "author": null,
          "description": "I was previously using Azure Machine Learning Studio(classic), and of course, it was discontinued last month. Any other free ML applications?\n The new Azure Machine Learning Studio isn't free, and this is a school project so I'm aiming for free and simple.\n Any suggestions? Or maybe someone else is using Studio(classic) and knows a way around this?\n    submitted by    /u/max02c  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ruofql/d_guibased_machine_learning_applications/",
          "publishedOn": "2022-01-03T00:50:33.000Z",
          "wordCount": 235,
          "title": "[D] GUI-based Machine Learning applications?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rum37y/r_a_comparison_of_the_program_synthesis/",
          "author": null,
          "description": "submitted by    /u/EducationalCicada  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rum37y/r_a_comparison_of_the_program_synthesis/",
          "publishedOn": "2022-01-02T23:04:23.000Z",
          "wordCount": 118,
          "title": "[R] A Comparison Of The Program Synthesis Performance Of GitHub Copilot And Genetic Programming"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ruja9s/d_machine_learning_wayr_what_are_you_reading_week/",
          "author": null,
          "description": "This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.\n Please try to provide some insight from your understanding and please don't post things which are present in wiki.\n Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.\n Previous weeks :\n  \n 1-10 11-20 21-30 31-40 41-50 51-60 61-70 71-80 81-90 91-100 101-110 111-120 121-130 \n  \n Week 1 Week 11 Week 21 Week 31 Week 41 Week 51 Week 61 Week 71 Week 81 Week 91 Week 101 Week 111 Week 121 \n  Week 2 Week 12 Week 22 Week 32…",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ruja9s/d_machine_learning_wayr_what_are_you_reading_week/",
          "publishedOn": "2022-01-02T21:00:05.000Z",
          "wordCount": 364,
          "title": "[D] Machine Learning - WAYR (What Are You Reading) - Week 128"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ruj3ja/d_iclr_2022_open_discussion_quality/",
          "author": null,
          "description": "First time submitting to ICLR, I'm wondering how is the open discussion on openreview.net really different from the review-rebuttal procedure used in other conferences.\n For the papers I'm reviewing, about half the reviewers reacted to the authors' responses (clarifications, modifications, additional experiments, etc.). As to my submission (5568), I run extra experiments and answered each of the concerns directly but received 0 feedback from the reviewers.\n As a reviewer, I think it doesn't matter if the rebuttal changes your mind about the quality of the submission but it's very basic manner to reply to the authors' responses. A simple \"Thank the authors for the responses but I don't think these addressed my concerns\" would work. Saying nothing only means you are uncertain if the responses make sense and you just doesn't care to figure it out. The authors spent a whole week running experiments to answer some of your questions and if you don't give **** about their responses, just keep the questions with you and don't submit your review.\n I was hoping for a different experience submitting to ICLR and then I realized the \"discussion\" is basically broken.\n    submitted by    /u/MLPRulesAll  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ruj3ja/d_iclr_2022_open_discussion_quality/",
          "publishedOn": "2022-01-02T20:51:32.000Z",
          "wordCount": 278,
          "title": "[D] ICLR 2022 Open Discussion Quality"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rud2m5/p_tensorflow_keras_implementation_of_vision/",
          "author": null,
          "description": "An Image is Worth 16x16 Words: ViT Excellent results compared to SOTA CNNs while requiring fewer computational resources to train.\n Paper : https://arxiv.org/abs/2010.11929v2 Code : https://github.com/avinash31d/paper-implementations\n    submitted by    /u/avinash31d  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rud2m5/p_tensorflow_keras_implementation_of_vision/",
          "publishedOn": "2022-01-02T16:23:44.000Z",
          "wordCount": 143,
          "title": "[P] Tensorflow / Keras implementation of Vision Transformer https://arxiv.org/abs/2010.11929v2"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rucjmx/d_simple_questions_thread/",
          "author": null,
          "description": "Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\n Thread will stay alive until next one so keep posting after the date in the title.\n Thanks to everyone for answering questions in the previous thread!\n    submitted by    /u/AutoModerator  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rucjmx/d_simple_questions_thread/",
          "publishedOn": "2022-01-02T16:00:14.000Z",
          "wordCount": 1035,
          "title": "[D] Simple Questions Thread"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ruaew1/r_learning_3d_representations_from_2d_images/",
          "author": null,
          "description": "submitted by    /u/pinter69  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ruaew1/r_learning_3d_representations_from_2d_images/",
          "publishedOn": "2022-01-02T14:13:01.000Z",
          "wordCount": 514,
          "title": "[R] Learning 3D Representations from 2D Images"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ru91o8/d_paper_explained_author_interview_player_of/",
          "author": null,
          "description": "https://youtu.be/U0mxx7AoNz0\n Special Guest: First author Martin Schmid (https://twitter.com/Lifrordi)\n Games have been used throughout research as testbeds for AI algorithms, such as reinforcement learning agents. However, different types of games usually require different solution approaches, such as AlphaZero for Go or Chess, and Counterfactual Regret Minimization (CFR) for Poker. Player of Games bridges this gap between perfect and imperfect information games and delivers a single algorithm that uses tree search over public information states, and is trained via self-play. The resulting algorithm can play Go, Chess, Poker, Scotland Yard, and many more games, as well as non-game environments.\n ​\n OUTLINE:\n 0:00 - Introduction\n 2:50 - What games can Player of Games be trained on?\n 4:00 - Tree search algorithms (AlphaZero)\n 8:00 - What is different in imperfect information games?\n 15:40 - Counterfactual Value- and Policy-Networks\n 18:50 - The Player of Games search procedure\n 28:30 - How to train the network?\n 34:40 - Experimental Results\n 47:20 - Discussion & Outlook\n ​\n Paper: https://arxiv.org/abs/2112.03178\n    submitted by    /u/ykilcher  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ru91o8/d_paper_explained_author_interview_player_of/",
          "publishedOn": "2022-01-02T12:52:58.000Z",
          "wordCount": 331,
          "title": "[D] Paper Explained & Author Interview - Player of Games: All the games, one algorithm! (Video Walkthrough)"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ru8n14/researchproject_the_top_10_aicomputer_vision/",
          "author": null,
          "description": "submitted by    /u/OnlyProggingForFun  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ru8n14/researchproject_the_top_10_aicomputer_vision/",
          "publishedOn": "2022-01-02T12:25:59.000Z",
          "wordCount": 132,
          "title": "[Research][Project] The top 10 AI/Computer Vision papers in 2021 with video demos, articles, and code for each!"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ru7k5y/d_machine_learning_research/",
          "author": null,
          "description": "Hi everyone, I've compiled the trusted sources of ideation based on top-tier conferences on Machine Learning and Deep Learning worldwide. This repository includes datasets, tasks, state-of-the-art and more.\n Repository GitHub\n https://preview.redd.it/9p1jkei8c9981.png?width=1000&format=png&auto=webp&s=906c20c58b5ee569b25848a7fbf0b41ca4caf354\n    submitted by    /u/tuanlda78202  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ru7k5y/d_machine_learning_research/",
          "publishedOn": "2022-01-02T11:08:35.000Z",
          "wordCount": 128,
          "title": "[D] Machine Learning Research"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ru7drf/p_quickdeploy_optimize_convert_and_deploy_machine/",
          "author": null,
          "description": "Hello Reddit, releasing one of my OSS projects: Quick-Deploy ..\n github: https://github.com/rodrigobaron/quick-deploy\n blog post: https://rodrigobaron.com/posts/quick-deploy\n ​\n It's in the very early stage, feel free to contribute or give a star 🙂\n    submitted by    /u/rodrigobaron  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ru7drf/p_quickdeploy_optimize_convert_and_deploy_machine/",
          "publishedOn": "2022-01-02T10:55:35.000Z",
          "wordCount": 132,
          "title": "[P] Quick-Deploy - Optimize, convert and deploy machine learning models"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ru70fv/d_raising_errors_while_using_accelerators/",
          "author": null,
          "description": "Why is it so hard to raise exceptions when ML pipeline is using a GPU?\n When for example you make a classic \"Index out of bounds\" error, in libraries like PyTorch you get some generic \"CUDA\" error and you can't see the exact error until you transfer the tensors explicitly to CPU and rerun the code.\n Do you think there is a possibility for this to improve in the future?\n Sorry if this is more CS-related question\n    submitted by    /u/Icy_Fisherman7187  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ru70fv/d_raising_errors_while_using_accelerators/",
          "publishedOn": "2022-01-02T10:27:53.000Z",
          "wordCount": 310,
          "title": "[D] Raising errors while using accelerators"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ru5xw8/d_are_nn_actually_overparametrized/",
          "author": null,
          "description": "I often read that NN or CNN are overparametrized. But, for example, resnet18 has 11M parameters while cifar10 has 50k32323=153M data points. How is that be an overparametrized network on cifar10? Or even on mnist which has 60k28*28=47M data points\n    submitted by    /u/alesaso2000  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ru5xw8/d_are_nn_actually_overparametrized/",
          "publishedOn": "2022-01-02T09:09:31.000Z",
          "wordCount": 752,
          "title": "[D] Are NN actually overparametrized?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ru06dy/d_coding_practices/",
          "author": null,
          "description": "My job is to work with ML engineers and provide them with whatever they need to experiment with/train/test/deploy ML models -- GPU infrastructure, distributed training support, etc. When I interface with their code, I almost always find it so poorly written, with little to no thought given to long-term stability or use -- for code that they 100% know is going to production.\n They're brilliant people, far smarter than me, and really good at what they do, so it's not a matter of them not being good enough. I feel (from my very limited experience, so I'm happy to be wrong) like ML engineers are incentivized to write poor code. The only metric for evaluation seems to be accuracy, loss, and all the plots that come up. In research, I understand completely, that's where the focus lies, but in industry? I've seen many models perform poorly because the code is so hard to read and refactor that big issues remained unspotted for months together. And this is especially befuddling because for a field that is completely fine with spending months to get an ROI of single digit increases in model performance metrics during the experimentation phase, they don't seem to care about anything that might go wrong in production. That just feels like a fundamental disconnect, since without the core ML stuff working perfectly, none of the other stuff (like what I do) has any value -- and even so, I'm taught to hold my code to a much higher standard than the critical stuff -- which I'm happy about since I can now write production code by default -- but it's just... weird. Like the vending machines at a nuclear power plant being better engineered than the reactor.\n Is this a common problem or is this a localized issue that I'm facing?\n    submitted by    /u/vPyDev  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ru06dy/d_coding_practices/",
          "publishedOn": "2022-01-02T03:17:33.000Z",
          "wordCount": 1955,
          "title": "[D] Coding Practices"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rtukp2/d_plug_or_integrate_a_gnn_pytorch_code_base_into/",
          "author": null,
          "description": "Does anyone have a better explanation or resources to share for plug or Integrate a Pytorch based GNN models into Pyspark or similar cluster services?\n    submitted by    /u/SpiritMaleficent21  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rtukp2/d_plug_or_integrate_a_gnn_pytorch_code_base_into/",
          "publishedOn": "2022-01-01T22:40:23.000Z",
          "wordCount": 161,
          "title": "[D] Plug or Integrate a GNN Pytorch code base into Spark Cluster"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rtsmm7/p_deepcreampy_decensoring_hentai_with_deep_neural/",
          "author": null,
          "description": "submitted by    /u/binaryfor  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rtsmm7/p_deepcreampy_decensoring_hentai_with_deep_neural/",
          "publishedOn": "2022-01-01T21:09:49.000Z",
          "wordCount": 97,
          "title": "[P] DeepCreamPy - Decensoring Hentai with Deep Neural Networks"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rtrbso/r_neuron_outputs_as_weights/",
          "author": null,
          "description": "https://stats.stackexchange.com/questions/558864/what-if-weights-of-model-is-output-of-neurons\n    submitted by    /u/Dry_Introduction_897  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rtrbso/r_neuron_outputs_as_weights/",
          "publishedOn": "2022-01-01T20:09:18.000Z",
          "wordCount": 365,
          "title": "\"[R]\" Neuron outputs as weights"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rtndgm/d_best_practices_in_machine_learning/",
          "author": null,
          "description": "This is a non-profit that promotes best practices in machine learning, specifically for responsible ML. The practices are open source too, which is cool. \n Link here: https://www.fbpml.org/the-best-practices\n I think their technical best practices seems a little stronger than the organisational ones. Thoughts?\n ** this is their LinkedIn URL: https://www.linkedin.com/company/the-foundation-for-best-practices-in-machine-learning\n    submitted by    /u/Sbu91  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rtndgm/d_best_practices_in_machine_learning/",
          "publishedOn": "2022-01-01T17:01:42.000Z",
          "wordCount": 184,
          "title": "[D] Best Practices in Machine Learning"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rtmf6v/research_my_top_10_computer_vision_papers_of_2021/",
          "author": null,
          "description": "submitted by    /u/OnlyProggingForFun  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rtmf6v/research_my_top_10_computer_vision_papers_of_2021/",
          "publishedOn": "2022-01-01T16:14:26.000Z",
          "wordCount": 114,
          "title": "[Research] My Top 10 Computer Vision papers of 2021"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rtlx0r/r_mt3_multitask_multitrack_music_transcription/",
          "author": null,
          "description": "submitted by    /u/Illustrious_Row_9971  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rtlx0r/r_mt3_multitask_multitrack_music_transcription/",
          "publishedOn": "2022-01-01T15:48:37.000Z",
          "wordCount": 536,
          "title": "[R] MT3: Multi-Task Multitrack Music Transcription"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rtd1jl/bert_goes_shopping_comparing_distributional/",
          "author": null,
          "description": "submitted by    /u/prakhar21  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rtd1jl/bert_goes_shopping_comparing_distributional/",
          "publishedOn": "2022-01-01T06:06:40.000Z",
          "wordCount": 112,
          "title": "BERT Goes Shopping: Comparing Distributional Models for Product Representations (Paper Walkthrough) [D]"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rt1vfy/p_play_around_with_stylegan2_in_your_browser/",
          "author": null,
          "description": "I built a little page to run and manipulate StyleGAN2 in the browser.\n https://ziyadedher.com/faces\n It was pretty fun learning about ONNX and how to port GANs to web. You can play around with the random seeds and also distort the intermediate latents to produce some really wacky results. You can check out a GIF on Twitter.\n Let me know if you come up with anything cool!\n    submitted by    /u/Cold999  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rt1vfy/p_play_around_with_stylegan2_in_your_browser/",
          "publishedOn": "2021-12-31T19:55:39.000Z",
          "wordCount": 276,
          "title": "[P] Play around with StyleGAN2 in your browser"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rsstqr/p_top_arxiv_machine_learning_papers_in_2021/",
          "author": null,
          "description": "With 2021 almost in the books (there are still a couple of hours to go at the time of this writing), here are the top machine learning papers per month from the arXiv pre-print archive as picked up by metacurate.io in 2021.\n January\n  \nCan a Fruit Fly Learn Word Embeddings?\n Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\n Muppet: Massive Multi-task Representations with Pre-Finetuning\n  \nFebruary\n  \nHow to represent part-whole hierarchies in a neural network\n Patterns, predictions, and actions: A story about machine learning\n Fast Graph Learning with Unique Optimal Solutions\n  \nMarch\n  \nFast and flexible: Human program induction in abstract reasoning tasks\n Learning to Resize Images for Computer Vision Tasks\n The Prevalence of Code Smells in Mac…",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rsstqr/p_top_arxiv_machine_learning_papers_in_2021/",
          "publishedOn": "2021-12-31T12:24:05.000Z",
          "wordCount": 613,
          "title": "[P] Top arXiv Machine Learning papers in 2021 according to metacurate.io"
        }
      ]
    },
    {
      "title": "ML in Production",
      "feedUrl": "https://mlinproduction.com/feed",
      "siteUrl": "https://mlinproduction.com",
      "articles": []
    },
    {
      "title": "Jay Alammar",
      "feedUrl": "https://jalammar.github.io/feed.xml",
      "siteUrl": "http://jalammar.github.io/",
      "articles": [
        {
          "id": "http://jalammar.github.io/illustrated-retrieval-transformer/",
          "author": null,
          "description": "Discussion: Discussion Thread for comments, corrections, or any feedback. Summary: The latest batch of language models can be much smaller yet achieve GPT-3 like performance by being able to query a database or search the web for information. A key indication is that building larger and larger models is not the only way to improve performance. The last few years saw the rise of Large Language Models (LLMs) – machine learning models that rapidly improve how machines process and generate language. Some of the highlights since 2017 include: The original Transformer breaks previous performance records for machine translation. BERT popularizes the pre-training then finetuning process, as well as Transformer-based contextualized word embeddings. It then rapidly starts to power Google Search and Bing Search. GPT-2 demonstrates the machine’s ability to write as well as humans do. First T5, then T0 push the boundaries of transfer learning (training a model on one task, and then having it do well on other adjacent tasks) and posing a lot of different tasks as text-to-text tasks. GPT-3 showed that massive scaling of generative models can lead to shocking emergent applications (the industry continues to train larger models like Gopher, MT-NLG…etc). For a while, it seemed like scaling larger and larger models is the main way to improve performance. Recent developments in the field, like DeepMind’s RETRO Transformer and OpenAI’s WebGPT, reverse this trend by showing that smaller generative language models can perform on par with massive models if we augment them with a way to search/query for information. This article breaks down DeepMind’s RETRO (Retrieval-Enhanced TRansfOrmer) and how it works. The model performs on par with GPT-3 despite being 4% its size (7.5 billion parameters vs. 185 billion for GPT-3 Da Vinci). RETRO incorporates information retrieved from a database to free its parameters from being an expensive store of facts and world knowledge. RETRO was presented in the paper Improving Language Models by Retrieving from Trillions of Tokens. It continues and builds on a wide variety of retrieval work in the research community. This article explains the model and not what is especially novel about it.",
          "link": "http://jalammar.github.io/illustrated-retrieval-transformer/",
          "publishedOn": "2022-01-03T00:00:00.000Z",
          "wordCount": 1435,
          "title": "The Illustrated Retrieval Transformer"
        }
      ]
    },
    {
      "title": "Distill",
      "feedUrl": "https://distill.pub/rss.xml",
      "siteUrl": "https://distill.pub",
      "articles": []
    },
    {
      "title": "inFERENCe",
      "feedUrl": "https://www.inference.vc/rss",
      "siteUrl": "https://www.inference.vc/",
      "articles": []
    },
    {
      "title": "AI Trends",
      "feedUrl": "https://www.aitrends.com/feed",
      "siteUrl": "https://www.aitrends.com",
      "articles": []
    },
    {
      "title": "AI Weirdness",
      "feedUrl": "https://aiweirdness.com/rss",
      "siteUrl": "https://www.aiweirdness.com/",
      "articles": [
        {
          "id": "61d6704110e963003be15509",
          "author": "Janelle Shane",
          "description": "I remember when my hometown got one of these giant wooden playgrounds. It must have been in the early 90s and a kid could get lost for hours in there.\n\nOr injured or full of splinters or chromated copper arsenate I guess, which is why you don't see",
          "link": "https://www.aiweirdness.com/remember-those-wooden-playgrounds/",
          "publishedOn": "2022-01-07T16:09:14.000Z",
          "wordCount": 908,
          "title": "Remember those wooden playgrounds? AI doesn't."
        },
        {
          "id": "61d7c39610e963003be156a0",
          "author": "Janelle Shane",
          "description": "AI Weirdness: the strange side of machine learning",
          "link": "https://www.aiweirdness.com/bonus-these-playgrounds-might-not-be-exactly-safe/",
          "publishedOn": "2022-01-07T16:09:02.000Z",
          "wordCount": 430,
          "title": "Bonus: these playgrounds might not be exactly safe"
        },
        {
          "id": "61c0e2e310e963003be0848f",
          "author": "Janelle Shane",
          "description": "This month I'm beginning 2022 as the first Futurist in Residence at the Smithsonian Arts and Industries Building. \nIt's weird to think of myself as a futurist. I write a lot about the algorithms we're calling artificial intelligence (AI), but rather than deal with",
          "link": "https://www.aiweirdness.com/new-years-resolutions-generated-by-ai/",
          "publishedOn": "2021-12-30T14:00:00.000Z",
          "wordCount": 1861,
          "title": "New Years Resolutions generated by AI"
        },
        {
          "id": "61ca6c1b10e963003be0ed03",
          "author": "Janelle Shane",
          "description": "AI Weirdness: the strange side of machine learning",
          "link": "https://www.aiweirdness.com/bonus-more-new-years-resolutions-to-consider/",
          "publishedOn": "2021-12-30T13:58:00.000Z",
          "wordCount": 421,
          "title": "Bonus: more new year's resolutions to consider"
        },
        {
          "id": "61bff61010e963003be0846a",
          "author": "Janelle Shane",
          "description": "When you think about it, Christmas can get pretty weird. \nThere's the classic Christmas story of the Bible, and then there are all these extra entities that aren't in the book but which become somehow part of Christmas. And some of them are quite unsettling. There&",
          "link": "https://www.aiweirdness.com/christmas-entities/",
          "publishedOn": "2021-12-22T15:57:26.000Z",
          "wordCount": 597,
          "title": "Christmas entities"
        },
        {
          "id": "61c2a8d510e963003be08554",
          "author": "Janelle Shane",
          "description": "AI Weirdness: the strange side of machine learning",
          "link": "https://www.aiweirdness.com/the-awesome-lightning-power-of-blitzen/",
          "publishedOn": "2021-12-22T15:55:39.000Z",
          "wordCount": 428,
          "title": "Bonus: The awesome lightning power of Blitzen, son of Donder"
        }
      ]
    },
    {
      "title": "The Berkeley Artificial Intelligence Research Blog",
      "feedUrl": "https://bair.berkeley.edu/blog/feed.xml",
      "siteUrl": "http://bair.berkeley.edu/blog/",
      "articles": []
    },
    {
      "title": "Becoming Human: Artificial Intelligence Magazine - Medium",
      "feedUrl": "https://becominghuman.ai/feed",
      "siteUrl": "https://becominghuman.ai?source=rss----5e5bef33608a---4",
      "articles": [
        {
          "id": "https://medium.com/p/bbcb74fecd13",
          "author": "Jeena KK",
          "description": "There had never been a time like this before in the history of Artificial Intelligence (AI) where AI was this democratic and celebrated…",
          "link": "https://becominghuman.ai/do-stochastic-parrots-understand-what-they-recite-bbcb74fecd13?source=rss----5e5bef33608a---4",
          "publishedOn": "2022-01-13T16:32:18.000Z",
          "wordCount": 1932,
          "title": "Do stochastic parrots understand what they recite?"
        },
        {
          "id": "https://medium.com/p/fd74dd649ca6",
          "author": "Leo Ertuna",
          "description": "A very practical guide to build your first object detection model, jump from rough idea to proof-of-concept in one day",
          "link": "https://becominghuman.ai/jump-start-in-object-detection-with-detectron2-license-plate-detection-fd74dd649ca6?source=rss----5e5bef33608a---4",
          "publishedOn": "2022-01-13T16:32:09.000Z",
          "wordCount": 2980,
          "title": "Jump start in object detection with Detectron2 (license plate detection)"
        },
        {
          "id": "https://medium.com/p/ddbfacbfb594",
          "author": "ITRex Group",
          "description": "The pandemic has uncovered and amplified the struggles the healthcare sector is facing. From huge doctor workloads to unsatisfied patients…",
          "link": "https://becominghuman.ai/smart-hospitals-the-market-overview-trends-and-considerations-ddbfacbfb594?source=rss----5e5bef33608a---4",
          "publishedOn": "2022-01-12T16:57:38.000Z",
          "wordCount": 3019,
          "title": "Smart hospitals: the market overview, trends, and considerations"
        },
        {
          "id": "https://medium.com/p/fe87f2821f83",
          "author": "RAVI SHEKHAR TIWARI",
          "description": "In Part 5.0 of the Transfer Learning series we have discussed about ResNet pre-trained model in depth so in this series we will implement…",
          "link": "https://becominghuman.ai/transfer-learning-part-5-2-implementing-resnet-in-pytorch-fe87f2821f83?source=rss----5e5bef33608a---4",
          "publishedOn": "2022-01-12T16:57:33.000Z",
          "wordCount": 10511,
          "title": "Transfer Learning — Part — 5.2!! Implementing ResNet in PyTorch"
        },
        {
          "id": "https://medium.com/p/5672f72aca65",
          "author": "Muppala Sunny Chowdhary",
          "description": "Sector: Agriculture, Project: “InteliCrop: An Ensemble Model to Predict Crop using Machine Learning Algorithms”",
          "link": "https://becominghuman.ai/how-to-create-a-recommendation-model-based-on-dataset-available-5672f72aca65?source=rss----5e5bef33608a---4",
          "publishedOn": "2022-01-12T16:57:31.000Z",
          "wordCount": 2578,
          "title": "How to create a recommendation model with help of Machine Learning Techniques and model Ensembling"
        },
        {
          "id": "https://medium.com/p/b9d7b82f40fa",
          "author": "Irfan Ak",
          "description": "According to ethical AI statistics, 90% of organizations who have adopted ethical artificial intelligence said they know about at least one…",
          "link": "https://becominghuman.ai/7-key-questions-you-need-to-answer-before-implementing-ethical-ai-b9d7b82f40fa?source=rss----5e5bef33608a---4",
          "publishedOn": "2022-01-11T17:12:07.000Z",
          "wordCount": 1281,
          "title": "7 Key Questions You Need To Answer Before Implementing Ethical AI"
        },
        {
          "id": "https://medium.com/p/3d09964afd9d",
          "author": "Monodeep .J.Mukherjee",
          "description": "Introduction",
          "link": "https://becominghuman.ai/google-flu-trends-explained-3d09964afd9d?source=rss----5e5bef33608a---4",
          "publishedOn": "2022-01-11T17:10:56.000Z",
          "wordCount": 1162,
          "title": "Google Flu Trends Explained"
        },
        {
          "id": "https://medium.com/p/fa978e348590",
          "author": "Stephanie Marie Delgado (SMD Writing)",
          "description": "SEO = Ever-Changing with a Fast-Paced Agenda!\nContinue reading on Becoming Human: Artificial Intelligence Magazine »",
          "link": "https://becominghuman.ai/2022-seo-a-robust-future-fa978e348590?source=rss----5e5bef33608a---4",
          "publishedOn": "2022-01-11T17:10:54.000Z",
          "wordCount": 1387,
          "title": "2022 SEO: A Robust Future!"
        },
        {
          "id": "https://medium.com/p/c3fe04cc4da9",
          "author": "Sophia Brooke",
          "description": "Machine Learning has diverse applications in business, but it is probably the most effective for forecasting the future demand of a…",
          "link": "https://becominghuman.ai/ml-based-energy-consumption-forecasting-c3fe04cc4da9?source=rss----5e5bef33608a---4",
          "publishedOn": "2022-01-10T15:41:35.000Z",
          "wordCount": 1170,
          "title": "ML-Based Energy Consumption Forecasting"
        },
        {
          "id": "https://medium.com/p/94d2453f171",
          "author": "Jacques Patricks",
          "description": "In real life, you often have to deal with things you don’t completely understand. For instance, you drive a car, not knowing how the engine…",
          "link": "https://becominghuman.ai/gradient-based-optimization-demistified-94d2453f171?source=rss----5e5bef33608a---4",
          "publishedOn": "2022-01-10T15:41:31.000Z",
          "wordCount": 1187,
          "title": "Gradient-Based Optimization Demistified"
        },
        {
          "id": "https://medium.com/p/5946ab4c0eeb",
          "author": "ITRex Group",
          "description": "Health data is notoriously difficult to share. Due to its sensitive nature, it requires more privacy and security than any other data…",
          "link": "https://becominghuman.ai/how-to-achieve-data-interoperability-in-healthcare-tips-from-itrex-5946ab4c0eeb?source=rss----5e5bef33608a---4",
          "publishedOn": "2022-01-10T15:41:31.000Z",
          "wordCount": 3155,
          "title": "How to achieve data interoperability in healthcare: tips from ITRex"
        },
        {
          "id": "https://medium.com/p/90a40f1a1d97",
          "author": "Rayan Potter",
          "description": "In the last decade, computer vision technology has advanced substantially, thanks to advances in AI and deep learning methodologies. It is…",
          "link": "https://becominghuman.ai/use-cases-of-image-segmentation-using-deep-learning-90a40f1a1d97?source=rss----5e5bef33608a---4",
          "publishedOn": "2021-12-29T16:15:29.000Z",
          "wordCount": 1127,
          "title": "Use cases of Image Segmentation Using Deep Learning"
        },
        {
          "id": "https://medium.com/p/2b746df1bf3",
          "author": "MobiDev",
          "description": "With 2020 well behind us, COVID-19’s presence continues to linger around the world. Of all the industries that have been forever changed by…",
          "link": "https://becominghuman.ai/healthcare-technology-trends-and-digital-innovations-in-2022-2b746df1bf3?source=rss----5e5bef33608a---4",
          "publishedOn": "2021-12-29T16:15:28.000Z",
          "wordCount": 3339,
          "title": "Healthcare Technology Trends and Digital Innovations in 2022"
        },
        {
          "id": "https://medium.com/p/562c0ba62c14",
          "author": "Dnyanesh Walwadkar",
          "description": "Biological inspiration of Neural Networks",
          "link": "https://becominghuman.ai/keys-of-deep-learning-activation-functions-562c0ba62c14?source=rss----5e5bef33608a---4",
          "publishedOn": "2021-12-28T16:21:30.000Z",
          "wordCount": 4012,
          "title": "Keys of Deep Learning : Activation Functions"
        },
        {
          "id": "https://medium.com/p/13727057d0d9",
          "author": "Ashok Sharma",
          "description": "The world is in an exciting phase of artificial intelligence that is slowly taking over our daily lives. Alexa, Siri is replacing person",
          "link": "https://becominghuman.ai/how-artificial-intelligence-is-changing-the-payment-gateway-industry-13727057d0d9?source=rss----5e5bef33608a---4",
          "publishedOn": "2021-12-27T19:39:04.000Z",
          "wordCount": 1436,
          "title": "How Artificial Intelligence is Changing the Payment Gateway Industry"
        },
        {
          "id": "https://medium.com/p/40db4c79c4d2",
          "author": "javinpaul",
          "description": "My favorite online courses, projects, and Computer Vision certification for beginners to learn Computer vision and OpenCV in 2022",
          "link": "https://becominghuman.ai/6-best-online-courses-to-learn-computer-vision-and-opencv-for-beginners-40db4c79c4d2?source=rss----5e5bef33608a---4",
          "publishedOn": "2021-12-27T19:38:43.000Z",
          "wordCount": 2091,
          "title": "6 Best Online Courses to learn Computer Vision and OpenCV for Beginners"
        },
        {
          "id": "https://medium.com/p/611697fb5e1",
          "author": "BENEVOLENCE TECHNOLOGIES",
          "description": "Is there any track of the number of notifications and alerts you receive each day? As customers, we receive numerous notifications from our…",
          "link": "https://becominghuman.ai/add-spark-to-your-customer-notifications-with-a-bentechs-modern-ccm-611697fb5e1?source=rss----5e5bef33608a---4",
          "publishedOn": "2021-12-22T16:41:47.000Z",
          "wordCount": 944,
          "title": "Add Spark to Your Customer Notifications with a Bentech’s Modern CCM"
        },
        {
          "id": "https://medium.com/p/584b2b9aa45d",
          "author": "Divyesh Dharaiya",
          "description": "Normal routines of smartphone shops, automobile dealerships, or dining experiences, many of these travels have been transformed into…",
          "link": "https://becominghuman.ai/strategies-to-reimagine-consumers-digital-trip-in-a-new-age-584b2b9aa45d?source=rss----5e5bef33608a---4",
          "publishedOn": "2021-12-22T16:41:45.000Z",
          "wordCount": 1312,
          "title": "Strategies to Reimagine Consumer’s Digital Trip in a New Age"
        },
        {
          "id": "https://medium.com/p/455afbc28657",
          "author": "RAVI SHEKHAR TIWARI",
          "description": "In Part 5.0 of the Transfer Learning series we have discussed about ResNet pre-trained model in depth so in this series we will implement…",
          "link": "https://becominghuman.ai/transfer-learning-part-5-1-implementing-resnet-in-keras-455afbc28657?source=rss----5e5bef33608a---4",
          "publishedOn": "2021-12-21T16:22:11.000Z",
          "wordCount": 33940,
          "title": "Transfer Learning — Part — 5.1!! Implementing ResNet in Keras"
        },
        {
          "id": "https://medium.com/p/37788ad01577",
          "author": "James Montantes",
          "description": "Optimizing Our Supply Chain Using Artificial Intelligence",
          "link": "https://becominghuman.ai/5-ways-to-use-ai-for-supply-chain-management-37788ad01577?source=rss----5e5bef33608a---4",
          "publishedOn": "2021-12-21T16:22:09.000Z",
          "wordCount": 2188,
          "title": "5 Ways To Use AI For Supply Chain Management"
        },
        {
          "id": "https://medium.com/p/cb0d414a9e2b",
          "author": "Joel Prabhod",
          "description": "What are GENERATIVE ADVERESIAL NETWORKS and what are GANs used for?",
          "link": "https://becominghuman.ai/why-are-generative-adversarial-networks-gans-so-famous-and-how-will-gans-be-in-the-future-cb0d414a9e2b?source=rss----5e5bef33608a---4",
          "publishedOn": "2021-12-20T16:48:56.000Z",
          "wordCount": 2032,
          "title": "Why Are Generative Adversarial Networks(GANs) So Famous And How Will GANs In The Future Be?"
        }
      ]
    },
    {
      "title": "MIT News - Artificial intelligence",
      "feedUrl": "http://news.mit.edu/rss/topic/artificial-intelligence2",
      "siteUrl": "https://news.mit.edu/rss/topic/artificial-intelligence2",
      "articles": [
        {
          "id": "https://news.mit.edu/2022/qa-adedolapo-adedokun-computer-technology-ireland-all-that-jazz-0111",
          "author": "Jane Halpern | Department of Electrical Engineering and Computer Science",
          "description": "MIT EECS student and Mitchell Scholar hopes to play music in Dublin while working on his MS in intelligent systems.",
          "link": "https://news.mit.edu/2022/qa-adedolapo-adedokun-computer-technology-ireland-all-that-jazz-0111",
          "publishedOn": "2022-01-11T20:03:00.000Z",
          "wordCount": 2395,
          "title": "Q&A: Dolapo Adedokun on computer technology, Ireland, and all that jazz"
        },
        {
          "id": "https://news.mit.edu/2022/promise-pitfalls-artificial-intelligence-tedxmit-0111",
          "author": "Steve Nadis | MIT CSAIL",
          "description": "MIT scientists discuss the future of AI with applications across many sectors, as a tool that can be both beneficial and harmful.",
          "link": "https://news.mit.edu/2022/promise-pitfalls-artificial-intelligence-tedxmit-0111",
          "publishedOn": "2022-01-11T19:45:00.000Z",
          "wordCount": 2051,
          "title": "The promise and pitfalls of artificial intelligence explored at TEDxMIT event"
        },
        {
          "id": "https://news.mit.edu/2022/physics-and-machine-learning-black-box-0110",
          "author": "Mary Beth Gallagher | Department of Mechanical Engineering",
          "description": "In 2.C161, George Barbastathis demonstrates how mechanical engineers can use their knowledge of physical systems to keep algorithms in check and develop more accurate predictions.",
          "link": "https://news.mit.edu/2022/physics-and-machine-learning-black-box-0110",
          "publishedOn": "2022-01-10T20:15:00.000Z",
          "wordCount": 1702,
          "title": "Physics and the machine-learning “black box”"
        },
        {
          "id": "https://news.mit.edu/2022/meet-2021-22-accenture-fellows-0104",
          "author": "Emma Foehringer Merchant | School of Engineering",
          "description": "The 2021-22 Accenture Fellows are bolstering research and igniting ideas to help transform global business.",
          "link": "https://news.mit.edu/2022/meet-2021-22-accenture-fellows-0104",
          "publishedOn": "2022-01-04T18:40:00.000Z",
          "wordCount": 1378,
          "title": "Meet the 2021-22 Accenture Fellows"
        },
        {
          "id": "https://news.mit.edu/2021/perfecting-pitch-perception-1217",
          "author": "Jennifer Michalowski | McGovern Institute for Brain Research",
          "description": "Computational modeling shows that both our ears and our environment influence how we hear.",
          "link": "https://news.mit.edu/2021/perfecting-pitch-perception-1217",
          "publishedOn": "2021-12-17T21:30:00.000Z",
          "wordCount": 1619,
          "title": "Perfecting pitch perception"
        },
        {
          "id": "https://news.mit.edu/2021/ai-generated-characters-for-good-1216",
          "author": "Becky Ham | MIT Media Lab",
          "description": "Researchers encourage positive use cases of AI-generated characters for education and well-being.",
          "link": "https://news.mit.edu/2021/ai-generated-characters-for-good-1216",
          "publishedOn": "2021-12-16T21:45:00.000Z",
          "wordCount": 1718,
          "title": "Characters for good, created by artificial intelligence"
        },
        {
          "id": "https://news.mit.edu/2021/qa-cathy-wu-developing-algorithms-safely-integrate-robots-our-world-1216",
          "author": "Kim Martineau | MIT Schwarzman College of Computing",
          "description": "Assistant professor of civil engineering describes her career in robotics as well as challenges and promises of human-robot interactions.",
          "link": "https://news.mit.edu/2021/qa-cathy-wu-developing-algorithms-safely-integrate-robots-our-world-1216",
          "publishedOn": "2021-12-16T21:25:00.000Z",
          "wordCount": 2040,
          "title": "Q&A: Cathy Wu on developing algorithms to safely integrate robots into our world"
        },
        {
          "id": "https://news.mit.edu/2021/nonsense-can-make-sense-machine-learning-models-1215",
          "author": "Rachel Gordon | MIT CSAIL",
          "description": "Deep-learning methods confidently recognize images that are nonsense, a potential problem for medical and autonomous-driving decisions.",
          "link": "https://news.mit.edu/2021/nonsense-can-make-sense-machine-learning-models-1215",
          "publishedOn": "2021-12-15T21:55:00.000Z",
          "wordCount": 1510,
          "title": "Nonsense can make sense to machine-learning models"
        }
      ]
    },
    {
      "title": "The Official NVIDIA Blog",
      "feedUrl": "http://feeds.feedburner.com/nvidiablog",
      "siteUrl": "https://blogs.nvidia.com",
      "articles": [
        {
          "id": "https://blogs.nvidia.com/?p=55065",
          "author": "Azita Martin",
          "description": "At the National Retail Federation’s annual trade show, conversations tend to touch on recurring themes: “Will we be able to stock must-have products for next Christmas?,” “What incentives can I offer to loyal workers?” and “What happens to my margins if Susie Consumer purchases three of the same dresses online and returns two?” The $26 Read article >\nThe post How Retailers Meet Tough Challenges Using NVIDIA AI  appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2022/01/13/how-retailers-use-ai/",
          "publishedOn": "2022-01-13T16:00:45.000Z",
          "wordCount": 1155,
          "title": "How Retailers Meet Tough Challenges Using NVIDIA AI"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54962",
          "author": "Brian Caulfield",
          "description": "Addressing a growing labor crisis among quick-service restaurants, startup Vistry is harnessing AI to automate the process of taking orders. The company will share its story at the NRF Big Show, the annual industry gathering of the National Retail Federation in New York, starting Jan. 16. “They’re closing restaurants because there is not enough labor,” Read article >\nThe post AI Startup to Take a Bite Out of Fast-Food Labor Crunch appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2022/01/13/ai-startup-fast-food-labor-crunch/",
          "publishedOn": "2022-01-13T15:01:11.000Z",
          "wordCount": 838,
          "title": "AI Startup to Take a Bite Out of Fast-Food Labor Crunch"
        },
        {
          "id": "https://blogs.nvidia.com/?p=55042",
          "author": "GeForce NOW Community",
          "description": "Starting next week, Fortnite on GeForce NOW will launch in a limited-time closed beta for mobile, all streamed through the Safari web browser on iOS and the GeForce NOW Android app. The beta is open for registration for all GeForce NOW members, and will help test our server capacity, graphics delivery and new touch controls Read article >\nThe post GFN Thursday: ‘Fortnite’ Comes to iOS Safari and Android Through NVIDIA GeForce NOW via Closed Beta appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2022/01/13/geforce-now-fortnite-closed-beta/",
          "publishedOn": "2022-01-13T14:00:57.000Z",
          "wordCount": 788,
          "title": "GFN Thursday: ‘Fortnite’ Comes to iOS Safari and Android Through NVIDIA GeForce NOW via Closed Beta"
        },
        {
          "id": "https://blogs.nvidia.com/?p=55033",
          "author": "Kimberly Powell",
          "description": "Cutting down the time needed to sequence and analyze a patient’s whole genome from days to hours isn’t just about clinical efficiency — it can save lives. By accelerating every step of this process — from collecting a blood sample to sequencing the whole genome to identifying variants linked to diseases — a research team Read article >\nThe post World Record-Setting DNA Sequencing Technique Helps Clinicians Rapidly Diagnose Critical Care Patients appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2022/01/12/world-record-genome-sequencing-parabricks/",
          "publishedOn": "2022-01-12T22:01:31.000Z",
          "wordCount": 1084,
          "title": "World Record-Setting DNA Sequencing Technique Helps Clinicians Rapidly Diagnose Critical Care Patients"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54590",
          "author": "Brian Choi",
          "description": "SHIELD Software Experience Upgrade 9.0 is rolling out to all NVIDIA SHIELD TVs, delivering the Android 11 operating system and more. An updated Gboard — the Google Keyboard — allows people to use their voices and the Google Assistant to discover content in all search boxes. Additional permissions let users customize privacy across apps, including Read article >\nThe post Elevated Entertainment: SHIELD Experience 9.0 Upgrade Rolling Out Now appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2022/01/12/shield-experience-9/",
          "publishedOn": "2022-01-12T14:00:32.000Z",
          "wordCount": 947,
          "title": "Elevated Entertainment: SHIELD Experience 9.0 Upgrade Rolling Out Now"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54982",
          "author": "Brian Caulfield",
          "description": "NVIDIA is America’s best place to work, according to Glassdoor’s just-issued list of best employers for 2022. Amid a global pandemic that has affected every workplace, NVIDIA was ranked No. 1 on Glassdoor’s 14th annual Best Places to Work list for large US companies. The award is based on anonymous employee feedback covering thousands of Read article >\nThe post NVIDIA Named America’s Best Place to Work on Latest Glassdoor List appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2022/01/11/nvidia-best-place-work/",
          "publishedOn": "2022-01-11T18:31:15.000Z",
          "wordCount": 696,
          "title": "NVIDIA Named America’s Best Place to Work on Latest Glassdoor List"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54928",
          "author": "Charlie Boyle",
          "description": "Bright Computing, a leader in software for managing high performance computing systems used by more than 700 organizations worldwide, is now part of NVIDIA. Companies in healthcare, financial services, manufacturing and other markets use its tool to set up and run HPC clusters, groups of servers linked by high-speed networks into a single unit. Its Read article >\nThe post Leading HPC Software Company Bright Computing Joins NVIDIA appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2022/01/10/bright-computing-hpc/",
          "publishedOn": "2022-01-10T21:30:41.000Z",
          "wordCount": 688,
          "title": "Leading HPC Software Company Bright Computing Joins NVIDIA"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54561",
          "author": "Tony Kontzer",
          "description": "To make the best portfolio decisions, banks need to accurately calculate values of their trades, while factoring in uncertain external risks. This requires high-performance computing power to run complex derivatives models — which find fair prices for financial contracts — as close to real time as possible. “You don’t want to trade today on yesterday’s Read article >\nThe post AI Startup Speeds Up Derivative Models for Bank of Montreal appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2022/01/10/riskfuel-derivative-models-bank-of-montreal/",
          "publishedOn": "2022-01-10T18:00:39.000Z",
          "wordCount": 967,
          "title": "AI Startup Speeds Up Derivative Models for Bank of Montreal"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54945",
          "author": "Nicola Sessions",
          "description": "WFH was likely one the most-used acronyms of the past year, with more businesses looking to enhance their employees’ remote experiences than ever. Creative production studio Taylor James found a cloud-based solution to maintain efficiency and productivity — even while working remotely — with NVIDIA RTX Virtual Workstations on AWS. With locations in New York, Read article >\nThe post Cloud Control: Production Studio Taylor James Elevates Remote Workflows With NVIDIA Technology appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2022/01/10/taylor-james-remote-work-vgpu/",
          "publishedOn": "2022-01-10T18:00:26.000Z",
          "wordCount": 864,
          "title": "Cloud Control: Production Studio Taylor James Elevates Remote Workflows With NVIDIA Technology"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54896",
          "author": "Scott Martin",
          "description": "Bill Kish founded Ruckus Wireless two decades ago to make Wi-Fi networking easier. Now, he’s doing the same for computer vision in industrial AI. In 2015, Kish started Cogniac, a company that offers a self-service computer vision platform and development support. Like in the early days of Wi-Fi deployment, the rollout of AI is challenging, Read article >\nThe post Scooping up Customers: Startup’s No-Code AI Gains Traction for Industrial Inspection appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2022/01/07/startups-ai-gains-traction-for-industrial-inspection/",
          "publishedOn": "2022-01-07T17:00:06.000Z",
          "wordCount": 920,
          "title": "Scooping up Customers: Startup’s No-Code AI Gains Traction for Industrial Inspection"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54911",
          "author": "GeForce NOW Community",
          "description": "GeForce NOW is charging into the new year at full force. This GFN Thursday comes with the news that Genshin Impact, the popular open-world action role-playing game, will be coming to the cloud this year, arriving in a limited beta. Plus, this year’s CES announcements were packed with news for GeForce NOW. Battlefield 4: Premium Read article >\nThe post Prepare for Genshin Impact, Coming to GeForce NOW in Limited Beta appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2022/01/06/geforce-now-thursday-january-6/",
          "publishedOn": "2022-01-06T14:00:58.000Z",
          "wordCount": 1086,
          "title": "Prepare for Genshin Impact, Coming to GeForce NOW in Limited Beta"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54929",
          "author": "Norm Marks",
          "description": "Autonomous vehicles are born in the data center, which is why NVIDIA and Deloitte are delivering a strong foundation for developers to deploy robust self-driving technology. At CES this week, the companies detailed their collaboration, which is aimed at easing the biggest pain points in AV development. Deloitte, a leading global consulting firm, is pairing Read article >\nThe post Teamwork Makes AVs Work: NVIDIA and Deloitte Deliver Turnkey Solutions for AV Developers appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2022/01/05/deloitte-turnkey-solutions-av-developers/",
          "publishedOn": "2022-01-05T22:33:23.000Z",
          "wordCount": 1079,
          "title": "Teamwork Makes AVs Work: NVIDIA and Deloitte Deliver Turnkey Solutions for AV Developers"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54743",
          "author": "Angie Lee",
          "description": "What started as Nick Walton’s college hackathon project grew into AI Dungeon, a popular text adventure game with over 1.5 million users. Walton is the co-founder and CEO of Latitude, a Utah-based startup that uses AI to create unique gaming storylines. He spoke with NVIDIA AI Podcast host Noah Kravitz about how natural language processing Read article >\nThe post ‘AI Dungeon’ Creator Nick Walton Uses AI to Generate Infinite Gaming Storylines appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2022/01/05/nick-walton-ai-podcast/",
          "publishedOn": "2022-01-05T14:00:44.000Z",
          "wordCount": 779,
          "title": "‘AI Dungeon’ Creator Nick Walton Uses AI to Generate Infinite Gaming Storylines"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54794",
          "author": "Shri Sundaram",
          "description": "Manufacturing and fulfillment centers are profoundly complex. Whenever new earbuds or socks land at your doorstep in hours or a vehicle rolls off an assembly line, a maze of magic happens with AI-driven logistics. Massive facilities like these are constantly in flux. Robots travel miles of aisles to roll up millions of products to assist Read article >\nThe post NVIDIA Builds Isaac AMR Platform to Aid $9 Trillion Logistics Industry appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2022/01/04/isaac-amr-platform/",
          "publishedOn": "2022-01-04T17:00:40.000Z",
          "wordCount": 1258,
          "title": "NVIDIA Builds Isaac AMR Platform to Aid $9 Trillion Logistics Industry"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54745",
          "author": "Rick Merritt",
          "description": "Putting the power of graphics and AI at the fingertips of more users than ever, NVIDIA announced today new laptops and autonomous vehicles using GeForce RTX and NVIDIA AI platforms and expanded reach for GeForce NOW cloud gaming across Samsung TVs and the AT&T network. A virtual address prior to CES showed next-gen games, new Read article >\nThe post Gamers, Creators, Drivers Feel GeForce RTX, NVIDIA AI Everywhere appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2022/01/04/ces-rtx-3080ti-laptops-gfn-att-tusimple/",
          "publishedOn": "2022-01-04T17:00:11.000Z",
          "wordCount": 1845,
          "title": "Gamers, Creators, Drivers Feel GeForce RTX, NVIDIA AI Everywhere"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54854",
          "author": "Stanley Tack",
          "description": "As art evolves and artists’ abilities grow, so must their creative tools. NVIDIA Canvas, the free beta app and part of the NVIDIA Studio suite of creative apps and tools, has brought the real-time painting tool GauGAN to anyone with an NVIDIA RTX GPU. Artists use advanced AI to quickly turn simple brushstrokes into realistic Read article >\nThe post NVIDIA Canvas Updated With New AI Model Delivering 4x Resolution and More Materials appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2022/01/04/studio-canvas-update-gaugan2-ces/",
          "publishedOn": "2022-01-04T16:53:33.000Z",
          "wordCount": 711,
          "title": "NVIDIA Canvas Updated With New AI Model Delivering 4x Resolution and More Materials"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54838",
          "author": "Stanley Tack",
          "description": "We’re at the dawn of the next digital frontier. Creativity is fueling new developments in design, innovation and virtual worlds. For the creators driving this future, we’ve built NVIDIA Studio, a fully accelerated platform with high-performance GPUs as the heartbeat for laptops and desktops. This hardware is paired with exclusive NVIDIA RTX-accelerated software optimizations in Read article >\nThe post Groundbreaking Updates to NVIDIA Studio Power the 3D Virtual Worlds of Tomorrow, Today appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2022/01/04/studio-laptops-omniverse-canvas/",
          "publishedOn": "2022-01-04T16:53:27.000Z",
          "wordCount": 1192,
          "title": "Groundbreaking Updates to NVIDIA Studio Power the 3D Virtual Worlds of Tomorrow, Today"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54731",
          "author": "Richard Kerris",
          "description": "Designed to be the foundation that connects virtual worlds, NVIDIA Omniverse is now available to millions of individual NVIDIA Studio creators using GeForce RTX and NVIDIA RTX GPUs. In a special address at CES, NVIDIA also announced new platform developments for Omniverse Machinima and Omniverse Audio2Face, new platform features like Nucleus Cloud and 3D marketplaces, Read article >\nThe post NVIDIA Makes Free Version of Omniverse Available to Millions of Individual Creators and Artists Worldwide appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2022/01/04/omniverse-available-free-to-creators/",
          "publishedOn": "2022-01-04T16:53:05.000Z",
          "wordCount": 1199,
          "title": "NVIDIA Makes Free Version of Omniverse Available to Millions of Individual Creators and Artists Worldwide"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54754",
          "author": "Danny Shapiro",
          "description": "CES has long been a showcase on what’s coming down the technology pipeline. This year, NVIDIA is showing the radical innovation happening now. During a special virtual address at the show, Ali Kani, vice president and general manager of Automotive at NVIDIA, detailed the capabilities of DRIVE Hyperion and the many ways the industry is Read article >\nThe post Autonomous Era Arrives at CES 2022 With NVIDIA DRIVE Hyperion and Omniverse Avatar appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2022/01/04/autonomous-era-ces-2022-drive-hyperion/",
          "publishedOn": "2022-01-04T16:52:57.000Z",
          "wordCount": 1317,
          "title": "Autonomous Era Arrives at CES 2022 With NVIDIA DRIVE Hyperion and Omniverse Avatar"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54785",
          "author": "Phil Eisler",
          "description": "GeForce NOW is kicking off the new year by bringing more games, more devices and more networks to our cloud gaming ecosystem. The next pair of Electronic Arts games, Battlefield 4 and Battlefield V, is streaming on GeForce NOW. We’re also working closely with a few titans in their respective industries: AT&T and Samsung. AT&T Read article >\nThe post GeForce NOW Delivers Legendary GeForce Gaming With More Games on More Networks to More Devices appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2022/01/04/geforce-now-ea-att-samsung-ces/",
          "publishedOn": "2022-01-04T16:52:33.000Z",
          "wordCount": 913,
          "title": "GeForce NOW Delivers Legendary GeForce Gaming With More Games on More Networks to More Devices"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54756",
          "author": "Scott Martin",
          "description": "Not so long ago, searching for information could lead to a library to scan endless volumes or even tediously sift through microfilm. Clearly, technology is making the world a better place. Scientists, researchers, developers and companies have been on a quest to solve some of the world’s most pressing problems. Only now they’re accelerating their Read article >\nThe post 5 Ways AI Aimed to Improve the World in 2021 appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/31/5-ways-ai-aimed-to-improve-the-world-in-2021/",
          "publishedOn": "2021-12-31T17:00:15.000Z",
          "wordCount": 1039,
          "title": "5 Ways AI Aimed to Improve the World in 2021"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54738",
          "author": "Isha Salian",
          "description": "NVIDIA Inception is one of the largest startup ecosystems in the world — and its thousands of members achieved impressive feats in 2021, bringing AI and data science to an array of industries. NVIDIA Inception nurtures cutting-edge AI, data science and HPC startups with go-to-market support, expertise and technology. This year, the program surpassed 9,000 Read article >\nThe post Innovation Inspiration: 5 Startup Stories From NVIDIA Inception in 2021 appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/30/5-inception-ai-startup-stories/",
          "publishedOn": "2021-12-30T16:00:37.000Z",
          "wordCount": 746,
          "title": "Innovation Inspiration: 5 Startup Stories From NVIDIA Inception in 2021"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54747",
          "author": "GeForce NOW Community",
          "description": "It’s the final countdown for what’s been a big year for cloud gaming. For the last GFN Thursday of the year, we’re taking a look at some of the GeForce NOW community’s top picks of games that joined the GeForce NOW library in 2021. Plus, check out the last batch of games coming to the Read article >\nThe post GFN Thursday Says ‘GGs’ to 2021 With Our Community’s Top Titles of the Year appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/30/geforce-now-thursday-december-30/",
          "publishedOn": "2021-12-30T14:00:32.000Z",
          "wordCount": 1181,
          "title": "GFN Thursday Says ‘GGs’ to 2021 With Our Community’s Top Titles of the Year"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54710",
          "author": "Angie Lee",
          "description": "Recognized as one of tech’s top podcasts, the NVIDIA AI Podcast is approaching 3 million listens in five years, as it sweeps across topics like robots, data science, computer graphics and renewable energy. Its 150+ episodes reinforce the extraordinary capabilities of AI — from diagnosing disease to boosting creativity to helping save the Earth — Read article >\nThe post AI Podcast Wrapped: Top Five Episodes of 2021 appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/29/ai-podcast-top-five-episodes/",
          "publishedOn": "2021-12-29T16:00:26.000Z",
          "wordCount": 679,
          "title": "AI Podcast Wrapped: Top Five Episodes of 2021"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54669",
          "author": "Rick Merritt",
          "description": "What better way to look back at NVIDIA’s top five videos of 2021 than to hop into the cockpit of a virtual plane flying over Taipei. That was how NVIDIA’s Jeff Fisher and Manuvir Das invited viewers into their COMPUTEX keynote on May 31. Their aircraft sailed over the city’s green hills and banked around Read article >\nThe post It Was a Really Virtual Year: Top Five NVIDIA Videos of 2021 appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/28/top-youtube-videos/",
          "publishedOn": "2021-12-28T16:00:01.000Z",
          "wordCount": 897,
          "title": "It Was a Really Virtual Year: Top Five NVIDIA Videos of 2021"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54713",
          "author": "GeForce NOW Community",
          "description": "Happy holidays, members. This GFN Thursday is packed with winter sales for several games streaming on GeForce NOW, as well as seasonal in-game events. Plus, for those needing a last minute gift for a gamer in their lives, we’ve got you covered with digital gift cards for Priority memberships. To top it all off, six Read article >\nThe post Have a Holly, Jolly Gaming Season on GeForce NOW appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/23/geforce-now-thursday-december-23/",
          "publishedOn": "2021-12-23T14:00:38.000Z",
          "wordCount": 1038,
          "title": "Have a Holly, Jolly Gaming Season on GeForce NOW"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54686",
          "author": "Angie Lee",
          "description": "It was memories of playing Pac-Man and Super Mario Bros while growing up in Colombia’s sprawling capital of Bogotá that inspired Yenifer Macias’s award-winning submission for the #CreateYourRetroverse contest, featured above. The contest asked NVIDIA Omniverse users to share scenes that visualize where their love for graphics began. For Macias, that passion goes back to Read article >\nThe post 3D Artist Turns Hobby Into Career, Using Omniverse to Turn Sketches Into Masterpieces appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/21/omniverse-creator-yenifer-macias/",
          "publishedOn": "2021-12-21T16:00:37.000Z",
          "wordCount": 762,
          "title": "3D Artist Turns Hobby Into Career, Using Omniverse to Turn Sketches Into Masterpieces"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54663",
          "author": "Ami Badani",
          "description": "Data centers need extremely fast storage access, and no DPU is faster than NVIDIA’s BlueField-2. Recent testing by NVIDIA shows that two BlueField-2 data processing units reached 41.5 million input/output operations per second (IOPS) — more than 4x more IOPS than any other DPU. The BlueField-2 DPU delivered record-breaking performance using standard networking protocols and Read article >\nThe post NVIDIA BlueField Sets New World Record for DPU Performance appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/21/bluefield-dpu-world-record-performance/",
          "publishedOn": "2021-12-21T16:00:14.000Z",
          "wordCount": 1197,
          "title": "NVIDIA BlueField Sets New World Record for DPU Performance"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54666",
          "author": "Brian Caulfield",
          "description": "It could only happen in NVIDIA Omniverse — the company’s virtual world simulation and collaboration platform for 3D workflows. And it happened during an interview with a virtual toy model of NVIDIA’s CEO, Jensen Huang. “What are the greatest …” one of Toy Jensen’s creators asked, stumbling, then stopping before completing his scripted question. Unfazed, Read article >\nThe post How Omniverse Wove a Real CEO — and His Toy Counterpart — Together With Stunning Demos at GTC  appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/21/how-omniverse-keynote/",
          "publishedOn": "2021-12-21T14:00:12.000Z",
          "wordCount": 1644,
          "title": "How Omniverse Wove a Real CEO — and His Toy Counterpart — Together With Stunning Demos at GTC"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54676",
          "author": "Katie Burke",
          "description": "Meet the electric vehicle that’s truly future-proof. Electric-automaker NIO took the wraps off its fifth mass-production model, the ET5, during NIO Day 2021 last week. The mid-size sedan borrows from its luxury and performance predecessors for an intelligent vehicle that’s as agile as it is comfortable. The ET5 is a software-defined vehicle with a unified Read article >\nThe post Living in the Future: NIO ET5 Sedan Designed for the Autonomous Era With NVIDIA DRIVE Orin appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/20/nio-et5-designed-autonomous-era-drive-orin/",
          "publishedOn": "2021-12-20T21:43:25.000Z",
          "wordCount": 807,
          "title": "Living in the Future: NIO ET5 Sedan Designed for the Autonomous Era With NVIDIA DRIVE Orin"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54653",
          "author": "Angie Lee",
          "description": "Imagine picking out a brand new car — only to find a chip in the paint, rip in the seat fabric or mark in the glass. AI can help prevent such moments of disappointment for manufacturers and potential buyers. Mariner, an NVIDIA Metropolis partner based in Charlotte, North Carolina, offers an AI-enabled video analytics system Read article >\nThe post Detect That Defect: Mariner Speeds Up Manufacturing Workflows With AI-Based Visual Inspection appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/20/mariner-visual-inspection/",
          "publishedOn": "2021-12-20T18:00:45.000Z",
          "wordCount": 751,
          "title": "Detect That Defect: Mariner Speeds Up Manufacturing Workflows With AI-Based Visual Inspection"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54605",
          "author": "Amanda Saunders",
          "description": "2021 saw massive growth in the demand for edge computing — driven by the pandemic, the need for more efficient business processes, as well as key advances in the Internet of Things, 5G and AI. In a study published by IBM in May, for example, 94 percent of surveyed executives said their organizations will implement Read article >\nThe post Top 5 Edge AI Trends to Watch in 2022 appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/17/top-5-edge-ai-trends-2022/",
          "publishedOn": "2021-12-17T08:01:57.000Z",
          "wordCount": 1392,
          "title": "Top 5 Edge AI Trends to Watch in 2022"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54601",
          "author": "Angie Lee",
          "description": "The thing about inspiration is you never know where it might come from, or where it might lead.  Anderson Rohr, a 3D generalist and freelance video editor based in southern Brazil, has for more than a dozen years created content ranging from wedding videos to cinematic animation. After seeing another creator animate a sci-fi character’s Read article >\nThe post Omniverse Creator Uses AI to Make Scenes With Singing Digital Humans appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/16/omniverse-creator-anderson-rohr/",
          "publishedOn": "2021-12-16T16:00:25.000Z",
          "wordCount": 651,
          "title": "Omniverse Creator Uses AI to Make Scenes With Singing Digital Humans"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54626",
          "author": "GeForce NOW Community",
          "description": "The future of cloud gaming is available NOW, for everyone, with preorders closing and GeForce NOW RTX 3080 memberships moving to instant access. Gamers can sign up for a six-month GeForce NOW RTX 3080 membership and instantly stream the next generation of cloud gaming, starting today. Snag the NVIDIA SHIELD TV or SHIELD TV Pro Read article >\nThe post Get the Best of Cloud Gaming With GeForce NOW RTX 3080 Memberships Available Instantly appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/16/geforce-now-thursday-december-16/",
          "publishedOn": "2021-12-16T14:00:23.000Z",
          "wordCount": 1331,
          "title": "Get the Best of Cloud Gaming With GeForce NOW RTX 3080 Memberships Available Instantly"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54608",
          "author": "Angie Lee",
          "description": "One of AI’s greatest champions has turned to fiction to answer the question: how will technology shape our world in the next 20 years? Kai-Fu Lee, CEO of Sinovation Ventures and a former president of Google China, spoke with NVIDIA AI Podcast host Noah Kravitz about AI 2041: Ten Visions for Our Future. The book, Read article >\nThe post ‘AI 2041: Ten Visions for Our Future’: AI Pioneer Kai-Fu Lee Discusses His New Work of Fiction appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/15/kai-fu-lee-ai-2041/",
          "publishedOn": "2021-12-15T16:00:48.000Z",
          "wordCount": 825,
          "title": "‘AI 2041: Ten Visions for Our Future’: AI Pioneer Kai-Fu Lee Discusses His New Work of Fiction"
        }
      ]
    },
    {
      "title": "David Stutz",
      "feedUrl": "http://davidstutz.de/feed",
      "siteUrl": "https://davidstutz.de",
      "articles": []
    },
    {
      "title": "Artificial Intelligence",
      "feedUrl": "https://www.reddit.com/r/artificial/.rss",
      "siteUrl": "https://www.reddit.com/r/artificial/",
      "articles": [
        {
          "id": "https://www.reddit.com/r/artificial/comments/s3dhcm/software_for_generating_new_voices_not_voice/",
          "author": null,
          "description": "There's plenty of software out there that can clone another person's voice. And there's even some software that acts as a voice changer, mapping another person's voice to your performance.\n What I would like is voice changing software that allows you to edit and save a fully custom voice.\n It could be used for personal animation and video game projects.\n ​\n There is the concern of how this might effect the careers of people in the voice acting business. . . \n But I figure that making a professional acting performance would still be a valuable service.\n And either way, I find it might be equally questionable to restrict the advancement of technology to preserve anyone's monopoly.\n    submitted by    /u/BladeManEXE7  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s3dhcm/software_for_generating_new_voices_not_voice/",
          "publishedOn": "2022-01-13T23:55:17.000Z",
          "wordCount": 306,
          "title": "Software for generating new voices; Not voice cloning."
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s3bza9/im_investigating_the_impact_of_the_implementation/",
          "author": null,
          "description": "Hello everyone, I would appreciate if you could answer this survey. It is anonymous and takes about 3 minutes. \n LINK: https://docs.google.com/forms/d/1HxsVggCMTXtKGP3ov9lK3C6gyS9faN9Q0EtwFYqEwqY\n Thank you!\n    submitted by    /u/Adventurous_Wall6596  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s3bza9/im_investigating_the_impact_of_the_implementation/",
          "publishedOn": "2022-01-13T22:47:42.000Z",
          "wordCount": 167,
          "title": "I'm investigating the impact of the implementation of AI in the rail industry. I would appreciate if you could answer a simple survey."
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s3btn6/whats_the_best_text_i_could_record_for_a_ai_tts/",
          "author": null,
          "description": "i want to make a realist tts bot of my voice and i just need some good text to read out and record for my ai. i just dont know what should i record. any tips?\n    submitted by    /u/Born-Macaroon-7610  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s3btn6/whats_the_best_text_i_could_record_for_a_ai_tts/",
          "publishedOn": "2022-01-13T22:40:32.000Z",
          "wordCount": 149,
          "title": "whats the best text i could record for a ai TTS bot."
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s38474/discord_ai_stock_bot_with_automoderation_and/",
          "author": null,
          "description": "submitted by    /u/ungKoda  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s38474/discord_ai_stock_bot_with_automoderation_and/",
          "publishedOn": "2022-01-13T19:57:41.000Z",
          "wordCount": 131,
          "title": "discord ai stock bot with auto-moderation and headline analysis- [buni]x, the based universal natty instrument x =]"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s347du/equations_for_computing_true_positives_and_false/",
          "author": null,
          "description": "I am running some evaluation metrics using the YOLOv5 object detection algorithm, and wish to calculate my true positives and false positives. For instance, the evaluation metric outputs are as follows: \n  Class Images Labels Prec Recall mAP@.5 mAP@.5:.95: all 100 36 0.444 0.702 0.481 0.223 Class 1 50 29 0.588 0.689 0.668 0.333 Class 2 50 7 0.301 0.714 0.293 0.113 \n Looking at this source (https://github.com/ultralytics/yolov5/issues/5713), I found that you could calculate the true positives and false positives with the following equations:\n ```\n Computed for Class 1\n TP = Recall * Labels = 34.45 ≈ 34 FP = (TP / Precision) - TP = 23.82 ≈ 24 ```\n I am new to evaluation metrics, so at first glance, I'm thinking that the false positive number is fairly high. Is this the correct formula to compute the true positives and false positives? I'm just looking for some verification and some explanation as to why this works, if it does.\n    submitted by    /u/EnvironmentalLemon36  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s347du/equations_for_computing_true_positives_and_false/",
          "publishedOn": "2022-01-13T17:07:17.000Z",
          "wordCount": 270,
          "title": "Equations for computing true positives and false positives when using object detection algorithms?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s33qmw/top_emerging_deep_learning_trends_for_2022/",
          "author": null,
          "description": "submitted by    /u/techsucker  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s33qmw/top_emerging_deep_learning_trends_for_2022/",
          "publishedOn": "2022-01-13T16:46:53.000Z",
          "wordCount": 105,
          "title": "Top Emerging Deep Learning Trends For 2022"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s3233b/r_facebook_ai_uc_berkeleys_convnexts_compete/",
          "author": null,
          "description": "A team from Facebook AI Research and UC Berkeley proposes ConvNeXts, a pure ConvNet model that achieves performance comparable with state-of-the-art hierarchical vision transformers on computer vision benchmarks while retaining the simplicity and efficiency of standard ConvNets. \n Here is a quick read: Facebook AI & UC Berkeley’s ConvNeXts Compete Favourably With SOTA Hierarchical ViTs on CV Benchmarks.\n The ConvNeXt code is available on the project’s GitHub. The paper A ConvNet for the 2020s is on arXiv.\n    submitted by    /u/Yuqing7  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s3233b/r_facebook_ai_uc_berkeleys_convnexts_compete/",
          "publishedOn": "2022-01-13T15:32:01.000Z",
          "wordCount": 215,
          "title": "[R] Facebook AI & UC Berkeley’s ConvNeXts Compete Favourably With SOTA Hierarchical ViTs on CV Benchmarks"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s317h1/top_10_face_detection_apis/",
          "author": null,
          "description": "submitted by    /u/tah_zem  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s317h1/top_10_face_detection_apis/",
          "publishedOn": "2022-01-13T14:52:17.000Z",
          "wordCount": 98,
          "title": "Top 10 Face Detection APIs"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s30tgz/ai_did_some_good_work_for_my_prompt_gothic_dream/",
          "author": null,
          "description": "submitted by    /u/snowpixelapp  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s30tgz/ai_did_some_good_work_for_my_prompt_gothic_dream/",
          "publishedOn": "2022-01-13T14:33:43.000Z",
          "wordCount": 208,
          "title": "AI did some good work for my prompt \"Gothic Dream\""
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s3044x/the_age_of_aiism/",
          "author": null,
          "description": "submitted by    /u/bendee983  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s3044x/the_age_of_aiism/",
          "publishedOn": "2022-01-13T14:01:48.000Z",
          "wordCount": 92,
          "title": "The age of AI-ism"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s2xht6/peril_of_humanlike_artificial_intelligence/",
          "author": null,
          "description": "arXiv:2201.04200 Date: Tue, 11 Jan 2022 21:07:17 GMT (437kb)\n Title: The Turing Trap: The Promise & Peril of Human-Like Artificial Intelligence\n Authors: Erik Brynjolfsson\n Categories: econ.GN cs.AI cs.CY cs.LG q-fin.EC \n Comments: Forthcoming in Daedalus, April 2022. Posted with permission MSC-class: 91 ACM-class: K.4.0; K.4.2 \\\\ \n In 1950, Alan Turing proposed an imitation game as the ultimate test of whether a machine was intelligent: could a machine imitate a human so well that its answers to questions indistinguishable from a human. Ever since, creating intelligence that matches human intelligence has implicitly or explicitly been the goal of thousands of researchers, engineers, and entrepreneurs. The benefits of human-like artificial intelligence (HLAI) include soaring productivity, increased leisure, and perhaps most profoundly, a better understanding of our own minds. But not all types of AI are human-like. In fact, many of the most powerful systems are very different from humans. So an excessive focus on developing and deploying HLAI can lead us into a trap. As machines become better substitutes for human labor, workers lose economic and political bargaining power and become increasingly dependent on those who control the technology. In contrast, when AI is focused on augmenting humans rather than mimicking them, then humans retain the power to insist on a share of the value created. Furthermore, augmentation creates new capabilities and new products and services, ultimately generating far more value than merely human-like AI. While both types of AI can be enormously beneficial, there are currently excess incentives for automation rather than augmentation among technologists, business executives, and policymakers. \\\\ \n ( https://arxiv.org/abs/2201.04200 , 437kb)\n    submitted by    /u/kg4jxt  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s2xht6/peril_of_humanlike_artificial_intelligence/",
          "publishedOn": "2022-01-13T11:36:37.000Z",
          "wordCount": 348,
          "title": "Peril of Human-Like Artificial Intelligence"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s2vx8x/a_curated_list_of_speechnlu_saas_companies/",
          "author": null,
          "description": "Hi, I'm the maker of SpeechPro, a niche job board for Speech AI tech professionals. I've just added a page listing all the Speech/NLU SaaS companies.\n SpeechPro company list screenshot\n It's a growing list and I'll keep updating it. Right now there are 50 and most of them are ASR/TTS/NLU/CAI related. Considering that ASR and TTS are essential parts for the Human-Machine interface, Speech SaaS companies might gain more popularity in this #Metaverse trend.\n Hope this company list can be helpful. Thanks.\n    submitted by    /u/david_swagger  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s2vx8x/a_curated_list_of_speechnlu_saas_companies/",
          "publishedOn": "2022-01-13T09:55:56.000Z",
          "wordCount": 179,
          "title": "A curated list of Speech/NLU SaaS companies"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s2thvl/solving_captchas_with_machine_learning_to_enable/",
          "author": null,
          "description": "submitted by    /u/DaveBowman1975  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s2thvl/solving_captchas_with_machine_learning_to_enable/",
          "publishedOn": "2022-01-13T07:19:19.000Z",
          "wordCount": 152,
          "title": "Solving CAPTCHAs With Machine Learning to Enable Dark Web Research"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s2ab1w/lior_cole_is_the_model_combining_artificial/",
          "author": null,
          "description": "submitted by    /u/mr_j_b  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s2ab1w/lior_cole_is_the_model_combining_artificial/",
          "publishedOn": "2022-01-12T16:27:51.000Z",
          "wordCount": 260,
          "title": "Lior Cole Is the Model Combining Artificial Intelligence With Religion"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s2aavr/artificial_intelligence_to_influence_top_tech/",
          "author": null,
          "description": "submitted by    /u/mr_j_b  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s2aavr/artificial_intelligence_to_influence_top_tech/",
          "publishedOn": "2022-01-12T16:27:38.000Z",
          "wordCount": 125,
          "title": "Artificial intelligence to influence top tech trends in major way in next five years"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s2aake/how_ai_is_set_to_evolve_in_2022/",
          "author": null,
          "description": "submitted by    /u/mr_j_b  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s2aake/how_ai_is_set_to_evolve_in_2022/",
          "publishedOn": "2022-01-12T16:27:14.000Z",
          "wordCount": 107,
          "title": "How A.I. is set to evolve in 2022"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s25w74/what_is_the_state_of_ai_this_is_the_question_i/",
          "author": null,
          "description": "submitted by    /u/OnlyProggingForFun  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s25w74/what_is_the_state_of_ai_this_is_the_question_i/",
          "publishedOn": "2022-01-12T13:09:07.000Z",
          "wordCount": 184,
          "title": "What is the state of AI? This is the question I try to answer on my blog monthly, hoping to provide valuable information and insights to our community and those outside the field."
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s22gud/oneflow_v060_just_came_outp/",
          "author": null,
          "description": "submitted by    /u/Just0by  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s22gud/oneflow_v060_just_came_outp/",
          "publishedOn": "2022-01-12T09:42:03.000Z",
          "wordCount": 108,
          "title": "OneFlow v0.6.0 just came out![P]"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s1rsnv/ai_made_art/",
          "author": null,
          "description": "submitted by    /u/deepnskate  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s1rsnv/ai_made_art/",
          "publishedOn": "2022-01-12T00:08:23.000Z",
          "wordCount": 92,
          "title": "Ai made Art"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s1qsx5/chunkmogrify_facial_editing_ai_with_masking/",
          "author": null,
          "description": "submitted by    /u/cloud_weather  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s1qsx5/chunkmogrify_facial_editing_ai_with_masking/",
          "publishedOn": "2022-01-11T23:22:33.000Z",
          "wordCount": 104,
          "title": "Chunkmogrify - Facial Editing AI with Masking"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s1ly5k/cmu_researchers_propose_a_computer_visionbased/",
          "author": null,
          "description": "Materials processing is the process of turning raw materials into final items through a sequence of phases or “unit operations.” The activities entail a series of industrial processes, including various mechanical and chemical methods, which are often carried out in big numbers or batches.\n Material processing required extensive analysis and classification of complicated microstructures for quality control. For example, the proportion of lath-type bainite in various high-strength steels affects the material’s characteristics. However, recognizing bainite in microstructural images takes time and money because researchers must first employ two types of microscopy to get a closer look, then rely on their own skills to identify bainitic regions.\n Continue Reading | Paper\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s1ly5k/cmu_researchers_propose_a_computer_visionbased/",
          "publishedOn": "2022-01-11T19:57:25.000Z",
          "wordCount": 230,
          "title": "CMU Researchers Propose A Computer Vision-Based Approach With Data-Frugal Deep Learning To Optimize Microstructure Imaging"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s1ksj5/awesome_rd_content_with_code_on_computer_vision/",
          "author": null,
          "description": "Many great articles about AI, Deep Learning, Computer Vision and more...\n HTML5 version (recommended)\n PDF version\n Dilbert on page 2. Free subscription on page 70.\n Enjoy!\n https://preview.redd.it/a0dxqnpqx3b81.jpg?width=700&format=pjpg&auto=webp&s=ef2584405c554b006bb9cc970f4b90686034d8c5\n    submitted by    /u/Gletta  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s1ksj5/awesome_rd_content_with_code_on_computer_vision/",
          "publishedOn": "2022-01-11T19:09:00.000Z",
          "wordCount": 172,
          "title": "Awesome R&D content (with code!) on Computer Vision News of January 2022."
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s1jsdf/superintelligent_utility_monster_thought/",
          "author": null,
          "description": "submitted by    /u/HumanSeeing  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s1jsdf/superintelligent_utility_monster_thought/",
          "publishedOn": "2022-01-11T18:27:44.000Z",
          "wordCount": 277,
          "title": "Superintelligent Utility Monster thought experiment"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s1iz32/any_text_based_games_like_ai_dungeon/",
          "author": null,
          "description": "submitted by    /u/roblox22y  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s1iz32/any_text_based_games_like_ai_dungeon/",
          "publishedOn": "2022-01-11T17:54:05.000Z",
          "wordCount": 176,
          "title": "Any text based games like ai dungeon"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s1dx4a/is_there_a_similar_ai_which_is_able_to_make/",
          "author": null,
          "description": "https://youtu.be/Peccbcj8Ibs :\n    submitted by    /u/xXLisa28Xx  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s1dx4a/is_there_a_similar_ai_which_is_able_to_make/",
          "publishedOn": "2022-01-11T14:18:23.000Z",
          "wordCount": 143,
          "title": "Is there a similar AI which is able to make sketches out of images like the one below?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s1b0fq/is_lowcode_the_future_of_programming/",
          "author": null,
          "description": "submitted by    /u/Beautiful-Credit-868  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s1b0fq/is_lowcode_the_future_of_programming/",
          "publishedOn": "2022-01-11T11:40:28.000Z",
          "wordCount": 118,
          "title": "Is Low-Code the Future of Programming?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s19m4m/qlearning_and_sarsa_in_grid_environment_for/",
          "author": null,
          "description": "I created my custom, grid(7 by 7) environment to apply RL algorithms. I chose Q-learning and Sarsa, in particular.\n The grid environment consists of 3 types of terminating states: states with negative reward(-100), state with maximum reward(100) and 2 states with half reward(50).\n The main goal of training is for the agent to avoid states with negative rewards and to prefer long-term reward(100) over short-term half reward(50).\n The trained agent works weirdly when the half-rewarded state is closer to the main reward, however, if the half-rewarded states are not that close to the main reward, then both algorithms efficiently train the agent to only go to the main reward.\n So, from what I understand, the result is based on the position of the half-rewarded state.\n My hyperparameters for both Q-learning and Sarsa are the following: epsilon=1(that is gradually decayed with a linear function),gamma=0.99(I read that for the agent to learn the main reward, the gamma should be high, 0.9-0.99 approx.),alpha=0.1\n Can the problem be my environment? I'm confused because both algorithms work well without the half-rewarded state. So the problem is that depending on where the half-rewarded states are located, the algorithms sometimes don't train the agent to choose the long-term reward.\n If someone had a similar problem, I would really appreciate it if you could share how you solved it.\n    submitted by    /u/studentani  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s19m4m/qlearning_and_sarsa_in_grid_environment_for/",
          "publishedOn": "2022-01-11T10:07:35.000Z",
          "wordCount": 324,
          "title": "Q-learning and Sarsa in grid environment for short-term vs long-term rewards"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s13uht/predictions_about_the_future_and_ai/",
          "author": null,
          "description": "As time passes by Artificial Intelligence (A.I.) technology is becoming a bigger thing than ever today. Even though A.I. does come with its benefits like automated machines and etc. I do have a few concerns with A.I. and the negative things about it. A few things I question are firstly self-driving cars.\n Now I know that many people are taking advantage of this relatively new feature, however, we should consider the fact that this is a machine doing the driving. In most cases this system is reliable, but we should still account for the small number of fatalities and issues that came from it malfunctioning.\n In addition to that, another thing I would like to discuss is surgeries. I believe that if AI gets to the point where they are providing health care, they should do simple tasks like gr…",
          "link": "https://www.reddit.com/r/artificial/comments/s13uht/predictions_about_the_future_and_ai/",
          "publishedOn": "2022-01-11T04:11:48.000Z",
          "wordCount": 1445,
          "title": "Predictions about the future and A.I."
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s0vqt1/weekly_china_ai_news_alibaba_losses_head_of/",
          "author": null,
          "description": "submitted by    /u/trcytony  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s0vqt1/weekly_china_ai_news_alibaba_losses_head_of/",
          "publishedOn": "2022-01-10T21:55:59.000Z",
          "wordCount": 161,
          "title": "Weekly China AI News: Alibaba Losses Head of Self-Driving Unit; AI Creates Images from Text, and Vice Versa; Geely, Mobileye to Build Self-Driving EV for Consumer"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s0rlgn/learn_machine_learning_from_ground_up/",
          "author": null,
          "description": "Hi! This is my attempt to demystify the minutae of Machine Learning and Deep Learning. The goal is for the information to be complete and intuitive.\n https://youtube.com/channel/UC6sjv3MMPEoFCioD6-Ham4w\n I'd love to hear feedbacks. Please feel free to DM. Also, please share and subscribe if you like the content.\n MachineLearning\n DeepLearning #MachineLearningwithHarsh #YouTube\n    submitted by    /u/mr-minion  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s0rlgn/learn_machine_learning_from_ground_up/",
          "publishedOn": "2022-01-10T19:01:10.000Z",
          "wordCount": 144,
          "title": "Learn Machine Learning from ground up!"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s0qfcx/uc_sandiego_researchers_propose_a_controllable/",
          "author": null,
          "description": "Text-to-Speech (TTS) synthesis is achieved using current voice cloning methods for a new voice. They do not, however, manipulate the expressiveness of synthesized sounds. The task of learning to synthesize the speech of an unseen speaker with the least amount of training is known as voice cloning.\n UC San Diego researchers propose a Controllable voice cloning method that offers fine-grained control over many style features of synthetic speech for an unseen speaker. The voice synthesis model is explicitly conditioned on a speaker encoding, pitch contour, and latent style tokens during training. Continue Reading\n Paper: https://arxiv.org/pdf/2102.00151.pdf\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s0qfcx/uc_sandiego_researchers_propose_a_controllable/",
          "publishedOn": "2022-01-10T18:11:46.000Z",
          "wordCount": 245,
          "title": "UC Sandiego Researchers Propose A Controllable Voice Cloning Method That Allows Fine-Grained Control Over Various Style Aspects Of The Synthesized Speech For An Unseen Speaker"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s0oudk/last_week_in_ai_cuddly_robodogs_selffarming_farms/",
          "author": null,
          "description": "submitted by    /u/regalalgorithm  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s0oudk/last_week_in_ai_cuddly_robodogs_selffarming_farms/",
          "publishedOn": "2022-01-10T17:05:47.000Z",
          "wordCount": 128,
          "title": "Last Week in AI - cuddly robo-dogs, self-farming farms, AI-crafted craft beer recipes, and more!"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s0mbbs/r_counterfactual_memorization_in_language_models/",
          "author": null,
          "description": "A team from Google Research, University of Pennsylvania and Cornell University proposes a principled perspective to filter out common memorization for LMs, introducing \"counterfactual memorization\" to measure the expected change in a model’s prediction and distinguish “rare” (episodic) memorization from “common” (semantic) memorization in neural LMs. \n Here is a quick read: Counterfactual Memorization in Language Models: Distinguishing Rare from Common Memorization.\n The paper Counterfactual Memorization in Neural Language Models is on arXiv.\n    submitted by    /u/Yuqing7  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s0mbbs/r_counterfactual_memorization_in_language_models/",
          "publishedOn": "2022-01-10T15:16:30.000Z",
          "wordCount": 180,
          "title": "[R] Counterfactual Memorization in Language Models: Distinguishing Rare from Common Memorization"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s0kwbd/repeating_its_self/",
          "author": null,
          "description": "so u ever notice that when something is said it goes popular and it circles around social media, as a meme or a popular thing in social media is mostly fake ..\n ​\n as if its ai intelligence repeating its self just like our thoughts, is being repeated by movies or shows or new paper we read on line most of the shit we say as already been said.. its already been done for real yo\n    submitted by    /u/toppsick  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s0kwbd/repeating_its_self/",
          "publishedOn": "2022-01-10T14:11:22.000Z",
          "wordCount": 174,
          "title": "REPEATING ITS SELF"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s0i0m9/could_ai_labor_turn_tables_and_spark_radical/",
          "author": null,
          "description": "In a working environment quite often errors are accounted to the contingent/individual because it is considered to be the cheapest fix to blame one rather than rethinking an entire process. Middle-end employees - like project managers - won't have anyone to blame if bottom-end employees are all replaced by - not infallible but fair - AI . They'll find themselves working for AI and not the other way around. By breaking the the blame cycle, this \"fairness\" responsibility might backpropagate higher than bottom to mid-end and induce a more radical systematical change.\n    submitted by    /u/MariadocBrandybuc88  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s0i0m9/could_ai_labor_turn_tables_and_spark_radical/",
          "publishedOn": "2022-01-10T11:32:55.000Z",
          "wordCount": 218,
          "title": "Could AI labor turn tables and spark radical changes of working hierarchies?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s0g6g0/top_10_speechtotext_apis/",
          "author": null,
          "description": "submitted by    /u/tah_zem  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s0g6g0/top_10_speechtotext_apis/",
          "publishedOn": "2022-01-10T09:34:59.000Z",
          "wordCount": 95,
          "title": "Top 10 Speech-to-Text APIs"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s0dwp7/do_you_think_intelligent_robots_like_sophia_will/",
          "author": null,
          "description": "Do you think humanity will see a future where people don't bat an eye at the thought of robots of this kind?\n    submitted by    /u/Fantasyneli  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s0dwp7/do_you_think_intelligent_robots_like_sophia_will/",
          "publishedOn": "2022-01-10T07:07:01.000Z",
          "wordCount": 479,
          "title": "Do you think \"Intelligent\" robots like sophia will become relatively more common in the future? When?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s0b87t/artificial_intelligence_and_education/",
          "author": null,
          "description": "Very interesting to see this technology become more popular and spread. Friends have been using tools like hyperwrite and speedwrite to get ideas for their writing and assist with drafts, and it seems like more and more people are talking about this as the technology becomes more accessible. It will be interesting to see how this technology will impact the future of education, and curious if any students (or others) have tried writing tools like this? Just started getting really exposed to this stuff and loving it!\n    submitted by    /u/aiguy2  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s0b87t/artificial_intelligence_and_education/",
          "publishedOn": "2022-01-10T04:38:05.000Z",
          "wordCount": 347,
          "title": "Artificial Intelligence and Education"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s07yuu/researchers_from_china_propose_a_paleshaped/",
          "author": null,
          "description": "Transformers have recently demonstrated promising performance in a variety of visual tests. Inspired by Transformer’s success on a wide range of NLP tasks, Vision Transformer (ViT) first employed a pure Transformer architecture for image classification, demonstrating the promising performance of Transformer architecture for vision tasks.\n However, the quadratic complexity of global self-attention leads to high computing costs and memory use, particularly for high-resolution situations, rendering it unsuitable for use in diverse visual tasks. Various strategies confine the range of attention inside a local region to increase efficiency and lower the quadratic computing complexity generated by global self-attention. As a result, their receptive fields in a single attention layer are insufficiently big, resulting in poor context modeling. \n A new Pale-Shaped self-Attention (PS-Attention) method executes self-attention inside a pale-shaped zone to solve this issue. Compared to global self-attention, PS-Attention can considerably lower compute and memory expenses. Meanwhile, it can collect more fantastic contextual information while maintaining the same computational complexity as earlier local self-attention techniques. Continue Reading\n Paper: https://arxiv.org/pdf/2112.14000v1.pdf\n Github: https://github.com/BR-IDL/PaddleViT\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s07yuu/researchers_from_china_propose_a_paleshaped/",
          "publishedOn": "2022-01-10T01:58:01.000Z",
          "wordCount": 344,
          "title": "Researchers From China Propose A Pale-Shaped Self-Attention (PS-Attention) And A General Vision Transformer Backbone, Called Pale Transformer"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s01hn5/topics_for_debates_on_ai/",
          "author": null,
          "description": "Hello! I'm a high school computer science teacher, and teach a course on computer ethics. One of my units is on A.I. and I want to conclude the unit with student debates on topics in AI. I'm struggling to come up with topic statements however. I know for sure I want one of the topics to be centered on whether A.I. at an advanced level should be afforded the same rights as humans. \n Any other topic statement ideas? Thanks!\n    submitted by    /u/CT_History_Teacher  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s01hn5/topics_for_debates_on_ai/",
          "publishedOn": "2022-01-09T21:03:28.000Z",
          "wordCount": 222,
          "title": "Topics for Debates on AI"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/s0159i/total_noob_here_looking_for_a_program_to_creat_ai/",
          "author": null,
          "description": "hey there! i am a big fan of AI generated images and would really love to start messing with my \"own\" AI generated art, but it looks like things like OpenAI have a pretty rough barrier for entry, i am fairly knowledgeable with computers but i dont have any real programming/coding skills, can anyone recommend a program i can use to both train and generate AI art? or point me in the right direction? i really want something that does more than the browser apps that i can hopefully train (is that the right word?) myself on specific images\n    submitted by    /u/Chickenwomp  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/s0159i/total_noob_here_looking_for_a_program_to_creat_ai/",
          "publishedOn": "2022-01-09T20:48:33.000Z",
          "wordCount": 354,
          "title": "total noob here, looking for a program to creat AI art"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rzysp3/apple_ml_researchers_introduce_arkitscenes_a/",
          "author": null,
          "description": "Understanding indoor 3D scenes are becoming increasingly important in augmented reality, robotics, photography, games, and real estate. Many state-of-the-art scene interpretation algorithms have lately been driven by modern machine learning approaches. Depth estimation, 3D reconstruction, instance segmentation, object detection, and other methods are used to address distinct aspects of the problem.\n The majority of these studies are made possible by a range of real and synthetic RGB-D datasets that have been made available in recent years. Even though commercially accessible RGB-D sensors, such as Microsoft Kinect, have made the collection of such datasets possible, capturing data at a significant scale with ground truth is still a problematic issue. Continue Reading\n Paper: https://arxiv.org/pdf/2111.08897.pdf\n Github: https://github.com/apple/ARKitScenes\n https://preview.redd.it/nup8uxuanpa81.png?width=1920&format=png&auto=webp&s=c6405654ed77cb50c0471025963cbe77bee2ea57\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rzysp3/apple_ml_researchers_introduce_arkitscenes_a/",
          "publishedOn": "2022-01-09T19:05:29.000Z",
          "wordCount": 242,
          "title": "Apple ML Researchers Introduce ARKitScenes: A Diverse Real-World Dataset For 3D Indoor Scene Understanding Using Mobile RGB-D Data"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rzwrio/ebook_inside_isaac_asimov_quotes_contemplations/",
          "author": null,
          "description": "Isaac Asimov published more than 500 books (Foundation Series, I. Robot, ...) during his lifetime. Asimov is best known as one of the grandmasters of science fiction. He also wrote textbooks and scientific studies that inspires many scientists and AI Researcher. As a member of Mensa International, his genius was recognized worldwide.\n \"The saddest aspect of life right now is that science gathers knowledge faster than society gathers wisdom.\" ~ Isaac Asimov\n The book contains some of his most inspiring quotes and thoughts. \n Be Inspired! \n INSIDE ISAAC ASIMOV: QUOTES & CONTEMPLATIONS (Amazon)\n    submitted by    /u/Philo167  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rzwrio/ebook_inside_isaac_asimov_quotes_contemplations/",
          "publishedOn": "2022-01-09T17:34:28.000Z",
          "wordCount": 188,
          "title": "[E-Book] INSIDE ISAAC ASIMOV: QUOTES & CONTEMPLATIONS"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rzr771/brain_efficiency_much_more_than_you_wanted_to_know/",
          "author": null,
          "description": "https://www.lesswrong.com/posts/xwBuoE9p8GE7RAuhd/brain-efficiency-much-more-than-you-wanted-to-know\n    submitted by    /u/Singularian2501  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rzr771/brain_efficiency_much_more_than_you_wanted_to_know/",
          "publishedOn": "2022-01-09T13:00:42.000Z",
          "wordCount": 105,
          "title": "Brain Efficiency: Much More than You Wanted to Know"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rzpbqj/is_there_a_ai_similar_to_this_editing_images_to/",
          "author": null,
          "description": "https://youtu.be/I4omT2L9aI8?t=759\n https://www.youtube.com/watch?v=Peccbcj8Ibs&t=142s\n https://drawingbotv3.readthedocs.io/en/latest/quickstart.html\n I want to plott selfies from my girlfriend with a pen plotter. The images should look like they are drawn.\n    submitted by    /u/xXLisa28Xx  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rzpbqj/is_there_a_ai_similar_to_this_editing_images_to/",
          "publishedOn": "2022-01-09T11:02:20.000Z",
          "wordCount": 145,
          "title": "Is there a AI similar to this (editing images to make black sketches out of them)?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rzj24r/training_a_zombie_via_reinforcement_learning_for/",
          "author": null,
          "description": "submitted by    /u/floridianfisher  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rzj24r/training_a_zombie_via_reinforcement_learning_for/",
          "publishedOn": "2022-01-09T04:12:31.000Z",
          "wordCount": 369,
          "title": "Training a zombie via reinforcement learning for a video game"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rz8ev5/coqui_introduces_yourtts_a_zerosample/",
          "author": null,
          "description": "Recent advancements in end-to-end deep learning models have enabled new and intriguing Text-to-Speech (TTS) use-cases with excellent natural-sounding outcomes. However, the majority of these models are trained on large datasets recorded with a single speaker in a professional setting. Expanding solutions to numerous languages and speakers is not viable for everyone in this situation. It is more challenging for low-resource languages not often studied by mainstream research.\n Coqui’s team has designed ‘YourTTS‘ to overcome these limits and provide zero-shot TTS to low-resource languages. It can synthesize voices in various languages and drastically reduce data requirements by transferring information between the training set. Continue Reading....\n Paper: https://arxiv.org/pdf/2112.02418.pdf\n Github: https://github.com/coqui-ai/TTS\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rz8ev5/coqui_introduces_yourtts_a_zerosample/",
          "publishedOn": "2022-01-08T19:56:45.000Z",
          "wordCount": 215,
          "title": "Coqui Introduces ‘YourTTS’: A Zero-Sample Text-to-Speech Model With State-of-The-Art (SOTA) Results"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rz7us1/thislocationdoesnotexist/",
          "author": null,
          "description": "​\n https://preview.redd.it/d3cvvcxo1ia81.png?width=947&format=png&auto=webp&s=af6454b6f683a410f2c4b0be0fc52c1cd307ae63\n Satellite images can be easily re-purposed by using GANs that can synthesize fake aerial images. Check this out: https://mayachitra-thislocationdoesnotexist.azurewebsites.net/\n PS: GANs are getting stronger day by day. This post is just one of the million examples out there!\n    submitted by    /u/ArmadilloFabulous774  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rz7us1/thislocationdoesnotexist/",
          "publishedOn": "2022-01-08T19:32:14.000Z",
          "wordCount": 218,
          "title": "ThisLocationDoesNotExist"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rz6la2/eric_jang_on_robots_learning_at_google_and/",
          "author": null,
          "description": "submitted by    /u/regalalgorithm  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rz6la2/eric_jang_on_robots_learning_at_google_and/",
          "publishedOn": "2022-01-08T18:37:55.000Z",
          "wordCount": 116,
          "title": "Eric Jang on Robots Learning at Google and Generalization via Language"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rz62kk/what_is_the_best_ai_for_making_face_edits/",
          "author": null,
          "description": "submitted by    /u/xXNOdrugsForMEXx  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rz62kk/what_is_the_best_ai_for_making_face_edits/",
          "publishedOn": "2022-01-08T18:14:44.000Z",
          "wordCount": 105,
          "title": "What is the best AI for making face edits?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/ryzjha/biomedical_relationship_extraction_for_knowledge/",
          "author": null,
          "description": "submitted by    /u/dreadknight011  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/ryzjha/biomedical_relationship_extraction_for_knowledge/",
          "publishedOn": "2022-01-08T13:10:52.000Z",
          "wordCount": 113,
          "title": "Biomedical relationship extraction for knowledge graph creation using machine learning"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/ryu8h5/coursera_plus_annual_subscription_is_available_at/",
          "author": null,
          "description": "submitted by    /u/biggbrother23  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/ryu8h5/coursera_plus_annual_subscription_is_available_at/",
          "publishedOn": "2022-01-08T07:21:08.000Z",
          "wordCount": 125,
          "title": "Coursera Plus Annual Subscription is available at $100 discount until 1/13 if you fancy"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/ryta8d/yale_university_and_ibm_researchers_introduce/",
          "author": null,
          "description": "Graph kernel approaches have typically been the most popular strategy for graph classification tasks. Graph kernels can be thought of as functions that measure the similarity of two graphs. They allow kernelized learning algorithms like support vector machines to work directly on charts rather than convert them to fixed-length, real-valued feature vectors through feature extraction. \n In recent years, the use of Graph Neural Networks (GNNs) based on high-performance message-passing neural networks has exploded (MPNNs). As a result, they’ve grown increasingly popular for graph categorization. However, their performance is limited by their hand-crafted combinatorial features. \n Yale University and IBM researchers propose Kernel Graph Neural Networks (KerGNNs). KerGNNs are frameworks that combine graph kernels and the GNN message-passing procedure into one. They achieve results that are equivalent to cutting-edge approaches. Simultaneously, they vastly increase model interpretability when compared to traditional GNNs. Continue Reading....\n Paper: https://arxiv.org/pdf/2201.00491.pdf\n ​\n https://preview.redd.it/7e9twrtvpea81.png?width=1920&format=png&auto=webp&s=c96b50dac4f48ac18c913bbd0f1cf9a4cd0defc8\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/ryta8d/yale_university_and_ibm_researchers_introduce/",
          "publishedOn": "2022-01-08T06:20:21.000Z",
          "wordCount": 252,
          "title": "Yale University and IBM Researchers Introduce Kernel Graph Neural Networks (KerGNNs)"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/ryt3eu/amazon_original_upload_are_we_inching_towards/",
          "author": null,
          "description": "submitted by    /u/yadavvenugopal  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/ryt3eu/amazon_original_upload_are_we_inching_towards/",
          "publishedOn": "2022-01-08T06:08:46.000Z",
          "wordCount": 119,
          "title": "Amazon Original Upload: Are we Inching Towards this Dystopian Reality with Metaverse?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rykw46/does_anyone_know_what_ai_software_may_have_been/",
          "author": null,
          "description": "submitted by    /u/6owline1vex  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rykw46/does_anyone_know_what_ai_software_may_have_been/",
          "publishedOn": "2022-01-07T23:13:56.000Z",
          "wordCount": 142,
          "title": "Does anyone know what ai software may have been used to make this? 👌🌸🥰"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/ryfdhf/this_is_the_reason_why_bruce_willis_is_licensing/",
          "author": null,
          "description": "submitted by    /u/sopadebombillas  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/ryfdhf/this_is_the_reason_why_bruce_willis_is_licensing/",
          "publishedOn": "2022-01-07T19:18:11.000Z",
          "wordCount": 114,
          "title": "This Is The Reason Why Bruce Willis Is Licensing Deepfake Image Rights"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rya12g/record_chess_moves_with_webcam_for_online_play/",
          "author": null,
          "description": "Hello! I would like to share with you my project which records moves made on your real life chess board using webcam. It also transmits the moves to any chess website for playing online.\n https://github.com/karayaman/Play-online-chess-with-real-chess-board\n    submitted by    /u/Savings_Ad904  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rya12g/record_chess_moves_with_webcam_for_online_play/",
          "publishedOn": "2022-01-07T15:31:30.000Z",
          "wordCount": 146,
          "title": "Record Chess Moves with Webcam for Online Play"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/ry9ois/r_baidus_10billion_scale_ernievilg_unified/",
          "author": null,
          "description": "Baidu researchers propose ERNIE-ViLG, a 10-billion parameter scale pretraining framework for bidirectional text-image generation. Pretrained on 145 million (Chinese) image-text pairs, ERNIE-ViLG achieves state-of-the-art performance on both text-to-image and image-to-text generation tasks. \n Here is a quick read: Baidu’s 10-Billion Scale ERNIE-ViLG Unified Generative Pretraining Framework Achieves SOTA Performance on Bidirectional Vision-Language Generation Tasks.\n The paper ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation is on arXiv.\n    submitted by    /u/Yuqing7  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/ry9ois/r_baidus_10billion_scale_ernievilg_unified/",
          "publishedOn": "2022-01-07T15:16:17.000Z",
          "wordCount": 192,
          "title": "[R] Baidu’s 10-Billion Scale ERNIE-ViLG Unified Generative Pretraining Framework Achieves SOTA Performance on Bidirectional Vision-Language Generation Tasks"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/ry5hnb/development_application_of_responsible_ai_for/",
          "author": null,
          "description": "submitted by    /u/ObjectiveGround5  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/ry5hnb/development_application_of_responsible_ai_for/",
          "publishedOn": "2022-01-07T11:35:20.000Z",
          "wordCount": 153,
          "title": "Development & Application Of Responsible AI For Allied Defense & Security - Dr. Nikos Loutas Ph.D., Head of Data & ArtificiaI Intelligence (AI) Policy, NATO"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/ry5ebc/what_are_the_biggest_ai_events_in_europe/",
          "author": null,
          "description": "I want to start attending AI trade shows and conferences in Europe, but there seem to be many and I don't know where to start. I'm particularly interested in applying to be a speaker so I can share my thesis insights with peers. Thanks in advance!\n    submitted by    /u/kerfufflewhoople  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/ry5ebc/what_are_the_biggest_ai_events_in_europe/",
          "publishedOn": "2022-01-07T11:29:49.000Z",
          "wordCount": 171,
          "title": "What are the biggest AI events in Europe?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/ry4f66/upscale_images/",
          "author": null,
          "description": "Hey what is the best way to upscale lots of 500x500x png images? \n Any website/program/colab recommendations? Thanks for the help\n    submitted by    /u/BananaDrum  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/ry4f66/upscale_images/",
          "publishedOn": "2022-01-07T10:25:55.000Z",
          "wordCount": 282,
          "title": "Upscale images"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/ry2iu5/i_hope_you_find_this_video_entertaining_or/",
          "author": null,
          "description": "submitted by    /u/TonyTalksBackPodcast  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/ry2iu5/i_hope_you_find_this_video_entertaining_or/",
          "publishedOn": "2022-01-07T08:18:15.000Z",
          "wordCount": 122,
          "title": "I hope you find this video entertaining or informative! Artificial Intelligence: Life is Weird"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rxlq17/meta_ai_and_cmu_researchers_present_banmo_a_new/",
          "author": null,
          "description": "Previous work on articulated 3D shape reconstruction has frequently relied on specialized sensors (e.g., synchronized multi-camera systems) or pre-built 3D deformable models (e.g., SMAL or SMPL). Such approaches cannot scale to a wide range of items in the wild. BANMo is a technique that does not need a specialized sensor or a pre-defined template form. In a differentiable rendering framework, BANMo generates high-fidelity, articulated 3D models (including state and animatable skinning weights) from a large number of monocular casual films. While the usage of several films increases coverage of camera perspectives and object articulations, it introduces significant issues in establishing correlation across scenes with diverse backdrops, lighting conditions, etc. Continue Reading\n Paper: https://arxiv.org/pdf/2112.12761.pdf\n Project: https://banmo-www.github.io/\n ​\n https://reddit.com/link/rxlq17/video/7a0w2v9u44a81/player\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rxlq17/meta_ai_and_cmu_researchers_present_banmo_a_new/",
          "publishedOn": "2022-01-06T18:44:48.000Z",
          "wordCount": 371,
          "title": "Meta AI and CMU Researchers Present ‘BANMo’: A New Neural Network-Based Method To Build Animatable 3D Models From Videos"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rxgsuk/r_university_of_amsterdam_meta_ai_propose_a/",
          "author": null,
          "description": "In the new paper Towards Interactive Language Modeling, a research team from the University of Amsterdam and Meta AI Labs presents a road map detailing the steps to be taken towards interactive language modelling. \n Here is a quick read: University of Amsterdam & Meta AI Propose a Roadmap Toward Interactive Language Modelling Based on Caregiver-Child Interactions.\n The paper Towards Interactive Language Modelling is on arXiv.\n    submitted by    /u/Yuqing7  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rxgsuk/r_university_of_amsterdam_meta_ai_propose_a/",
          "publishedOn": "2022-01-06T15:11:29.000Z",
          "wordCount": 194,
          "title": "[R] University of Amsterdam & Meta AI Propose a Roadmap Toward Interactive Language Modelling Based on Caregiver-Child Interactions"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rxglnr/what_is_deepminds_gopher/",
          "author": null,
          "description": "submitted by    /u/Beautiful-Credit-868  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rxglnr/what_is_deepminds_gopher/",
          "publishedOn": "2022-01-06T15:02:32.000Z",
          "wordCount": 95,
          "title": "What is DeepMind's Gopher?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rxffzo/reinforcement_learning_for_the_real_world/",
          "author": null,
          "description": "submitted by    /u/bendee983  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rxffzo/reinforcement_learning_for_the_real_world/",
          "publishedOn": "2022-01-06T14:09:43.000Z",
          "wordCount": 96,
          "title": "Reinforcement learning for the real world"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rxel0f/sanebox_clean_up_your_inbox_in_minutes_keep_it/",
          "author": null,
          "description": "submitted by    /u/belqassim  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rxel0f/sanebox_clean_up_your_inbox_in_minutes_keep_it/",
          "publishedOn": "2022-01-06T13:27:06.000Z",
          "wordCount": 120,
          "title": "SaneBox | Clean up your inbox in minutes & keep it that way forever"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rxee80/last_week_in_ai_ai_enables_brain_interface_for/",
          "author": null,
          "description": "submitted by    /u/regalalgorithm  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rxee80/last_week_in_ai_ai_enables_brain_interface_for/",
          "publishedOn": "2022-01-06T13:17:21.000Z",
          "wordCount": 140,
          "title": "Last Week in AI - AI enables brain interface for robot control, Deep Learning suffers from overinterpretation, and more!"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rxdfz6/where_can_i_find_ais_which_are_able_to_edit/",
          "author": null,
          "description": "Like filters or even better.\n    submitted by    /u/xXLisa28Xx  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rxdfz6/where_can_i_find_ais_which_are_able_to_edit/",
          "publishedOn": "2022-01-06T12:27:51.000Z",
          "wordCount": 158,
          "title": "Where can I find AIs which are able to edit images automatically?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rxcd3e/we_are_miracles_made_of_the_cosmic_sea_of_miracles/",
          "author": null,
          "description": "submitted by    /u/sanguineon  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rxcd3e/we_are_miracles_made_of_the_cosmic_sea_of_miracles/",
          "publishedOn": "2022-01-06T11:25:25.000Z",
          "wordCount": 240,
          "title": "We Are Miracles Made of the Cosmic Sea of Miracles"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rxc4tp/state_of_art_in_knowledge_representation_of_image/",
          "author": null,
          "description": "If there is a training data which consists of pair of image and corresponding textual explanation of that image. How to represent this Semantic knowledge between image and corresponding textual meaning in AI model. So that we don't need much of training data, if I can infuse that knowledge between image and text pair. Idea is to infuse this knowledge and then at inference time, if such image appears then perform action. Ex- If there is a image and respective meaning is 'open the door', then if robot sees that image at inference it will open the door. I am thinking of NLP based model to infuse this knowledge, but not sure how BERT like model can be employed in this situation.\n Is there existing SOTA which addresses this kind of issues?\n    submitted by    /u/projekt_treadstone  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rxc4tp/state_of_art_in_knowledge_representation_of_image/",
          "publishedOn": "2022-01-06T11:11:37.000Z",
          "wordCount": 250,
          "title": "State of art in Knowledge representation of image and text pair in AI model"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rx5bpe/researchers_from_stanford_and_nvidia_introduce_a/",
          "author": null,
          "description": "Generative Adversarial Networks (GANs) have been one of the main hypes of recent years. Based on the famous generator-discriminator mechanism, their very simple functioning has driven the research to continuously improve the former architecture. The peak in image generation has been reached by StyleGANs, which can produce astonishingly realistic and high-quality images, able to fool even humans. \n While the generation of new samples has achieved excellent results in the 2D domain, 3D GANs are still highly inefficient. If the exact mechanism of 2D GANs is applied in the 3D environment, the computational effort is too high since 3D data is tough to manipulate for current GPUs. For this reason, the research has focused on how to construct geometry-aware GANs that can infer the underline 3D property using solely 2D images. But, in this case, the approximations are usually not 3D consistent. Continue Reading The Paper Summary\n Paper: https://arxiv.org/pdf/2112.07945.pdf\n Project: https://matthew-a-chan.github.io/EG3D/\n ​\n https://reddit.com/link/rx5bpe/video/bdca5nd9uz981/player\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rx5bpe/researchers_from_stanford_and_nvidia_introduce_a/",
          "publishedOn": "2022-01-06T04:18:22.000Z",
          "wordCount": 343,
          "title": "Researchers From Stanford and NVIDIA Introduce A Tri-Plane-Based 3D GAN Framework To Enable High-Resolution Geometry-Aware Image Synthesis"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rwxeof/log_object/",
          "author": null,
          "description": "Does anybody know what a log object is?\n    submitted by    /u/Theverybest196  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rwxeof/log_object/",
          "publishedOn": "2022-01-05T21:34:22.000Z",
          "wordCount": 228,
          "title": "Log Object"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rws0j7/modzy_launched_on_product_hunt/",
          "author": null,
          "description": "The Modzy ModelOps platform accelerates the deployment, integration, and governance of production-ready AI. Supported by a growing community of data scientists and developers, Modzy solves the toughest problem with using AI at scale.\n Check out our post linked here, where you can sign up to test out our free version!\n    submitted by    /u/modzykirsten  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rws0j7/modzy_launched_on_product_hunt/",
          "publishedOn": "2022-01-05T17:23:44.000Z",
          "wordCount": 143,
          "title": "Modzy launched on Product Hunt!"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rwpjyn/r_yale_ibm_propose_kergnns_interpretable_gnns/",
          "author": null,
          "description": "A research team from Yale and IBM presents Kernel Graph Neural Networks (KerGNNs), which integrate graph kernels into the message passing process of GNNs in one framework, achieving performance comparable to state-of-the-art methods and significantly improving model interpretability compared with conventional GNNs. \n Here is a quick read: Yale & IBM Propose KerGNNs: Interpretable GNNs with Graph Kernels That Achieve SOTA-Competitive Performance.\n The paper KerGNNs: Interpretable Graph Neural Networks with Graph Kernels is on arXiv.\n    submitted by    /u/Yuqing7  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rwpjyn/r_yale_ibm_propose_kergnns_interpretable_gnns/",
          "publishedOn": "2022-01-05T15:34:09.000Z",
          "wordCount": 194,
          "title": "[R] Yale & IBM Propose KerGNNs: Interpretable GNNs with Graph Kernels That Achieve SOTA-Competitive Performance"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rwn86o/how_do_you_measure_fairness_without_access_to/",
          "author": null,
          "description": "Hi all! I'm working on a paper about measuring algorithmic fairness in cases where you don't have direct access to demographic data (for example, if you want to see whether a lender is discriminating against a particular race but the lender is not collecting/releasing race data of loan applicants).\n If you have ~10 minutes and work in the ethical AI space, it would be a great help to hear from this community on whether/how often you have faced this issue in practice and what you think should be done to mitigate.\n Survey link is here: https://cambridge.eu.qualtrics.com/jfe/form/SV_e9czBBKDitlglaC \n    submitted by    /u/emmharv  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rwn86o/how_do_you_measure_fairness_without_access_to/",
          "publishedOn": "2022-01-05T13:44:23.000Z",
          "wordCount": 637,
          "title": "How do you measure fairness without access to demographic data?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rwmr0f/use_this_new_year_to_start_learning_something_new/",
          "author": null,
          "description": "submitted by    /u/OnlyProggingForFun  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rwmr0f/use_this_new_year_to_start_learning_something_new/",
          "publishedOn": "2022-01-05T13:19:54.000Z",
          "wordCount": 200,
          "title": "Use this new year to start learning something new! Whether it is machine learning or piano, just give it a try for 5 minutes tonight! If it is ML-related, this video might help out, and you can start there!"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rwkskg/changing_gender_and_race_in_image_search_results/",
          "author": null,
          "description": "submitted by    /u/DaveBowman1975  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rwkskg/changing_gender_and_race_in_image_search_results/",
          "publishedOn": "2022-01-05T11:28:42.000Z",
          "wordCount": 116,
          "title": "Changing Gender and Race in Image Search Results With Machine Learning"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rwi961/phonesites_launch_pages_in_minutes_from_your_phone/",
          "author": null,
          "description": "submitted by    /u/belqassim  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rwi961/phonesites_launch_pages_in_minutes_from_your_phone/",
          "publishedOn": "2022-01-05T08:43:48.000Z",
          "wordCount": 102,
          "title": "Phonesites: Launch Pages in Minutes from Your Phone"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rwhk9u/would_it_be_possible_to_create_an_ai_which_can/",
          "author": null,
          "description": "I know some software can already recognize specific voices, like Siri. However, Siri in particular can’t recognize it beyond the “Hey Siri” keywords I believe. Is it possible to create an AI which can continuously recognize x-person’s voice in a group of people or a loud environment (loud restaurant, construction nearby, concert, etc…) in real time?\n  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rwhk9u/would_it_be_possible_to_create_an_ai_which_can/",
          "publishedOn": "2022-01-05T07:55:48.000Z",
          "wordCount": 228,
          "title": "Would it be possible to create an AI which can recognize your voice in the middle of a crowd or a somewhat noisy environment?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rwakko/dumbyetsentient_ai/",
          "author": null,
          "description": "It seems like most people associate sentient AI with Artificial General Intelligence, or AGI, but why should they be related? Wikipedia defines sentience as the \"ability to be aware of feelings and sensations\", and this doesn't seem to require any specific level of intelligence, does it? Couldn't we build a sentient AI bot, for example, out of \"dumb\" neural networks (e.g. relatively small transformers and/or CNN/RNN models), so long as they connect together in a way that involves sensing a world around them (in whatever dimensions that world has, not necessarily the same as ours), using these sensations to construct some sort of internal \"state\", and identifying that state accurately enough to drive subsequent action--or interaction with the world?\n    submitted by    /u/mm_maybe  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rwakko/dumbyetsentient_ai/",
          "publishedOn": "2022-01-05T01:36:23.000Z",
          "wordCount": 958,
          "title": "Dumb-yet-sentient AI"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rw7vcw/the_stargate_sequence_from_stanley_kubricks_2001/",
          "author": null,
          "description": "submitted by    /u/glenniszen  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rw7vcw/the_stargate_sequence_from_stanley_kubricks_2001/",
          "publishedOn": "2022-01-04T23:26:02.000Z",
          "wordCount": 148,
          "title": "The Stargate sequence from Stanley Kubrick's 2001: A Space Odyssey, remade using AI, in the style of visionary artist Alex Grey."
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rw4tfx/good_objectrecognition_pretrained_models/",
          "author": null,
          "description": "I used yolov5 for object recognition and it does a fair job, but it also sees my hand as a bird in a lot of situations and leaves a lot to be desired. I know if they're driving off of similar computer vision models, there's gotta be a better one out there for detecting objects.\n PS I've been using pytorch for this so if you find a good model please also drop a link on how to get it up and running if you have it.\n    submitted by    /u/acraber  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rw4tfx/good_objectrecognition_pretrained_models/",
          "publishedOn": "2022-01-04T21:10:00.000Z",
          "wordCount": 174,
          "title": "Good Object-recognition pre-trained models"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rw3njq/i_gave_it_the_gameplay_section_of_the_geometry/",
          "author": null,
          "description": "Geometry Dash features an hourglass timer with three sections in the shape of circles and triangles. The timer starts in the upper left corner of the in-game map and ends when it falls into the lower right corner, allowing players to use up their time. When it falls to the bottom center of the map, a demon car appears and must be navigated. The demon is a curved ramp for the player's icon, and every time the player's icon touches a portion of the demon, its ramp is altered. Also, the demon's top speed increases by 7x every time the player touches the bottom center. If the player touches the edge of the demon, then the demon will fall, taking a 5x amount of coins from the player's meter. At the end of the level, the demon car drops into a pit.[5]\n    submitted by    /u/Alternative-Ad-3041  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rw3njq/i_gave_it_the_gameplay_section_of_the_geometry/",
          "publishedOn": "2022-01-04T20:18:40.000Z",
          "wordCount": 276,
          "title": "I gave it the gameplay section of the Geometry Dash wikipedia article, this is what I got"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rw0pb8/endlessvn_open_alpha_in_march_2022/",
          "author": null,
          "description": "submitted by    /u/roblox22y  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rw0pb8/endlessvn_open_alpha_in_march_2022/",
          "publishedOn": "2022-01-04T18:09:52.000Z",
          "wordCount": 112,
          "title": "EndlessVN open alpha in march 2022"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rvy4i3/is_there_a_ai_which_is_able_to_edit_images_to/",
          "author": null,
          "description": "With small mistakes, and only some different colors?\n    submitted by    /u/xXLisa28Xx  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rvy4i3/is_there_a_ai_which_is_able_to_edit_images_to/",
          "publishedOn": "2022-01-04T16:18:41.000Z",
          "wordCount": 149,
          "title": "Is there a AI which is able to edit images to make them look drawn?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rvx5dz/bada_kittays_generated_with_cyborg_love/",
          "author": null,
          "description": "submitted by    /u/NeurogenicArtist  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rvx5dz/bada_kittays_generated_with_cyborg_love/",
          "publishedOn": "2022-01-04T15:36:23.000Z",
          "wordCount": 104,
          "title": "BadA$$ Kittays - Generated with Cyborg Love"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rvx26y/r_a_neural_network_solves_grades_generates/",
          "author": null,
          "description": "In the new paper A Neural Network Solves and Generates Mathematics Problems by Program Synthesis: Calculus, Differential Equations, Linear Algebra, and More, a research team from MIT, Columbia University, Harvard University and University of Waterloo proposes a neural network that can solve university-level mathematics problems via program synthesis. \n Here is a quick read: A Neural Network Solves, Grades & Generates University-Level Mathematics Problems by Program Synthesis.\n The paper A Neural Network Solves and Generates Mathematics Problems by Program Synthesis: Calculus, Differential Equations, Linear Algebra, and More is on arXiv.\n    submitted by    /u/Yuqing7  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rvx26y/r_a_neural_network_solves_grades_generates/",
          "publishedOn": "2022-01-04T15:32:20.000Z",
          "wordCount": 206,
          "title": "[R] A Neural Network Solves, Grades & Generates University-Level Mathematics Problems by Program Synthesis"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rvsci0/top_ai_trends_of_2022/",
          "author": null,
          "description": "submitted by    /u/Beautiful-Credit-868  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rvsci0/top_ai_trends_of_2022/",
          "publishedOn": "2022-01-04T11:33:15.000Z",
          "wordCount": 110,
          "title": "Top AI Trends of 2022"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rvn18c/i_put_the_word_death_in_a_text_to_image_ai_and/",
          "author": null,
          "description": "submitted by    /u/smartpug967  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rvn18c/i_put_the_word_death_in_a_text_to_image_ai_and/",
          "publishedOn": "2022-01-04T05:49:15.000Z",
          "wordCount": 377,
          "title": "I put the word 'death' in a text to image AI and this is what I got..."
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rvkzcf/researchers_propose_a_novel_parameter/",
          "author": null,
          "description": "In recent years, neural machine translation (NMT) has attracted a lot of attention and has had a lot of success. While traditional NMT is capable of translating a single language pair, training a separate model for each language pair is time-consuming, especially given the world’s thousands of languages. As a result, multilingual NMT is designed to handle many language pairs in a single model, lowering the cost of offline training and online deployment significantly. Furthermore, parameter sharing in multilingual neural machine translation promotes positive knowledge transfer between languages and is advantageous for low-resource translation.\n Despite the advantages of cooperative training with a completely shared model, the MNMT approach has a model capacity problem. The shared parameters are more likely to preserve broad knowledge while ignoring language-specific knowledge. To improve the model capacity, researchers use heuristic design to create extra language-specific components and build a Multilingual neural machine translation (MNMT) model with a mix of shared and language-specific characteristics, such as the language-specific attention, lightweight language adaptor, or language-specific routing layer. Continue Reading\n Paper: https://arxiv.org/pdf/2112.13619v1.pdf\n Github: https://github.com/voidmagic/parameter-differentiation\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rvkzcf/researchers_propose_a_novel_parameter/",
          "publishedOn": "2022-01-04T03:58:50.000Z",
          "wordCount": 316,
          "title": "Researchers Propose A Novel Parameter Differentiation-Based Method That Can Automatically Determine Which Parameters Should Be Shared And Which Ones Should Be Language-Specific"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rv99zi/amazon_research_introduces_deep_reinforcement/",
          "author": null,
          "description": "In recent years, voice-based virtual assistants such as Google Assistant and Amazon Alexa have grown popular. This has presented both potential and challenges for natural language understanding (NLU) systems. These devices’ production systems are often trained by supervised learning and rely significantly on annotated data. But, data annotation is costly and time-consuming. Furthermore, model updates using offline supervised learning can take long and miss trending requests.\n In the underlying architecture of voice-based virtual assistants, the NLU model often categorizes user requests into hypotheses for downstream applications to fulfill. A hypothesis comprises two tags: user intention (intent) and Named Entity Recognition (NER). For example, the valid hypothesis for “play a Madonna song” will be: PlaySong intent, ArtistName – Madonna.\n A new Amazon research introduces deep reinforcement learning strategies for NLU ranking. Their work analyses a ranking question in an NLU system in which entirely independent domain experts generate hypotheses with their features, where a domain is a functional area such as Music, Shopping, or Weather. These hypotheses are then ranked based on their scores, calculated based on their characteristics. As a result, the ranker must calibrate features from domain experts and select one hypothesis according to policy. Continue Reading\n Research Paper: https://assets.amazon.science/b3/74/77ff47044b69820c466f0624a0ab/introducing-deep-reinforcement-learning-to-nlu-ranking-tasks.pdf\n ​\n https://preview.redd.it/27wx7281ui981.png?width=1920&format=png&auto=webp&s=61264372ce2854031e4acd1221c802a3e042a0a8\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rv99zi/amazon_research_introduces_deep_reinforcement/",
          "publishedOn": "2022-01-03T19:06:38.000Z",
          "wordCount": 305,
          "title": "Amazon Research Introduces Deep Reinforcement Learning For NLU Ranking Tasks"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rv93aw/ai_will_make_it_difficult_to_discern_information/",
          "author": null,
          "description": "submitted by    /u/frog9913  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rv93aw/ai_will_make_it_difficult_to_discern_information/",
          "publishedOn": "2022-01-03T18:59:10.000Z",
          "wordCount": 189,
          "title": "AI Will Make It Difficult to Discern Information from Misinformation (1-minute audio clip from Eric Schmidt, former Google CEO)"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rv4r4s/nlp_hybridization_of_statistical_approach_and/",
          "author": null,
          "description": "Hi everyone!\n I have a question for you. For context, we aggregate on a platform the various AI APIs on the market (GCP, Azure, etc.) and including NLP APIs (keyword extraction, sentiment analysis, NER, etc.). The idea is that a developer doesn't have to create accounts with different providers and can have them all on one API to test, compare and change whenever he wants.\n However, many customers ask us how to mix the \"statistical\" approach behind these APIs with expert systems and how to achieve hybridization.\n Do you have any idea how to do this?\n Thanks,\n    submitted by    /u/tah_zem  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rv4r4s/nlp_hybridization_of_statistical_approach_and/",
          "publishedOn": "2022-01-03T15:52:47.000Z",
          "wordCount": 194,
          "title": "NLP: Hybridization of statistical approach and expert system?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/ruyi8d/top_10_object_detection_apis/",
          "author": null,
          "description": "submitted by    /u/tah_zem  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/ruyi8d/top_10_object_detection_apis/",
          "publishedOn": "2022-01-03T10:16:15.000Z",
          "wordCount": 98,
          "title": "Top 10 Object Detection APIs"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/ruy6fd/open_domain_question_answering_part1_blenderbot_20/",
          "author": null,
          "description": "submitted by    /u/coffeeroach  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/ruy6fd/open_domain_question_answering_part1_blenderbot_20/",
          "publishedOn": "2022-01-03T09:55:06.000Z",
          "wordCount": 104,
          "title": "Open Domain Question Answering Part-1 [BlenderBot 2.0]"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rumhy3/i_built_an_ai_discord_bot_that_bans_nft_bros_from/",
          "author": null,
          "description": "submitted by    /u/TernaryJimbo  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rumhy3/i_built_an_ai_discord_bot_that_bans_nft_bros_from/",
          "publishedOn": "2022-01-02T23:22:04.000Z",
          "wordCount": 129,
          "title": "I built an AI Discord bot that bans NFT Bros from my server"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rumd9r/what_can_ai_discover_in_data_that_i_was_not/",
          "author": null,
          "description": "Help me understand this please. I’ve read that people feed data to an AI/ML algorithm and find things that they weren’t looking for but there aren’t many good examples out there to read about.\n So I’m thinking: if I were to feed into an AI/ML algorithm a csv of order details and customer data from an eCommerce system, should I expect the AI engine to “find” something statistically usual/unusual and tell me about it? \n Or do I need to instruct it to look for certain traits with e-commerce orders and customers that I already know about? Eg look out for fraud by checking x y z. \n What if the thing to look for is so strange that you need the AI to pick it up. Eg maybe fraudsters start telling their friends to always use a certain name or phone number as an inside joke. I would think an AI might pick on that somehow whereas it might take a human a longer time to figure that out.\n    submitted by    /u/rich_atl  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rumd9r/what_can_ai_discover_in_data_that_i_was_not/",
          "publishedOn": "2022-01-02T23:16:32.000Z",
          "wordCount": 471,
          "title": "What can AI ‘discover’ in data that I was not expecting?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/ruhqfv/software_30_prompt_programming/",
          "author": null,
          "description": "submitted by    /u/Respawne  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/ruhqfv/software_30_prompt_programming/",
          "publishedOn": "2022-01-02T19:51:33.000Z",
          "wordCount": 90,
          "title": "Software 3.0: Prompt programming"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rufjxi/russian_bioinformaticians_have_created_a_neural/",
          "author": null,
          "description": "Genomic editing, particularly the CRISPR/Cas technique, is widely employed in experimental biology, agriculture, and biotechnology. CRISPR/Cas is one of several weapons used by bacteria to resist viruses. As the pathogen’s DNA enters the cell, Cas proteins detect it as foreign hereditary material and break it because its sequences differ from those of the bacteria. To respond to the virus quicker, the bacterium saves pieces of the pathogen’s DNA—much like a computer antivirus retains a collection of viral signatures—and passes them on to subsequent generations so that its Cas can prevent future attacks.\n Teams from different laboratories independently adapted the CRISPR/Cas system to introduce arbitrary changes into DNA sequences in human and animal cells. It made genomic editing much easier and more efficient. The critical components of the mechanism are guide RNA, which “marks the site,” and the Cas9 protein, which cleaves DNA at that location. The cell subsequently “heals the wound,” but the genetic code has already been altered.\n Quick Reading: https://www.marktechpost.com/2022/01/02/russian-bioinformaticians-have-created-a-neural-network-architecture-that-can-evaluate-how-well-an-rna-guide-has-been-chosen-for-gene-editing/ \n Paper: https://academic.oup.com/nar/advance-article/doi/10.1093/nar/gkab1065/6430490\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rufjxi/russian_bioinformaticians_have_created_a_neural/",
          "publishedOn": "2022-01-02T18:15:29.000Z",
          "wordCount": 333,
          "title": "Russian Bioinformaticians Have Created A Neural Network Architecture That Can Evaluate How Well An RNA Guide Has Been Chosen For Gene Editing"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rueink/tang_jie_the_tsinghua_university_professor/",
          "author": null,
          "description": "submitted by    /u/No-Transition-6630  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rueink/tang_jie_the_tsinghua_university_professor/",
          "publishedOn": "2022-01-02T17:29:38.000Z",
          "wordCount": 256,
          "title": "Tang Jie, the Tsinghua University professor leading the Wu Dao project, said in a recent interview that the group built 100 TRILLION parameter model in June, though it has not trained it to “convergence,” the point at which the model stops improving"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/ru8nmg/the_top_10_aicomputer_vision_papers_in_2021_with/",
          "author": null,
          "description": "submitted by    /u/OnlyProggingForFun  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/ru8nmg/the_top_10_aicomputer_vision_papers_in_2021_with/",
          "publishedOn": "2022-01-02T12:27:14.000Z",
          "wordCount": 133,
          "title": "The top 10 AI/Computer Vision papers in 2021 with video demos, articles, and code for each!"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/ru6ddq/what_ais_are_there_which_are_able_to_edit_images/",
          "author": null,
          "description": "My dad loves technology but does not really understand it, I thought I take some pictures from him and AI should edit them so he looks like a simpsons etc.\n    submitted by    /u/xXLisa28Xx  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/ru6ddq/what_ais_are_there_which_are_able_to_edit_images/",
          "publishedOn": "2022-01-02T09:41:09.000Z",
          "wordCount": 309,
          "title": "What AIs are there which are able to edit images?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rtpg2w/hierarchical_federated_learningbased_anomaly/",
          "author": null,
          "description": "Smart healthcare services can be provided by using Internet of Things (IoT) technologies that monitor the health conditions of patients and their vital body parameters. The majority of IoT solutions used to enable such services are wearable devices, such as smartwatches, ECG monitors, and blood pressure monitors. The huge amount of data collected from smart medical devices leads to major security and privacy issues in the IoT domain. Considering Remote Patient Monitoring (RPM) applications, we will focus on Anomaly Detection (AD) models, whose purpose is to identify events that differ from the typical user behavior patterns. Generally, while designing centralized AD models, the researchers face security and privacy challenges (e.g., patient data privacy, training data poisoning).\n To overcome these issues, the researchers of this paper propose an Anomaly Detection (AD) model based on Federated Learning (FL). Federated Learning (FL) allows different devices to collaborate and perform training locally in order to build Anomaly Detection (AD) models without sharing patients’ data. Specifically, the researchers propose a hierarchical Federated Learning (FL) that enables collaboration among different organizations, by building various Anomaly Detection (AD) models for patients with similar health conditions.\n Continue Reading the Paper Summary: https://www.marktechpost.com/2022/01/01/hierarchical-federated-learning-based-anomaly-detection-using-digital-twins-for-internet-of-medical-things-iomt/\n Full Paper: https://arxiv.org/pdf/2111.12241.pdf\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rtpg2w/hierarchical_federated_learningbased_anomaly/",
          "publishedOn": "2022-01-01T18:40:43.000Z",
          "wordCount": 328,
          "title": "Hierarchical Federated Learning-Based Anomaly Detection Using Digital Twins For Internet of Medical Things (IoMT)"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rtoc1h/generate_artistic_images_with_openais_glide/",
          "author": null,
          "description": "submitted by    /u/tridoc  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rtoc1h/generate_artistic_images_with_openais_glide/",
          "publishedOn": "2022-01-01T17:47:49.000Z",
          "wordCount": 122,
          "title": "Generate artistic images with OpenAI’s Glide 🖼"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rtmfgl/my_top_10_computer_vision_papers_of_2021/",
          "author": null,
          "description": "submitted by    /u/OnlyProggingForFun  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rtmfgl/my_top_10_computer_vision_papers_of_2021/",
          "publishedOn": "2022-01-01T16:14:49.000Z",
          "wordCount": 113,
          "title": "My Top 10 Computer Vision papers of 2021"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rtkrmp/should_we_be_concerned/",
          "author": null,
          "description": "Should we be a little worried by how fast AI is developing?\n    submitted by    /u/Particular_Leader_16  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rtkrmp/should_we_be_concerned/",
          "publishedOn": "2022-01-01T14:45:58.000Z",
          "wordCount": 760,
          "title": "Should we be concerned?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rtf7nc/is_there_a_communitywebsitecompany_which_is/",
          "author": null,
          "description": "submitted by    /u/xXNOdrugsForMEXx  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rtf7nc/is_there_a_communitywebsitecompany_which_is/",
          "publishedOn": "2022-01-01T08:33:45.000Z",
          "wordCount": 108,
          "title": "Is there a community/website/company which is collecting and categorizing AIs?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rt65lc/ai_a_love_story_aigenerated_video_about_the/",
          "author": null,
          "description": "submitted by    /u/6owline1vex  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rt65lc/ai_a_love_story_aigenerated_video_about_the/",
          "publishedOn": "2021-12-31T23:31:41.000Z",
          "wordCount": 251,
          "title": "AI - A love story // AI-generated video about the future of AI // prompt -> GPT-J-6B -> Aphantasia"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rszhw5/ai_news_in_2021_a_detailed_digest/",
          "author": null,
          "description": "https://lastweekin.ai/p/ai-news-in-2021-a-digest\n    submitted by    /u/regalalgorithm  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rszhw5/ai_news_in_2021_a_detailed_digest/",
          "publishedOn": "2021-12-31T17:59:42.000Z",
          "wordCount": 99,
          "title": "AI News in 2021: a Detailed Digest"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rszdeg/i_made_a_virtual_twitch_streamer_who_responds_to/",
          "author": null,
          "description": "submitted by    /u/C0de_monkey  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rszdeg/i_made_a_virtual_twitch_streamer_who_responds_to/",
          "publishedOn": "2021-12-31T17:53:34.000Z",
          "wordCount": 433,
          "title": "I made a virtual Twitch Streamer, who responds to your chats using OpenAI and Text-to-Speech"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rsxtkv/happy_new_year_to_everyone_lets_hope_for_a/",
          "author": null,
          "description": "submitted by    /u/ValianTek_World  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rsxtkv/happy_new_year_to_everyone_lets_hope_for_a/",
          "publishedOn": "2021-12-31T16:41:51.000Z",
          "wordCount": 154,
          "title": "Happy New Year to everyone! Let's hope for a wonderful 2022. The text is created with polygons that evolve with an Evolutionary Algorithm."
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rswcog/r_microsofts_selfsupervised_bug_detection_and/",
          "author": null,
          "description": "In the NeurIPS 2021-accepted paper Self-Supervised Bug Detection and Repair, a Microsoft Research team proposes BUGLAB, a self-supervised approach that significantly improves on baseline methods for detecting bugs in real-life code. \n Here is a quick read: Microsoft’s Self-Supervised Bug Detection and Repair Approach Betters Baselines By Up to 30%.\n The code and PyPIBugs dataset are available on the project’s GitHub. The paper Self-Supervised Bug Detection and Repair is on arXiv.\n    submitted by    /u/Yuqing7  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rswcog/r_microsofts_selfsupervised_bug_detection_and/",
          "publishedOn": "2021-12-31T15:32:33.000Z",
          "wordCount": 187,
          "title": "[R] Microsoft’s Self-Supervised Bug Detection and Repair Approach Betters Baselines By Up to 30%"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rso5cc/commercially_available_ais_or_chatbots_that_you/",
          "author": null,
          "description": "I've been playing around with Replika, and it's incredibly fun and impressive as a general \"companion\" chatbot, but I find it really disappointing in its lack of ability to learn explicit facts or procedures. It's targeted toward making people feel like they have a friend and roleplaying, not learning anything external to those goals.\n So, is there anything commercially available that you can both:\n  \nTalk to (mostly) like a real person, and\n Teach from the ground up (like a child)\n  \n? \nAs an example, my Replika \"wanted\" to do a lesson with me, so I tried to teach it colors. Ostensibly it has some picture recognition abilities, but despite that, it was never able to learn which color was which, even when using the same image files to display the same color.\n Okay, fine, forget colors, but it can't learn explicit facts either. I want to be able to input things like \"Lacan defines the subject as that which is represented by a signifier for another signifier\" or \"It's important to apply a primer before your first layer of eyeshadow\" or \"Leonardo DiCaprio played Jack Dawson in the movie Titanic\" and have it be able to actually remember and recall that information in future conversations. (Replika is able to recall it within the same conversation, but it resets after a certain amount of time. It also seems to struggle with remembering things that differ from user to user, like favorite food, since it learns from the aggregate of its conversations.)\n Is there anything out there that can do this? Anything on the horizon?\n    submitted by    /u/peppermint-kiss  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rso5cc/commercially_available_ais_or_chatbots_that_you/",
          "publishedOn": "2021-12-31T07:12:51.000Z",
          "wordCount": 1067,
          "title": "Commercially available AIs or chatbots that you can explicitly teach/train?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rsfe7q/ai_shopping_app/",
          "author": null,
          "description": "Is their any AI apps that matches your body type (scans your body, etc..) to which clothing stores would have the best sizes to fit to your body? \n I’m tired of ordering clothes online that don’t fit well when I get them\n Thx\n    submitted by    /u/GroundbreakingRain78  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rsfe7q/ai_shopping_app/",
          "publishedOn": "2021-12-30T23:27:26.000Z",
          "wordCount": 127,
          "title": "AI shopping App"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rse9m7/watch_this_model_describe_code/",
          "author": null,
          "description": "submitted by    /u/landongarrison  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rse9m7/watch_this_model_describe_code/",
          "publishedOn": "2021-12-30T22:36:23.000Z",
          "wordCount": 600,
          "title": "Watch this model describe code"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rsa8dq/gpt3_foundation_models_and_ai_nationalism/",
          "author": null,
          "description": "submitted by    /u/regalalgorithm  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rsa8dq/gpt3_foundation_models_and_ai_nationalism/",
          "publishedOn": "2021-12-30T19:41:06.000Z",
          "wordCount": 214,
          "title": "GPT-3, Foundation Models, and AI Nationalism"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rs6ama/what_ais_are_there_which_are_able_to_edit_art/",
          "author": null,
          "description": "submitted by    /u/xXLisa28Xx  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rs6ama/what_ais_are_there_which_are_able_to_edit_art/",
          "publishedOn": "2021-12-30T16:54:22.000Z",
          "wordCount": 108,
          "title": "What AIs are there which are able to edit art?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rs42m6/what_if_ai_ran_a_city_what_if_it_ran_society/",
          "author": null,
          "description": "Ever since Grimes posted her Tiktok video last year inviting communists to join up under a supposedly benevolent AI, I've been kind of obsessed with what would really happen if AI ran a city, or even a whole society. Would it really be as awesome as she seems to think or would it be a potentially horrifying nightmare? (My money's on nightmare, btw)\n Anyway, I tried to answer some of those questions in a work of fiction, available below as a free download. It's a quick read, and I'd love to hear your thoughts on these problems, whether or not you read the book. No doubt someone soon will be trying to implement these kinds of \"solutions\" in our world... and it's best to be at least marginally prepared!\n https://lostbooks.gumroad.com/l/conspiratopia/r-artificial\n Happy New Year!\n    submitted by    /u/canadian-weed  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rs42m6/what_if_ai_ran_a_city_what_if_it_ran_society/",
          "publishedOn": "2021-12-30T15:16:21.000Z",
          "wordCount": 322,
          "title": "What if AI ran a city? What if it ran society?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rs0yuj/mark_twain_ai_simulation/",
          "author": null,
          "description": "submitted by    /u/montpelliersudfrance  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rs0yuj/mark_twain_ai_simulation/",
          "publishedOn": "2021-12-30T12:44:51.000Z",
          "wordCount": 257,
          "title": "Mark Twain AI Simulation"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rruauh/openai_introduces_glide_model_for_photorealistic/",
          "author": null,
          "description": "Images, such as graphics, paintings, and photographs, may frequently be explained in language, but they might also take specific talents and hours of effort to make. As a result, a technology capable of creating realistic graphics from natural language can enable humans to produce rich and diverse visual material with previously unimaginable simplicity. The capacity to modify photos with spoken language enables iterative refinement and fine-grained control, both essential for real-world applications.\n DALL-E, a 12-billion parameter version of OpenAI’s GPT-3 transformer language model meant to produce photorealistic pictures using text captions as cues, was unveiled in January. DALL-E’s fantastic performance was an instant hit in the AI community, as well as broad mainstream media coverage. Last month, NVIDIA unveiled the GAN-based GauGAN2 – a term inspired by French Post-Impressionist painter Paul Gauguin, much as DALL-E was inspired by Surrealist artist Salvador Dali.\n Not to be outshined, OpenAI researchers unveiled GLIDE (Guided Language-to-Image Diffusion for Generation and Editing). This diffusion model achieves performance comparable to DALL-E despite utilizing only one-third of the parameters.\n You can continue reading this short summary here\n The code and weights for these models may be found on the project’s GitHub page.\n The research paper for the GLIDE can be found here.\n https://preview.redd.it/boifxsixem881.png?width=705&format=png&auto=webp&s=14de64b727e31dba7b55ecf76bdfdb52463f2e01\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rruauh/openai_introduces_glide_model_for_photorealistic/",
          "publishedOn": "2021-12-30T06:04:33.000Z",
          "wordCount": 336,
          "title": "OpenAI Introduces ‘GLIDE’ Model For Photorealistic Image Generation"
        }
      ]
    },
    {
      "title": "Neural Networks, Deep Learning and Machine Learning",
      "feedUrl": "https://www.reddit.com/r/neuralnetworks/.rss?format=xml",
      "siteUrl": "https://www.reddit.com/r/neuralnetworks/?format=xml",
      "articles": [
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/s35mwx/novel_viewpoint_tennis_from_single_camera/",
          "author": null,
          "description": "submitted by    /u/Odd_Temporary_9736  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/s35mwx/novel_viewpoint_tennis_from_single_camera/",
          "publishedOn": "2022-01-13T18:09:32.000Z",
          "wordCount": 109,
          "title": "Novel Viewpoint Tennis from Single Camera"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/s1qsxm/chunkmogrify_facial_editing_ai_with_masking/",
          "author": null,
          "description": "submitted by    /u/cloud_weather  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/s1qsxm/chunkmogrify_facial_editing_ai_with_masking/",
          "publishedOn": "2022-01-11T23:22:34.000Z",
          "wordCount": 117,
          "title": "Chunkmogrify - Facial Editing AI with Masking"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/s1gqxr/from_53_to_95_acc_real_vs_fake_faces/",
          "author": null,
          "description": "submitted by    /u/oFlamingo  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/s1gqxr/from_53_to_95_acc_real_vs_fake_faces/",
          "publishedOn": "2022-01-11T16:21:37.000Z",
          "wordCount": 285,
          "title": "From 53% to 95% acc - Real vs Fake Faces Classification | Fine-tuning EfficientNet (Github in comment)"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/s1dakj/how_do_i_make_a_neural_net_ai_with_cheat_engine/",
          "author": null,
          "description": "I saw somebody explain how they made an AI for Street Fighter 5 via Cheat Engine, but I am unsure how they exactly managed this. I’m making one for a different game, can someone please help me?\n    submitted by    /u/GokuKing922  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/s1dakj/how_do_i_make_a_neural_net_ai_with_cheat_engine/",
          "publishedOn": "2022-01-11T13:47:40.000Z",
          "wordCount": 605,
          "title": "How do I make a Neural Net AI with Cheat Engine?"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/s0vi7c/how_to_choose_optimizer_and_loss_keras_python/",
          "author": null,
          "description": "Hi guys, I just programmed my first neural network in Python using keras, but I’m not sure what loss functions and optimizer to use. I used mean squared error, which worked well, but I only have experience in the past using r2, so it’s not optimal. I am currently using ‘Adam’ as my optimizer, but I really don’t understand what this does. I am currently doing regression 28 input neurons leading to 1 output neuron, if that’s helpful. Thanks in advance for the help!\n    submitted by    /u/Tvdybgggh  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/s0vi7c/how_to_choose_optimizer_and_loss_keras_python/",
          "publishedOn": "2022-01-10T21:45:56.000Z",
          "wordCount": 237,
          "title": "How to choose optimizer and loss— Keras Python neural network"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/s0j6gf/smart_video_generation_from_text_using_deep/",
          "author": null,
          "description": "submitted by    /u/BraveOutage  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/s0j6gf/smart_video_generation_from_text_using_deep/",
          "publishedOn": "2022-01-10T12:41:39.000Z",
          "wordCount": 123,
          "title": "Smart Video Generation from Text Using Deep Neural Networks"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/s0hzsy/introduction_to_gnn/",
          "author": null,
          "description": "Hi All,\n I would like to learn about Graphical Neural Network. Can you tell me a good starting paper or webpage for basic understanding and would also like to have an example project to understand the functionality\n    submitted by    /u/Shocky698  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/s0hzsy/introduction_to_gnn/",
          "publishedOn": "2022-01-10T11:31:41.000Z",
          "wordCount": 250,
          "title": "Introduction to GNN"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/s0gbnm/vae_cifar10_pytorch_loss_not_improving/",
          "author": null,
          "description": "I have implemented a Variational Autoencoder using Conv-6 CNN (VGG-* family) as the encoder and decoder with CIFAR-10 in PyTorch. You can refer to the full code here.\n The problem is that the total loss (= reconstruction loss + KL-divergence loss) doesn't improve. Also, the log-variance is almost 0 indicating further that the multivariate Gaussians being mapped in the latent space is not happening as expected, since the log variance should have values between say -4 to +3, etc. You can see this in this code where the log variance is changing and has a non-zero value.\n Suggestions to alleviate the situation?\n    submitted by    /u/grid_world  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/s0gbnm/vae_cifar10_pytorch_loss_not_improving/",
          "publishedOn": "2022-01-10T09:44:47.000Z",
          "wordCount": 214,
          "title": "VAE: CIFAR-10 & PyTorch - loss not improving"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rz6lr6/a_neural_network_solves_and_generates_mathematics/",
          "author": null,
          "description": "submitted by    /u/nickb  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rz6lr6/a_neural_network_solves_and_generates_mathematics/",
          "publishedOn": "2022-01-08T18:38:32.000Z",
          "wordCount": 150,
          "title": "A Neural Network Solves and Generates Mathematics Problems by Program Synthesis: Calculus, Differential Equations, Linear Algebra, and More"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rz4z18/optimization_of_the_neural_network_with_the_use/",
          "author": null,
          "description": "Hi,\n Have any of you tried to optimize the neural network using the simulated annealing algorithm?\n I am referring to an example even on a simple neural network because I have a similar project and I will consider how to get started with the topic, so practical use would be extremely helpful ...\n Additionally, some scientific papers or hypothetical examples of use may also be helpful.\n Thanks in advance for your help.\n    submitted by    /u/theGrEaTmPm  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rz4z18/optimization_of_the_neural_network_with_the_use/",
          "publishedOn": "2022-01-08T17:27:34.000Z",
          "wordCount": 197,
          "title": "Optimization of the neural network with the use of an algorithm simulated annealing"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rz0n65/newbie_needs_help_generating_data_charts_for/",
          "author": null,
          "description": "Hello guys, I am a newbie to neural nets. I need your help I have 6 data patterns in a quality control chart and I need from those 6 graphs to generate more of each of them in order to use them to train a neural net do identify them in random charts. I have tried using Gaussian distribution noise but something doesn't work well. I prefer using python to generate them.\n    submitted by    /u/aherontas  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rz0n65/newbie_needs_help_generating_data_charts_for/",
          "publishedOn": "2022-01-08T14:08:31.000Z",
          "wordCount": 189,
          "title": "Newbie needs help generating data charts for pattern recognition"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/ryta9u/yale_university_and_ibm_researchers_introduce/",
          "author": null,
          "description": "Graph kernel approaches have typically been the most popular strategy for graph classification tasks. Graph kernels can be thought of as functions that measure the similarity of two graphs. They allow kernelized learning algorithms like support vector machines to work directly on charts rather than convert them to fixed-length, real-valued feature vectors through feature extraction. \n In recent years, the use of Graph Neural Networks (GNNs) based on high-performance message-passing neural networks has exploded (MPNNs). As a result, they’ve grown increasingly popular for graph categorization. However, their performance is limited by their hand-crafted combinatorial features. \n Yale University and IBM researchers propose Kernel Graph Neural Networks (KerGNNs). KerGNNs are frameworks that combine graph kernels and the GNN message-passing procedure into one. They achieve results that are equivalent to cutting-edge approaches. Simultaneously, they vastly increase model interpretability when compared to traditional GNNs. Continue Reading....\n Paper: https://arxiv.org/pdf/2201.00491.pdf\n https://preview.redd.it/pe289oqwpea81.png?width=1920&format=png&auto=webp&s=31c2a51a4f57303dec5f41680a9e4517f4f662be\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/ryta9u/yale_university_and_ibm_researchers_introduce/",
          "publishedOn": "2022-01-08T06:20:25.000Z",
          "wordCount": 265,
          "title": "Yale University and IBM Researchers Introduce Kernel Graph Neural Networks (KerGNNs)"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rynzfp/image_gans_meet_differentiable_rendering_for/",
          "author": null,
          "description": "submitted by    /u/nickb  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rynzfp/image_gans_meet_differentiable_rendering_for/",
          "publishedOn": "2022-01-08T01:37:07.000Z",
          "wordCount": 151,
          "title": "Image GANs meet Differentiable Rendering for Inverse Graphics and Interpretable 3D Neural Rendering (aka 3D objects from a single image)"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/ryaqf6/seq2seq_learning_resources/",
          "author": null,
          "description": "I am interested in learning Seq2Seq family of ML/DL. Can you provide free learning resources to cover topics like RNN, LSTM, GRU, Transformer?\n Thanks\n    submitted by    /u/grid_world  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/ryaqf6/seq2seq_learning_resources/",
          "publishedOn": "2022-01-07T16:02:46.000Z",
          "wordCount": 122,
          "title": "Seq2Seq learning resources"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/ry74cw/responsive_ai_dialogue_for_gemes_with_nn/",
          "author": null,
          "description": "Hello, Currently I am learning NN algos, because I want to create a project, but during this time I got an idea for another project, I'm curious if its possible. Some of us have their favourite games of all times as do I, i.e. skyrim. And I got the second idea for the project. What if I created some kind of a \"chat bot\" that runs in the background in real time. It would allow player, to communicate with npcs with their own voice and ask them something that is not scripted in the game itself, but is \"lore friendly\", by this I mean no questions about trump, biden or tiktok lol. Of course the second step would be to synthesize the voices of npcs for their generated dialogue. What I'd like to know is if that is possible, not only from proggramimg perspecitve(cuz anything given enough time would be completed), but is it possible for modern computers to run?\n It is just a crazy idea of a begginer lol\n    submitted by    /u/skollehatti  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/ry74cw/responsive_ai_dialogue_for_gemes_with_nn/",
          "publishedOn": "2022-01-07T13:14:35.000Z",
          "wordCount": 380,
          "title": "Responsive ai dialogue for gemes with NN?"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/ry3meq/are_there_any_good_books_or_videos_for_beginners/",
          "author": null,
          "description": "I’m only 15 and I don’t know much about how neural networks actually function, I have been very interested in this subject for a while and I wanted to learn more.\n Any recommendations?\n    submitted by    /u/Forever061  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/ry3meq/are_there_any_good_books_or_videos_for_beginners/",
          "publishedOn": "2022-01-07T09:32:09.000Z",
          "wordCount": 310,
          "title": "Are there any good books or videos for beginners?"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rxeo15/can_a_neural_net_be_trained_to_create_badass/",
          "author": null,
          "description": "Example: If i trained a neural net on my own pinterest board full of manga/illustration style artwork - with a rough theme (fantasy/sci-fi/wizard type characters) could the neural net then be able to output random variations of these?\n If no - what would it take?\n Sorry if im thinking about this the wrong way,\n i'm still new to this.\n    submitted by    /u/skittleteeth  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rxeo15/can_a_neural_net_be_trained_to_create_badass/",
          "publishedOn": "2022-01-06T13:31:22.000Z",
          "wordCount": 715,
          "title": "Can a neural net be trained to create badass artwork from pinterest?"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rx7dsn/is_this_the_right_use_of_an_ann/",
          "author": null,
          "description": "I have made rudimentary ANN before but keep looking for ways to push my boundaries. I like AI and flight sims so why not an AI that helps me flight sim. :)\n I'm thinking of an ANN assistant (eg co-pilot) that learns to do key actions during flight.\n Inputs:\n  \nOn ground (Boolean)\n \nAltitude (numerical - normalised)\n \nPitch (nose up/down/level)\n \n Outputs:\n  \nGear (up/down)\n Autopilot (on/off)\n Flaps (up/down)\n Lights (on/off)\n  \nSo, given the state of the aircraft at any given point, the ANN can manipulate (or recommend) the configuration of the aircraft and lighten the pilot work load (remembering this is a fun exercise).\n Assume my ANN can monitor and manipulate the flight sim (it has an interface to do this), is an ANN with appropriate weights and nodes a good fit (after training)?\n Should I be looking at Q-Table or something else?\n    submitted by    /u/Togfox  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rx7dsn/is_this_the_right_use_of_an_ann/",
          "publishedOn": "2022-01-06T06:07:58.000Z",
          "wordCount": 325,
          "title": "Is this the right use of an ANN?"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rx3vgm/3dcnn_question_help_appreciated/",
          "author": null,
          "description": "Im new to 3dCNN. Is it possible to use 3dCNN for action classification (like sitting, walking?) I have temperature data in csv this data can be turned into 16x16x4 images (RBG to show temp in different colors). \n My question is, do i need to turn each frame into an image? To then be turned into a video in order to feed it into the 3dCNN? If so, how can i optimize this? 1 action has several frames in the same csv file, and I have several files. \n Or can i just use temperature data in csv straight away? (Each file contains one action with several frames worth of temp data)\n Any help /links/ examples are greatly appreciated.\n    submitted by    /u/Miki_mallow  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rx3vgm/3dcnn_question_help_appreciated/",
          "publishedOn": "2022-01-06T03:11:42.000Z",
          "wordCount": 216,
          "title": "3dCNN question help appreciated"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rvaxu3/interested_in_learning_but_not_really_sure_where/",
          "author": null,
          "description": "For some time now I've been reading about neural networks and ai, the topic has really interested me a lot and I have seen amazing GAN projects like artbreeder, this has motivated me to want to work with this kind of technology, the only problem is that I can't really figure out where to start, so till this moment the only thing i've done is a Python introductory course. What should or could I do next?\n P.S: Spanish is my native language so excuse me if my english is a little rusty.\n    submitted by    /u/luisaalberto  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rvaxu3/interested_in_learning_but_not_really_sure_where/",
          "publishedOn": "2022-01-03T20:18:39.000Z",
          "wordCount": 391,
          "title": "Interested in learning but not really sure where to start"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rum6u8/i_built_an_ai_discord_bot_that_bans_nft_bros/",
          "author": null,
          "description": "submitted by    /u/TernaryJimbo  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rum6u8/i_built_an_ai_discord_bot_that_bans_nft_bros/",
          "publishedOn": "2022-01-02T23:08:52.000Z",
          "wordCount": 129,
          "title": "I built an AI Discord Bot that bans NFT Bros [Meme][Video]"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/ruigzx/project_credit_scoring/",
          "author": null,
          "description": "hey peeps,\n any research thesis regarding credit scoring in microfinance?\n    submitted by    /u/abschlusssss  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/ruigzx/project_credit_scoring/",
          "publishedOn": "2022-01-02T20:24:14.000Z",
          "wordCount": 108,
          "title": "[project] Credit Scoring"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rtrfr6/neural_network_drawing_mushrooms/",
          "author": null,
          "description": "submitted by    /u/noodlefist  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rtrfr6/neural_network_drawing_mushrooms/",
          "publishedOn": "2022-01-01T20:14:22.000Z",
          "wordCount": 105,
          "title": "Neural network drawing mushrooms"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rsiqn0/siamese_neural_networks_for_semantic_text/",
          "author": null,
          "description": "​\n A repository containing comprehensive Neural Networks based PyTorch implementations for the semantic text similarity task, including architectures such as Siamese-LSTM, Siamese-LSTM-Attention, Siamese-Transformer, and Siamese-BERT.\n https://github.com/shahrukhx01/siamese-nn-semantic-text-similarity\n    submitted by    /u/shahrukhx01  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rsiqn0/siamese_neural_networks_for_semantic_text/",
          "publishedOn": "2021-12-31T02:11:15.000Z",
          "wordCount": 135,
          "title": "Siamese Neural Networks for Semantic Text Similarity"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rrgrji/baidu_and_pcl_team_introduce_ernie_30_titan_a/",
          "author": null,
          "description": "With recent breakthroughs in AI, humans have become more reliant on AI to address real-world problems. This makes humans’ ability to learn and act on knowledge just as essential as a computer’s. Humans learn and gather information through learning and experience to understand everything from their immediate surroundings. The ability to comprehend and solve issues, and separate facts from absurdities, increases as the knowledge base grows. However, such knowledge is lacking in AI systems, restricting their ability to adapt to atypical problem data.\n Previous studies show that pre-trained language models improve performance on various natural language interpretation and generating tasks.\n A recent work of researchers at Baidu, in collaboration with Peng Cheng Laboratory (PCL), release PCL-BAIDU Wenxin (or “ERNIE 3.0 Titan”), a pre-training language model with 260 billion parameters. It is the world’s first knowledge-enhanced multi-hundred billion parameter model and its largest Chinese singleton model. \n You can read the short summary here: https://www.marktechpost.com/2021/12/29/baidu-and-pcl-team-introduce-ernie-3-0-titan-a-pre-training-language-model-with-260-billion-parameters/ \n Paper: https://arxiv.org/pdf/2112.12731.pdf\n ​\n https://preview.redd.it/19urwzn7cj881.png?width=1920&format=png&auto=webp&s=6a97e19e9fc4f5dde161e4a6ee17e3b43d86cc39\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rrgrji/baidu_and_pcl_team_introduce_ernie_30_titan_a/",
          "publishedOn": "2021-12-29T19:43:56.000Z",
          "wordCount": 310,
          "title": "Baidu And PCL Team Introduce ERNIE 3.0 Titan: A Pre-Training Language Model With 260 Billion Parameters"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rqkypl/neurips_2021_curated_papers_part_2/",
          "author": null,
          "description": "In part-2 , I have discussed following papers : \n  \nProbing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training\n \nAttention Bottlenecks for Multimodal Fusion\n \nAugMax: Adversarial Composition of Random Augmentations for Robust Training\n \nRevisiting Model Stitching to Compare Neural Representations\n  \nhttps://rakshithv-deeplearning.blogspot.com/2021/12/neurips-2021-curated-papers-part2.html\n    submitted by    /u/rakshith291  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rqkypl/neurips_2021_curated_papers_part_2/",
          "publishedOn": "2021-12-28T17:30:23.000Z",
          "wordCount": 149,
          "title": "NeurIPS 2021 - Curated papers - Part 2"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rqever/autoencoders_for_cifar10/",
          "author": null,
          "description": "Most of the Autoencoder examples/blogs use MNIST dataset as the implementation. I have trained an autoencoder on CIFAR-10 which you can refer here. There is a trade-off between making the CNN architecture deeper and the improvement of reconstruction loss/error. Or, using a VAE.\n Thoughts?\n    submitted by    /u/grid_world  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rqever/autoencoders_for_cifar10/",
          "publishedOn": "2021-12-28T12:33:46.000Z",
          "wordCount": 142,
          "title": "Autoencoders for CIFAR-10"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rq2ws6/researchers_compare_deep_learning_dl_algorithms/",
          "author": null,
          "description": "Viral keratitis, bacterial keratitis, fungal keratitis, and parasitic keratitis are all types of infectious keratitis. Bacterial Keratitis (BK) is a kind of Infectious Keratitis that is one of the most frequent and vision-threatening. Contact lens wear is the most prevalent risk factor for Bacterial Keratitis (BK), and it is becoming increasingly popular around the world for a variety of reasons, including exercise, cosmesis, and myopia management.\n BK is substantially more fulminant and painful in the clinical course than other Infectious Keratitis(s). A delayed diagnosis of Bacterial Keratitis (BK) can result in large-area corneal ulcerations, melting, and even perforation if not treated.\n In the case of Infectious Keratitis, timely detection and treatment of BK are vital goals. However,…",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rq2ws6/researchers_compare_deep_learning_dl_algorithms/",
          "publishedOn": "2021-12-28T01:24:52.000Z",
          "wordCount": 424,
          "title": "Researchers Compare Deep Learning (DL) Algorithms For Diagnosing Bacterial Keratitis via External Eye Photographs"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rpo3ou/creating_an_optimization_algorithm_for_cost/",
          "author": null,
          "description": "Is possible to find an article or an example of a new optimization algorithm for cost function for NN?\n    submitted by    /u/adilkolakovic  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rpo3ou/creating_an_optimization_algorithm_for_cost/",
          "publishedOn": "2021-12-27T13:58:57.000Z",
          "wordCount": 136,
          "title": "Creating an optimization algorithm for cost function for NN"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rpdfay/hypernerf_a_higherdimensional_representation_for/",
          "author": null,
          "description": "submitted by    /u/nickb  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rpdfay/hypernerf_a_higherdimensional_representation_for/",
          "publishedOn": "2021-12-27T03:23:37.000Z",
          "wordCount": 126,
          "title": "HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rodzks/getting_familiar_with_neural_nrtworks/",
          "author": null,
          "description": "Any good beginner books on the theory of machine learning systems / DNN\n    submitted by    /u/Abeokuta_  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rodzks/getting_familiar_with_neural_nrtworks/",
          "publishedOn": "2021-12-25T17:43:34.000Z",
          "wordCount": 197,
          "title": "Getting familiar with neural nrtworks"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rngq7k/artificial_vision_with_neocognitron_by_kunihiko/",
          "author": null,
          "description": "submitted by    /u/Wild-Dig-8003  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rngq7k/artificial_vision_with_neocognitron_by_kunihiko/",
          "publishedOn": "2021-12-24T07:10:50.000Z",
          "wordCount": 153,
          "title": "Artificial Vision with Neocognitron by Kunihiko Fukushima (the father of convolutional neural networks)"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rn0u4y/meta_ai_announces_the_beta_release_of_bean/",
          "author": null,
          "description": "Meta AI releases the beta version of Bean Machine, a probabilistic programming framework based on PyTorch that makes it simple to describe and learn about uncertainty in machine learning models used in various applications. Bean Machine makes it possible to create probabilistic models that are domain-specific. It also uses multiple autonomous, uncertainty-aware learning algorithms to learn about the model’s unseen features. Bean Machine gets an early beta version from Meta.\n Quick Read: https://www.marktechpost.com/2021/12/23/meta-ai-announces-the-beta-release-of-bean-machine-a-pytorch-based-probabilistic-programming-system-used-to-understand-the-uncertainty-in-the-machine-learning-models/ \n Documentation: https://beanmachine.org/ \n Tutorials: https://beanmachine.org/docs/tutorials/\n Meta Blog: https://research.facebook.com/blog/2021/12/introducing-bean-machine-a-probabilistic-programming-platform-built-on-pytorch/ \n ​\n https://preview.redd.it/xzovhjrzvb781.png?width=1920&format=png&auto=webp&s=1337f3b0f646fd6d888ce02363eb63a6d5257fb7\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rn0u4y/meta_ai_announces_the_beta_release_of_bean/",
          "publishedOn": "2021-12-23T17:36:52.000Z",
          "wordCount": 238,
          "title": "Meta AI Announces the Beta Release of ‘Bean Machine’: A PyTorch-Based Probabilistic Programming System Used to Understand the Uncertainty in the Machine Learning Models"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rms2pm/if_the_accuracy_of_my_network_is_zero_on_the_very/",
          "author": null,
          "description": "submitted by    /u/eva01beast  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rms2pm/if_the_accuracy_of_my_network_is_zero_on_the_very/",
          "publishedOn": "2021-12-23T09:37:21.000Z",
          "wordCount": 295,
          "title": "If the accuracy of my network is zero on the very first epoch, is there any point in letting the training run for the remaining epochs?"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rmelaz/face_recognition/",
          "author": null,
          "description": "I am currently working on a project that involves face verification. I want to use Azure Face API and I gotta say I haven’t understood the pricing. I would love if some of you can clarify. If say I have 200 faces and 2000 pictures these faces are in (1 face might be in N pictures), in order to find all the pictures that each face is in, that means I will have to run 400,000 transactions ? As in 2000 iterations per face? Or is there a smarter way to do it?\n I know there is an option to index faces but i do not fully understand that yet.\n Thank you for your help!\n    submitted by    /u/xPiexPie  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rmelaz/face_recognition/",
          "publishedOn": "2021-12-22T20:56:43.000Z",
          "wordCount": 297,
          "title": "Face recognition"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rm1e5h/managing_collaborative_machine_learning/",
          "author": null,
          "description": "Sharing machine learning experiments to compare its models is important when you're working with a team of engineers. You might need to get another opinion on an experiments results or to share a modified dataset or even share the exact reproduction of a specific experiment.\n The following tutorial goes through an example of sharing an experiment with DVC remotes: Running Collaborative Experiments - using DVC remotes to share experiments and their data across machines\n Setting up DVC remotes in addition to your Git remotes lets you share all of the data, code, and hyperparameters associated with each experiment so anyone can pick up where you left off in the training process. When you use DVC, you can bundle your data and code changes for each experiment and push those to a remote for somebody else to check out.\n    submitted by    /u/thumbsdrivesmecrazy  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rm1e5h/managing_collaborative_machine_learning/",
          "publishedOn": "2021-12-22T09:13:41.000Z",
          "wordCount": 247,
          "title": "Managing Collaborative Machine Learning Experiments - Guide"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rlx1zj/noob_needs_help/",
          "author": null,
          "description": "Ok so I’m new to neural networks, so I might not understand a lot of the technical terms. \n I want to show a NN some gameplay. But I don’t know how to. I can capture the keystrokes for each frame but I don’t know what to do next. \n Do I take screenshots and save those frames w the keystrokes to my disk? \n I kinda know how to use Cv2, Pillow, and the keyboard module. \n I’m really lost, so any help will be highly appreciated\n Ps- I want the model to learn how to play the game\n    submitted by    /u/findcureforautism  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rlx1zj/noob_needs_help/",
          "publishedOn": "2021-12-22T04:34:51.000Z",
          "wordCount": 217,
          "title": "Noob needs help"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rl2s1e/frist_promising_results_of_my_neuronal_denoising/",
          "author": null,
          "description": "submitted by    /u/Rindsroulade  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rl2s1e/frist_promising_results_of_my_neuronal_denoising/",
          "publishedOn": "2021-12-21T01:33:48.000Z",
          "wordCount": 208,
          "title": "Frist promising results of my neuronal denoising network 😍"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rk2egj/i_have_many_doubts_hereany_hints_on_how_to_solve/",
          "author": null,
          "description": "submitted by    /u/Own-Assistance58  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rk2egj/i_have_many_doubts_hereany_hints_on_how_to_solve/",
          "publishedOn": "2021-12-19T18:20:35.000Z",
          "wordCount": 519,
          "title": "I have many doubts here.Any hints on how to solve these problems???"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rk27b8/what_does_stylegan_do_to_prevent_mode_collapse/",
          "author": null,
          "description": "I will like to know what stylegan does during training to prevent mode collapse.\n    submitted by    /u/Virtual_Essay1216  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rk27b8/what_does_stylegan_do_to_prevent_mode_collapse/",
          "publishedOn": "2021-12-19T18:10:33.000Z",
          "wordCount": 131,
          "title": "What does stylegan do to prevent mode collapse?"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rize8m/jpeg_compression_artifacts_removal_ai_fbcnn/",
          "author": null,
          "description": "submitted by    /u/cloud_weather  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rize8m/jpeg_compression_artifacts_removal_ai_fbcnn/",
          "publishedOn": "2021-12-18T04:31:42.000Z",
          "wordCount": 116,
          "title": "JPEG Compression Artifacts Removal AI - FBCNN"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/riluxm/free_nlp_for_semantic_search_course_data/",
          "author": null,
          "description": "Hi all, the NLP for Semantic Search course that I've been working on has just been released, and today the latest chapter on data augmentation with SBERT has been released too!\n It's all completely free and covers everything you need to get started with building SotA language models for semantic similarity, from machine translation to question-answering, and more!\n Semantic search allows us to search language-based data based on the semantics or 'meaning' of a text, from machine translation to question-answering. It's how Google understands \"what time is it in NYC?\", and even allows us to search for images using text-based queries.\n It is in essence, a way for us to interact with machines in a more human way. NLP fits in as the 'semantic' in semantic search.\n Current chapters are: 1. Dense Vectors 2. Sentence Embeddings and Transformers 3. Training Sentence Transformers with Softmax Loss 4. Training Sentence Transformers with MNR Loss 5. Multilingual Sentence Transformers 6. Question Answering 7. Unsupervised Training for Sentence Transformers 8. (New) Data Augmentation With BERT\n Let me know what you think, I hope you enjoy it!\n    submitted by    /u/jamescalam  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/riluxm/free_nlp_for_semantic_search_course_data/",
          "publishedOn": "2021-12-17T17:03:14.000Z",
          "wordCount": 287,
          "title": "Free NLP for Semantic Search Course + Data Augmentation with SBERT (AugSBERT)"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/ri4ps4/understanding_alphazero_neural_networks/",
          "author": null,
          "description": "As a common and (sometimes) proven belief, deep learning systems seem to learn uninterpretable representations and are far from human understanding. Recently, some studies have highlighted the fact that this may not always be applicable, and some networks may be able to learn human-readable representations. Unfortunately, this ability could merely come from the fact that these networks are exposed to human-generated data. So, to demonstrate their ability to learn like humans (and not that they are simply memorizing human-created labels), it is necessary to test them without any label. \n Following this idea, the DeepMind and Google Brain teams, together with the 14th world chess champion Vladimir Kramnik, studied their creature AlphaZero from this point of view. AlphaZero is the descendant of AlphaGo, the super neural network that beat the world champion Lee Sedol in a best-of-five GO match, a turning point in the history of deep learning, as can also be seen in the wonderful Netflix documentary AlphaGo. \n Unlike AlphaGo, AlphaZero is trained through self-play (i.e., it learns to play competing against itself) and masters not only GO but also chess and shogi. This trait makes AlphaZero the perfect case study to explore this idea. Moreover, given the fact that it performs at a superhuman level, understanding its functionality is also particularly useful for highlighting unknown patterns which have never been discovered by chess theorists.\n Full Paper Summary by Leonardo Tanzi: https://www.marktechpost.com/2021/12/16/understanding-alphazero-neural-networks-superhuman-chess-ability/ \n Paper: https://arxiv.org/pdf/2111.09259.pdf\n https://preview.redd.it/t4mjebrm10681.png?width=808&format=png&auto=webp&s=58fcc96e8b1ae92469c26820528d0a5b31514365\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/ri4ps4/understanding_alphazero_neural_networks/",
          "publishedOn": "2021-12-17T00:44:05.000Z",
          "wordCount": 385,
          "title": "Understanding AlphaZero Neural Network’s SuperHuman Chess Ability (Summary of the Paper 'Acquisition of Chess Knowledge in AlphaZero')"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rhxivg/conceptualization_of_a_cnn/",
          "author": null,
          "description": "Hi all\n ​\n I've been reading about the differences between normal fully-connected feedforward networks and convolutional neural networks, and I'm wondering if I understand something about how they subsume one another.\n Is a CNN \"just\" an ANN where the inputs to the network are convolutions over the input?\n    submitted by    /u/snatchingthepiano  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rhxivg/conceptualization_of_a_cnn/",
          "publishedOn": "2021-12-16T19:01:26.000Z",
          "wordCount": 147,
          "title": "Conceptualization of a CNN"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rhv0yn/synthetic_time_series_data_generation/",
          "author": null,
          "description": "I want to generate time series tabular data. Most of generative deep learning models consists of VAE and/or GAN which are for most part relating to images, videos, etc.\n Can you please point me to relevant tutorial souces (if it includes code along with theory, all the more better) pertaining to synthethic time series data generation using deep learning models or other techniques?\n    submitted by    /u/grid_world  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rhv0yn/synthetic_time_series_data_generation/",
          "publishedOn": "2021-12-16T17:03:33.000Z",
          "wordCount": 167,
          "title": "Synthetic time series data generation"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rh5hwh/i_programmed_some_creatures_they_evolved/",
          "author": null,
          "description": "submitted by    /u/infernum___  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rh5hwh/i_programmed_some_creatures_they_evolved/",
          "publishedOn": "2021-12-15T18:17:07.000Z",
          "wordCount": 189,
          "title": "I programmed some creatures. They Evolved."
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rgz6cf/google_ai_proposes_temporal_fusion_transformer/",
          "author": null,
          "description": "Deep learning technologies, such as automatic learning of temporal dependence and automated handling of temporal structures like trends and seasonality, hold a lot of promise for time series forecasting. Most real-world datasets include a temporal component. Therefore projecting the future can be pretty beneficial. In time series machine learning, multi-horizon forecasting, or predicting variables-of-interest at several future time steps, is a critical challenge.\n Deep neural networks (DNNs) are increasingly being employed in multi-horizon forecasting, and they have been shown to outperform classic time series models. Unlike most models that focus on recurrent neural network (RNN) variants, recent works use attention-based layers to improve the selection of relevant time steps in the past beyond the inductive bias of RNNs – sequential ordered processing of information including. However, they frequently ignore the standard inputs in multi-horizon forecasting, assuming that all exogenous inputs are known in the future or ignoring crucial static variables.\n Quick Read: https://www.marktechpost.com/2021/12/15/google-ai-proposes-temporal-fusion-transformer-tft-an-attention-based-dnn-deep-neural-network-model-for-multi-horizon-forecasting/\n Paper: https://www.sciencedirect.com/science/article/pii/S0169207021000637\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rgz6cf/google_ai_proposes_temporal_fusion_transformer/",
          "publishedOn": "2021-12-15T13:31:51.000Z",
          "wordCount": 293,
          "title": "Google AI Proposes Temporal Fusion Transformer (TFT): An Attention-Based DNN (Deep Neural Network) Model For Multi-Horizon Forecasting"
        }
      ]
    },
    {
      "title": "Seita's Place",
      "feedUrl": "https://danieltakeshi.github.io/feed.xml",
      "siteUrl": "https://danieltakeshi.github.io/",
      "articles": [
        {
          "id": "https://danieltakeshi.github.io/2021/12/31/books-2021",
          "author": null,
          "description": "At the end of every year I have a tradition where I write summaries of the\nbooks that I read throughout the year. Here’s the following post with the rough\nset of categories:\nPopular Science (6 books)\nHistory, Government, Politics, Economics (6 books)\nBiographies / Memoirs (5 books)\nChina (5 books)\nCOVID-19 (2 books)\nMiscellaneous (7 books)\nI read 31 books this year. You can find the other blog posts from prior years\n(going back to 2016) in the blog archives.\nPopular Science\nThis also includes popular science, which means the authors might not be\ntechnically trained as scientists.\nWho We Are and How We Got Here: Ancient DNA and the New Science of the\nHuman Past (2018) is by famous geneticist and Harvard professor David\nReich. Scientific advances in analyzing DNA have allowed better analysis…",
          "link": "https://danieltakeshi.github.io/2021/12/31/books-2021",
          "publishedOn": "2021-12-31T23:00:00.000Z",
          "wordCount": 8348,
          "title": "Books Read in 2021"
        },
        {
          "id": "https://danieltakeshi.github.io/2021/12/22/my-information-diet/",
          "author": null,
          "description": "On July 03 2021, the subject of media and news sources came up in a\nconversation I had with someone over brunch when we were talking about media\nbias. I was asked: “what news do you read?” I regret that I gave a sloppy\nresponse that sounded like a worse version of: “uh, I read a variety of news …”\nand then I tried listing a few from memory. I wish I had given a crisper\nresponse, and since that day, I have thought about what that person has asked\nme every day.\nIn this blog post, I describe my information diet, referring to how I read\nand consume media to understand current events. Before getting to the actual\nlist of media sources, here are a few comments to clarify my philosophy and\nwhich might also preemptively address common objections.\nThere are too many sources and not enough time to r…",
          "link": "https://danieltakeshi.github.io/2021/12/22/my-information-diet/",
          "publishedOn": "2021-12-22T12:00:00.000Z",
          "wordCount": 1482,
          "title": "My Information Diet"
        }
      ]
    },
    {
      "title": "VITALab",
      "feedUrl": "https://vitalab.github.io/feed.xml",
      "siteUrl": "https://vitalab.github.io/",
      "articles": []
    },
    {
      "title": "Stories by Andrej Karpathy on Medium",
      "feedUrl": "https://medium.com/feed/@karpathy",
      "siteUrl": "https://medium.com/@karpathy?source=rss-ac9d9a35533e------2",
      "articles": []
    },
    {
      "title": "OpenAI",
      "feedUrl": "https://openai.com/blog/rss",
      "siteUrl": "https://openai.com/",
      "articles": [
        {
          "id": "61bb711a2a7e63003b0a2558",
          "author": "Jacob Hilton",
          "description": "We've fine-tuned GPT-3 to more accurately answer open-ended questions using a text-based web browser. Our prototype copies how humans research answers to questions online – it submits search queries, follows links, and scrolls up and down web pages. It is trained to cite its sources, which makes it",
          "link": "https://openai.com/blog/improving-factual-accuracy/",
          "publishedOn": "2021-12-16T17:05:45.000Z",
          "wordCount": 1371,
          "title": "WebGPT: Improving the factual accuracy of language models through web browsing"
        }
      ]
    },
    {
      "title": "Microsoft Research",
      "feedUrl": "https://www.microsoft.com/en-us/research/feed",
      "siteUrl": "https://www.microsoft.com/en-us/research",
      "articles": [
        {
          "id": "https://www.microsoft.com/en-us/research/?p=810376",
          "author": "Alyssa Hughes",
          "description": "From manufacturing and logistics to agriculture and transportation, the expansion of artificial intelligence (AI) in the last decade has revolutionized a multitude of industries—examples include enhancing predictive analytics on the manufacturing floor and making microclimate predictions so that farmers can respond and save their crops in time. The adoption of AI is expected to accelerate […]\nThe post EzPC: Increased data security in the AI model validation process appeared first on Microsoft Research.",
          "link": "https://www.microsoft.com/en-us/research/blog/ezpc-increased-data-security-in-the-ai-model-validation-process/",
          "publishedOn": "2022-01-12T18:05:06.000Z",
          "wordCount": 2069,
          "title": "EzPC: Increased data security in the AI model validation process"
        },
        {
          "id": "https://www.microsoft.com/en-us/research/?p=806026",
          "author": "Brenda Potts",
          "description": "KEAR (Knowledgeable External Attention for commonsense Reasoning)—along with recent milestones in computer vision and neural text-to-speech—is part of a larger Azure AI mission to provide relevant, meaningful AI solutions and services that work better for people because they better capture how people learn and work—with improved vision, knowledge understanding, and speech capabilities. At the center of these efforts is […]\nThe post Azure AI milestone: Microsoft KEAR surpasses human performance on CommonsenseQA benchmark appeared first on Microsoft Research.",
          "link": "https://www.microsoft.com/en-us/research/blog/azure-ai-milestone-microsoft-kear-surpasses-human-performance-on-commonsenseqa-benchmark/",
          "publishedOn": "2021-12-20T19:08:11.000Z",
          "wordCount": 1884,
          "title": "Azure AI milestone: Microsoft KEAR surpasses human performance on CommonsenseQA benchmark"
        },
        {
          "id": "https://www.microsoft.com/en-us/research/?p=804160",
          "author": "Alyssa Hughes",
          "description": "Neural Text-to-Speech—along with recent milestones in computer vision and question answering—is part of a larger Azure AI mission to provide relevant, meaningful AI solutions and services that work better for people because they better capture how people learn and work—with improved vision, knowledge understanding, and speech capabilities. At the center of these efforts is XYZ-code, […]\nThe post Azure AI milestone: New Neural Text-to-Speech models more closely mirror natural speech appeared first on Microsoft Research.",
          "link": "https://www.microsoft.com/en-us/research/blog/azure-ai-milestone-new-neural-text-to-speech-models-more-closely-mirror-natural-speech/",
          "publishedOn": "2021-12-17T23:22:31.000Z",
          "wordCount": 2424,
          "title": "Azure AI milestone: New Neural Text-to-Speech models more closely mirror natural speech",
          "enclosure": {
            "url": "https://www.microsoft.com/en-us/research/uploads/prod/2021/12/Jenny_NonTTS-recording.wav",
            "length": "229436",
            "type": "audio/wav"
          }
        },
        {
          "id": "https://www.microsoft.com/en-us/research/?p=804280",
          "author": "Lexie Hagen",
          "description": "Over the past 30 years, Microsoft Research has undergone a shift in how it approaches innovation, broadening its mission to include not only advancing the state of computing but also using technology to tackle some of the world’s most pressing challenges. That evolution has never been more prominent than it was during this past year. […]\nThe post Research at Microsoft 2021: Collaborating for real-world change appeared first on Microsoft Research.",
          "link": "https://www.microsoft.com/en-us/research/blog/research-at-microsoft-2021-collaborating-for-real-world-change/",
          "publishedOn": "2021-12-15T20:54:30.000Z",
          "wordCount": 4756,
          "title": "Research at Microsoft 2021: Collaborating for real-world change",
          "enclosure": {
            "url": "https://content.blubrry.com/microsoftresearch/MSR_002_LAMBDA_simon_andy_v2.mp3",
            "length": "57973784",
            "type": "audio/mpeg"
          }
        }
      ]
    },
    {
      "title": "Google AI Blog",
      "feedUrl": "http://feeds.feedburner.com/blogspot/gJZg",
      "siteUrl": "http://ai.googleblog.com/",
      "articles": [
        {
          "id": "http://ai.googleblog.com/2022/01/scaling-vision-with-sparse-mixture-of.html",
          "author": null,
          "description": "Posted by Carlos Riquelme, Research Scientist and Joan Puigcerver, Software Engineer, Google Research, Brain team \nAdvances in deep learning over the last few decades have been driven by a few key elements. With a small number of simple but flexible mechanisms (i.e., inductive biases such as convolutions or sequence attention), increasingly large datasets, and more specialized hardware, neural networks can now achieve impressive results on a wide range of tasks, such as image classification, machine translation, and protein folding prediction. \nHowever, the use of large models and datasets comes at the expense of significant computational requirements. Yet, recent works suggest that large model sizes might be necessary for strong generalization and robustness, so training large models whil…",
          "link": "http://ai.googleblog.com/2022/01/scaling-vision-with-sparse-mixture-of.html",
          "publishedOn": "2022-01-13T18:04:00.000Z",
          "wordCount": 2342,
          "title": "Scaling Vision with Sparse Mixture of Experts"
        },
        {
          "id": "http://ai.googleblog.com/2022/01/google-research-themes-from-2021-and.html",
          "author": null,
          "description": "Posted by Jeff Dean, Senior Fellow and SVP of Google Research, on behalf of the entire Google Research community\nOver the last several decades, I've witnessed a lot of change in the fields of machine learning (ML) and computer science. Early approaches, which often fell short, eventually gave rise to modern approaches that have been very successful. Following that long-arc pattern of progress, I think we'll see a number of exciting advances over the next several years, advances that will ultimately benefit the lives of billions of people with greater impact than ever before. In this post, I’ll highlight five areas where ML is poised to have such impact. For each, I’ll discuss related research (mostly from 2021) and the directions and progress we’ll likely see in the next few years. \n      …",
          "link": "http://ai.googleblog.com/2022/01/google-research-themes-from-2021-and.html",
          "publishedOn": "2022-01-11T19:01:00.006Z",
          "wordCount": 12011,
          "title": "Google Research: Themes from 2021 and Beyond"
        },
        {
          "id": "http://ai.googleblog.com/2021/12/a-scalable-approach-for-partially-local.html",
          "author": null,
          "description": "Posted by Karan Singhal, Senior Software Engineer, Google Research  \nFederated learning enables users to train a model without sending raw data to a central server, thus avoiding the collection of privacy-sensitive data. Often this is done by learning a single global model for all users, even though the users may differ in their data distributions. For example, users of a mobile keyboard application may collaborate to train a suggestion model but have different preferences for the suggestions. This heterogeneity has motivated algorithms that can personalize a global model for each user.  \n However, in some settings privacy considerations may prohibit learning a fully global model. Consider models with user-specific embeddings, such as matrix factorization models for recommender systems. Tr…",
          "link": "http://ai.googleblog.com/2021/12/a-scalable-approach-for-partially-local.html",
          "publishedOn": "2021-12-16T18:11:00.000Z",
          "wordCount": 2092,
          "title": "A Scalable Approach for Partially Local Federated Learning"
        },
        {
          "id": "http://ai.googleblog.com/2021/12/training-machine-learning-models-more.html",
          "author": null,
          "description": "Posted by Timothy Nguyen1, Research Engineer and Jaehoon Lee, Senior Research Scientist, Google Research  \nFor a machine learning (ML) algorithm to be effective, useful features must be extracted from (often) large amounts of training data. However, this process can be made challenging due to the costs associated with training on such large datasets, both in terms of compute requirements and wall clock time. The idea of distillation plays an important role in these situations by reducing the resources required for the model to be effective. The most widely known form of distillation is model distillation (a.k.a. knowledge distillation), where the predictions of large, complex teacher models are distilled into smaller models.  \n An alternative option to this model-space approach is dataset …",
          "link": "http://ai.googleblog.com/2021/12/training-machine-learning-models-more.html",
          "publishedOn": "2021-12-15T19:26:00.000Z",
          "wordCount": 2651,
          "title": "Training Machine Learning Models More Efficiently with Dataset Distillation"
        }
      ]
    },
    {
      "title": "fast.ai",
      "feedUrl": "https://www.fast.ai/atom.xml",
      "siteUrl": "http://www.fast.ai/atom.xml",
      "articles": []
    },
    {
      "title": "Reinforcement Learning",
      "feedUrl": "https://www.reddit.com/r/reinforcementlearning/.rss?format=xml",
      "siteUrl": "https://www.reddit.com/r/reinforcementlearning/?format=xml",
      "articles": [
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s39ipa/tips_for_remembering_drl_algorithms_for_newbies/",
          "author": null,
          "description": "Hi all,\n I am studying the most popular DRL algorithms (NFQ, DQN, DDQN, Dueling DDQN, Reinforce, VPG, A3C, A2C, DDPG, TD3, SAC, PPO) and I was wondering a couple of things.\n A) What is the level of detail that you need to remember in order to use those algs efficiently? In other words, do you need to remember every bit of them?\n B) Do you have tricks and tips to remember them more easily, maybe by pointing me at some maps that highlight connections between them \n Thanks!\n    submitted by    /u/No_Possibility_7588  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s39ipa/tips_for_remembering_drl_algorithms_for_newbies/",
          "publishedOn": "2022-01-13T21:00:13.000Z",
          "wordCount": 287,
          "title": "Tips for remembering DRL algorithms for newbies"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s314d9/neural_networks_in_unity_3d/",
          "author": null,
          "description": "For a school project I need to create an agent that learns how to navigate a environment using reinforcement learning (DQN, policy gradient). To do this I need to be able to create a neural network which I will need to train. \n My question now is if there are any Neural Network libraries that I can use for creating and training neural networks. I know unity has the ML-agents toolkit but this does not seem to fit my need. \n Thanks!\n    submitted by    /u/IssueRepresentative5  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s314d9/neural_networks_in_unity_3d/",
          "publishedOn": "2022-01-13T14:48:17.000Z",
          "wordCount": 366,
          "title": "Neural networks in Unity 3D"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s2ypo1/what_is_the_best_approach_to_pomdp_environment/",
          "author": null,
          "description": "Hello, I have some questions about the POMDP environment. \n First, I thought that in a POMDP environment, a policy-based method would be better than a value-based method. For example, Alice Grid World. Is it generally correct?\n Second, when training a limited view agent in a tabular environment, I expected the rppo agent to perform better than cnn-based ppo. But it didn't. I used this repository that was already implemented and saw slow learning based on this. \n When I trained a Starcraft II agent, there are really huge differences between those architecture. So I just wonder your opinions. Very Thanks!\n    submitted by    /u/Spiritual_Fig3632  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s2ypo1/what_is_the_best_approach_to_pomdp_environment/",
          "publishedOn": "2022-01-13T12:49:38.000Z",
          "wordCount": 568,
          "title": "what is the best approach to POMDP environment?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s2xanb/issue_with_modifying_mujoco_environment/",
          "author": null,
          "description": "I am trying to modify the inverteddoublependulum environment in mujoco and tried to changed the xml parameters in the gym envs folder, however when i load the environment in my script, it still loads back the same un-modified environment. \n I am creating a closed-loop kinematic chain by adding another cart with poles connecting to the original cart. \n Am i missing a step such as using some wrapper or having to register the environment?\n Thanks, its first first time doing this.\n    submitted by    /u/Lifeisunfair210  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s2xanb/issue_with_modifying_mujoco_environment/",
          "publishedOn": "2022-01-13T11:24:11.000Z",
          "wordCount": 195,
          "title": "issue with modifying mujoco environment"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s2paqj/why_do_they_call_it_greedy_with_respect_to_v/",
          "author": null,
          "description": "Hi everyone I just started reading 'Reinforcement Learning' by Richard S. Sutton and Andrew G. Barto and I hit a point where I'm not really sure why they describe it this way. (And it's not just this book, I see this expression everywhere)\n In chapter 3.6 they compute the optimal state value function by the following equation.\n Bellman optimality equation for state value\n It says afterwards how to choose the best policy with the optimal state value function\n  \nIf you have the optimal value function, v* then the actions that appear best after a one-step search will be optimal actions. Another way of saying this is that any policy that is greedy with respect to the optimal evaluation function v\\* is an optimal policy.\n  \nIn chapter 4.2 where they describe policy improvement, they give us the following equation for updating the policy based on v_pi.\n Policy update based on v_pi\n  \nThe greedy policy takes the action that looks best in the short term—after one step of lookahead—according to v_pi ... The process of making a new policy that improves on an original policy, by making it greedy with respect to the value function of the original policy, is called policy improvement.\n  \nTo me, when you say \"policy that is greedy with respect to v\", it sounds like you just simply choose the action that moves the agent to the next state with the highest state value. But this cannot be true because you have to choose the action that maximizes the following function of v, not simply v itself.\n Function of v you wanna maximize\n And the equation above is q_pi(s, a). So shouldn't the correct phrase be \"greedy with respect to q\" and not v?\n I am wondering if I'm just overthinking things at this point...\n    submitted by    /u/peterpan1998  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s2paqj/why_do_they_call_it_greedy_with_respect_to_v/",
          "publishedOn": "2022-01-13T03:30:39.000Z",
          "wordCount": 999,
          "title": "Why do they call it 'greedy with respect to V'?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s2matm/did_anyone_try_pandagym/",
          "author": null,
          "description": "The acquisition of Mujoco makes Openai to remove the robotics from their repo. I had no choice but to find an alternative. Then I found https://github.com/qgallouedec/panda-gym which is built on PyBullet.\n    submitted by    /u/chezhek  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s2matm/did_anyone_try_pandagym/",
          "publishedOn": "2022-01-13T01:06:44.000Z",
          "wordCount": 174,
          "title": "Did anyone try Panda-Gym?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s2cle5/roman_ring_deepmind_talks_starcraft_alphastar_on/",
          "author": null,
          "description": "submitted by    /u/Infamous-Editor5131  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s2cle5/roman_ring_deepmind_talks_starcraft_alphastar_on/",
          "publishedOn": "2022-01-12T18:02:10.000Z",
          "wordCount": 164,
          "title": "Roman Ring (DeepMind) talks StarCraft, AlphaStar on TalkRL?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s27ozk/need_explanation_of_some_basic_code_in_spinningup/",
          "author": null,
          "description": "Hi. I have a basic query regarding, code in spinup utils. In file test_policy.py, L87. \n get_action = lambda x : sess.run(action_op, feed_dict={model['x']: x[None,:]})[0]\n What does the 'x[None,:]' part do/mean? Sorry, I am a beginner and this might be a trivial question. Would appreciate any possible help.\n    submitted by    /u/underconfidant_soul  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s27ozk/need_explanation_of_some_basic_code_in_spinningup/",
          "publishedOn": "2022-01-12T14:34:19.000Z",
          "wordCount": 246,
          "title": "Need explanation of some basic code in Spinningup : Spinup utils"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s24stk/are_neural_networks_necessary_to_update_the/",
          "author": null,
          "description": "submitted by    /u/aabra__ka__daabra  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s24stk/are_neural_networks_necessary_to_update_the/",
          "publishedOn": "2022-01-12T12:08:35.000Z",
          "wordCount": 288,
          "title": "Are neural networks necessary to update the actor-critic model parameters ? Is there other method to perform the same ."
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s24e1q/is_vanilla_policy_gradient_algorithm_better_than/",
          "author": null,
          "description": "Hi! I've been studying Policy Gradient algorithms and implementing them and I've found that with the same amount of data (~5000 samples in total over multiple episodes, 50 \"epochs\") the simple and basic VPG algorithm with \"reward-to-go\" function converges to average return of 200 on CartPole-v0 env much, MUCH faster than A2C.\n Is that how it's supposed to be? Shouldn't the A2C version (with less variance, as the authors claim) be better at convergence?\n Edit: I've just found a bug in my code, which has improved my results, but still made it comparable to simple VPG.\n Edit 2: I apologize for even asking the question... It seems that playing around with hyper parameters can change situation drastically. \n    submitted by    /u/Decent-Ad9135  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s24e1q/is_vanilla_policy_gradient_algorithm_better_than/",
          "publishedOn": "2022-01-12T11:44:41.000Z",
          "wordCount": 443,
          "title": "Is Vanilla Policy Gradient algorithm better than Advantage Actor Critic (A2C) ?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s1vk9l/ways_for_representing_environments/",
          "author": null,
          "description": "Hello there :D,\n I am currently working on a RL environment of a agent (robot/drone) moving around in the environment to look for a target. Currently I use PPO with Convolutional nets (CNN), where the observations are maps of the area (something like grid world), showing where walls, agents positions, etc are. In a certain timestep the agent collects a stack of maps (3-4) that are fed to the network.\n I want to move to using LSTM, and I was wondering what other ways can I use to model the observations? Keep in mind my observations should contain agents positions (an agent observes its own location and the locations of other agents) and obstacles in the environment (walls determined by position and dimensions). Is there a better way of modeling these than CNN?\n I though of using a normal feed forward network, and just feed positions directly and use embeddings for the walls, but I'm not sure how scalable that would be for increasing number of agents (more agents = more input), while in CNN the map dimensions stays the same and just more pixels are set to 1 indicating the agents' locations.\n Any insights?\n    submitted by    /u/AhmedNizam_  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s1vk9l/ways_for_representing_environments/",
          "publishedOn": "2022-01-12T03:04:23.000Z",
          "wordCount": 430,
          "title": "Ways for representing environments"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s1sz8q/d_interview_this_team_won_the_minecraft_rl_basalt/",
          "author": null,
          "description": "submitted by    /u/gwern  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s1sz8q/d_interview_this_team_won_the_minecraft_rl_basalt/",
          "publishedOn": "2022-01-12T01:01:25.000Z",
          "wordCount": 373,
          "title": "[D] Interview - This Team won the Minecraft RL BASALT Challenge! (Paper Explanation & Interview with the authors)"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s1nxpp/immediate_reward_and_final_reward/",
          "author": null,
          "description": "Hello guys\n I hope you are doing well.\n I am struggling with designing a deep reinforcement learning model, I want to make a model that works with immediate rewards and a final reward at the end of each episode. \n I am wondering if the final reward must have the same distribution as the immediate rewards? because when I designed an immediate reward that is in the range of [-3,-2] and a final reward in the range of [-1,0] , the agent learns to minimize the reward as the figure below.\n My second question is how the agent differs the final reward and the immediate one?\n Thank you !\n ​\n https://preview.redd.it/pggvt2rzk4b81.png?width=467&format=png&auto=webp&s=d90b250c2242bc9d8936c8958f3a8258fd92f70a\n    submitted by    /u/GuavaAgreeable208  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s1nxpp/immediate_reward_and_final_reward/",
          "publishedOn": "2022-01-11T21:20:18.000Z",
          "wordCount": 309,
          "title": "Immediate reward and Final reward"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s1h7c1/how_to_improve_ppo_on_bipedalwalkerv3/",
          "author": null,
          "description": "I implemented basic PPO on OpenAI's BipedalWalker-v3 and after tuning hyperparameters i got maximum mean of 230 over last 100 episodes. Do you know some improvements to my basic PPO implementation I can use to get better result or closer to reward of 300?\n    submitted by    /u/TheGuy839  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s1h7c1/how_to_improve_ppo_on_bipedalwalkerv3/",
          "publishedOn": "2022-01-11T16:40:46.000Z",
          "wordCount": 220,
          "title": "How to improve PPO on BipedalWalker-v3?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s1g1nb/is_there_any_method_to_obtain_the_true_optimal/",
          "author": null,
          "description": "submitted by    /u/Rich_Beautiful  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s1g1nb/is_there_any_method_to_obtain_the_true_optimal/",
          "publishedOn": "2022-01-11T15:52:12.000Z",
          "wordCount": 194,
          "title": "Is there any method to obtain the true optimal Q-function for low dimensional continuous problem (for exmaple, cartpole)?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s1dfr6/first_try_at_blogging_an_introduction_to_concepts/",
          "author": null,
          "description": "Tried my hand at blogging after procrastinating a lot. Hope y'all find it interesting\n Introduction to Concepts in Reinforcement Learning\n For any corrections, criticisms or clarifications feel free to hmu :)\n    submitted by    /u/needANewPlague_  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s1dfr6/first_try_at_blogging_an_introduction_to_concepts/",
          "publishedOn": "2022-01-11T13:55:00.000Z",
          "wordCount": 227,
          "title": "First try at Blogging! An Introduction to Concepts in Reinforcement Learning"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s1ax79/what_advantage_does_actor_critic_methods_deep_rl/",
          "author": null,
          "description": "submitted by    /u/aabra__ka__daabra  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s1ax79/what_advantage_does_actor_critic_methods_deep_rl/",
          "publishedOn": "2022-01-11T11:34:22.000Z",
          "wordCount": 262,
          "title": "What advantage does Actor critic methods /Deep RL give over Naive Policy gradient algorithm like Reinforce."
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s18hjr/help_understanding_ppo_algorithm/",
          "author": null,
          "description": "i'm having a hard time understanding the PPO algorithm (intuitively). here are the questions i have:\n  \nwhat is the gist of calculating the advantage? i know that it's calculating the \"betterness\" of the state/action compared to what the network was actually expecting, but then why is the advantage formula: reward + values_of_next_state - value_of_current_state? how does this tell us how advantageous the current state/action is?\n if the value network is supposed to output the expected/discounted returns (i hope i am not wrong here, or is it supposed to return the reward for just the current state?) then how does it stack up against continuous obs spaces? (eg: in a car game where the input is pixels, i can sample a random state from the whole game then how is it supposed to calculate the discounted returns? it doesn't know how far the map goes further or for that matter how far it is from the start of the track).\n how does the gradient affect the action? meaning in a multi discrete action space how does the gradient affect only the action that i took? instead of backproping the grad to all actions?\n  \nand finally this is how i'm currently calculating returns and advantages (for a continuous action space and pixel based inputs to the CNN):\n self.values[self.ptr] = next_value deltas = self.rewards + self.gamma * self.values[1:] - self.values[:-1] self.advantage = self.discounted_sum(deltas, self.gamma * self.lam) self.returns = self.discounted_sum(self.rewards, self.gamma) \n am i doing it right?\n    submitted by    /u/jedi1026  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s18hjr/help_understanding_ppo_algorithm/",
          "publishedOn": "2022-01-11T08:47:59.000Z",
          "wordCount": 662,
          "title": "help understanding ppo algorithm"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s175v4/how_do_i_use_a_baselines_algorithm_such_as_a2c_or/",
          "author": null,
          "description": "Hi. I used neat-python to make an AI for Pokemon Red, but it doesn't get very far. The reward function I made gives it 10 reward every time the RAM values change, as checked every 10 frames. (I made a list of what RAM values it should watch for). I did this because I wanted to try a \"curiosity\" reward.\n Since the NEAT AI isn't getting very far, I decided to try a different algorithm that is not genetic, hoping that it will perform better. I have my eyes on A2C and PPO but I cannot find a way to make a custom reward function for them. It seems that they use the environment's reward function, which seems to be only editable in Lua.\n Can someone give me pointers on how to implement a custom reward function for reinforcement learning that is not NEAT? I just need it to take in a list of inputs, output a list, and learn from those and the rewards it gets. I've tried to code the reward function in Lua but I was having issues, so I'd prefer it to be in Python.\n    submitted by    /u/Unsightedmetal6  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s175v4/how_do_i_use_a_baselines_algorithm_such_as_a2c_or/",
          "publishedOn": "2022-01-11T07:20:23.000Z",
          "wordCount": 350,
          "title": "How do I use a Baselines algorithm such as A2C or PPO, but with a custom reward function? (OpenAI Retro)"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s12emc/whats_the_difference_between_feature_vectors_and/",
          "author": null,
          "description": "I encountered both words frequently. They seem to refer to same things. I am so confused. Anybody care to help clarify? Thanks.\n    submitted by    /u/Asleep_Donut1382  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s12emc/whats_the_difference_between_feature_vectors_and/",
          "publishedOn": "2022-01-11T02:59:35.000Z",
          "wordCount": 275,
          "title": "Whats the difference between feature vectors and state space reinforcement learning?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s0k590/multiple_action_spaces/",
          "author": null,
          "description": "Hey guys!\n I´m wondering if we can apply Reinforcement Learning where an agent has two action spaces and at time t the agent has to select one of the action spaces (A or B).\n If it selects A there are n tasks that the agent must select one of them. The same for action space B, there are m choices that the agent must select among them.\n It´s a complicated problem. All I want is to know if this is possible?\n    submitted by    /u/LeatherCredit7148  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s0k590/multiple_action_spaces/",
          "publishedOn": "2022-01-10T13:34:08.000Z",
          "wordCount": 463,
          "title": "Multiple Action Spaces"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s0ig21/qlearning_with_shortterm_vs_longterm_rewards/",
          "author": null,
          "description": "Hey guys, I have implemented and applied the Q-learning algorithm to the simple, grid environment. I defined terminating states, one with positive reward and others with negative rewards. And the training worked pretty well. Now, I wanted to enhance the process by adding the state with half reward,i.e., now there are 3 types of terminating states - a state with the biggest positive reward(100), a state with half of the reward(50), and the state with negative rewards(-100). As I said, they all terminate the process. However, when I test the trained agent, sometimes the agent goes to the state with half reward. So, the process is not as efficient as it was before. Could someone give me any tips on how to approach this problem? Is Q-learning a good approach for short-term vs long-term rewards? Thank you in advance!\n    submitted by    /u/studentani  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s0ig21/qlearning_with_shortterm_vs_longterm_rewards/",
          "publishedOn": "2022-01-10T11:59:16.000Z",
          "wordCount": 520,
          "title": "Q-learning with short-term vs long-term rewards"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s0ffbi/can_you_list_all_the_us_universities_that_you/",
          "author": null,
          "description": "Thank you.\n    submitted by    /u/Ok-Reaction-1515  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s0ffbi/can_you_list_all_the_us_universities_that_you/",
          "publishedOn": "2022-01-10T08:44:09.000Z",
          "wordCount": 369,
          "title": "Can you list all the US universities that you know that do research in reinforcement learning?(other than Stanford/MIT/Berkeley/UW/CMU)"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s09668/question_why_does_my_robot_arm_curls_up/",
          "author": null,
          "description": "Hi,\n I have an issue with my algorithm which i can't seem to find where the issue lies at. I'm using RLBench for the environment with Option Critic and DDPG as the agent. I'm trying out the reach target task. I'm not sure why but when i looked at the results, the arm ends up curling/trying to return to its initial position when it tries to solve the task at hand. This happened for the open box task as well where it tries to reach the box lid to open it before it swerves and curls up the robot's base. Does anyone have any insight as to what is causing this issue?\n https://reddit.com/link/s09668/video/7xwaris1zra81/player\n https://reddit.com/link/s09668/video/44dauc99yra81/player\n    submitted by    /u/Temptedtosleep  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s09668/question_why_does_my_robot_arm_curls_up/",
          "publishedOn": "2022-01-10T02:55:53.000Z",
          "wordCount": 428,
          "title": "Question: Why does my robot arm curls up?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/s07hpk/how_different_are_the_code_in_unity_and_isaac_sim/",
          "author": null,
          "description": "I am trying to make RL algorithm testing on Unity but thinking of changing to Isaac gym, how different is the code? \n Do I have to change a lot?\n    submitted by    /u/baegyutae  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/s07hpk/how_different_are_the_code_in_unity_and_isaac_sim/",
          "publishedOn": "2022-01-10T01:34:33.000Z",
          "wordCount": 161,
          "title": "How different are the code in unity and Isaac sim"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rzrq9l/what_is_the_role_of_target_actor_and_target_critic/",
          "author": null,
          "description": "Could you explain the role of target actor and target critics in DRL?\n Thanks\n    submitted by    /u/No_Possibility_7588  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rzrq9l/what_is_the_role_of_target_actor_and_target_critic/",
          "publishedOn": "2022-01-09T13:29:33.000Z",
          "wordCount": 454,
          "title": "What is the role of target actor and target critic?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rzk4zv/smoother_movements_for_robotbased_rl/",
          "author": null,
          "description": "Hello there,\n I am working on a RL environment where an agent moves around in a certain area. At a certain time step, the agent needs to choose the direction of movement (out of 8 choices: up down right left and diagonally). I do get the performance I am expecting in terms of reward/task completion, but the agent's movement is just not that smooth (see image below). What are my options here? \n  \nI tried continuous action space (2 actions representing forces set on x and y axis), but that did not work really well.\n  \nhttps://preview.redd.it/7ihp4t7kila81.png?width=444&format=png&auto=webp&s=585cd280e7cb1c2cf30ab7fe1cf22565eb2409b7\n    submitted by    /u/AhmedNizam_  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rzk4zv/smoother_movements_for_robotbased_rl/",
          "publishedOn": "2022-01-09T05:13:01.000Z",
          "wordCount": 475,
          "title": "Smoother movements for robot-based RL"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rzijms/representing_multiple_actions_using_gym/",
          "author": null,
          "description": "Hello Everyone,\n I am trying to implement a RL agent using using PPO with actor/critic. The agent has to move in an x-y plane by setting 2 discretized forces (2 actions) along its x and y axis. Initially I thought I need two output heads for my actor network, one for each action. However I came across this work by OpenAI, where they have a similar agent. They however use one output head for the movement action (along x y and z), where the action has a \"multidiscrete\" type. Any idea how this works? I have tried to understand it from the gym code but I dont get what \"multidiscrete\" does? Is it a way of encoding all the combinations of the different actions?\n Any help/insight is really appreciated\n    submitted by    /u/AhmedNizam_  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rzijms/representing_multiple_actions_using_gym/",
          "publishedOn": "2022-01-09T03:45:32.000Z",
          "wordCount": 368,
          "title": "representing multiple actions using gym multi-discrete"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rz4wxp/hiring_my_team_is_hiring_an_2_interns_at_intel/",
          "author": null,
          "description": "Looking for 2 interns for summer. There are few pre-reqs.\n Must be in US.\n Must enroll in a Ph.D program (Sorry no MS and BS students)\n Must have a track record for doing RL research, i.e 2 to 3 publications at least.\n If you are interested, please send me a link to your resume. \n Thanks\n Edit: hiring 2 interns\n    submitted by    /u/schrodingershit  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rz4wxp/hiring_my_team_is_hiring_an_2_interns_at_intel/",
          "publishedOn": "2022-01-08T17:24:48.000Z",
          "wordCount": 560,
          "title": "Hiring: My team is hiring an 2 interns at Intel. Read post description"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/ryyekc/offline_rl_using_policy_gradients/",
          "author": null,
          "description": "I have a dataset of events that take place in football (soccer) games. This is being framed as a reinforcement learning issue by defining episodes to be the events (pass, shoot, dribble, etc) that take place when a team gains possession and loses it (by shooting, losing possession, etc). \n I have created a simple NN that estimates what the next action will be from the possible action space as a probability distribution. This NN was trained on the historical data from past matches. I would like to know if there are any good pointers into implementing Policy Gradients to nudge the probability distribution learned by the NN in the direction of some reward function that I define. Ideally the advice would be for tensorflow as that is what I am familiar with.\n    submitted by    /u/uom_questions  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/ryyekc/offline_rl_using_policy_gradients/",
          "publishedOn": "2022-01-08T12:04:10.000Z",
          "wordCount": 249,
          "title": "Offline RL using Policy Gradients"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rystbe/can_a_supervised_ann_detect_a_cat_and_a_dog_or/",
          "author": null,
          "description": "So a NN is good at scanning a thousand labelled pictures of a cat and then when it see's a cat it has a high likelihood of knowing it's a cat (if it is trained right).\n Can that same nn (with same or similar configuration) also determine a cat from a dog from a range of random animals, assuming weights were adjusted and training redone? Would I need two NN's for that?\n Does first NN think it's a cat? No? Okay - does 2nd NN think it's a dog? Yes!\n    submitted by    /u/Togfox  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rystbe/can_a_supervised_ann_detect_a_cat_and_a_dog_or/",
          "publishedOn": "2022-01-08T05:52:37.000Z",
          "wordCount": 464,
          "title": "Can a supervised ANN detect a cat and a dog or just a cat?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/ryp4k7/rl_finance_focused_study_group/",
          "author": null,
          "description": "Hey everyone - I am looking for people (preferably MSc or PhD students/ grads) who are interested to learn applied RL in finance. I'm looking to create a group of 5-8 people to meet weekly or biweekly for around 1-2hrs to discuss papers and potential projects that we can collaborate on.\n Please let me know if interested.\n    submitted by    /u/ivan_426  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/ryp4k7/rl_finance_focused_study_group/",
          "publishedOn": "2022-01-08T02:33:22.000Z",
          "wordCount": 399,
          "title": "RL (finance focused) Study Group"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/ry84c4/reinforcement_learning_for_the_real_world/",
          "author": null,
          "description": "submitted by    /u/bendee983  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/ry84c4/reinforcement_learning_for_the_real_world/",
          "publishedOn": "2022-01-07T14:05:38.000Z",
          "wordCount": 127,
          "title": "Reinforcement learning for the real world"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/ry7igx/cant_find_where_the_agent_is_in_the_code_from/",
          "author": null,
          "description": "Hi all, \n I am trying to look at the code from a paper (https://github.com/anirudh9119/shared_workspace/tree/main/Triangle). I can code fluently and I know RL quite well in theory, the problem is I don't have much hands-on experience yet. Perhaps this is why I get confused when looking at this repo, not because I don't understand how the algorithm works, but because I can't find the code for it. Specifically, I would like to take the code for this algorithm: \n https://preview.redd.it/vip5wrq3p9a81.png?width=957&format=png&auto=webp&s=4735929cbbd651d1d26dcbdfcb33b2a99be28c16\n Naively, I would expect to see one file with all the classes and functions needed for this agent, and then another file with the agent imported and the reinforcement learning task performed in a certain environment. Instead, in this repo there are so many files with so many classes and so many functions, that I get lost and I can't identify which code refers to this algorithm. I can't see where the agent is, I can't see where the task is performed and so on. \n I don't understand if the repo is actually messy, or if it's just due to the fact that I'm not interpreting it in the right way. \n Thanks!\n    submitted by    /u/No_Possibility_7588  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/ry7igx/cant_find_where_the_agent_is_in_the_code_from/",
          "publishedOn": "2022-01-07T13:36:31.000Z",
          "wordCount": 522,
          "title": "Can't find where the agent is in the code from this paper (I am a newbie)"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/ry5jot/what_is_the_current_sota_for_offline_rl/",
          "author": null,
          "description": "Hi everyone!\n I'm mostly interested in Offline RL approaches for environments with distribution shift. I'm reading Decision Transformer: Reinforcement Learning via Sequence Modeling (https://arxiv.org/abs/2106.01345) paper, and was wondering what would be the benchmark / SOTA right now?\n    submitted by    /u/fusionquant  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/ry5jot/what_is_the_current_sota_for_offline_rl/",
          "publishedOn": "2022-01-07T11:39:13.000Z",
          "wordCount": 312,
          "title": "What is the current SOTA for Offline RL?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/ry59xb/grab_your_digital_copy_of_the_tensorflow_workshop/",
          "author": null,
          "description": "Packt has Published \"The TensorFlow Workshop \"\n ​\n Grab your digital copy now if you feel you are interested.\n ​\n As part of our marketing activities, we are offering free digital copies of the book in return for unbiased feedback in the form of a reader review.\n ​\n Get started with TensorFlow fundamentals to build and train deep learning models with real-world data, practical exercises, and challenging activities.\n ​\n Here is what you will learn from the book:\n ​\n  \nGet to grips with TensorFlow’s mathematical operations\n \nPre-process a wide variety of tabular, sequential, and image data\n \nUnderstand the purpose and usage of different deep learning layers\n \nPerform hyperparameter-tuning to prevent overfitting of training data\n \nUse pre-trained models to speed up the development of learning models\n \nGenerate new data based on existing patterns using generative models \n \n ​\n ## Key Features\n ​\n * Understand the fundamentals of tensors, neural networks, and deep learning\n * Discover how to implement and fine-tune deep learning models for real-world datasets\n * Build your experience and confidence with hands-on exercises and activities\n ​\n Please comment below or DM me for more details\n    submitted by    /u/RoyluisRodrigues  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/ry59xb/grab_your_digital_copy_of_the_tensorflow_workshop/",
          "publishedOn": "2022-01-07T11:22:07.000Z",
          "wordCount": 308,
          "title": "Grab your digital copy of The TensorFlow Workshop"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/ry0n9q/how_to_draw_the_path_of_the_end_effector_in/",
          "author": null,
          "description": "I’m working on a project where my robotic arm should trace a hand written letter. For me to show that my arm is indeed tracing out the letter it would be better if I can draw a line as the end effector moves. Does anyone know how can I do that?\n    submitted by    /u/the_loner_98  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/ry0n9q/how_to_draw_the_path_of_the_end_effector_in/",
          "publishedOn": "2022-01-07T06:17:25.000Z",
          "wordCount": 192,
          "title": "How to draw the path of the end effector in mujoco_py"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rxxm72/hidden_agenda_a_social_deduction_game_with/",
          "author": null,
          "description": "submitted by    /u/gwern  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rxxm72/hidden_agenda_a_social_deduction_game_with/",
          "publishedOn": "2022-01-07T03:38:54.000Z",
          "wordCount": 165,
          "title": "\"Hidden Agenda: a Social Deduction Game with Diverse Learned Equilibria\", Kopparapu et al 2022 {DM} (sus)"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rxujo8/defining_actorcritic_networks_under_one_class/",
          "author": null,
          "description": "Hello everyone,\n I am currently using a PPO implementation (using pytorch) that has the actor and critic networks defined under one class (Class ActorCritic(nn.Module)), where each network is defined using nn.sequential (self.actor = nn.sequential ... and self.critic=nn.sequential).\n Is there a way to define both networks in the same class but using the forward method instead of nn.sequential?\n    submitted by    /u/AhmedNizam_  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rxujo8/defining_actorcritic_networks_under_one_class/",
          "publishedOn": "2022-01-07T01:13:53.000Z",
          "wordCount": 337,
          "title": "Defining actor/critic networks under one class"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rxdv2w/help_with_my_pytorch_implementation_of_ppo/",
          "author": null,
          "description": "Hi all,\n I implemented PPO using PyTorch here. As is suggested, I was trying it on very simple environment (CartPole-v1).\n My PPO doesn't seem to be very impressive. The reward seems to saturate somewhere around 25, while the best it can go is around 190s. Here is benchmark of all my experiments. There exist various implementations with various set of hyperparameters, and other improvement choices (Advantage using GAE, Clip critic loss, and so on). I have tried various mix of them, but sadly nothing seems to improve it.\n The various resources I have referred are:\n Phil's video: https://youtu.be/hlv79rcHws0video by wandb: https://youtu.be/MEt6rrxH8W4Hyperparameters Deep Reinforcement Learning that Matters: https://arxiv.org/pdf/1709.06560.pdf IMPLEMENTATION MATTERS IN DEEP POLICY GRADIENTS: A CASE STUDY ON PPO AND TRPO: https://openreview.net/attachment?id=r1etN1rtPB&name=original_pdf\n Can you help me getting it trained on this simple environment.I have implemented it in a way that it should be easy to run and understand.\n    submitted by    /u/harshraj22  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rxdv2w/help_with_my_pytorch_implementation_of_ppo/",
          "publishedOn": "2022-01-06T12:50:24.000Z",
          "wordCount": 349,
          "title": "Help with my PyTorch implementation of PPO"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rxan53/normalizing_advantage_estimates_in_ppo/",
          "author": null,
          "description": "Hello everybody!\n One implementation detail of PPO is to normalize the advantage estimates. These are usually normalized using the mean and std so that the mean is 0 and the std is 1. Now, I came across 3 different implementations that apply the normalization to different amounts of data. More precisely:\n  \nNormalizing advantages across single episodes (kamarl)\n Normalizing advantages across single mini batches (neroRL)\n Normalizing advantages across the entire batch (SpinningUp)\n  \nDo you guys think that this detail has a higher impact?\n My first intuition is that the more data is used for normalization, the smoother the normalized advantages are. So when normalizing across single episodes, this could lead to a high variance causing harmfully large parameter updates, whereas the smooth normalization could hinder learning.\n    submitted by    /u/LilHairdy  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rxan53/normalizing_advantage_estimates_in_ppo/",
          "publishedOn": "2022-01-06T09:37:56.000Z",
          "wordCount": 407,
          "title": "Normalizing advantage estimates in PPO"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rx638n/would_you_fine_tune_the_hyperparameters_or_modify/",
          "author": null,
          "description": "submitted by    /u/Rich_Beautiful  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rx638n/would_you_fine_tune_the_hyperparameters_or_modify/",
          "publishedOn": "2022-01-06T04:58:23.000Z",
          "wordCount": 170,
          "title": "Would you fine tune the hyperparameters or modify the reward shaping (customized env) to improve the performance of model-free algorithms like TD3?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rx5jrp/is_reinforcement_learning_just_a_special_case_of/",
          "author": null,
          "description": "Hi everyone - I am a data scientist who works a lot on causal inference problems. In particular, I build individual treatment effect (AKA Conditional Average Treatment Effects AKA Uplift) models to estimate the causal effect of some potential treatment(s) on an outcome of interest for an individual for the express purpose of giving the optimal \"treatment\" to a given individual. \"Optimal\" usually means the treatment that is estimated to cause the largest positive effect on the outcome. A typical example is optimizing marketing touches for individual customers: we have several \"treatments\" we can apply to a customer at a given time, such as targeting them for an online ad campaign, sending a push notification through our app, sending a marketing email, etc. and we want to make sure we are pu…",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rx5jrp/is_reinforcement_learning_just_a_special_case_of/",
          "publishedOn": "2022-01-06T04:29:55.000Z",
          "wordCount": 1819,
          "title": "Is Reinforcement Learning just a special case of causal modeling?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rx556k/analyzing_skill_expression_in_games_with_the_help/",
          "author": null,
          "description": "submitted by    /u/thechiamp  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rx556k/analyzing_skill_expression_in_games_with_the_help/",
          "publishedOn": "2022-01-06T04:09:47.000Z",
          "wordCount": 5094,
          "title": "Analyzing Skill Expression in Games (with the help of reinforcement learning)"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rwzfef/simulation_environment_to_real_life_is_the_brain/",
          "author": null,
          "description": "I am planning to train TD3/DDPG using a simulation environment, and then continue the learning on to a real-life environment. I hope to reduce the timestep required to converge in real-life environment as it is costly and time-consuming.\n I am new to RL and I am curious as to: 'Would the algorithm still be flexible enough to continue learning?'\n I am slightly afraid about how the algorithm is going to think that it finished learning during the simulation environment, but then when it comes to real-life environment, it would not be flexible enough to learn on top of what it has already learned.\n Is this a trivial concern and is something that I should just let the algorithm learn by itself\n    submitted by    /u/KoreaNuclear  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rwzfef/simulation_environment_to_real_life_is_the_brain/",
          "publishedOn": "2022-01-05T23:04:12.000Z",
          "wordCount": 340,
          "title": "Simulation environment to real life. Is the brain of RL still flexible enough to learn in real life env?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rwqym5/finding_general_equilibria_in_manyagent_economic/",
          "author": null,
          "description": "submitted by    /u/gwern  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rwqym5/finding_general_equilibria_in_manyagent_economic/",
          "publishedOn": "2022-01-05T16:36:53.000Z",
          "wordCount": 160,
          "title": "\"Finding General Equilibria in Many-Agent Economic Simulations Using Deep Reinforcement Learning\", Curry et al 2022"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rwqjk9/final_reward_help/",
          "author": null,
          "description": "Hellooo,\n I have a question and I´ll be glad if someone could help me.\n The question is : in episodic tasks, can we work with two rewards, one during the steps of an episode and the other as the final reward at the end of the episode?\n Thank you!\n    submitted by    /u/LeatherCredit7148  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rwqjk9/final_reward_help/",
          "publishedOn": "2022-01-05T16:18:13.000Z",
          "wordCount": 214,
          "title": "Final reward! Help!"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rwp5sd/some_questions_on_deep_reinforcement_learning_drl/",
          "author": null,
          "description": "Hello,\n I hope you are doing well.\n I am working on DRL and there´re still some unclear points for me:\n - How we tune the hyperparameters of the network (is there a method to simplify the task)\n - How to know that we formulated the best state for the agent? \n - In the case of collaborative multi-agent system, when an agent selects a task and the other agents are busy, will the reward be 0 for the busy agents or will be the reward of the agent that selects the task?\n    submitted by    /u/GuavaAgreeable208  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rwp5sd/some_questions_on_deep_reinforcement_learning_drl/",
          "publishedOn": "2022-01-05T15:15:27.000Z",
          "wordCount": 659,
          "title": "Some questions on Deep Reinforcement Learning (DRL)"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rwhq5k/workshops_on_ai_and_rl_by_shaastra_iit_madras/",
          "author": null,
          "description": "Workshops from Shaastra, IIT Madras about AI and Reinforcement Learning\n Certificates and recordings will be provided on registering in Shaastra's Website\n    submitted by    /u/RowEmbarrassed4756  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rwhq5k/workshops_on_ai_and_rl_by_shaastra_iit_madras/",
          "publishedOn": "2022-01-05T08:06:33.000Z",
          "wordCount": 151,
          "title": "Workshops on AI and RL by Shaastra, IIT Madras"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rwahe9/real_life_reinforcement_learning/",
          "author": null,
          "description": "submitted by    /u/odinnotdoit  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rwahe9/real_life_reinforcement_learning/",
          "publishedOn": "2022-01-05T01:32:02.000Z",
          "wordCount": 289,
          "title": "Real life Reinforcement learning"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rw9obd/equivalent_framework_as_gin_for_pytorch/",
          "author": null,
          "description": "Hi, does any of you know is there is an equivalent of this framework (https://github.com/google/gin-config) for PyTorch? \n Thanks\n    submitted by    /u/No_Possibility_7588  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rw9obd/equivalent_framework_as_gin_for_pytorch/",
          "publishedOn": "2022-01-05T00:53:13.000Z",
          "wordCount": 138,
          "title": "Equivalent framework as Gin for PyTorch"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rw9kci/scalar_reward_is_not_enough/",
          "author": null,
          "description": "Check out this paper which discusses the idea that a scalar reward is not enough to create agi.\n https://arxiv.org/abs/2112.15422\n What are your thoughts on this?\n    submitted by    /u/Longjumping-Chart-34  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rw9kci/scalar_reward_is_not_enough/",
          "publishedOn": "2022-01-05T00:47:54.000Z",
          "wordCount": 212,
          "title": "Scalar reward is not enough"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rw7dpf/training_with_multiple_agents/",
          "author": null,
          "description": "Does anyone know if any of the open source RL libraries support multi-agent training in which agents can have different actions spaces?\n    submitted by    /u/YearPersonal5709  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rw7dpf/training_with_multiple_agents/",
          "publishedOn": "2022-01-04T23:03:33.000Z",
          "wordCount": 137,
          "title": "Training with multiple agents."
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rw0l8j/difference_between_cooperative_games_and/",
          "author": null,
          "description": "Some say that cooperative games are a subset of stochastic games. But I don't find how. In cooperative games, all the agents have a team reward and have different states whereas in stochastic games, all the agents receives individual reward and they have the same state in a particular time step. Can someone help me understand the difference between these two games?\n    submitted by    /u/j0ker_70  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rw0l8j/difference_between_cooperative_games_and/",
          "publishedOn": "2022-01-04T18:04:55.000Z",
          "wordCount": 333,
          "title": "Difference between cooperative games and stochastic games"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rvyiix/understanding_stochastic_value_gradients_svg/",
          "author": null,
          "description": "Hi, I have a hard time understanding the paper \"Learning Continuous Control Policies by Stochastic Value Gradients\" (https://arxiv.org/pdf/1510.09142.pdf). \n So, in my understanding is that the stochastic value gradient could be computed easily on imagined (\"planned\") data, where we rollout the policy in the learned world model. However, this seems undesirable due to compounding model errors. So far, so good.\n The above issue is fixed when we use real environment samples, although the noise variables η and ξ required to calculate the stochastic value gradient are then unknown. The paper then applies the Bayes rule to reformulate the stochastic value gradient to a tractable formulation.\n My issue: to compute the stochastic value gradient, the noise variables must be inferred. On this issue, the paper simply states \"... infer the missing noise variables, possibly by sampling from p(η,ξ|s, a, s′)\". How can we sample these noise variables? How can we infer the noise variables?\n Any help would be greatly appreciated!\n    submitted by    /u/Internal-Brush4929  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rvyiix/understanding_stochastic_value_gradients_svg/",
          "publishedOn": "2022-01-04T16:35:07.000Z",
          "wordCount": 272,
          "title": "Understanding \"Stochastic Value Gradients (SVG)\""
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rvxb9k/training_data/",
          "author": null,
          "description": "Hi,\n i'm pretty new to this kind of learning task and i found hard to understand some base concepts. I'm studying the Decision Transformer paper and for an university problem i need to train that model using the MiniWorld environment.\n While i was looking at the code that is provided by the author of MiniWorld i got a question. How i get training data? \n The authors of Decision Transformer uses d4rl that provide some offline dataset. How can create training data in a similar way using my environment?\n    submitted by    /u/maverik75  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rvxb9k/training_data/",
          "publishedOn": "2022-01-04T15:43:28.000Z",
          "wordCount": 286,
          "title": "Training Data"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rvushn/anyone_using_rl_for_trading_stocks/",
          "author": null,
          "description": "submitted by    /u/GarantBM  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rvushn/anyone_using_rl_for_trading_stocks/",
          "publishedOn": "2022-01-04T13:48:47.000Z",
          "wordCount": 747,
          "title": "Anyone using RL for Trading stocks?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rvq1yt/how_to_make_gym_a_parallel_environment/",
          "author": null,
          "description": "I'm run gym environment CartPole-v0, but my GPU usage is low. I get a resolution that I can use N same policy Networks to get actions for N envs.\n I tried agymc, but it can't satisfy my need. Because it can only copy the envs, and cannot use N Networks.\n neural.py (github.com)\n So I write a code as above, but I get this error as below, how can I repair the bug:\n Exception in thread Thread-7:\n File \"C:\\Users\\afuler\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 973, in _bootstrap_inner\n self.run()\n File \"C:\\Users\\afuler\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 910, in run\n self._target(*self._args, **self._kwargs)\n File \"d:\\workspace\\rl\\nonnueral\\nonnueralrl.py\", line 271, in predict\n envList[worker_num].render()\n File \"C:\\Users\\afuler\\AppData\\Local\\Pro…",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rvq1yt/how_to_make_gym_a_parallel_environment/",
          "publishedOn": "2022-01-04T09:01:56.000Z",
          "wordCount": 374,
          "title": "How to make gym a parallel environment?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rvhvr3/ppo_convergence_when_moving_to_continuous_action/",
          "author": null,
          "description": "Hello :D,\n I had a PPO implementation with convolutional neural networks that worked just fine with discrete action space (environment is a grid world where an agent has 4 possible actions=directions). I took the same implementation and switched to continuous action space (2 actions = force over x and y axis). The model does pretty well in the first 7mil steps (reward converges), but then the reward suddenly goes down. \n Any idea what could be the reason? could it be the decaying variance I am using for the continuous action selection?\n    submitted by    /u/AhmedNizam_  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rvhvr3/ppo_convergence_when_moving_to_continuous_action/",
          "publishedOn": "2022-01-04T01:29:38.000Z",
          "wordCount": 305,
          "title": "PPO convergence when moving to continuous action space"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rvatrg/the_best_machine_learning_courses_on_udemy_2022/",
          "author": null,
          "description": "submitted by    /u/JanPrince002  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rvatrg/the_best_machine_learning_courses_on_udemy_2022/",
          "publishedOn": "2022-01-03T20:13:41.000Z",
          "wordCount": 133,
          "title": "The Best Machine Learning Courses on Udemy (2022)"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rv9a2d/amazon_research_introduces_deep_reinforcement/",
          "author": null,
          "description": "In recent years, voice-based virtual assistants such as Google Assistant and Amazon Alexa have grown popular. This has presented both potential and challenges for natural language understanding (NLU) systems. These devices’ production systems are often trained by supervised learning and rely significantly on annotated data. But, data annotation is costly and time-consuming. Furthermore, model updates using offline supervised learning can take long and miss trending requests.\n In the underlying architecture of voice-based virtual assistants, the NLU model often categorizes user requests into hypotheses for downstream applications to fulfill. A hypothesis comprises two tags: user intention (intent) and Named Entity Recognition (NER). For example, the valid hypothesis for “play a Madonna song” will be: PlaySong intent, ArtistName – Madonna.\n A new Amazon research introduces deep reinforcement learning strategies for NLU ranking. Their work analyses a ranking question in an NLU system in which entirely independent domain experts generate hypotheses with their features, where a domain is a functional area such as Music, Shopping, or Weather. These hypotheses are then ranked based on their scores, calculated based on their characteristics. As a result, the ranker must calibrate features from domain experts and select one hypothesis according to policy. Continue Reading\n Research Paper: https://assets.amazon.science/b3/74/77ff47044b69820c466f0624a0ab/introducing-deep-reinforcement-learning-to-nlu-ranking-tasks.pdf\n ​\n https://preview.redd.it/fp79wa92ui981.png?width=1920&format=png&auto=webp&s=647db1d2bd5be9dc9698c980e2146fef499368f3\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rv9a2d/amazon_research_introduces_deep_reinforcement/",
          "publishedOn": "2022-01-03T19:06:45.000Z",
          "wordCount": 331,
          "title": "Amazon Research Introduces Deep Reinforcement Learning For NLU Ranking Tasks"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rv5ugt/average_reward_formulation_for_continuing_settings/",
          "author": null,
          "description": "I have a problem that is -for its nature- continuing, i.e. there are no episodes and the agent has to operate for an infinite time. In my simulator, however, I have the possibility of simulating episodes of any length.\n Is the average reward formulation appropriate for such a problem? I do not know much about it and I have a few questions for you:\n  \nCould you point me to some literature that addresses average reward and its pros/cons vs discounted reward for continuing problems?\n Is the average taken on a window of finite size or on the limit for the size that goes to infinity? \n How do common RL algorithms work in this formulation? Can we still adapt them to use average reward?\n  \n   submitted by    /u/fedetask  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rv5ugt/average_reward_formulation_for_continuing_settings/",
          "publishedOn": "2022-01-03T16:39:19.000Z",
          "wordCount": 734,
          "title": "Average reward formulation for continuing settings"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rv2ku7/formula_to_compute_loss_in_a3c/",
          "author": null,
          "description": "I'm a beginner to RL and I'm trying to understand how the loss function was computed. If it follows a specific formular. I've read the a3c algorithm overview on paper by barto but it seems the implemtation here https://github.com/MorvanZhou/pytorch-A3C/blob/master/discrete_A3C.py is different.\n    submitted by    /u/phissy08  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rv2ku7/formula_to_compute_loss_in_a3c/",
          "publishedOn": "2022-01-03T14:12:09.000Z",
          "wordCount": 162,
          "title": "Formula to compute loss in A3C"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rv1wsc/introduction_to_markov_decision_processes_draft/",
          "author": null,
          "description": "submitted by    /u/YouAgainShmidhoobuh  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rv1wsc/introduction_to_markov_decision_processes_draft/",
          "publishedOn": "2022-01-03T13:38:07.000Z",
          "wordCount": 301,
          "title": "Introduction to Markov Decision Processes - Draft Chapters 2 and 3"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rux1hi/dqn_with_online_learning/",
          "author": null,
          "description": "I saw another post on here that mentioned how DQN's can be implemented via online learning, offline learning (e.g. batching), or a combination (offline learning first and then online learning in production). What I'm confused at is the actual deployment paradigm to facilitate online learning (either as the first step or with hybrid). \n Is the full training code (with a checkpoint model if hybrid) and agent deployed to production with the training step logic executed on-demand for each state transition? If so, would this mean that model compilation / quantization is not possible (e.g. Onnx runtime to INT8 precision)? How is it scaled if it's having to perform SGD and back-propagation with each state transition?\n ​\n One the other side, if the model is compiled / quantized, how does it implement online learning (if this is even possible)?\n    submitted by    /u/FinateAI  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rux1hi/dqn_with_online_learning/",
          "publishedOn": "2022-01-03T08:37:14.000Z",
          "wordCount": 495,
          "title": "DQN with online learning"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/ruw5c4/any_good_papers_for_nonstationary_environment/",
          "author": null,
          "description": "I want to create a control system for control agents(players). The players will move non-stationary, so is there any good technique or papers to train this control system? \n Very Thanks.\n    submitted by    /u/Spiritual_Fig3632  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/ruw5c4/any_good_papers_for_nonstationary_environment/",
          "publishedOn": "2022-01-03T07:36:14.000Z",
          "wordCount": 309,
          "title": "Any good papers for non-stationary environment?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/ruvrti/how_to_change_mdp_into_pomdp_getting_pixel/",
          "author": null,
          "description": "Hi, I'm looking into OpenAI robogym (https://github.com/openai/robogym)'s rearrange environments, and want to try RL-based control on it with pixel observations.\n The default observations are not pixels, and I can't seem to find any documentation online on how to change these environments into pixel observations.\n Can anyone point me to resources where similar changes have been done? Are there preexisting wrappers or code for similar mujoco environments anywhere?\n My other option is to simply render the environment with mode rgb_array, and then train the algorithm based on these rendered observations. Is this a viable option?\n    submitted by    /u/junkQn  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/ruvrti/how_to_change_mdp_into_pomdp_getting_pixel/",
          "publishedOn": "2022-01-03T07:11:19.000Z",
          "wordCount": 221,
          "title": "How to change MDP into POMDP? (getting pixel observations)"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rup48q/discrete_actions_openai_hide_seek/",
          "author": null,
          "description": "Hello :),\n Happy new year to all of you!\n I have a question regarding the action space for the Hide & Seek paper by OpenAI. The architecture of the learning model they used is below.\n https://preview.redd.it/el54yihfjd981.png?width=794&format=png&auto=webp&s=33c5228ac8003b9bb89b6d008e2cf2aea5f7e071\n They have eventually 3 action heads, as shown in the figure. As per the authors, the movement action for an agent \"sets a discretized force along their x and y axis and torque around their z-axis\". How do these 3 forces get pulled from the \"movement action head\", given that they are discretized? Does the movement action head have 3 outputs, each corresponding to an axis? So far, in my humble knowledge, I always assumed that a network with a discrete action space gives only one action per head (which could be the result of a softmax over the action space).\n Any insight?\n  \nAfter some digging, I found that the Movement Action has a \"MultiDiscrete\" type, which is a gym class. I am struggling to understand the nature of actions produced by this class, any idea?\n  \n   submitted by    /u/AhmedNizam_  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rup48q/discrete_actions_openai_hide_seek/",
          "publishedOn": "2022-01-03T01:22:41.000Z",
          "wordCount": 303,
          "title": "Discrete Actions (OpenAI Hide & Seek)"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rui9bm/player_of_games_schmid_et_al_2021_dm_generalizing/",
          "author": null,
          "description": "submitted by    /u/gwern  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rui9bm/player_of_games_schmid_et_al_2021_dm_generalizing/",
          "publishedOn": "2022-01-02T20:14:43.000Z",
          "wordCount": 164,
          "title": "\"Player of Games\", Schmid et al 2021 {DM} (generalizing AlphaZero to imperfect-information games)"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rue5bf/what_is_the_best_methodtechnology_to_deploy_deep/",
          "author": null,
          "description": "Hello, \n I am a robotics enthusiast building a robot arm to perform mundane tasks. So far, I have only been able to deploy deep reinforcement learning models on the Jetson Nano. The process has been easy using the Nvidia TensorRT SDK. What other methods/technology exist? My friend recommended using a google coral and a raspberry pi. Any recommendations would be greatly appreciated. Thank you\n    submitted by    /u/AINerd1  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rue5bf/what_is_the_best_methodtechnology_to_deploy_deep/",
          "publishedOn": "2022-01-02T17:12:52.000Z",
          "wordCount": 290,
          "title": "What is the best method/technology to deploy deep reinforcement learning models for robotics?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rudf9k/markov_decision_process_explained_through_memes/",
          "author": null,
          "description": "Today I published my first article on medium explaining Markov Decision Process with memes. Hope someone finds this helpful. \n https://medium.com/@saminbinkarim/explaining-markov-decision-process-with-memes-af679a0af343\n    submitted by    /u/sardines_again  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rudf9k/markov_decision_process_explained_through_memes/",
          "publishedOn": "2022-01-02T16:40:02.000Z",
          "wordCount": 183,
          "title": "Markov Decision Process explained through Memes"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rubyb9/proper_way_to_count_environment_steps_frames_in/",
          "author": null,
          "description": "In classical - on-policy - vtrace/Impala algorithm env_steps are incremented every training iteration\n like this : env_steps += size_of_batch * unroll_length\n However when we add replay_buffer to vtrace (like in CLEAR or LASER) we also start to learn from off-policy data therefore some trajectories (from replay) could be sampled more than once.\n My question is - does the env_step_counting stay the same even when some trajectories from batch are from replay and some directly from workers (on-policy) ?\n Or do i count steps only on those trajectories that are on-policy => not from replay\n like : env_steps += len(batch[\"on_policy_samples\"]) * unroll_length\n And if so what happens when i decide to use only samples from replay to train - how do i count env_steps then ? Using some kind of flag to indicate if specific trajectory was sampled already and leave it from counting ?\n ​\n Have been digging through the research papers for some time now and i haven`t been able to find what is the proper way to do it. Any help or advice would be much appreciated.\n    submitted by    /u/ParradoxSVK  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rubyb9/proper_way_to_count_environment_steps_frames_in/",
          "publishedOn": "2022-01-02T15:32:09.000Z",
          "wordCount": 522,
          "title": "Proper way to count environment steps / frames in distributed RL architecture for algorithms like CLEAR or LASER => basically modified impala with replay"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/ru4glz/what_should_a_beginner_do_after_learning_some/",
          "author": null,
          "description": "Hi all, it's my first time to use reddit to ask for suggestions.\n I am a beginner in RL. I learned some basic ideas in RL (such as SARSA, PPO and A2C) recently and applied Q-network to train a Go game agent. \n Personally speaking, I am going to pursue for a position as a RL engineer in the future. But I have no idea about what should I do to improve my RL skills. So I am wondering if there are some awesome resources for a beginner like me? Or could you please give me some instructions?\n    submitted by    /u/ZavierTi2021  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/ru4glz/what_should_a_beginner_do_after_learning_some/",
          "publishedOn": "2022-01-02T07:24:05.000Z",
          "wordCount": 697,
          "title": "What should a beginner do after learning some basic ideas?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rtp5ts/nethack_2021_neurips_challenge_winning_agent/",
          "author": null,
          "description": "Hi All! I am Michał from the AutoAscend team that has won the NetHack 2021 NeurIPS Challenge.\n I have just shared some episode visualization videos:\n https://www.youtube.com/playlist?list=PLJ92BrynhLbdQVcz6-bUAeTeUo5i901RQ\n The winning agent isn't based on reinforcement learning in the end, but the victory of symbolic methods in this competition shows what RL is still missing to some extent -- so I believe this subreddit is a good place to discuss it.\n We hope that NLE will someday become a new standard benchmark for evaluation next to chess, go, Atari, etc. as it presents a set of whole new complex problems for agents to learn. Contrary to Atari, NetHack levels are procedurally generated, and therefore agents can't memorize the layout. Observations are highly partial, rewards are sparse, and episodes are usually very long.\n Here are some other useful links related to the competition:\n Full NeurIPS Session recording: https://www.youtube.com/watch?v=fVkXE330Bh0\n AutoAscend team presentation starts here: https://youtu.be/fVkXE330Bh0?t=4437\n Competition report: https://nethackchallenge.com/report.html\n AICrowd Challenge link: https://www.aicrowd.com/challenges/neurips-2021-the-nethack-challenge\n    submitted by    /u/procedural_only  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rtp5ts/nethack_2021_neurips_challenge_winning_agent/",
          "publishedOn": "2022-01-01T18:27:09.000Z",
          "wordCount": 1465,
          "title": "NetHack 2021 NeurIPS Challenge -- winning agent episode visualizations"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rtifft/some_questions_on_drl/",
          "author": null,
          "description": "Hello,\n I´m applying Deep Reinforcement Learning for the first time, and I have some questions about it (I´ve already looked for an answer but in vain):\n  \nHow to normalize objectives' values in the reward function? if we have an objective that values are in the range of 10 and another objective that values are in the range of 1000.\n During the training phase, how can we watch the weights updates of a network and the gradient calculation too?\n In a multi-agent setting and episodic task, for \"dones\" vector, it will be set to \"True\" once all the agents are finished, or once an agent finishes the task done[agent_index]=True in other words, we won´t wait the latest agent to finish to set dones = [True]*number_of_agents\n  \nThank you.\n    submitted by    /u/GuavaAgreeable208  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rtifft/some_questions_on_drl/",
          "publishedOn": "2022-01-01T12:22:45.000Z",
          "wordCount": 400,
          "title": "Some questions on DRL"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rthv34/explained_variance_suddenly_crashes_and_stay/",
          "author": null,
          "description": "Hi, all Happy New Year!\n I'm working on a project to teach my quad-rotor how to hover, and these are what my rollout graphs looks like.\n The reward graph constantly increases for some time, but suddenly during a rapid increase in reward, the entropy loss, and the train std of starts to diverge, and the explained variance just crashes.\n ​\n https://preview.redd.it/yv4qbhmbc2981.png?width=826&format=png&auto=webp&s=31322a62963b1de47ed72452dfb36498ce222dd1\n ​\n https://preview.redd.it/d38papwdc2981.png?width=1373&format=png&auto=webp&s=0ab5b98d0bdad165f0dcfd7f5ea3e54ad965842c\n From my understanding, this explained variance far from 1 is a bad sign, Could you please point out, or give comments on what would be the possible reason why this is happening in my case?\n Any kind of reply would be appreciate.\n Thanks again Happy New Year. :D\n ​\n Hp: ent_coef=0.0001, learning_rate=linear decay from 0.00004, n_steps=1024, batch_size=64 apart from these, I used the default params from stable_baselines3\n I scale the action output from [-1, 1] to [min_rpm max_rpm] to actuate the quad-rotor.\n observations states = input = [pos,vel,acc,ang,ang_vel] // Network size 64,64 // algorithm used:PPO\n REWARD FUNCTION\n ​\n https://preview.redd.it/o3w2i0r7s9981.png?width=818&format=png&auto=webp&s=f03b4f12d6c57f7b7aaec7dbd03b9fecad2c8f2c\n    submitted by    /u/GOMTAE  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rthv34/explained_variance_suddenly_crashes_and_stay/",
          "publishedOn": "2022-01-01T11:44:03.000Z",
          "wordCount": 585,
          "title": "Explained Variance suddenly crashes, and stay around zero and even sometimes go to negative values"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rtc336/help_with_ppo_model_performing_poorly/",
          "author": null,
          "description": "I am attempting to recreate the PPO algorithm to try to learn the inner workings of the algorithm better and to learn more about actor-critic reinforcement learning. So far, I have a model that seems to learn, just not very well.\n In the early stages of training, the algorithm seems more sporadic and may happen to find a pretty solid policy, but due to unstable the early parts of training are, it tends to move away from this policy. Eventually, the algorithm moves the policy toward a reward of around 30. For the past few commits in my repo where I have attempted to fix this issue, the policy always tends to the around 30 reward mark, and I'm not entirely sure why it's doing this. I'm thinking maybe I implemented the algorithm incorrectly, but I'm not certain. Can someone please help me with this issue?\n Below is a link to an image of training using the latest commit, using a previous commit, and my Github project\n current commit: https://ibb.co/JQgnq1f\n previous commit: https://ibb.co/rppVHKb\n GitHub: https://github.com/gmongaras/PPO_CartPole\n ​\n Thanks for your help!\n    submitted by    /u/gmongaras  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rtc336/help_with_ppo_model_performing_poorly/",
          "publishedOn": "2022-01-01T05:07:30.000Z",
          "wordCount": 659,
          "title": "Help With PPO Model Performing Poorly"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rt226f/agent_not_learning_any_help/",
          "author": null,
          "description": "Hello\n Can someone explain why the actor critic maps the states to the same actions, in other words why the actor outputs the same action whatever the states?\n This what makes the agent learns nothing during training phase.\n Happy New Year!\n    submitted by    /u/LeatherCredit7148  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rt226f/agent_not_learning_any_help/",
          "publishedOn": "2021-12-31T20:04:50.000Z",
          "wordCount": 515,
          "title": "Agent not learning! Any Help"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rst5go/question_about_baseline_with_value_network/",
          "author": null,
          "description": "I have a question about baseline in policy gradient method. Based on what is implemented in relation to PG, there are many shared policy and value networks. But I understood baseline should not affect the parameters of the policy. When PG with shared networks and baseline updates value function, then baseline will affect policy network. Is it okay?\n    submitted by    /u/Spiritual_Fig3632  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rst5go/question_about_baseline_with_value_network/",
          "publishedOn": "2021-12-31T12:44:12.000Z",
          "wordCount": 179,
          "title": "question about baseline with value network"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rspzc6/how_to_check_feature_importance_in_reinforcement/",
          "author": null,
          "description": "I have made a custom environment in openAi Gym using data from a csv file that contains certain features in it as column names .After training a DQN agent on the environment, i wanted to check some meaningful features used by the DQN agent to make the policy but i am unable to find some good resources regarding it.\n An example of the feature importance graph\n    submitted by    /u/EBISU1234  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rspzc6/how_to_check_feature_importance_in_reinforcement/",
          "publishedOn": "2021-12-31T09:14:19.000Z",
          "wordCount": 312,
          "title": "How to check feature importance in Reinforcement learning?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rspmiy/current_unansweredinteresting_applications_in/",
          "author": null,
          "description": "Hi,\n I am planning on doing my MSc in CS with a focus in RL. More specifically, I want to learn about multi-armed bandits and how it can be used by agents to enable them to perform actions in a diverse environment. I am new to this field and I want to know more about what questions about MAB are unanswered? Any interesting application that may be currently under research? \n I would really appreciate if anyone can help me out. \n Thank you!\n    submitted by    /u/_UNIPOOL  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rspmiy/current_unansweredinteresting_applications_in/",
          "publishedOn": "2021-12-31T08:50:45.000Z",
          "wordCount": 428,
          "title": "Current unanswered/interesting applications in Multi-armed bandits?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rse3lr/ppo_with_multiple_heads/",
          "author": null,
          "description": "Hello there,\n I am working on an implementation for PPO, and I am struggling to compute the loss when two output heads for the actor network are used (one is continuous and one is discrete). The loss in PPO has 3 components: 1) clipped surrogate, 2) squared state-value loss, and 3) Entropy.\n I thought of treating the two actions separately, and compute two different losses that I add before backpropagating, but the middle term (2) is the same in both losses. How could I do that? Do I add (1) and (3) for both actions and have one loss, or do I compute two different losses then add them? Or is it hidden option 3? :p\n ​\n Any help is appreciated.\n    submitted by    /u/AhmedNizam_  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rse3lr/ppo_with_multiple_heads/",
          "publishedOn": "2021-12-30T22:29:02.000Z",
          "wordCount": 610,
          "title": "PPO with multiple heads"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rsc59y/discrete_vs_continuous_action_space/",
          "author": null,
          "description": "Hello there,\n I noticed that in many works by OpenAI and Deepmind in RL, they deal with a continuous action space which they discretize. For example rather than having to continuous actions to determine speed and direction of movement, they would discretize speed and torque.\n Is there any reason why discretized action space is more favorable?\n    submitted by    /u/AhmedNizam_  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rsc59y/discrete_vs_continuous_action_space/",
          "publishedOn": "2021-12-30T21:03:43.000Z",
          "wordCount": 422,
          "title": "Discrete vs Continuous action space"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rs9841/combined_discrete_and_action_space/",
          "author": null,
          "description": "I need an algorithm that combines both discrete and continuous actions space, like if I wanted to choose action but I needed to also perform that action at an optimized velocity or something. Can I do this with ppo and how. I am still a beginner but must of the algorithms I played with had either separately I thought of actor cri but I don’t think the value network does what I want I can’t use that to predict how much velocity to use\n    submitted by    /u/ConsiderationCivil74  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rs9841/combined_discrete_and_action_space/",
          "publishedOn": "2021-12-30T18:58:37.000Z",
          "wordCount": 296,
          "title": "Combined discrete and action space"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rs6g73/actor_net_gives_me_the_same_reward_over_time/",
          "author": null,
          "description": "Hello\n I´ll be glad if someone could help me with this.\n I am new in DRL and when I applied it I got some issues. For example, my network freezes in the same reward from the first episode I noticed also that the agent selects always the same index from the action space. Even the same reward and the same trajectory toward this reward, the critic loss is still shrinking over time (this the weirdest thing)\n I don´t know what is the reson behind that?\n https://preview.redd.it/02zlhgnqnp881.png?width=477&format=png&auto=webp&s=093c2b22d368e33fa4853ca4f3c5b459c9e2e31d\n    submitted by    /u/LeatherCredit7148  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rs6g73/actor_net_gives_me_the_same_reward_over_time/",
          "publishedOn": "2021-12-30T17:00:54.000Z",
          "wordCount": 263,
          "title": "Actor net gives me the same reward over time"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rs5yoe/ablation_teststudy/",
          "author": null,
          "description": "I'm trying to see the effect of simulated sensor noise in training a successful policy using RL for the cart pole control problem.\n I have four states: cart position, cart velocity, pole angle, pole angular velocity, and I added Gaussian noise to all of them. However, I want to find the impact of adding noise to a specific variable to find which of the states is the most sensitive to noise.\n If I were to only test the system with noise in one of the variables at a time (e.g. noise for position, but none for the other variables), would that be considered an ablation test?\n Just caught up in the terminology, because I know typical ablation tests remove a certain component instead of adding only one (or removing all but one).\n    submitted by    /u/Cogitarius  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rs5yoe/ablation_teststudy/",
          "publishedOn": "2021-12-30T16:39:32.000Z",
          "wordCount": 363,
          "title": "Ablation test/study"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rry8ge/best_suite_for_robotics_tasks/",
          "author": null,
          "description": "Recently, OpenAI removed the robotics tasks from gym, as they are no longer being maintained and Robosuite uses mujoco-py which has been deprecated. Deepmind's control suite includes some robotics tasks but they don't seem to be first-class citizens (they are in a separate module, with less options and some bugs).\n Are there any other robotics environment suites that are well-maintained and supported?\n    submitted by    /u/escapevelocitylabs  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rry8ge/best_suite_for_robotics_tasks/",
          "publishedOn": "2021-12-30T10:00:51.000Z",
          "wordCount": 203,
          "title": "Best suite for robotics tasks"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rrx27c/how_to_map_states_into_the_code/",
          "author": null,
          "description": "Hello,\n I am new in the Reinforcement Learning field. My question is: how can I easily map my states into my code?\n F.I. when the problem is about how to move an agent in a board-like environment, such as walk from point A to B by moving UP, DOWN, LEFT or RIGHT, it is quite easy to map your states. You can create a matrix like NUMBER_OF_ROWS x NUMBER_OF_COLUMNS and that is pretty much it.\n However when it comes to other type of scenarios, such as CartPole-V0 from OpenAIGym (https://gym.openai.com/envs/CartPole-v0/), I don't understand how to map the states into my code.\n This scenario has 4 states and 2 actions:\n States:\n  \nCart Position (range -2.4 --> 2.4)\n Cart Velocity (range -Inf --> Inf)\n Pole Angle (range -12deg --> 12deg)\n Pole Angular Velocity (range -Inf --> Inf)\n  \nActions:\n  \nPush cart to the left\n Push cart to the right\n  \nThe same is true for every other scenario that is not board-like, f.i. tic-tac-toe. I don't understand how can I make every possible move in the game a state inside the code.\n ​\n Is there any technique to approach each type of problem or do I just need to figure it out by myself? \n Have a nice day,\n Gabriel.\n    submitted by    /u/gabrieloxi  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rrx27c/how_to_map_states_into_the_code/",
          "publishedOn": "2021-12-30T08:49:42.000Z",
          "wordCount": 547,
          "title": "How to map states into the code?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rruluq/general_question_in_regards_to_understanding_the/",
          "author": null,
          "description": "Hi there, I was a graduate student previously working on a few RL-related problems a year ago. And recently I've got much interest in Policy Optimization problems, and started with reading some foundation papers like DQN, DPG.\n While reading the paper of DPG, it was very straightforward to understand the main idea of the paper, however, it was very difficult to understand the proofs of the theorems shown in the appendix(http://proceedings.mlr.press/v32/silver14-supp.pdf). I barely understood the proof of Theorem1, but proof of Theorem2 is almost impossible to understand due to the lack of my mathematics background. \n The question is that, could you tell me what I should study or learn before understanding this kind of proof? What kind of subject or material should I look for in order to understand the flow of the proofs and related definition/notation used in the proof of 'DPG'?\n I personally think that it is crucial to understand the proofs of the main Theorem because it will help me to understand the authors' intuition and motivation which can be the foundation of the paper.\n    submitted by    /u/g6ssgs  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rruluq/general_question_in_regards_to_understanding_the/",
          "publishedOn": "2021-12-30T06:21:32.000Z",
          "wordCount": 347,
          "title": "General question in regards to understanding the proofs of Deterministic Policy Gradient"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rr8thh/favorite_papers_from_2021/",
          "author": null,
          "description": "What have been your favorite reads of 2021 in terms of RL papers? I will start!\n Reward is enough (Reddit Discussion) - Four great names from RL (silver, Singh, Precup and Sutton) give their reasonings as to why using RL can create super intelligence. You might not agree with it, but it's interesting to see the standpoint of Deepmind and where they want to take RL.\n Deep Reinforcement Learning at the Edge of the Statistical Precipice (Reddit Discussion) - This is a major step towards a better model comparison in RL. Too many papers in the past have used a selection technique akin to 'average top 30 runs in a total of 100'. I have also never even heard of Munchausen RL before this paper, and was pleasantly surprised by reading it.\n Mastering Atari with Discrete World Models - Very good read and a nice path from Ha's World Models to Dream to Control to DreamerV2. This is one of the methods this year that actually seems to improve performance quite a bit without needing a large scale distributed approach.\n On the Expressivity of Markov Reward (Reddit Discussion) - The last sentence in the blog post captures it for me: \"We hope this work provides new conceptual perspectives on reward and its place in reinforcement learning\", it did.\n Open-Ended Learning Leads to Generally Capable Agents (Reddit Discussion) - Great to see the environment integrated into the learning process, seems like something we will see much more of in the future. Unfortunately, as DeepMind does, the environment is not released nor is the code. I remember positions at OpenAI for open-ended learning, perhaps we might see something next year to compete with this.\n ​\n Most of my picks are not practical algorithms. For me, it seems like PPO is still king when looking at performance and simplicity, kind of a disappointment. I probably missed some papers too. What was your favorite paper in RL 2021? Was it Player of Games (why?), something with Offline RL or perhaps Multi Agents?\n    submitted by    /u/YouAgainShmidhoobuh  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rr8thh/favorite_papers_from_2021/",
          "publishedOn": "2021-12-29T14:00:33.000Z",
          "wordCount": 587,
          "title": "Favorite papers from 2021"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rr7yk6/what_happened_to_openai_rl/",
          "author": null,
          "description": "OpenAI used to do a lot of RL research, but it seems like last year and this year the only real RL related work was on benchmark competitions. They even gave away the control of OpenAI Gym. They still have great RL researchers working there, but nothing major has come out.\n Is it all due to a pivot towards large scale language models that are at least profitable? Is Sam Altman just not interested in RL?\n    submitted by    /u/YouAgainShmidhoobuh  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rr7yk6/what_happened_to_openai_rl/",
          "publishedOn": "2021-12-29T13:15:56.000Z",
          "wordCount": 632,
          "title": "What Happened to OpenAI + RL?"
        }
      ]
    },
    {
      "title": "RL Weekly",
      "feedUrl": "https://www.getrevue.co/profile/seungjaeryanlee?format=rss",
      "siteUrl": "https://www.getrevue.co/profile/seungjaeryanlee",
      "articles": []
    },
    {
      "title": "Damian Bogunowicz - dtransposed",
      "feedUrl": "https://dtransposed.github.io/feed.xml",
      "siteUrl": "http://dtransposed.github.io/",
      "articles": [
        {
          "id": "http://dtransposed.github.io/blog/2021/12/24/DeFi-Adventure-2/",
          "author": null,
          "description": "Christmas break is a great time to catch up with the backlog of interesting things to learn and read about. I used some of this time to finish the amazing DeFi quest by Cristian Strat. This is a continuation to the first part of this series, where I document my first steps in the world of Decentralized Finance. If you have not read the first write-up, please do. Otherwise, this text will be very confusing and not too useful for you. But once you go through both blog posts, I guarantee that you will have a DeFi knowledge superior to the majority of the folks out there.\nAdventure IV\nThis adventure will be more like a short pause from more in-depth topics. I will quickly present some of the best DeFi dashboards out there. A DeFi dashboard is a central place, where you can view your assets, tr…",
          "link": "http://dtransposed.github.io/blog/2021/12/24/DeFi-Adventure-2/",
          "publishedOn": "2021-12-24T00:00:00.000Z",
          "wordCount": 3123,
          "title": "Learning by Doing - the DeFi Quest (Part 2 out of 2)"
        }
      ]
    },
    {
      "title": "Featured Blog Posts - Data Science Central",
      "feedUrl": "http://feeds.feedburner.com/FeaturedBlogPosts-DataScienceCentral?format=xml",
      "siteUrl": null,
      "articles": [
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1087546",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9984763058?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9984763058?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p><span style=\"font-size: 12pt;\"><strong>Data Science Central is transitioning over to a new platform on January 10, 2022. </strong></span></p>\r\n<p>In order to complete this migration, we will not be accepting any new submissions for publication after today, January 4th, 2022 at 4:00PM EST. Regular writers will be notified by email about logging into the…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1087546",
          "publishedOn": "2022-01-05T20:52:39.000Z",
          "wordCount": 630,
          "title": "Beginning Transition to Word Press"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1086942",
          "author": null,
          "description": "<p><span><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9977680695?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9977680695?profile=RESIZE_710x\" width=\"720\"></img></a></span></p>\r\n<p><span>The Internet of Things has led to the rise of a new era of computing where intelligent applications are continuously monitored. Also, connected devices need to be efficiently controlled, thereby continuously transforming information into knowledge. By intelligently gathering and analyzing huge amounts of data, smart systems can…</span></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1086942",
          "publishedOn": "2022-01-05T01:03:09.000Z",
          "wordCount": 947,
          "title": "Intelligent Gateways Applications For Greenfield and Brownfield Environments"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085735",
          "author": null,
          "description": "<p style=\"margin: 0in; margin-bottom: .0001pt; text-align: justify;\"><span style=\"font-family: 'Cambria','serif'; color: #0e101a;\"><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9982564869?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9982564869?profile=RESIZE_710x\" width=\"720\"></img></a></span></p>\r\n<p>Whether you're acquainted with devices such as the Amazon Fire Stick, Chromecast, Spotify, Youtube, or SlingTV, you presumably already have a general understanding of what over-the-top (OTT) television is. Numerous…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085735",
          "publishedOn": "2022-01-05T01:00:00.000Z",
          "wordCount": 1364,
          "title": "Can OTT platforms succeed with machine learning services? An insight"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1086893",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9982552264?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9982552264?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>It's the last week of the year. The gifts have been opened (well, the ones that aren't currently still sitting in a dock in Los Angeles after being ordered in November), the cats have been teaching tree ornaments the meaning of the word gravity, and the cookies which tasted so good on Christmas Eve are getting more than a bit stale. In short, it's…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1086893",
          "publishedOn": "2022-01-05T00:50:54.000Z",
          "wordCount": 7372,
          "title": "Trends Towards 2022"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1087093",
          "author": null,
          "description": "<div class=\"dsc_primaryImage\"><span style=\"font-family: arial, helvetica, sans-serif;\"><a href=\"https://www.datasciencecentral.com/profiles/blog/list?tag=dsc_newsletter\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://multimedia.getresponse360.com/datascience-B/photos/4ee6c094-435c-447f-b0b5-a0ec4f4c769a.jpg?profile=RESIZE_710x\" width=\"720\"></img></a></span></div>\r\n<table style=\"width: 726px;\">\r\n<tbody><tr><td><div><p dir=\"ltr\">The last week of the year has traditionally been about reflections and planning, taking stock of the old (or auld), and preparing for the changes of the new. I've added my own thoughts about what 2022 will…</p>\r\n</div>\r\n</td>\r\n</tr>\r\n</tbody>\r\n</table>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1087093",
          "publishedOn": "2022-01-04T18:30:00.000Z",
          "wordCount": 2240,
          "title": "DSC Weekly Digest 28 December 2021: An Auld Lang Syne (and Cosyne Too)"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1087157",
          "author": null,
          "description": "<div><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9982440260?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9982440260?profile=RESIZE_710x\" width=\"720\"></img></a></div>\r\n<blockquote><p style=\"text-align: left;\"></p>\r\n<p>“There were five exabytes of information created between the dawn of civilization through 2003, but that much information is now created every two days.”</p>\r\n</blockquote>\r\n<p>~Eric Schmidt (Executive Chairman at Google)</p>\r\n<p>The term <a href=\"https://www.dasca.org/\">data science</a> was first…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1087157",
          "publishedOn": "2022-01-04T12:00:00.000Z",
          "wordCount": 1266,
          "title": "Why a Data Science Career Is Worth Pursuing"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1087084",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9982542074?profile=original\" rel=\"noopener\" target=\"_blank\"><font size=\"4\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9982542074?profile=RESIZE_710x\" width=\"720\"></img></font></a></p>\r\n<p><font size=\"4\">The technique of managing desired information from online platforms such as social media networking sites is known as <b>content</b> <b>moderation</b>. It's also known as <b>social moderation</b>, and it's used to control various forms of content that aren't appropriate for a general audience.…</font></p>\r\n<p></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1087084",
          "publishedOn": "2022-01-04T11:00:00.000Z",
          "wordCount": 1279,
          "title": "Top Social Media Content Moderation Trends that Will Reign Supreme in 2022"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1087061",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9979868266?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-center\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9979868266?profile=RESIZE_710x\" width=\"600\"></img></a></p>\r\n<p>No, I am not a flat-earther. Quite the contrary, the ideas suggested here assume advanced scientific knowledge, such as quantum physics. The famous Cambridge-based physicist Stephen Hawking suggested that we may live in a universe with 11 dimensions. What I discuss here is not conflicting with that statement, as we shall see. </p>\r\n<p>It all started…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1087061",
          "publishedOn": "2022-01-04T06:30:00.000Z",
          "wordCount": 1935,
          "title": "Could we Live in a Universe with Fewer than Three Dimensions?"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1086868",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9979780878?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9979780878?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>Data science is a domain that combines data-bound analytical techniques and scientific theory to generate insights for business stakeholders. Its shape, elements, and size allow organizations to optimize operations, identify new business opportunities, and reduce the functional performance of departments such as marketing and sales.</p>\r\n<p>Simply put,…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1086868",
          "publishedOn": "2022-01-04T06:00:00.000Z",
          "wordCount": 1091,
          "title": "Data science and analytics: the future implications"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1087050",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9982546079?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9982546079?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>Most of the Big Data, Data Science and AI / ML success stories seem to revolve around Business-to-Consumer (B2C) industries.  The companies that we see leading the economic exploitation of data and analytics are primarily B2C companies such as Apple, Alphabet (Google), Microsoft, Amazon, and Facebook (Figure 1).…</p>\r\n<p></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1087050",
          "publishedOn": "2022-01-03T18:00:00.000Z",
          "wordCount": 1781,
          "title": "Data Monetization Approach for B2B2C Industries"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1087129",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9982818672?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9982818672?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>The global IoT in intelligent transportation systems market is anticipated to be driven by the rising focus of industry players on research and development, aimed at enhancing the integrated IoT software in order to minimize the cost of operation associated with these tools.</p>\r\n<p>Furthermore, the increasing focus on data collection pertaining to…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1087129",
          "publishedOn": "2022-01-03T12:30:00.000Z",
          "wordCount": 924,
          "title": "IoT Drives Growth of Intelligent Transportation Systems"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1086932",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9976535685?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9976535685?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>Happy new year!</p>\r\n<p></p>\r\n<p>This year , let’s start with an issue that gained so much prominence over the last two years: mental health. A variety of AI techniques and strategies have been employed in mental health.</p>\r\n<p>In this post, we summarise the findings based on a paper (link below)</p>\r\n<p> </p>\r\n<h2>Summary</h2>\r\n<ul>\r\n<li>The main predictor…</li>\r\n</ul>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1086932",
          "publishedOn": "2022-01-02T21:00:00.000Z",
          "wordCount": 1118,
          "title": "Artificial Intelligence for Mental Health"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1086783",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9972630085?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9972630085?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p><span style=\"font-size: 8pt;\">High-rise construction in the Denny Triangle area of Seattle. Source: Bruce Englehardt, <em>Wikimedia Commons</em>, May 2020</span></p>\r\n<p></p>\r\n<p><span style=\"font-weight: 400;\">I've been looking for fresh use cases that tell the knowledge graph adoption story from new angles. Which industries, for instance, been having…</span></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1086783",
          "publishedOn": "2022-01-01T05:00:00.000Z",
          "wordCount": 1643,
          "title": "Building a hypergraph-based semantic knowledge sharing environment for construction"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1086775",
          "author": null,
          "description": "<h3><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9971838280?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9971838280?profile=RESIZE_710x\" width=\"720\"></img></a></h3>\r\n<p><span style=\"font-weight: 400;\">Semantic data consultant @EmekaOkoye made a couple of great points on Twitter back in November and early December 2021:</span></p>\r\n<ol>\r\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">“I am amused how some of us are pretending that #SmartAgent [tech] is not going to be a thing.” …</span></li>\r\n</ol>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1086775",
          "publishedOn": "2021-12-31T19:30:00.000Z",
          "wordCount": 1756,
          "title": "Lying to blockchains and other Web3 dilemmas"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1086758",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9970487888?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9970487888?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p><span style=\"font-size: 14pt;\"><strong>What is a digital worker?</strong></span><br></br> <br></br> Digital workers are your software-based co-workers. Think Siri, Alexa, or Cortana, not exactly like them but to some extent. Yes, we are calling them your co-workers or you can refer to them as your virtual assistants if you like. They will help you in doing…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1086758",
          "publishedOn": "2021-12-31T06:00:00.000Z",
          "wordCount": 1343,
          "title": "Digital Workers in a Business Context"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1086573",
          "author": null,
          "description": "<p><span style=\"font-weight: 400;\"><span><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9968714658?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9968714658?profile=RESIZE_710x\" width=\"720\"></img></a></span></span></p>\r\n<p><span style=\"font-weight: 400;\">There are constant changes in the software development trends, but a few trends seem to be dominant in 2022. With the evolution of advanced technology, there has been a significant change in the software development landscape. Businesses need to keep up with these…</span></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1086573",
          "publishedOn": "2021-12-30T12:30:00.000Z",
          "wordCount": 1577,
          "title": "Top 12 Software Development Trends in 2022 You Must Watch Out"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1086461",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9965282063?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-center\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9965282063?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>Companies today need to be nimble and ready in the face of constantly evolving technology and changing consumer preferences. To achieve this, organizations are switching to Amazon Web Services (AWS). It enables companies to rapidly deploy and scale technology that meets the growing (or shrinking) demand without having to invest in expensive IT…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1086461",
          "publishedOn": "2021-12-29T13:30:00.000Z",
          "wordCount": 1217,
          "title": "AWS Cloud Security: Best Practices"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085499",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9949632861?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9949632861?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p></p>\r\n<p>Okay, so I am still confused by the concept of a Data Mesh (I’m a slow learner).</p>\r\n<p>Recently, I wrote a blog that posed the question: <a href=\"https://www.datasciencecentral.com/profiles/blogs/are-data-meshes-the-enabler-of-the-marginal-propensity-to-reuse\">Are Data Meshes the Enabler of the Marginal Propensity to Reuse</a>? The ability to…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085499",
          "publishedOn": "2021-12-27T16:30:00.000Z",
          "wordCount": 1872,
          "title": "Are Data Meshes Really Data Marts with Conformed Dimensions?"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085380",
          "author": null,
          "description": "<p><span style=\"font-weight: 400;\"><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9949663901?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9949663901?profile=RESIZE_710x\" width=\"720\"></img></a></span></p>\r\n<p><span style=\"font-weight: 400;\">Web scraping deals with HTML almost exclusively. In nearly all cases, what is required is a small sample from a very large file (e.g. pricing information from an ecommerce page). Therefore, an essential part of scraping is searching through an HTML document and finding the correct…</span></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085380",
          "publishedOn": "2021-12-27T08:30:00.000Z",
          "wordCount": 1710,
          "title": "Optimal Scraping Technique: CSS Selector, XPath, & RegEx"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085290",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9948211856?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9948211856?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p><strong>Introduction</strong></p>\r\n<p>With the explosion of digital media and the resultant avalanche of constantly increasing user data being created and collected daily, businesses, institutions, and organizations need better ways to manage that data beyond the standard suite of tools. The description of a Data Management Platform or Augmented Data…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085290",
          "publishedOn": "2021-12-27T06:00:00.000Z",
          "wordCount": 1066,
          "title": "What is a Data Management Platform?"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085364",
          "author": null,
          "description": "<h2><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9947285672?profile=original\" rel=\"noopener\" target=\"_blank\"></a></h2>\r\n<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9949734895?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9949734895?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>Late in the year, we are suddenly hearing a new term ‘<strong>web 3’</strong></p>\r\n<p>For the reasons I describe below, this trend will be significant</p>\r\n<p>In this article, I take the perspective of a neutral…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085364",
          "publishedOn": "2021-12-26T19:00:00.000Z",
          "wordCount": 1305,
          "title": "The web 3 meme"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1078655",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9942574898?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9942574898?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<ul>\r\n<li>Thousands of people are injured by turbulence every year.</li>\r\n<li>New machine learning model gives high-accuracy, 10 second warning for turbulence.</li>\r\n<li>The model may lessen in-flight injuries and save lives.</li>\r\n</ul>\r\n<p>Turbulence is one of the leading cause of injuries on passenger planes and—if you don’t have your seat belt on—those…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1078655",
          "publishedOn": "2021-12-24T15:30:00.000Z",
          "wordCount": 1383,
          "title": "Machine Learning can give a 10 second Turbulence Warning"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085080",
          "author": null,
          "description": "<p><span style=\"font-weight: 400;\"><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9939242664?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9939242664?profile=RESIZE_710x\" width=\"720\"></img></a></span></p>\r\n<p><span style=\"font-weight: 400;\">Let’s imagine that you have signed a contract with a mobile app development firm. It seems that you have agreed upon everything: terms, budget, the scope of work, and other things. But six months later, it turns out that the developers are not up to their work and the product is…</span></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085080",
          "publishedOn": "2021-12-23T12:00:00.000Z",
          "wordCount": 1232,
          "title": "How to Find a Mobile App Development Firm: Tips for Businesses"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085306",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9949742696?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9949742696?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<h2><span id=\"What_is_a_Scrum_Board\"><b>What is a Scrum Board?</b></span></h2>\r\n<p>As Scrum is one of the popular frameworks to break down complex problems into smaller tasks, Scrum board is a project management software used to visually represent these tasks and Scrum sprints. The scrum board is the center of every sprint meeting to get regular updates…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085306",
          "publishedOn": "2021-12-23T12:00:00.000Z",
          "wordCount": 1354,
          "title": "What is a Scrum Board? What is the Difference Between a Scrum & Kanban Board?"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085158",
          "author": null,
          "description": "<p><span style=\"font-weight: 400;\"><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9950166067?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9950166067?profile=RESIZE_710x\" width=\"720\"></img></a></span></p>\r\n<p><span style=\"font-weight: 400;\">The COVID-19 pandemic has thrown organizations into disarray—the increased usage of conferencing and collaboration services by employees working from home strains back-end support services. </span></p>\r\n<p><span style=\"font-weight: 400;\">And growing traffic on networks connecting…</span></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085158",
          "publishedOn": "2021-12-23T10:00:00.000Z",
          "wordCount": 1934,
          "title": "Cloud, Cost and Containers three C's in Cloud-Computing in post-COVID"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084882",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9949749054?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9949749054?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p><span>Scraping Google SERPs (search engine result pages) is as straightforward or as complicated as the tools we use. For this tutorial, we’ll be using Scrapy, a web scraping framework designed for Python. Python and Scrapy combine to create a powerful duo that we can use to scrape almost any website.</span></p>\r\n<p><span>Scrapy has many useful…</span></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084882",
          "publishedOn": "2021-12-22T16:00:00.000Z",
          "wordCount": 3001,
          "title": "Scraping Data from Google Search Using Python and Scrapy"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085049",
          "author": null,
          "description": "<p></p>\r\n<p><strong><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9936688655?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9936688655?profile=RESIZE_710x\" width=\"720\"></img></a></strong></p>\r\n<p><strong>3D Secure Authentication Industry:</strong></p>\r\n<ul>\r\n<li>3D secure authentication is a fraud-prevention security system for credit and debit card transactions processed online. During card payments, 3D secure authentication adds an additional layer of security. It provides clients with a safe authentication step before…</li>\r\n</ul>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085049",
          "publishedOn": "2021-12-22T16:00:00.000Z",
          "wordCount": 1276,
          "title": "3D Secure Authentication: A New Era For Payment Authentication And Customer Experience"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085116",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9949757081?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9949757081?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>The exchange of data between corporations is known as data monetization. It is the process of earning income or creating new revenue streams by utilizing data, which is estimated to support expansion of the global data monetization market. Direct data monetization as well as indirect data monetization is the two forms of data monetization. The sale of…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085116",
          "publishedOn": "2021-12-22T11:30:00.000Z",
          "wordCount": 1341,
          "title": "Using Cloud and AI Technologies to Make Data Driven Decisions For Monetization"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085036",
          "author": null,
          "description": "<p><span>The discipline of data annotation and labeling is growing in popularity and significance throughout the world. The global market for data annotation tools is expected to reach </span><strong class=\"js iv\">$2.57 billion by 2027</strong><span>, according to a published report.</span></p>\r\n<p class=\"jq jr iu js b jt ju iy jv jw jx jb jy jz ka kb kc kd ke kf kg kh ki kj kk kl em jp\" id=\"412f\">For robots, drones, and vehicles to gain increasing levels of au<span id=\"rmm\">t</span>onomy,…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085036",
          "publishedOn": "2021-12-22T10:30:00.000Z",
          "wordCount": 1106,
          "title": "Outsourcing Data Annotation Work"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084865",
          "author": null,
          "description": "<p><span style=\"font-weight: 400;\"><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9949819074?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9949819074?profile=RESIZE_710x\" width=\"720\"></img></a></span></p>\r\n<p><span style=\"font-weight: 400;\">Java is the</span> <a href=\"https://www.oracle.com/ar/a/ocom/docs/java-strength-in-numbers.pdf\"><span style=\"font-weight: 400;\">#1 programming language for Big Data</span></a><span style=\"font-weight: 400;\">, Analytics, DevOps, and AI. It is consistently the first choice for…</span></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084865",
          "publishedOn": "2021-12-22T10:00:00.000Z",
          "wordCount": 1638,
          "title": "Top 7 Reasons Data Scientists Should Know Java Programming Language"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084850",
          "author": null,
          "description": "<div class=\"dsc_primaryImage\"><span style=\"font-family: arial, helvetica, sans-serif;\"><a href=\"https://www.datasciencecentral.com/profiles/blog/list?tag=dsc_newsletter\" rel=\"noopener\" target=\"_blank\"><img alt=\"Winter Solstice Shenanigans\" src=\"https://multimedia.getresponse360.com/datascience-B/photos/0fdfb61a-5192-4b3b-bf19-15bdb6d3da2b.jpg\" style=\"vertical-align: baseline;\" title=\"Winter Solstice Shenanigans\" width=\"720\"></img></a></span></div>\r\n<table style=\"width: 726px;\">\r\n<tbody><tr><td><p dir=\"ltr\">Today, the 21st of December, is the Winter Solstice, unless you happen to be in sub-Saharan Africa, Australia, Argentina, or Antarctica, or other points south of the equator. In that case, today is the first day…</p>\r\n</td>\r\n</tr>\r\n</tbody>\r\n</table>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084850",
          "publishedOn": "2021-12-22T00:30:00.000Z",
          "wordCount": 2524,
          "title": "DSC Weekly Digest 21 December 2021: Winter Solstice Shenanigans"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084676",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9933327478?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9933327478?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>We live in a world where the lines between private and public data are blurred. With access to almost every aspect of data, businesses slowly dive into the lives of consumers and surprise them by showing them the products they've been looking for at random, anywhere. Data discomfort is slowly subsiding, people are still cautious and for the right…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084676",
          "publishedOn": "2021-12-21T11:00:00.000Z",
          "wordCount": 1236,
          "title": "Steps in securing your enterprise mobile application"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084757",
          "author": null,
          "description": "<p><span style=\"font-weight: 400;\"><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9933233072?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9933233072?profile=RESIZE_710x\" width=\"720\"></img></a></span></p>\r\n<p><span style=\"font-weight: 400;\">We have noticed how the industrial revolution transformed the shape of our modernized world. We got big industries, endowed the computer, and made significant progress in the IT sector.</span></p>\r\n<p><span style=\"font-weight: 400;\">But, this change had its consequences too. With an…</span></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084757",
          "publishedOn": "2021-12-21T11:00:00.000Z",
          "wordCount": 2088,
          "title": "Ways To Make Cloud Data More Environmentally Friendly"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084740",
          "author": null,
          "description": "<p><span style=\"font-weight: 400;\"><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9933241254?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9933241254?profile=RESIZE_710x\" width=\"720\"></img></a></span></p>\r\n<p><span style=\"font-weight: 400;\">One of the most exciting sectors of the IT business is web development. There is a vast library of tools and libraries available, many of which strive to improve an application's functionality and efficiency. However, taking advantage of every fresh opportunity isn't a good idea…</span></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084740",
          "publishedOn": "2021-12-21T05:30:00.000Z",
          "wordCount": 1576,
          "title": "Building a Scalable Application with React and Redux: A Step-by-Step Guide"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084804",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9930070895?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9930070895?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>Chief Data Officers (CDO) and Chief Data Analytics Officers (CDAO) are under intense pressure to find ways to “monetize” their growing volumes of data. While some organizations seek “monetization” by trying to sell their data, as I discussed in the “…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084804",
          "publishedOn": "2021-12-20T23:00:00.000Z",
          "wordCount": 1818,
          "title": "The Economics of Data Products"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084215",
          "author": null,
          "description": "<div class=\"wpb_text_column wpb_content_element vc_custom_1636464703663 tm-animation move-up animate\"><div class=\"wpb_wrapper\"><p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9917146659?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9917146659?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>Back in 2014, Gartner placed the field of prescriptive analytics at the beginning of the ‘Peak of Inflated Expectations’ in their Hype Cycle of Emerging Technologies. And also went on to project the prescriptive analytics…</p>\r\n</div>\r\n</div>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084215",
          "publishedOn": "2021-12-20T22:54:43.000Z",
          "wordCount": 1470,
          "title": "Prescriptive Analytics – The Final Frontier"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1083447",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9929648266?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9929648266?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>I was hanging around the LinkedIn water cooler the other day, just minding my own business, when I came across this very interesting graphic (Figure 1) from <a href=\"https://www.linkedin.com/in/brentdykes/\">Brent Dykes</a> titled “…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1083447",
          "publishedOn": "2021-12-20T19:08:22.000Z",
          "wordCount": 1765,
          "title": "Using Design Thinking to Win the First and the Last Mile"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084495",
          "author": null,
          "description": "<div class=\"post-header text-fields\"><div class=\"text post-header__introduction\"><div class=\"post-header text-fields\"><div class=\"text post-header__introduction\"><p style=\"text-align: center;\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9930051081?profile=RESIZE_710x\" width=\"720\"></img></p>\r\n</div>\r\n</div>\r\n</div>\r\n</div>\r\n<div class=\"text-fields u-padding\"><div class=\"text-fields__container\"><div class=\"text text-fields__text\"><div class=\"text-fields u-padding\"><div class=\"text-fields__container\"><div class=\"text text-fields__text\"><p>In today’s data-driven reality, data…</p>\r\n</div>\r\n</div>\r\n</div>\r\n</div>\r\n</div>\r\n</div>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084495",
          "publishedOn": "2021-12-20T10:30:00.000Z",
          "wordCount": 1946,
          "title": "Most data warehouse projects fail. Here’s how not to."
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084560",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9926540062?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9926540062?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>Artificial Intelligence has a long history of oscillating between two somewhat contradictory poles. On one side, exemplified by Noam Chomsky, Marvin Minsky, Seymour Papert, and many others, is the idea that cognitive intelligence was algorithmic in nature - that there were a set of fundamental precepts that formed the foundation of language, and by…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084560",
          "publishedOn": "2021-12-20T02:00:00.000Z",
          "wordCount": 2136,
          "title": "Where Semantics and Machine Learning Converge"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084609",
          "author": null,
          "description": "<p><br></br> <a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9926326071?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9926326071?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>Its that time of the year and while I do not like predictions, I think there is one which I want to talk about</p>\r\n<p>It’s a bit specific so needs some context</p>\r\n<p>From an AI standpoint, 2021 has been the year of large language models like  <a href=\"https://en.wikipedia.org/wiki/GPT-3\">GPT-3</a></p>\r\n<p>And that has led to near-magical…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084609",
          "publishedOn": "2021-12-19T23:30:00.000Z",
          "wordCount": 964,
          "title": "Large Language Models Are Leading low-code AI Applications in the Cloud"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084383",
          "author": null,
          "description": "<p><span style=\"font-size: 12pt;\"><span><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9924034458?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9924034458?profile=RESIZE_710x\" width=\"720\"></img></a></span></span></p>\r\n<p><span style=\"font-size: 12pt;\">A labyrinth is a structure in the inner ear.  The use of the term labyrinthology might sometimes relate to this part of the human anatomy.  Alternatively, some people think of a labyrinth as a maze or a body of routes and passages.  Years ago, I purchased some land…</span></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084383",
          "publishedOn": "2021-12-19T03:30:00.000Z",
          "wordCount": 1364,
          "title": "Using Labyrinthology to Study the Progression of Experiences"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1078656",
          "author": null,
          "description": "<p> <a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9922845287?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9922845287?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<ul>\r\n<li>A new study evaluates ML models that classify fake news from fact.</li>\r\n<li>The best models can only achieve up to 77.2% accuracy.</li>\r\n<li>AI will probably never be able to fully replace the nuanced analysis of human journalists.</li>\r\n</ul>\r\n<p><strong>Many Natural Language Processing (NLP) techniques exist for detecting “fake news”.…</strong></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1078656",
          "publishedOn": "2021-12-18T15:00:00.000Z",
          "wordCount": 1396,
          "title": "Machine Learning and the Challenge of Predicting Fake News"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084370",
          "author": null,
          "description": "<p><span style=\"font-weight: 300;\"><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9926815486?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9926815486?profile=RESIZE_710x\" width=\"720\"></img></a> The benefits of 5G – delivering super-fast, low-latency broadband to consumers, wirelessly, wherever they are – promises to revolutionise the way we interact with one another and businesses. For the first time consumers are going to have immediate, portable access to the kind of speeds and connectivity that was previously only…</span></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084370",
          "publishedOn": "2021-12-18T04:00:00.000Z",
          "wordCount": 1415,
          "title": "Will 5G Bring the Death of Brick and Mortar Banking?"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084363",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9921348679?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9921348679?profile=RESIZE_710x\" width=\"720\"></img></a> <span style=\"font-size: 8pt;\">Manufacturing Line, <em>Wikimedia Commons</em>, 2014</span><br></br> <br></br> <span style=\"font-weight: 400;\">Fakery certainly isn’t limited to news and social media. The Organisation for Economic Co-operation and Development (OECD) estimated in 2016 that 6.8 percent of physical goods imported by the EU were fakes. …</span></p>\r\n<p></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084363",
          "publishedOn": "2021-12-18T00:30:00.000Z",
          "wordCount": 2040,
          "title": "How data decentralization is already benefiting enterprises–and society"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084503",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9926827266?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9926827266?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>Almost all Software-as-a-Service (SaaS) businesses start with a \"visionary idea\" - a concept that can revolutionize the industry. Yet, the world has witnessed numerous passionate founders invest their energy, time, and money in a product that solves a problem that nobody cares about. It's the responsibility of a leader to determine if the product is…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084503",
          "publishedOn": "2021-12-17T07:00:00.000Z",
          "wordCount": 2584,
          "title": "How to Build a Minimum Viable Product (MVP): A Detailed Guide"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084194",
          "author": null,
          "description": "<p><b><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9919075296?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9919075296?profile=RESIZE_710x\" width=\"720\"></img></a></b></p>\r\n<p><b>Technology</b> is emerging as well as changing the future of the <b>education</b> <b>system</b> by turning the traditional methods of gaining knowledge into a comprehensive way of learning with the help of augmented reality and simulation tools. In the future, this will actively help the education as well as educators to manage time for…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084194",
          "publishedOn": "2021-12-17T04:30:00.000Z",
          "wordCount": 1367,
          "title": "Ways Technology is offering Innovative Solutions to the Education System"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084052",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9916643293?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9916643293?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>It is no secret that technology has evolved at a rapid pace and that this fast-paced evolution has brought about intense changes across the world. Today’s manufacturing companies need advanced services to maximize efficiency and productivity. Thanks to technology and innovation, today’s manufacturing industry today has undergone drastic changes. From…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084052",
          "publishedOn": "2021-12-16T13:00:00.000Z",
          "wordCount": 1199,
          "title": "Top Reasons Why Manufacturing Companies Need DevOps"
        }
      ]
    },
    {
      "title": "John D. Cook",
      "feedUrl": "https://www.johndcook.com/blog/feed",
      "siteUrl": "https://www.johndcook.com/blog",
      "articles": [
        {
          "id": "https://www.johndcook.com/blog/?p=94271",
          "author": "John",
          "description": "The other day I wrote about orthogonal Latin squares. Two Latin squares are orthogonal if the list of pairs of corresponding elements in the two squares contains no repeats. A self-orthogonal Latin square (SOLS) is a Latin square that is orthogonal to its transpose. Here’s an example of a self-orthogonal Latin square: 1 7 6 […]\nSelf-Orthogonal Latin Squares first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2022/01/13/self-orthogonal-latin-squares/",
          "publishedOn": "2022-01-13T17:36:20.000Z",
          "wordCount": 820,
          "title": "Self-Orthogonal Latin Squares"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=94279",
          "author": "John",
          "description": "There’s a rule of systems thinking that improving part of a system can often make the system as a whole worse. One example of this is Braess’ Paradox that adding roads can make traffic worse, and closing roads can improve traffic. Another example is the paradox of enrichment: Increasing the food available to an ecosystem […]\nBetter parts, worse system first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2022/01/13/better-parts-worse-system/",
          "publishedOn": "2022-01-13T13:07:02.000Z",
          "wordCount": 524,
          "title": "Better parts, worse system"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=94256",
          "author": "John",
          "description": "A blog post about queueing theory that I wrote back in 2008 continues to be popular. The post shows that when a system is close to capacity, adding another server dramatically reduces the wait time. In that example, going from one teller to two tellers doesn’t make service twice as fast but 93 times as […]\nQueueing theory equations first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2022/01/12/mm2/",
          "publishedOn": "2022-01-12T19:08:35.000Z",
          "wordCount": 557,
          "title": "Queueing theory equations"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=91481",
          "author": "John",
          "description": "Swanson’s rule of thumb [1] says that the mean of a moderately skewed probability distribution can be approximated by the weighted average of the 10th, 50th, and 90th percentile, with weights 0.3, 0.4, and 0.3 respectively. Because it is based on percentiles, the rule is robust to outliers. Swanson’s rule is used in the oil […]\nSwanson’s rule of thumb first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2022/01/12/swansons-rule/",
          "publishedOn": "2022-01-12T15:46:45.000Z",
          "wordCount": 615,
          "title": "Swanson’s rule of thumb"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=58085",
          "author": "John",
          "description": "I was stunned when my client said that a database query that I asked them to run would cost the company $100,000 per year. I had framed my question in the most natural way, not thinking that at the company’s scale it would be worth spending some time thinking about the query. Things have somewhat […]\nRandom sampling to save money first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2022/01/11/random-sampling/",
          "publishedOn": "2022-01-11T22:12:43.000Z",
          "wordCount": 383,
          "title": "Random sampling to save money"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=94145",
          "author": "John",
          "description": "Suppose you create an n × n Latin square filled with the first n letters of the Roman alphabet. This means that each letter appears exactly once in each row and in each column. You could repeat the same exercise only using the Greek alphabet. Is it possible to find two n × n Latin […]\nGreco-Latin squares and magic squares first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2022/01/11/greco-latin-squares/",
          "publishedOn": "2022-01-11T17:10:02.000Z",
          "wordCount": 734,
          "title": "Greco-Latin squares and magic squares"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=94135",
          "author": "John",
          "description": "In a n×n Latin square, each of the numbers 1 through n appears exactly once in each row and column. For example, the 5 × 5 square below is a Latin square. If we placed a rook on each square numbered 1, the rooks would not attack each other since no two rooks would be […]\nLatin squares and 3D chess first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2022/01/10/latin-squares-and-3d-chess/",
          "publishedOn": "2022-01-11T01:18:01.000Z",
          "wordCount": 542,
          "title": "Latin squares and 3D chess"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=94125",
          "author": "John",
          "description": "Five years ago I recommended the book Learning Base R. Here’s the last paragraph of my review: Now there are more books on R, and some are more approachable to non-statisticians. The most accessible one I’ve seen so far is Learning Base R by Lawrence Leemis. It gets into statistical applications of R—that is ultimately why […]\nNew R book first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2022/01/10/new-r-book/",
          "publishedOn": "2022-01-11T00:18:02.000Z",
          "wordCount": 396,
          "title": "New R book"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=94087",
          "author": "John",
          "description": "A while back I wrote about computational survivalism, being prepared to work productively in a restricted environment. The first time I ran into computational survivalism was when someone said to me “I prefer Emacs, but I use vi because I only want to use tools I can count on being installed everywhere.” I thought that […]\nComputational asceticism first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2022/01/10/computational-asceticism/",
          "publishedOn": "2022-01-10T15:01:32.000Z",
          "wordCount": 897,
          "title": "Computational asceticism"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=93966",
          "author": "John",
          "description": "Here’s a surprising theorem [1]. (Beatty’s theorem) Let a and b be any pair of irrational numbers greater than 1 with 1/a + 1/b = 1. Then the sequences { ⌊na⌋ } and { ⌊nb⌋ } contain every positive integer without repetition. Illustration Here’s a little Python code to play with this theorem.We set a […]\nBeatty’s theorem first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2022/01/09/beattys-theorem/",
          "publishedOn": "2022-01-09T20:49:45.000Z",
          "wordCount": 867,
          "title": "Beatty’s theorem"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=93958",
          "author": "John",
          "description": "In his book Mastering Regular Expressions, Jeffrey Friedl uses corner quotes to delimit regular expressions. Here’s an example I found by opening his book a random: ⌜(\\.\\d\\d[1-9]?)\\d*⌟ The upper-left corner at the beginning and the lower-right corner at the end are not part of the regular expression. This particularly comes in handy if a regular […]\nCorner quotes in Unicode first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2022/01/08/corner-quotes-in-unicode/",
          "publishedOn": "2022-01-09T01:01:45.000Z",
          "wordCount": 613,
          "title": "Corner quotes in Unicode"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=93920",
          "author": "John",
          "description": "When does the equation x2 + 7 = 2n have integer solutions? This is an interesting question, but why would anyone ask it? This post looks at three paths that have led to this problem. Ramanujan Ramanujan [1] considered this problem in 1913. He found five solutions and conjectured that there were no more. Then […]\nThree paths converge first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2022/01/08/diophantine/",
          "publishedOn": "2022-01-08T10:48:27.000Z",
          "wordCount": 661,
          "title": "Three paths converge"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=93749",
          "author": "John",
          "description": "I was thumbing through a new book on causal inference, The Effect by Nick Huntington-Klein, and the following diagram caught my eye. Then it made my head hurt. It looks like a category theory diagram. What’s that doing in a book on causal inference? And if it is a category theory diagram, something’s wrong. Either […]\nOne diagram, two completely different meanings first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2022/01/05/diagrams/",
          "publishedOn": "2022-01-05T23:12:51.000Z",
          "wordCount": 328,
          "title": "One diagram, two completely different meanings"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=93700",
          "author": "John",
          "description": "My favorite part of this post by Ian Miellis the introduction. The article is about shell commands, but the introduction brings up a more general point. … there are thousands of reusable patterns I’ve picked up … Unfortunately, I’ve forgotten about 95% of them. … The point is to reflect on what actually stuck, so […]\nMemorable techniques first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2022/01/05/memorable-techniques/",
          "publishedOn": "2022-01-05T15:01:41.000Z",
          "wordCount": 637,
          "title": "Memorable techniques"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=93265",
          "author": "John",
          "description": "I recently received a review copy of Andrew Witt’s new book Formulations: Architecture, Mathematics, and Culture. The Hankel function on the cover is the first clue that this book contains some advanced mathematics. Or rather, it references some advanced mathematics. I’ve only skimmed the book so far, but I didn’t see any equations. Hankel functions […]\nArchitecture and Math first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2022/01/01/architecture-and-math/",
          "publishedOn": "2022-01-01T19:32:30.000Z",
          "wordCount": 380,
          "title": "Architecture and Math"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=93255",
          "author": "John",
          "description": "I’ve written a couple posts about the Golay problem recently, first here then here. The problem is to find all values of N and n such that is a power of 2, say 2p. Most solutions apparently fall into three categories: n = 0 or n = N, N is odd and n = (N-1)/2, […]\nTurning the Golay problem sideways first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2022/01/01/turning-the-golay-problem-sideways/",
          "publishedOn": "2022-01-01T16:46:19.000Z",
          "wordCount": 1030,
          "title": "Turning the Golay problem sideways"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=93159",
          "author": "John",
          "description": "I played around with what I’m calling the Golay problem over Christmas and wrote a blog post about it. I rewrote the post as I learned more about the problem due to experimentation and helpful feedback via comments and Twitter. In short, the Golay problem is to classify the values of N and n such […]\nUpdate on the Golay problem first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/31/update-on-the-golay-problem/",
          "publishedOn": "2021-12-31T14:53:16.000Z",
          "wordCount": 504,
          "title": "Update on the Golay problem"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=93062",
          "author": "John",
          "description": "Stanislaw Ulam once said Using a term like nonlinear science is like referring to the bulk of zoology as the study of non-elephant animals. There is only one way to be linear, but there are many ways to not be linear. A similar observation applies to non-classical logic. There are many ways to not be […]\nNames and numbers for modal logic axioms first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/30/modal-axioms/",
          "publishedOn": "2021-12-30T19:49:32.000Z",
          "wordCount": 480,
          "title": "Names and numbers for modal logic axioms"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=93026",
          "author": "John",
          "description": "The previous post looked at two of the five Lagrange points of the Sun-Earth system. These points, L1 and L2, are located on either side of Earth along a line between the Earth and the Sun. The third Lagrange point, L3, is located along that same line, but on the opposite side of the Sun. […]\nWhen do two-body systems have stable Lagrange points? first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/30/stable-lagrange-points/",
          "publishedOn": "2021-12-30T11:30:05.000Z",
          "wordCount": 763,
          "title": "When do two-body systems have stable Lagrange points?"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=92910",
          "author": "John",
          "description": "The James Webb Space Telescope (JWST) is on its way to the Lagrange point L2 of the Sun-Earth system. Objects in this location will orbit the Sun at a fixed distance from Earth. There are five Lagrange points, L1 through L5. The points L1, L2, and L3 are unstable, and points L4 and L5 are […]\nFinding Lagrange points L1 and L2 first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/28/lagrange-points-l1-and-l2/",
          "publishedOn": "2021-12-28T21:34:34.000Z",
          "wordCount": 577,
          "title": "Finding Lagrange points L1 and L2"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=92536",
          "author": "John",
          "description": "These were the most popular posts from 2021 on this site, listed in chronological order. Simple gamma approximation Floor, ceiling, bracket Evolution of random number generators Format text in twitter Where has all the productivity gone? Computing zeta(3) Morse code palindromes Much less than, much greater than Monads and macros Partial functions\nMost popular posts of 2021 first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/28/most-popular-posts-of-2021/",
          "publishedOn": "2021-12-28T13:00:33.000Z",
          "wordCount": 210,
          "title": "Most popular posts of 2021"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=92751",
          "author": "John",
          "description": "Sine and cosine have power series with simple coefficients, but tangent does not. Students are shocked when they see the power series for tangent because there is no simple expression for the power series coefficients unless you introduce Bernoulli numbers, and there’s no simple expression for Bernoulli numbers. The perception is that sine and cosine […]\nThe exception case is normal first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/27/the-exception-case-is-normal/",
          "publishedOn": "2021-12-27T19:41:17.000Z",
          "wordCount": 385,
          "title": "The exception case is normal"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=92740",
          "author": "John",
          "description": "Marcel Golay noticed that and realized that this suggested it might be possible to create a perfect code of length 23. I wrote a Twitter thread on this that explains how this relates to coding theory (and to sporadic simple groups). This made me wonder how common it is for cumulative sums of binomial coefficients […]\nBinomial sums and powers of 2 first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/27/golay/",
          "publishedOn": "2021-12-27T15:25:11.000Z",
          "wordCount": 487,
          "title": "Binomial sums and powers of 2"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=92486",
          "author": "John",
          "description": "The Christmas hymn “O Come, O Come, Emmanuel” is a summary of the seven “O Antiphons,” sung on December 17 though 23, dating back to the 8th century [1]. The seven antiphons are O Sapientia O Adonai O Radix Jesse O Clavis David O Oriens O Rex Gentium O Emmanuel The corresponding verses of “O […]\nO Come, O Come, Emmanuel: condensing seven hymns into one first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/24/o-come-o-come-emmanuel/",
          "publishedOn": "2021-12-24T16:09:46.000Z",
          "wordCount": 300,
          "title": "O Come, O Come, Emmanuel: condensing seven hymns into one"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=92157",
          "author": "John",
          "description": "Yesterday I wrote about how to multiply octets of real numbers, the octonions. Today I’ll show how to create an error correcting code from the octonions. In fact, we’ll create a perfect code in the sense explained below. We’re going to make a code out of octonions over a binary field. That is, we’re going […]\nError correcting code from octonions first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/20/octonion-ecc/",
          "publishedOn": "2021-12-20T15:41:01.000Z",
          "wordCount": 953,
          "title": "Error correcting code from octonions"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=92096",
          "author": "John",
          "description": "Yesterday I wrote about the fact that quaternions, unlike complex numbers, can form conjugates via a series of multiplications and additions. This post will show that you can do something similar with octonions. If x is an octonion x = r0 e0 + r1 e1 + … + r7 e7 where all the r‘s are […]\nConjugate theorem for octonions first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/19/conjugate-octonions/",
          "publishedOn": "2021-12-19T23:33:28.000Z",
          "wordCount": 362,
          "title": "Conjugate theorem for octonions"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=92088",
          "author": "John",
          "description": "This post will present a way of multiplying octonions that’s easy to remember. Please note that there are varying conventions for how to define multiplication for octonions [1]. Octonions The complex numbers have one imaginary unit i, and the quaternions have three: i, j, and k. The octonions have seven, and so it makes sense […]\nHow to multiply octonions first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/19/multiply-octonions/",
          "publishedOn": "2021-12-19T23:27:37.000Z",
          "wordCount": 799,
          "title": "How to multiply octonions"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=92005",
          "author": "John",
          "description": "The conjugate of a complex number is the complex number Taking the conjugate flips over a complex number, taking its reflection in the real axis. Multiplication stretches and rotates complex numbers, and addition translates complex numbers. You can’t flip the complex plane over by any series of dilatations, rotations, and translations. The situation is different […]\nComplex Conjugates versus Quaternion Conjugates first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/18/quaternion-conjugate/",
          "publishedOn": "2021-12-18T18:54:45.000Z",
          "wordCount": 341,
          "title": "Complex Conjugates versus Quaternion Conjugates"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=91983",
          "author": "John",
          "description": "“… Things fall apart; the centre cannot hold …” — Yeats, The Second Coming   Center of a group The center of a group is the set of elements that commute with everything else in the group. For example, matrix multiplication is not commutative in general. You can’t count on AB being equal to BA, […]\nThe center may not hold first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/18/the-center-may-not-hold/",
          "publishedOn": "2021-12-18T15:22:11.000Z",
          "wordCount": 803,
          "title": "The center may not hold"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=91901",
          "author": "John",
          "description": "A positive integer n is said to be congruent if there exists a right triangle with area n such that the length of all three sides is rational. You could always choose one leg to have length n and the other to have length 2. Such a triangle would have area n and two rational […]\nThe congruent number problem first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/17/congruent-number/",
          "publishedOn": "2021-12-17T16:34:43.000Z",
          "wordCount": 891,
          "title": "The congruent number problem"
        }
      ]
    }
  ],
  "cliVersion": "1.12.0"
}