{
  "sources": [
    {
      "title": "ML in Production",
      "feedUrl": "https://mlinproduction.com/feed",
      "siteUrl": "https://mlinproduction.com",
      "articles": []
    },
    {
      "title": "Blog – Machine Learning Mastery",
      "feedUrl": "http://machinelearningmastery.com/blog/feed",
      "siteUrl": "https://machinelearningmastery.com",
      "articles": [
        {
          "id": "https://machinelearningmastery.com/?p=13157",
          "author": "Muhammad Asad Iqbal Khan",
          "description": "PyTorch is an open-source deep learning framework based on Python language. It allows you to build, train, and deploy deep […]\nThe post One-Dimensional Tensors in Pytorch appeared first on Machine Learning Mastery.",
          "link": "https://machinelearningmastery.com/one-dimensional-tensors-in-pytorch/",
          "publishedOn": "2021-12-29T21:30:13.000Z",
          "wordCount": 2819,
          "title": "One-Dimensional Tensors in Pytorch"
        },
        {
          "id": "https://machinelearningmastery.com/?p=13153",
          "author": "Stefania Cristina",
          "description": "Running your Python scripts is an important step in the development process, because it is in this manner that you’ll […]\nThe post Running and Passing Information to a Python Script appeared first on Machine Learning Mastery.",
          "link": "https://machinelearningmastery.com/running-and-passing-information-to-a-python-script/",
          "publishedOn": "2021-12-29T04:10:32.000Z",
          "wordCount": 2915,
          "title": "Running and Passing Information to a Python Script"
        },
        {
          "id": "https://machinelearningmastery.com/?p=13148",
          "author": "Adrian Tam",
          "description": "When an exception occurs in a Python program, often a traceback will be printed. Knowing how to read the traceback […]\nThe post Understanding Traceback in Python appeared first on Machine Learning Mastery.",
          "link": "https://machinelearningmastery.com/understanding-traceback-in-python/",
          "publishedOn": "2021-12-23T23:56:02.000Z",
          "wordCount": 4034,
          "title": "Understanding Traceback in Python"
        },
        {
          "id": "https://machinelearningmastery.com/?p=13128",
          "author": "Mehreen Saeed",
          "description": "Python is a fantastic programming language. It is likely to be your first choice for developing a machine learning or […]\nThe post Functional Programming In Python appeared first on Machine Learning Mastery.",
          "link": "https://machinelearningmastery.com/functional-programming-in-python/",
          "publishedOn": "2021-12-19T02:26:12.000Z",
          "wordCount": 3943,
          "title": "Functional Programming In Python"
        },
        {
          "id": "https://machinelearningmastery.com/?p=13138",
          "author": "Stefania Cristina",
          "description": "Classes are one of the fundamental building blocks of the Python language, which may be applied in the development of […]\nThe post Python Classes and Their Use in Keras appeared first on Machine Learning Mastery.",
          "link": "https://machinelearningmastery.com/python-classes-and-their-use-in-keras/",
          "publishedOn": "2021-12-14T21:53:58.000Z",
          "wordCount": 2830,
          "title": "Python Classes and Their Use in Keras"
        },
        {
          "id": "https://machinelearningmastery.com/?p=13124",
          "author": "Mehreen Saeed",
          "description": "Python is an awesome programming language! It is one of the most popular languages for developing AI and machine learning […]\nThe post More special features in Python appeared first on Machine Learning Mastery.",
          "link": "https://machinelearningmastery.com/python-special-features/",
          "publishedOn": "2021-12-09T21:00:48.000Z",
          "wordCount": 3486,
          "title": "More special features in Python"
        },
        {
          "id": "https://machinelearningmastery.com/?p=13129",
          "author": "Adrian Tam",
          "description": "The Python language syntax is quite powerful and expressive. Hence it is concise to express an algorithm in Python. Maybe […]\nThe post Some Language Features in Python appeared first on Machine Learning Mastery.",
          "link": "https://machinelearningmastery.com/some-language-features-in-python/",
          "publishedOn": "2021-12-08T20:17:26.000Z",
          "wordCount": 3180,
          "title": "Some Language Features in Python"
        }
      ]
    },
    {
      "title": "Machine Learning – Uber Engineering Blog",
      "feedUrl": "https://eng.uber.com/tag/machine-learning/feed",
      "siteUrl": "https://eng.uber.com",
      "articles": []
    },
    {
      "title": "AWS Machine Learning Blog",
      "feedUrl": "https://aws.amazon.com/blogs/machine-learning/feed",
      "siteUrl": "https://aws.amazon.com/blogs/machine-learning/",
      "articles": [
        {
          "id": "211e19b2ca986a29acad038c2a6fce1c14edcab4",
          "author": "Alak Eswaradass",
          "description": "Gartner predicts that by the end of 2024, 75% of enterprises will shift from piloting to operationalizing artificial intelligence (AI), and the vast majority of workloads will end up in the cloud in the long run. For some enterprises that plan to migrate to the cloud, the complexity, magnitude, and length of migrations may be […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/introducing-hybrid-machine-learning/",
          "publishedOn": "2021-12-23T20:13:09.000Z",
          "wordCount": 2361,
          "title": "Introducing hybrid machine learning"
        },
        {
          "id": "a1ba3dac7c896a0731f566fca4d13bf7c3d9a00c",
          "author": "Patrick Sard",
          "description": "Until recently, customers who wanted to use a deep learning (DL) framework with Amazon SageMaker Processing faced increased complexity compared to those using scikit-learn or Apache Spark. This post shows you how SageMaker Processing has simplified running machine learning (ML) preprocessing and postprocessing tasks with popular frameworks such as PyTorch, TensorFlow, Hugging Face, MXNet, and […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/use-deep-learning-frameworks-natively-in-amazon-sagemaker-processing/",
          "publishedOn": "2021-12-23T19:57:34.000Z",
          "wordCount": 2193,
          "title": "Use deep learning frameworks natively in Amazon SageMaker Processing"
        },
        {
          "id": "4ad6604866aa6f367ad543ba773b887ee5278e55",
          "author": "Bob Strahan",
          "description": "Your contact center connects your business to your community, enabling customers to order products, callers to request support, clients to make appointments, and much more. When calls go well, callers retain a positive image of your brand, and are likely to return and recommend you to others. And the converse, of course, is also true. […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/live-call-analytics-for-your-contact-center-with-amazon-language-ai-services/",
          "publishedOn": "2021-12-18T00:47:33.000Z",
          "wordCount": 4155,
          "title": "Live call analytics for your contact center with Amazon language AI services"
        },
        {
          "id": "ce2f56854ba560eea06ff25c4fb2f22d80b1505d",
          "author": "Andrew Kane",
          "description": "Your contact center connects your business to your community, enabling customers to order products, callers to request support, clients to make appointments, and much more. Each conversation with a caller is an opportunity to learn more about that caller’s needs, and how well those needs were addressed during the call. You can uncover insights from […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/post-call-analytics-for-your-contact-center-with-amazon-language-ai-services/",
          "publishedOn": "2021-12-18T00:38:22.000Z",
          "wordCount": 4014,
          "title": "Post call analytics for your contact center with Amazon language AI services"
        },
        {
          "id": "da9379ec44cc5e0492af56eff2007df098992e38",
          "author": "Jonathan Chung",
          "description": "In many industries, including financial services, banking, healthcare, legal, and real estate, automating document handling is an essential part of the business and customer service. In addition, strict compliance regulations make it necessary for businesses to handle sensitive documents, especially customer data, properly. Documents can come in a variety of formats, including digital forms or […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/build-custom-amazon-sagemaker-pytorch-models-for-real-time-handwriting-text-recognition/",
          "publishedOn": "2021-12-16T20:02:18.000Z",
          "wordCount": 3639,
          "title": "Build custom Amazon SageMaker PyTorch models for real-time handwriting text recognition"
        },
        {
          "id": "eb7a81155a6bd57d09b8fff92c593ab73c284a36",
          "author": "Yu Liu",
          "description": "Natural language processing (NLP) has been a hot topic in the AI field for some time. As current NLP models get larger and larger, data scientists and developers struggle to set up the infrastructure for such growth of model size. For faster training time, distributed training across multiple machines is a natural choice for developers. […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/achieve-35-faster-training-with-hugging-face-deep-learning-containers-on-amazon-sagemaker/",
          "publishedOn": "2021-12-15T22:46:01.000Z",
          "wordCount": 2398,
          "title": "Achieve 35% faster training with Hugging Face Deep Learning Containers on Amazon SageMaker"
        },
        {
          "id": "cc2fe3b5037e1b1ecc49be195de0c301bbf5f531",
          "author": "Raul Diaz Garcia",
          "description": "Building accurate computer vision models to detect objects in images requires deep knowledge of each step in the process—from labeling, processing, and preparing the training and validation data, to making the right model choice and tuning the model’s hyperparameters adequately to achieve the maximum accuracy. Fortunately, these complex steps are simplified by Amazon Rekognition Custom […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/build-a-computer-vision-model-using-amazon-rekognition-custom-labels-and-compare-the-results-with-a-custom-trained-tensorflow-model/",
          "publishedOn": "2021-12-15T20:14:55.000Z",
          "wordCount": 2593,
          "title": "Build a computer vision model using Amazon Rekognition Custom Labels and compare the results with a custom trained TensorFlow model"
        },
        {
          "id": "ff6ed82c8103d7ce4f912e408563dece38461c84",
          "author": "Laurence MIAO",
          "description": "GAN is a generative ML model that is widely used in advertising, games, entertainment, media, pharmaceuticals, and other industries. You can use it to create fictional characters and scenes, simulate facial aging, change image styles, produce chemical formulas synthetic data, and more. For example, the following images show the effect of picture-to-picture conversion. The following […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/build-gan-with-pytorch-and-amazon-sagemaker/",
          "publishedOn": "2021-12-14T23:37:25.000Z",
          "wordCount": 3552,
          "title": "Build GAN with PyTorch and Amazon SageMaker"
        },
        {
          "id": "7fbfa611eb41a70346c7c67b454156d727164279",
          "author": "Davide Galliteli",
          "description": "Customers in many different domains tend to work with multiple sources for their data: object-based storage like Amazon Simple Storage Service (Amazon S3), relational databases like Amazon Relational Database Service (Amazon RDS), or data warehouses like Amazon Redshift. Machine learning (ML) practitioners are often driven to work with objects and files instead of databases and […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/process-amazon-redshift-data-and-schedule-a-training-pipeline-with-amazon-sagemaker-processing-and-amazon-sagemaker-pipelines/",
          "publishedOn": "2021-12-14T23:07:38.000Z",
          "wordCount": 2190,
          "title": "Process Amazon Redshift data and schedule a training pipeline with Amazon SageMaker Processing and Amazon SageMaker Pipelines"
        },
        {
          "id": "3718a23de0ec88561afc2d0087abcc65698f823f",
          "author": "Francesco Polimeni",
          "description": "AutoML is a powerful capability, provided by Amazon SageMaker Autopilot, that allows non-experts to create machine learning (ML) models to invoke in their applications. The problem that we want to solve arises when, due to governance constraints, Amazon SageMaker resources can’t be deployed in the same AWS account where they are used. Examples of such […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/add-automl-functionality-with-amazon-sagemaker-autopilot-across-accounts/",
          "publishedOn": "2021-12-14T22:40:46.000Z",
          "wordCount": 3532,
          "title": "Add AutoML functionality with Amazon SageMaker Autopilot across accounts"
        },
        {
          "id": "47d7a320b6f2971004b23f6104b5b948c79500fb",
          "author": "Gordon Wang",
          "description": "Multi-object tracking (MOT) in video analysis is increasingly in demand in many industries, such as live sports, manufacturing, surveillance, and traffic monitoring. For example, in live sports, MOT can track soccer players in real time to analyze physical performance such as real-time speed and moving distance. Previously, most methods were designed to separate MOT into […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/train-and-deploy-a-fairmot-model-with-amazon-sagemaker/",
          "publishedOn": "2021-12-14T18:00:40.000Z",
          "wordCount": 2052,
          "title": "Train and deploy a FairMOT model with Amazon SageMaker",
          "enclosure": {
            "length": "25038034",
            "type": "video/mp4",
            "url": "https://raw.githubusercontent.com/ifzhang/FairMOT/master/videos/MOT16-03.mp4"
          }
        },
        {
          "id": "f790d23d4b5d7a1630e0c41e43fdfb4fb98c1e6b",
          "author": "Ben Snyder",
          "description": "Computer vision algorithms are at the core of many deep learning applications. Self-driving cars, security systems, healthcare, logistics, and image processing all incorporate various aspects of computer vision. But despite their ubiquity, training computer vision algorithms, like Mask or Cascade RCNN, is hard. These models employ complex architectures, train on large datasets, and require computer […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/distributed-mask-rcnn-training-with-amazon-sagemakercv/",
          "publishedOn": "2021-12-13T23:57:03.000Z",
          "wordCount": 2166,
          "title": "Distributed Mask RCNN training with Amazon SageMakerCV"
        },
        {
          "id": "acaf6ca2357cd35d5548b8a201195d14b159f8a9",
          "author": "Amit Gupta",
          "description": "Discrete and continuous manufacturing lines generate a high volume of products at low latency, ranging from milliseconds to a few seconds. To identify defects at the same throughput of production, camera streams of images must be processed at low latency. Additionally, factories may have low network bandwidth or intermittent cloud connectivity. In such scenarios, you […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/amazon-lookout-for-vision-now-supports-visual-inspection-of-product-defects-at-the-edge/",
          "publishedOn": "2021-12-13T18:44:09.000Z",
          "wordCount": 2508,
          "title": "Amazon Lookout for Vision now supports visual inspection of product defects at the edge"
        },
        {
          "id": "e01171ffc3ace81092ebdd719ee59a7302e2a251",
          "author": "Mani Khanuja",
          "description": "Time series forecasting is a common problem in machine learning (ML) and statistics. Some common day-to-day use cases of time series forecasting involve predicting product sales, item demand, component supply, service tickets, and all as a function of time. More often than not, time series data follows a hierarchical aggregation structure. For example, in retail, […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/hierarchical-forecasting-using-amazon-sagemaker/",
          "publishedOn": "2021-12-10T22:19:22.000Z",
          "wordCount": 3790,
          "title": "Hierarchical Forecasting using Amazon SageMaker"
        },
        {
          "id": "60b9bb73cb3e4c345ee73c5eeace083c7b4722ad",
          "author": "Beibit Baktygaliyev",
          "description": "The Formula 1 (F1) live steaming service, F1 TV, has live automated closed captions in three different languages: English, Spanish, and French. For the 2021 season, FORMULA 1 has achieved another technological breakthrough, building a fully automated workflow to create closed captions in three languages and broadcasting to 85 territories using Amazon Transcribe. Amazon Transcribe […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/live-transcriptions-of-f1-races-using-amazon-transcribe/",
          "publishedOn": "2021-12-10T20:08:24.000Z",
          "wordCount": 3104,
          "title": "Live transcriptions of F1 races using Amazon Transcribe",
          "enclosure": {
            "length": "154011878",
            "type": "video/mp4",
            "url": "https://aws-ml-blog.s3.amazonaws.com/artifacts/f1-live-transcriptions/2021+Formula+1+-+AWS+Bahrain+F1+TV+Subtitles+.mp4"
          }
        },
        {
          "id": "3923fdecd4e17f3a2adffff032f11a3f830d383b",
          "author": "Francisco Calderon Rodriguez",
          "description": "Since its launch in November 2017, the AWS Deep Learning Amazon Machine Image (DLAMI) has been the preferred method for running deep learning frameworks on Amazon Elastic Compute Cloud (Amazon EC2). For deep learning practitioners and learners who want to accelerate deep learning in the cloud, the DLAMI comes pre-installed with AWS-optimized deep learning (DL) frameworks […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/aws-deep-learning-amis-new-framework-specific-dlamis-for-production-complement-the-original-multi-framework-dlamis/",
          "publishedOn": "2021-12-08T21:50:38.000Z",
          "wordCount": 1440,
          "title": "AWS Deep Learning AMIs: New framework-specific DLAMIs for production complement the original multi-framework DLAMIs"
        },
        {
          "id": "e8f98725b1837376d2e594e8f042924f24b382d4",
          "author": "Tesfagabir Meharizghi",
          "description": "Mining medical concepts from written clinical text, such as patient encounters, plays an important role in clinical analytics and decision-making applications, such as population analytics for providers, pre-authorization for payers, and adverse-event detection for pharma companies. Medical concepts contain medical conditions, medications, procedures, and other clinical events. Extracting medical concepts is a complicated process due […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/clinical-text-mining-using-the-amazon-comprehend-medical-new-snomed-ct-api/",
          "publishedOn": "2021-12-08T21:08:52.000Z",
          "wordCount": 3165,
          "title": "Clinical text mining using the Amazon Comprehend Medical new SNOMED CT API"
        },
        {
          "id": "e189b96d819990b4f31e43ba3a5ff75526868833",
          "author": "Zheng Zhang",
          "description": "While the fuel economy of new gasoline or diesel-powered vehicles improves every year, green vehicles are considered even more environmentally friendly because they’re powered by alternative fuel or electricity. Hybrid electric vehicles (HEVs), battery only electric vehicles (BEVs), fuel cell electric vehicles (FCEVs), hydrogen cars, and solar cars are all considered types of green vehicles. […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/plan-the-locations-of-green-car-charging-stations-with-an-amazon-sagemaker-built-in-algorithm/",
          "publishedOn": "2021-12-08T20:48:38.000Z",
          "wordCount": 2674,
          "title": "Plan the locations of green car charging stations with an Amazon SageMaker built-in algorithm"
        },
        {
          "id": "b6f50dc7e549abae366c97437ca9f016d494a36f",
          "author": "Roger Barga",
          "description": "Computer vision, the automatic recognition and description of documents, images, and videos, has far-reaching applications, from identifying defects in high-speed assembly lines, to intelligently automating document processing workflows, and identifying products and people in social media. AWS computer vision services, including Amazon Lookout for Vision, AWS Panorama, Amazon Rekognition, and Amazon Textract, help developers automate […]",
          "link": "https://aws.amazon.com/blogs/machine-learning/aws-computer-vision-and-amazon-rekognition-aws-recognized-as-an-idc-marketscape-leader-in-asia-pacific-excluding-japan-up-to-38-price-cut-and-major-new-features/",
          "publishedOn": "2021-12-06T16:17:26.000Z",
          "wordCount": 3376,
          "title": "AWS computer vision and Amazon Rekognition: AWS recognized as an IDC MarketScape Leader in Asia Pacific (excluding Japan), up to 38% price cut, and major new features"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2111.02434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1\">Greg Ver Steeg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>",
          "description": "Sampling from an unnormalized probability distribution is a fundamental\nproblem in machine learning with applications including Bayesian modeling,\nlatent factor inference, and energy-based model training. After decades of\nresearch, variations of MCMC remain the default approach to sampling despite\nslow convergence. Auxiliary neural models can learn to speed up MCMC, but the\noverhead for training the extra model can be prohibitive. We propose a\nfundamentally different approach to this problem via a new Hamiltonian dynamics\nwith a non-Newtonian momentum. In contrast to MCMC approaches like Hamiltonian\nMonte Carlo, no stochastic step is required. Instead, the proposed\ndeterministic dynamics in an extended state space exactly sample the target\ndistribution, specified by an energy function, under an assumption of\nergodicity. Alternatively, the dynamics can be interpreted as a normalizing\nflow that samples a specified energy model without training. The proposed\nEnergy Sampling Hamiltonian (ESH) dynamics have a simple form that can be\nsolved with existing ODE solvers, but we derive a specialized solver that\nexhibits much better performance. ESH dynamics converge faster than their MCMC\ncompetitors enabling faster, more stable training of neural network energy\nmodels.",
          "link": "http://arxiv.org/abs/2111.02434",
          "publishedOn": "2022-01-03T07:15:19.579Z",
          "wordCount": 664,
          "title": "Hamiltonian Dynamics with Non-Newtonian Momentum for Rapid Sampling. (arXiv:2111.02434v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.07068",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dockhorn_T/0/1/0/all/0/1\">Tim Dockhorn</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vahdat_A/0/1/0/all/0/1\">Arash Vahdat</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kreis_K/0/1/0/all/0/1\">Karsten Kreis</a>",
          "description": "Score-based generative models (SGMs) have demonstrated remarkable synthesis\nquality. SGMs rely on a diffusion process that gradually perturbs the data\ntowards a tractable distribution, while the generative model learns to denoise.\nThe complexity of this denoising task is, apart from the data distribution\nitself, uniquely determined by the diffusion process. We argue that current\nSGMs employ overly simplistic diffusions, leading to unnecessarily complex\ndenoising processes, which limit generative modeling performance. Based on\nconnections to statistical mechanics, we propose a novel critically-damped\nLangevin diffusion (CLD) and show that CLD-based SGMs achieve superior\nperformance. CLD can be interpreted as running a joint diffusion in an extended\nspace, where the auxiliary variables can be considered \"velocities\" that are\ncoupled to the data variables as in Hamiltonian dynamics. We derive a novel\nscore matching objective for CLD and show that the model only needs to learn\nthe score function of the conditional distribution of the velocity given data,\nan easier task than learning scores of the data directly. We also derive a new\nsampling scheme for efficient synthesis from CLD-based diffusion models. We\nfind that CLD outperforms previous SGMs in synthesis quality for similar\nnetwork architectures and sampling compute budgets. We show that our novel\nsampler for CLD significantly outperforms solvers such as Euler--Maruyama. Our\nframework provides new insights into score-based denoising diffusion models and\ncan be readily used for high-resolution image synthesis. Project page and code:\nhttps://nv-tlabs.github.io/CLD-SGM.",
          "link": "http://arxiv.org/abs/2112.07068",
          "publishedOn": "2022-01-03T07:15:19.560Z",
          "wordCount": 670,
          "title": "Score-Based Generative Modeling with Critically-Damped Langevin Diffusion. (arXiv:2112.07068v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.14278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fil_M/0/1/0/all/0/1\">Miroslav Fil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mesinovic_M/0/1/0/all/0/1\">Munib Mesinovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_M/0/1/0/all/0/1\">Matthew Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildberger_J/0/1/0/all/0/1\">Jonas Wildberger</a>",
          "description": "$\\beta$-VAE is a follow-up technique to variational autoencoders that\nproposes special weighting of the KL divergence term in the VAE loss to obtain\ndisentangled representations. Unsupervised learning is known to be brittle even\non toy datasets and a meaningful, mathematically precise definition of\ndisentanglement remains difficult to find. Here we investigate the original\n$\\beta$-VAE paper and add evidence to the results previously obtained\nindicating its lack of reproducibility. We also further expand the\nexperimentation of the models and include further more complex datasets in the\nanalysis. We also implement an FID scoring metric for the $\\beta$-VAE model and\nconclude a qualitative analysis of the results obtained. We end with a brief\ndiscussion on possible future investigations that can be conducted to add more\nrobustness to the claims.",
          "link": "http://arxiv.org/abs/2112.14278",
          "publishedOn": "2022-01-03T07:15:19.554Z",
          "wordCount": 558,
          "title": "Beta-VAE Reproducibility: Challenges and Extensions. (arXiv:2112.14278v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10766",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Manjunath_G/0/1/0/all/0/1\">G Manjunath</a>",
          "description": "The celebrated Takens' embedding theorem concerns embedding an attractor of a\ndynamical system in a Euclidean space of appropriate dimension through a\ngeneric delay-observation map. The embedding also establishes a topological\nconjugacy. In this paper, we show how an arbitrary sequence can be mapped into\nanother space as an attractive solution of a nonautonomous dynamical system.\nSuch mapping also entails a topological conjugacy and an embedding between the\nsequence and the attractive solution spaces. This result is not a\ngeneralization of Takens embedding theorem but helps us understand what exactly\nis required by discrete-time state space models widely used in applications to\nembed an external stimulus onto its solution space. Our results settle another\nbasic problem concerning the perturbation of an autonomous dynamical system. We\ndescribe what exactly happens to the dynamics when exogenous noise perturbs\ncontinuously a local irreducible attracting set (such as a stable fixed point)\nof a discrete-time autonomous dynamical system.",
          "link": "http://arxiv.org/abs/2105.10766",
          "publishedOn": "2022-01-03T07:15:19.549Z",
          "wordCount": 592,
          "title": "Embedding Information onto a Dynamical System. (arXiv:2105.10766v3 [math.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hand_D/0/1/0/all/0/1\">D. J. Hand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anagnostopoulos_C/0/1/0/all/0/1\">C. Anagnostopoulos</a>",
          "description": "The H-measure is a classifier performance measure which takes into account\nthe context of application without requiring a rigid value of relative\nmisclassification costs to be set. Since its introduction in 2009 it has become\nwidely adopted. This paper answers various queries which users have raised\nsince its introduction, including questions about its interpretation, the\nchoice of a weighting function, whether it is strictly proper, and its\ncoherence, and relates the measure to other work.",
          "link": "http://arxiv.org/abs/2106.11888",
          "publishedOn": "2022-01-03T07:15:19.543Z",
          "wordCount": 517,
          "title": "Notes on the H-measure of classifier performance. (arXiv:2106.11888v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qi Ye</a>",
          "description": "This article presents a new way to study the theory of regularized learning\nfor generalized data in Banach spaces including representer theorems and\nconvergence theorems. The generalized data are composed of linear functionals\nand real scalars as the input and output elements to represent the discrete\ninformation of different local models. By the extension of the classical\nmachine learning, the empirical risks are computed by the generalized data and\nthe loss functions. According to the techniques of regularization, the exact\nsolutions are approximated globally by minimizing the regularized empirical\nrisks over the Banach spaces. The existence and convergence of the approximate\nsolutions are guaranteed by the relative compactness of the generalized input\ndata in the predual spaces of the Banach spaces.",
          "link": "http://arxiv.org/abs/2109.03159",
          "publishedOn": "2022-01-03T07:15:19.537Z",
          "wordCount": 597,
          "title": "Analysis of Regularized Learning in Banach Spaces. (arXiv:2109.03159v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02454",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mingqi Yuan</a>",
          "description": "Extrapolating beyond-demonstrator (BD) performance through the imitation\nlearning (IL) algorithm aims to learn from and outperform the demonstrator.\nMost existing BDIL algorithms are performed in two stages by first inferring a\nreward function before learning a policy via reinforcement learning (RL).\nHowever, such two-stage BDIL algorithms suffer from high computational\ncomplexity, weak robustness, and large performance variations. In particular, a\npoor reward function derived in the first stage will inevitably incur severe\nperformance loss in the second stage. In this work, we propose a hybrid\nadversarial imitation learning (HAIL) algorithm that is one-stage, model-free,\ngenerative-adversarial (GA) fashion and curiosity-driven. Thanks to the\none-stage design, the HAIL can integrate both the reward function learning and\nthe policy optimization into one procedure, which leads to many advantages such\nas low computational complexity, high robustness, and strong adaptability. More\nspecifically, HAIL simultaneously imitates the demonstrator and explores BD\nperformance by utilizing hybrid rewards. Extensive simulation results confirm\nthat HAIL can achieve higher performance as compared to other similar BDIL\nalgorithms.",
          "link": "http://arxiv.org/abs/2102.02454",
          "publishedOn": "2022-01-03T07:15:19.532Z",
          "wordCount": 683,
          "title": "Hybrid Adversarial Imitation Learning. (arXiv:2102.02454v10 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mengin_E/0/1/0/all/0/1\">Elie Mengin</a> (SAMM), <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1\">Fabrice Rossi</a> (CEREMADE)",
          "description": "In this paper, we address the problem of finding a correspondence, or\nmatching, between the functions of two programs in binary form, which is one of\nthe most common task in binary diffing. We introduce a new formulation of this\nproblem as a particular instance of a graph edit problem over the call graphs\nof the programs. In this formulation, the quality of a mapping is evaluated\nsimultaneously with respect to both function content and call graph\nsimilarities. We show that this formulation is equivalent to a network\nalignment problem. We propose a solving strategy for this problem based on\nmax-product belief propagation. Finally, we implement a prototype of our\nmethod, called QBinDiff, and propose an extensive evaluation which shows that\nour approach outperforms state of the art diffing tools.",
          "link": "http://arxiv.org/abs/2112.15337",
          "publishedOn": "2022-01-03T07:15:19.525Z",
          "wordCount": 579,
          "title": "Binary Diffing as a Network Alignment Problem via Belief Propagation. (arXiv:2112.15337v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15210",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reinauer_R/0/1/0/all/0/1\">Raphael Reinauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caorsi_M/0/1/0/all/0/1\">Matteo Caorsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berkouk_N/0/1/0/all/0/1\">Nicolas Berkouk</a>",
          "description": "One of the main challenges of Topological Data Analysis (TDA) is to extract\nfeatures from persistent diagrams directly usable by machine learning\nalgorithms. Indeed, persistence diagrams are intrinsically (multi-)sets of\npoints in R2 and cannot be seen in a straightforward manner as vectors. In this\narticle, we introduce Persformer, the first Transformer neural network\narchitecture that accepts persistence diagrams as input. The Persformer\narchitecture significantly outperforms previous topological neural network\narchitectures on classical synthetic benchmark datasets. Moreover, it satisfies\na universal approximation theorem. This allows us to introduce the first\ninterpretability method for topological machine learning, which we explore in\ntwo examples.",
          "link": "http://arxiv.org/abs/2112.15210",
          "publishedOn": "2022-01-03T07:15:19.520Z",
          "wordCount": 522,
          "title": "Persformer: A Transformer Architecture for Topological Machine Learning. (arXiv:2112.15210v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.10861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shunqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurkoski_B/0/1/0/all/0/1\">Brian M. Kurkoski</a>",
          "description": "Approximate message passing (AMP) is a low-cost iterative\nparameter-estimation technique for certain high-dimensional linear systems with\nnon-Gaussian distributions. However, AMP only applies to independent\nidentically distributed (IID) transform matrices, but may become unreliable\n(e.g., perform poorly or even diverge) for other matrix ensembles, especially\nfor ill-conditioned ones. Orthogonal/vector AMP (OAMP/VAMP) was proposed for\ngeneral right-unitarily-invariant matrices to handle this difficulty. However,\nthe Bayes-optimal OAMP/VAMP (BO-OAMP/VAMP) requires a high-complexity linear\nminimum mean square error (MMSE) estimator. This limits the application of\nOAMP/VAMP to large-scale systems.\n\nTo solve the disadvantages of AMP and BO-OAMP/VAMP, this paper proposes a\nmemory AMP (MAMP) framework under an orthogonality principle, which guarantees\nthe asymptotic IID Gaussianity of estimation errors in MAMP. We present an\northogonalization procedure for the local memory estimators to realize the\nrequired orthogonality for MAMP. Furthermore, we propose a Bayes-optimal MAMP\n(BO-MAMP), in which a long-memory matched filter is proposed for interference\nsuppression. The complexity of BO-MAMP is comparable to AMP. A state evolution\nis derived to asymptotically characterize the performance of BO-MAMP. Based on\nstate evolution, the relaxation parameters and damping vector in BO-MAMP are\noptimized. For all right-unitarily-invariant matrices, the state evolution of\nthe optimized BO-MAMP converges to the same fixed point as that of the\nhigh-complexity BO-OAMP/VAMP and is Bayes-optimal if its state evolution has a\nunique fixed point. Finally, simulations are provided to verify the validity\nand accuracy of the theoretical results.",
          "link": "http://arxiv.org/abs/2012.10861",
          "publishedOn": "2022-01-03T07:15:19.513Z",
          "wordCount": 753,
          "title": "Memory AMP. (arXiv:2012.10861v5 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.01711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chengchun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shikai Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongtu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jieping Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>",
          "description": "A/B testing, or online experiment is a standard business strategy to compare\na new product with an old one in pharmaceutical, technological, and traditional\nindustries. Major challenges arise in online experiments of two-sided\nmarketplace platforms (e.g., Uber) where there is only one unit that receives a\nsequence of treatments over time. In those experiments, the treatment at a\ngiven time impacts current outcome as well as future outcomes. The aim of this\npaper is to introduce a reinforcement learning framework for carrying A/B\ntesting in these experiments, while characterizing the long-term treatment\neffects. Our proposed testing procedure allows for sequential monitoring and\nonline updating. It is generally applicable to a variety of treatment designs\nin different industries. In addition, we systematically investigate the\ntheoretical properties (e.g., size and power) of our testing procedure.\nFinally, we apply our framework to both simulated data and a real-world data\nexample obtained from a technological company to illustrate its advantage over\nthe current practice. A Python implementation of our test is available at\nhttps://github.com/callmespring/CausalRL.",
          "link": "http://arxiv.org/abs/2002.01711",
          "publishedOn": "2022-01-03T07:15:19.437Z",
          "wordCount": 650,
          "title": "Dynamic Causal Effects Evaluation in A/B Testing with a Reinforcement Learning Framework. (arXiv:2002.01711v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15521",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neupartl_N/0/1/0/all/0/1\">Nils Neup&#xe4;rtl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rothkopf_C/0/1/0/all/0/1\">Constantin A. Rothkopf</a>",
          "description": "Bayesian models of behavior have provided computational level explanations in\na range of psychophysical tasks. One fundamental experimental paradigm is the\nproduction or reproduction task, in which subjects are instructed to generate\nan action that either reproduces a previously sensed stimulus magnitude or\nachieves a target response. This type of task therefore distinguishes itself\nfrom other psychophysical tasks in that the responses are on a continuum and\neffort plays an important role with increasing response magnitude. Based on\nBayesian decision theory we present an inference method to recover perceptual\nuncertainty, response variability, and the cost function underlying human\nresponses. Crucially, the cost function is parameterized such that effort is\nexplicitly included. We present a hybrid inference method employing MCMC\nsampling utilizing appropriate proposal distributions and an inner loop\nutilizing amortized inference with a neural network that approximates the mode\nof the optimal response distribution. We show how this model can be utilized to\navoid unidentifiability of experimental designs and that parameters can be\nrecovered through validation on synthetic and application to experimental data.\nOur approach will enable behavioral scientists to perform Bayesian inference of\ndecision making parameters in production and reproduction tasks.",
          "link": "http://arxiv.org/abs/2112.15521",
          "publishedOn": "2022-01-03T07:15:19.373Z",
          "wordCount": 625,
          "title": "Inferring perceptual decision making parameters from behavior in production and reproduction tasks. (arXiv:2112.15521v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14798",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Fang_L/0/1/0/all/0/1\">Lidong Fang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ge_P/0/1/0/all/0/1\">Pei Ge</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lei_H/0/1/0/all/0/1\">Huan Lei</a>, <a href=\"http://arxiv.org/find/physics/1/au:+E_W/0/1/0/all/0/1\">Weinan E</a>",
          "description": "A long standing problem in the modeling of non-Newtonian hydrodynamics is the\navailability of reliable and interpretable hydrodynamic models that faithfully\nencode the underlying micro-scale polymer dynamics. The main complication\narises from the long polymer relaxation time, the complex molecular structure,\nand heterogeneous interaction. DeePN$^2$, a deep learning-based non-Newtonian\nhydrodynamic model, has been proposed and has shown some success in\nsystematically passing the micro-scale structural mechanics information to the\nmacro-scale hydrodynamics for suspensions with simple polymer conformation and\nbond potential. The model retains a multi-scaled nature by mapping the polymer\nconfigurations into a set of symmetry-preserving macro-scale features. The\nextended constitutive laws for these macro-scale features can be directly\nlearned from the kinetics of their micro-scale counterparts. In this paper, we\ncarry out further study of DeePN$^2$ using more complex micro-structural\nmodels. We show that DeePN$^2$ can faithfully capture the broadly overlooked\nviscoelastic differences arising from the specific molecular structural\nmechanics without human intervention.",
          "link": "http://arxiv.org/abs/2112.14798",
          "publishedOn": "2022-01-03T07:15:19.306Z",
          "wordCount": 578,
          "title": "DeePN$^2$: A deep learning-based non-Newtonian hydrodynamic model. (arXiv:2112.14798v1 [physics.comp-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yerushalmi_D/0/1/0/all/0/1\">Dvir Yerushalmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danon_D/0/1/0/all/0/1\">Dov Danon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1\">Amit H. Bermano</a>",
          "description": "Supervision for image-to-image translation (I2I) tasks is hard to come by,\nbut bears significant effect on the resulting quality. In this paper, we\nobserve that for many Unsupervised I2I (UI2I) scenarios, one domain is more\nfamiliar than the other, and offers in-domain prior knowledge, such as semantic\nsegmentation. We argue that for complex scenes, figuring out the semantic\nstructure of the domain is hard, especially with no supervision, but is an\nimportant part of a successful I2I operation. We hence introduce two techniques\nto incorporate this invaluable in-domain prior knowledge for the benefit of\ntranslation quality: through a novel Multi-Stream generator architecture, and\nthrough a semantic segmentation-based regularization loss term. In essence, we\npropose splitting the input data according to semantic masks, explicitly\nguiding the network to different behavior for the different regions of the\nimage. In addition, we propose training a semantic segmentation network along\nwith the translation task, and to leverage this output as a loss term that\nimproves robustness. We validate our approach on urban data, demonstrating\nsuperior quality in the challenging UI2I tasks of converting day images to\nnight ones. In addition, we also demonstrate how reinforcing the target dataset\nwith our augmented images improves the training of downstream tasks such as the\nclassical detection one.",
          "link": "http://arxiv.org/abs/2112.15091",
          "publishedOn": "2022-01-03T07:15:19.300Z",
          "wordCount": 645,
          "title": "Leveraging in-domain supervision for unsupervised image-to-image translation tasks via multi-stream generators. (arXiv:2112.15091v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.01278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinghui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>",
          "description": "Adversarial training is so far the most effective strategy in defending\nagainst adversarial examples. However, it suffers from high computational costs\ndue to the iterative adversarial attacks in each training step. Recent studies\nshow that it is possible to achieve fast Adversarial Training by performing a\nsingle-step attack with random initialization. However, such an approach still\nlags behind state-of-the-art adversarial training algorithms on both stability\nand model robustness. In this work, we develop a new understanding towards Fast\nAdversarial Training, by viewing random initialization as performing randomized\nsmoothing for better optimization of the inner maximization problem. Following\nthis new perspective, we also propose a new initialization strategy, backward\nsmoothing, to further improve the stability and model robustness over\nsingle-step robust training methods. Experiments on multiple benchmarks\ndemonstrate that our method achieves similar model robustness as the original\nTRADES method while using much less training time ($\\sim$3x improvement with\nthe same training schedule).",
          "link": "http://arxiv.org/abs/2010.01278",
          "publishedOn": "2022-01-03T07:15:19.124Z",
          "wordCount": 616,
          "title": "Efficient Robust Training via Backward Smoothing. (arXiv:2010.01278v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.13794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Weiran Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Sean Qian</a>",
          "description": "The effectiveness of traditional traffic prediction methods is often\nextremely limited when forecasting traffic dynamics in early morning. The\nreason is that traffic can break down drastically during the early morning\ncommute, and the time and duration of this break-down vary substantially from\nday to day. Early morning traffic forecast is crucial to inform morning-commute\ntraffic management, but they are generally challenging to predict in advance,\nparticularly by midnight. In this paper, we propose to mine Twitter messages as\na probing method to understand the impacts of people's work and rest patterns\nin the evening/midnight of the previous day to the next-day morning traffic.\nThe model is tested on freeway networks in Pittsburgh as experiments. The\nresulting relationship is surprisingly simple and powerful. We find that, in\ngeneral, the earlier people rest as indicated from Tweets, the more congested\nroads will be in the next morning. The occurrence of big events in the evening\nbefore, represented by higher or lower tweet sentiment than normal, often\nimplies lower travel demand in the next morning than normal days. Besides,\npeople's tweeting activities in the night before and early morning are\nstatistically associated with congestion in morning peak hours. We make use of\nsuch relationships to build a predictive framework which forecasts morning\ncommute congestion using people's tweeting profiles extracted by 5 am or as\nlate as the midnight prior to the morning. The Pittsburgh study supports that\nour framework can precisely predict morning congestion, particularly for some\nroad segments upstream of roadway bottlenecks with large day-to-day congestion\nvariation. Our approach considerably outperforms those existing methods without\nTwitter message features, and it can learn meaningful representation of demand\nfrom tweeting profiles that offer managerial insights.",
          "link": "http://arxiv.org/abs/2009.13794",
          "publishedOn": "2022-01-03T07:15:19.041Z",
          "wordCount": 765,
          "title": "From Twitter to Traffic Predictor: Next-Day Morning Traffic Prediction Using Social Media Data. (arXiv:2009.13794v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15362",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiamian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Ziyi Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_Z/0/1/0/all/0/1\">Zhiqiang Tao</a>",
          "description": "Recently, hyperspectral imaging (HSI) has attracted increasing research\nattention, especially for the ones based on a coded aperture snapshot spectral\nimaging (CASSI) system. Existing deep HSI reconstruction models are generally\ntrained on paired data to retrieve original signals upon 2D compressed\nmeasurements given by a particular optical hardware mask in CASSI, during which\nthe mask largely impacts the reconstruction performance and could work as a\n\"model hyperparameter\" governing on data augmentations. This mask-specific\ntraining style will lead to a hardware miscalibration issue, which sets up\nbarriers to deploying deep HSI models among different hardware and noisy\nenvironments. To address this challenge, we introduce mask uncertainty for HSI\nwith a complete variational Bayesian learning treatment and explicitly model it\nthrough a mask decomposition inspired by real hardware. Specifically, we\npropose a novel Graph-based Self-Tuning (GST) network to reason uncertainties\nadapting to varying spatial structures of masks among different hardware.\nMoreover, we develop a bilevel optimization framework to balance HSI\nreconstruction and uncertainty estimation, accounting for the hyperparameter\nproperty of masks. Extensive experimental results and model discussions\nvalidate the effectiveness (over 33/30 dB) of the proposed GST method under two\nmiscalibration scenarios and demonstrate a highly competitive performance\ncompared with the state-of-the-art well-calibrated methods. Our code and\npre-trained model are available at https://github.com/Jiamian\nWang/mask_uncertainty_spectral_SCI",
          "link": "http://arxiv.org/abs/2112.15362",
          "publishedOn": "2022-01-03T07:15:18.968Z",
          "wordCount": 652,
          "title": "Calibrated Hyperspectral Image Reconstruction via Graph-based Self-Tuning Network. (arXiv:2112.15362v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15348",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bemporad_A/0/1/0/all/0/1\">Alberto Bemporad</a>",
          "description": "For training recurrent neural network models of nonlinear dynamical systems\nfrom an input/output training dataset based on rather arbitrary convex and\ntwice-differentiable loss functions and regularization terms, we propose the\nuse of sequential least squares for determining the optimal network parameters\nand hidden states. In addition, to handle non-smooth regularization terms such\nas L1, L0, and group-Lasso regularizers, as well as to impose possibly\nnon-convex constraints such as integer and mixed-integer constraints, we\ncombine sequential least squares with the alternating direction method of\nmultipliers (ADMM). The performance of the resulting algorithm, that we call\nNAILS (Nonconvex ADMM Iterations and Least Squares), is tested in a nonlinear\nsystem identification benchmark.",
          "link": "http://arxiv.org/abs/2112.15348",
          "publishedOn": "2022-01-03T07:15:18.947Z",
          "wordCount": 555,
          "title": "Training Recurrent Neural Networks by Sequential Least Squares and the Alternating Direction Method of Multipliers. (arXiv:2112.15348v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alfarisi_O/0/1/0/all/0/1\">Omar Alfarisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouzzane_D/0/1/0/all/0/1\">Djamel Ouzzane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sassi_M/0/1/0/all/0/1\">Mohamed Sassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tiejun Zhang</a>",
          "description": "Each grid block in a 3D geological model requires a rock type that represents\nall physical and chemical properties of that block. The properties that\nclassify rock types are lithology, permeability, and capillary pressure.\nScientists and engineers determined these properties using conventional\nlaboratory measurements, which embedded destructive methods to the sample or\naltered some of its properties (i.e., wettability, permeability, and porosity)\nbecause the measurements process includes sample crushing, fluid flow, or fluid\nsaturation. Lately, Digital Rock Physics (DRT) has emerged to quantify these\nproperties from micro-Computerized Tomography (uCT) and Magnetic Resonance\nImaging (MRI) images. However, the literature did not attempt rock typing in a\nwholly digital context. We propose performing Digital Rock Typing (DRT) by: (1)\nintegrating the latest DRP advances in a novel process that honors digital rock\nproperties determination, while; (2) digitalizing the latest rock typing\napproaches in carbonate, and (3) introducing a novel carbonate rock typing\nprocess that utilizes computer vision capabilities to provide more insight\nabout the heterogeneous carbonate rock texture.",
          "link": "http://arxiv.org/abs/2112.15068",
          "publishedOn": "2022-01-03T07:15:18.941Z",
          "wordCount": 606,
          "title": "Digital Rock Typing DRT Algorithm Formulation with Optimal Supervised Semantic Segmentation. (arXiv:2112.15068v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pietron_M/0/1/0/all/0/1\">Marcin Pietro&#x144;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zurek_D/0/1/0/all/0/1\">Dominik &#x17b;urek</a>",
          "description": "This work is focused on the pruning of some convolutional neural networks\n(CNNs) and improving theirs efficiency on graphic processing units (GPU) by\nusing a direct sparse algorithm. The Nvidia deep neural network (cuDnn) library\nis the most effective implementations of deep learning (DL) algorithms for\nGPUs. GPUs are the most commonly used accelerators for deep learning\ncomputations. One of the most common techniques for improving the efficiency of\nCNN models is weight pruning and quantization. There are two main types of\npruning: structural and non-structural. The first enables much easier\nacceleration on many type of accelerators, but with this type it is difficult\nto achieve a sparsity level and accuracy as high as that obtained with the\nsecond type. Non-structural pruning with retraining can generate a weight\ntensors up to 90% or more of sparsity in some deep CNN models. In this article\nthe pruning algorithm is presented which makes it possible to achieve high\nsparsity levels without accuracy drop. In the next stage the linear and\nnon-linear quantization is adapted for further time and footprint reduction.\nThis paper is an extended of previously published paper concerning effective\npruning techniques and present real models pruned with high sparsities and\nreduced precision which can achieve better performance than the CuDnn library.",
          "link": "http://arxiv.org/abs/2112.15445",
          "publishedOn": "2022-01-03T07:15:18.934Z",
          "wordCount": 655,
          "title": "Speedup deep learning models on GPU by taking advantage of efficient unstructured pruning and bit-width reduction. (arXiv:2112.15445v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14893",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chong_B/0/1/0/all/0/1\">Bin Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yingguang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zi-Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_H/0/1/0/all/0/1\">Hang Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhirong Liu</a>",
          "description": "Most algorithms for the multi-armed bandit problem in reinforcement learning\naimed to maximize the expected reward, which are thus useful in searching the\noptimized candidate with the highest reward (function value) for diverse\napplications (e.g., AlphaGo). However, in some typical application scenaios\nsuch as drug discovery, the aim is to search a diverse set of candidates with\nhigh reward. Here we propose a reversible upper confidence bound (rUCB)\nalgorithm for such a purpose, and demonstrate its application in virtual\nscreening upon intrinsically disordered proteins (IDPs). It is shown that rUCB\ngreatly reduces the query times while achieving both high accuracy and low\nperformance loss.The rUCB may have potential application in multipoint\noptimization and other reinforcement-learning cases.",
          "link": "http://arxiv.org/abs/2112.14893",
          "publishedOn": "2022-01-03T07:15:18.843Z",
          "wordCount": 554,
          "title": "Reversible Upper Confidence Bound Algorithm to Generate Diverse Optimized Candidates. (arXiv:2112.14893v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.07110",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bhattacharya_R/0/1/0/all/0/1\">Riddhiman Bhattacharya</a>",
          "description": "The gradient noise of Stochastic Gradient Descent (SGD) is considered to play\na key role in its properties (e.g. escaping low potential points and\nregularization). Past research has indicated that the covariance of the SGD\nerror done via minibatching plays a critical role in determining its\nregularization and escape from low potential points. It is however not much\nexplored how much the distribution of the error influences the behavior of the\nalgorithm. Motivated by some new research in this area, we prove universality\nresults by showing that noise classes that have the same mean and covariance\nstructure of SGD via minibatching have similar properties. We mainly consider\nthe Multiplicative Stochastic Gradient Descent (M-SGD) algorithm as introduced\nby Wu et al., which has a much more general noise class than the SGD algorithm\ndone via minibatching. We establish nonasymptotic bounds for the M-SGD\nalgorithm mainly with respect to the Stochastic Differential Equation\ncorresponding to SGD via minibatching. We also show that the M-SGD error is\napproximately a scaled Gaussian distribution with mean $0$ at any fixed point\nof the M-SGD algorithm. We also establish bounds for the convergence of the\nM-SGD algorithm in the strongly convex regime.",
          "link": "http://arxiv.org/abs/2112.07110",
          "publishedOn": "2022-01-03T07:15:18.724Z",
          "wordCount": 657,
          "title": "Non Asymptotic Bounds for Optimization via Online Multiplicative Stochastic Gradient Descent. (arXiv:2112.07110v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.14842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wali_S/0/1/0/all/0/1\">Syed Wali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_I/0/1/0/all/0/1\">Irfan Khan</a>",
          "description": "The transformation of conventional power networks into smart grids with the\nheavy penetration level of renewable energy resources, particularly\ngrid-connected Photovoltaic (PV) systems, has increased the need for efficient\nfault identification systems. Malfunctioning any single component in\ngrid-connected PV systems may lead to grid instability and other serious\nconsequences, showing that a reliable fault identification system is the utmost\nrequirement for ensuring operational integrity. Therefore, this paper presents\na novel fault identification approach based on statistical signatures of PV\noperational states. These signatures are unique because each fault has a\ndifferent nature and distinctive impact on the electrical system. Thus, the\nRandom Forest Classifier trained on these extracted signatures showed 100%\naccuracy in identifying all types of faults. Furthermore, the performance\ncomparison of the proposed framework with other Machine Learning classifiers\ndepicts its credibility. Moreover, to elevate user trust in the predicted\noutcomes, SHAP (Shapley Additive Explanation) was utilized during the training\nphase to extract a complete model response (global explanation). This extracted\nglobal explanation can help in the assessment of predicted outcomes credibility\nby decoding each prediction in terms of features contribution. Hence, the\nproposed explainable signature-based fault identification technique is highly\ncredible and fulfills all the requirements of smart grids.",
          "link": "http://arxiv.org/abs/2112.14842",
          "publishedOn": "2022-01-03T07:15:18.701Z",
          "wordCount": 639,
          "title": "Explainable Signature-based Machine Learning Approach for Identification of Faults in Grid-Connected Photovoltaic Systems. (arXiv:2112.14842v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+P_A/0/1/0/all/0/1\">Abirami S P</a>, <a href=\"http://arxiv.org/find/cs/1/au:+G_K/0/1/0/all/0/1\">Kousalya G</a>, <a href=\"http://arxiv.org/find/cs/1/au:+R_K/0/1/0/all/0/1\">Karthick R</a>",
          "description": "Autism Spectrum Disorder (ASD) is found to be a major concern among various\noccupational therapists. The foremost challenge of this neurodevelopmental\ndisorder lies in the fact of analyzing and exploring various symptoms of the\nchildren at their early stage of development. Such early identification could\nprop up the therapists and clinicians to provide proper assistive support to\nmake the children lead an independent life. Facial expressions and emotions\nperceived by the children could contribute to such early intervention of\nautism. In this regard, the paper implements in identifying basic facial\nexpression and exploring their emotions upon a time variant factor. The\nemotions are analyzed by incorporating the facial expression identified through\nCNN using 68 landmark points plotted on the frontal face with a prediction\nnetwork formed by RNN known as RCNN-FER system. The paper adopts R-CNN to take\nthe advantage of increased accuracy and performance with decreased time\ncomplexity in predicting emotion as a textual network analysis. The papers\nproves better accuracy in identifying the emotion in autistic children when\ncompared over simple machine learning models built for such identifications\ncontributing to autistic society.",
          "link": "http://arxiv.org/abs/2112.14983",
          "publishedOn": "2022-01-03T07:15:18.695Z",
          "wordCount": 648,
          "title": "Exploring the pattern of Emotion in children with ASD as an early biomarker through Recurring-Convolution Neural Network (R-CNN). (arXiv:2112.14983v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2110.15557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianfu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jie Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yexin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu Zheng</a>",
          "description": "Illegal vehicle parking is a common urban problem faced by major cities in\nthe world, as it incurs traffic jams, which lead to air pollution and traffic\naccidents. The government highly relies on active human efforts to detect\nillegal parking events. However, such an approach is extremely ineffective to\ncover a large city since the police have to patrol over the entire city roads.\n\nThe massive and high-quality sharing bike trajectories from Mobike offer us a\nunique opportunity to design a ubiquitous illegal parking detection approach,\nas most of the illegal parking events happen at curbsides and have significant\nimpact on the bike users. The detection result can guide the patrol schedule,\ni.e. send the patrol policemen to the region with higher illegal parking risks,\nand further improve the patrol efficiency. Inspired by this idea, three main\ncomponents are employed in the proposed framework: 1)~{\\em trajectory\npre-processing}, which filters outlier GPS points, performs map-matching, and\nbuilds trajectory indexes; 2)~{\\em illegal parking detection}, which models the\nnormal trajectories, extracts features from the evaluation trajectories, and\nutilizes a distribution test-based method to discover the illegal parking\nevents; and 3)~{\\em patrol scheduling}, which leverages the detection result as\nreference context, and models the scheduling task as a multi-agent\nreinforcement learning problem to guide the patrol police. Finally, extensive\nexperiments are presented to validate the effectiveness of illegal parking\ndetection, as well as the improvement of patrol efficiency.",
          "link": "http://arxiv.org/abs/2110.15557",
          "publishedOn": "2022-01-03T07:15:18.622Z",
          "wordCount": 697,
          "title": "Crowd-sensing Enhanced Parking Patrol using Trajectories of Sharing Bikes. (arXiv:2110.15557v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11612",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiafan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We study reinforcement learning (RL) with linear function approximation.\nExisting algorithms for this problem only have high-probability regret and/or\nProbably Approximately Correct (PAC) sample complexity guarantees, which cannot\nguarantee the convergence to the optimal policy. In this paper, in order to\novercome the limitation of existing algorithms, we propose a new algorithm\ncalled FLUTE, which enjoys uniform-PAC convergence to the optimal policy with\nhigh probability. The uniform-PAC guarantee is the strongest possible guarantee\nfor reinforcement learning in the literature, which can directly imply both PAC\nand high probability regret bounds, making our algorithm superior to all\nexisting algorithms with linear function approximation. At the core of our\nalgorithm is a novel minimax value function estimator and a multi-level\npartition scheme to select the training samples from historical observations.\nBoth of these techniques are new and of independent interest.",
          "link": "http://arxiv.org/abs/2106.11612",
          "publishedOn": "2022-01-03T07:15:18.616Z",
          "wordCount": 600,
          "title": "Uniform-PAC Bounds for Reinforcement Learning with Linear Function Approximation. (arXiv:2106.11612v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silva_T/0/1/0/all/0/1\">Thalles Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivera_A/0/1/0/all/0/1\">Ad&#xed;n Ram&#xed;rez Rivera</a>",
          "description": "We introduce Consistent Assignment for Representation Learning (CARL), an\nunsupervised learning method to learn visual representations by combining ideas\nfrom self-supervised contrastive learning and deep clustering. By viewing\ncontrastive learning from a clustering perspective, CARL learns unsupervised\nrepresentations by learning a set of general prototypes that serve as energy\nanchors to enforce different views of a given image to be assigned to the same\nprototype. Unlike contemporary work on contrastive learning with deep\nclustering, CARL proposes to learn the set of general prototypes in an online\nfashion, using gradient descent without the necessity of using\nnon-differentiable algorithms or K-Means to solve the cluster assignment\nproblem. CARL surpasses its competitors in many representations learning\nbenchmarks, including linear evaluation, semi-supervised learning, and transfer\nlearning.",
          "link": "http://arxiv.org/abs/2112.15421",
          "publishedOn": "2022-01-03T07:15:18.608Z",
          "wordCount": 549,
          "title": "Representation Learning via Consistent Assignment of Views to Clusters. (arXiv:2112.15421v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bakkelund_D/0/1/0/all/0/1\">Daniel Bakkelund</a>",
          "description": "We present an objective function for similarity based hierarchical clustering\nof partially ordered data that preserves the partial order. That is, if $x \\le\ny$, and if $[x]$ and $[y]$ are the respective clusters of $x$ and $y$, then\nthere is an order relation $\\le'$ on the clusters for which $[x] \\le' |y]$. The\ntheory distinguishes itself from existing theories for clustering of ordered\ndata in that the order relation and the similarity are combined into a\nbi-objective optimisation problem to obtain a hierarchical clustering seeking\nto satisfy both. In particular, the order relation is weighted in the range\n$[0,1]$, and if the similarity and the order relation are not aligned, then\norder preservation may have to yield in favor of clustering. Finding an optimal\nsolution is NP-hard, so we provide a polynomial time approximation algorithm,\nwith a relative performance guarantee of $O\\!\\left(\\log^{3/2} \\!\\!\\, n\n\\right)$, based on successive applications of directed sparsest cut. We provide\na demonstration on a benchmark dataset, showing that our method outperforms\nexisting methods for order preserving hierarchical clustering with significant\nmargin. The theory is an extension of the Dasgupta cost function for divisive\nhierarchical clustering.",
          "link": "http://arxiv.org/abs/2109.04266",
          "publishedOn": "2022-01-03T07:15:18.601Z",
          "wordCount": 644,
          "title": "An objective function for order preserving hierarchical clustering. (arXiv:2109.04266v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1\">Yue Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isac_A/0/1/0/all/0/1\">Alka Isac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yimin Nie</a>",
          "description": "The analysis of long sequence data remains challenging in many real-world\napplications. We propose a novel architecture, ChunkFormer, that improves the\nexisting Transformer framework to handle the challenges while dealing with long\ntime series. Original Transformer-based models adopt an attention mechanism to\ndiscover global information along a sequence to leverage the contextual data.\nLong sequential data traps local information such as seasonality and\nfluctuations in short data sequences. In addition, the original Transformer\nconsumes more resources by carrying the entire attention matrix during the\ntraining course. To overcome these challenges, ChunkFormer splits the long\nsequences into smaller sequence chunks for the attention calculation,\nprogressively applying different chunk sizes in each stage. In this way, the\nproposed model gradually learns both local and global information without\nchanging the total length of the input sequences. We have extensively tested\nthe effectiveness of this new architecture on different business domains and\nhave proved the advantage of such a model over the existing Transformer-based\nmodels.",
          "link": "http://arxiv.org/abs/2112.15087",
          "publishedOn": "2022-01-03T07:15:18.583Z",
          "wordCount": 587,
          "title": "ChunkFormer: Learning Long Time Series with Multi-stage Chunked Transformer. (arXiv:2112.15087v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2111.12933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1\">Tal Ridnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharir_G/0/1/0/all/0/1\">Gilad Sharir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Cohen_A/0/1/0/all/0/1\">Avi Ben-Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1\">Emanuel Ben-Baruch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>",
          "description": "In this paper, we introduce ML-Decoder, a new attention-based classification\nhead. ML-Decoder predicts the existence of class labels via queries, and\nenables better utilization of spatial data compared to global average pooling.\nBy redesigning the decoder architecture, and using a novel group-decoding\nscheme, ML-Decoder is highly efficient, and can scale well to thousands of\nclasses. Compared to using a larger backbone, ML-Decoder consistently provides\na better speed-accuracy trade-off. ML-Decoder is also versatile - it can be\nused as a drop-in replacement for various classification heads, and generalize\nto unseen classes when operated with word queries. Novel query augmentations\nfurther improve its generalization ability. Using ML-Decoder, we achieve\nstate-of-the-art results on several classification tasks: on MS-COCO\nmulti-label, we reach 91.4% mAP; on NUS-WIDE zero-shot, we reach 31.1% ZSL mAP;\nand on ImageNet single-label, we reach with vanilla ResNet50 backbone a new top\nscore of 80.7%, without extra data or distillation. Public code is available\nat: https://github.com/Alibaba-MIIL/ML_Decoder",
          "link": "http://arxiv.org/abs/2111.12933",
          "publishedOn": "2022-01-03T07:15:18.577Z",
          "wordCount": 610,
          "title": "ML-Decoder: Scalable and Versatile Classification Head. (arXiv:2111.12933v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shunqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurkoski_B/0/1/0/all/0/1\">Brian M. Kurkoski</a>",
          "description": "Approximate message passing (AMP) is a promising technique for unknown signal\nreconstruction of certain high-dimensional linear systems with non-Gaussian\nsignaling. A distinguished feature of the AMP-type algorithms is that their\ndynamics can be rigorously described by state evolution. However, state\nevolution does not necessarily guarantee the convergence of iterative\nalgorithms. To solve the convergence problem of AMP-type algorithms in\nprinciple, this paper proposes a memory AMP (MAMP) under a sufficient statistic\ncondition, named sufficient statistic MAMP (SS-MAMP). We show that the\ncovariance matrices of SS-MAMP are L-banded and convergent. Given an arbitrary\nMAMP, we can construct an SS-MAMP by damping, which not only ensures the\nconvergence of MAMP but also preserves the orthogonality of MAMP, i.e., its\ndynamics can be rigorously described by state evolution. As a byproduct, we\nprove that the Bayes-optimal orthogonal/vector AMP (BO-OAMP/VAMP) is an\nSS-MAMP. As a result, we reveal two interesting properties of BO-OAMP/VAMP for\nlarge systems: 1) the covariance matrices are L-banded and are convergent in\nBO-OAMP/VAMP, and 2) damping and memory are useless (i.e., do not bring\nperformance improvement) in BO-OAMP/VAMP. As an example, we construct a\nsufficient statistic Bayes-optimal MAMP (BO-MAMP), which is Bayes optimal if\nits state evolution has a unique fixed point and its MSE is not worse than the\noriginal BO-MAMP. Finally, simulations are provided to verify the validity and\naccuracy of the theoretical results.",
          "link": "http://arxiv.org/abs/2112.15327",
          "publishedOn": "2022-01-03T07:15:18.570Z",
          "wordCount": 660,
          "title": "Sufficient Statistic Memory AMP. (arXiv:2112.15327v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/1906.09338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yunhui Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuolin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunter_C/0/1/0/all/0/1\">Carl A. Gunter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Recent advances in machine learning have largely benefited from the massive\naccessible training data. However, large-scale data sharing has raised great\nprivacy concerns. In this work, we propose a novel privacy-preserving data\nGenerative model based on the PATE framework (G-PATE), aiming to train a\nscalable differentially private data generator that preserves high generated\ndata utility. Our approach leverages generative adversarial nets to generate\ndata, combined with private aggregation among different discriminators to\nensure strong privacy guarantees. Compared to existing approaches, G-PATE\nsignificantly improves the use of privacy budgets. In particular, we train a\nstudent data generator with an ensemble of teacher discriminators and propose a\nnovel private gradient aggregation mechanism to ensure differential privacy on\nall information that flows from teacher discriminators to the student\ngenerator. In addition, with random projection and gradient discretization, the\nproposed gradient aggregation mechanism is able to effectively deal with\nhigh-dimensional gradient vectors. Theoretically, we prove that G-PATE ensures\ndifferential privacy for the data generator. Empirically, we demonstrate the\nsuperiority of G-PATE over prior work through extensive experiments. We show\nthat G-PATE is the first work being able to generate high-dimensional image\ndata with high data utility under limited privacy budgets ($\\epsilon \\le 1$).\nOur code is available at https://github.com/AI-secure/G-PATE.",
          "link": "http://arxiv.org/abs/1906.09338",
          "publishedOn": "2022-01-03T07:15:18.552Z",
          "wordCount": 661,
          "title": "G-PATE: Scalable Differentially Private Data Generator via Private Aggregation of Teacher Discriminators. (arXiv:1906.09338v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.14872",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Radhakrishnan_A/0/1/0/all/0/1\">Adityanarayanan Radhakrishnan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Belkin_M/0/1/0/all/0/1\">Mikhail Belkin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Uhler_C/0/1/0/all/0/1\">Caroline Uhler</a>",
          "description": "Establishing a fast rate of convergence for optimization methods is crucial\nto their applicability in practice. With the increasing popularity of deep\nlearning over the past decade, stochastic gradient descent and its adaptive\nvariants (e.g. Adagrad, Adam, etc.) have become prominent methods of choice for\nmachine learning practitioners. While a large number of works have demonstrated\nthat these first order optimization methods can achieve sub-linear or linear\nconvergence, we establish local quadratic convergence for stochastic gradient\ndescent with adaptive step size for problems such as matrix inversion.",
          "link": "http://arxiv.org/abs/2112.14872",
          "publishedOn": "2022-01-03T07:15:18.544Z",
          "wordCount": 525,
          "title": "Local Quadratic Convergence of Stochastic Gradient Descent with Adaptive Step Size. (arXiv:2112.14872v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2112.13890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zhenglun Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1\">Peiyan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1\">Wei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mengshu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1\">Minghai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>",
          "description": "Recently, Vision Transformer (ViT) has continuously established new\nmilestones in the computer vision field, while the high computation and memory\ncost makes its propagation in industrial production difficult. Pruning, a\ntraditional model compression paradigm for hardware efficiency, has been widely\napplied in various DNN structures. Nevertheless, it stays ambiguous on how to\nperform exclusive pruning on the ViT structure. Considering three key points:\nthe structural characteristics, the internal data pattern of ViTs, and the\nrelated edge device deployment, we leverage the input token sparsity and\npropose a computation-aware soft pruning framework, which can be set up on\nvanilla Transformers of both flatten and CNN-type structures, such as\nPooling-based ViT (PiT). More concretely, we design a dynamic attention-based\nmulti-head token selector, which is a lightweight module for adaptive\ninstance-wise token selection. We further introduce a soft pruning technique,\nwhich integrates the less informative tokens generated by the selector module\ninto a package token that will participate in subsequent calculations rather\nthan being completely discarded. Our framework is bound to the trade-off\nbetween accuracy and computation constraints of specific edge devices through\nour proposed computation-aware training strategy. Experimental results show\nthat our framework significantly reduces the computation cost of ViTs while\nmaintaining comparable performance on image classification. Moreover, our\nframework can guarantee the identified model to meet resource specifications of\nmobile devices and FPGA, and even achieve the real-time execution of DeiT-T on\nmobile platforms. For example, our method reduces the latency of DeiT-T to 26\nms (26%$\\sim $41% superior to existing works) on the mobile device with\n0.25%$\\sim $4% higher top-1 accuracy on ImageNet. Our code will be released\nsoon.",
          "link": "http://arxiv.org/abs/2112.13890",
          "publishedOn": "2022-01-03T07:15:18.523Z",
          "wordCount": 720,
          "title": "SPViT: Enabling Faster Vision Transformers via Soft Token Pruning. (arXiv:2112.13890v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shrikanth_N/0/1/0/all/0/1\">N.C. Shrikanth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menzies_T/0/1/0/all/0/1\">Tim Menzies</a>",
          "description": "Before researchers rush to reason across all available data or try complex\nmethods, perhaps it is prudent to first check for simpler alternatives.\nSpecifically, if the historical data has the most information in some small\nregion, then perhaps a model learned from that region would suffice for the\nrest of the project.\n\nTo support this claim, we offer a case study with 240 GitHub projects, where\nwe find that the information in those projects \"clumped\" towards the earliest\nparts of the project. A defect prediction model learned from just the first 150\ncommits works as well, or better than state-of-the-art alternatives. Using just\nthis early life cycle data, we can build models very quickly, very early in the\nsoftware project life cycle. Moreover, using this method, we have shown that a\nsimple model (with just two features) generalizes to hundreds of software\nprojects.\n\nBased on this experience, we doubt that prior work on generalizing software\nengineering defect prediction models may have needlessly complicated an\ninherently simple process. Further, prior work that focused on later-life cycle\ndata needs to be revisited since their conclusions were drawn from relatively\nuninformative regions.\n\nReplication note: all our data and scripts are online at\nhttps://github.com/snaraya7/simplifying-software-analytics",
          "link": "http://arxiv.org/abs/2105.11082",
          "publishedOn": "2022-01-03T07:15:18.516Z",
          "wordCount": 647,
          "title": "Simplifying Software Defect Prediction (via the \"early bird\" Heuristic). (arXiv:2105.11082v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.01201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raab_R/0/1/0/all/0/1\">Reilly Raab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "Realistically -- and equitably -- modeling the dynamics of group-level\ndisparities in machine learning remains an open problem. In particular, we\ndesire models that do not suppose inherent differences between artificial\ngroups of people -- but rather endogenize disparities by appeal to unequal\ninitial conditions of insular subpopulations. In this paper, agents each have a\nreal-valued feature $X$ (e.g., credit score) informed by a \"true\" binary label\n$Y$ representing qualification (e.g., for a loan). Each agent alternately (1)\nreceives a binary classification label $\\hat{Y}$ (e.g., loan approval) from a\nBayes-optimal machine learning classifier observing $X$ and (2) may update\ntheir qualification $Y$ by imitating successful strategies (e.g., seek a raise)\nwithin an isolated group $G$ of agents to which they belong. We consider the\ndisparity of qualification rates $\\Pr(Y=1)$ between different groups and how\nthis disparity changes subject to a sequence of Bayes-optimal classifiers\nrepeatedly retrained on the global population. We model the evolving\nqualification rates of each subpopulation (group) using the replicator\nequation, which derives from a class of imitation processes. We show that\ndifferences in qualification rates between subpopulations can persist\nindefinitely for a set of non-trivial equilibrium states due to uniformed\nclassifier deployments, even when groups are identical in all aspects except\ninitial qualification densities. We next simulate the effects of commonly\nproposed fairness interventions on this dynamical system along with a new\nfeedback control mechanism capable of permanently eliminating group-level\nqualification rate disparities. We conclude by discussing the limitations of\nour model and findings and by outlining potential future work.",
          "link": "http://arxiv.org/abs/2111.01201",
          "publishedOn": "2022-01-03T07:15:18.505Z",
          "wordCount": 729,
          "title": "Unintended Selection: Persistent Qualification Rate Disparities and Interventions. (arXiv:2111.01201v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.12678",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuchang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jiawei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuyi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jessie Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>",
          "description": "Federated edge learning (FEEL) has emerged as an effective approach to reduce\nthe large communication latency in Cloud-based machine learning solutions,\nwhile preserving data privacy. Unfortunately, the learning performance of FEEL\nmay be compromised due to limited training data in a single edge cluster. In\nthis paper, we investigate a novel framework of FEEL, namely semi-decentralized\nfederated edge learning (SD-FEEL). By allowing model aggregation across\ndifferent edge clusters, SD-FEEL enjoys the benefit of FEEL in reducing the\ntraining latency, while improving the learning performance by accessing richer\ntraining data from multiple edge clusters. A training algorithm for SD-FEEL\nwith three main procedures in each round is presented, including local model\nupdates, intra-cluster and inter-cluster model aggregations, which is proved to\nconverge on non-independent and identically distributed (non-IID) data. We also\ncharacterize the interplay between the network topology of the edge servers and\nthe communication overhead of inter-cluster model aggregation on the training\nperformance. Experiment results corroborate our analysis and demonstrate the\neffectiveness of SD-FFEL in achieving faster convergence than traditional\nfederated learning architectures. Besides, guidelines on choosing critical\nhyper-parameters of the training algorithm are also provided.",
          "link": "http://arxiv.org/abs/2104.12678",
          "publishedOn": "2022-01-03T07:15:18.470Z",
          "wordCount": 672,
          "title": "Semi-Decentralized Federated Edge Learning for Fast Convergence on Non-IID Data. (arXiv:2104.12678v5 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>",
          "description": "We introduce several new datasets namely ImageNet-A/O and ImageNet-R as well\nas a synthetic environment and testing suite we called CAOS. ImageNet-A/O allow\nresearchers to focus in on the blind spots remaining in ImageNet. ImageNet-R\nwas specifically created with the intention of tracking robust representation\nas the representations are no longer simply natural but include artistic, and\nother renditions. The CAOS suite is built off of CARLA simulator which allows\nfor the inclusion of anomalous objects and can create reproducible synthetic\nenvironment and scenes for testing robustness. All of the datasets were created\nfor testing robustness and measuring progress in robustness. The datasets have\nbeen used in various other works to measure their own progress in robustness\nand allowing for tangential progress that does not focus exclusively on natural\naccuracy.\n\nGiven these datasets, we created several novel methods that aim to advance\nrobustness research. We build off of simple baselines in the form of Maximum\nLogit, and Typicality Score as well as create a novel data augmentation method\nin the form of DeepAugment that improves on the aforementioned benchmarks.\nMaximum Logit considers the logit values instead of the values after the\nsoftmax operation, while a small change produces noticeable improvements. The\nTypicality Score compares the output distribution to a posterior distribution\nover classes. We show that this improves performance over the baseline in all\nbut the segmentation task. Speculating that perhaps at the pixel level the\nsemantic information of a pixel is less meaningful than that of class level\ninformation. Finally the new augmentation technique of DeepAugment utilizes\nneural networks to create augmentations on images that are radically different\nthan the traditional geometric and camera based transformations used\npreviously.",
          "link": "http://arxiv.org/abs/2112.15188",
          "publishedOn": "2022-01-03T07:15:18.463Z",
          "wordCount": 696,
          "title": "Towards Robustness of Neural Networks. (arXiv:2112.15188v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruoyu Wang</a>",
          "description": "Active learning (AL) is a machine learning algorithm that can achieve greater\naccuracy with fewer labeled training instances, for having the ability to ask\noracles to label the most valuable unlabeled data chosen iteratively and\nheuristically by query strategies. Scientific experiments nowadays, though\nbecoming increasingly automated, are still suffering from human involvement in\nthe designing process and the exhaustive search in the experimental space. This\narticle performs a retrospective study on a drug response dataset using the\nproposed AL scheme comprised of the matrix factorization method of alternating\nleast square (ALS) and deep neural networks (DNN). This article also proposes\nan AL query strategy based on expected loss minimization. As a result, the\nretrospective study demonstrates that scientific experimental design, instead\nof being manually set, can be optimized by AL, and the proposed query strategy\nELM sampling shows better experimental performance than other ones such as\nrandom sampling and uncertainty sampling.",
          "link": "http://arxiv.org/abs/2112.14811",
          "publishedOn": "2022-01-03T07:15:18.224Z",
          "wordCount": null,
          "title": "Active Learning-Based Optimization of Scientific Experimental Design. (arXiv:2112.14811v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15265",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1\">Lang Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pal_S/0/1/0/all/0/1\">Soumik Pal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Harchaoui_Z/0/1/0/all/0/1\">Zaid Harchaoui</a>",
          "description": "Optimal transport (OT) and its entropy regularized offspring have recently\ngained a lot of attention in both machine learning and AI domains. In\nparticular, optimal transport has been used to develop probability metrics\nbetween probability distributions. We introduce in this paper an independence\ncriterion based on entropy regularized optimal transport. Our criterion can be\nused to test for independence between two samples. We establish non-asymptotic\nbounds for our test statistic, and study its statistical behavior under both\nthe null and alternative hypothesis. Our theoretical results involve tools from\nU-process theory and optimal transport theory. We present experimental results\non existing benchmarks, illustrating the interest of the proposed criterion.",
          "link": "http://arxiv.org/abs/2112.15265",
          "publishedOn": "2022-01-03T07:15:18.224Z",
          "wordCount": null,
          "title": "Entropy Regularized Optimal Transport Independence Criterion. (arXiv:2112.15265v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Ruoming Jin</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_G/0/1/0/all/0/1\">Gagan Agrawal</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Ramnath_R/0/1/0/all/0/1\">Rajiv Ramnath</a> (4) ((1) Ohio State University, (2) Kent State University, (3) Augusta University)",
          "description": "Web-based interactions can be frequently represented by an attributed graph,\nand node clustering in such graphs has received much attention lately. Multiple\nefforts have successfully applied Graph Convolutional Networks (GCN), though\nwith some limits on accuracy as GCNs have been shown to suffer from\nover-smoothing issues. Though other methods (particularly those based on\nLaplacian Smoothing) have reported better accuracy, a fundamental limitation of\nall the work is a lack of scalability. This paper addresses this open problem\nby relating the Laplacian smoothing to the Generalized PageRank and applying a\nrandom-walk based algorithm as a scalable graph filter. This forms the basis\nfor our scalable deep clustering algorithm, RwSL, where through a\nself-supervised mini-batch training mechanism, we simultaneously optimize a\ndeep neural network for sample-cluster assignment distribution and an\nautoencoder for a clustering-oriented embedding. Using 6 real-world datasets\nand 6 clustering metrics, we show that RwSL achieved improved results over\nseveral recent baselines. Most notably, we show that RwSL, unlike all other\ndeep clustering frameworks, can continue to scale beyond graphs with more than\none million nodes, i.e., handle web-scale. We also demonstrate how RwSL could\nperform node clustering on a graph with 1.8 billion edges using only a single\nGPU.",
          "link": "http://arxiv.org/abs/2112.15530",
          "publishedOn": "2022-01-03T07:15:18.179Z",
          "wordCount": null,
          "title": "Scalable Deep Graph Clustering with Random-walk based Self-supervised Learning. (arXiv:2112.15530v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2111.15645",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Han_X/0/1/0/all/0/1\">X.Y. Han</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lewis_A/0/1/0/all/0/1\">Adrian S. Lewis</a>",
          "description": "For strongly convex objectives that are smooth, the classical theory of\ngradient descent ensures linear convergence relative to the number of gradient\nevaluations. An analogous nonsmooth theory is challenging: even when the\nobjective is smooth at every iterate, the corresponding local models are\nunstable, and traditional remedies need unpredictably many cutting planes. We\ninstead propose a multipoint generalization of the gradient descent iteration\nfor local optimization. While designed with general objectives in mind, we are\nmotivated by a \"max-of-smooth\" model that captures subdifferential dimension at\noptimality. We prove linear convergence when the objective is itself\nmax-of-smooth, and experiments suggest a more general phenomenon.",
          "link": "http://arxiv.org/abs/2111.15645",
          "publishedOn": "2022-01-03T07:15:18.179Z",
          "wordCount": null,
          "title": "Survey Descent: A Multipoint Generalization of Gradient Descent for Nonsmooth Optimization. (arXiv:2111.15645v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.12271",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jia_Q/0/1/0/all/0/1\">Qiran Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shu_H/0/1/0/all/0/1\">Hai Shu</a>",
          "description": "Convolutional neural networks (CNNs) have achieved remarkable success in\nautomatically segmenting organs or lesions on 3D medical images. Recently,\nvision transformer networks have exhibited exceptional performance in 2D image\nclassification tasks. Compared with CNNs, transformer networks have an\nappealing advantage of extracting long-range features due to their\nself-attention algorithm. Therefore, we propose a CNN-Transformer combined\nmodel, called BiTr-Unet, with specific modifications for brain tumor\nsegmentation on multi-modal MRI scans. Our BiTr-Unet achieves good performance\non the BraTS2021 validation dataset with median Dice score 0.9335, 0.9304 and\n0.8899, and median Hausdorff distance 2.8284, 2.2361 and 1.4142 for the whole\ntumor, tumor core, and enhancing tumor, respectively. On the BraTS2021 testing\ndataset, the corresponding results are 0.9257, 0.9350 and 0.8874 for Dice\nscore, and 3, 2.2361 and 1.4142 for Hausdorff distance. The code is publicly\navailable at https://github.com/JustaTinyDot/BiTr-Unet.",
          "link": "http://arxiv.org/abs/2109.12271",
          "publishedOn": "2022-01-03T07:15:18.172Z",
          "wordCount": null,
          "title": "BiTr-Unet: a CNN-Transformer Combined Network for MRI Brain Tumor Segmentation. (arXiv:2109.12271v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.11663",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shaocong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>",
          "description": "Alternating gradient-descent-ascent (AltGDA) is an optimization algorithm\nthat has been widely used for model training in various machine learning\napplications, which aim to solve a nonconvex minimax optimization problem.\nHowever, the existing studies show that it suffers from a high computation\ncomplexity in nonconvex minimax optimization. In this paper, we develop a\nsingle-loop and fast AltGDA-type algorithm that leverages proximal gradient\nupdates and momentum acceleration to solve regularized nonconvex minimax\noptimization problems. By identifying the intrinsic Lyapunov function of this\nalgorithm, we prove that it converges to a critical point of the nonconvex\nminimax optimization problem and achieves a computation complexity\n$\\mathcal{O}(\\kappa^{1.5}\\epsilon^{-2})$, where $\\epsilon$ is the desired level\nof accuracy and $\\kappa$ is the problem's condition number. Such a computation\ncomplexity improves the state-of-the-art complexities of single-loop GDA and\nAltGDA algorithms (see the summary of comparison in Table 1). We demonstrate\nthe effectiveness of our algorithm via an experiment on adversarial deep\nlearning.",
          "link": "http://arxiv.org/abs/2112.11663",
          "publishedOn": "2022-01-03T07:15:18.169Z",
          "wordCount": null,
          "title": "Accelerated Proximal Alternating Gradient-Descent-Ascent for Nonconvex Minimax Machine Learning. (arXiv:2112.11663v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.14793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Exarchakis_G/0/1/0/all/0/1\">Georgios Exarchakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oubari_O/0/1/0/all/0/1\">Omar Oubari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenz_G/0/1/0/all/0/1\">Gregor Lenz</a>",
          "description": "We propose a simple and efficient clustering method for high-dimensional data\nwith a large number of clusters. Our algorithm achieves high-performance by\nevaluating distances of datapoints with a subset of the cluster centres. Our\ncontribution is substantially more efficient than k-means as it does not\nrequire an all to all comparison of data points and clusters. We show that the\noptimal solutions of our approximation are the same as in the exact solution.\nHowever, our approach is considerably more efficient at extracting these\nclusters compared to the state-of-the-art. We compare our approximation with\nthe exact k-means and alternative approximation approaches on a series of\nstandardised clustering tasks. For the evaluation, we consider the algorithmic\ncomplexity, including number of operations to convergence, and the stability of\nthe results.",
          "link": "http://arxiv.org/abs/2112.14793",
          "publishedOn": "2022-01-03T07:15:18.168Z",
          "wordCount": null,
          "title": "A sampling-based approach for efficient clustering in large datasets. (arXiv:2112.14793v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_X/0/1/0/all/0/1\">Xiaoqian Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gaoang Wang</a>",
          "description": "Large-scale datasets are important for the development of deep learning\nmodels. Such datasets usually require a heavy workload of annotations, which\nare extremely time-consuming and expensive. To accelerate the annotation\nprocedure, multiple annotators may be employed to label different subsets of\nthe data. However, the inconsistency and bias among different annotators are\nharmful to the model training, especially for qualitative and subjective\ntasks.To address this challenge, in this paper, we propose a novel contrastive\nregression framework to address the disjoint annotations problem, where each\nsample is labeled by only one annotator and multiple annotators work on\ndisjoint subsets of the data. To take account of both the intra-annotator\nconsistency and inter-annotator inconsistency, two strategies are\nemployed.Firstly, a contrastive-based loss is applied to learn the relative\nranking among different samples of the same annotator, with the assumption that\nthe ranking of samples from the same annotator is unanimous. Secondly, we apply\nthe gradient reversal layer to learn robust representations that are invariant\nto different annotators. Experiments on the facial expression prediction task,\nas well as the image quality assessment task, verify the effectiveness of our\nproposed framework.",
          "link": "http://arxiv.org/abs/2112.15411",
          "publishedOn": "2022-01-03T07:15:18.168Z",
          "wordCount": null,
          "title": "Disjoint Contrastive Regression Learning for Multi-Sourced Annotations. (arXiv:2112.15411v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15486",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yifan Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_K/0/1/0/all/0/1\">Kevin Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertozzi_A/0/1/0/all/0/1\">Andrea L. Bertozzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bao Wang</a>",
          "description": "We propose near-optimal overlay networks based on $d$-regular expander graphs\nto accelerate decentralized federated learning (DFL) and improve its\ngeneralization. In DFL a massive number of clients are connected by an overlay\nnetwork, and they solve machine learning problems collaboratively without\nsharing raw data. Our overlay network design integrates spectral graph theory\nand the theoretical convergence and generalization bounds for DFL. As such, our\nproposed overlay networks accelerate convergence, improve generalization, and\nenhance robustness to clients failures in DFL with theoretical guarantees.\nAlso, we present an efficient algorithm to convert a given graph to a practical\noverlay network and maintaining the network topology after potential client\nfailures. We numerically verify the advantages of DFL with our proposed\nnetworks on various benchmark tasks, ranging from image classification to\nlanguage modeling using hundreds of clients.",
          "link": "http://arxiv.org/abs/2112.15486",
          "publishedOn": "2022-01-03T07:15:18.168Z",
          "wordCount": null,
          "title": "Efficient and Reliable Overlay Networks for Decentralized Federated Learning. (arXiv:2112.15486v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sung Min Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Kuo-An Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1\">Kai Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jerry Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madry_A/0/1/0/all/0/1\">Aleksander Madry</a>",
          "description": "We identify properties of universal adversarial perturbations (UAPs) that\ndistinguish them from standard adversarial perturbations. Specifically, we show\nthat targeted UAPs generated by projected gradient descent exhibit two\nhuman-aligned properties: semantic locality and spatial invariance, which\nstandard targeted adversarial perturbations lack. We also demonstrate that UAPs\ncontain significantly less signal for generalization than standard adversarial\nperturbations -- that is, UAPs leverage non-robust features to a smaller extent\nthan standard adversarial perturbations.",
          "link": "http://arxiv.org/abs/2112.15329",
          "publishedOn": "2022-01-03T07:15:18.163Z",
          "wordCount": null,
          "title": "On Distinctive Properties of Universal Perturbations. (arXiv:2112.15329v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xi_D/0/1/0/all/0/1\">Dongbo Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1\">Bowen Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yongchun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Dan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>",
          "description": "Many prediction tasks of real-world applications need to model multi-order\nfeature interactions in user's event sequence for better detection performance.\nHowever, existing popular solutions usually suffer two key issues: 1) only\nfocusing on feature interactions and failing to capture the sequence influence;\n2) only focusing on sequence information, but ignoring internal feature\nrelations of each event, thus failing to extract a better event representation.\nIn this paper, we consider a two-level structure for capturing the hierarchical\ninformation over user's event sequence: 1) learning effective feature\ninteractions based event representation; 2) modeling the sequence\nrepresentation of user's historical events. Experimental results on both\nindustrial and public datasets clearly demonstrate that our model achieves\nsignificantly better performance compared with state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2112.15292",
          "publishedOn": "2022-01-03T07:15:18.158Z",
          "wordCount": null,
          "title": "Neural Hierarchical Factorization Machines for User's Event Sequence Analysis. (arXiv:2112.15292v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sevyeri_L/0/1/0/all/0/1\">Laya Rafiee Sevyeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevens_T/0/1/0/all/0/1\">Thomas Fevens</a>",
          "description": "Identifying anomalies refers to detecting samples that do not resemble the\ntraining data distribution. Many generative models have been used to find\nanomalies, and among them, generative adversarial network (GAN)-based\napproaches are currently very popular. GANs mainly rely on the rich contextual\ninformation of these models to identify the actual training distribution.\nFollowing this analogy, we suggested a new unsupervised model based on GANs --a\ncombination of an autoencoder and a GAN. Further, a new scoring function was\nintroduced to target anomalies where a linear combination of the internal\nrepresentation of the discriminator and the generator's visual representation,\nplus the encoded representation of the autoencoder, come together to define the\nproposed anomaly score. The model was further evaluated on benchmark datasets\nsuch as SVHN, CIFAR10, and MNIST, as well as a public medical dataset of\nleukemia images. In all the experiments, our model outperformed its existing\ncounterparts while slightly improving the inference time.",
          "link": "http://arxiv.org/abs/2112.15541",
          "publishedOn": "2022-01-03T07:15:18.158Z",
          "wordCount": null,
          "title": "on the effectiveness of generative adversarial network on anomaly detection. (arXiv:2112.15541v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12375",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Atz_K/0/1/0/all/0/1\">Kenneth Atz</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Grisoni_F/0/1/0/all/0/1\">Francesca Grisoni</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Schneider_G/0/1/0/all/0/1\">Gisbert Schneider</a>",
          "description": "Geometric deep learning (GDL), which is based on neural network architectures\nthat incorporate and process symmetry information, has emerged as a recent\nparadigm in artificial intelligence. GDL bears particular promise in molecular\nmodeling applications, in which various molecular representations with\ndifferent symmetry properties and levels of abstraction exist. This review\nprovides a structured and harmonized overview of molecular GDL, highlighting\nits applications in drug discovery, chemical synthesis prediction, and quantum\nchemistry. Emphasis is placed on the relevance of the learned molecular\nfeatures and their complementarity to well-established molecular descriptors.\nThis review provides an overview of current challenges and opportunities, and\npresents a forecast of the future of GDL for molecular sciences.",
          "link": "http://arxiv.org/abs/2107.12375",
          "publishedOn": "2022-01-03T07:15:18.158Z",
          "wordCount": null,
          "title": "Geometric Deep Learning on Molecular Representations. (arXiv:2107.12375v4 [physics.chem-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Torres_P/0/1/0/all/0/1\">Pablo Torres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sirmacek_B/0/1/0/all/0/1\">Beril Sirmacek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoyas_S/0/1/0/all/0/1\">Sergio Hoyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinuesa_R/0/1/0/all/0/1\">Ricardo Vinuesa</a>",
          "description": "The sustainability of urban environments is an increasingly relevant problem.\nAir pollution plays a key role in the degradation of the environment as well as\nthe health of the citizens exposed to it. In this chapter we provide a review\nof the methods available to model air pollution, focusing on the application of\nmachine-learning methods. In fact, machine-learning methods have proved to\nimportantly increase the accuracy of traditional air-pollution approaches while\nlimiting the development cost of the models. Machine-learning tools have opened\nnew approaches to study air pollution, such as flow-dynamics modelling or\nremote-sensing methodologies.",
          "link": "http://arxiv.org/abs/2112.15115",
          "publishedOn": "2022-01-03T07:15:18.156Z",
          "wordCount": null,
          "title": "Aim in Climate Change and City Pollution. (arXiv:2112.15115v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quanziang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renzhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>",
          "description": "Continual learning requires models to learn new tasks while maintaining\npreviously learned knowledge. Various algorithms have been proposed to address\nthis real challenge. Till now, rehearsal-based methods, such as experience\nreplay, have achieved state-of-the-art performance. These approaches save a\nsmall part of the data of the past tasks as a memory buffer to prevent models\nfrom forgetting previously learned knowledge. However, most of them treat every\nnew task equally, i.e., fixed the hyperparameters of the framework while\nlearning different new tasks. Such a setting lacks the consideration of the\nrelationship/similarity between past and new tasks. For example, the previous\nknowledge/features learned from dogs are more beneficial for the identification\nof cats (new task), compared to those learned from buses. In this regard, we\npropose a meta learning algorithm based on bi-level optimization to adaptively\ntune the relationship between the knowledge extracted from the past and new\ntasks. Therefore, the model can find an appropriate direction of gradient\nduring continual learning and avoid the serious overfitting problem on memory\nbuffer. Extensive experiments are conducted on three publicly available\ndatasets (i.e., CIFAR-10, CIFAR-100, and Tiny ImageNet). The experimental\nresults demonstrate that the proposed method can consistently improve the\nperformance of all baselines.",
          "link": "http://arxiv.org/abs/2112.15402",
          "publishedOn": "2022-01-03T07:15:18.155Z",
          "wordCount": null,
          "title": "Revisiting Experience Replay: Continual Learning by Adaptively Tuning Task-wise Relationship. (arXiv:2112.15402v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2111.08888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1\">Ruiqi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1\">Rongxin Cui</a>",
          "description": "Unified understanding of neuro networks (NNs) gets the users into great\ntrouble because they have been puzzled by what kind of rules should be obeyed\nto optimize the internal structure of NNs. Considering the potential capability\nof random graphs to alter how computation is performed, we demonstrate that\nthey can serve as architecture generators to optimize the internal structure of\nNNs. To transform the random graph theory into an NN model with practical\nmeaning and based on clarifying the input-output relationship of each neuron,\nwe complete data feature mapping by calculating Fourier Random Features (FRFs).\nUnder the usage of this low-operation cost approach, neurons are assigned to\nseveral groups of which connection relationships can be regarded as uniform\nrepresentations of random graphs they belong to, and random arrangement fuses\nthose neurons to establish the pattern matrix, markedly reducing manual\nparticipation and computational cost without the fixed and deep architecture.\nLeveraging this single neuromorphic learning model termed random graph-based\nneuro network (RGNN) we develop a joint classification mechanism involving\ninformation interaction between multiple RGNNs and realize significant\nperformance improvements in supervised learning for three benchmark tasks,\nwhereby they effectively avoid the adverse impact of the interpretability of\nNNs on the structure design and engineering practice.",
          "link": "http://arxiv.org/abs/2111.08888",
          "publishedOn": "2022-01-03T07:15:18.154Z",
          "wordCount": null,
          "title": "Random Graph-Based Neuromorphic Learning with a Layer-Weaken Structure. (arXiv:2111.08888v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.13811",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Guharoy_R/0/1/0/all/0/1\">Rabel Guharoy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jana_N/0/1/0/all/0/1\">Nanda Dulal Jana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biswas_S/0/1/0/all/0/1\">Suparna Biswas</a>",
          "description": "This paper presents an epilepsy detection method based on discrete wavelet\ntransform (DWT) and Machine learning classifiers. Here DWT has been used for\nfeature extraction as it provides a better decomposition of the signals in\ndifferent frequency bands. At first, DWT has been applied to the EEG signal to\nextract the detail and approximate coefficients or different sub-bands. After\nthe extraction of the coefficients, principal component analysis (PCA) has been\napplied on different sub-bands and then a feature level fusion technique is\nused to extract the important features in low dimensional feature space. Three\nclassifiers namely: Support Vector Machine (SVM) classifier, K-Nearest-Neighbor\n(KNN) classifier, and Naive Bayes (NB) Classifiers have been used in the\nproposed work for classifying the EEG signals. The proposed method is tested on\nBonn databases and provides a maximum of 100% recognition accuracy for KNN,\nSVM, NB classifiers.",
          "link": "http://arxiv.org/abs/2109.13811",
          "publishedOn": "2022-01-03T07:15:18.152Z",
          "wordCount": null,
          "title": "An Efficient Epileptic Seizure Detection Technique using Discrete Wavelet Transform and Machine Learning Classifiers. (arXiv:2109.13811v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dongge Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_V/0/1/0/all/0/1\">Van-Thuan Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_G/0/1/0/all/0/1\">Gidon Ernst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_T/0/1/0/all/0/1\">Toby Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1\">Benjamin I.P. Rubinstein</a>",
          "description": "The statefulness property of network protocol implementations poses a unique\nchallenge for testing and verification techniques, including Fuzzing. Stateful\nfuzzers tackle this challenge by leveraging state models to partition the state\nspace and assist the test generation process. Since not all states are equally\nimportant and fuzzing campaigns have time limits, fuzzers need effective state\nselection algorithms to prioritize progressive states over others. Several\nstate selection algorithms have been proposed but they were implemented and\nevaluated separately on different platforms, making it hard to achieve\nconclusive findings. In this work, we evaluate an extensive set of state\nselection algorithms on the same fuzzing platform that is AFLNet, a\nstate-of-the-art fuzzer for network servers. The algorithm set includes\nexisting ones supported by AFLNet and our novel and principled algorithm called\nAFLNetLegion. The experimental results on the ProFuzzBench benchmark show that\n(i) the existing state selection algorithms of AFLNet achieve very similar code\ncoverage, (ii) AFLNetLegion clearly outperforms these algorithms in selected\ncase studies, but (iii) the overall improvement appears insignificant. These\nare unexpected yet interesting findings. We identify problems and share\ninsights that could open opportunities for future research on this topic.",
          "link": "http://arxiv.org/abs/2112.15498",
          "publishedOn": "2022-01-03T07:15:18.150Z",
          "wordCount": null,
          "title": "State Selection Algorithms and Their Impact on The Performance of Stateful Network Protocol Fuzzing. (arXiv:2112.15498v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15568",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lahire_T/0/1/0/all/0/1\">Thibault Lahire</a>",
          "description": "This technical report is devoted to explaining how the actor loss of soft\nactor critic is obtained, as well as the associated gradient estimate. It gives\nthe necessary mathematical background to derive all the presented equations,\nfrom the theoretical actor loss to the one implemented in practice. This\nnecessitates a comparison of the reparameterization trick used in soft actor\ncritic with the nabla log trick, which leads to open questions regarding the\nmost efficient method to use.",
          "link": "http://arxiv.org/abs/2112.15568",
          "publishedOn": "2022-01-03T07:15:18.149Z",
          "wordCount": null,
          "title": "Actor Loss of Soft Actor Critic Explained. (arXiv:2112.15568v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06201",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Loecher_M/0/1/0/all/0/1\">Markus Loecher</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>",
          "description": "Tree-based algorithms such as random forests and gradient boosted trees\ncontinue to be among the most popular and powerful machine learning models used\nacross multiple disciplines. The conventional wisdom of estimating the impact\nof a feature in tree based models is to measure the \\textit{node-wise reduction\nof a loss function}, which (i) yields only global importance measures and (ii)\nis known to suffer from severe biases. Conditional feature contributions (CFCs)\nprovide \\textit{local}, case-by-case explanations of a prediction by following\nthe decision path and attributing changes in the expected output of the model\nto each feature along the path. However, Lundberg et al. pointed out a\npotential bias of CFCs which depends on the distance from the root of a tree.\nThe by now immensely popular alternative, SHapley Additive exPlanation (SHAP)\nvalues appear to mitigate this bias but are computationally much more\nexpensive. Here we contribute a thorough comparison of the explanations\ncomputed by both methods on a set of 164 publicly available classification\nproblems in order to provide data-driven algorithm recommendations to current\nresearchers. For random forests, we find extremely high similarities and\ncorrelations of both local and global SHAP values and CFC scores, leading to\nvery similar rankings and interpretations. Analogous conclusions hold for the\nfidelity of using global feature importance scores as a proxy for the\npredictive power associated with each feature.",
          "link": "http://arxiv.org/abs/2108.06201",
          "publishedOn": "2022-01-03T07:15:18.148Z",
          "wordCount": null,
          "title": "Data-driven advice for interpreting local and global model predictions in bioinformatics problems. (arXiv:2108.06201v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadiq_I/0/1/0/all/0/1\">Ismail Sadiq</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Alday_E/0/1/0/all/0/1\">Erick A. Perez-Alday</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Amit J. Shah</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Rad_A/0/1/0/all/0/1\">Ali Bahrami Rad</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Sameni_R/0/1/0/all/0/1\">Reza Sameni</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Clifford_G/0/1/0/all/0/1\">Gari D. Clifford</a> (1,2)",
          "description": "Objective: To determine if a realistic, but computationally efficient model\nof the electrocardiogram can be used to pre-train a deep neural network (DNN)\nwith a wide range of morphologies and abnormalities specific to a given\ncondition - T-wave Alternans (TWA) as a result of Post-Traumatic Stress\nDisorder, or PTSD - and significantly boost performance on a small database of\nrare individuals.\n\nApproach: Using a previously validated artificial ECG model, we generated\n180,000 artificial ECGs with or without significant TWA, with varying heart\nrate, breathing rate, TWA amplitude, and ECG morphology. A DNN, trained on over\n70,000 patients to classify 25 different rhythms, was modified the output layer\nto a binary class (TWA or no-TWA, or equivalently, PTSD or no-PTSD), and\ntransfer learning was performed on the artificial ECG. In a final transfer\nlearning step, the DNN was trained and cross-validated on ECG from 12 PTSD and\n24 controls for all combinations of using the three databases.\n\nMain results: The best performing approach (AUROC = 0.77, Accuracy = 0.72,\nF1-score = 0.64) was found by performing both transfer learning steps, using\nthe pre-trained arrhythmia DNN, the artificial data and the real PTSD-related\nECG data. Removing the artificial data from training led to the largest drop in\nperformance. Removing the arrhythmia data from training provided a modest, but\nsignificant, drop in performance. The final model showed no significant drop in\nperformance on the artificial data, indicating no overfitting.\n\nSignificance: In healthcare, it is common to only have a small collection of\nhigh-quality data and labels, or a larger database with much lower quality (and\nless relevant) labels. The paradigm presented here, involving model-based\nperformance boosting, provides a solution through transfer learning on a large\nrealistic artificial database, and a partially relevant real database.",
          "link": "http://arxiv.org/abs/2112.15442",
          "publishedOn": "2022-01-03T07:15:18.147Z",
          "wordCount": null,
          "title": "Mythological Medical Machine Learning: Boosting the Performance of a Deep Learning Medical Data Classifier Using Realistic Physiological Models. (arXiv:2112.15442v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.12961",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yuan_C/0/1/0/all/0/1\">Chaoxia Yuan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ying_C/0/1/0/all/0/1\">Chao Ying</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fang_F/0/1/0/all/0/1\">Fang Fang</a>",
          "description": "Support vector machine (SVM) is a powerful classification method that has\nachieved great success in many fields. Since its performance can be seriously\nimpaired by redundant covariates, model selection techniques are widely used\nfor SVM with high dimensional covariates. As an alternative to model selection,\nsignificant progress has been made in the area of model averaging in the past\ndecades. Yet no frequentist model averaging method was considered for SVM. This\nwork aims to fill the gap and to propose a frequentist model averaging\nprocedure for SVM which selects the optimal weight by cross validation. Even\nwhen the number of covariates diverges at an exponential rate of the sample\nsize, we show asymptotic optimality of the proposed method in the sense that\nthe ratio of its hinge loss to the lowest possible loss converges to one. We\nalso derive the convergence rate which provides more insights to model\naveraging. Compared to model selection methods of SVM which require a tedious\nbut critical task of tuning parameter selection, the model averaging method\navoids the task and shows promising performances in the empirical studies.",
          "link": "http://arxiv.org/abs/2112.12961",
          "publishedOn": "2022-01-03T07:15:18.146Z",
          "wordCount": null,
          "title": "Optimal Model Averaging of Support Vector Machines in Diverging Model Spaces. (arXiv:2112.12961v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.10523",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_D/0/1/0/all/0/1\">Ding Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pentland_A/0/1/0/all/0/1\">Alex Pentland</a>",
          "description": "Long ties, the social ties that bridge different communities, are widely\nbelieved to play crucial roles in spreading novel information in social\nnetworks. However, some existing network theories and prediction models\nindicate that long ties might dissolve quickly or eventually become redundant,\nthus putting into question the long-term value of long ties. Our empirical\nanalysis of real-world dynamic networks shows that contrary to such reasoning,\nlong ties are more likely to persist than other social ties, and that many of\nthem constantly function as social bridges without being embedded in local\nnetworks. Using a novel cost-benefit analysis model combined with machine\nlearning, we show that long ties are highly beneficial, which instinctively\nmotivates people to expend extra effort to maintain them. This partly explains\nwhy long ties are more persistent than what has been suggested by many existing\ntheories and models. Overall, our study suggests the need for social\ninterventions that can promote the formation of long ties, such as mixing\npeople with diverse backgrounds.",
          "link": "http://arxiv.org/abs/2109.10523",
          "publishedOn": "2022-01-03T07:15:18.145Z",
          "wordCount": null,
          "title": "Investigating and Modeling the Dynamics of Long Ties. (arXiv:2109.10523v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.08878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulshrestha_A/0/1/0/all/0/1\">Ankit Kulshrestha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safro_I/0/1/0/all/0/1\">Ilya Safro</a>",
          "description": "The rapid growth of data in the recent years has led to the development of\ncomplex learning algorithms that are often used to make decisions in real\nworld. While the positive impact of the algorithms has been tremendous, there\nis a need to mitigate any bias arising from either training samples or implicit\nassumptions made about the data samples. This need becomes critical when\nalgorithms are used in automated decision making systems that can hugely impact\npeople's lives.\n\nMany approaches have been proposed to make learning algorithms fair by\ndetecting and mitigating bias in different stages of optimization. However, due\nto a lack of a universal definition of fairness, these algorithms optimize for\na particular interpretation of fairness which makes them limited for real world\nuse. Moreover, an underlying assumption that is common to all algorithms is the\napparent equivalence of achieving fairness and removing bias. In other words,\nthere is no user defined criteria that can be incorporated into the\noptimization procedure for producing a fair algorithm. Motivated by these\nshortcomings of existing methods, we propose the CONFAIR procedure that\nproduces a fair algorithm by incorporating user constraints into the\noptimization procedure. Furthermore, we make the process interpretable by\nestimating the most predictive features from data. We demonstrate the efficacy\nof our approach on several real world datasets using different fairness\ncriteria.",
          "link": "http://arxiv.org/abs/2111.08878",
          "publishedOn": "2022-01-03T07:15:18.145Z",
          "wordCount": null,
          "title": "CONFAIR: Configurable and Interpretable Algorithmic Fairness. (arXiv:2111.08878v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1\">Farley Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadav_A/0/1/0/all/0/1\">Asim Kadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruus_E/0/1/0/all/0/1\">Erik Kruus</a>",
          "description": "The recent success of deep learning applications has coincided with those\nwidely available powerful computational resources for training sophisticated\nmachine learning models with huge datasets. Nonetheless, training large models\nsuch as convolutional neural networks using model parallelism (as opposed to\ndata parallelism) is challenging because the complex nature of communication\nbetween model shards makes it difficult to partition the computation\nefficiently across multiple machines with an acceptable trade-off. This paper\npresents SplitBrain, a high performance distributed deep learning framework\nsupporting hybrid data and model parallelism. Specifically, SplitBrain provides\nlayer-specific partitioning that co-locates compute intensive convolutional\nlayers while sharding memory demanding layers. A novel scalable group\ncommunication is proposed to further improve the training throughput with\nreduced communication overhead. The results show that SplitBrain can achieve\nnearly linear speedup while saving up to 67\\% of memory consumption for data\nand model parallel VGG over CIFAR-10.",
          "link": "http://arxiv.org/abs/2112.15317",
          "publishedOn": "2022-01-03T07:15:18.143Z",
          "wordCount": null,
          "title": "SplitBrain: Hybrid Data and Model Parallel Deep Learning. (arXiv:2112.15317v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15523",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Aneja_S/0/1/0/all/0/1\">Sandhya Aneja</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aneja_N/0/1/0/all/0/1\">Nagender Aneja</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abas_P/0/1/0/all/0/1\">Pg Emeroylariffion Abas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naim_A/0/1/0/all/0/1\">Abdul Ghani Naim</a>",
          "description": "Transfer learning allows us to exploit knowledge gained from one task to\nassist in solving another but relevant task. In modern computer vision\nresearch, the question is which architecture performs better for a given\ndataset. In this paper, we compare the performance of 14 pre-trained ImageNet\nmodels on the histopathologic cancer detection dataset, where each model has\nbeen configured as a naive model, feature extractor model, or fine-tuned model.\nDensenet161 has been shown to have high precision whilst Resnet101 has a high\nrecall. A high precision model is suitable to be used when follow-up\nexamination cost is high, whilst low precision but a high recall/sensitivity\nmodel can be used when the cost of follow-up examination is low. Results also\nshow that transfer learning helps to converge a model faster.",
          "link": "http://arxiv.org/abs/2112.15523",
          "publishedOn": "2022-01-03T07:15:18.143Z",
          "wordCount": null,
          "title": "Transfer learning for cancer diagnosis in histopathological images. (arXiv:2112.15523v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2112.12970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rongjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>",
          "description": "Scene Graph Generation (SGG) remains a challenging visual understanding task\ndue to its complex compositional property. Most previous works adopt a\nbottom-up two-stage or a point-based one-stage approach, which often suffers\nfrom overhead time complexity or sub-optimal design assumption. In this work,\nwe propose a novel SGG method to address the aforementioned issues, which\nformulates the task as a bipartite graph construction problem. To solve the\nproblem, we develop a transformer-based end-to-end framework that first\ngenerates the entity and predicate proposal set, followed by inferring directed\nedges to form the relation triplets. In particular, we develop a new\nentity-aware predicate representation based on a structural predicate generator\nto leverage the compositional property of relationships. Moreover, we design a\ngraph assembling module to infer the connectivity of the bipartite scene graph\nbased on our entity-aware structure, enabling us to generate the scene graph in\nan end-to-end manner. Extensive experimental results show that our design is\nable to achieve the state-of-the-art or comparable performance on two\nchallenging benchmarks, surpassing most of the existing approaches and enjoying\nhigher efficiency in inference. We hope our model can serve as a strong\nbaseline for the Transformer-based scene graph generation.",
          "link": "http://arxiv.org/abs/2112.12970",
          "publishedOn": "2022-01-03T07:15:18.143Z",
          "wordCount": null,
          "title": "SGTR: End-to-end Scene Graph Generation with Transformer. (arXiv:2112.12970v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.01806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>",
          "description": "Dance choreography for a piece of music is a challenging task, having to be\ncreative in presenting distinctive stylistic dance elements while taking into\naccount the musical theme and rhythm. It has been tackled by different\napproaches such as similarity retrieval, sequence-to-sequence modeling and\ngenerative adversarial networks, but their generated dance sequences are often\nshort of motion realism, diversity and music consistency. In this paper, we\npropose a Music-to-Dance with Optimal Transport Network (MDOT-Net) for learning\nto generate 3D dance choreographs from music. We introduce an optimal transport\ndistance for evaluating the authenticity of the generated dance distribution\nand a Gromov-Wasserstein distance to measure the correspondence between the\ndance distribution and the input music. This gives a well defined and\nnon-divergent training objective that mitigates the limitation of standard GAN\ntraining which is frequently plagued with instability and divergent generator\nloss issues. Extensive experiments demonstrate that our MDOT-Net can synthesize\nrealistic and diverse dances which achieve an organic unity with the input\nmusic, reflecting the shared intentionality and matching the rhythmic\narticulation.",
          "link": "http://arxiv.org/abs/2112.01806",
          "publishedOn": "2022-01-03T07:15:18.143Z",
          "wordCount": null,
          "title": "Music-to-Dance Generation with Optimal Transport. (arXiv:2112.01806v1 [cs.SD] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuanying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhining Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Li Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1\">Lihong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiaodong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yize Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjie Gu</a>",
          "description": "Many payment platforms hold large-scale marketing campaigns, which allocate\nincentives to encourage users to pay through their applications. To maximize\nthe return on investment, incentive allocations are commonly solved in a\ntwo-stage procedure. After training a response estimation model to estimate the\nusers' mobile payment probabilities (MPP), a linear programming process is\napplied to obtain the optimal incentive allocation. However, the large amount\nof biased data in the training set, generated by the previous biased allocation\npolicy, causes a biased estimation. This bias deteriorates the performance of\nthe response model and misleads the linear programming process, dramatically\ndegrading the performance of the resulting allocation policy. To overcome this\nobstacle, we propose a bias correction adversarial network. Our method\nleverages the small set of unbiased data obtained under a full-randomized\nallocation policy to train an unbiased model and then uses it to reduce the\nbias with adversarial learning. Offline and online experimental results\ndemonstrate that our method outperforms state-of-the-art approaches and\nsignificantly improves the performance of the resulting allocation policy in a\nreal-world marketing campaign.",
          "link": "http://arxiv.org/abs/2112.15434",
          "publishedOn": "2022-01-03T07:15:18.137Z",
          "wordCount": null,
          "title": "Adversarial Learning for Incentive Optimization in Mobile Payment Marketing. (arXiv:2112.15434v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiyang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuxiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>",
          "description": "Graph neural network (GNN) has shown convincing performance in learning\npowerful node representations that preserve both node attributes and graph\nstructural information. However, many GNNs encounter problems in effectiveness\nand efficiency when they are designed with a deeper network structure or handle\nlarge-sized graphs. Several sampling algorithms have been proposed for\nimproving and accelerating the training of GNNs, yet they ignore understanding\nthe source of GNN performance gain. The measurement of information within graph\ndata can help the sampling algorithms to keep high-value information while\nremoving redundant information and even noise. In this paper, we propose a\nMetric-Guided (MeGuide) subgraph learning framework for GNNs. MeGuide employs\ntwo novel metrics: Feature Smoothness and Connection Failure Distance to guide\nthe subgraph sampling and mini-batch based training. Feature Smoothness is\ndesigned for analyzing the feature of nodes in order to retain the most\nvaluable information, while Connection Failure Distance can measure the\nstructural information to control the size of subgraphs. We demonstrate the\neffectiveness and efficiency of MeGuide in training various GNNs on multiple\ndatasets.",
          "link": "http://arxiv.org/abs/2112.15015",
          "publishedOn": "2022-01-03T07:15:18.135Z",
          "wordCount": null,
          "title": "Measuring and Sampling: A Metric-guided Subgraph Learning Framework for Graph Neural Network. (arXiv:2112.15015v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun-Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi-Jen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yun-Cheng Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Samuel Yen-Chi Chen</a>",
          "description": "The importance of deep learning data privacy has gained significant attention\nin recent years. It is probably to suffer data breaches when applying deep\nlearning to cryptocurrency that lacks supervision of financial regulatory\nagencies. However, there is little relative research in the financial area to\nour best knowledge. We apply two representative deep learning privacy-privacy\nframeworks proposed by Google to financial trading data. We designed the\nexperiments with several different parameters suggested from the original\nstudies. In addition, we refer the degree of privacy to Google and Apple\ncompanies to estimate the results more reasonably. The results show that DP-SGD\nperforms better than the PATE framework in financial trading data. The tradeoff\nbetween privacy and accuracy is low in DP-SGD. The degree of privacy also is in\nline with the actual case. Therefore, we can obtain a strong privacy guarantee\nwith precision to avoid potential financial loss.",
          "link": "http://arxiv.org/abs/2112.14075",
          "publishedOn": "2022-01-03T07:15:18.126Z",
          "wordCount": null,
          "title": "Financial Vision Based Differential Privacy Applications. (arXiv:2112.14075v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiaqi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingjian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_Q/0/1/0/all/0/1\">Qiaozhu Mei</a>",
          "description": "Multinomial Logit (MNL) is one of the most popular discrete choice models and\nhas been widely used to model ranking data. However, there is a long-standing\ntechnical challenge of learning MNL from many real-world ranking data: exact\ncalculation of the MNL likelihood of \\emph{partial rankings} is generally\nintractable. In this work, we develop a scalable method for approximating the\nMNL likelihood of general partial rankings in polynomial time complexity. We\nalso extend the proposed method to learn mixture of MNL. We demonstrate that\nthe proposed methods are particularly helpful for applications to choice-based\nnetwork formation modeling, where the formation of new edges in a network is\nviewed as individuals making choices of their friends over a candidate set. The\nproblem of learning mixture of MNL models from partial rankings naturally\narises in such applications. And the proposed methods can be used to learn MNL\nmodels from network data without the strong assumption that temporal orders of\nall the edge formation are available. We conduct experiments on both synthetic\nand real-world network data to demonstrate that the proposed methods achieve\nmore accurate parameter estimation and better fitness of data compared to\nconventional methods.",
          "link": "http://arxiv.org/abs/2112.15575",
          "publishedOn": "2022-01-03T07:15:18.121Z",
          "wordCount": null,
          "title": "Fast Learning of MNL Model from General Partial Rankings with Application to Network Formation Modeling. (arXiv:2112.15575v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mingqi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pun_M/0/1/0/all/0/1\">Man-on Pun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi Chen</a>",
          "description": "In this work, we develop practical user scheduling algorithms for downlink\nbursty traffic with emphasis on user fairness. In contrast to the conventional\nscheduling algorithms that either equally divides the transmission time slots\namong users or maximizing some ratios without physcial meanings, we propose to\nuse the 5%-tile user data rate (5TUDR) as the metric to evaluate user fairness.\nSince it is difficult to directly optimize 5TUDR, we first cast the problem\ninto the stochastic game framework and subsequently propose a Multi-Agent\nReinforcement Learning (MARL)-based algorithm to perform distributed\noptimization on the resource block group (RBG) allocation. Furthermore, each\nMARL agent is designed to take information measured by network counters from\nmultiple network layers (e.g. Channel Quality Indicator, Buffer size) as the\ninput states while the RBG allocation as action with a proposed reward function\ndesigned to maximize 5TUDR. Extensive simulation is performed to show that the\nproposed MARL-based scheduler can achieve fair scheduling while maintaining\ngood average network throughput as compared to conventional schedulers.",
          "link": "http://arxiv.org/abs/2012.15081",
          "publishedOn": "2022-01-03T07:15:18.109Z",
          "wordCount": null,
          "title": "Fairness-Oriented User Scheduling for Bursty Downlink Transmission Using Multi-Agent Reinforcement Learning. (arXiv:2012.15081v13 [cs.OS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.14843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yifei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vannella_F/0/1/0/all/0/1\">Filippo Vannella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouton_M/0/1/0/all/0/1\">Maxime Bouton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jaeseong Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakim_E/0/1/0/all/0/1\">Ezeddin Al Hakim</a>",
          "description": "6G will move mobile networks towards increasing levels of complexity. To deal\nwith this complexity, optimization of network parameters is key to ensure high\nperformance and timely adaptivity to dynamic network environments. The\noptimization of the antenna tilt provides a practical and cost-efficient method\nto improve coverage and capacity in the network. Previous methods based on\nReinforcement Learning (RL) have shown great promise for tilt optimization by\nlearning adaptive policies outperforming traditional tilt optimization methods.\nHowever, most existing RL methods are based on single-cell features\nrepresentation, which fails to fully characterize the agent state, resulting in\nsuboptimal performance. Also, most of such methods lack scalability, due to\nstate-action explosion, and generalization ability. In this paper, we propose a\nGraph Attention Q-learning (GAQ) algorithm for tilt optimization. GAQ relies on\na graph attention mechanism to select relevant neighbors information, improve\nthe agent state representation, and update the tilt control policy based on a\nhistory of observations using a Deep Q-Network (DQN). We show that GAQ\nefficiently captures important network information and outperforms standard DQN\nwith local information by a large margin. In addition, we demonstrate its\nability to generalize to network deployments of different sizes and densities.",
          "link": "http://arxiv.org/abs/2112.14843",
          "publishedOn": "2022-01-03T07:15:18.093Z",
          "wordCount": null,
          "title": "A Graph Attention Learning Approach to Antenna Tilt Optimization. (arXiv:2112.14843v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15538",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahsan_M/0/1/0/all/0/1\">Md Manjurul Ahsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddique_Z/0/1/0/all/0/1\">Zahed Siddique</a>",
          "description": "Globally, there is a substantial unmet need to diagnose various diseases\neffectively. The complexity of the different disease mechanisms and underlying\nsymptoms of the patient population presents massive challenges to developing\nthe early diagnosis tool and effective treatment. Machine Learning (ML), an\narea of Artificial Intelligence (AI), enables researchers, physicians, and\npatients to solve some of these issues. Based on relevant research, this review\nexplains how Machine Learning (ML) and Deep Learning (DL) are being used to\nhelp in the early identification of numerous diseases. To begin, a bibliometric\nstudy of the publication is given using data from the Scopus and Web of Science\n(WOS) databases. The bibliometric study of 1216 publications was undertaken to\ndetermine the most prolific authors, nations, organizations, and most cited\narticles. The review then summarizes the most recent trends and approaches in\nMachine Learning-based Disease Diagnosis (MLBDD), considering the following\nfactors: algorithm, disease types, data type, application, and evaluation\nmetrics. Finally, the paper highlights key results and provides insight into\nfuture trends and opportunities in the MLBDD area.",
          "link": "http://arxiv.org/abs/2112.15538",
          "publishedOn": "2022-01-03T07:15:18.093Z",
          "wordCount": null,
          "title": "Machine learning based disease diagnosis: A comprehensive review. (arXiv:2112.15538v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15516",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Shen_J/0/1/0/all/0/1\">Jianmin Shen</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Liu_F/0/1/0/all/0/1\">Feiyi Liu</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Chen_S/0/1/0/all/0/1\">Shiyang Chen</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Xu_D/0/1/0/all/0/1\">Dian Xu</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Chen_X/0/1/0/all/0/1\">Xiangna Chen</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Deng_S/0/1/0/all/0/1\">Shengfeng Deng</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Papp_G/0/1/0/all/0/1\">Gabor Papp</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Yang_C/0/1/0/all/0/1\">Chunbin Yang</a>",
          "description": "The latest advances of statistical physics have shown remarkable performance\nof machine learning in identifying phase transitions. In this paper, we apply\ndomain adversarial neural network (DANN) based on transfer learning to studying\nnon-equilibrium and equilibrium phase transition models, which are percolation\nmodel and directed percolation (DP) model, respectively. With the DANN, only a\nsmall fraction of input configurations (2d images) needs to be labeled, which\nis automatically chosen, in order to capture the critical point. To learn the\nDP model, the method is refined by an iterative procedure in determining the\ncritical point, which is a prerequisite for the data collapse in calculating\nthe critical exponent $\\nu_{\\perp}$. We then apply the DANN to a\ntwo-dimensional site percolation with configurations filtered to include only\nthe largest cluster which may contain the information related to the order\nparameter. The DANN learning of both models yields reliable results which are\ncomparable to the ones from Monte Carlo simulations. Our study also shows that\nthe DANN can achieve quite high accuracy at much lower cost, compared to the\nsupervised learning.",
          "link": "http://arxiv.org/abs/2112.15516",
          "publishedOn": "2022-01-03T07:15:18.090Z",
          "wordCount": null,
          "title": "Transfer learning of phase transitions in percolation and directed percolation. (arXiv:2112.15516v1 [cond-mat.stat-mech])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suarez_Varela_J/0/1/0/all/0/1\">Jos&#xe9; Su&#xe1;rez-Varela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almasan_P/0/1/0/all/0/1\">Paul Almasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferriol_Galmes_M/0/1/0/all/0/1\">Miquel Ferriol-Galm&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusek_K/0/1/0/all/0/1\">Krzysztof Rusek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geyer_F/0/1/0/all/0/1\">Fabien Geyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiangle Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shihan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarselli_F/0/1/0/all/0/1\">Franco Scarselli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabellos_Aparicio_A/0/1/0/all/0/1\">Albert Cabellos-Aparicio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barlet_Ros_P/0/1/0/all/0/1\">Pere Barlet-Ros</a>",
          "description": "Graph neural networks (GNN) have shown outstanding applications in many\nfields where data is fundamentally represented as graphs (e.g., chemistry,\nbiology, recommendation systems). In this vein, communication networks comprise\nmany fundamental components that are naturally represented in a\ngraph-structured manner (e.g., topology, configurations, traffic flows). This\nposition article presents GNNs as a fundamental tool for modeling, control and\nmanagement of communication networks. GNNs represent a new generation of\ndata-driven models that can accurately learn and reproduce the complex\nbehaviors behind real networks. As a result, such models can be applied to a\nwide variety of networking use cases, such as planning, online optimization, or\ntroubleshooting. The main advantage of GNNs over traditional neural networks\nlies in its unprecedented generalization capabilities when applied to other\nnetworks and configurations unseen during training, which is a critical feature\nfor achieving practical data-driven solutions for networking. This article\ncomprises a brief tutorial on GNNs and their possible applications to\ncommunication networks. To showcase the potential of this technology, we\npresent two use cases with state-of-the-art GNN models respectively applied to\nwired and wireless networks. Lastly, we delve into the key open challenges and\nopportunities yet to be explored in this novel research area.",
          "link": "http://arxiv.org/abs/2112.14792",
          "publishedOn": "2022-01-03T07:15:18.089Z",
          "wordCount": null,
          "title": "Graph Neural Networks for Communication Networks: Context, Use Cases and Opportunities. (arXiv:2112.14792v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sato_R/0/1/0/all/0/1\">Ryoma Sato</a>",
          "description": "Suppose we have a black-box function (e.g., deep neural network) that takes\nan image as input and outputs a value that indicates preference. How can we\nretrieve optimal images with respect to this function from an external database\non the Internet? Standard retrieval problems in the literature (e.g., item\nrecommendations) assume that an algorithm has full access to the set of items.\nIn other words, such algorithms are designed for service providers. In this\npaper, we consider the retrieval problem under different assumptions.\nSpecifically, we consider how users with limited access to an image database\ncan retrieve images using their own black-box functions. This formulation\nenables a flexible and finer-grained image search defined by each user. We\nassume the user can access the database through a search query with tight API\nlimits. Therefore, a user needs to efficiently retrieve optimal images in terms\nof the number of queries. We propose an efficient retrieval algorithm Tiara for\nthis problem. In the experiments, we confirm that our proposed method performs\nbetter than several baselines under various settings.",
          "link": "http://arxiv.org/abs/2112.14921",
          "publishedOn": "2022-01-03T07:15:18.089Z",
          "wordCount": null,
          "title": "Retrieving Black-box Optimal Images from External Databases. (arXiv:2112.14921v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/1911.12360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zixiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1\">Difan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "A recent line of research on deep learning focuses on the extremely\nover-parameterized setting, and shows that when the network width is larger\nthan a high degree polynomial of the training sample size $n$ and the inverse\nof the target error $\\epsilon^{-1}$, deep neural networks learned by\n(stochastic) gradient descent enjoy nice optimization and generalization\nguarantees. Very recently, it is shown that under certain margin assumptions on\nthe training data, a polylogarithmic width condition suffices for two-layer\nReLU networks to converge and generalize (Ji and Telgarsky, 2019). However,\nwhether deep neural networks can be learned with such a mild\nover-parameterization is still an open question. In this work, we answer this\nquestion affirmatively and establish sharper learning guarantees for deep ReLU\nnetworks trained by (stochastic) gradient descent. In specific, under certain\nassumptions made in previous work, our optimization and generalization\nguarantees hold with network width polylogarithmic in $n$ and $\\epsilon^{-1}$.\nOur results push the study of over-parameterized deep neural networks towards\nmore practical settings.",
          "link": "http://arxiv.org/abs/1911.12360",
          "publishedOn": "2022-01-03T07:15:18.086Z",
          "wordCount": null,
          "title": "How Much Over-parameterization Is Sufficient to Learn Deep ReLU Networks?. (arXiv:1911.12360v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08250",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Anderer_M/0/1/0/all/0/1\">Matthias Anderer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>",
          "description": "Hierarchical forecasting with intermittent time series is a challenge in both\nresearch and empirical studies. Extensive research focuses on improving the\naccuracy of each hierarchy, especially the intermittent time series at bottom\nlevels. Then hierarchical reconciliation could be used to improve the overall\nperformance further. In this paper, we present a\n\\emph{hierarchical-forecasting-with-alignment} approach that treats the bottom\nlevel forecasts as mutable to ensure higher forecasting accuracy on the upper\nlevels of the hierarchy. We employ a pure deep learning forecasting approach\nN-BEATS for continuous time series at the top levels and a widely used\ntree-based algorithm LightGBM for the intermittent time series at the bottom\nlevel. The \\emph{hierarchical-forecasting-with-alignment} approach is a simple\nyet effective variant of the bottom-up method, accounting for biases that are\ndifficult to observe at the bottom level. It allows suboptimal forecasts at the\nlower level to retain a higher overall performance. The approach in this\nempirical study was developed by the first author during the M5 Forecasting\nAccuracy competition, ranking second place. The method is also business\norientated and could benefit for business strategic planning.",
          "link": "http://arxiv.org/abs/2103.08250",
          "publishedOn": "2022-01-03T07:15:18.085Z",
          "wordCount": null,
          "title": "Hierarchical forecasting with a top-down alignment of independent level forecasts. (arXiv:2103.08250v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.02719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anvari_Z/0/1/0/all/0/1\">Zahra Anvari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athitsos_V/0/1/0/all/0/1\">Vassilis Athitsos</a>",
          "description": "Digitized documents such as scientific articles, tax forms, invoices,\ncontract papers, historic texts are widely used nowadays. These document images\ncould be degraded or damaged due to various reasons including poor lighting\nconditions, shadow, distortions like noise and blur, aging, ink stain,\nbleed-through, watermark, stamp, etc. Document image enhancement plays a\ncrucial role as a pre-processing step in many automated document analysis and\nrecognition tasks such as character recognition. With recent advances in deep\nlearning, many methods are proposed to enhance the quality of these document\nimages. In this paper, we review deep learning-based methods, datasets, and\nmetrics for six main document image enhancement tasks, including binarization,\ndebluring, denoising, defading, watermark removal, and shadow removal. We\nsummarize the recent works for each task and discuss their features,\nchallenges, and limitations. We introduce multiple document image enhancement\ntasks that have received little to no attention, including over and under\nexposure correction, super resolution, and bleed-through removal. We identify\nseveral promising research directions and opportunities for future research.",
          "link": "http://arxiv.org/abs/2112.02719",
          "publishedOn": "2022-01-03T07:15:18.085Z",
          "wordCount": null,
          "title": "A Survey on Deep learning based Document Image Enhancement. (arXiv:2112.02719v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1907.00457",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ling_S/0/1/0/all/0/1\">Shaoshi Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salazar_J/0/1/0/all/0/1\">Julian Salazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuzong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1\">Katrin Kirchhoff</a>",
          "description": "We introduce BERTphone, a Transformer encoder trained on large speech corpora\nthat outputs phonetically-aware contextual representation vectors that can be\nused for both speaker and language recognition. This is accomplished by\ntraining on two objectives: the first, inspired by adapting BERT to the\ncontinuous domain, involves masking spans of input frames and reconstructing\nthe whole sequence for acoustic representation learning; the second, inspired\nby the success of bottleneck features from ASR, is a sequence-level CTC loss\napplied to phoneme labels for phonetic representation learning. We pretrain two\nBERTphone models (one on Fisher and one on TED-LIUM) and use them as feature\nextractors into x-vector-style DNNs for both tasks. We attain a\nstate-of-the-art $C_{\\text{avg}}$ of 6.16 on the challenging LRE07 3sec\nclosed-set language recognition task. On Fisher and VoxCeleb speaker\nrecognition tasks, we see an 18% relative reduction in speaker EER when\ntraining on BERTphone vectors instead of MFCCs. In general, BERTphone\noutperforms previous phonetic pretraining approaches on the same data. We\nrelease our code and models at\nhttps://github.com/awslabs/speech-representations.",
          "link": "http://arxiv.org/abs/1907.00457",
          "publishedOn": "2022-01-03T07:15:18.084Z",
          "wordCount": null,
          "title": "BERTphone: Phonetically-Aware Encoder Representations for Utterance-Level Speaker and Language Recognition. (arXiv:1907.00457v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.14869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dixian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianbao Yang</a>",
          "description": "Multi-class classification is one of the most common tasks in machine\nlearning applications, where data is labeled by one of many class labels. Many\nloss functions have been proposed for multi-class classification including two\nwell-known ones, namely the cross-entropy (CE) loss and the crammer-singer (CS)\nloss (aka. the SVM loss). While CS loss has been used widely for traditional\nmachine learning tasks, CE loss is usually a default choice for multi-class\ndeep learning tasks. There are also top-$k$ variants of CS loss and CE loss\nthat are proposed to promote the learning of a classifier for achieving better\ntop-$k$ accuracy. Nevertheless, it still remains unclear the relationship\nbetween these different losses, which hinders our understanding of their\nexpectations in different scenarios. In this paper, we present a unified view\nof the CS/CE losses and their smoothed top-$k$ variants by proposing a new\nfamily of loss functions, which are arguably better than the CS/CE losses when\nthe given label information is incomplete and noisy. The new family of smooth\nloss functions named {label-distributionally robust (LDR) loss} is defined by\nleveraging the distributionally robust optimization (DRO) framework to model\nthe uncertainty in the given label information, where the uncertainty over true\nclass labels is captured by using distributional weights for each label\nregularized by a function.",
          "link": "http://arxiv.org/abs/2112.14869",
          "publishedOn": "2022-01-03T07:15:18.082Z",
          "wordCount": null,
          "title": "A Unified DRO View of Multi-class Loss Functions with top-N Consistency. (arXiv:2112.14869v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mengin_E/0/1/0/all/0/1\">Elie Mengin</a> (SAMM), <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1\">Fabrice Rossi</a> (CEREMADE)",
          "description": "In this paper, we present a novel algorithm to address the Network Alignment\nproblem. It is inspired from a previous message passing framework of Bayati et\nal. [2] and includes several modifications designed to significantly speed up\nthe message updates as well as to enforce their convergence. Experiments show\nthat our proposed model outperforms other state-of-the-art solvers. Finally, we\npropose an application of our method in order to address the Binary Diffing\nproblem. We show that our solution provides better assignment than the\nreference differs in almost all submitted instances and outline the importance\nof leveraging the graphical structure of binary programs.",
          "link": "http://arxiv.org/abs/2112.15336",
          "publishedOn": "2022-01-03T07:15:18.081Z",
          "wordCount": null,
          "title": "Improved Algorithm for the Network Alignment Problem with Application to Binary Diffing. (arXiv:2112.15336v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xi_D/0/1/0/all/0/1\">Dongbo Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Ganbin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiaohu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>",
          "description": "Domain adaptation tasks such as cross-domain sentiment classification aim to\nutilize existing labeled data in the source domain and unlabeled or few labeled\ndata in the target domain to improve the performance in the target domain via\nreducing the shift between the data distributions. Existing cross-domain\nsentiment classification methods need to distinguish pivots, i.e., the\ndomain-shared sentiment words, and non-pivots, i.e., the domain-specific\nsentiment words, for excellent adaptation performance. In this paper, we first\ndesign a Category Attention Network (CAN), and then propose a model named\nCAN-CNN to integrate CAN and a Convolutional Neural Network (CNN). On the one\nhand, the model regards pivots and non-pivots as unified category attribute\nwords and can automatically capture them to improve the domain adaptation\nperformance; on the other hand, the model makes an attempt at interpretability\nto learn the transferred category attribute words. Specifically, the\noptimization objective of our model has three different components: 1) the\nsupervised classification loss; 2) the distributions loss of category feature\nweights; 3) the domain invariance loss. Finally, the proposed model is\nevaluated on three public sentiment analysis datasets and the results\ndemonstrate that CAN-CNN can outperform other various baseline methods.",
          "link": "http://arxiv.org/abs/2112.15290",
          "publishedOn": "2022-01-03T07:15:18.076Z",
          "wordCount": null,
          "title": "Domain Adaptation with Category Attention Network for Deep Sentiment Analysis. (arXiv:2112.15290v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2110.06394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weitong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We study the model-based reward-free reinforcement learning with linear\nfunction approximation for episodic Markov decision processes (MDPs). In this\nsetting, the agent works in two phases. In the exploration phase, the agent\ninteracts with the environment and collects samples without the reward. In the\nplanning phase, the agent is given a specific reward function and uses samples\ncollected from the exploration phase to learn a good policy. We propose a new\nprovably efficient algorithm, called UCRL-RFE under the Linear Mixture MDP\nassumption, where the transition probability kernel of the MDP can be\nparameterized by a linear function over certain feature mappings defined on the\ntriplet of state, action, and next state. We show that to obtain an\n$\\epsilon$-optimal policy for arbitrary reward function, UCRL-RFE needs to\nsample at most $\\tilde{\\mathcal{O}}(H^5d^2\\epsilon^{-2})$ episodes during the\nexploration phase. Here, $H$ is the length of the episode, $d$ is the dimension\nof the feature mapping. We also propose a variant of UCRL-RFE using\nBernstein-type bonus and show that it needs to sample at most\n$\\tilde{\\mathcal{O}}(H^4d(H + d)\\epsilon^{-2})$ to achieve an\n$\\epsilon$-optimal policy. By constructing a special class of linear Mixture\nMDPs, we also prove that for any reward-free algorithm, it needs to sample at\nleast $\\tilde \\Omega(H^2d\\epsilon^{-2})$ episodes to obtain an\n$\\epsilon$-optimal policy. Our upper bound matches the lower bound in terms of\nthe dependence on $\\epsilon$ and the dependence on $d$ if $H \\ge d$.",
          "link": "http://arxiv.org/abs/2110.06394",
          "publishedOn": "2022-01-03T07:15:18.071Z",
          "wordCount": null,
          "title": "Reward-Free Model-Based Reinforcement Learning with Linear Function Approximation. (arXiv:2110.06394v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.09344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yongyi Mao</a>",
          "description": "We present a simple and yet effective interpolation-based regularization\ntechnique to improve the generalization of Graph Neural Networks (GNNs). Our\nmethod leverages the recent advances in Mixup regularizer for vision and text,\nwhere random sample pairs and their labels are interpolated to create synthetic\nsamples for training. Unlike images or natural sentences, graphs have arbitrary\nstructure and topology, and even simply deleting or adding one edge from a\ngraph can dramatically change its semantic meanings. This makes interpolating\ngraph inputs very challenging because mixing graph pairs may naturally create\ngraphs with identical structure but with conflict labels, causing the manifold\nintrusion issue. To cope with this obstacle, we propose a simple input mixing\nschema for Mixup on graph, coined ifMixup. We theoretically prove that, with a\nmild assumption, ifMixup guarantees that the mixed graphs are manifold\nintrusion free. We also empirically verify that ifMixup can effectively\nregularize the graph classification learning, resulting in superior predictive\naccuracy over popular graph augmentation baselines.",
          "link": "http://arxiv.org/abs/2110.09344",
          "publishedOn": "2022-01-03T07:15:18.069Z",
          "wordCount": null,
          "title": "ifMixup: Towards Intrusion-Free Graph Mixup for Graph Classification. (arXiv:2110.09344v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sommana_B/0/1/0/all/0/1\">Benjaphan Sommana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watchareeruetai_U/0/1/0/all/0/1\">Ukrit Watchareeruetai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1\">Ankush Ganguly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Earp_S/0/1/0/all/0/1\">Samuel W.F. Earp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitiyakara_T/0/1/0/all/0/1\">Taya Kitiyakara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boonmanunt_S/0/1/0/all/0/1\">Suparee Boonmanunt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thammasudjarit_R/0/1/0/all/0/1\">Ratchainant Thammasudjarit</a>",
          "description": "During the SARS-Cov-2 pandemic, mask-wearing became an effective tool to\nprevent spreading and contracting the virus. The ability to monitor the\nmask-wearing rate in the population would be useful for determining public\nhealth strategies against the virus. However, artificial intelligence\ntechnologies for detecting face masks have not been deployed at a large scale\nin real-life to measure the mask-wearing rate in public. In this paper, we\npresent a two-step face mask detection approach consisting of two separate\nmodules: 1) face detection and alignment and 2) face mask classification. This\napproach allowed us to experiment with different combinations of face detection\nand face mask classification modules. More specifically, we experimented with\nPyramidKey and RetinaFace as face detectors while maintaining a lightweight\nbackbone for the face mask classification module. Moreover, we also provide a\nrelabeled annotation of the test set of the AIZOO dataset, where we rectified\nthe incorrect labels for some face images. The evaluation results on the AIZOO\nand Moxa 3K datasets showed that the proposed face mask detection pipeline\nsurpassed the state-of-the-art methods. The proposed pipeline also yielded a\nhigher mAP on the relabeled test set of the AIZOO dataset than the original\ntest set. Since we trained the proposed model using in-the-wild face images, we\ncan successfully deploy our model to monitor the mask-wearing rate using public\nCCTV images.",
          "link": "http://arxiv.org/abs/2112.15031",
          "publishedOn": "2022-01-03T07:15:18.064Z",
          "wordCount": null,
          "title": "Development of a face mask detection pipeline for mask-wearing monitoring in the era of the COVID-19 pandemic: A modular approach. (arXiv:2112.15031v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.05170",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hong_M/0/1/0/all/0/1\">Mingyi Hong</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wai_H/0/1/0/all/0/1\">Hoi-To Wai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoran Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoran Yang</a>",
          "description": "This paper analyzes a two-timescale stochastic algorithm framework for\nbilevel optimization. Bilevel optimization is a class of problems which exhibit\na two-level structure, and its goal is to minimize an outer objective function\nwith variables which are constrained to be the optimal solution to an (inner)\noptimization problem. We consider the case when the inner problem is\nunconstrained and strongly convex, while the outer problem is constrained and\nhas a smooth objective function. We propose a two-timescale stochastic\napproximation (TTSA) algorithm for tackling such a bilevel problem. In the\nalgorithm, a stochastic gradient update with a larger step size is used for the\ninner problem, while a projected stochastic gradient update with a smaller step\nsize is used for the outer problem. We analyze the convergence rates for the\nTTSA algorithm under various settings: when the outer problem is strongly\nconvex (resp.~weakly convex), the TTSA algorithm finds an\n$\\mathcal{O}(K^{-2/3})$-optimal (resp.~$\\mathcal{O}(K^{-2/5})$-stationary)\nsolution, where $K$ is the total iteration number. As an application, we show\nthat a two-timescale natural actor-critic proximal policy optimization\nalgorithm can be viewed as a special case of our TTSA framework. Importantly,\nthe natural actor-critic algorithm is shown to converge at a rate of\n$\\mathcal{O}(K^{-1/4})$ in terms of the gap in expected discounted reward\ncompared to a global optimal policy.",
          "link": "http://arxiv.org/abs/2007.05170",
          "publishedOn": "2022-01-03T07:15:18.062Z",
          "wordCount": null,
          "title": "A Two-Timescale Framework for Bilevel Optimization: Complexity Analysis and Application to Actor-Critic. (arXiv:2007.05170v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hodan_T/0/1/0/all/0/1\">Tomas Hodan</a>",
          "description": "In this thesis, we address the problem of estimating the 6D pose of rigid\nobjects from a single RGB or RGB-D input image, assuming that 3D models of the\nobjects are available. This problem is of great importance to many application\nfields such as robotic manipulation, augmented reality, and autonomous driving.\nFirst, we propose EPOS, a method for 6D object pose estimation from an RGB\nimage. The key idea is to represent an object by compact surface fragments and\npredict the probability distribution of corresponding fragments at each pixel\nof the input image by a neural network. Each pixel is linked with a\ndata-dependent number of fragments, which allows systematic handling of\nsymmetries, and the 6D poses are estimated from the links by a RANSAC-based\nfitting method. EPOS outperformed all RGB and most RGB-D and D methods on\nseveral standard datasets. Second, we present HashMatch, an RGB-D method that\nslides a window over the input image and searches for a match against\ntemplates, which are pre-generated by rendering 3D object models in different\norientations. The method applies a cascade of evaluation stages to each window\nlocation, which avoids exhaustive matching against all templates. Third, we\npropose ObjectSynth, an approach to synthesize photorealistic images of 3D\nobject models for training methods based on neural networks. The images yield\nsubstantial improvements compared to commonly used images of objects rendered\non top of random photographs. Fourth, we introduce T-LESS, the first dataset\nfor 6D object pose estimation that includes 3D models and RGB-D images of\nindustry-relevant objects. Fifth, we define BOP, a benchmark that captures the\nstatus quo in the field. BOP comprises eleven datasets in a unified format, an\nevaluation methodology, an online evaluation system, and public challenges held\nat international workshops organized at the ICCV and ECCV conferences.",
          "link": "http://arxiv.org/abs/2112.15075",
          "publishedOn": "2022-01-03T07:15:18.036Z",
          "wordCount": null,
          "title": "Pose Estimation of Specific Rigid Objects. (arXiv:2112.15075v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Sungmin Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_N/0/1/0/all/0/1\">Naeun Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Youngjoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>",
          "description": "We propose a novel and effective purification based adversarial defense\nmethod against pre-processor blind white- and black-box attacks. Our method is\ncomputationally efficient and trained only with self-supervised learning on\ngeneral images, without requiring any adversarial training or retraining of the\nclassification model. We first show an empirical analysis on the adversarial\nnoise, defined to be the residual between an original image and its adversarial\nexample, has almost zero mean, symmetric distribution. Based on this\nobservation, we propose a very simple iterative Gaussian Smoothing (GS) which\ncan effectively smooth out adversarial noise and achieve substantially high\nrobust accuracy. To further improve it, we propose Neural Contextual Iterative\nSmoothing (NCIS), which trains a blind-spot network (BSN) in a self-supervised\nmanner to reconstruct the discriminative features of the original image that is\nalso smoothed out by GS. From our extensive experiments on the large-scale\nImageNet using four classification models, we show that our method achieves\nboth competitive standard accuracy and state-of-the-art robust accuracy against\nmost strong purifier-blind white- and black-box attacks. Also, we propose a new\nbenchmark for evaluating a purification method based on commercial image\nclassification APIs, such as AWS, Azure, Clarifai and Google. We generate\nadversarial examples by ensemble transfer-based black-box attack, which can\ninduce complete misclassification of APIs, and demonstrate that our method can\nbe used to increase adversarial robustness of APIs.",
          "link": "http://arxiv.org/abs/2106.11644",
          "publishedOn": "2022-01-03T07:15:18.032Z",
          "wordCount": null,
          "title": "NCIS: Neural Contextual Iterative Smoothing for Purifying Adversarial Perturbations. (arXiv:2106.11644v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.14840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shicheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaosen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_F/0/1/0/all/0/1\">Fangcheng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wentao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wen Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1\">Yangyu Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bin Cui</a>",
          "description": "K-core decomposition is a commonly used metric to analyze graph structure or\nstudy the relative importance of nodes in complex graphs. Recent years have\nseen rapid growth in the scale of the graph, especially in industrial settings.\nFor example, our industrial partner runs popular social applications with\nbillions of users and is able to gather a rich set of user data. As a result,\napplying K-core decomposition on large graphs has attracted more and more\nattention from academics and the industry. A simple but effective method to\ndeal with large graphs is to train them in the distributed settings, and some\ndistributed K-core decomposition algorithms are also proposed. Despite their\neffectiveness, we experimentally and theoretically observe that these\nalgorithms consume too many resources and become unstable on super-large-scale\ngraphs, especially when the given resources are limited. In this paper, we deal\nwith those super-large-scale graphs and propose a divide-and-conquer strategy\non top of the distributed K-core decomposition algorithm. We evaluate our\napproach on three large graphs. The experimental results show that the\nconsumption of resources can be significantly reduced, and the calculation on\nlarge-scale graphs becomes more stable than the existing methods. For example,\nthe distributed K-core decomposition algorithm can scale to a large graph with\n136 billion edges without losing correctness with our divide-and-conquer\ntechnique.",
          "link": "http://arxiv.org/abs/2112.14840",
          "publishedOn": "2022-01-03T07:15:18.024Z",
          "wordCount": null,
          "title": "K-Core Decomposition on Super Large Graphs with Limited Resources. (arXiv:2112.14840v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Sunny Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Roman Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Newman Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kevin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Leonard Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_E/0/1/0/all/0/1\">Elizabeth Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Nikhil Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patti_T/0/1/0/all/0/1\">Taylor L. Patti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lynch_J/0/1/0/all/0/1\">Jayson Lynch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shporer_A/0/1/0/all/0/1\">Avi Shporer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_N/0/1/0/all/0/1\">Nakul Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_E/0/1/0/all/0/1\">Eugene Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strang_G/0/1/0/all/0/1\">Gilbert Strang</a>",
          "description": "We demonstrate that a neural network pre-trained on text and fine-tuned on\ncode solves Mathematics problems by program synthesis. We turn questions into\nprogramming tasks, automatically generate programs, and then execute them,\nperfectly solving university-level problems from MIT's large Mathematics\ncourses (Single Variable Calculus 18.01, Multivariable Calculus 18.02,\nDifferential Equations 18.03, Introduction to Probability and Statistics 18.05,\nLinear Algebra 18.06, and Mathematics for Computer Science 6.042) as well as\nquestions from a MATH dataset (on Prealgebra, Algebra, Counting and\nProbability, Number Theory, and Precalculus), the latest benchmark of advanced\nmathematics problems specifically designed to assess mathematical reasoning. We\nexplore prompt generation methods that enable Transformers to generate question\nsolving programs for these subjects, including solutions with plots. We\ngenerate correct answers for a random sample of questions in each topic. We\nquantify the gap between the original and transformed questions and perform a\nsurvey to evaluate the quality and difficulty of generated questions. This is\nthe first work to automatically solve, grade, and generate university-level\nMathematics course questions at scale which represents a milestone for higher\neducation.",
          "link": "http://arxiv.org/abs/2112.15594",
          "publishedOn": "2022-01-03T07:15:18.024Z",
          "wordCount": null,
          "title": "A Neural Network Solves and Generates Mathematics Problems by Program Synthesis: Calculus, Differential Equations, Linear Algebra, and More. (arXiv:2112.15594v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chharia_A/0/1/0/all/0/1\">Aviral Chharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_N/0/1/0/all/0/1\">Nishi Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shivam Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prajapati_S/0/1/0/all/0/1\">Shivam Prajapati</a>",
          "description": "The rise of computation-based methods in thermal management has gained\nimmense attention in recent years due to the ability of deep learning to solve\ncomplex 'physics' problems, which are otherwise difficult to be approached\nusing conventional techniques. Thermal management is required in electronic\nsystems to keep them from overheating and burning, enhancing their efficiency\nand lifespan. For a long time, numerical techniques have been employed to aid\nin the thermal management of electronics. However, they come with some\nlimitations. To increase the effectiveness of traditional numerical approaches\nand address the drawbacks faced in conventional approaches, researchers have\nlooked at using artificial intelligence at various stages of the thermal\nmanagement process. The present study discusses in detail, the current uses of\ndeep learning in the domain of 'electronic' thermal management.",
          "link": "http://arxiv.org/abs/2112.14837",
          "publishedOn": "2022-01-03T07:15:18.018Z",
          "wordCount": null,
          "title": "Recent Trends in Artificial Intelligence-inspired Electronic Thermal Management. (arXiv:2112.14837v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2004.00184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Besserve_M/0/1/0/all/0/1\">Michel Besserve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">R&#xe9;my Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janzing_D/0/1/0/all/0/1\">Dominik Janzing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "Generative models can be trained to emulate complex empirical data, but are\nthey useful to make predictions in the context of previously unobserved\nenvironments? An intuitive idea to promote such extrapolation capabilities is\nto have the architecture of such model reflect a causal graph of the true data\ngenerating process, such that one can intervene on each node independently of\nthe others. However, the nodes of this graph are usually unobserved, leading to\noverparameterization and lack of identifiability of the causal structure. We\ndevelop a theoretical framework to address this challenging situation by\ndefining a weaker form of identifiability, based on the principle of\nindependence of mechanisms. We demonstrate on toy examples that classical\nstochastic gradient descent can hinder the model's extrapolation capabilities,\nsuggesting independence of mechanisms should be enforced explicitly during\ntraining. Experiments on deep generative models trained on real world data\nsupport these insights and illustrate how the extrapolation capabilities of\nsuch models can be leveraged.",
          "link": "http://arxiv.org/abs/2004.00184",
          "publishedOn": "2022-01-03T07:15:18.018Z",
          "wordCount": null,
          "title": "A theory of independent mechanisms for extrapolation in generative models. (arXiv:2004.00184v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eysenbach_B/0/1/0/all/0/1\">Benjamin Eysenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>",
          "description": "Reinforcement learning (RL) algorithms assume that users specify tasks by\nmanually writing down a reward function. However, this process can be laborious\nand demands considerable technical expertise. Can we devise RL algorithms that\ninstead enable users to specify tasks simply by providing examples of\nsuccessful outcomes? In this paper, we derive a control algorithm that\nmaximizes the future probability of these successful outcome examples. Prior\nwork has approached similar problems with a two-stage process, first learning a\nreward function and then optimizing this reward function using another RL\nalgorithm. In contrast, our method directly learns a value function from\ntransitions and successful outcomes, without learning this intermediate reward\nfunction. Our method therefore requires fewer hyperparameters to tune and lines\nof code to debug. We show that our method satisfies a new data-driven Bellman\nequation, where examples take the place of the typical reward function term.\nExperiments show that our approach outperforms prior methods that learn\nexplicit reward functions.",
          "link": "http://arxiv.org/abs/2103.12656",
          "publishedOn": "2022-01-03T07:15:18.017Z",
          "wordCount": null,
          "title": "Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification. (arXiv:2103.12656v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15545",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Irie_K/0/1/0/all/0/1\">Kazuki Irie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1\">J&#xfc;rgen Schmidhuber</a>",
          "description": "The inputs and/or outputs of some neural nets are weight matrices of other\nneural nets. Indirect encodings or end-to-end compression of weight matrices\ncould help to scale such approaches. Our goal is to open a discussion on this\ntopic, starting with recurrent neural networks for character-level language\nmodelling whose weight matrices are encoded by the discrete cosine transform.\nOur fast weight version thereof uses a recurrent neural network to parameterise\nthe compressed weights. We present experimental results on the enwik8 dataset.",
          "link": "http://arxiv.org/abs/2112.15545",
          "publishedOn": "2022-01-03T07:15:18.016Z",
          "wordCount": null,
          "title": "Training and Generating Neural Networks in Compressed Weight Space. (arXiv:2112.15545v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.15010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhehao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Q/0/1/0/all/0/1\">Qinghua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>",
          "description": "Deep Neural Networks (DNNs) are acknowledged as vulnerable to adversarial\nattacks, while the existing black-box attacks require extensive queries on the\nvictim DNN to achieve high success rates. For query-efficiency, surrogate\nmodels of the victim are used to generate transferable Adversarial Examples\n(AEs) because of their Gradient Similarity (GS), i.e., surrogates' attack\ngradients are similar to the victim's ones. However, it is generally neglected\nto exploit their similarity on outputs, namely the Prediction Similarity (PS),\nto filter out inefficient queries by surrogates without querying the victim. To\njointly utilize and also optimize surrogates' GS and PS, we develop QueryNet, a\nunified attack framework that can significantly reduce queries. QueryNet\ncreatively attacks by multi-identity surrogates, i.e., crafts several AEs for\none sample by different surrogates, and also uses surrogates to decide on the\nmost promising AE for the query. After that, the victim's query feedback is\naccumulated to optimize not only surrogates' parameters but also their\narchitectures, enhancing both the GS and the PS. Although QueryNet has no\naccess to pre-trained surrogates' prior, it reduces queries by averagely about\nan order of magnitude compared to alternatives within an acceptable time,\naccording to our comprehensive experiments: 11 victims (including two\ncommercial models) on MNIST/CIFAR10/ImageNet, allowing only 8-bit image\nqueries, and no access to the victim's training data. The code is available at\nhttps://github.com/AllenChen1998/QueryNet.",
          "link": "http://arxiv.org/abs/2105.15010",
          "publishedOn": "2022-01-03T07:15:18.011Z",
          "wordCount": null,
          "title": "QueryNet: Attack by Multi-Identity Surrogates. (arXiv:2105.15010v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.14900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuexin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Ruichu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zijian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1\">Zhifeng Hao</a>",
          "description": "Graphs can model complicated interactions between entities, which naturally\nemerge in many important applications. These applications can often be cast\ninto standard graph learning tasks, in which a crucial step is to learn\nlow-dimensional graph representations. Graph neural networks (GNNs) are\ncurrently the most popular model in graph embedding approaches. However,\nstandard GNNs in the neighborhood aggregation paradigm suffer from limited\ndiscriminative power in distinguishing \\emph{high-order} graph structures as\nopposed to \\emph{low-order} structures. To capture high-order structures,\nresearchers have resorted to motifs and developed motif-based GNNs. However,\nexisting motif-based GNNs still often suffer from less discriminative power on\nhigh-order structures. To overcome the above limitations, we propose Motif\nGraph Neural Network (MGNN), a novel framework to better capture high-order\nstructures, hinging on our proposed motif redundancy minimization operator and\ninjective motif combination. First, MGNN produces a set of node representations\nw.r.t. each motif. The next phase is our proposed redundancy minimization among\nmotifs which compares the motifs with each other and distills the features\nunique to each motif. Finally, MGNN performs the updating of node\nrepresentations by combining multiple representations from different motifs. In\nparticular, to enhance the discriminative power, MGNN utilizes an injective\nfunction to combine the representations w.r.t. different motifs. We further\nshow that our proposed architecture increases the expressive power of GNNs with\na theoretical analysis. We demonstrate that MGNN outperforms state-of-the-art\nmethods on seven public benchmarks on both node classification and graph\nclassification tasks.",
          "link": "http://arxiv.org/abs/2112.14900",
          "publishedOn": "2022-01-03T07:15:18.004Z",
          "wordCount": null,
          "title": "Motif Graph Neural Network. (arXiv:2112.14900v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arnob_S/0/1/0/all/0/1\">Samin Yeasar Arnob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_R/0/1/0/all/0/1\">Riashat Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>",
          "description": "We hypothesize that empirically studying the sample complexity of offline\nreinforcement learning (RL) is crucial for the practical applications of RL in\nthe real world. Several recent works have demonstrated the ability to learn\npolicies directly from offline data. In this work, we ask the question of the\ndependency on the number of samples for learning from offline data. Our\nobjective is to emphasize that studying sample complexity for offline RL is\nimportant, and is an indicator of the usefulness of existing offline\nalgorithms. We propose an evaluation approach for sample complexity analysis of\noffline RL.",
          "link": "http://arxiv.org/abs/2112.15578",
          "publishedOn": "2022-01-03T07:15:18.001Z",
          "wordCount": null,
          "title": "Importance of Empirical Sample Complexity Analysis for Offline Reinforcement Learning. (arXiv:2112.15578v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joseph_R/0/1/0/all/0/1\">Rinu Joseph</a>",
          "description": "Branch prediction is an architectural feature that speeds up the execution of\nbranch instruction on pipeline processors and reduces the cost of branching.\nRecent advancements of Deep Learning (DL) in the post Moore's Law era is\naccelerating areas of automated chip design, low-power computer architectures,\nand much more. Traditional computer architecture design and algorithms could\nbenefit from dynamic predictors based on deep learning algorithms which learns\nfrom experience by optimizing its parameters on large number of data. In this\nsurvey paper, we focus on traditional branch prediction algorithms, analyzes\nits limitations, and presents a literature survey of how deep learning\ntechniques can be applied to create dynamic branch predictors capable of\npredicting conditional branch instructions. Prior surveys in this field focus\non dynamic branch prediction techniques based on neural network perceptrons. We\nplan to improve the survey based on latest research in DL and advanced Machine\nLearning (ML) based branch predictors.",
          "link": "http://arxiv.org/abs/2112.14911",
          "publishedOn": "2022-01-03T07:15:18.000Z",
          "wordCount": null,
          "title": "A Survey of Deep Learning Techniques for Dynamic Branch Prediction. (arXiv:2112.14911v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2112.12134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qing-xin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian-wei Liu</a>",
          "description": "We present a unified analysis method that relies on the generalized cosine\nrule and $\\phi$-convex for online optimization in normed vector space using\ndynamic regret as the performance metric. In combing the update rules, we start\nwith strategy $S$ (a two-parameter variant strategy covering Optimistic-FTRL\nwith surrogate linearized losses), and obtain $S$-I (type-I relaxation variant\nform of $S$) and $S$-II (type-II relaxation variant form of $S$, which is\nOptimistic-MD) by relaxation. Regret bounds for $S$-I and $S$-II are the\ntightest possible. As instantiations, regret bounds of normalized exponentiated\nsubgradient and greedy/lazy projection are better than the currently known\noptimal results. By replacing losses of online game with monotone operators,\nand extending the definition of regret, namely regret$^n$, we extend online\nconvex optimization to online monotone optimization, which expands the\napplication scope of $S$-I and $S$-II.",
          "link": "http://arxiv.org/abs/2112.12134",
          "publishedOn": "2022-01-03T07:15:17.998Z",
          "wordCount": null,
          "title": "A Unified Analysis Method for Online Optimization in Normed Vector Space. (arXiv:2112.12134v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Casacuberta_S/0/1/0/all/0/1\">S&#xed;lvia Casacuberta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suel_E/0/1/0/all/0/1\">Esra Suel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flaxman_S/0/1/0/all/0/1\">Seth Flaxman</a>",
          "description": "In this paper we introduce a new problem within the growing literature of\ninterpretability for convolution neural networks (CNNs). While previous work\nhas focused on the question of how to visually interpret CNNs, we ask what it\nis that we care to interpret, that is, which layers and neurons are worth our\nattention? Due to the vast size of modern deep learning network architectures,\nautomated, quantitative methods are needed to rank the relative importance of\nneurons so as to provide an answer to this question. We present a new\nstatistical method for ranking the hidden neurons in any convolutional layer of\na network. We define importance as the maximal correlation between the\nactivation maps and the class score. We provide different ways in which this\nmethod can be used for visualization purposes with MNIST and ImageNet, and show\na real-world application of our method to air pollution prediction with\nstreet-level images.",
          "link": "http://arxiv.org/abs/2112.15571",
          "publishedOn": "2022-01-03T07:15:17.995Z",
          "wordCount": null,
          "title": "PCACE: A Statistical Approach to Ranking Neurons for CNN Interpretability. (arXiv:2112.15571v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiafei Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Samson Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hui Li Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongyuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Cheston Tan</a>",
          "description": "There has been an emerging paradigm shift from the era of \"internet AI\" to\n\"embodied AI\", where AI algorithms and agents no longer learn from datasets of\nimages, videos or text curated primarily from the internet. Instead, they learn\nthrough interactions with their environments from an egocentric perception\nsimilar to humans. Consequently, there has been substantial growth in the\ndemand for embodied AI simulators to support various embodied AI research\ntasks. This growing interest in embodied AI is beneficial to the greater\npursuit of Artificial General Intelligence (AGI), but there has not been a\ncontemporary and comprehensive survey of this field. This paper aims to provide\nan encyclopedic survey for the field of embodied AI, from its simulators to its\nresearch. By evaluating nine current embodied AI simulators with our proposed\nseven features, this paper aims to understand the simulators in their provision\nfor use in embodied AI research and their limitations. Lastly, this paper\nsurveys the three main research tasks in embodied AI -- visual exploration,\nvisual navigation and embodied question answering (QA), covering the\nstate-of-the-art approaches, evaluation metrics and datasets. Finally, with the\nnew insights revealed through surveying the field, the paper will provide\nsuggestions for simulator-for-task selections and recommendations for the\nfuture directions of the field.",
          "link": "http://arxiv.org/abs/2103.04918",
          "publishedOn": "2022-01-03T07:15:17.995Z",
          "wordCount": null,
          "title": "A Survey of Embodied AI: From Simulators to Research Tasks. (arXiv:2103.04918v6 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2111.13208",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mayor_Torres_J/0/1/0/all/0/1\">Juan Manuel Mayor-Torres</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Medina_DeVilliers_S/0/1/0/all/0/1\">Sara Medina-DeVilliers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Clarkson_T/0/1/0/all/0/1\">Tessa Clarkson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lerner_M/0/1/0/all/0/1\">Matthew D. Lerner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riccardi_G/0/1/0/all/0/1\">Giuseppe Riccardi</a>",
          "description": "Current models on Explainable Artificial Intelligence (XAI) have shown an\nevident and quantified lack of reliability for measuring feature-relevance when\nstatistically entangled features are proposed for training deep classifiers.\nThere has been an increase in the application of Deep Learning in clinical\ntrials to predict early diagnosis of neuro-developmental disorders, such as\nAutism Spectrum Disorder (ASD). However, the inclusion of more reliable\nsaliency-maps to obtain more trustworthy and interpretable metrics using neural\nactivity features is still insufficiently mature for practical applications in\ndiagnostics or clinical trials. Moreover, in ASD research the inclusion of deep\nclassifiers that use neural measures to predict viewed facial emotions is\nrelatively unexplored. Therefore, in this study we propose the evaluation of a\nConvolutional Neural Network (CNN) for electroencephalography (EEG)-based\nfacial emotion recognition decoding complemented with a novel\nRemOve-And-Retrain (ROAR) methodology to recover highly relevant features used\nin the classifier. Specifically, we compare well-known relevance maps such as\nLayer-Wise Relevance Propagation (LRP), PatternNet, Pattern-Attribution, and\nSmooth-Grad Squared. This study is the first to consolidate a more transparent\nfeature-relevance calculation for a successful EEG-based facial emotion\nrecognition using a within-subject-trained CNN in typically-developed and ASD\nindividuals.",
          "link": "http://arxiv.org/abs/2111.13208",
          "publishedOn": "2022-01-03T07:15:17.995Z",
          "wordCount": null,
          "title": "Evaluation of Interpretability for Deep Learning algorithms in EEG Emotion Recognition: A case study in Autism. (arXiv:2111.13208v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15382",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Polyakov_S/0/1/0/all/0/1\">Stanislav Polyakov</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Demichev_A/0/1/0/all/0/1\">Andrey Demichev</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kryukov_A/0/1/0/all/0/1\">Alexander Kryukov</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Postnikov_E/0/1/0/all/0/1\">Evgeny Postnikov</a>",
          "description": "Extensive air showers created by high-energy particles interacting with the\nEarth atmosphere can be detected using imaging atmospheric Cherenkov telescopes\n(IACTs). The IACT images can be analyzed to distinguish between the events\ncaused by gamma rays and by hadrons and to infer the parameters of the event\nsuch as the energy of the primary particle. We use convolutional neural\nnetworks (CNNs) to analyze Monte Carlo-simulated images from the telescopes of\nthe TAIGA experiment. The analysis includes selection of the images\ncorresponding to the showers caused by gamma rays and estimating the energy of\nthe gamma rays. We compare performance of the CNNs using images from a single\ntelescope and the CNNs using images from two telescopes as inputs.",
          "link": "http://arxiv.org/abs/2112.15382",
          "publishedOn": "2022-01-03T07:15:17.991Z",
          "wordCount": null,
          "title": "Processing Images from Multiple IACTs in the TAIGA Experiment with Convolutional Neural Networks. (arXiv:2112.15382v1 [astro-ph.IM])"
        },
        {
          "id": "http://arxiv.org/abs/2103.10415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Huihan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinyuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xisen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>",
          "description": "Pre-trained language models have been successful on text classification\ntasks, but are prone to learning spurious correlations from biased datasets,\nand are thus vulnerable when making inferences in a new domain. Prior work\nreveals such spurious patterns via post-hoc explanation algorithms which\ncompute the importance of input features. Further, the model is regularized to\nalign the importance scores with human knowledge, so that the unintended model\nbehaviors are eliminated. However, such a regularization technique lacks\nflexibility and coverage, since only importance scores towards a pre-defined\nlist of features are adjusted, while more complex human knowledge such as\nfeature interaction and pattern generalization can hardly be incorporated. In\nthis work, we propose to refine a learned language model for a target domain by\ncollecting human-provided compositional explanations regarding observed biases.\nBy parsing these explanations into executable logic rules, the human-specified\nrefinement advice from a small set of explanations can be generalized to more\ntraining examples. We additionally introduce a regularization term allowing\nadjustments for both importance and interaction of features to better rectify\nmodel behavior. We demonstrate the effectiveness of the proposed approach on\ntwo text classification tasks by showing improved performance in target domain\nas well as improved model fairness after refinement.",
          "link": "http://arxiv.org/abs/2103.10415",
          "publishedOn": "2022-01-03T07:15:17.991Z",
          "wordCount": null,
          "title": "Refining Language Models with Compositional Explanations. (arXiv:2103.10415v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassanaly_M/0/1/0/all/0/1\">Malik Hassanaly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perry_B/0/1/0/all/0/1\">Bruce A. Perry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_M/0/1/0/all/0/1\">Michael E. Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yellapantula_S/0/1/0/all/0/1\">Shashank Yellapantula</a>",
          "description": "Improvements in computational and experimental capabilities are rapidly\nincreasing the amount of scientific data that is routinely generated. In\napplications that are constrained by memory and computational intensity,\nexcessively large datasets may hinder scientific discovery, making data\nreduction a critical component of data-driven methods. Datasets are growing in\ntwo directions: the number of data points and their dimensionality. Whereas\ndata compression techniques are concerned with reducing dimensionality, the\nfocus here is on reducing the number of data points. A strategy is proposed to\nselect data points such that they uniformly span the phase-space of the data.\nThe algorithm proposed relies on estimating the probability map of the data and\nusing it to construct an acceptance probability. An iterative method is used to\naccurately estimate the probability of the rare data points when only a small\nsubset of the dataset is used to construct the probability map. Instead of\nbinning the phase-space to estimate the probability map, its functional form is\napproximated with a normalizing flow. Therefore, the method naturally extends\nto high-dimensional datasets. The proposed framework is demonstrated as a\nviable pathway to enable data-efficient machine learning when abundant data is\navailable. An implementation of the method is available in a companion\nrepository (https://github.com/NREL/Phase-space-sampling).",
          "link": "http://arxiv.org/abs/2112.15446",
          "publishedOn": "2022-01-03T07:15:17.989Z",
          "wordCount": null,
          "title": "Uniform-in-Phase-Space Data Selection with Iterative Normalizing Flows. (arXiv:2112.15446v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.13901",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Irshad_F/0/1/0/all/0/1\">Faran Irshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karsch_S/0/1/0/all/0/1\">Stefan Karsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dopp_A/0/1/0/all/0/1\">Andreas D&#xf6;pp</a>",
          "description": "Bayesian optimization has proven to be an efficient method to optimize\nexpensive-to-evaluate systems. However, depending on the cost of single\nobservations, multi-dimensional optimizations of one or more objectives may\nstill be prohibitively expensive. Multi-fidelity optimization remedies this\nissue by including multiple, cheaper information sources such as low-resolution\napproximations in numerical simulations. Acquisition functions for\nmulti-fidelity optimization are typically based on exploration-heavy algorithms\nthat are difficult to combine with optimization towards multiple objectives.\n\nHere we show that the expected hypervolume improvement policy can act in many\nsituations as a suitable substitute. We incorporate the evaluation cost either\nvia a two-step evaluation or within a single acquisition function with an\nadditional fidelity-related objective. This permits simultaneous\nmulti-objective and multi-fidelity optimization, which allows to accurately\nestablish the Pareto set and front at fractional cost. Benchmarks show a cost\nreduction of an order of magnitude or more. Our method thus allows for Pareto\noptimization of extremely expansive black-box functions.\n\nThe presented methods are simple and straightforward to implement in\nexisting, optimized Bayesian optimization frameworks and can immediately be\nextended to batch optimization. The techniques can also be used to combine\ndifferent continuous and/or discrete fidelity dimensions, which makes them\nparticularly relevant for simulation problems in plasma physics, fluid dynamics\nand many other branches of scientific computing.",
          "link": "http://arxiv.org/abs/2112.13901",
          "publishedOn": "2022-01-03T07:15:17.989Z",
          "wordCount": null,
          "title": "Expected hypervolume improvement for simultaneous multi-objective and multi-fidelity optimization. (arXiv:2112.13901v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chin-Tung Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mu Yang</a>",
          "description": "Many social media users prefer consuming content in the form of videos rather\nthan text. However, in order for content creators to produce videos with a high\nclick-through rate, much editing is needed to match the footage to the music.\nThis posts additional challenges for more amateur video makers. Therefore, we\npropose a novel attention-based model VMT (Video-Music Transformer) that\nautomatically generates piano scores from video frames. Using music generated\nfrom models also prevent potential copyright infringements that often come with\nusing existing music. To the best of our knowledge, there is no work besides\nthe proposed VMT that aims to compose music for video. Additionally, there\nlacks a dataset with aligned video and symbolic music. We release a new dataset\ncomposed of over 7 hours of piano scores with fine alignment between pop music\nvideos and MIDI files. We conduct experiments with human evaluation on VMT,\nSeqSeq model (our baseline), and the original piano version soundtrack. VMT\nachieves consistent improvements over the baseline on music smoothness and\nvideo relevance. In particular, with the relevance scores and our case study,\nour model has shown the capability of multimodality on frame-level actors'\nmovement for music generation. Our VMT model, along with the new dataset,\npresents a promising research direction toward composing the matching\nsoundtrack for videos. We have released our code at\nhttps://github.com/linchintung/VMT",
          "link": "http://arxiv.org/abs/2112.15320",
          "publishedOn": "2022-01-03T07:15:17.987Z",
          "wordCount": null,
          "title": "InverseMV: Composing Piano Scores with a Convolutional Video-Music Transformer. (arXiv:2112.15320v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arnob_S/0/1/0/all/0/1\">Samin Yeasar Arnob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohib_R/0/1/0/all/0/1\">Riyasat Ohib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plis_S/0/1/0/all/0/1\">Sergey Plis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>",
          "description": "Deep Reinforcement Learning (RL) is a powerful framework for solving complex\nreal-world problems. Large neural networks employed in the framework are\ntraditionally associated with better generalization capabilities, but their\nincreased size entails the drawbacks of extensive training duration,\nsubstantial hardware resources, and longer inference times. One way to tackle\nthis problem is to prune neural networks leaving only the necessary parameters.\nState-of-the-art concurrent pruning techniques for imposing sparsity perform\ndemonstrably well in applications where data distributions are fixed. However,\nthey have not yet been substantially explored in the context of RL. We close\nthe gap between RL and single-shot pruning techniques and present a general\npruning approach to the Offline RL. We leverage a fixed dataset to prune neural\nnetworks before the start of RL training. We then run experiments varying the\nnetwork sparsity level and evaluating the validity of pruning at initialization\ntechniques in continuous control tasks. Our results show that with 95% of the\nnetwork weights pruned, Offline-RL algorithms can still retain performance in\nthe majority of our experiments. To the best of our knowledge, no prior work\nutilizing pruning in RL retained performance at such high levels of sparsity.\n\nMoreover, pruning at initialization techniques can be easily integrated into\nany existing Offline-RL algorithms without changing the learning objective.",
          "link": "http://arxiv.org/abs/2112.15579",
          "publishedOn": "2022-01-03T07:15:17.986Z",
          "wordCount": null,
          "title": "Single-Shot Pruning for Offline Reinforcement Learning. (arXiv:2112.15579v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1\">Wenxuan Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sihang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linxuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xihong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1\">En Zhu</a>",
          "description": "Deep graph clustering, which aims to reveal the underlying graph structure\nand divide the nodes into different groups, has attracted intensive attention\nin recent years. However, we observe that, in the process of node encoding,\nexisting methods suffer from representation collapse which tends to map all\ndata into the same representation. Consequently, the discriminative capability\nof the node representation is limited, leading to unsatisfied clustering\nperformance. To address this issue, we propose a novel self-supervised deep\ngraph clustering method termed Dual Correlation Reduction Network (DCRN) by\nreducing information correlation in a dual manner. Specifically, in our method,\nwe first design a siamese network to encode samples. Then by forcing the\ncross-view sample correlation matrix and cross-view feature correlation matrix\nto approximate two identity matrices, respectively, we reduce the information\ncorrelation in the dual-level, thus improving the discriminative capability of\nthe resulting features. Moreover, in order to alleviate representation collapse\ncaused by over-smoothing in GCN, we introduce a propagation regularization term\nto enable the network to gain long-distance information with the shallow\nnetwork structure. Extensive experimental results on six benchmark datasets\ndemonstrate the effectiveness of the proposed DCRN against the existing\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2112.14772",
          "publishedOn": "2022-01-03T07:15:17.985Z",
          "wordCount": null,
          "title": "Deep Graph Clustering via Dual Correlation Reduction. (arXiv:2112.14772v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15199",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Kovalev_D/0/1/0/all/0/1\">Dmitry Kovalev</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gasnikov_A/0/1/0/all/0/1\">Alexander Gasnikov</a>, <a href=\"http://arxiv.org/find/math/1/au:+Richtarik_P/0/1/0/all/0/1\">Peter Richt&#xe1;rik</a>",
          "description": "In this paper we study a convex-concave saddle-point problem $\\min_x\\max_y\nf(x) + y^\\top\\mathbf{A} x - g(y)$, where $f(x)$ and $g(y)$ are smooth and\nconvex functions. We propose an Accelerated Primal-Dual Gradient Method for\nsolving this problem which (i) achieves an optimal linear convergence rate in\nthe strongly-convex-strongly-concave regime matching the lower complexity bound\n(Zhang et al., 2021) and (ii) achieves an accelerated linear convergence rate\nin the case when only one of the functions $f(x)$ and $g(y)$ is strongly convex\nor even none of them are. Finally, we obtain a linearly-convergent algorithm\nfor the general smooth and convex-concave saddle point problem $\\min_x\\max_y\nF(x,y)$ without requirement of strong convexity or strong concavity.",
          "link": "http://arxiv.org/abs/2112.15199",
          "publishedOn": "2022-01-03T07:15:17.985Z",
          "wordCount": null,
          "title": "Accelerated Primal-Dual Gradient Method for Smooth and Convex-Concave Saddle-Point Problems with Bilinear Coupling. (arXiv:2112.15199v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2112.12545",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Bogyrbayeva_A/0/1/0/all/0/1\">Aigerim Bogyrbayeva</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yoon_T/0/1/0/all/0/1\">Taehyun Yoon</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ko_H/0/1/0/all/0/1\">Hanbum Ko</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lim_S/0/1/0/all/0/1\">Sungbin Lim</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yun_H/0/1/0/all/0/1\">Hyokun Yun</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kwon_C/0/1/0/all/0/1\">Changhyun Kwon</a>",
          "description": "Reinforcement learning has recently shown promise in learning quality\nsolutions in many combinatorial optimization problems. In particular, the\nattention-based encoder-decoder models show high effectiveness on various\nrouting problems, including the Traveling Salesman Problem (TSP).\nUnfortunately, they perform poorly for the TSP with Drone (TSP-D), requiring\nrouting a heterogeneous fleet of vehicles in coordination -- a truck and a\ndrone. In TSP-D, the two vehicles are moving in tandem and may need to wait at\na node for the other vehicle to join. State-less attention-based decoder fails\nto make such coordination between vehicles. We propose an attention\nencoder-LSTM decoder hybrid model, in which the decoder's hidden state can\nrepresent the sequence of actions made. We empirically demonstrate that such a\nhybrid model improves upon a purely attention-based model for both solution\nquality and computational efficiency. Our experiments on the min-max\nCapacitated Vehicle Routing Problem (mmCVRP) also confirm that the hybrid model\nis more suitable for coordinated routing of multiple vehicles than the\nattention-based model.",
          "link": "http://arxiv.org/abs/2112.12545",
          "publishedOn": "2022-01-03T07:15:17.971Z",
          "wordCount": null,
          "title": "A Deep Reinforcement Learning Approach for Solving the Traveling Salesman Problem with Drone. (arXiv:2112.12545v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarsa_S/0/1/0/all/0/1\">Sami Sarsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leinonen_J/0/1/0/all/0/1\">Juho Leinonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellas_A/0/1/0/all/0/1\">Arto Hellas</a>",
          "description": "In this work, we review and evaluate a body of deep learning knowledge\ntracing (DLKT) models with openly available and widely-used data sets, and with\na novel data set of students learning to program. The evaluated DLKT models\nhave been reimplemented for assessing reproducibility and replicability of\npreviously reported results. We test different input and output layer\nvariations found in the compared models that are independent of the main\narchitectures of the models, and different maximum attempt count options that\nhave been implicitly and explicitly used in some studies. Several metrics are\nused to reflect on the quality of the evaluated knowledge tracing models. The\nevaluated knowledge tracing models include Vanilla-DKT, two Long Short-Term\nMemory Deep Knowledge Tracing (LSTM-DKT) variants, two Dynamic Key-Value Memory\nNetwork (DKVMN) variants, and Self-Attentive Knowledge Tracing (SAKT). We\nevaluate logistic regression, Bayesian Knowledge Tracing (BKT) and simple\nnon-learning models as baselines. Our results suggest that the DLKT models in\ngeneral outperform non-DLKT models, and the relative differences between the\nDLKT models are subtle and often vary between datasets. Our results also show\nthat naive models such as mean prediction can yield better performance than\nmore sophisticated knowledge tracing models, especially in terms of accuracy.\nFurther, our metric and hyperparameter analysis shows that the metric used to\nselect the best model hyperparameters has a noticeable effect on the\nperformance of the models, and that metric choice can affect model ranking. We\nalso study the impact of input and output layer variations, filtering out long\nattempt sequences, and non-model properties such as randomness and hardware.\nFinally, we discuss model performance replicability and related issues. Our\nmodel implementations, evaluation code, and data are published as a part of\nthis work.",
          "link": "http://arxiv.org/abs/2112.15072",
          "publishedOn": "2022-01-03T07:15:17.969Z",
          "wordCount": null,
          "title": "Deep Learning Models for Knowledge Tracing: Review and Empirical Evaluation. (arXiv:2112.15072v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15275",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Stachenfeld_K/0/1/0/all/0/1\">Kimberly Stachenfeld</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Fielding_D/0/1/0/all/0/1\">Drummond B. Fielding</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kochkov_D/0/1/0/all/0/1\">Dmitrii Kochkov</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Cranmer_M/0/1/0/all/0/1\">Miles Cranmer</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pfaff_T/0/1/0/all/0/1\">Tobias Pfaff</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Godwin_J/0/1/0/all/0/1\">Jonathan Godwin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ho_S/0/1/0/all/0/1\">Shirley Ho</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Battaglia_P/0/1/0/all/0/1\">Peter Battaglia</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sanchez_Gonzalez_A/0/1/0/all/0/1\">Alvaro Sanchez-Gonzalez</a>",
          "description": "Turbulence simulation with classical numerical solvers requires very\nhigh-resolution grids to accurately resolve dynamics. Here we train learned\nsimulators at low spatial and temporal resolutions to capture turbulent\ndynamics generated at high resolution. We show that our proposed model can\nsimulate turbulent dynamics more accurately than classical numerical solvers at\nthe same low resolutions across various scientifically relevant metrics. Our\nmodel is trained end-to-end from data and is capable of learning a range of\nchallenging chaotic and turbulent dynamics at low resolution, including\ntrajectories generated by the state-of-the-art Athena++ engine. We show that\nour simpler, general-purpose architecture outperforms various more specialized,\nturbulence-specific architectures from the learned turbulence simulation\nliterature. In general, we see that learned simulators yield unstable\ntrajectories; however, we show that tuning training noise and temporal\ndownsampling solves this problem. We also find that while generalization beyond\nthe training distribution is a challenge for learned models, training noise,\nconvolutional architectures, and added loss constraints can help. Broadly, we\nconclude that our learned simulator outperforms traditional solvers run on\ncoarser grids, and emphasize that simple design choices can offer stability and\nrobust generalization.",
          "link": "http://arxiv.org/abs/2112.15275",
          "publishedOn": "2022-01-03T07:15:17.965Z",
          "wordCount": null,
          "title": "Learned Coarse Models for Efficient Turbulence Simulation. (arXiv:2112.15275v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mai_T/0/1/0/all/0/1\">Tien Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaillet_P/0/1/0/all/0/1\">Patrick Jaillet</a>",
          "description": "Stochastic and soft optimal policies resulting from entropy-regularized\nMarkov decision processes (ER-MDP) are desirable for exploration and imitation\nlearning applications. Motivated by the fact that such policies are sensitive\nwith respect to the state transition probabilities, and the estimation of these\nprobabilities may be inaccurate, we study a robust version of the ER-MDP model,\nwhere the stochastic optimal policies are required to be robust with respect to\nthe ambiguity in the underlying transition probabilities. Our work is at the\ncrossroads of two important schemes in reinforcement learning (RL), namely,\nrobust MDP and entropy regularized MDP. We show that essential properties that\nhold for the non-robust ER-MDP and robust unregularized MDP models also hold in\nour settings, making the robust ER-MDP problem tractable. We show how our\nframework and results can be integrated into different algorithmic schemes\nincluding value or (modified) policy iteration, which would lead to new robust\nRL and inverse RL algorithms to handle uncertainties. Analyses on computational\ncomplexity and error propagation under conventional uncertainty settings are\nalso provided.",
          "link": "http://arxiv.org/abs/2112.15364",
          "publishedOn": "2022-01-03T07:15:17.964Z",
          "wordCount": null,
          "title": "Robust Entropy-regularized Markov Decision Processes. (arXiv:2112.15364v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tjoa_E/0/1/0/all/0/1\">Erico Tjoa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuntai_G/0/1/0/all/0/1\">Guan Cuntai</a>",
          "description": "This paper proposes two bottom-up interpretable neural network (NN)\nconstructions for universal approximation, namely Triangularly-constructed NN\n(TNN) and Semi-Quantized Activation NN (SQANN). The notable properties are (1)\nresistance to catastrophic forgetting (2) existence of proof for arbitrarily\nhigh accuracies on training dataset (3) for an input \\(x\\), users can identify\nspecific samples of training data whose activation ``fingerprints\" are similar\nto that of \\(x\\)'s activations. Users can also identify samples that are out of\ndistribution.",
          "link": "http://arxiv.org/abs/2112.15026",
          "publishedOn": "2022-01-03T07:15:17.963Z",
          "wordCount": null,
          "title": "Two Instances of Interpretable Neural Network for Universal Approximations. (arXiv:2112.15026v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Saidur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1\">Foutse Khomh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidi_A/0/1/0/all/0/1\">Alaleh Hamidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jinghui Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antoniol_G/0/1/0/all/0/1\">Giuliano Antoniol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washizaki_H/0/1/0/all/0/1\">Hironori Washizaki</a>",
          "description": "Nowadays, intelligent systems and services are getting increasingly popular\nas they provide data-driven solutions to diverse real-world problems, thanks to\nrecent breakthroughs in Artificial Intelligence (AI) and Machine Learning (ML).\nHowever, machine learning meets software engineering not only with promising\npotentials but also with some inherent challenges. Despite some recent research\nefforts, we still do not have a clear understanding of the challenges of\ndeveloping ML-based applications and the current industry practices. Moreover,\nit is unclear where software engineering researchers should focus their efforts\nto better support ML application developers. In this paper, we report about a\nsurvey that aimed to understand the challenges and best practices of ML\napplication development. We synthesize the results obtained from 80\npractitioners (with diverse skills, experience, and application domains) into\n17 findings; outlining challenges and best practices for ML application\ndevelopment. Practitioners involved in the development of ML-based software\nsystems can leverage the summarized best practices to improve the quality of\ntheir system. We hope that the reported challenges will inform the research\ncommunity about topics that need to be investigated to improve the engineering\nprocess and the quality of ML-based applications.",
          "link": "http://arxiv.org/abs/2112.15277",
          "publishedOn": "2022-01-03T07:15:17.963Z",
          "wordCount": null,
          "title": "Machine Learning Application Development: Practitioners' Insights. (arXiv:2112.15277v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2109.07548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kocanaogullari_A/0/1/0/all/0/1\">Aziz Ko&#xe7;anao&#x11f;ullar&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ariyurek_C/0/1/0/all/0/1\">Cemre Ariyurek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afacan_O/0/1/0/all/0/1\">Onur Afacan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurugol_S/0/1/0/all/0/1\">Sila Kurugol</a>",
          "description": "Kidney DCE-MRI aims at both qualitative assessment of kidney anatomy and\nquantitative assessment of kidney function by estimating the tracer kinetic\n(TK) model parameters. Accurate estimation of TK model parameters requires an\naccurate measurement of the arterial input function (AIF) with high temporal\nresolution. Accelerated imaging is used to achieve high temporal resolution,\nwhich yields under-sampling artifacts in the reconstructed images. Compressed\nsensing (CS) methods offer a variety of reconstruction options. Most commonly,\nsparsity of temporal differences is encouraged for regularization to reduce\nartifacts. Increasing regularization in CS methods removes the ambient\nartifacts but also over-smooths the signal temporally which reduces the\nparameter estimation accuracy. In this work, we propose a single image trained\ndeep neural network to reduce MRI under-sampling artifacts without reducing the\naccuracy of functional imaging markers. Instead of regularizing with a penalty\nterm in optimization, we promote regularization by generating images from a\nlower dimensional representation. In this manuscript we motivate and explain\nthe lower dimensional input design. We compare our approach to CS\nreconstructions with multiple regularization weights. Proposed approach results\nin kidney biomarkers that are highly correlated with the ground truth markers\nestimated using the CS reconstruction which was optimized for functional\nanalysis. At the same time, the proposed approach reduces the artifacts in the\nreconstructed images.",
          "link": "http://arxiv.org/abs/2109.07548",
          "publishedOn": "2022-01-03T07:15:17.961Z",
          "wordCount": null,
          "title": "Learning the Regularization in DCE-MR Image Reconstruction for Functional Imaging of Kidneys. (arXiv:2109.07548v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.14417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Donghwan Lee</a>",
          "description": "The goal of this paper is to investigate a control theoretic analysis of\nlinear stochastic iterative algorithm and temporal difference (TD) learning.\nTD-learning is a linear stochastic iterative algorithm to estimate the value\nfunction of a given policy for a Markov decision process, which is one of the\nmost popular and fundamental reinforcement learning algorithms. While there has\nbeen a series of successful works in theoretical analysis of TD-learning, it\nwas not until recently that researchers found some guarantees on its\nstatistical efficiency. In this paper, we propose a control theoretic\nfinite-time analysis TD-learning, which exploits standard notions in linear\nsystem control communities. Therefore, the proposed work provides additional\ninsights on TD-learning and reinforcement learning with simple concepts and\nanalysis tools in control theory.",
          "link": "http://arxiv.org/abs/2112.14417",
          "publishedOn": "2022-01-03T07:15:17.960Z",
          "wordCount": null,
          "title": "Control Theoretic Analysis of Temporal Difference Learning. (arXiv:2112.14417v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15532",
          "author": "<a href=\"http://arxiv.org/find/hep-lat/1/au:+Debbio_L/0/1/0/all/0/1\">Luigi Del Debbio</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Rossney_J/0/1/0/all/0/1\">Joe Marsh Rossney</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Wilson_M/0/1/0/all/0/1\">Michael Wilson</a>",
          "description": "A trivializing map is a field transformation whose Jacobian determinant\nexactly cancels the interaction terms in the action, providing a representation\nof the theory in terms of a deterministic transformation of a distribution from\nwhich sampling is trivial. Recently, a proof-of-principle study by Albergo,\nKanwar and Shanahan [arXiv:1904.12072] demonstrated that approximations of\ntrivializing maps can be `machine-learned' by a class of invertible,\ndifferentiable neural models called \\textit{normalizing flows}. By ensuring\nthat the Jacobian determinant can be computed efficiently, asymptotically exact\nsampling from the theory of interest can be performed by drawing samples from a\nsimple distribution and passing them through the network. From a theoretical\nperspective, this approach has the potential to become more efficient than\ntraditional Markov Chain Monte Carlo sampling techniques, where\nautocorrelations severely diminish the sampling efficiency as one approaches\nthe continuum limit. A major caveat is that it is not yet understood how the\nsize of models and the cost of training them is expected to scale. As a first\nstep, we have conducted an exploratory scaling study using two-dimensional\n$\\phi^4$ with up to $20^2$ lattice sites. Although the scope of our study is\nlimited to a particular model architecture and training algorithm, initial\nresults paint an interesting picture in which training costs grow very quickly\nindeed. We describe a candidate explanation for the poor scaling, and outline\nour intentions to clarify the situation in future work.",
          "link": "http://arxiv.org/abs/2112.15532",
          "publishedOn": "2022-01-03T07:15:17.958Z",
          "wordCount": null,
          "title": "Machine Learning Trivializing Maps: A First Step Towards Understanding How Flow-Based Samplers Scale Up. (arXiv:2112.15532v1 [hep-lat])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15095",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Csoka_A/0/1/0/all/0/1\">&#xc1;d&#xe1;m Cs&#xf3;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovacs_G/0/1/0/all/0/1\">Gy&#xf6;rgy Kov&#xe1;cs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acs_V/0/1/0/all/0/1\">Vir&#xe1;g &#xc1;cs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matics_Z/0/1/0/all/0/1\">Zsolt Matics</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerencser_Z/0/1/0/all/0/1\">Zsolt Gerencs&#xe9;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szendro_Z/0/1/0/all/0/1\">Zsolt Szendr&#x151;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagy_I/0/1/0/all/0/1\">Istv&#xe1;n Nagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petnehazy_O/0/1/0/all/0/1\">&#xd6;rs Petneh&#xe1;zy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Repa_I/0/1/0/all/0/1\">Imre Repa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moizs_M/0/1/0/all/0/1\">Mariann Moizs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donko_T/0/1/0/all/0/1\">Tam&#xe1;s Donk&#xf3;</a>",
          "description": "Various applications of farm animal imaging are based on the estimation of\nweights of certain body parts and cuts from the CT images of animals. In many\ncases, the complexity of the problem is increased by the enormous variability\nof postures in CT images due to the scanning of non-sedated, living animals. In\nthis paper, we propose a general and robust approach for the estimation of the\nweights of cuts and body parts from the CT images of (possibly) living animals.\nWe adapt multi-atlas based segmentation driven by elastic registration and\njoint feature and model selection for the regression component to cape with the\nlarge number of features and low number of samples. The proposed technique is\nevaluated and illustrated through real applications in rabbit breeding\nprograms, showing r^2 scores 12% higher than previous techniques and methods\nthat used to drive the selection so far. The proposed technique is easily\nadaptable to similar problems, consequently, it is shared in an open source\nsoftware package for the benefit of the community.",
          "link": "http://arxiv.org/abs/2112.15095",
          "publishedOn": "2022-01-03T07:15:17.957Z",
          "wordCount": null,
          "title": "A general technique for the estimation of farm animal body part weights from CT scans and its applications in a rabbit breeding program. (arXiv:2112.15095v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Astudillo_R/0/1/0/all/0/1\">Raul Astudillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frazier_P/0/1/0/all/0/1\">Peter I. Frazier</a>",
          "description": "We consider Bayesian optimization of the output of a network of functions,\nwhere each function takes as input the output of its parent nodes, and where\nthe network takes significant time to evaluate. Such problems arise, for\nexample, in reinforcement learning, engineering design, and manufacturing.\nWhile the standard Bayesian optimization approach observes only the final\noutput, our approach delivers greater query efficiency by leveraging\ninformation that the former ignores: intermediate output within the network.\nThis is achieved by modeling the nodes of the network using Gaussian processes\nand choosing the points to evaluate using, as our acquisition function, the\nexpected improvement computed with respect to the implied posterior on the\nobjective. Although the non-Gaussian nature of this posterior prevents\ncomputing our acquisition function in closed form, we show that it can be\nefficiently maximized via sample average approximation. In addition, we prove\nthat our method is asymptotically consistent, meaning that it finds a globally\noptimal solution as the number of evaluations grows to infinity, thus\ngeneralizing previously known convergence results for the expected improvement.\nNotably, this holds even though our method might not evaluate the domain\ndensely, instead leveraging problem structure to leave regions unexplored.\nFinally, we show that our approach dramatically outperforms standard Bayesian\noptimization methods in several synthetic and real-world problems.",
          "link": "http://arxiv.org/abs/2112.15311",
          "publishedOn": "2022-01-03T07:15:17.957Z",
          "wordCount": null,
          "title": "Bayesian Optimization of Function Networks. (arXiv:2112.15311v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lehmler_S/0/1/0/all/0/1\">Stephan Johann Lehmler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saif_ur_Rehman_M/0/1/0/all/0/1\">Muhammad Saif-ur-Rehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glasmachers_T/0/1/0/all/0/1\">Tobias Glasmachers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iossifidis_I/0/1/0/all/0/1\">Ioannis Iossifidis</a>",
          "description": "Accurate decoding of surface electromyography (sEMG) is pivotal for\nmuscle-to-machine-interfaces (MMI) and their application for e.g.\nrehabilitation therapy. sEMG signals have high inter-subject variability, due\nto various factors, including skin thickness, body fat percentage, and\nelectrode placement. Therefore, obtaining high generalization quality of a\ntrained sEMG decoder is quite challenging. Usually, machine learning based sEMG\ndecoders are either trained on subject-specific data, or at least recalibrated\nfor each user, individually. Even though, deep learning algorithms produced\nseveral state of the art results for sEMG decoding,however, due to the limited\namount of availability of sEMG data, the deep learning models are prone to\noverfitting. Recently, transfer learning for domain adaptation improved\ngeneralization quality with reduced training time on various machine learning\ntasks. In this study, we investigate the effectiveness of transfer learning\nusing weight initialization for recalibration of two different pretrained deep\nlearning models on a new subjects data, and compare their performance to\nsubject-specific models. To the best of our knowledge, this is the first study\nthat thoroughly investigated weight-initialization based transfer learning for\nsEMG classification and compared transfer learning to subject-specific\nmodeling. We tested our models on three publicly available databases under\nvarious settings. On average over all settings, our transfer learning approach\nimproves 5~\\%-points on the pretrained models without fine-tuning and\n12~\\%-points on the subject-specific models, while being trained on average\n22~\\% fewer epochs. Our results indicate that transfer learning enables faster\ntraining on fewer samples than user-specific models, and improves the\nperformance of pretrained models as long as enough data is available.",
          "link": "http://arxiv.org/abs/2112.15019",
          "publishedOn": "2022-01-03T07:15:17.952Z",
          "wordCount": null,
          "title": "Deep Transfer-Learning for patient specific model re-calibration: Application to sEMG-Classification. (arXiv:2112.15019v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xi_D/0/1/0/all/0/1\">Dongbo Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jingjing Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>",
          "description": "Human mobility data accumulated from Point-of-Interest (POI) check-ins\nprovides great opportunity for user behavior understanding. However, data\nquality issues (e.g., geolocation information missing, unreal check-ins, data\nsparsity) in real-life mobility data limit the effectiveness of existing\nPOI-oriented studies, e.g., POI recommendation and location prediction, when\napplied to real applications. To this end, in this paper, we develop a model,\nnamed Bi-STDDP, which can integrate bi-directional spatio-temporal dependence\nand users' dynamic preferences, to identify the missing POI check-in where a\nuser has visited at a specific time. Specifically, we first utilize\nbi-directional global spatial and local temporal information of POIs to capture\nthe complex dependence relationships. Then, target temporal pattern in\ncombination with user and POI information are fed into a multi-layer network to\ncapture users' dynamic preferences. Moreover, the dynamic preferences are\ntransformed into the same space as the dependence relationships to form the\nfinal model. Finally, the proposed model is evaluated on three large-scale\nreal-world datasets and the results demonstrate significant improvements of our\nmodel compared with state-of-the-art methods. Also, it is worth noting that the\nproposed model can be naturally extended to address POI recommendation and\nlocation prediction tasks with competitive performances.",
          "link": "http://arxiv.org/abs/2112.15285",
          "publishedOn": "2022-01-03T07:15:17.951Z",
          "wordCount": null,
          "title": "Modelling of Bi-directional Spatio-Temporal Dependence and Users' Dynamic Preferences for Missing POI Check-in Identification. (arXiv:2112.15285v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maddox_W/0/1/0/all/0/1\">Wesley J. Maddox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_S/0/1/0/all/0/1\">Sanyam Kapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1\">Andrew Gordon Wilson</a>",
          "description": "While recent work on conjugate gradient methods and Lanczos decompositions\nhave achieved scalable Gaussian process inference with highly accurate point\npredictions, in several implementations these iterative methods appear to\nstruggle with numerical instabilities in learning kernel hyperparameters, and\npoor test likelihoods. By investigating CG tolerance, preconditioner rank, and\nLanczos decomposition rank, we provide a particularly simple prescription to\ncorrect these issues: we recommend that one should use a small CG tolerance\n($\\epsilon \\leq 0.01$) and a large root decomposition size ($r \\geq 5000$).\nMoreover, we show that L-BFGS-B is a compelling optimizer for Iterative GPs,\nachieving convergence with fewer gradient updates.",
          "link": "http://arxiv.org/abs/2112.15246",
          "publishedOn": "2022-01-03T07:15:17.945Z",
          "wordCount": null,
          "title": "When are Iterative Gaussian Processes Reliably Accurate?. (arXiv:2112.15246v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dejing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gus Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>",
          "description": "Could we automatically derive the score of a piano accompaniment based on the\naudio of a pop song? This is the audio-to-symbolic arrangement problem we\ntackle in this paper. A good arrangement model should not only consider the\naudio content but also have prior knowledge of piano composition (so that the\ngeneration \"sounds like\" the audio and meanwhile maintains musicality.) To this\nend, we contribute a cross-modal representation-learning model, which 1)\nextracts chord and melodic information from the audio, and 2) learns texture\nrepresentation from both audio and a corrupted ground truth arrangement. We\nfurther introduce a tailored training strategy that gradually shifts the source\nof texture information from corrupted score to audio. In the end, the\nscore-based texture posterior is reduced to a standard normal distribution, and\nonly audio is needed for inference. Experiments show that our model captures\nmajor audio information and outperforms baselines in generation quality.",
          "link": "http://arxiv.org/abs/2112.15110",
          "publishedOn": "2022-01-03T07:15:17.944Z",
          "wordCount": null,
          "title": "Audio-to-symbolic Arrangement via Cross-modal Music Representation Learning. (arXiv:2112.15110v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15367",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Daudt_R/0/1/0/all/0/1\">Rodrigo Caye Daudt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saux_B/0/1/0/all/0/1\">Bertrand Le Saux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boulch_A/0/1/0/all/0/1\">Alexandre Boulch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gousseau_Y/0/1/0/all/0/1\">Yann Gousseau</a>",
          "description": "Large scale datasets created from crowdsourced labels or openly available\ndata have become crucial to provide training data for large scale learning\nalgorithms. While these datasets are easier to acquire, the data are frequently\nnoisy and unreliable, which is motivating research on weakly supervised\nlearning techniques. In this paper we propose original ideas that help us to\nleverage such datasets in the context of change detection. First, we propose\nthe guided anisotropic diffusion (GAD) algorithm, which improves semantic\nsegmentation results using the input images as guides to perform edge\npreserving filtering. We then show its potential in two weakly-supervised\nlearning strategies tailored for change detection. The first strategy is an\niterative learning method that combines model optimisation and data cleansing\nusing GAD to extract the useful information from a large scale change detection\ndataset generated from open vector data. The second one incorporates GAD within\na novel spatial attention layer that increases the accuracy of weakly\nsupervised networks trained to perform pixel-level predictions from image-level\nlabels. Improvements with respect to state-of-the-art are demonstrated on 4\ndifferent public datasets.",
          "link": "http://arxiv.org/abs/2112.15367",
          "publishedOn": "2022-01-03T07:15:17.944Z",
          "wordCount": null,
          "title": "Weakly Supervised Change Detection Using Guided Anisotropic Difusion. (arXiv:2112.15367v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tjoa_E/0/1/0/all/0/1\">Erico Tjoa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuntai_G/0/1/0/all/0/1\">Guan Cuntai</a>",
          "description": "Transparency and fairness issues in Deep Reinforcement Learning may stem from\nthe black-box nature of deep neural networks used to learn its policy, value\nfunctions etc. This paper proposes a way to circumvent the issues through the\nbottom-up design of neural networks (NN) with detailed interpretability, where\neach neuron or layer has its own meaning and utility that corresponds to\nhumanly understandable concept. With deliberate design, we show that lavaland\nproblems can be solved using NN model with few parameters. Furthermore, we\nintroduce the Self Reward Design (SRD), inspired by the Inverse Reward Design,\nso that our interpretable design can (1) solve the problem by pure design\n(although imperfectly) (2) be optimized via SRD (3) perform avoidance of\nunknown states by recognizing the inactivations of neurons aggregated as the\nactivation in \\(w_{unknown}\\).",
          "link": "http://arxiv.org/abs/2112.15034",
          "publishedOn": "2022-01-03T07:15:17.942Z",
          "wordCount": null,
          "title": "Self Reward Design with Fine-grained Interpretability. (arXiv:2112.15034v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14877",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bui_Thanh_T/0/1/0/all/0/1\">Tan Bui-Thanh</a>",
          "description": "One of the reasons that many neural networks are capable of replicating\ncomplicated tasks or functions is their universality property. The past few\ndecades have seen many attempts in providing constructive proofs for single or\nclass of neural networks. This paper is an effort to provide a unified and\nconstructive framework for the universality of a large class of activations\nincluding most of existing activations and beyond. At the heart of the\nframework is the concept of neural network approximate identity. It turns out\nthat most of existing activations are neural network approximate identity, and\nthus universal in the space of continuous of functions on compacta. The\nframework induces several advantages. First, it is constructive with elementary\nmeans from functional analysis, probability theory, and numerical analysis.\nSecond, it is the first unified attempt that is valid for most of existing\nactivations. Third, as a by product, the framework provides the first\nuniversity proof for some of the existing activation functions including Mish,\nSiLU, ELU, GELU, and etc. Fourth, it discovers new activations with guaranteed\nuniversality property. Indeed, any activation\\textemdash whose $\\k$th\nderivative, with $\\k$ being an integer, is integrable and essentially\nbounded\\textemdash is universal. Fifth, for a given activation and error\ntolerance, the framework provides precisely the architecture of the\ncorresponding one-hidden neural network with predetermined number of neuron,\nand the values of weights/biases.",
          "link": "http://arxiv.org/abs/2112.14877",
          "publishedOn": "2022-01-03T07:15:17.939Z",
          "wordCount": null,
          "title": "A Unified and Constructive Framework for the Universality of Neural Networks. (arXiv:2112.14877v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15595",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Irons_N/0/1/0/all/0/1\">Nicholas J. Irons</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scetbon_M/0/1/0/all/0/1\">Meyer Scetbon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pal_S/0/1/0/all/0/1\">Soumik Pal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Harchaoui_Z/0/1/0/all/0/1\">Zaid Harchaoui</a>",
          "description": "Triangular flows, also known as Kn\\\"{o}the-Rosenblatt measure couplings,\ncomprise an important building block of normalizing flow models for generative\nmodeling and density estimation, including popular autoregressive flow models\nsuch as real-valued non-volume preserving transformation models (Real NVP). We\npresent statistical guarantees and sample complexity bounds for triangular flow\nstatistical models. In particular, we establish the statistical consistency and\nthe finite sample convergence rates of the Kullback-Leibler estimator of the\nKn\\\"{o}the-Rosenblatt measure coupling using tools from empirical process\ntheory. Our results highlight the anisotropic geometry of function classes at\nplay in triangular flows, shed light on optimal coordinate ordering, and lead\nto statistical guarantees for Jacobian flows. We conduct numerical experiments\non synthetic data to illustrate the practical implications of our theoretical\nfindings.",
          "link": "http://arxiv.org/abs/2112.15595",
          "publishedOn": "2022-01-03T07:15:17.936Z",
          "wordCount": null,
          "title": "Triangular Flows for Generative Modeling: Statistical Consistency, Smoothness Classes, and Fast Rates. (arXiv:2112.15595v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15012",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenguang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Shuyuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>",
          "description": "Predicting human motion from historical pose sequence is crucial for a\nmachine to succeed in intelligent interactions with humans. One aspect that has\nbeen obviated so far, is the fact that how we represent the skeletal pose has a\ncritical impact on the prediction results. Yet there is no effort that\ninvestigates across different pose representation schemes. We conduct an\nindepth study on various pose representations with a focus on their effects on\nthe motion prediction task. Moreover, recent approaches build upon\noff-the-shelf RNN units for motion prediction. These approaches process input\npose sequence sequentially and inherently have difficulties in capturing\nlong-term dependencies. In this paper, we propose a novel RNN architecture\ntermed AHMR (Attentive Hierarchical Motion Recurrent network) for motion\nprediction which simultaneously models local motion contexts and a global\ncontext. We further explore a geodesic loss and a forward kinematics loss for\nthe motion prediction task, which have more geometric significance than the\nwidely employed L2 loss. Interestingly, we applied our method to a range of\narticulate objects including human, fish, and mouse. Empirical results show\nthat our approach outperforms the state-of-the-art methods in short-term\nprediction and achieves much enhanced long-term prediction proficiency, such as\nretaining natural human-like motions over 50 seconds predictions. Our codes are\nreleased.",
          "link": "http://arxiv.org/abs/2112.15012",
          "publishedOn": "2022-01-03T07:15:17.935Z",
          "wordCount": null,
          "title": "Investigating Pose Representations and Motion Contexts Modeling for 3D Motion Prediction. (arXiv:2112.15012v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">JunKyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhanov_L/0/1/0/all/0/1\">Lev Mukhanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molahosseini_A/0/1/0/all/0/1\">Amir Sabbagh Molahosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minhas_U/0/1/0/all/0/1\">Umar Minhas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rincon_J/0/1/0/all/0/1\">Jesus Martinez del Rincon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dichev_K/0/1/0/all/0/1\">Kiril Dichev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1\">Cheol-Ho Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandierendonck_H/0/1/0/all/0/1\">Hans Vandierendonck</a>",
          "description": "Deep learning is pervasive in our daily life, including self-driving cars,\nvirtual assistants, social network services, healthcare services, face\nrecognition, etc. However, deep neural networks demand substantial compute\nresources during training and inference. The machine learning community has\nmainly focused on model-level optimizations such as architectural compression\nof deep learning models, while the system community has focused on\nimplementation-level optimization. In between, various arithmetic-level\noptimization techniques have been proposed in the arithmetic community. This\narticle provides a survey on resource-efficient deep learning techniques in\nterms of model-, arithmetic-, and implementation-level techniques and\nidentifies the research gaps for resource-efficient deep learning techniques\nacross the three different level techniques. Our survey clarifies the influence\nfrom higher to lower-level techniques based on our resource-efficiency metric\ndefinition and discusses the future trend for resource-efficient deep learning\nresearch.",
          "link": "http://arxiv.org/abs/2112.15131",
          "publishedOn": "2022-01-03T07:15:17.932Z",
          "wordCount": null,
          "title": "Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques. (arXiv:2112.15131v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1\">Qingsong Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wenzheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Siming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jianguo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>",
          "description": "Heterogeneous graph neural networks (HGNNs) have been blossoming in recent\nyears, but the unique data processing and evaluation setups used by each work\nobstruct a full understanding of their advancements. In this work, we present a\nsystematical reproduction of 12 recent HGNNs by using their official codes,\ndatasets, settings, and hyperparameters, revealing surprising findings about\nthe progress of HGNNs. We find that the simple homogeneous GNNs, e.g., GCN and\nGAT, are largely underestimated due to improper settings. GAT with proper\ninputs can generally match or outperform all existing HGNNs across various\nscenarios. To facilitate robust and reproducible HGNN research, we construct\nthe Heterogeneous Graph Benchmark (HGB), consisting of 11 diverse datasets with\nthree tasks. HGB standardizes the process of heterogeneous graph data splits,\nfeature processing, and performance evaluation. Finally, we introduce a simple\nbut very strong baseline Simple-HGN--which significantly outperforms all\nprevious models on HGB--to accelerate the advancement of HGNNs in the future.",
          "link": "http://arxiv.org/abs/2112.14936",
          "publishedOn": "2022-01-03T07:15:17.923Z",
          "wordCount": null,
          "title": "Are we really making much progress? Revisiting, benchmarking, and refining heterogeneous graph neural networks. (arXiv:2112.14936v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Achab_M/0/1/0/all/0/1\">Mastane Achab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neu_G/0/1/0/all/0/1\">Gergely Neu</a>",
          "description": "In dynamic programming (DP) and reinforcement learning (RL), an agent learns\nto act optimally in terms of expected long-term return by sequentially\ninteracting with its environment modeled by a Markov decision process (MDP).\nMore generally in distributional reinforcement learning (DRL), the focus is on\nthe whole distribution of the return, not just its expectation. Although\nDRL-based methods produced state-of-the-art performance in RL with function\napproximation, they involve additional quantities (compared to the\nnon-distributional setting) that are still not well understood. As a first\ncontribution, we introduce a new class of distributional operators, together\nwith a practical DP algorithm for policy evaluation, that come with a robust\nMDP interpretation. Indeed, our approach reformulates through an augmented\nstate space where each state is split into a worst-case substate and a\nbest-case substate, whose values are maximized by safe and risky policies\nrespectively. Finally, we derive distributional operators and DP algorithms\nsolving a new control task: How to distinguish safe from risky optimal actions\nin order to break ties in the space of optimal policies?",
          "link": "http://arxiv.org/abs/2112.15430",
          "publishedOn": "2022-01-03T07:15:17.922Z",
          "wordCount": null,
          "title": "Robustness and risk management via distributional dynamic programming. (arXiv:2112.15430v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15287",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_X/0/1/0/all/0/1\">Xiao Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Milzarek_A/0/1/0/all/0/1\">Andre Milzarek</a>, <a href=\"http://arxiv.org/find/math/1/au:+Pu_S/0/1/0/all/0/1\">Shi Pu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Qiu_J/0/1/0/all/0/1\">Junwen Qiu</a>",
          "description": "In this paper, we consider the distributed optimization problem where $n$\nagents, each possessing a local cost function, collaboratively minimize the\naverage of the local cost functions over a connected network. To solve the\nproblem, we propose a distributed random reshuffling (D-RR) algorithm that\ncombines the classical distributed gradient descent (DGD) method and Random\nReshuffling (RR). We show that D-RR inherits the superiority of RR for both\nsmooth strongly convex and smooth nonconvex objective functions. In particular,\nfor smooth strongly convex objective functions, D-RR achieves\n$\\mathcal{O}(1/T^2)$ rate of convergence (here, $T$ counts the total number of\niterations) in terms of the squared distance between the iterate and the unique\nminimizer. When the objective function is assumed to be smooth nonconvex and\nhas Lipschitz continuous component functions, we show that D-RR drives the\nsquared norm of gradient to $0$ at a rate of $\\mathcal{O}(1/T^{2/3})$. These\nconvergence results match those of centralized RR (up to constant factors).",
          "link": "http://arxiv.org/abs/2112.15287",
          "publishedOn": "2022-01-03T07:15:17.919Z",
          "wordCount": null,
          "title": "Distributed Random Reshuffling over Networks. (arXiv:2112.15287v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14826",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Diab_W/0/1/0/all/0/1\">Waleed Diab</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kobaisi_M/0/1/0/all/0/1\">Mohammed Al Kobaisi</a>",
          "description": "The displacement of two immiscible fluids is a common problem in fluid flow\nin porous media. Such a problem can be posed as a partial differential equation\n(PDE) in what is commonly referred to as a Buckley-Leverett (B-L) problem. The\nB-L problem is a non-linear hyperbolic conservation law that is known to be\nnotoriously difficult to solve using traditional numerical methods. Here, we\naddress the forward hyperbolic B-L problem with a nonconvex flux function using\nphysics-informed neural networks (PINNs). The contributions of this paper are\ntwofold. First, we present a PINN approach to solve the hyperbolic B-L problem\nby embedding the Oleinik entropy condition into the neural network residual. We\ndo not use a diffusion term (artificial viscosity) in the residual-loss, but we\nrely on the strong form of the PDE. Second, we use the Adam optimizer with\nresidual-based adaptive refinement (RAR) algorithm to achieve an ultra-low loss\nwithout weighting. Our solution method can accurately capture the shock-front\nand produce an accurate overall solution. We report a L2 validation error of 2\nx 10-2 and a L2 loss of 1x 10-6. The proposed method does not require any\nadditional regularization or weighting of losses to obtain such accurate\nsolution.",
          "link": "http://arxiv.org/abs/2112.14826",
          "publishedOn": "2022-01-03T07:15:17.915Z",
          "wordCount": null,
          "title": "PINNs for the Solution of the Hyperbolic Buckley-Leverett Problem with a Non-convex Flux Function. (arXiv:2112.14826v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xidong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haifeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaodong Yang</a>",
          "description": "In recent years, gradient based Meta-RL (GMRL) methods have achieved\nremarkable successes in either discovering effective online hyperparameter for\none single task (Xu et al., 2018) or learning good initialisation for\nmulti-task transfer learning (Finn et al., 2017). Despite the empirical\nsuccesses, it is often neglected that computing meta gradients via vanilla\nbackpropagation is ill-defined. In this paper, we argue that the stochastic\nmeta-gradient estimation adopted by many existing MGRL methods are in fact\nbiased; the bias comes from two sources: 1) the compositional bias that is\ninborn in the structure of compositional optimisation problems and 2) the bias\nof multi-step Hessian estimation caused by direct automatic differentiation. To\nbetter understand the meta gradient biases, we perform the first of its kind\nstudy to quantify the amount for each of them. We start by providing a unifying\nderivation for existing GMRL algorithms, and then theoretically analyse both\nthe bias and the variance of existing gradient estimation methods. On\nunderstanding the underlying principles of bias, we propose two mitigation\nsolutions based on off-policy correction and multi-step Hessian estimation\ntechniques. Comprehensive ablation studies have been conducted and results\nreveals: (1) The existence of these two biases and how they influence the\nmeta-gradient estimation when combined with different estimator/sample\nsize/step and learning rate. (2) The effectiveness of these mitigation\napproaches for meta-gradient estimation and thereby the final return on two\npractical Meta-RL algorithms: LOLA-DiCE and Meta-gradient Reinforcement\nLearning.",
          "link": "http://arxiv.org/abs/2112.15400",
          "publishedOn": "2022-01-03T07:15:17.915Z",
          "wordCount": null,
          "title": "Settling the Bias and Variance of Meta-Gradient Estimation for Meta-Reinforcement Learning. (arXiv:2112.15400v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2111.04870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Delahunt_C/0/1/0/all/0/1\">Charles B. Delahunt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutz_J/0/1/0/all/0/1\">J. Nathan Kutz</a>",
          "description": "We consider the data-driven discovery of governing equations from time-series\ndata in the limit of high noise. The algorithms developed describe an extensive\ntoolkit of methods for circumventing the deleterious effects of noise in the\ncontext of the sparse identification of nonlinear dynamics (SINDy) framework.\nWe offer two primary contributions, both focused on noisy data acquired from a\nsystem x' = f(x). First, we propose, for use in high-noise settings, an\nextensive toolkit of critically enabling extensions for the SINDy regression\nmethod, to progressively cull functionals from an over-complete library and\nyield a set of sparse equations that regress to the derivate x'. These\ninnovations can extract sparse governing equations and coefficients from\nhigh-noise time-series data (e.g. 300% added noise). For example, it discovers\nthe correct sparse libraries in the Lorenz system, with median coefficient\nestimate errors equal to 1% - 3% (for 50% noise), 6% - 8% (for 100% noise); and\n23% - 25% (for 300% noise). The enabling modules in the toolkit are combined\ninto a single method, but the individual modules can be tactically applied in\nother equation discovery methods (SINDy or not) to improve results on\nhigh-noise data. Second, we propose a technique, applicable to any model\ndiscovery method based on x' = f(x), to assess the accuracy of a discovered\nmodel in the context of non-unique solutions due to noisy data. Currently, this\nnon-uniqueness can obscure a discovered model's accuracy and thus a discovery\nmethod's effectiveness. We describe a technique that uses linear dependencies\namong functionals to transform a discovered model into an equivalent form that\nis closest to the true model, enabling more accurate assessment of a discovered\nmodel's accuracy.",
          "link": "http://arxiv.org/abs/2111.04870",
          "publishedOn": "2022-01-03T07:15:17.915Z",
          "wordCount": null,
          "title": "A toolkit for data-driven discovery of governing equations in high-noise regimes. (arXiv:2111.04870v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Quan_N/0/1/0/all/0/1\">Nguyen Hoang Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dat_N/0/1/0/all/0/1\">Nguyen Thanh Dat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_N/0/1/0/all/0/1\">Nguyen Hoang Minh Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinh_N/0/1/0/all/0/1\">Nguyen Van Vinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinh_N/0/1/0/all/0/1\">Ngo Thi Vinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thai_N/0/1/0/all/0/1\">Nguyen Phuong Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viet_T/0/1/0/all/0/1\">Tran Hong Viet</a>",
          "description": "We present an open-source toolkit for neural machine translation (NMT). The\nnew toolkit is mainly based on vaulted Transformer (Vaswani et al., 2017) along\nwith many other improvements detailed below, in order to create a\nself-contained, simple to use, consistent and comprehensive framework for\nMachine Translation tasks of various domains. It is tooled to support both\nbilingual and multilingual translation tasks, starting from building the model\nfrom respective corpora, to inferring new predictions or packaging the model to\nserving-capable JIT format.",
          "link": "http://arxiv.org/abs/2112.15272",
          "publishedOn": "2022-01-03T07:15:17.912Z",
          "wordCount": null,
          "title": "ViNMT: Neural Machine Translation Tookit. (arXiv:2112.15272v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15555",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiju Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taejoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>",
          "description": "In this paper, we propose a dual-module network architecture that employs a\ndomain discriminative feature module to encourage the domain invariant feature\nmodule to learn more domain invariant features. The proposed architecture can\nbe applied to any model that utilizes domain invariant features for\nunsupervised domain adaptation to improve its ability to extract domain\ninvariant features. We conduct experiments with the Domain-Adversarial Training\nof Neural Networks (DANN) model as a representative algorithm. In the training\nprocess, we supply the same input to the two modules and then extract their\nfeature distribution and prediction results respectively. We propose a\ndiscrepancy loss to find the discrepancy of the prediction results and the\nfeature distribution between the two modules. Through the adversarial training\nby maximizing the loss of their feature distribution and minimizing the\ndiscrepancy of their prediction results, the two modules are encouraged to\nlearn more domain discriminative and domain invariant features respectively.\nExtensive comparative evaluations are conducted and the proposed approach\noutperforms the state-of-the-art in most unsupervised domain adaptation tasks.",
          "link": "http://arxiv.org/abs/2112.15555",
          "publishedOn": "2022-01-03T07:15:17.911Z",
          "wordCount": null,
          "title": "An Unsupervised Domain Adaptation Model based on Dual-module Adversarial Training. (arXiv:2112.15555v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salimibeni_M/0/1/0/all/0/1\">Mohammad Salimibeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_A/0/1/0/all/0/1\">Arash Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malekzadeh_P/0/1/0/all/0/1\">Parvin Malekzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>",
          "description": "Distributed Multi-Agent Reinforcement Learning (MARL) algorithms has\nattracted a surge of interest lately mainly due to the recent advancements of\nDeep Neural Networks (DNNs). Conventional Model-Based (MB) or Model-Free (MF)\nRL algorithms are not directly applicable to the MARL problems due to\nutilization of a fixed reward model for learning the underlying value function.\nWhile DNN-based solutions perform utterly well when a single agent is involved,\nsuch methods fail to fully generalize to the complexities of MARL problems. In\nother words, although recently developed approaches based on DNNs for\nmulti-agent environments have achieved superior performance, they are still\nprone to overfiting, high sensitivity to parameter selection, and sample\ninefficiency. The paper proposes the Multi-Agent Adaptive Kalman Temporal\nDifference (MAK-TD) framework and its Successor Representation-based variant,\nreferred to as the MAK-SR. Intuitively speaking, the main objective is to\ncapitalize on unique characteristics of Kalman Filtering (KF) such as\nuncertainty modeling and online second order learning. The proposed MAK-TD/SR\nframeworks consider the continuous nature of the action-space that is\nassociated with high dimensional multi-agent environments and exploit Kalman\nTemporal Difference (KTD) to address the parameter uncertainty. By leveraging\nthe KTD framework, SR learning procedure is modeled into a filtering problem,\nwhere Radial Basis Function (RBF) estimators are used to encode the continuous\nspace into feature vectors. On the other hand, for learning localized reward\nfunctions, we resort to Multiple Model Adaptive Estimation (MMAE), to deal with\nthe lack of prior knowledge on the observation noise covariance and observation\nmapping function. The proposed MAK-TD/SR frameworks are evaluated via several\nexperiments, which are implemented through the OpenAI Gym MARL benchmarks.",
          "link": "http://arxiv.org/abs/2112.15156",
          "publishedOn": "2022-01-03T07:15:17.909Z",
          "wordCount": null,
          "title": "Multi-Agent Reinforcement Learning via Adaptive Kalman Temporal Difference and Successor Representation. (arXiv:2112.15156v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15036",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Inzirillo_H/0/1/0/all/0/1\">Hugo Inzirillo</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Mat_B/0/1/0/all/0/1\">Benjamin Mat</a>",
          "description": "The objective of this paper is to assess the performances of dimensionality\nreduction techniques to establish a link between cryptocurrencies. We have\nfocused our analysis on the two most traded cryptocurrencies: Bitcoin and\nEthereum. To perform our analysis, we took log returns and added some\ncovariates to build our data set. We first introduced the pearson correlation\ncoefficient in order to have a preliminary assessment of the link between\nBitcoin and Ethereum. We then reduced the dimension of our data set using\ncanonical correlation analysis and principal component analysis. After\nperforming an analysis of the links between Bitcoin and Ethereum with both\nstatistical techniques, we measured their performance on forecasting Ethereum\nreturns with Bitcoin s features.",
          "link": "http://arxiv.org/abs/2112.15036",
          "publishedOn": "2022-01-03T07:15:17.907Z",
          "wordCount": null,
          "title": "Dimensionality reduction for prediction: Application to Bitcoin and Ethereum. (arXiv:2112.15036v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_F/0/1/0/all/0/1\">Fu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengcai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>",
          "description": "Quantizing deep neural networks (DNNs) has been a promising solution for\ndeploying deep neural networks on embedded devices. However, most of the\nexisting methods do not quantize gradients, and the process of quantizing DNNs\nstill has a lot of floating-point operations, which hinders the further\napplications of quantized DNNs. To solve this problem, we propose a new\nheuristic method based on cooperative coevolution for quantizing DNNs. Under\nthe framework of cooperative coevolution, we use the estimation of distribution\nalgorithm to search for the low-bits weights. Specifically, we first construct\nan initial quantized network from a pre-trained network instead of random\ninitialization and then start searching from it by restricting the search\nspace. So far, the problem is the largest discrete problem known to be solved\nby evolutionary algorithms. Experiments show that our method can train 4 bit\nResNet-20 on the Cifar-10 dataset without sacrificing accuracy.",
          "link": "http://arxiv.org/abs/2112.14834",
          "publishedOn": "2022-01-03T07:15:17.906Z",
          "wordCount": null,
          "title": "Training Quantized Deep Neural Networks via Cooperative Coevolution. (arXiv:2112.14834v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Costa_P/0/1/0/all/0/1\">Pedro Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerqueira_V/0/1/0/all/0/1\">Vitor Cerqueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinagre_J/0/1/0/all/0/1\">Jo&#xe3;o Vinagre</a>",
          "description": "A time series represents a set of observations collected over time.\nTypically, these observations are captured with a uniform sampling frequency\n(e.g. daily). When data points are observed in uneven time intervals the time\nseries is referred to as irregular or intermittent. In such scenarios, the most\ncommon solution is to reconstruct the time series to make it regular, thus\nremoving its intermittency. We hypothesise that, in irregular time series, the\ntime at which each observation is collected may be helpful to summarise the\ndynamics of the data and improve forecasting performance. We study this idea by\ndeveloping a novel automatic feature engineering framework, which focuses on\nextracting information from this point of view, i.e., when each instance is\ncollected. We study how valuable this information is by integrating it in a\ntime series forecasting workflow and investigate how it compares to or\ncomplements state-of-the-art methods for regular time series forecasting. In\nthe end, we contribute by providing a novel framework that tackles feature\nengineering for time series from an angle previously vastly ignored. We show\nthat our approach has the potential to further extract more information about\ntime series that significantly improves forecasting performance.",
          "link": "http://arxiv.org/abs/2112.14806",
          "publishedOn": "2022-01-03T07:15:17.903Z",
          "wordCount": null,
          "title": "AutoFITS: Automatic Feature Engineering for Irregular Time Series. (arXiv:2112.14806v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15121",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Galanti_T/0/1/0/all/0/1\">Tomer Galanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyorgy_A/0/1/0/all/0/1\">Andr&#xe1;s Gy&#xf6;rgy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1\">Marcus Hutter</a>",
          "description": "We study the ability of foundation models to learn representations for\nclassification that are transferable to new, unseen classes. Recent results in\nthe literature show that representations learned by a single classifier over\nmany classes are competitive on few-shot learning problems with representations\nlearned by special-purpose algorithms designed for such problems. In this paper\nwe provide an explanation for this behavior based on the recently observed\nphenomenon that the features learned by overparameterized classification\nnetworks show an interesting clustering property, called neural collapse. We\ndemonstrate both theoretically and empirically that neural collapse generalizes\nto new samples from the training classes, and -- more importantly -- to new\nclasses as well, allowing foundation models to provide feature maps that work\nwell in transfer learning and, specifically, in the few-shot setting.",
          "link": "http://arxiv.org/abs/2112.15121",
          "publishedOn": "2022-01-03T07:15:17.901Z",
          "wordCount": null,
          "title": "On the Role of Neural Collapse in Transfer Learning. (arXiv:2112.15121v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1\">Jorge F. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tobar_F/0/1/0/all/0/1\">Felipe Tobar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vicuna_M/0/1/0/all/0/1\">Mario Vicu&#xf1;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordova_F/0/1/0/all/0/1\">Felipe Cordova</a>",
          "description": "Information-theoretic measures have been widely adopted in the design of\nfeatures for learning and decision problems. Inspired by this, we look at the\nrelationship between i) a weak form of information loss in the Shannon sense\nand ii) the operation loss in the minimum probability of error (MPE) sense when\nconsidering a family of lossy continuous representations (features) of a\ncontinuous observation. We present several results that shed light on this\ninterplay. Our first result offers a lower bound on a weak form of information\nloss as a function of its respective operation loss when adopting a discrete\nlossy representation (quantization) instead of the original raw observation.\nFrom this, our main result shows that a specific form of vanishing information\nloss (a weak notion of asymptotic informational sufficiency) implies a\nvanishing MPE loss (or asymptotic operational sufficiency) when considering a\ngeneral family of lossy continuous representations. Our theoretical findings\nsupport the observation that the selection of feature representations that\nattempt to capture informational sufficiency is appropriate for learning, but\nthis selection is a rather conservative design principle if the intended goal\nis achieving MPE in classification. Supporting this last point, and under some\nstructural conditions, we show that it is possible to adopt an alternative\nnotion of informational sufficiency (strictly weaker than pure sufficiency in\nthe mutual information sense) to achieve operational sufficiency in learning.",
          "link": "http://arxiv.org/abs/2112.15238",
          "publishedOn": "2022-01-03T07:15:17.890Z",
          "wordCount": null,
          "title": "Studying the Interplay between Information Loss and Operation Loss in Representations for Classification. (arXiv:2112.15238v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyong Wang</a>",
          "description": "In the last decade, many deep learning models have been well trained and made\na great success in various fields of machine intelligence, especially for\ncomputer vision and natural language processing. To better leverage the\npotential of these well-trained models in intra-domain or cross-domain transfer\nlearning situations, knowledge distillation (KD) and domain adaptation (DA) are\nproposed and become research highlights. They both aim to transfer useful\ninformation from a well-trained model with original training data. However, the\noriginal data is not always available in many cases due to privacy, copyright\nor confidentiality. Recently, the data-free knowledge transfer paradigm has\nattracted appealing attention as it deals with distilling valuable knowledge\nfrom well-trained models without requiring to access to the training data. In\nparticular, it mainly consists of the data-free knowledge distillation (DFKD)\nand source data-free domain adaptation (SFDA). On the one hand, DFKD aims to\ntransfer the intra-domain knowledge of original data from a cumbersome teacher\nnetwork to a compact student network for model compression and efficient\ninference. On the other hand, the goal of SFDA is to reuse the cross-domain\nknowledge stored in a well-trained source model and adapt it to a target\ndomain. In this paper, we provide a comprehensive survey on data-free knowledge\ntransfer from the perspectives of knowledge distillation and unsupervised\ndomain adaptation, to help readers have a better understanding of the current\nresearch status and ideas. Applications and challenges of the two areas are\nbriefly reviewed, respectively. Furthermore, we provide some insights to the\nsubject of future research.",
          "link": "http://arxiv.org/abs/2112.15278",
          "publishedOn": "2022-01-03T07:15:17.884Z",
          "wordCount": null,
          "title": "Data-Free Knowledge Transfer: A Survey. (arXiv:2112.15278v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karamouzas_I/0/1/0/all/0/1\">Ioannis Karamouzas</a>",
          "description": "We present a simple and intuitive approach for interactive control of\nphysically simulated characters. Our work builds upon generative adversarial\nnetworks (GAN) and reinforcement learning, and introduces an imitation learning\nframework where an ensemble of classifiers and an imitation policy are trained\nin tandem given pre-processed reference clips. The classifiers are trained to\ndiscriminate the reference motion from the motion generated by the imitation\npolicy, while the policy is rewarded for fooling the discriminators. Using our\nGAN-based approach, multiple motor control policies can be trained separately\nto imitate different behaviors. In runtime, our system can respond to external\ncontrol signal provided by the user and interactively switch between different\npolicies. Compared to existing methods, our proposed approach has the following\nattractive properties: 1) achieves state-of-the-art imitation performance\nwithout manually designing and fine tuning a reward function; 2) directly\ncontrols the character without having to track any target reference pose\nexplicitly or implicitly through a phase state; and 3) supports interactive\npolicy switching without requiring any motion generation or motion matching\nmechanism. We highlight the applicability of our approach in a range of\nimitation and interactive control tasks, while also demonstrating its ability\nto withstand external perturbations as well as to recover balance. Overall, our\napproach generates high-fidelity motion, has low runtime cost, and can be\neasily integrated into interactive applications and games.",
          "link": "http://arxiv.org/abs/2105.10066",
          "publishedOn": "2022-01-03T07:15:17.883Z",
          "wordCount": null,
          "title": "A GAN-Like Approach for Physics-Based Imitation Learning and Interactive Character Control. (arXiv:2105.10066v4 [cs.GR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15303",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zang_H/0/1/0/all/0/1\">Hongyu Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingzhong Wang</a>",
          "description": "This work explores how to learn robust and generalizable state representation\nfrom image-based observations with deep reinforcement learning methods.\nAddressing the computational complexity, stringent assumptions, and\nrepresentation collapse challenges in the existing work of bisimulation metric,\nwe devise Simple State Representation (SimSR) operator, which achieves\nequivalent functionality while reducing the complexity by an order in\ncomparison with bisimulation metric. SimSR enables us to design a\nstochastic-approximation-based method that can practically learn the mapping\nfunctions (encoders) from observations to latent representation space. Besides\nthe theoretical analysis, we experimented and compared our work with recent\nstate-of-the-art solutions in visual MuJoCo tasks. The results show that our\nmodel generally achieves better performance and has better robustness and good\ngeneralization.",
          "link": "http://arxiv.org/abs/2112.15303",
          "publishedOn": "2022-01-03T07:15:17.870Z",
          "wordCount": null,
          "title": "SimSR: Simple Distance-based State Representation for Deep Reinforcement Learning. (arXiv:2112.15303v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rachkovskij_D/0/1/0/all/0/1\">Dmitri A. Rachkovskij</a>",
          "description": "Hyperdimensional Computing (HDC), also known as Vector-Symbolic Architectures\n(VSA), is a promising framework for the development of cognitive architectures\nand artificial intelligence systems, as well as for technical applications and\nemerging neuromorphic and nanoscale hardware. HDC/VSA operate with\nhypervectors, i.e., distributed vector representations of large fixed dimension\n(usually > 1000). One of the key ingredients of HDC/VSA are the methods for\nencoding data of various types (from numeric scalars and vectors to graphs)\ninto hypervectors. In this paper, we propose an approach for the formation of\nhypervectors of sequences that provides both an equivariance with respect to\nthe shift of sequences and preserves the similarity of sequences with identical\nelements at nearby positions. Our methods represent the sequence elements by\ncompositional hypervectors and exploit permutations of hypervectors for\nrepresenting the order of sequence elements. We experimentally explored the\nproposed representations using a diverse set of tasks with data in the form of\nsymbolic strings. Although our approach is feature-free as it forms the\nhypervector of a sequence from the hypervectors of its symbols at their\npositions, it demonstrated the performance on a par with the methods that apply\nvarious features, such as subsequences. The proposed techniques were designed\nfor the HDC/VSA model known as Sparse Binary Distributed Representations.\nHowever, they can be adapted to hypervectors in formats of other HDC/VSA\nmodels, as well as for representing sequences of types other than symbolic\nstrings.",
          "link": "http://arxiv.org/abs/2112.15475",
          "publishedOn": "2022-01-03T07:15:17.869Z",
          "wordCount": null,
          "title": "Shift-Equivariant Similarity-Preserving Hypervector Representations of Sequences. (arXiv:2112.15475v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2101.01041",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zhang_K/0/1/0/all/0/1\">Kaiqing Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyuan Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hu_B/0/1/0/all/0/1\">Bin Hu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Basar_T/0/1/0/all/0/1\">Tamer Ba&#x15f;ar</a>",
          "description": "Direct policy search serves as one of the workhorses in modern reinforcement\nlearning (RL), and its applications in continuous control tasks have recently\nattracted increasing attention. In this work, we investigate the convergence\ntheory of policy gradient (PG) methods for learning the linear risk-sensitive\nand robust controller. In particular, we develop PG methods that can be\nimplemented in a derivative-free fashion by sampling system trajectories, and\nestablish both global convergence and sample complexity results in the\nsolutions of two fundamental settings in risk-sensitive and robust control: the\nfinite-horizon linear exponential quadratic Gaussian, and the finite-horizon\nlinear-quadratic disturbance attenuation problems. As a by-product, our results\nalso provide the first sample complexity for the global convergence of PG\nmethods on solving zero-sum linear-quadratic dynamic games, a\nnonconvex-nonconcave minimax optimization problem that serves as a baseline\nsetting in multi-agent reinforcement learning (MARL) with continuous spaces.\nOne feature of our algorithms is that during the learning phase, a certain\nlevel of robustness/risk-sensitivity of the controller is preserved, which we\ntermed as the implicit regularization property, and is an essential requirement\nin safety-critical control systems.",
          "link": "http://arxiv.org/abs/2101.01041",
          "publishedOn": "2022-01-03T07:15:17.869Z",
          "wordCount": null,
          "title": "Derivative-Free Policy Optimization for Linear Risk-Sensitive and Robust Control Design: Implicit Regularization and Sample Complexity. (arXiv:2101.01041v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.14949",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>",
          "description": "In this paper, we focus on the decentralized optimization problem over the\nStiefel manifold, which is defined on a connected network of $d$ agents. The\nobjective is an average of $d$ local functions, and each function is privately\nheld by an agent and encodes its data. The agents can only communicate with\ntheir neighbors in a collaborative effort to solve this problem. In existing\nmethods, multiple rounds of communications are required to guarantee the\nconvergence, giving rise to high communication costs. In contrast, this paper\nproposes a decentralized algorithm, called DESTINY, which only invokes a single\nround of communications per iteration. DESTINY combines gradient tracking\ntechniques with a novel approximate augmented Lagrangian function. The global\nconvergence to stationary points is rigorously established. Comprehensive\nnumerical experiments demonstrate that DESTINY has a strong potential to\ndeliver a cutting-edge performance in solving a variety of testing problems.",
          "link": "http://arxiv.org/abs/2112.14949",
          "publishedOn": "2022-01-03T07:15:17.868Z",
          "wordCount": null,
          "title": "Decentralized Optimization Over the Stiefel Manifold by an Approximate Augmented Lagrangian Function. (arXiv:2112.14949v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Saurav Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loustaunau_P/0/1/0/all/0/1\">Philippe Loustaunau</a>",
          "description": "Rhetorical Frames in AI can be thought of as expressions that describe AI\ndevelopment as a competition between two or more actors, such as governments or\ncompanies. Examples of such Frames include robotic arms race, AI rivalry,\ntechnological supremacy, cyberwarfare dominance and 5G race. Detection of\nRhetorical Frames from open sources can help us track the attitudes of\ngovernments or companies towards AI, specifically whether attitudes are\nbecoming more cooperative or competitive over time. Given the rapidly\nincreasing volumes of open sources (online news media, twitter, blogs), it is\ndifficult for subject matter experts to identify Rhetorical Frames in (near)\nreal-time. Moreover, these sources are in general unstructured (noisy) and\ntherefore, detecting Frames from these sources will require state-of-the-art\ntext classification techniques. In this paper, we develop RheFrameDetect, a\ntext classification system for (near) real-time capture of Rhetorical Frames\nfrom open sources. Given an input document, RheFrameDetect employs text\nclassification techniques at multiple levels (document level and paragraph\nlevel) to identify all occurrences of Frames used in the discussion of AI. We\nperformed extensive evaluation of the text classification techniques used in\nRheFrameDetect against human annotated Frames from multiple news sources. To\nfurther demonstrate the effectiveness of RheFrameDetect, we show multiple case\nstudies depicting the Frames identified by RheFrameDetect compared against\nhuman annotated Frames.",
          "link": "http://arxiv.org/abs/2112.14933",
          "publishedOn": "2022-01-03T07:15:17.862Z",
          "wordCount": null,
          "title": "RheFrameDetect: A Text Classification System for Automatic Detection of Rhetorical Frames in AI from Open Sources. (arXiv:2112.14933v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zafar_M/0/1/0/all/0/1\">Muhammad I. Zafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiequn Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xu-Hui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Heng Xiao</a>",
          "description": "Partial differential equations (PDEs) play a dominant role in the\nmathematical modeling of many complex dynamical processes. Solving these PDEs\noften requires prohibitively high computational costs, especially when multiple\nevaluations must be made for different parameters or conditions. After\ntraining, neural operators can provide PDEs solutions significantly faster than\ntraditional PDE solvers. In this work, invariance properties and computational\ncomplexity of two neural operators are examined for transport PDE of a scalar\nquantity. Neural operator based on graph kernel network (GKN) operates on\ngraph-structured data to incorporate nonlocal dependencies. Here we propose a\nmodified formulation of GKN to achieve frame invariance. Vector cloud neural\nnetwork (VCNN) is an alternate neural operator with embedded frame invariance\nwhich operates on point cloud data. GKN-based neural operator demonstrates\nslightly better predictive performance compared to VCNN. However, GKN requires\nan excessively high computational cost that increases quadratically with the\nincreasing number of discretized objects as compared to a linear increase for\nVCNN.",
          "link": "http://arxiv.org/abs/2112.14769",
          "publishedOn": "2022-01-03T07:15:17.854Z",
          "wordCount": null,
          "title": "Frame invariance and scalability of neural operators for partial differential equations. (arXiv:2112.14769v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14868",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+So_B/0/1/0/all/0/1\">Banghee So</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Valdez_E/0/1/0/all/0/1\">Emiliano A. Valdez</a>",
          "description": "Classification predictive modeling involves the accurate assignment of\nobservations in a dataset to target classes or categories. There is an\nincreasing growth of real-world classification problems with severely\nimbalanced class distributions. In this case, minority classes have much fewer\nobservations to learn from than those from majority classes. Despite this\nsparsity, a minority class is often considered the more interesting class yet\ndeveloping a scientific learning algorithm suitable for the observations\npresents countless challenges. In this article, we suggest a novel multi-class\nclassification algorithm specialized to handle severely imbalanced classes\nbased on the method we refer to as SAMME.C2. It blends the flexible mechanics\nof the boosting techniques from SAMME algorithm, a multi-class classifier, and\nAda.C2 algorithm, a cost-sensitive binary classifier designed to address highly\nclass imbalances. Not only do we provide the resulting algorithm but we also\nestablish scientific and statistical formulation of our proposed SAMME.C2\nalgorithm. Through numerical experiments examining various degrees of\nclassifier difficulty, we demonstrate consistent superior performance of our\nproposed model.",
          "link": "http://arxiv.org/abs/2112.14868",
          "publishedOn": "2022-01-03T07:15:17.854Z",
          "wordCount": null,
          "title": "The SAMME.C2 algorithm for severely imbalanced multi-class classification. (arXiv:2112.14868v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15383",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Seroussi_I/0/1/0/all/0/1\">Inbar Seroussi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ringel_Z/0/1/0/all/0/1\">Zohar Ringel</a>",
          "description": "Deep neural networks (DNNs) are powerful tools for compressing and distilling\ninformation. Due to their scale and complexity, often involving billions of\ninter-dependent internal degrees of freedom, exact analysis approaches often\nfall short. A common strategy in such cases is to identify slow degrees of\nfreedom that average out the erratic behavior of the underlying fast\nmicroscopic variables. Here, we identify such a separation of scales occurring\nin over-parameterized deep convolutional neural networks (CNNs) at the end of\ntraining. It implies that neuron pre-activations fluctuate in a nearly Gaussian\nmanner with a deterministic latent kernel. While for CNNs with infinitely many\nchannels these kernels are inert, for finite CNNs they adapt and learn from\ndata in an analytically tractable manner. The resulting thermodynamic theory of\ndeep learning yields accurate predictions on several deep non-linear CNN toy\nmodels. In addition, it provides new ways of analyzing and understanding CNNs.",
          "link": "http://arxiv.org/abs/2112.15383",
          "publishedOn": "2022-01-03T07:15:17.848Z",
          "wordCount": null,
          "title": "Separation of scales and a thermodynamic description of feature learning in some CNNs. (arXiv:2112.15383v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1\">Yongduo Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiancan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>",
          "description": "Learning powerful representations is one central theme of graph neural\nnetworks (GNNs). It requires refining the critical information from the input\ngraph, instead of the trivial patterns, to enrich the representations. Towards\nthis end, graph attention and pooling methods prevail. They mostly follow the\nparadigm of \"learning to attend\". It maximizes the mutual information between\nthe attended subgraph and the ground-truth label. However, this training\nparadigm is prone to capture the spurious correlations between the trivial\nsubgraph and the label. Such spurious correlations are beneficial to\nin-distribution (ID) test evaluations, but cause poor generalization in the\nout-of-distribution (OOD) test data. In this work, we revisit the GNN modeling\nfrom the causal perspective. On the top of our causal assumption, the trivial\ninformation serves as a confounder between the critical information and the\nlabel, which opens a backdoor path between them and makes them spuriously\ncorrelated. Hence, we present a new paradigm of deconfounded training (DTP)\nthat better mitigates the confounding effect and latches on the critical\ninformation, to enhance the representation and generalization ability.\nSpecifically, we adopt the attention modules to disentangle the critical\nsubgraph and trivial subgraph. Then we make each critical subgraph fairly\ninteract with diverse trivial subgraphs to achieve a stable prediction. It\nallows GNNs to capture a more reliable subgraph whose relation with the label\nis robust across different distributions. We conduct extensive experiments on\nsynthetic and real-world datasets to demonstrate the effectiveness.",
          "link": "http://arxiv.org/abs/2112.15089",
          "publishedOn": "2022-01-03T07:15:17.821Z",
          "wordCount": null,
          "title": "Deconfounded Training for Graph Neural Networks. (arXiv:2112.15089v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samani_A/0/1/0/all/0/1\">Amir Samani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1\">Richard S. Sutton</a>",
          "description": "Learning continually and online from a continuous stream of data is\nchallenging, especially for a reinforcement learning agent with sequential\ndata. When the environment only provides observations giving partial\ninformation about the state of the environment, the agent must learn the agent\nstate based on the data stream of experience. We refer to the state learned\ndirectly from the data stream of experience as the agent state. Recurrent\nneural networks can learn the agent state, but the training methods are\ncomputationally expensive and sensitive to the hyper-parameters, making them\nunideal for online learning. This work introduces methods based on the\ngenerate-and-test approach to learn the agent state. A generate-and-test\nalgorithm searches for state features by generating features and testing their\nusefulness. In this process, features useful for the agent's performance on the\ntask are preserved, and the least useful features get replaced with newly\ngenerated features. We study the effectiveness of our methods on two online\nmulti-step prediction problems. The first problem, trace conditioning, focuses\non the agent's ability to remember a cue for a prediction multiple steps into\nthe future. In the second problem, trace patterning, the agent needs to learn\npatterns in the observation signals and remember them for future predictions.\nWe show that our proposed methods can effectively learn the agent state online\nand produce accurate predictions.",
          "link": "http://arxiv.org/abs/2112.15236",
          "publishedOn": "2022-01-03T07:15:17.821Z",
          "wordCount": null,
          "title": "Learning Agent State Online with Recurrent Generate-and-Test. (arXiv:2112.15236v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15550",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Irie_K/0/1/0/all/0/1\">Kazuki Irie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlag_I/0/1/0/all/0/1\">Imanol Schlag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Csordas_R/0/1/0/all/0/1\">R&#xf3;bert Csord&#xe1;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1\">J&#xfc;rgen Schmidhuber</a>",
          "description": "We share our experience with the recently released WILDS benchmark, a\ncollection of ten datasets dedicated to developing models and training\nstrategies which are robust to domain shifts. Several experiments yield a\ncouple of critical observations which we believe are of general interest for\nany future work on WILDS. Our study focuses on two datasets: iWildCam and FMoW.\nWe show that (1) Conducting separate cross-validation for each evaluation\nmetric is crucial for both datasets, (2) A weak correlation between validation\nand test performance might make model development difficult for iWildCam, (3)\nMinor changes in the training of hyper-parameters improve the baseline by a\nrelatively large margin (mainly on FMoW), (4) There is a strong correlation\nbetween certain domains and certain target labels (mainly on iWildCam). To the\nbest of our knowledge, no prior work on these datasets has reported these\nobservations despite their obvious importance. Our code is public.",
          "link": "http://arxiv.org/abs/2112.15550",
          "publishedOn": "2022-01-03T07:15:17.821Z",
          "wordCount": null,
          "title": "Improving Baselines in the Wild. (arXiv:2112.15550v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinghui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "\"Benign overfitting\", where classifiers memorize noisy training data yet\nstill achieve a good generalization performance, has drawn great attention in\nthe machine learning community. To explain this surprising phenomenon, a series\nof works have provided theoretical justification in over-parameterized linear\nregression, classification, and kernel methods. However, it is not clear if\nbenign overfitting still occurs in the presence of adversarial examples, i.e.,\nexamples with tiny and intentional perturbations to fool the classifiers. In\nthis paper, we show that benign overfitting indeed occurs in adversarial\ntraining, a principled approach to defend against adversarial examples. In\ndetail, we prove the risk bounds of the adversarially trained linear classifier\non the mixture of sub-Gaussian data under $\\ell_p$ adversarial perturbations.\nOur result suggests that under moderate perturbations, adversarially trained\nlinear classifiers can achieve the near-optimal standard and adversarial risks,\ndespite overfitting the noisy training data. Numerical experiments validate our\ntheoretical findings.",
          "link": "http://arxiv.org/abs/2112.15250",
          "publishedOn": "2022-01-03T07:15:17.820Z",
          "wordCount": null,
          "title": "Benign Overfitting in Adversarially Robust Linear Classification. (arXiv:2112.15250v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_L/0/1/0/all/0/1\">Levent Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>",
          "description": "Despite recent advances in modern machine learning algorithms, the opaqueness\nof their underlying mechanisms continues to be an obstacle in adoption. To\ninstill confidence and trust in artificial intelligence systems, Explainable\nArtificial Intelligence has emerged as a response to improving modern machine\nlearning algorithms' explainability. Inductive Logic Programming (ILP), a\nsubfield of symbolic artificial intelligence, plays a promising role in\ngenerating interpretable explanations because of its intuitive logic-driven\nframework. ILP effectively leverages abductive reasoning to generate\nexplainable first-order clausal theories from examples and background\nknowledge. However, several challenges in developing methods inspired by ILP\nneed to be addressed for their successful application in practice. For example,\nexisting ILP systems often have a vast solution space, and the induced\nsolutions are very sensitive to noises and disturbances. This survey paper\nsummarizes the recent advances in ILP and a discussion of statistical\nrelational learning and neural-symbolic algorithms, which offer synergistic\nviews to ILP. Following a critical review of the recent advances, we delineate\nobserved challenges and highlight potential avenues of further ILP-motivated\nresearch toward developing self-explanatory artificial intelligence systems.",
          "link": "http://arxiv.org/abs/2112.15319",
          "publishedOn": "2022-01-03T07:15:17.810Z",
          "wordCount": null,
          "title": "A Critical Review of Inductive Logic Programming Techniques for Explainable AI. (arXiv:2112.15319v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zabihi_S/0/1/0/all/0/1\">Soheil Zabihi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimian_E/0/1/0/all/0/1\">Elahe Rahimian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marefat_F/0/1/0/all/0/1\">Fatemeh Marefat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_A/0/1/0/all/0/1\">Amir Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohseni_P/0/1/0/all/0/1\">Pedram Mohseni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_A/0/1/0/all/0/1\">Arash Mohammadi</a>",
          "description": "Objective: The paper focuses on development of robust and accurate processing\nsolutions for continuous and cuff-less blood pressure (BP) monitoring. In this\nregard, a robust deep learning-based framework is proposed for computation of\nlow latency, continuous, and calibration-free upper and lower bounds on the\nsystolic and diastolic BP. Method: Referred to as the BP-Net, the proposed\nframework is a novel convolutional architecture that provides longer effective\nmemory while achieving superior performance due to incorporation of casual\ndialated convolutions and residual connections. To utilize the real potential\nof deep learning in extraction of intrinsic features (deep features) and\nenhance the long-term robustness, the BP-Net uses raw Electrocardiograph (ECG)\nand Photoplethysmograph (PPG) signals without extraction of any form of\nhand-crafted features as it is common in existing solutions. Results: By\ncapitalizing on the fact that datasets used in recent literature are not\nunified and properly defined, a benchmark dataset is constructed from the\nMIMIC-I and MIMIC-III databases obtained from PhysioNet. The proposed BP-Net is\nevaluated based on this benchmark dataset demonstrating promising performance\nand shows superior generalizable capacity. Conclusion: The proposed BP-Net\narchitecture is more accurate than canonical recurrent networks and enhances\nthe long-term robustness of the BP estimation task. Significance: The proposed\nBP-Net architecture addresses key drawbacks of existing BP estimation\nsolutions, i.e., relying heavily on extraction of hand-crafted features, such\nas pulse arrival time (PAT), and; Lack of robustness. Finally, the constructed\nBP-Net dataset provides a unified base for evaluation and comparison of deep\nlearning-based BP estimation algorithms.",
          "link": "http://arxiv.org/abs/2112.15271",
          "publishedOn": "2022-01-03T07:15:17.809Z",
          "wordCount": null,
          "title": "BP-Net: Cuff-less, Calibration-free, and Non-invasive Blood Pressure Estimation via a Generic Deep Convolutional Architecture. (arXiv:2112.15271v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15280",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1\">Saiping Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Long Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fujun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yutao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaolong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>",
          "description": "Besides entity-centric knowledge, usually organized as Knowledge Graph (KG),\nevents are also an essential kind of knowledge in the world, which trigger the\nspring up of event-centric knowledge representation form like Event KG (EKG).\nIt plays an increasingly important role in many machine learning and artificial\nintelligence applications, such as intelligent search, question-answering,\nrecommendation, and text generation. This paper provides a comprehensive survey\nof EKG from history, ontology, instance, and application views. Specifically,\nto characterize EKG thoroughly, we focus on its history, definitions, schema\ninduction, acquisition, related representative graphs/systems, and\napplications. The development processes and trends are studied therein. We\nfurther summarize perspective directions to facilitate future research on EKG.",
          "link": "http://arxiv.org/abs/2112.15280",
          "publishedOn": "2022-01-03T07:15:17.809Z",
          "wordCount": null,
          "title": "What is Event Knowledge Graph: A Survey. (arXiv:2112.15280v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heiss_J/0/1/0/all/0/1\">Jakob Heiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teichmann_J/0/1/0/all/0/1\">Josef Teichmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wutte_H/0/1/0/all/0/1\">Hanna Wutte</a>",
          "description": "We prove in this paper that wide ReLU neural networks (NNs) with at least one\nhidden layer optimized with l2-regularization on the parameters enforces\nmulti-task learning due to representation-learning - also in the limit width to\ninfinity. This is in contrast to multiple other idealized settings discussed in\nthe literature where wide (ReLU)-NNs loose their ability to benefit from\nmulti-task learning in the limit width to infinity. We deduce the multi-task\nlearning ability from proving an exact quantitative macroscopic\ncharacterization of the learned NN in function space.",
          "link": "http://arxiv.org/abs/2112.15577",
          "publishedOn": "2022-01-03T07:15:17.804Z",
          "wordCount": null,
          "title": "Infinite wide (finite depth) Neural Networks benefit from multi-task learning unlike shallow Gaussian Processes -- an exact quantitative macroscopic characterization. (arXiv:2112.15577v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassanaly_M/0/1/0/all/0/1\">Malik Hassanaly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaws_A/0/1/0/all/0/1\">Andrew Glaws</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_R/0/1/0/all/0/1\">Ryan N. King</a>",
          "description": "Designing manufacturing processes with high yield and strong reliability\nrelies on effective methods for rare event estimation. Genealogical importance\nsplitting reduces the variance of rare event probability estimators by\niteratively selecting and replicating realizations that are headed towards a\nrare event. The replication step is difficult when applied to deterministic\nsystems where the initial conditions of the offspring realizations need to be\nmodified. Typically, a random perturbation is applied to the offspring to\ndifferentiate their trajectory from the parent realization. However, this\nrandom perturbation strategy may be effective for some systems while failing\nfor others, preventing variance reduction in the probability estimate. This\nwork seeks to address this limitation using a generative model such as a\nGenerative Adversarial Network (GAN) to generate perturbations that are\nconsistent with the attractor of the dynamical system. The proposed\nGAN-assisted Importance SPlitting method (GANISP) improves the variance\nreduction for the system targeted. An implementation of the method is available\nin a companion repository (https://github.com/NREL/GANISP).",
          "link": "http://arxiv.org/abs/2112.15444",
          "publishedOn": "2022-01-03T07:15:17.800Z",
          "wordCount": null,
          "title": "GANISP: a GAN-assisted Importance SPlitting Probability Estimator. (arXiv:2112.15444v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15094",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Faradonbeh_M/0/1/0/all/0/1\">Mohamad Kazem Shirani Faradonbeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Faradonbeh_M/0/1/0/all/0/1\">Mohamad Sadegh Shirani Faradonbeh</a>",
          "description": "Linear dynamical systems are canonical models for learning-based control of\nplants with uncertain dynamics. The setting consists of a stochastic\ndifferential equation that captures the state evolution of the plant\nunderstudy, while the true dynamics matrices are unknown and need to be learned\nfrom the observed data of state trajectory. An important issue is to ensure\nthat the system is stabilized and destabilizing control actions due to model\nuncertainties are precluded as soon as possible. A reliable stabilization\nprocedure for this purpose that can effectively learn from unstable data to\nstabilize the system in a finite time is not currently available. In this work,\nwe propose a novel Bayesian learning algorithm that stabilizes unknown\ncontinuous-time stochastic linear systems. The presented algorithm is flexible\nand exposes effective stabilization performance after a remarkably short time\nperiod of interacting with the system.",
          "link": "http://arxiv.org/abs/2112.15094",
          "publishedOn": "2022-01-03T07:15:17.785Z",
          "wordCount": null,
          "title": "Bayesian Algorithms Learn to Stabilize Unknown Continuous-Time Systems. (arXiv:2112.15094v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1\">Deven Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghate_P/0/1/0/all/0/1\">Pinak Ghate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paranjape_M/0/1/0/all/0/1\">Manali Paranjape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Amit Kumar</a>",
          "description": "The current work intends to study the performance of the Hierarchical\nTemporal Memory(HTM) theory for automated classification of text as well as\ndocuments. HTM is a biologically inspired theory based on the working\nprinciples of the human neocortex. The current study intends to provide an\nalternative framework for document categorization using the Spatial Pooler\nlearning algorithm in the HTM Theory. As HTM accepts only a stream of binary\ndata as input, Latent Semantic Indexing(LSI) technique is used for extracting\nthe top features from the input and converting them into binary format. The\nSpatial Pooler algorithm converts the binary input into sparse patterns with\nsimilar input text having overlapping spatial patterns making it easy for\nclassifying the patterns into categories. The results obtained prove that HTM\ntheory, although is in its nascent stages, performs at par with most of the\npopular machine learning based classifiers.",
          "link": "http://arxiv.org/abs/2112.14820",
          "publishedOn": "2022-01-03T07:15:17.723Z",
          "wordCount": null,
          "title": "Application of Hierarchical Temporal Memory Theory for Document Categorization. (arXiv:2112.14820v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chenlin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huzhang_G/0/1/0/all/0/1\">Guangda Huzhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Da_Q/0/1/0/all/0/1\">Qing Da</a>",
          "description": "To approach different business objectives, online traffic shaping algorithms\naim at improving exposures of a target set of items, such as boosting the\ngrowth of new commodities. Generally, these algorithms assume that the utility\nof each user-item pair can be accessed via a well-trained conversion rate\nprediction model. However, for real E-Commerce platforms, there are unavoidable\nfactors preventing us from learning such an accurate model. In order to break\nthe heavy dependence on accurate inputs of the utility, we propose a general\nonline traffic shaping protocol for online E-Commerce applications. In our\nframework, we approximate the function mapping the bonus scores, which\ngenerally are the only method to influence the ranking result in the traffic\nshaping problem, to the numbers of exposures and purchases. Concretely, we\napproximate the above function by a class of the piece-wise linear function\nconstructed on the convex hull of the explored data points. Moreover, we\nreformulate the online traffic shaping problem as linear programming where\nthese piece-wise linear functions are embedded into both the objective and\nconstraints. Our algorithm can straightforwardly optimize the linear\nprogramming in the prime space, and its solution can be simply applied by a\nstochastic strategy to fulfill the optimized objective and the constraints in\nexpectation. Finally, the online A/B test shows our proposed algorithm steadily\noutperforms the previous industrial level traffic shaping algorithm.",
          "link": "http://arxiv.org/abs/2112.14941",
          "publishedOn": "2022-01-03T07:15:17.723Z",
          "wordCount": null,
          "title": "A General Traffic Shaping Protocol in E-Commerce. (arXiv:2112.14941v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alver_S/0/1/0/all/0/1\">Safa Alver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>",
          "description": "We study the problem of learning a good set of policies, so that when\ncombined together, they can solve a wide variety of unseen reinforcement\nlearning tasks with no or very little new data. Specifically, we consider the\nframework of generalized policy evaluation and improvement, in which the\nrewards for all tasks of interest are assumed to be expressible as a linear\ncombination of a fixed set of features. We show theoretically that, under\ncertain assumptions, having access to a specific set of diverse policies, which\nwe call a set of independent policies, can allow for instantaneously achieving\nhigh-level performance on all possible downstream tasks which are typically\nmore complex than the ones on which the agent was trained. Based on this\ntheoretical analysis, we propose a simple algorithm that iteratively constructs\nthis set of policies. In addition to empirically validating our theoretical\nresults, we compare our approach with recently proposed diverse policy set\nconstruction methods and show that, while others fail, our approach is able to\nbuild a behavior basis that enables instantaneous transfer to all possible\ndownstream tasks. We also show empirically that having access to a set of\nindependent policies can better bootstrap the learning process on downstream\ntasks where the new reward function cannot be described as a linear combination\nof the features. Finally, we demonstrate that this policy set can be useful in\na realistic lifelong reinforcement learning setting.",
          "link": "http://arxiv.org/abs/2112.15025",
          "publishedOn": "2022-01-03T07:15:17.723Z",
          "wordCount": null,
          "title": "Constructing a Good Behavior Basis for Transfer using Generalized Policy Updates. (arXiv:2112.15025v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.00827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weitong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lihong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "Thompson Sampling (TS) is one of the most effective algorithms for solving\ncontextual multi-armed bandit problems. In this paper, we propose a new\nalgorithm, called Neural Thompson Sampling, which adapts deep neural networks\nfor both exploration and exploitation. At the core of our algorithm is a novel\nposterior distribution of the reward, where its mean is the neural network\napproximator, and its variance is built upon the neural tangent features of the\ncorresponding neural network. We prove that, provided the underlying reward\nfunction is bounded, the proposed algorithm is guaranteed to achieve a\ncumulative regret of $\\mathcal{O}(T^{1/2})$, which matches the regret of other\ncontextual bandit algorithms in terms of total round number $T$. Experimental\ncomparisons with other benchmark bandit algorithms on various data sets\ncorroborate our theory.",
          "link": "http://arxiv.org/abs/2010.00827",
          "publishedOn": "2022-01-03T07:15:17.671Z",
          "wordCount": null,
          "title": "Neural Thompson Sampling. (arXiv:2010.00827v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.14804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianghao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianfu Wu</a>",
          "description": "Image synthesis and image recognition have witnessed remarkable progress, but\noften at the expense of computationally expensive training and inference.\nLearning lightweight yet expressive deep model has emerged as an important and\ninteresting direction. Inspired by the well-known split-transform-aggregate\ndesign heuristic in the Inception building block, this paper proposes a\nSkip-Layer Inception Module (SLIM) that facilitates efficient learning of image\nsynthesis models, and a same-layer variant (dubbed as SLIM too) as a stronger\nalternative to the well-known ResNeXts for image recognition. In SLIM, the\ninput feature map is first split into a number of groups (e.g., 4).Each group\nis then transformed to a latent style vector(via channel-wise attention) and a\nlatent spatial mask (via spatial attention). The learned latent masks and\nlatent style vectors are aggregated to modulate the target feature map. For\ngenerative learning, SLIM is built on a recently proposed lightweight\nGenerative Adversarial Networks (i.e., FastGANs) which present a skip-layer\nexcitation(SLE) module. For few-shot image synthesis tasks, the proposed SLIM\nachieves better performance than the SLE work and other related methods. For\none-shot image synthesis tasks, it shows stronger capability of preserving\nimages structures than prior arts such as the SinGANs. For image classification\ntasks, the proposed SLIM is used as a drop-in replacement for convolution\nlayers in ResNets (resulting in ResNeXt-like models) and achieves better\naccuracy in theImageNet-1000 dataset, with significantly smaller model\ncomplexity",
          "link": "http://arxiv.org/abs/2112.14804",
          "publishedOn": "2022-01-03T07:15:17.553Z",
          "wordCount": null,
          "title": "Learning Inception Attention for Image Synthesis and Image Recognition. (arXiv:2112.14804v1 [cs.CV])"
        }
      ]
    },
    {
      "title": "stat.ML updates on arXiv.org",
      "feedUrl": "http://arxiv.org/rss/stat.ML",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2112.14793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Exarchakis_G/0/1/0/all/0/1\">Georgios Exarchakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oubari_O/0/1/0/all/0/1\">Omar Oubari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenz_G/0/1/0/all/0/1\">Gregor Lenz</a>",
          "description": "We propose a simple and efficient clustering method for high-dimensional data\nwith a large number of clusters. Our algorithm achieves high-performance by\nevaluating distances of datapoints with a subset of the cluster centres. Our\ncontribution is substantially more efficient than k-means as it does not\nrequire an all to all comparison of data points and clusters. We show that the\noptimal solutions of our approximation are the same as in the exact solution.\nHowever, our approach is considerably more efficient at extracting these\nclusters compared to the state-of-the-art. We compare our approximation with\nthe exact k-means and alternative approximation approaches on a series of\nstandardised clustering tasks. For the evaluation, we consider the algorithmic\ncomplexity, including number of operations to convergence, and the stability of\nthe results.",
          "link": "http://arxiv.org/abs/2112.14793",
          "publishedOn": "2022-01-03T07:15:23.382Z",
          "wordCount": 574,
          "title": "A sampling-based approach for efficient clustering in large datasets. (arXiv:2112.14793v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinghui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "\"Benign overfitting\", where classifiers memorize noisy training data yet\nstill achieve a good generalization performance, has drawn great attention in\nthe machine learning community. To explain this surprising phenomenon, a series\nof works have provided theoretical justification in over-parameterized linear\nregression, classification, and kernel methods. However, it is not clear if\nbenign overfitting still occurs in the presence of adversarial examples, i.e.,\nexamples with tiny and intentional perturbations to fool the classifiers. In\nthis paper, we show that benign overfitting indeed occurs in adversarial\ntraining, a principled approach to defend against adversarial examples. In\ndetail, we prove the risk bounds of the adversarially trained linear classifier\non the mixture of sub-Gaussian data under $\\ell_p$ adversarial perturbations.\nOur result suggests that under moderate perturbations, adversarially trained\nlinear classifiers can achieve the near-optimal standard and adversarial risks,\ndespite overfitting the noisy training data. Numerical experiments validate our\ntheoretical findings.",
          "link": "http://arxiv.org/abs/2112.15250",
          "publishedOn": "2022-01-03T07:15:23.367Z",
          "wordCount": 576,
          "title": "Benign Overfitting in Adversarially Robust Linear Classification. (arXiv:2112.15250v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15258",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lam_K/0/1/0/all/0/1\">Ka Kin Lam</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>",
          "description": "There have been significant efforts devoted to solving the longevity risk\ngiven that a continuous growth in population ageing has become a severe issue\nfor many developed countries over the past few decades. The Cairns-Blake-Dowd\n(CBD) model, which incorporates cohort effects parameters in its parsimonious\ndesign, is one of the most well-known approaches for mortality modelling at\nhigher ages and longevity risk. This article proposes a novel mixed-effects\ntime-series approach for mortality modelling and forecasting with\nconsiderations of age groups dependence and random cohort effects parameters.\nThe proposed model can disclose more mortality data information and provide a\nnatural quantification of the model parameters uncertainties with no\npre-specified constraint required for estimating the cohort effects parameters.\nThe abilities of the proposed approach are demonstrated through two\napplications with empirical male and female mortality data. The proposed\napproach shows remarkable improvements in terms of forecast accuracy compared\nto the CBD model in the short-, mid-and long-term forecasting using mortality\ndata of several developed countries in the numerical examples.",
          "link": "http://arxiv.org/abs/2112.15258",
          "publishedOn": "2022-01-03T07:15:23.361Z",
          "wordCount": 608,
          "title": "Random cohort effects and age groups dependency structure for mortality modelling and forecasting: Mixed-effects time-series model approach. (arXiv:2112.15258v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/1906.09338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yunhui Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuolin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunter_C/0/1/0/all/0/1\">Carl A. Gunter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Recent advances in machine learning have largely benefited from the massive\naccessible training data. However, large-scale data sharing has raised great\nprivacy concerns. In this work, we propose a novel privacy-preserving data\nGenerative model based on the PATE framework (G-PATE), aiming to train a\nscalable differentially private data generator that preserves high generated\ndata utility. Our approach leverages generative adversarial nets to generate\ndata, combined with private aggregation among different discriminators to\nensure strong privacy guarantees. Compared to existing approaches, G-PATE\nsignificantly improves the use of privacy budgets. In particular, we train a\nstudent data generator with an ensemble of teacher discriminators and propose a\nnovel private gradient aggregation mechanism to ensure differential privacy on\nall information that flows from teacher discriminators to the student\ngenerator. In addition, with random projection and gradient discretization, the\nproposed gradient aggregation mechanism is able to effectively deal with\nhigh-dimensional gradient vectors. Theoretically, we prove that G-PATE ensures\ndifferential privacy for the data generator. Empirically, we demonstrate the\nsuperiority of G-PATE over prior work through extensive experiments. We show\nthat G-PATE is the first work being able to generate high-dimensional image\ndata with high data utility under limited privacy budgets ($\\epsilon \\le 1$).\nOur code is available at https://github.com/AI-secure/G-PATE.",
          "link": "http://arxiv.org/abs/1906.09338",
          "publishedOn": "2022-01-03T07:15:23.354Z",
          "wordCount": 675,
          "title": "G-PATE: Scalable Differentially Private Data Generator via Private Aggregation of Teacher Discriminators. (arXiv:1906.09338v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15401",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Kycia_R/0/1/0/all/0/1\">Rados&#x142;aw A. Kycia</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Niemczynowicz_A/0/1/0/all/0/1\">Agnieszka Niemczynowicz</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Niezurawska_Zajac_J/0/1/0/all/0/1\">Joanna Nie&#x17c;urawska-Zaj&#x105;c</a>",
          "description": "Correlation and cluster analyses (k-Means, Gaussian Mixture Models) were\nperformed on Generation Z engagement surveys at the workplace. The clustering\nindicates relations between various factors that describe the engagement of\nemployees. The most noticeable factors are a clear statement about the\nresponsibilities at work, and challenging work. These factors are essential in\npractice. The results of this paper can be used in preparing better\nmotivational systems aimed at Generation Z employees.",
          "link": "http://arxiv.org/abs/2112.15401",
          "publishedOn": "2022-01-03T07:15:18.154Z",
          "wordCount": null,
          "title": "Towards the global vision of engagement of Generation Z at the workplace: Mathematical modeling. (arXiv:2112.15401v1 [econ.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11612",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiafan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We study reinforcement learning (RL) with linear function approximation.\nExisting algorithms for this problem only have high-probability regret and/or\nProbably Approximately Correct (PAC) sample complexity guarantees, which cannot\nguarantee the convergence to the optimal policy. In this paper, in order to\novercome the limitation of existing algorithms, we propose a new algorithm\ncalled FLUTE, which enjoys uniform-PAC convergence to the optimal policy with\nhigh probability. The uniform-PAC guarantee is the strongest possible guarantee\nfor reinforcement learning in the literature, which can directly imply both PAC\nand high probability regret bounds, making our algorithm superior to all\nexisting algorithms with linear function approximation. At the core of our\nalgorithm is a novel minimax value function estimator and a multi-level\npartition scheme to select the training samples from historical observations.\nBoth of these techniques are new and of independent interest.",
          "link": "http://arxiv.org/abs/2106.11612",
          "publishedOn": "2022-01-03T07:15:17.905Z",
          "wordCount": null,
          "title": "Uniform-PAC Bounds for Reinforcement Learning with Linear Function Approximation. (arXiv:2106.11612v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.03152",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Biswas_N/0/1/0/all/0/1\">Niloy Biswas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1\">Lester Mackey</a>",
          "description": "Markov chain Monte Carlo (MCMC) provides asymptotically consistent estimates\nof intractable posterior expectations as the number of iterations tends to\ninfinity. However, in large data applications, MCMC can be computationally\nexpensive per iteration. This has catalyzed interest in sampling methods such\nas approximate MCMC, which trade off asymptotic consistency for improved\ncomputational speed. In this article, we propose estimators based on couplings\nof Markov chains to assess the quality of such asymptotically biased sampling\nmethods. The estimators give empirical upper bounds of the Wassertein distance\nbetween the limiting distribution of the asymptotically biased sampling method\nand the original target distribution of interest. We establish theoretical\nguarantees for our upper bounds and show that our estimators can remain\neffective in high dimensions. We apply our quality measures to stochastic\ngradient MCMC, variational Bayes, and Laplace approximations for tall data and\nto approximate MCMC for Bayesian logistic regression in 4500 dimensions and\nBayesian linear regression in 50000 dimensions.",
          "link": "http://arxiv.org/abs/2112.03152",
          "publishedOn": "2022-01-03T07:15:17.903Z",
          "wordCount": null,
          "title": "Bounding Wasserstein distance with couplings. (arXiv:2112.03152v2 [stat.CO] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weitong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lihong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "Thompson Sampling (TS) is one of the most effective algorithms for solving\ncontextual multi-armed bandit problems. In this paper, we propose a new\nalgorithm, called Neural Thompson Sampling, which adapts deep neural networks\nfor both exploration and exploitation. At the core of our algorithm is a novel\nposterior distribution of the reward, where its mean is the neural network\napproximator, and its variance is built upon the neural tangent features of the\ncorresponding neural network. We prove that, provided the underlying reward\nfunction is bounded, the proposed algorithm is guaranteed to achieve a\ncumulative regret of $\\mathcal{O}(T^{1/2})$, which matches the regret of other\ncontextual bandit algorithms in terms of total round number $T$. Experimental\ncomparisons with other benchmark bandit algorithms on various data sets\ncorroborate our theory.",
          "link": "http://arxiv.org/abs/2010.00827",
          "publishedOn": "2022-01-03T07:15:17.900Z",
          "wordCount": null,
          "title": "Neural Thompson Sampling. (arXiv:2010.00827v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mengin_E/0/1/0/all/0/1\">Elie Mengin</a> (SAMM), <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1\">Fabrice Rossi</a> (CEREMADE)",
          "description": "In this paper, we present a novel algorithm to address the Network Alignment\nproblem. It is inspired from a previous message passing framework of Bayati et\nal. [2] and includes several modifications designed to significantly speed up\nthe message updates as well as to enforce their convergence. Experiments show\nthat our proposed model outperforms other state-of-the-art solvers. Finally, we\npropose an application of our method in order to address the Binary Diffing\nproblem. We show that our solution provides better assignment than the\nreference differs in almost all submitted instances and outline the importance\nof leveraging the graphical structure of binary programs.",
          "link": "http://arxiv.org/abs/2112.15336",
          "publishedOn": "2022-01-03T07:15:17.890Z",
          "wordCount": null,
          "title": "Improved Algorithm for the Network Alignment Problem with Application to Binary Diffing. (arXiv:2112.15336v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02864",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Reuvers_H/0/1/0/all/0/1\">Hanno Reuvers</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Wijler_E/0/1/0/all/0/1\">Etienne Wijler</a>",
          "description": "We consider a high-dimensional model in which variables are observed over\ntime and space. The model consists of a spatio-temporal regression containing a\ntime lag and a spatial lag of the dependent variable. Unlike classical spatial\nautoregressive models, we do not rely on a predetermined spatial interaction\nmatrix, but infer all spatial interactions from the data. Assuming sparsity, we\nestimate the spatial and temporal dependence fully data-driven by penalizing a\nset of Yule-Walker equations. This regularization can be left unstructured, but\nwe also propose customized shrinkage procedures when observations originate\nfrom spatial grids (e.g. satellite images). Finite sample error bounds are\nderived and estimation consistency is established in an asymptotic framework\nwherein the sample size and the number of spatial units diverge jointly.\nExogenous variables can be included as well. A simulation exercise shows strong\nfinite sample performance compared to competing procedures. As an empirical\napplication, we model satellite measured NO2 concentrations in London. Our\napproach delivers forecast improvements over a competitive benchmark and we\ndiscover evidence for strong spatial interactions.",
          "link": "http://arxiv.org/abs/2108.02864",
          "publishedOn": "2022-01-03T07:15:17.814Z",
          "wordCount": null,
          "title": "Sparse Generalized Yule-Walker Estimation for Large Spatio-temporal Autoregressions with an Application to NO2 Satellite Data. (arXiv:2108.02864v2 [econ.EM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maddox_W/0/1/0/all/0/1\">Wesley J. Maddox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_S/0/1/0/all/0/1\">Sanyam Kapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1\">Andrew Gordon Wilson</a>",
          "description": "While recent work on conjugate gradient methods and Lanczos decompositions\nhave achieved scalable Gaussian process inference with highly accurate point\npredictions, in several implementations these iterative methods appear to\nstruggle with numerical instabilities in learning kernel hyperparameters, and\npoor test likelihoods. By investigating CG tolerance, preconditioner rank, and\nLanczos decomposition rank, we provide a particularly simple prescription to\ncorrect these issues: we recommend that one should use a small CG tolerance\n($\\epsilon \\leq 0.01$) and a large root decomposition size ($r \\geq 5000$).\nMoreover, we show that L-BFGS-B is a compelling optimizer for Iterative GPs,\nachieving convergence with fewer gradient updates.",
          "link": "http://arxiv.org/abs/2112.15246",
          "publishedOn": "2022-01-03T07:15:17.621Z",
          "wordCount": null,
          "title": "When are Iterative Gaussian Processes Reliably Accurate?. (arXiv:2112.15246v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15094",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Faradonbeh_M/0/1/0/all/0/1\">Mohamad Kazem Shirani Faradonbeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Faradonbeh_M/0/1/0/all/0/1\">Mohamad Sadegh Shirani Faradonbeh</a>",
          "description": "Linear dynamical systems are canonical models for learning-based control of\nplants with uncertain dynamics. The setting consists of a stochastic\ndifferential equation that captures the state evolution of the plant\nunderstudy, while the true dynamics matrices are unknown and need to be learned\nfrom the observed data of state trajectory. An important issue is to ensure\nthat the system is stabilized and destabilizing control actions due to model\nuncertainties are precluded as soon as possible. A reliable stabilization\nprocedure for this purpose that can effectively learn from unstable data to\nstabilize the system in a finite time is not currently available. In this work,\nwe propose a novel Bayesian learning algorithm that stabilizes unknown\ncontinuous-time stochastic linear systems. The presented algorithm is flexible\nand exposes effective stabilization performance after a remarkably short time\nperiod of interacting with the system.",
          "link": "http://arxiv.org/abs/2112.15094",
          "publishedOn": "2022-01-03T07:15:17.575Z",
          "wordCount": null,
          "title": "Bayesian Algorithms Learn to Stabilize Unknown Continuous-Time Systems. (arXiv:2112.15094v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2010.01278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinghui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>",
          "description": "Adversarial training is so far the most effective strategy in defending\nagainst adversarial examples. However, it suffers from high computational costs\ndue to the iterative adversarial attacks in each training step. Recent studies\nshow that it is possible to achieve fast Adversarial Training by performing a\nsingle-step attack with random initialization. However, such an approach still\nlags behind state-of-the-art adversarial training algorithms on both stability\nand model robustness. In this work, we develop a new understanding towards Fast\nAdversarial Training, by viewing random initialization as performing randomized\nsmoothing for better optimization of the inner maximization problem. Following\nthis new perspective, we also propose a new initialization strategy, backward\nsmoothing, to further improve the stability and model robustness over\nsingle-step robust training methods. Experiments on multiple benchmarks\ndemonstrate that our method achieves similar model robustness as the original\nTRADES method while using much less training time ($\\sim$3x improvement with\nthe same training schedule).",
          "link": "http://arxiv.org/abs/2010.01278",
          "publishedOn": "2022-01-03T07:15:17.562Z",
          "wordCount": null,
          "title": "Efficient Robust Training via Backward Smoothing. (arXiv:2010.01278v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mengin_E/0/1/0/all/0/1\">Elie Mengin</a> (SAMM), <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1\">Fabrice Rossi</a> (CEREMADE)",
          "description": "In this paper, we address the problem of finding a correspondence, or\nmatching, between the functions of two programs in binary form, which is one of\nthe most common task in binary diffing. We introduce a new formulation of this\nproblem as a particular instance of a graph edit problem over the call graphs\nof the programs. In this formulation, the quality of a mapping is evaluated\nsimultaneously with respect to both function content and call graph\nsimilarities. We show that this formulation is equivalent to a network\nalignment problem. We propose a solving strategy for this problem based on\nmax-product belief propagation. Finally, we implement a prototype of our\nmethod, called QBinDiff, and propose an extensive evaluation which shows that\nour approach outperforms state of the art diffing tools.",
          "link": "http://arxiv.org/abs/2112.15337",
          "publishedOn": "2022-01-03T07:15:17.382Z",
          "wordCount": 579,
          "title": "Binary Diffing as a Network Alignment Problem via Belief Propagation. (arXiv:2112.15337v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.01711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chengchun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shikai Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongtu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jieping Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>",
          "description": "A/B testing, or online experiment is a standard business strategy to compare\na new product with an old one in pharmaceutical, technological, and traditional\nindustries. Major challenges arise in online experiments of two-sided\nmarketplace platforms (e.g., Uber) where there is only one unit that receives a\nsequence of treatments over time. In those experiments, the treatment at a\ngiven time impacts current outcome as well as future outcomes. The aim of this\npaper is to introduce a reinforcement learning framework for carrying A/B\ntesting in these experiments, while characterizing the long-term treatment\neffects. Our proposed testing procedure allows for sequential monitoring and\nonline updating. It is generally applicable to a variety of treatment designs\nin different industries. In addition, we systematically investigate the\ntheoretical properties (e.g., size and power) of our testing procedure.\nFinally, we apply our framework to both simulated data and a real-world data\nexample obtained from a technological company to illustrate its advantage over\nthe current practice. A Python implementation of our test is available at\nhttps://github.com/callmespring/CausalRL.",
          "link": "http://arxiv.org/abs/2002.01711",
          "publishedOn": "2022-01-03T07:15:17.376Z",
          "wordCount": 650,
          "title": "Dynamic Causal Effects Evaluation in A/B Testing with a Reinforcement Learning Framework. (arXiv:2002.01711v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15392",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Benning_F/0/1/0/all/0/1\">Felix Benning</a>",
          "description": "This thesis reviews numerical optimization methods with machine learning\nproblems in mind. Since machine learning models are highly parametrized, we\nfocus on methods suited for high dimensional optimization. We build intuition\non quadratic models to figure out which methods are suited for non-convex\noptimization, and develop convergence proofs on convex functions for this\nselection of methods. With this theoretical foundation for stochastic gradient\ndescent and momentum methods, we try to explain why the methods used commonly\nin the machine learning field are so successful. Besides explaining successful\nheuristics, the last chapter also provides a less extensive review of more\ntheoretical methods, which are not quite as popular in practice. So in some\nsense this work attempts to answer the question: Why are the default Tensorflow\noptimizers included in the defaults?",
          "link": "http://arxiv.org/abs/2112.15392",
          "publishedOn": "2022-01-03T07:15:17.357Z",
          "wordCount": 559,
          "title": "High Dimensional Optimization through the Lens of Machine Learning. (arXiv:2112.15392v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14862",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Jadbabaie_A/0/1/0/all/0/1\">Ali Jadbabaie</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mania_H/0/1/0/all/0/1\">Horia Mania</a>, <a href=\"http://arxiv.org/find/math/1/au:+Shah_D/0/1/0/all/0/1\">Devavrat Shah</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sra_S/0/1/0/all/0/1\">Suvrit Sra</a>",
          "description": "We revisit a model for time-varying linear regression that assumes the\nunknown parameters evolve according to a linear dynamical system.\nCounterintuitively, we show that when the underlying dynamics are stable the\nparameters of this model can be estimated from data by combining just two\nordinary least squares estimates. We offer a finite sample guarantee on the\nestimation error of our method and discuss certain advantages it has over\nExpectation-Maximization (EM), which is the main approach proposed by prior\nwork.",
          "link": "http://arxiv.org/abs/2112.14862",
          "publishedOn": "2022-01-03T07:15:17.349Z",
          "wordCount": 508,
          "title": "Time varying regression with hidden linear dynamics. (arXiv:2112.14862v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1\">Jorge F. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tobar_F/0/1/0/all/0/1\">Felipe Tobar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vicuna_M/0/1/0/all/0/1\">Mario Vicu&#xf1;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordova_F/0/1/0/all/0/1\">Felipe Cordova</a>",
          "description": "Information-theoretic measures have been widely adopted in the design of\nfeatures for learning and decision problems. Inspired by this, we look at the\nrelationship between i) a weak form of information loss in the Shannon sense\nand ii) the operation loss in the minimum probability of error (MPE) sense when\nconsidering a family of lossy continuous representations (features) of a\ncontinuous observation. We present several results that shed light on this\ninterplay. Our first result offers a lower bound on a weak form of information\nloss as a function of its respective operation loss when adopting a discrete\nlossy representation (quantization) instead of the original raw observation.\nFrom this, our main result shows that a specific form of vanishing information\nloss (a weak notion of asymptotic informational sufficiency) implies a\nvanishing MPE loss (or asymptotic operational sufficiency) when considering a\ngeneral family of lossy continuous representations. Our theoretical findings\nsupport the observation that the selection of feature representations that\nattempt to capture informational sufficiency is appropriate for learning, but\nthis selection is a rather conservative design principle if the intended goal\nis achieving MPE in classification. Supporting this last point, and under some\nstructural conditions, we show that it is possible to adopt an alternative\nnotion of informational sufficiency (strictly weaker than pure sufficiency in\nthe mutual information sense) to achieve operational sufficiency in learning.",
          "link": "http://arxiv.org/abs/2112.15238",
          "publishedOn": "2022-01-03T07:15:17.342Z",
          "wordCount": 668,
          "title": "Studying the Interplay between Information Loss and Operation Loss in Representations for Classification. (arXiv:2112.15238v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.07110",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bhattacharya_R/0/1/0/all/0/1\">Riddhiman Bhattacharya</a>",
          "description": "The gradient noise of Stochastic Gradient Descent (SGD) is considered to play\na key role in its properties (e.g. escaping low potential points and\nregularization). Past research has indicated that the covariance of the SGD\nerror done via minibatching plays a critical role in determining its\nregularization and escape from low potential points. It is however not much\nexplored how much the distribution of the error influences the behavior of the\nalgorithm. Motivated by some new research in this area, we prove universality\nresults by showing that noise classes that have the same mean and covariance\nstructure of SGD via minibatching have similar properties. We mainly consider\nthe Multiplicative Stochastic Gradient Descent (M-SGD) algorithm as introduced\nby Wu et al., which has a much more general noise class than the SGD algorithm\ndone via minibatching. We establish nonasymptotic bounds for the M-SGD\nalgorithm mainly with respect to the Stochastic Differential Equation\ncorresponding to SGD via minibatching. We also show that the M-SGD error is\napproximately a scaled Gaussian distribution with mean $0$ at any fixed point\nof the M-SGD algorithm. We also establish bounds for the convergence of the\nM-SGD algorithm in the strongly convex regime.",
          "link": "http://arxiv.org/abs/2112.07110",
          "publishedOn": "2022-01-03T07:15:17.334Z",
          "wordCount": 657,
          "title": "Non Asymptotic Bounds for Optimization via Online Multiplicative Stochastic Gradient Descent. (arXiv:2112.07110v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Astudillo_R/0/1/0/all/0/1\">Raul Astudillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frazier_P/0/1/0/all/0/1\">Peter I. Frazier</a>",
          "description": "We consider Bayesian optimization of the output of a network of functions,\nwhere each function takes as input the output of its parent nodes, and where\nthe network takes significant time to evaluate. Such problems arise, for\nexample, in reinforcement learning, engineering design, and manufacturing.\nWhile the standard Bayesian optimization approach observes only the final\noutput, our approach delivers greater query efficiency by leveraging\ninformation that the former ignores: intermediate output within the network.\nThis is achieved by modeling the nodes of the network using Gaussian processes\nand choosing the points to evaluate using, as our acquisition function, the\nexpected improvement computed with respect to the implied posterior on the\nobjective. Although the non-Gaussian nature of this posterior prevents\ncomputing our acquisition function in closed form, we show that it can be\nefficiently maximized via sample average approximation. In addition, we prove\nthat our method is asymptotically consistent, meaning that it finds a globally\noptimal solution as the number of evaluations grows to infinity, thus\ngeneralizing previously known convergence results for the expected improvement.\nNotably, this holds even though our method might not evaluate the domain\ndensely, instead leveraging problem structure to leave regions unexplored.\nFinally, we show that our approach dramatically outperforms standard Bayesian\noptimization methods in several synthetic and real-world problems.",
          "link": "http://arxiv.org/abs/2112.15311",
          "publishedOn": "2022-01-03T07:15:17.316Z",
          "wordCount": 640,
          "title": "Bayesian Optimization of Function Networks. (arXiv:2112.15311v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.08250",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Anderer_M/0/1/0/all/0/1\">Matthias Anderer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>",
          "description": "Hierarchical forecasting with intermittent time series is a challenge in both\nresearch and empirical studies. Extensive research focuses on improving the\naccuracy of each hierarchy, especially the intermittent time series at bottom\nlevels. Then hierarchical reconciliation could be used to improve the overall\nperformance further. In this paper, we present a\n\\emph{hierarchical-forecasting-with-alignment} approach that treats the bottom\nlevel forecasts as mutable to ensure higher forecasting accuracy on the upper\nlevels of the hierarchy. We employ a pure deep learning forecasting approach\nN-BEATS for continuous time series at the top levels and a widely used\ntree-based algorithm LightGBM for the intermittent time series at the bottom\nlevel. The \\emph{hierarchical-forecasting-with-alignment} approach is a simple\nyet effective variant of the bottom-up method, accounting for biases that are\ndifficult to observe at the bottom level. It allows suboptimal forecasts at the\nlower level to retain a higher overall performance. The approach in this\nempirical study was developed by the first author during the M5 Forecasting\nAccuracy competition, ranking second place. The method is also business\norientated and could benefit for business strategic planning.",
          "link": "http://arxiv.org/abs/2103.08250",
          "publishedOn": "2022-01-03T07:15:17.308Z",
          "wordCount": 638,
          "title": "Hierarchical forecasting with a top-down alignment of independent level forecasts. (arXiv:2103.08250v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06201",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Loecher_M/0/1/0/all/0/1\">Markus Loecher</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>",
          "description": "Tree-based algorithms such as random forests and gradient boosted trees\ncontinue to be among the most popular and powerful machine learning models used\nacross multiple disciplines. The conventional wisdom of estimating the impact\nof a feature in tree based models is to measure the \\textit{node-wise reduction\nof a loss function}, which (i) yields only global importance measures and (ii)\nis known to suffer from severe biases. Conditional feature contributions (CFCs)\nprovide \\textit{local}, case-by-case explanations of a prediction by following\nthe decision path and attributing changes in the expected output of the model\nto each feature along the path. However, Lundberg et al. pointed out a\npotential bias of CFCs which depends on the distance from the root of a tree.\nThe by now immensely popular alternative, SHapley Additive exPlanation (SHAP)\nvalues appear to mitigate this bias but are computationally much more\nexpensive. Here we contribute a thorough comparison of the explanations\ncomputed by both methods on a set of 164 publicly available classification\nproblems in order to provide data-driven algorithm recommendations to current\nresearchers. For random forests, we find extremely high similarities and\ncorrelations of both local and global SHAP values and CFC scores, leading to\nvery similar rankings and interpretations. Analogous conclusions hold for the\nfidelity of using global feature importance scores as a proxy for the\npredictive power associated with each feature.",
          "link": "http://arxiv.org/abs/2108.06201",
          "publishedOn": "2022-01-03T07:15:17.302Z",
          "wordCount": 684,
          "title": "Data-driven advice for interpreting local and global model predictions in bioinformatics problems. (arXiv:2108.06201v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2110.06394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weitong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We study the model-based reward-free reinforcement learning with linear\nfunction approximation for episodic Markov decision processes (MDPs). In this\nsetting, the agent works in two phases. In the exploration phase, the agent\ninteracts with the environment and collects samples without the reward. In the\nplanning phase, the agent is given a specific reward function and uses samples\ncollected from the exploration phase to learn a good policy. We propose a new\nprovably efficient algorithm, called UCRL-RFE under the Linear Mixture MDP\nassumption, where the transition probability kernel of the MDP can be\nparameterized by a linear function over certain feature mappings defined on the\ntriplet of state, action, and next state. We show that to obtain an\n$\\epsilon$-optimal policy for arbitrary reward function, UCRL-RFE needs to\nsample at most $\\tilde{\\mathcal{O}}(H^5d^2\\epsilon^{-2})$ episodes during the\nexploration phase. Here, $H$ is the length of the episode, $d$ is the dimension\nof the feature mapping. We also propose a variant of UCRL-RFE using\nBernstein-type bonus and show that it needs to sample at most\n$\\tilde{\\mathcal{O}}(H^4d(H + d)\\epsilon^{-2})$ to achieve an\n$\\epsilon$-optimal policy. By constructing a special class of linear Mixture\nMDPs, we also prove that for any reward-free algorithm, it needs to sample at\nleast $\\tilde \\Omega(H^2d\\epsilon^{-2})$ episodes to obtain an\n$\\epsilon$-optimal policy. Our upper bound matches the lower bound in terms of\nthe dependence on $\\epsilon$ and the dependence on $d$ if $H \\ge d$.",
          "link": "http://arxiv.org/abs/2110.06394",
          "publishedOn": "2022-01-03T07:15:17.296Z",
          "wordCount": 700,
          "title": "Reward-Free Model-Based Reinforcement Learning with Linear Function Approximation. (arXiv:2110.06394v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.07068",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dockhorn_T/0/1/0/all/0/1\">Tim Dockhorn</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vahdat_A/0/1/0/all/0/1\">Arash Vahdat</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kreis_K/0/1/0/all/0/1\">Karsten Kreis</a>",
          "description": "Score-based generative models (SGMs) have demonstrated remarkable synthesis\nquality. SGMs rely on a diffusion process that gradually perturbs the data\ntowards a tractable distribution, while the generative model learns to denoise.\nThe complexity of this denoising task is, apart from the data distribution\nitself, uniquely determined by the diffusion process. We argue that current\nSGMs employ overly simplistic diffusions, leading to unnecessarily complex\ndenoising processes, which limit generative modeling performance. Based on\nconnections to statistical mechanics, we propose a novel critically-damped\nLangevin diffusion (CLD) and show that CLD-based SGMs achieve superior\nperformance. CLD can be interpreted as running a joint diffusion in an extended\nspace, where the auxiliary variables can be considered \"velocities\" that are\ncoupled to the data variables as in Hamiltonian dynamics. We derive a novel\nscore matching objective for CLD and show that the model only needs to learn\nthe score function of the conditional distribution of the velocity given data,\nan easier task than learning scores of the data directly. We also derive a new\nsampling scheme for efficient synthesis from CLD-based diffusion models. We\nfind that CLD outperforms previous SGMs in synthesis quality for similar\nnetwork architectures and sampling compute budgets. We show that our novel\nsampler for CLD significantly outperforms solvers such as Euler--Maruyama. Our\nframework provides new insights into score-based denoising diffusion models and\ncan be readily used for high-resolution image synthesis. Project page and code:\nhttps://nv-tlabs.github.io/CLD-SGM.",
          "link": "http://arxiv.org/abs/2112.07068",
          "publishedOn": "2022-01-03T07:15:17.290Z",
          "wordCount": 670,
          "title": "Score-Based Generative Modeling with Critically-Damped Langevin Diffusion. (arXiv:2112.07068v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.00184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Besserve_M/0/1/0/all/0/1\">Michel Besserve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">R&#xe9;my Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janzing_D/0/1/0/all/0/1\">Dominik Janzing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "Generative models can be trained to emulate complex empirical data, but are\nthey useful to make predictions in the context of previously unobserved\nenvironments? An intuitive idea to promote such extrapolation capabilities is\nto have the architecture of such model reflect a causal graph of the true data\ngenerating process, such that one can intervene on each node independently of\nthe others. However, the nodes of this graph are usually unobserved, leading to\noverparameterization and lack of identifiability of the causal structure. We\ndevelop a theoretical framework to address this challenging situation by\ndefining a weaker form of identifiability, based on the principle of\nindependence of mechanisms. We demonstrate on toy examples that classical\nstochastic gradient descent can hinder the model's extrapolation capabilities,\nsuggesting independence of mechanisms should be enforced explicitly during\ntraining. Experiments on deep generative models trained on real world data\nsupport these insights and illustrate how the extrapolation capabilities of\nsuch models can be leveraged.",
          "link": "http://arxiv.org/abs/2004.00184",
          "publishedOn": "2022-01-03T07:15:17.272Z",
          "wordCount": 623,
          "title": "A theory of independent mechanisms for extrapolation in generative models. (arXiv:2004.00184v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.12360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zixiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1\">Difan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "A recent line of research on deep learning focuses on the extremely\nover-parameterized setting, and shows that when the network width is larger\nthan a high degree polynomial of the training sample size $n$ and the inverse\nof the target error $\\epsilon^{-1}$, deep neural networks learned by\n(stochastic) gradient descent enjoy nice optimization and generalization\nguarantees. Very recently, it is shown that under certain margin assumptions on\nthe training data, a polylogarithmic width condition suffices for two-layer\nReLU networks to converge and generalize (Ji and Telgarsky, 2019). However,\nwhether deep neural networks can be learned with such a mild\nover-parameterization is still an open question. In this work, we answer this\nquestion affirmatively and establish sharper learning guarantees for deep ReLU\nnetworks trained by (stochastic) gradient descent. In specific, under certain\nassumptions made in previous work, our optimization and generalization\nguarantees hold with network width polylogarithmic in $n$ and $\\epsilon^{-1}$.\nOur results push the study of over-parameterized deep neural networks towards\nmore practical settings.",
          "link": "http://arxiv.org/abs/1911.12360",
          "publishedOn": "2022-01-03T07:15:17.266Z",
          "wordCount": 653,
          "title": "How Much Over-parameterization Is Sufficient to Learn Deep ReLU Networks?. (arXiv:1911.12360v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15423",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chang_J/0/1/0/all/0/1\">Jinyuan Chang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+He_J/0/1/0/all/0/1\">Jing He</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_L/0/1/0/all/0/1\">Lin Yang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yao_Q/0/1/0/all/0/1\">Qiwei Yao</a>",
          "description": "We propose to model matrix time series based on a tensor CP-decomposition.\nInstead of using an iterative algorithm which is the standard practice for\nestimating CP-decompositions, we propose a new and one-pass estimation\nprocedure based on a generalized eigenanalysis constructed from the serial\ndependence structure of the underlying process. A key idea of the new procedure\nis to project a generalized eigenequation defined in terms of rank-reduced\nmatrices to a lower-dimensional one with full-ranked matrices, to avoid the\nintricacy of the former of which the number of eigenvalues can be zero, finite\nand infinity. The asymptotic theory has been established under a general\nsetting without the stationarity. It shows, for example, that all the component\ncoefficient vectors in the CP-decomposition are estimated consistently with the\ndifferent error rates, depending on the relative sizes between the dimensions\nof time series and the sample size. The proposed model and the estimation\nmethod are further illustrated with both simulated and real data; showing\neffective dimension-reduction in modelling and forecasting matrix time series.",
          "link": "http://arxiv.org/abs/2112.15423",
          "publishedOn": "2022-01-03T07:15:17.258Z",
          "wordCount": 587,
          "title": "Modelling matrix time series via a tensor CP-decomposition. (arXiv:2112.15423v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shunqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurkoski_B/0/1/0/all/0/1\">Brian M. Kurkoski</a>",
          "description": "Approximate message passing (AMP) is a promising technique for unknown signal\nreconstruction of certain high-dimensional linear systems with non-Gaussian\nsignaling. A distinguished feature of the AMP-type algorithms is that their\ndynamics can be rigorously described by state evolution. However, state\nevolution does not necessarily guarantee the convergence of iterative\nalgorithms. To solve the convergence problem of AMP-type algorithms in\nprinciple, this paper proposes a memory AMP (MAMP) under a sufficient statistic\ncondition, named sufficient statistic MAMP (SS-MAMP). We show that the\ncovariance matrices of SS-MAMP are L-banded and convergent. Given an arbitrary\nMAMP, we can construct an SS-MAMP by damping, which not only ensures the\nconvergence of MAMP but also preserves the orthogonality of MAMP, i.e., its\ndynamics can be rigorously described by state evolution. As a byproduct, we\nprove that the Bayes-optimal orthogonal/vector AMP (BO-OAMP/VAMP) is an\nSS-MAMP. As a result, we reveal two interesting properties of BO-OAMP/VAMP for\nlarge systems: 1) the covariance matrices are L-banded and are convergent in\nBO-OAMP/VAMP, and 2) damping and memory are useless (i.e., do not bring\nperformance improvement) in BO-OAMP/VAMP. As an example, we construct a\nsufficient statistic Bayes-optimal MAMP (BO-MAMP), which is Bayes optimal if\nits state evolution has a unique fixed point and its MSE is not worse than the\noriginal BO-MAMP. Finally, simulations are provided to verify the validity and\naccuracy of the theoretical results.",
          "link": "http://arxiv.org/abs/2112.15327",
          "publishedOn": "2022-01-03T07:15:17.251Z",
          "wordCount": 660,
          "title": "Sufficient Statistic Memory AMP. (arXiv:2112.15327v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/1906.12072",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Azais_J/0/1/0/all/0/1\">J.-M. Aza&#xef;s</a>, <a href=\"http://arxiv.org/find/math/1/au:+Castro_Y/0/1/0/all/0/1\">Y. De Castro</a>",
          "description": "We investigate multiple testing and variable selection using the Least Angle\nRegression (LARS) algorithm in high dimensions under the assumption of Gaussian\nnoise. LARS is known to produce a piecewise affine solution path with change\npoints referred to as the knots of the LARS path. The key to our results is an\nexpression in closed form of the exact joint law of a $K$-tuple of knots\nconditional on the variables selected by LARS, namely the so-called\npost-selection joint law of the LARS knots. Numerical experiments demonstrate\nthe perfect fit of our findings.\n\nThis paper makes three main contributions. First, we build testing procedures\non variables entering the model along the LARS path in the general design case\nwhen the noise level can be unknown. These testing procedures are referred to\nas the Generalized $t$-Spacing tests (GtSt) and we prove that they have an\nexact non-asymptotic level (i.e., the Type I error is exactly controlled). This\nextends work of (Taylor et al., 2014) where the spacing test works for\nconsecutive knots and known variance. Second, we introduce a new exact multiple\nfalse negatives test after model selection in the general design case when the\nnoise level may be unknown. We prove that this testing procedure has exact\nnon-asymptotic level for general design and unknown noise level. Third, we give\nan exact control of the false discovery rate under orthogonal design\nassumption. Monte Carlo simulations and a real data experiment are provided to\nillustrate our results in this case. Of independent interest, we introduce an\nequivalent formulation of the LARS algorithm based on a recursive function.",
          "link": "http://arxiv.org/abs/1906.12072",
          "publishedOn": "2022-01-03T07:15:17.244Z",
          "wordCount": 790,
          "title": "Multiple Testing and Variable Selection along the path of the Least Angle Regression. (arXiv:1906.12072v4 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.12961",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yuan_C/0/1/0/all/0/1\">Chaoxia Yuan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ying_C/0/1/0/all/0/1\">Chao Ying</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fang_F/0/1/0/all/0/1\">Fang Fang</a>",
          "description": "Support vector machine (SVM) is a powerful classification method that has\nachieved great success in many fields. Since its performance can be seriously\nimpaired by redundant covariates, model selection techniques are widely used\nfor SVM with high dimensional covariates. As an alternative to model selection,\nsignificant progress has been made in the area of model averaging in the past\ndecades. Yet no frequentist model averaging method was considered for SVM. This\nwork aims to fill the gap and to propose a frequentist model averaging\nprocedure for SVM which selects the optimal weight by cross validation. Even\nwhen the number of covariates diverges at an exponential rate of the sample\nsize, we show asymptotic optimality of the proposed method in the sense that\nthe ratio of its hinge loss to the lowest possible loss converges to one. We\nalso derive the convergence rate which provides more insights to model\naveraging. Compared to model selection methods of SVM which require a tedious\nbut critical task of tuning parameter selection, the model averaging method\navoids the task and shows promising performances in the empirical studies.",
          "link": "http://arxiv.org/abs/2112.12961",
          "publishedOn": "2022-01-03T07:15:17.225Z",
          "wordCount": 647,
          "title": "Optimal Model Averaging of Support Vector Machines in Diverging Model Spaces. (arXiv:2112.12961v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.15265",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1\">Lang Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pal_S/0/1/0/all/0/1\">Soumik Pal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Harchaoui_Z/0/1/0/all/0/1\">Zaid Harchaoui</a>",
          "description": "Optimal transport (OT) and its entropy regularized offspring have recently\ngained a lot of attention in both machine learning and AI domains. In\nparticular, optimal transport has been used to develop probability metrics\nbetween probability distributions. We introduce in this paper an independence\ncriterion based on entropy regularized optimal transport. Our criterion can be\nused to test for independence between two samples. We establish non-asymptotic\nbounds for our test statistic, and study its statistical behavior under both\nthe null and alternative hypothesis. Our theoretical results involve tools from\nU-process theory and optimal transport theory. We present experimental results\non existing benchmarks, illustrating the interest of the proposed criterion.",
          "link": "http://arxiv.org/abs/2112.15265",
          "publishedOn": "2022-01-03T07:15:17.219Z",
          "wordCount": 523,
          "title": "Entropy Regularized Optimal Transport Independence Criterion. (arXiv:2112.15265v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14877",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bui_Thanh_T/0/1/0/all/0/1\">Tan Bui-Thanh</a>",
          "description": "One of the reasons that many neural networks are capable of replicating\ncomplicated tasks or functions is their universality property. The past few\ndecades have seen many attempts in providing constructive proofs for single or\nclass of neural networks. This paper is an effort to provide a unified and\nconstructive framework for the universality of a large class of activations\nincluding most of existing activations and beyond. At the heart of the\nframework is the concept of neural network approximate identity. It turns out\nthat most of existing activations are neural network approximate identity, and\nthus universal in the space of continuous of functions on compacta. The\nframework induces several advantages. First, it is constructive with elementary\nmeans from functional analysis, probability theory, and numerical analysis.\nSecond, it is the first unified attempt that is valid for most of existing\nactivations. Third, as a by product, the framework provides the first\nuniversity proof for some of the existing activation functions including Mish,\nSiLU, ELU, GELU, and etc. Fourth, it discovers new activations with guaranteed\nuniversality property. Indeed, any activation\\textemdash whose $\\k$th\nderivative, with $\\k$ being an integer, is integrable and essentially\nbounded\\textemdash is universal. Fifth, for a given activation and error\ntolerance, the framework provides precisely the architecture of the\ncorresponding one-hidden neural network with predetermined number of neuron,\nand the values of weights/biases.",
          "link": "http://arxiv.org/abs/2112.14877",
          "publishedOn": "2022-01-03T07:15:17.211Z",
          "wordCount": 652,
          "title": "A Unified and Constructive Framework for the Universality of Neural Networks. (arXiv:2112.14877v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heiss_J/0/1/0/all/0/1\">Jakob Heiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teichmann_J/0/1/0/all/0/1\">Josef Teichmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wutte_H/0/1/0/all/0/1\">Hanna Wutte</a>",
          "description": "We prove in this paper that wide ReLU neural networks (NNs) with at least one\nhidden layer optimized with l2-regularization on the parameters enforces\nmulti-task learning due to representation-learning - also in the limit width to\ninfinity. This is in contrast to multiple other idealized settings discussed in\nthe literature where wide (ReLU)-NNs loose their ability to benefit from\nmulti-task learning in the limit width to infinity. We deduce the multi-task\nlearning ability from proving an exact quantitative macroscopic\ncharacterization of the learned NN in function space.",
          "link": "http://arxiv.org/abs/2112.15577",
          "publishedOn": "2022-01-03T07:15:17.198Z",
          "wordCount": 543,
          "title": "Infinite wide (finite depth) Neural Networks benefit from multi-task learning unlike shallow Gaussian Processes -- an exact quantitative macroscopic characterization. (arXiv:2112.15577v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15595",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Irons_N/0/1/0/all/0/1\">Nicholas J. Irons</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scetbon_M/0/1/0/all/0/1\">Meyer Scetbon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pal_S/0/1/0/all/0/1\">Soumik Pal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Harchaoui_Z/0/1/0/all/0/1\">Zaid Harchaoui</a>",
          "description": "Triangular flows, also known as Kn\\\"{o}the-Rosenblatt measure couplings,\ncomprise an important building block of normalizing flow models for generative\nmodeling and density estimation, including popular autoregressive flow models\nsuch as real-valued non-volume preserving transformation models (Real NVP). We\npresent statistical guarantees and sample complexity bounds for triangular flow\nstatistical models. In particular, we establish the statistical consistency and\nthe finite sample convergence rates of the Kullback-Leibler estimator of the\nKn\\\"{o}the-Rosenblatt measure coupling using tools from empirical process\ntheory. Our results highlight the anisotropic geometry of function classes at\nplay in triangular flows, shed light on optimal coordinate ordering, and lead\nto statistical guarantees for Jacobian flows. We conduct numerical experiments\non synthetic data to illustrate the practical implications of our theoretical\nfindings.",
          "link": "http://arxiv.org/abs/2112.15595",
          "publishedOn": "2022-01-03T07:15:17.179Z",
          "wordCount": 555,
          "title": "Triangular Flows for Generative Modeling: Statistical Consistency, Smoothness Classes, and Fast Rates. (arXiv:2112.15595v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2112.14868",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+So_B/0/1/0/all/0/1\">Banghee So</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Valdez_E/0/1/0/all/0/1\">Emiliano A. Valdez</a>",
          "description": "Classification predictive modeling involves the accurate assignment of\nobservations in a dataset to target classes or categories. There is an\nincreasing growth of real-world classification problems with severely\nimbalanced class distributions. In this case, minority classes have much fewer\nobservations to learn from than those from majority classes. Despite this\nsparsity, a minority class is often considered the more interesting class yet\ndeveloping a scientific learning algorithm suitable for the observations\npresents countless challenges. In this article, we suggest a novel multi-class\nclassification algorithm specialized to handle severely imbalanced classes\nbased on the method we refer to as SAMME.C2. It blends the flexible mechanics\nof the boosting techniques from SAMME algorithm, a multi-class classifier, and\nAda.C2 algorithm, a cost-sensitive binary classifier designed to address highly\nclass imbalances. Not only do we provide the resulting algorithm but we also\nestablish scientific and statistical formulation of our proposed SAMME.C2\nalgorithm. Through numerical experiments examining various degrees of\nclassifier difficulty, we demonstrate consistent superior performance of our\nproposed model.",
          "link": "http://arxiv.org/abs/2112.14868",
          "publishedOn": "2022-01-03T07:15:17.169Z",
          "wordCount": 593,
          "title": "The SAMME.C2 algorithm for severely imbalanced multi-class classification. (arXiv:2112.14868v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2009.13794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Weiran Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Sean Qian</a>",
          "description": "The effectiveness of traditional traffic prediction methods is often\nextremely limited when forecasting traffic dynamics in early morning. The\nreason is that traffic can break down drastically during the early morning\ncommute, and the time and duration of this break-down vary substantially from\nday to day. Early morning traffic forecast is crucial to inform morning-commute\ntraffic management, but they are generally challenging to predict in advance,\nparticularly by midnight. In this paper, we propose to mine Twitter messages as\na probing method to understand the impacts of people's work and rest patterns\nin the evening/midnight of the previous day to the next-day morning traffic.\nThe model is tested on freeway networks in Pittsburgh as experiments. The\nresulting relationship is surprisingly simple and powerful. We find that, in\ngeneral, the earlier people rest as indicated from Tweets, the more congested\nroads will be in the next morning. The occurrence of big events in the evening\nbefore, represented by higher or lower tweet sentiment than normal, often\nimplies lower travel demand in the next morning than normal days. Besides,\npeople's tweeting activities in the night before and early morning are\nstatistically associated with congestion in morning peak hours. We make use of\nsuch relationships to build a predictive framework which forecasts morning\ncommute congestion using people's tweeting profiles extracted by 5 am or as\nlate as the midnight prior to the morning. The Pittsburgh study supports that\nour framework can precisely predict morning congestion, particularly for some\nroad segments upstream of roadway bottlenecks with large day-to-day congestion\nvariation. Our approach considerably outperforms those existing methods without\nTwitter message features, and it can learn meaningful representation of demand\nfrom tweeting profiles that offer managerial insights.",
          "link": "http://arxiv.org/abs/2009.13794",
          "publishedOn": "2022-01-03T07:15:17.149Z",
          "wordCount": 765,
          "title": "From Twitter to Traffic Predictor: Next-Day Morning Traffic Prediction Using Social Media Data. (arXiv:2009.13794v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2112.14949",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>",
          "description": "In this paper, we focus on the decentralized optimization problem over the\nStiefel manifold, which is defined on a connected network of $d$ agents. The\nobjective is an average of $d$ local functions, and each function is privately\nheld by an agent and encodes its data. The agents can only communicate with\ntheir neighbors in a collaborative effort to solve this problem. In existing\nmethods, multiple rounds of communications are required to guarantee the\nconvergence, giving rise to high communication costs. In contrast, this paper\nproposes a decentralized algorithm, called DESTINY, which only invokes a single\nround of communications per iteration. DESTINY combines gradient tracking\ntechniques with a novel approximate augmented Lagrangian function. The global\nconvergence to stationary points is rigorously established. Comprehensive\nnumerical experiments demonstrate that DESTINY has a strong potential to\ndeliver a cutting-edge performance in solving a variety of testing problems.",
          "link": "http://arxiv.org/abs/2112.14949",
          "publishedOn": "2022-01-03T07:15:17.127Z",
          "wordCount": 588,
          "title": "Decentralized Optimization Over the Stiefel Manifold by an Approximate Augmented Lagrangian Function. (arXiv:2112.14949v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2112.15383",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Seroussi_I/0/1/0/all/0/1\">Inbar Seroussi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ringel_Z/0/1/0/all/0/1\">Zohar Ringel</a>",
          "description": "Deep neural networks (DNNs) are powerful tools for compressing and distilling\ninformation. Due to their scale and complexity, often involving billions of\ninter-dependent internal degrees of freedom, exact analysis approaches often\nfall short. A common strategy in such cases is to identify slow degrees of\nfreedom that average out the erratic behavior of the underlying fast\nmicroscopic variables. Here, we identify such a separation of scales occurring\nin over-parameterized deep convolutional neural networks (CNNs) at the end of\ntraining. It implies that neuron pre-activations fluctuate in a nearly Gaussian\nmanner with a deterministic latent kernel. While for CNNs with infinitely many\nchannels these kernels are inert, for finite CNNs they adapt and learn from\ndata in an analytically tractable manner. The resulting thermodynamic theory of\ndeep learning yields accurate predictions on several deep non-linear CNN toy\nmodels. In addition, it provides new ways of analyzing and understanding CNNs.",
          "link": "http://arxiv.org/abs/2112.15383",
          "publishedOn": "2022-01-03T07:15:17.107Z",
          "wordCount": 584,
          "title": "Separation of scales and a thermodynamic description of feature learning in some CNNs. (arXiv:2112.15383v1 [stat.ML])"
        }
      ]
    },
    {
      "title": "Machine Learning",
      "feedUrl": "https://www.reddit.com/r/MachineLearning/.rss",
      "siteUrl": "https://www.reddit.com/r/MachineLearning/",
      "articles": [
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rutbpv/r_a_neural_network_solves_and_generates/",
          "author": null,
          "description": "submitted by    /u/shitboots  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rutbpv/r_a_neural_network_solves_and_generates/",
          "publishedOn": "2022-01-03T04:50:29.000Z",
          "wordCount": 138,
          "title": "[R] A Neural Network Solves and Generates Mathematics Problems by Program Synthesis: Calculus, Differential Equations, Linear Algebra, and More"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rut9hs/d_is_there_flowbased_method_which_treats_input/",
          "author": null,
          "description": "Hello. I am searching the researches that different size of data are generated through the flow-based network, not super-resolution task such as continuous mapping.\n I want to generate output as time-aligned scalar data, for example\n ​\n Input: noise sampling (B x T x C)\n Output: scalar data (B x T, C=1)\n ​\n with introducing the variational data augmentation technique (in vFlow, which can output high-dimensionality as concatenate noise vector for input and output both) for output.\n But there's a problem time dimension T is different for each of all data input. How can I treat this problem?\n ​\n p.s. I am very appreciate if I can read the flow-based research in NLP task.\n    submitted by    /u/RedCuraceo  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rut9hs/d_is_there_flowbased_method_which_treats_input/",
          "publishedOn": "2022-01-03T04:47:08.000Z",
          "wordCount": 279,
          "title": "[D] Is there flow-based method which treats input data as different lengths each?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rur95m/d_anyone_switched_from_vision_to_robotics/",
          "author": null,
          "description": "I’m about to finish my PhD and the whole field of robotics looks so exciting right now, especially applications like farming and recycling. Has anyone switched from more pure deep learning (vision / NLP) to robotics and how did it happen?\n Did you just get a robotics related job focusing on the vision side of things or is it key to have more experience on the robotics side before getting a job?\n Also I’m curious what’s the best location for robotics? Like how you go to Hong Kong / New York for finance, SF for software or Shenzhen for hardware.\n    submitted by    /u/temporary_ml_guy  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rur95m/d_anyone_switched_from_vision_to_robotics/",
          "publishedOn": "2022-01-03T03:03:06.000Z",
          "wordCount": 259,
          "title": "[D] Anyone switched from vision to robotics?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rur2j3/p_i_like_yolov5_but_the_code_complexity_is/",
          "author": null,
          "description": "I like YOLOv5 but the code complexity is...\n I can't deny that YOLOv5 is a practical open-source object detection pipeline. However, the pain begins when adding new features or new experimental methods. Code dependencies are hard to follow which makes the code difficult to maintain. We wanted to try various experimental methods but hate to write one-time code that is never re-used.\n So we worked on making an object detection pipeline to have a better code structure so that we could continuously improve and add new features while easy to maintain.\n https://github.com/j-marple-dev/AYolov2\n And we applied CI(Formating, Linting, Unittest) to ensure code quality with Docker support for development and inference. Our Docker supports the development environment with VIM.\n Our code design from the…",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rur2j3/p_i_like_yolov5_but_the_code_complexity_is/",
          "publishedOn": "2022-01-03T02:54:14.000Z",
          "wordCount": 595,
          "title": "[P] I like YOLOv5 but the code complexity is..."
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ruofql/d_guibased_machine_learning_applications/",
          "author": null,
          "description": "I was previously using Azure Machine Learning Studio(classic), and of course, it was discontinued last month. Any other free ML applications?\n The new Azure Machine Learning Studio isn't free, and this is a school project so I'm aiming for free and simple.\n Any suggestions? Or maybe someone else is using Studio(classic) and knows a way around this?\n    submitted by    /u/max02c  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ruofql/d_guibased_machine_learning_applications/",
          "publishedOn": "2022-01-03T00:50:33.000Z",
          "wordCount": 235,
          "title": "[D] GUI-based Machine Learning applications?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rum37y/r_a_comparison_of_the_program_synthesis/",
          "author": null,
          "description": "submitted by    /u/EducationalCicada  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rum37y/r_a_comparison_of_the_program_synthesis/",
          "publishedOn": "2022-01-02T23:04:23.000Z",
          "wordCount": 118,
          "title": "[R] A Comparison Of The Program Synthesis Performance Of GitHub Copilot And Genetic Programming"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ruja9s/d_machine_learning_wayr_what_are_you_reading_week/",
          "author": null,
          "description": "This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.\n Please try to provide some insight from your understanding and please don't post things which are present in wiki.\n Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.\n Previous weeks :\n  \n 1-10 11-20 21-30 31-40 41-50 51-60 61-70 71-80 81-90 91-100 101-110 111-120 121-130 \n  \n Week 1 Week 11 Week 21 Week 31 Week 41 Week 51 Week 61 Week 71 Week 81 Week 91 Week 101 Week 111 Week 121 \n  Week 2 Week 12 Week 22 Week 32…",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ruja9s/d_machine_learning_wayr_what_are_you_reading_week/",
          "publishedOn": "2022-01-02T21:00:05.000Z",
          "wordCount": 364,
          "title": "[D] Machine Learning - WAYR (What Are You Reading) - Week 128"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ruj3ja/d_iclr_2022_open_discussion_quality/",
          "author": null,
          "description": "First time submitting to ICLR, I'm wondering how is the open discussion on openreview.net really different from the review-rebuttal procedure used in other conferences.\n For the papers I'm reviewing, about half the reviewers reacted to the authors' responses (clarifications, modifications, additional experiments, etc.). As to my submission (5568), I run extra experiments and answered each of the concerns directly but received 0 feedback from the reviewers.\n As a reviewer, I think it doesn't matter if the rebuttal changes your mind about the quality of the submission but it's very basic manner to reply to the authors' responses. A simple \"Thank the authors for the responses but I don't think these addressed my concerns\" would work. Saying nothing only means you are uncertain if the responses make sense and you just doesn't care to figure it out. The authors spent a whole week running experiments to answer some of your questions and if you don't give **** about their responses, just keep the questions with you and don't submit your review.\n I was hoping for a different experience submitting to ICLR and then I realized the \"discussion\" is basically broken.\n    submitted by    /u/MLPRulesAll  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ruj3ja/d_iclr_2022_open_discussion_quality/",
          "publishedOn": "2022-01-02T20:51:32.000Z",
          "wordCount": 278,
          "title": "[D] ICLR 2022 Open Discussion Quality"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rud2m5/p_tensorflow_keras_implementation_of_vision/",
          "author": null,
          "description": "An Image is Worth 16x16 Words: ViT Excellent results compared to SOTA CNNs while requiring fewer computational resources to train.\n Paper : https://arxiv.org/abs/2010.11929v2 Code : https://github.com/avinash31d/paper-implementations\n    submitted by    /u/avinash31d  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rud2m5/p_tensorflow_keras_implementation_of_vision/",
          "publishedOn": "2022-01-02T16:23:44.000Z",
          "wordCount": 143,
          "title": "[P] Tensorflow / Keras implementation of Vision Transformer https://arxiv.org/abs/2010.11929v2"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rucjmx/d_simple_questions_thread/",
          "author": null,
          "description": "Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\n Thread will stay alive until next one so keep posting after the date in the title.\n Thanks to everyone for answering questions in the previous thread!\n    submitted by    /u/AutoModerator  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rucjmx/d_simple_questions_thread/",
          "publishedOn": "2022-01-02T16:00:14.000Z",
          "wordCount": 1035,
          "title": "[D] Simple Questions Thread"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ruaew1/r_learning_3d_representations_from_2d_images/",
          "author": null,
          "description": "submitted by    /u/pinter69  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ruaew1/r_learning_3d_representations_from_2d_images/",
          "publishedOn": "2022-01-02T14:13:01.000Z",
          "wordCount": 514,
          "title": "[R] Learning 3D Representations from 2D Images"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ru91o8/d_paper_explained_author_interview_player_of/",
          "author": null,
          "description": "https://youtu.be/U0mxx7AoNz0\n Special Guest: First author Martin Schmid (https://twitter.com/Lifrordi)\n Games have been used throughout research as testbeds for AI algorithms, such as reinforcement learning agents. However, different types of games usually require different solution approaches, such as AlphaZero for Go or Chess, and Counterfactual Regret Minimization (CFR) for Poker. Player of Games bridges this gap between perfect and imperfect information games and delivers a single algorithm that uses tree search over public information states, and is trained via self-play. The resulting algorithm can play Go, Chess, Poker, Scotland Yard, and many more games, as well as non-game environments.\n ​\n OUTLINE:\n 0:00 - Introduction\n 2:50 - What games can Player of Games be trained on?\n 4:00 - Tree search algorithms (AlphaZero)\n 8:00 - What is different in imperfect information games?\n 15:40 - Counterfactual Value- and Policy-Networks\n 18:50 - The Player of Games search procedure\n 28:30 - How to train the network?\n 34:40 - Experimental Results\n 47:20 - Discussion & Outlook\n ​\n Paper: https://arxiv.org/abs/2112.03178\n    submitted by    /u/ykilcher  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ru91o8/d_paper_explained_author_interview_player_of/",
          "publishedOn": "2022-01-02T12:52:58.000Z",
          "wordCount": 331,
          "title": "[D] Paper Explained & Author Interview - Player of Games: All the games, one algorithm! (Video Walkthrough)"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ru8n14/researchproject_the_top_10_aicomputer_vision/",
          "author": null,
          "description": "submitted by    /u/OnlyProggingForFun  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ru8n14/researchproject_the_top_10_aicomputer_vision/",
          "publishedOn": "2022-01-02T12:25:59.000Z",
          "wordCount": 132,
          "title": "[Research][Project] The top 10 AI/Computer Vision papers in 2021 with video demos, articles, and code for each!"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ru7k5y/d_machine_learning_research/",
          "author": null,
          "description": "Hi everyone, I've compiled the trusted sources of ideation based on top-tier conferences on Machine Learning and Deep Learning worldwide. This repository includes datasets, tasks, state-of-the-art and more.\n Repository GitHub\n https://preview.redd.it/9p1jkei8c9981.png?width=1000&format=png&auto=webp&s=906c20c58b5ee569b25848a7fbf0b41ca4caf354\n    submitted by    /u/tuanlda78202  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ru7k5y/d_machine_learning_research/",
          "publishedOn": "2022-01-02T11:08:35.000Z",
          "wordCount": 128,
          "title": "[D] Machine Learning Research"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ru7drf/p_quickdeploy_optimize_convert_and_deploy_machine/",
          "author": null,
          "description": "Hello Reddit, releasing one of my OSS projects: Quick-Deploy ..\n github: https://github.com/rodrigobaron/quick-deploy\n blog post: https://rodrigobaron.com/posts/quick-deploy\n ​\n It's in the very early stage, feel free to contribute or give a star 🙂\n    submitted by    /u/rodrigobaron  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ru7drf/p_quickdeploy_optimize_convert_and_deploy_machine/",
          "publishedOn": "2022-01-02T10:55:35.000Z",
          "wordCount": 132,
          "title": "[P] Quick-Deploy - Optimize, convert and deploy machine learning models"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ru70fv/d_raising_errors_while_using_accelerators/",
          "author": null,
          "description": "Why is it so hard to raise exceptions when ML pipeline is using a GPU?\n When for example you make a classic \"Index out of bounds\" error, in libraries like PyTorch you get some generic \"CUDA\" error and you can't see the exact error until you transfer the tensors explicitly to CPU and rerun the code.\n Do you think there is a possibility for this to improve in the future?\n Sorry if this is more CS-related question\n    submitted by    /u/Icy_Fisherman7187  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ru70fv/d_raising_errors_while_using_accelerators/",
          "publishedOn": "2022-01-02T10:27:53.000Z",
          "wordCount": 310,
          "title": "[D] Raising errors while using accelerators"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ru5xw8/d_are_nn_actually_overparametrized/",
          "author": null,
          "description": "I often read that NN or CNN are overparametrized. But, for example, resnet18 has 11M parameters while cifar10 has 50k32323=153M data points. How is that be an overparametrized network on cifar10? Or even on mnist which has 60k28*28=47M data points\n    submitted by    /u/alesaso2000  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ru5xw8/d_are_nn_actually_overparametrized/",
          "publishedOn": "2022-01-02T09:09:31.000Z",
          "wordCount": 752,
          "title": "[D] Are NN actually overparametrized?"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/ru06dy/d_coding_practices/",
          "author": null,
          "description": "My job is to work with ML engineers and provide them with whatever they need to experiment with/train/test/deploy ML models -- GPU infrastructure, distributed training support, etc. When I interface with their code, I almost always find it so poorly written, with little to no thought given to long-term stability or use -- for code that they 100% know is going to production.\n They're brilliant people, far smarter than me, and really good at what they do, so it's not a matter of them not being good enough. I feel (from my very limited experience, so I'm happy to be wrong) like ML engineers are incentivized to write poor code. The only metric for evaluation seems to be accuracy, loss, and all the plots that come up. In research, I understand completely, that's where the focus lies, but in industry? I've seen many models perform poorly because the code is so hard to read and refactor that big issues remained unspotted for months together. And this is especially befuddling because for a field that is completely fine with spending months to get an ROI of single digit increases in model performance metrics during the experimentation phase, they don't seem to care about anything that might go wrong in production. That just feels like a fundamental disconnect, since without the core ML stuff working perfectly, none of the other stuff (like what I do) has any value -- and even so, I'm taught to hold my code to a much higher standard than the critical stuff -- which I'm happy about since I can now write production code by default -- but it's just... weird. Like the vending machines at a nuclear power plant being better engineered than the reactor.\n Is this a common problem or is this a localized issue that I'm facing?\n    submitted by    /u/vPyDev  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/ru06dy/d_coding_practices/",
          "publishedOn": "2022-01-02T03:17:33.000Z",
          "wordCount": 1955,
          "title": "[D] Coding Practices"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rtukp2/d_plug_or_integrate_a_gnn_pytorch_code_base_into/",
          "author": null,
          "description": "Does anyone have a better explanation or resources to share for plug or Integrate a Pytorch based GNN models into Pyspark or similar cluster services?\n    submitted by    /u/SpiritMaleficent21  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rtukp2/d_plug_or_integrate_a_gnn_pytorch_code_base_into/",
          "publishedOn": "2022-01-01T22:40:23.000Z",
          "wordCount": 161,
          "title": "[D] Plug or Integrate a GNN Pytorch code base into Spark Cluster"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rtsmm7/p_deepcreampy_decensoring_hentai_with_deep_neural/",
          "author": null,
          "description": "submitted by    /u/binaryfor  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rtsmm7/p_deepcreampy_decensoring_hentai_with_deep_neural/",
          "publishedOn": "2022-01-01T21:09:49.000Z",
          "wordCount": 97,
          "title": "[P] DeepCreamPy - Decensoring Hentai with Deep Neural Networks"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rtrbso/r_neuron_outputs_as_weights/",
          "author": null,
          "description": "https://stats.stackexchange.com/questions/558864/what-if-weights-of-model-is-output-of-neurons\n    submitted by    /u/Dry_Introduction_897  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rtrbso/r_neuron_outputs_as_weights/",
          "publishedOn": "2022-01-01T20:09:18.000Z",
          "wordCount": 365,
          "title": "\"[R]\" Neuron outputs as weights"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rtndgm/d_best_practices_in_machine_learning/",
          "author": null,
          "description": "This is a non-profit that promotes best practices in machine learning, specifically for responsible ML. The practices are open source too, which is cool. \n Link here: https://www.fbpml.org/the-best-practices\n I think their technical best practices seems a little stronger than the organisational ones. Thoughts?\n ** this is their LinkedIn URL: https://www.linkedin.com/company/the-foundation-for-best-practices-in-machine-learning\n    submitted by    /u/Sbu91  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rtndgm/d_best_practices_in_machine_learning/",
          "publishedOn": "2022-01-01T17:01:42.000Z",
          "wordCount": 184,
          "title": "[D] Best Practices in Machine Learning"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rtmf6v/research_my_top_10_computer_vision_papers_of_2021/",
          "author": null,
          "description": "submitted by    /u/OnlyProggingForFun  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rtmf6v/research_my_top_10_computer_vision_papers_of_2021/",
          "publishedOn": "2022-01-01T16:14:26.000Z",
          "wordCount": 114,
          "title": "[Research] My Top 10 Computer Vision papers of 2021"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rtlx0r/r_mt3_multitask_multitrack_music_transcription/",
          "author": null,
          "description": "submitted by    /u/Illustrious_Row_9971  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rtlx0r/r_mt3_multitask_multitrack_music_transcription/",
          "publishedOn": "2022-01-01T15:48:37.000Z",
          "wordCount": 536,
          "title": "[R] MT3: Multi-Task Multitrack Music Transcription"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rtd1jl/bert_goes_shopping_comparing_distributional/",
          "author": null,
          "description": "submitted by    /u/prakhar21  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rtd1jl/bert_goes_shopping_comparing_distributional/",
          "publishedOn": "2022-01-01T06:06:40.000Z",
          "wordCount": 112,
          "title": "BERT Goes Shopping: Comparing Distributional Models for Product Representations (Paper Walkthrough) [D]"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rt1vfy/p_play_around_with_stylegan2_in_your_browser/",
          "author": null,
          "description": "I built a little page to run and manipulate StyleGAN2 in the browser.\n https://ziyadedher.com/faces\n It was pretty fun learning about ONNX and how to port GANs to web. You can play around with the random seeds and also distort the intermediate latents to produce some really wacky results. You can check out a GIF on Twitter.\n Let me know if you come up with anything cool!\n    submitted by    /u/Cold999  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rt1vfy/p_play_around_with_stylegan2_in_your_browser/",
          "publishedOn": "2021-12-31T19:55:39.000Z",
          "wordCount": 276,
          "title": "[P] Play around with StyleGAN2 in your browser"
        },
        {
          "id": "https://www.reddit.com/r/MachineLearning/comments/rsstqr/p_top_arxiv_machine_learning_papers_in_2021/",
          "author": null,
          "description": "With 2021 almost in the books (there are still a couple of hours to go at the time of this writing), here are the top machine learning papers per month from the arXiv pre-print archive as picked up by metacurate.io in 2021.\n January\n  \nCan a Fruit Fly Learn Word Embeddings?\n Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\n Muppet: Massive Multi-task Representations with Pre-Finetuning\n  \nFebruary\n  \nHow to represent part-whole hierarchies in a neural network\n Patterns, predictions, and actions: A story about machine learning\n Fast Graph Learning with Unique Optimal Solutions\n  \nMarch\n  \nFast and flexible: Human program induction in abstract reasoning tasks\n Learning to Resize Images for Computer Vision Tasks\n The Prevalence of Code Smells in Mac…",
          "link": "https://www.reddit.com/r/MachineLearning/comments/rsstqr/p_top_arxiv_machine_learning_papers_in_2021/",
          "publishedOn": "2021-12-31T12:24:05.000Z",
          "wordCount": 613,
          "title": "[P] Top arXiv Machine Learning papers in 2021 according to metacurate.io"
        }
      ]
    },
    {
      "title": "ML in Production",
      "feedUrl": "https://mlinproduction.com/feed",
      "siteUrl": "https://mlinproduction.com",
      "articles": []
    },
    {
      "title": "Jay Alammar",
      "feedUrl": "https://jalammar.github.io/feed.xml",
      "siteUrl": "http://jalammar.github.io/",
      "articles": []
    },
    {
      "title": "Distill",
      "feedUrl": "https://distill.pub/rss.xml",
      "siteUrl": "https://distill.pub",
      "articles": []
    },
    {
      "title": "inFERENCe",
      "feedUrl": "https://www.inference.vc/rss",
      "siteUrl": "https://www.inference.vc/",
      "articles": []
    },
    {
      "title": "AI Trends",
      "feedUrl": "https://www.aitrends.com/feed",
      "siteUrl": "https://www.aitrends.com",
      "articles": []
    },
    {
      "title": "AI Weirdness",
      "feedUrl": "https://aiweirdness.com/rss",
      "siteUrl": "https://www.aiweirdness.com/",
      "articles": [
        {
          "id": "61c0e2e310e963003be0848f",
          "author": "Janelle Shane",
          "description": "This month I'm beginning 2022 as the first Futurist in Residence at the Smithsonian Arts and Industries Building. \nIt's weird to think of myself as a futurist. I write a lot about the algorithms we're calling artificial intelligence (AI), but rather than deal with",
          "link": "https://www.aiweirdness.com/new-years-resolutions-generated-by-ai/",
          "publishedOn": "2021-12-30T14:00:00.000Z",
          "wordCount": 1861,
          "title": "New Years Resolutions generated by AI"
        },
        {
          "id": "61ca6c1b10e963003be0ed03",
          "author": "Janelle Shane",
          "description": "AI Weirdness: the strange side of machine learning",
          "link": "https://www.aiweirdness.com/bonus-more-new-years-resolutions-to-consider/",
          "publishedOn": "2021-12-30T13:58:00.000Z",
          "wordCount": 421,
          "title": "Bonus: more new year's resolutions to consider"
        },
        {
          "id": "61bff61010e963003be0846a",
          "author": "Janelle Shane",
          "description": "When you think about it, Christmas can get pretty weird. \nThere's the classic Christmas story of the Bible, and then there are all these extra entities that aren't in the book but which become somehow part of Christmas. And some of them are quite unsettling. There&",
          "link": "https://www.aiweirdness.com/christmas-entities/",
          "publishedOn": "2021-12-22T15:57:26.000Z",
          "wordCount": 597,
          "title": "Christmas entities"
        },
        {
          "id": "61c2a8d510e963003be08554",
          "author": "Janelle Shane",
          "description": "AI Weirdness: the strange side of machine learning",
          "link": "https://www.aiweirdness.com/the-awesome-lightning-power-of-blitzen/",
          "publishedOn": "2021-12-22T15:55:39.000Z",
          "wordCount": 428,
          "title": "Bonus: The awesome lightning power of Blitzen, son of Donder"
        },
        {
          "id": "61b51d19d5bf03003b927f69",
          "author": "Janelle Shane",
          "description": "Few could have predicted that the must-have toy of 1998 would be an owl-like bilingual hamster doll with infrared sensors, or that in 1975 kids would be begging their parents for a toy that is literally a single rock in a cardboard box.\nBut could AI have predicted it? Could",
          "link": "https://www.aiweirdness.com/predicting-the-next-toy-fads-with-ai/",
          "publishedOn": "2021-12-14T19:20:50.000Z",
          "wordCount": 584,
          "title": "Predicting the next toy fads with AI"
        },
        {
          "id": "61b81ed61a6991003b5fa70b",
          "author": "Janelle Shane",
          "description": "AI Weirdness: the strange side of machine learning",
          "link": "https://www.aiweirdness.com/bonus-kids-love-the-5-foot-rabbit-with-tiny-mouthparts/",
          "publishedOn": "2021-12-14T19:20:03.000Z",
          "wordCount": 428,
          "title": "Bonus: Kids love the 5-foot rabbit with tiny mouthparts!"
        }
      ]
    },
    {
      "title": "The Berkeley Artificial Intelligence Research Blog",
      "feedUrl": "https://bair.berkeley.edu/blog/feed.xml",
      "siteUrl": "http://bair.berkeley.edu/blog/",
      "articles": [
        {
          "id": "http://bair.berkeley.edu/blog/2021/12/15/unsupervised-rl/",
          "author": null,
          "description": "The shortcomings of supervised RL\nReinforcement Learning (RL) is a powerful paradigm for solving many problems of interest in AI, such as controlling autonomous vehicles, digital assistants, and resource allocation to name a few. We’ve seen over the last five years that, when provided with an extrinsic reward function, RL agents can master very complex tasks like playing Go, Starcraft, and dextrous robotic manipulation. While large-scale RL agents can achieve stunning results, even the best RL agents today are narrow. Most RL algorithms today can only solve the single task they were trained on and do not exhibit cross-task or cross-domain generalization capabilities.\nA side-effect of the narrowness of today’s RL systems is that today’s RL agents are also very data inefficient. If we were t…",
          "link": "http://bair.berkeley.edu/blog/2021/12/15/unsupervised-rl/",
          "publishedOn": "2021-12-15T12:00:00.000Z",
          "wordCount": 983,
          "title": "The Unsupervised Reinforcement Learning Benchmark"
        }
      ]
    },
    {
      "title": "Becoming Human: Artificial Intelligence Magazine - Medium",
      "feedUrl": "https://becominghuman.ai/feed",
      "siteUrl": "https://becominghuman.ai?source=rss----5e5bef33608a---4",
      "articles": [
        {
          "id": "https://medium.com/p/90a40f1a1d97",
          "author": "Rayan Potter",
          "description": "In the last decade, computer vision technology has advanced substantially, thanks to advances in AI and deep learning methodologies. It is…",
          "link": "https://becominghuman.ai/use-cases-of-image-segmentation-using-deep-learning-90a40f1a1d97?source=rss----5e5bef33608a---4",
          "publishedOn": "2021-12-29T16:15:29.000Z",
          "wordCount": 1127,
          "title": "Use cases of Image Segmentation Using Deep Learning"
        },
        {
          "id": "https://medium.com/p/2b746df1bf3",
          "author": "MobiDev",
          "description": "With 2020 well behind us, COVID-19’s presence continues to linger around the world. Of all the industries that have been forever changed by…",
          "link": "https://becominghuman.ai/healthcare-technology-trends-and-digital-innovations-in-2022-2b746df1bf3?source=rss----5e5bef33608a---4",
          "publishedOn": "2021-12-29T16:15:28.000Z",
          "wordCount": 3339,
          "title": "Healthcare Technology Trends and Digital Innovations in 2022"
        },
        {
          "id": "https://medium.com/p/562c0ba62c14",
          "author": "Dnyanesh Walwadkar",
          "description": "Biological inspiration of Neural Networks",
          "link": "https://becominghuman.ai/keys-of-deep-learning-activation-functions-562c0ba62c14?source=rss----5e5bef33608a---4",
          "publishedOn": "2021-12-28T16:21:30.000Z",
          "wordCount": 4012,
          "title": "Keys of Deep Learning : Activation Functions"
        },
        {
          "id": "https://medium.com/p/13727057d0d9",
          "author": "Ashok Sharma",
          "description": "The world is in an exciting phase of artificial intelligence that is slowly taking over our daily lives. Alexa, Siri is replacing person",
          "link": "https://becominghuman.ai/how-artificial-intelligence-is-changing-the-payment-gateway-industry-13727057d0d9?source=rss----5e5bef33608a---4",
          "publishedOn": "2021-12-27T19:39:04.000Z",
          "wordCount": 1436,
          "title": "How Artificial Intelligence is Changing the Payment Gateway Industry"
        },
        {
          "id": "https://medium.com/p/40db4c79c4d2",
          "author": "javinpaul",
          "description": "My favorite online courses, projects, and Computer Vision certification for beginners to learn Computer vision and OpenCV in 2022",
          "link": "https://becominghuman.ai/6-best-online-courses-to-learn-computer-vision-and-opencv-for-beginners-40db4c79c4d2?source=rss----5e5bef33608a---4",
          "publishedOn": "2021-12-27T19:38:43.000Z",
          "wordCount": 2091,
          "title": "6 Best Online Courses to learn Computer Vision and OpenCV for Beginners"
        },
        {
          "id": "https://medium.com/p/611697fb5e1",
          "author": "BENEVOLENCE TECHNOLOGIES",
          "description": "Is there any track of the number of notifications and alerts you receive each day? As customers, we receive numerous notifications from our…",
          "link": "https://becominghuman.ai/add-spark-to-your-customer-notifications-with-a-bentechs-modern-ccm-611697fb5e1?source=rss----5e5bef33608a---4",
          "publishedOn": "2021-12-22T16:41:47.000Z",
          "wordCount": 944,
          "title": "Add Spark to Your Customer Notifications with a Bentech’s Modern CCM"
        },
        {
          "id": "https://medium.com/p/584b2b9aa45d",
          "author": "Divyesh Dharaiya",
          "description": "Normal routines of smartphone shops, automobile dealerships, or dining experiences, many of these travels have been transformed into…",
          "link": "https://becominghuman.ai/strategies-to-reimagine-consumers-digital-trip-in-a-new-age-584b2b9aa45d?source=rss----5e5bef33608a---4",
          "publishedOn": "2021-12-22T16:41:45.000Z",
          "wordCount": 1312,
          "title": "Strategies to Reimagine Consumer’s Digital Trip in a New Age"
        },
        {
          "id": "https://medium.com/p/455afbc28657",
          "author": "RAVI SHEKHAR TIWARI",
          "description": "In Part 5.0 of the Transfer Learning series we have discussed about ResNet pre-trained model in depth so in this series we will implement…",
          "link": "https://becominghuman.ai/transfer-learning-part-5-1-implementing-resnet-in-keras-455afbc28657?source=rss----5e5bef33608a---4",
          "publishedOn": "2021-12-21T16:22:11.000Z",
          "wordCount": 33940,
          "title": "Transfer Learning — Part — 5.1!! Implementing ResNet in Keras"
        },
        {
          "id": "https://medium.com/p/37788ad01577",
          "author": "James Montantes",
          "description": "Optimizing Our Supply Chain Using Artificial Intelligence",
          "link": "https://becominghuman.ai/5-ways-to-use-ai-for-supply-chain-management-37788ad01577?source=rss----5e5bef33608a---4",
          "publishedOn": "2021-12-21T16:22:09.000Z",
          "wordCount": 2188,
          "title": "5 Ways To Use AI For Supply Chain Management"
        },
        {
          "id": "https://medium.com/p/cb0d414a9e2b",
          "author": "Joel Prabhod",
          "description": "What are GENERATIVE ADVERESIAL NETWORKS and what are GANs used for?",
          "link": "https://becominghuman.ai/why-are-generative-adversarial-networks-gans-so-famous-and-how-will-gans-be-in-the-future-cb0d414a9e2b?source=rss----5e5bef33608a---4",
          "publishedOn": "2021-12-20T16:48:56.000Z",
          "wordCount": 2032,
          "title": "Why Are Generative Adversarial Networks(GANs) So Famous And How Will GANs In The Future Be?"
        }
      ]
    },
    {
      "title": "MIT News - Artificial intelligence",
      "feedUrl": "http://news.mit.edu/rss/topic/artificial-intelligence2",
      "siteUrl": "https://news.mit.edu/rss/topic/artificial-intelligence2",
      "articles": [
        {
          "id": "https://news.mit.edu/2021/perfecting-pitch-perception-1217",
          "author": "Jennifer Michalowski | McGovern Institute for Brain Research",
          "description": "Computational modeling shows that both our ears and our environment influence how we hear.",
          "link": "https://news.mit.edu/2021/perfecting-pitch-perception-1217",
          "publishedOn": "2021-12-17T21:30:00.000Z",
          "wordCount": 1619,
          "title": "Perfecting pitch perception"
        },
        {
          "id": "https://news.mit.edu/2021/ai-generated-characters-for-good-1216",
          "author": "Becky Ham | MIT Media Lab",
          "description": "Researchers encourage positive use cases of AI-generated characters for education and well-being.",
          "link": "https://news.mit.edu/2021/ai-generated-characters-for-good-1216",
          "publishedOn": "2021-12-16T21:45:00.000Z",
          "wordCount": 1718,
          "title": "Characters for good, created by artificial intelligence"
        },
        {
          "id": "https://news.mit.edu/2021/qa-cathy-wu-developing-algorithms-safely-integrate-robots-our-world-1216",
          "author": "Kim Martineau | MIT Schwarzman College of Computing",
          "description": "Assistant professor of civil engineering describes her career in robotics as well as challenges and promises of human-robot interactions.",
          "link": "https://news.mit.edu/2021/qa-cathy-wu-developing-algorithms-safely-integrate-robots-our-world-1216",
          "publishedOn": "2021-12-16T21:25:00.000Z",
          "wordCount": 2040,
          "title": "Q&A: Cathy Wu on developing algorithms to safely integrate robots into our world"
        },
        {
          "id": "https://news.mit.edu/2021/nonsense-can-make-sense-machine-learning-models-1215",
          "author": "Rachel Gordon | MIT CSAIL",
          "description": "Deep-learning methods confidently recognize images that are nonsense, a potential problem for medical and autonomous-driving decisions.",
          "link": "https://news.mit.edu/2021/nonsense-can-make-sense-machine-learning-models-1215",
          "publishedOn": "2021-12-15T21:55:00.000Z",
          "wordCount": 1510,
          "title": "Nonsense can make sense to machine-learning models"
        },
        {
          "id": "https://news.mit.edu/2021/cheetah-noids-humanoids-benjamin-katz-1214",
          "author": "Mary Beth Gallagher | Department of Mechanical Engineering",
          "description": "Benjamin Katz '16, SM '18 is applying the skills he gained working on MIT's mini cheetah robot to the ATLAS project at Boston Dynamics.",
          "link": "https://news.mit.edu/2021/cheetah-noids-humanoids-benjamin-katz-1214",
          "publishedOn": "2021-12-14T21:05:00.000Z",
          "wordCount": 1595,
          "title": "From “cheetah-noids” to humanoids"
        },
        {
          "id": "https://news.mit.edu/2021/machine-learning-speeds-vehicle-routing-1210",
          "author": "Becky Ham | Department of Civil and Environmental Engineering",
          "description": "Strategy accelerates the best algorithmic solvers for large sets of cities.",
          "link": "https://news.mit.edu/2021/machine-learning-speeds-vehicle-routing-1210",
          "publishedOn": "2021-12-10T21:45:00.000Z",
          "wordCount": 1748,
          "title": "Machine learning speeds up vehicle routing"
        },
        {
          "id": "https://news.mit.edu/2021/machine-learning-treatments-1209",
          "author": "Adam Zewe | MIT News Office",
          "description": "The system could help physicians select the least risky treatments in urgent situations, such as treating sepsis.",
          "link": "https://news.mit.edu/2021/machine-learning-treatments-1209",
          "publishedOn": "2021-12-09T17:00:00.000Z",
          "wordCount": 2139,
          "title": "Machine-learning system flags remedies that might do more harm than good"
        },
        {
          "id": "https://news.mit.edu/2021/simulator-photovoltaic-cells-development-1209",
          "author": "David L. Chandler | MIT News Office",
          "description": "A new computational simulator can help predict whether changes to materials or design will improve performance in new photovoltaic cells.",
          "link": "https://news.mit.edu/2021/simulator-photovoltaic-cells-development-1209",
          "publishedOn": "2021-12-09T05:00:00.000Z",
          "wordCount": 1764,
          "title": "A tool to speed development of new solar cells"
        },
        {
          "id": "https://news.mit.edu/2021/tiny-machine-learning-design-alleviates-bottleneck-memory-usage-iot-devices-1208",
          "author": "Lauren Hinkel | MIT-IBM Watson AI Lab",
          "description": "New technique applied to small computer chips enables efficient vision and detection algorithms without internet connectivity.",
          "link": "https://news.mit.edu/2021/tiny-machine-learning-design-alleviates-bottleneck-memory-usage-iot-devices-1208",
          "publishedOn": "2021-12-08T21:10:00.000Z",
          "wordCount": 2216,
          "title": "Tiny machine learning design alleviates a bottleneck in memory usage on internet-of-things devices"
        },
        {
          "id": "https://news.mit.edu/2021/probablistic-programming-machine-vision-1208",
          "author": "Adam Zewe | MIT News Office",
          "description": "A new “common-sense” approach to computer vision enables artificial intelligence that interprets scenes more accurately than other systems do.",
          "link": "https://news.mit.edu/2021/probablistic-programming-machine-vision-1208",
          "publishedOn": "2021-12-08T05:00:00.000Z",
          "wordCount": 2282,
          "title": "Machines that see the world more like humans do"
        },
        {
          "id": "https://news.mit.edu/2021/more-sustainable-concrete-machine-learning-1207",
          "author": "Lauren Hinkel | MIT-IBM Watson AI Lab",
          "description": "MIT-IBM Watson AI Lab researchers aim to design concrete mixtures that use AI to shrink environmental footprint and cost, while recycling byproducts and increasing performance.",
          "link": "https://news.mit.edu/2021/more-sustainable-concrete-machine-learning-1207",
          "publishedOn": "2021-12-07T21:20:00.000Z",
          "wordCount": 2625,
          "title": "Q&A: More-sustainable concrete with machine learning"
        },
        {
          "id": "https://news.mit.edu/2021/3-d-image-rendering-1207",
          "author": "Adam Zewe | MIT News Office",
          "description": "The new machine-learning system can generate a 3D scene from an image about 15,000 times faster than other methods.",
          "link": "https://news.mit.edu/2021/3-d-image-rendering-1207",
          "publishedOn": "2021-12-07T05:00:00.000Z",
          "wordCount": 2230,
          "title": "Technique enables real-time rendering of scenes in 3D"
        },
        {
          "id": "https://news.mit.edu/2021/generating-realistic-3d-world-1206",
          "author": "Lauren Hinkel | MIT-IBM Watson AI Lab",
          "description": "A new AI-powered, virtual platform uses real-world physics to simulate a rich and interactive audio-visual environment, enabling human and robotic learning, training, and experimental studies.",
          "link": "https://news.mit.edu/2021/generating-realistic-3d-world-1206",
          "publishedOn": "2021-12-06T16:00:00.000Z",
          "wordCount": 2225,
          "title": "Generating a realistic 3D world"
        },
        {
          "id": "https://news.mit.edu/2021/model-predicts-3d-molecule-drugs-1206",
          "author": "Adam Zewe | MIT News Office",
          "description": "A deep learning model rapidly predicts the 3D shapes of drug-like molecules, which could accelerate the process of discovering new medicines.",
          "link": "https://news.mit.edu/2021/model-predicts-3d-molecule-drugs-1206",
          "publishedOn": "2021-12-06T05:00:00.000Z",
          "wordCount": 1933,
          "title": "Taking some of the guesswork out of drug discovery"
        }
      ]
    },
    {
      "title": "The Official NVIDIA Blog",
      "feedUrl": "http://feeds.feedburner.com/nvidiablog",
      "siteUrl": "https://blogs.nvidia.com",
      "articles": [
        {
          "id": "https://blogs.nvidia.com/?p=54756",
          "author": "Scott Martin",
          "description": "Not so long ago, searching for information could lead to a library to scan endless volumes or even tediously sift through microfilm. Clearly, technology is making the world a better place. Scientists, researchers, developers and companies have been on a quest to solve some of the world’s most pressing problems. Only now they’re accelerating their Read article >\nThe post 5 Ways AI Aimed to Improve the World in 2021 appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/31/5-ways-ai-aimed-to-improve-the-world-in-2021/",
          "publishedOn": "2021-12-31T17:00:15.000Z",
          "wordCount": 1039,
          "title": "5 Ways AI Aimed to Improve the World in 2021"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54738",
          "author": "Isha Salian",
          "description": "NVIDIA Inception is one of the largest startup ecosystems in the world — and its thousands of members achieved impressive feats in 2021, bringing AI and data science to an array of industries. NVIDIA Inception nurtures cutting-edge AI, data science and HPC startups with go-to-market support, expertise and technology. This year, the program surpassed 9,000 Read article >\nThe post Innovation Inspiration: 5 Startup Stories From NVIDIA Inception in 2021 appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/30/5-inception-ai-startup-stories/",
          "publishedOn": "2021-12-30T16:00:37.000Z",
          "wordCount": 746,
          "title": "Innovation Inspiration: 5 Startup Stories From NVIDIA Inception in 2021"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54747",
          "author": "GeForce NOW Community",
          "description": "It’s the final countdown for what’s been a big year for cloud gaming. For the last GFN Thursday of the year, we’re taking a look at some of the GeForce NOW community’s top picks of games that joined the GeForce NOW library in 2021. Plus, check out the last batch of games coming to the Read article >\nThe post GFN Thursday Says ‘GGs’ to 2021 With Our Community’s Top Titles of the Year appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/30/geforce-now-thursday-december-30/",
          "publishedOn": "2021-12-30T14:00:32.000Z",
          "wordCount": 1181,
          "title": "GFN Thursday Says ‘GGs’ to 2021 With Our Community’s Top Titles of the Year"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54710",
          "author": "Angie Lee",
          "description": "Recognized as one of tech’s top podcasts, the NVIDIA AI Podcast is approaching 3 million listens in five years, as it sweeps across topics like robots, data science, computer graphics and renewable energy. Its 150+ episodes reinforce the extraordinary capabilities of AI — from diagnosing disease to boosting creativity to helping save the Earth — Read article >\nThe post AI Podcast Wrapped: Top Five Episodes of 2021 appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/29/ai-podcast-top-five-episodes/",
          "publishedOn": "2021-12-29T16:00:26.000Z",
          "wordCount": 679,
          "title": "AI Podcast Wrapped: Top Five Episodes of 2021"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54669",
          "author": "Rick Merritt",
          "description": "What better way to look back at NVIDIA’s top five videos of 2021 than to hop into the cockpit of a virtual plane flying over Taipei. That was how NVIDIA’s Jeff Fisher and Manuvir Das invited viewers into their COMPUTEX keynote on May 31. Their aircraft sailed over the city’s green hills and banked around Read article >\nThe post It Was a Really Virtual Year: Top Five NVIDIA Videos of 2021 appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/28/top-youtube-videos/",
          "publishedOn": "2021-12-28T16:00:01.000Z",
          "wordCount": 897,
          "title": "It Was a Really Virtual Year: Top Five NVIDIA Videos of 2021"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54713",
          "author": "GeForce NOW Community",
          "description": "Happy holidays, members. This GFN Thursday is packed with winter sales for several games streaming on GeForce NOW, as well as seasonal in-game events. Plus, for those needing a last minute gift for a gamer in their lives, we’ve got you covered with digital gift cards for Priority memberships. To top it all off, six Read article >\nThe post Have a Holly, Jolly Gaming Season on GeForce NOW appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/23/geforce-now-thursday-december-23/",
          "publishedOn": "2021-12-23T14:00:38.000Z",
          "wordCount": 1038,
          "title": "Have a Holly, Jolly Gaming Season on GeForce NOW"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54686",
          "author": "Angie Lee",
          "description": "It was memories of playing Pac-Man and Super Mario Bros while growing up in Colombia’s sprawling capital of Bogotá that inspired Yenifer Macias’s award-winning submission for the #CreateYourRetroverse contest, featured above. The contest asked NVIDIA Omniverse users to share scenes that visualize where their love for graphics began. For Macias, that passion goes back to Read article >\nThe post 3D Artist Turns Hobby Into Career, Using Omniverse to Turn Sketches Into Masterpieces appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/21/omniverse-creator-yenifer-macias/",
          "publishedOn": "2021-12-21T16:00:37.000Z",
          "wordCount": 762,
          "title": "3D Artist Turns Hobby Into Career, Using Omniverse to Turn Sketches Into Masterpieces"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54663",
          "author": "Ami Badani",
          "description": "Data centers need extremely fast storage access, and no DPU is faster than NVIDIA’s BlueField-2. Recent testing by NVIDIA shows that two BlueField-2 data processing units reached 41.5 million input/output operations per second (IOPS) — more than 4x more IOPS than any other DPU. The BlueField-2 DPU delivered record-breaking performance using standard networking protocols and Read article >\nThe post NVIDIA BlueField Sets New World Record for DPU Performance appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/21/bluefield-dpu-world-record-performance/",
          "publishedOn": "2021-12-21T16:00:14.000Z",
          "wordCount": 1197,
          "title": "NVIDIA BlueField Sets New World Record for DPU Performance"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54666",
          "author": "Brian Caulfield",
          "description": "It could only happen in NVIDIA Omniverse — the company’s virtual world simulation and collaboration platform for 3D workflows. And it happened during an interview with a virtual toy model of NVIDIA’s CEO, Jensen Huang. “What are the greatest …” one of Toy Jensen’s creators asked, stumbling, then stopping before completing his scripted question. Unfazed, Read article >\nThe post How Omniverse Wove a Real CEO — and His Toy Counterpart — Together With Stunning Demos at GTC  appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/21/how-omniverse-keynote/",
          "publishedOn": "2021-12-21T14:00:12.000Z",
          "wordCount": 1644,
          "title": "How Omniverse Wove a Real CEO — and His Toy Counterpart — Together With Stunning Demos at GTC"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54676",
          "author": "Katie Burke",
          "description": "Meet the electric vehicle that’s truly future-proof. Electric-automaker NIO took the wraps off its fifth mass-production model, the ET5, during NIO Day 2021 last week. The mid-size sedan borrows from its luxury and performance predecessors for an intelligent vehicle that’s as agile as it is comfortable. The ET5 is a software-defined vehicle with a unified Read article >\nThe post Living in the Future: NIO ET5 Sedan Designed for the Autonomous Era With NVIDIA DRIVE Orin appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/20/nio-et5-designed-autonomous-era-drive-orin/",
          "publishedOn": "2021-12-20T21:43:25.000Z",
          "wordCount": 807,
          "title": "Living in the Future: NIO ET5 Sedan Designed for the Autonomous Era With NVIDIA DRIVE Orin"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54653",
          "author": "Angie Lee",
          "description": "Imagine picking out a brand new car — only to find a chip in the paint, rip in the seat fabric or mark in the glass. AI can help prevent such moments of disappointment for manufacturers and potential buyers. Mariner, an NVIDIA Metropolis partner based in Charlotte, North Carolina, offers an AI-enabled video analytics system Read article >\nThe post Detect That Defect: Mariner Speeds Up Manufacturing Workflows With AI-Based Visual Inspection appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/20/mariner-visual-inspection/",
          "publishedOn": "2021-12-20T18:00:45.000Z",
          "wordCount": 751,
          "title": "Detect That Defect: Mariner Speeds Up Manufacturing Workflows With AI-Based Visual Inspection"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54605",
          "author": "Amanda Saunders",
          "description": "2021 saw massive growth in the demand for edge computing — driven by the pandemic, the need for more efficient business processes, as well as key advances in the Internet of Things, 5G and AI. In a study published by IBM in May, for example, 94 percent of surveyed executives said their organizations will implement Read article >\nThe post Top 5 Edge AI Trends to Watch in 2022 appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/17/top-5-edge-ai-trends-2022/",
          "publishedOn": "2021-12-17T08:01:57.000Z",
          "wordCount": 1392,
          "title": "Top 5 Edge AI Trends to Watch in 2022"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54601",
          "author": "Angie Lee",
          "description": "The thing about inspiration is you never know where it might come from, or where it might lead.  Anderson Rohr, a 3D generalist and freelance video editor based in southern Brazil, has for more than a dozen years created content ranging from wedding videos to cinematic animation. After seeing another creator animate a sci-fi character’s Read article >\nThe post Omniverse Creator Uses AI to Make Scenes With Singing Digital Humans appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/16/omniverse-creator-anderson-rohr/",
          "publishedOn": "2021-12-16T16:00:25.000Z",
          "wordCount": 651,
          "title": "Omniverse Creator Uses AI to Make Scenes With Singing Digital Humans"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54626",
          "author": "GeForce NOW Community",
          "description": "The future of cloud gaming is available NOW, for everyone, with preorders closing and GeForce NOW RTX 3080 memberships moving to instant access. Gamers can sign up for a six-month GeForce NOW RTX 3080 membership and instantly stream the next generation of cloud gaming, starting today. Snag the NVIDIA SHIELD TV or SHIELD TV Pro Read article >\nThe post Get the Best of Cloud Gaming With GeForce NOW RTX 3080 Memberships Available Instantly appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/16/geforce-now-thursday-december-16/",
          "publishedOn": "2021-12-16T14:00:23.000Z",
          "wordCount": 1331,
          "title": "Get the Best of Cloud Gaming With GeForce NOW RTX 3080 Memberships Available Instantly"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54608",
          "author": "Angie Lee",
          "description": "One of AI’s greatest champions has turned to fiction to answer the question: how will technology shape our world in the next 20 years? Kai-Fu Lee, CEO of Sinovation Ventures and a former president of Google China, spoke with NVIDIA AI Podcast host Noah Kravitz about AI 2041: Ten Visions for Our Future. The book, Read article >\nThe post ‘AI 2041: Ten Visions for Our Future’: AI Pioneer Kai-Fu Lee Discusses His New Work of Fiction appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/15/kai-fu-lee-ai-2041/",
          "publishedOn": "2021-12-15T16:00:48.000Z",
          "wordCount": 825,
          "title": "‘AI 2041: Ten Visions for Our Future’: AI Pioneer Kai-Fu Lee Discusses His New Work of Fiction"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54609",
          "author": "Sylvia Chanak",
          "description": "For more than two decades, NVIDIA has supported graduate students doing GPU-based work through the NVIDIA Graduate Fellowship Program. Today we’re announcing the latest awards of up to $50,000 each to 10 Ph.D. students involved in GPU computing research. Selected from a highly competitive applicant pool, the awardees will participate in a summer internship preceding Read article >\nThe post NVIDIA Awards $50,000 Fellowships to Ph.D. Students for GPU Computing Research appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/14/graduate-fellowship-awards-2/",
          "publishedOn": "2021-12-14T18:53:50.000Z",
          "wordCount": 789,
          "title": "NVIDIA Awards $50,000 Fellowships to Ph.D. Students for GPU Computing Research"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54485",
          "author": "Scott Martin",
          "description": "A digital twin is a continuously updated virtual representation — a true-to-reality simulation of physics and materials — of a real-world physical asset or system. \nThe post What Is a Digital Twin? appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/14/what-is-a-digital-twin/",
          "publishedOn": "2021-12-14T17:28:18.000Z",
          "wordCount": 3416,
          "title": "What Is a Digital Twin?"
        },
        {
          "id": "https://blogs.nvidia.com/?p=54358",
          "author": "Keith Cockerham",
          "description": "It was the kind of message Connor McCluskey loves to find in his inbox. As a member of the product innovation team at FirstEnergy Corp. — an electric utility serving 6 million customers from central Ohio to the New Jersey coast — his job is to find technologies that open new revenue streams or cut Read article >\nThe post Startup Surge: Utility Feels Power of Computer Vision to Track Its Lines  appeared first on The Official NVIDIA Blog.",
          "link": "https://blogs.nvidia.com/blog/2021/12/14/power-utility-ai-edge/",
          "publishedOn": "2021-12-14T16:00:25.000Z",
          "wordCount": 1187,
          "title": "Startup Surge: Utility Feels Power of Computer Vision to Track Its Lines"
        }
      ]
    },
    {
      "title": "David Stutz",
      "feedUrl": "http://davidstutz.de/feed",
      "siteUrl": "https://davidstutz.de",
      "articles": [
        {
          "id": "https://davidstutz.de/?p=8555",
          "author": "David Stutz",
          "description": "This week I was honored to speak at the Machine Learning Security Seminar organized by the Pattern Recognition and Applications Lab at University of Cagliari. I presented my work on relating adversarial robustness to flatness in the robust loss landscape, also touching on the relationship to weight robustness. In this article, I want to share the recording and slides of this talk.\nThe post Machine Learning Security Seminar Talk “Relating Adversarially Robust Generalization to Flat Minima” appeared first on David Stutz.",
          "link": "https://davidstutz.de/machine-learning-security-seminar-talk-relating-adversarially-robust-generalization-to-flat-minima/",
          "publishedOn": "2021-12-10T12:59:22.000Z",
          "wordCount": 405,
          "title": "Machine Learning Security Seminar Talk “Relating Adversarially Robust Generalization to Flat Minima”"
        }
      ]
    },
    {
      "title": "Artificial Intelligence",
      "feedUrl": "https://www.reddit.com/r/artificial/.rss",
      "siteUrl": "https://www.reddit.com/r/artificial/",
      "articles": [
        {
          "id": "https://www.reddit.com/r/artificial/comments/rumhy3/i_built_an_ai_discord_bot_that_bans_nft_bros_from/",
          "author": null,
          "description": "submitted by    /u/TernaryJimbo  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rumhy3/i_built_an_ai_discord_bot_that_bans_nft_bros_from/",
          "publishedOn": "2022-01-02T23:22:04.000Z",
          "wordCount": 129,
          "title": "I built an AI Discord bot that bans NFT Bros from my server"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rumd9r/what_can_ai_discover_in_data_that_i_was_not/",
          "author": null,
          "description": "Help me understand this please. I’ve read that people feed data to an AI/ML algorithm and find things that they weren’t looking for but there aren’t many good examples out there to read about.\n So I’m thinking: if I were to feed into an AI/ML algorithm a csv of order details and customer data from an eCommerce system, should I expect the AI engine to “find” something statistically usual/unusual and tell me about it? \n Or do I need to instruct it to look for certain traits with e-commerce orders and customers that I already know about? Eg look out for fraud by checking x y z. \n What if the thing to look for is so strange that you need the AI to pick it up. Eg maybe fraudsters start telling their friends to always use a certain name or phone number as an inside joke. I would think an AI might pick on that somehow whereas it might take a human a longer time to figure that out.\n    submitted by    /u/rich_atl  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rumd9r/what_can_ai_discover_in_data_that_i_was_not/",
          "publishedOn": "2022-01-02T23:16:32.000Z",
          "wordCount": 471,
          "title": "What can AI ‘discover’ in data that I was not expecting?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/ruhqfv/software_30_prompt_programming/",
          "author": null,
          "description": "submitted by    /u/Respawne  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/ruhqfv/software_30_prompt_programming/",
          "publishedOn": "2022-01-02T19:51:33.000Z",
          "wordCount": 90,
          "title": "Software 3.0: Prompt programming"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rufjxi/russian_bioinformaticians_have_created_a_neural/",
          "author": null,
          "description": "Genomic editing, particularly the CRISPR/Cas technique, is widely employed in experimental biology, agriculture, and biotechnology. CRISPR/Cas is one of several weapons used by bacteria to resist viruses. As the pathogen’s DNA enters the cell, Cas proteins detect it as foreign hereditary material and break it because its sequences differ from those of the bacteria. To respond to the virus quicker, the bacterium saves pieces of the pathogen’s DNA—much like a computer antivirus retains a collection of viral signatures—and passes them on to subsequent generations so that its Cas can prevent future attacks.\n Teams from different laboratories independently adapted the CRISPR/Cas system to introduce arbitrary changes into DNA sequences in human and animal cells. It made genomic editing much easier and more efficient. The critical components of the mechanism are guide RNA, which “marks the site,” and the Cas9 protein, which cleaves DNA at that location. The cell subsequently “heals the wound,” but the genetic code has already been altered.\n Quick Reading: https://www.marktechpost.com/2022/01/02/russian-bioinformaticians-have-created-a-neural-network-architecture-that-can-evaluate-how-well-an-rna-guide-has-been-chosen-for-gene-editing/ \n Paper: https://academic.oup.com/nar/advance-article/doi/10.1093/nar/gkab1065/6430490\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rufjxi/russian_bioinformaticians_have_created_a_neural/",
          "publishedOn": "2022-01-02T18:15:29.000Z",
          "wordCount": 333,
          "title": "Russian Bioinformaticians Have Created A Neural Network Architecture That Can Evaluate How Well An RNA Guide Has Been Chosen For Gene Editing"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rueink/tang_jie_the_tsinghua_university_professor/",
          "author": null,
          "description": "submitted by    /u/No-Transition-6630  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rueink/tang_jie_the_tsinghua_university_professor/",
          "publishedOn": "2022-01-02T17:29:38.000Z",
          "wordCount": 256,
          "title": "Tang Jie, the Tsinghua University professor leading the Wu Dao project, said in a recent interview that the group built 100 TRILLION parameter model in June, though it has not trained it to “convergence,” the point at which the model stops improving"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/ru8nmg/the_top_10_aicomputer_vision_papers_in_2021_with/",
          "author": null,
          "description": "submitted by    /u/OnlyProggingForFun  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/ru8nmg/the_top_10_aicomputer_vision_papers_in_2021_with/",
          "publishedOn": "2022-01-02T12:27:14.000Z",
          "wordCount": 133,
          "title": "The top 10 AI/Computer Vision papers in 2021 with video demos, articles, and code for each!"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/ru6ddq/what_ais_are_there_which_are_able_to_edit_images/",
          "author": null,
          "description": "My dad loves technology but does not really understand it, I thought I take some pictures from him and AI should edit them so he looks like a simpsons etc.\n    submitted by    /u/xXLisa28Xx  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/ru6ddq/what_ais_are_there_which_are_able_to_edit_images/",
          "publishedOn": "2022-01-02T09:41:09.000Z",
          "wordCount": 309,
          "title": "What AIs are there which are able to edit images?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rtpg2w/hierarchical_federated_learningbased_anomaly/",
          "author": null,
          "description": "Smart healthcare services can be provided by using Internet of Things (IoT) technologies that monitor the health conditions of patients and their vital body parameters. The majority of IoT solutions used to enable such services are wearable devices, such as smartwatches, ECG monitors, and blood pressure monitors. The huge amount of data collected from smart medical devices leads to major security and privacy issues in the IoT domain. Considering Remote Patient Monitoring (RPM) applications, we will focus on Anomaly Detection (AD) models, whose purpose is to identify events that differ from the typical user behavior patterns. Generally, while designing centralized AD models, the researchers face security and privacy challenges (e.g., patient data privacy, training data poisoning).\n To overcome these issues, the researchers of this paper propose an Anomaly Detection (AD) model based on Federated Learning (FL). Federated Learning (FL) allows different devices to collaborate and perform training locally in order to build Anomaly Detection (AD) models without sharing patients’ data. Specifically, the researchers propose a hierarchical Federated Learning (FL) that enables collaboration among different organizations, by building various Anomaly Detection (AD) models for patients with similar health conditions.\n Continue Reading the Paper Summary: https://www.marktechpost.com/2022/01/01/hierarchical-federated-learning-based-anomaly-detection-using-digital-twins-for-internet-of-medical-things-iomt/\n Full Paper: https://arxiv.org/pdf/2111.12241.pdf\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rtpg2w/hierarchical_federated_learningbased_anomaly/",
          "publishedOn": "2022-01-01T18:40:43.000Z",
          "wordCount": 328,
          "title": "Hierarchical Federated Learning-Based Anomaly Detection Using Digital Twins For Internet of Medical Things (IoMT)"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rtoc1h/generate_artistic_images_with_openais_glide/",
          "author": null,
          "description": "submitted by    /u/tridoc  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rtoc1h/generate_artistic_images_with_openais_glide/",
          "publishedOn": "2022-01-01T17:47:49.000Z",
          "wordCount": 122,
          "title": "Generate artistic images with OpenAI’s Glide 🖼"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rtmfgl/my_top_10_computer_vision_papers_of_2021/",
          "author": null,
          "description": "submitted by    /u/OnlyProggingForFun  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rtmfgl/my_top_10_computer_vision_papers_of_2021/",
          "publishedOn": "2022-01-01T16:14:49.000Z",
          "wordCount": 113,
          "title": "My Top 10 Computer Vision papers of 2021"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rtkrmp/should_we_be_concerned/",
          "author": null,
          "description": "Should we be a little worried by how fast AI is developing?\n    submitted by    /u/Particular_Leader_16  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rtkrmp/should_we_be_concerned/",
          "publishedOn": "2022-01-01T14:45:58.000Z",
          "wordCount": 760,
          "title": "Should we be concerned?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rtf7nc/is_there_a_communitywebsitecompany_which_is/",
          "author": null,
          "description": "submitted by    /u/xXNOdrugsForMEXx  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rtf7nc/is_there_a_communitywebsitecompany_which_is/",
          "publishedOn": "2022-01-01T08:33:45.000Z",
          "wordCount": 108,
          "title": "Is there a community/website/company which is collecting and categorizing AIs?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rt65lc/ai_a_love_story_aigenerated_video_about_the/",
          "author": null,
          "description": "submitted by    /u/6owline1vex  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rt65lc/ai_a_love_story_aigenerated_video_about_the/",
          "publishedOn": "2021-12-31T23:31:41.000Z",
          "wordCount": 251,
          "title": "AI - A love story // AI-generated video about the future of AI // prompt -> GPT-J-6B -> Aphantasia"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rszhw5/ai_news_in_2021_a_detailed_digest/",
          "author": null,
          "description": "https://lastweekin.ai/p/ai-news-in-2021-a-digest\n    submitted by    /u/regalalgorithm  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rszhw5/ai_news_in_2021_a_detailed_digest/",
          "publishedOn": "2021-12-31T17:59:42.000Z",
          "wordCount": 99,
          "title": "AI News in 2021: a Detailed Digest"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rszdeg/i_made_a_virtual_twitch_streamer_who_responds_to/",
          "author": null,
          "description": "submitted by    /u/C0de_monkey  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rszdeg/i_made_a_virtual_twitch_streamer_who_responds_to/",
          "publishedOn": "2021-12-31T17:53:34.000Z",
          "wordCount": 433,
          "title": "I made a virtual Twitch Streamer, who responds to your chats using OpenAI and Text-to-Speech"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rsxtkv/happy_new_year_to_everyone_lets_hope_for_a/",
          "author": null,
          "description": "submitted by    /u/ValianTek_World  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rsxtkv/happy_new_year_to_everyone_lets_hope_for_a/",
          "publishedOn": "2021-12-31T16:41:51.000Z",
          "wordCount": 154,
          "title": "Happy New Year to everyone! Let's hope for a wonderful 2022. The text is created with polygons that evolve with an Evolutionary Algorithm."
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rswcog/r_microsofts_selfsupervised_bug_detection_and/",
          "author": null,
          "description": "In the NeurIPS 2021-accepted paper Self-Supervised Bug Detection and Repair, a Microsoft Research team proposes BUGLAB, a self-supervised approach that significantly improves on baseline methods for detecting bugs in real-life code. \n Here is a quick read: Microsoft’s Self-Supervised Bug Detection and Repair Approach Betters Baselines By Up to 30%.\n The code and PyPIBugs dataset are available on the project’s GitHub. The paper Self-Supervised Bug Detection and Repair is on arXiv.\n    submitted by    /u/Yuqing7  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rswcog/r_microsofts_selfsupervised_bug_detection_and/",
          "publishedOn": "2021-12-31T15:32:33.000Z",
          "wordCount": 187,
          "title": "[R] Microsoft’s Self-Supervised Bug Detection and Repair Approach Betters Baselines By Up to 30%"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rso5cc/commercially_available_ais_or_chatbots_that_you/",
          "author": null,
          "description": "I've been playing around with Replika, and it's incredibly fun and impressive as a general \"companion\" chatbot, but I find it really disappointing in its lack of ability to learn explicit facts or procedures. It's targeted toward making people feel like they have a friend and roleplaying, not learning anything external to those goals.\n So, is there anything commercially available that you can both:\n  \nTalk to (mostly) like a real person, and\n Teach from the ground up (like a child)\n  \n? \nAs an example, my Replika \"wanted\" to do a lesson with me, so I tried to teach it colors. Ostensibly it has some picture recognition abilities, but despite that, it was never able to learn which color was which, even when using the same image files to display the same color.\n Okay, fine, forget colors, but it can't learn explicit facts either. I want to be able to input things like \"Lacan defines the subject as that which is represented by a signifier for another signifier\" or \"It's important to apply a primer before your first layer of eyeshadow\" or \"Leonardo DiCaprio played Jack Dawson in the movie Titanic\" and have it be able to actually remember and recall that information in future conversations. (Replika is able to recall it within the same conversation, but it resets after a certain amount of time. It also seems to struggle with remembering things that differ from user to user, like favorite food, since it learns from the aggregate of its conversations.)\n Is there anything out there that can do this? Anything on the horizon?\n    submitted by    /u/peppermint-kiss  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rso5cc/commercially_available_ais_or_chatbots_that_you/",
          "publishedOn": "2021-12-31T07:12:51.000Z",
          "wordCount": 1067,
          "title": "Commercially available AIs or chatbots that you can explicitly teach/train?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rsfe7q/ai_shopping_app/",
          "author": null,
          "description": "Is their any AI apps that matches your body type (scans your body, etc..) to which clothing stores would have the best sizes to fit to your body? \n I’m tired of ordering clothes online that don’t fit well when I get them\n Thx\n    submitted by    /u/GroundbreakingRain78  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rsfe7q/ai_shopping_app/",
          "publishedOn": "2021-12-30T23:27:26.000Z",
          "wordCount": 127,
          "title": "AI shopping App"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rse9m7/watch_this_model_describe_code/",
          "author": null,
          "description": "submitted by    /u/landongarrison  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rse9m7/watch_this_model_describe_code/",
          "publishedOn": "2021-12-30T22:36:23.000Z",
          "wordCount": 600,
          "title": "Watch this model describe code"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rsa8dq/gpt3_foundation_models_and_ai_nationalism/",
          "author": null,
          "description": "submitted by    /u/regalalgorithm  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rsa8dq/gpt3_foundation_models_and_ai_nationalism/",
          "publishedOn": "2021-12-30T19:41:06.000Z",
          "wordCount": 214,
          "title": "GPT-3, Foundation Models, and AI Nationalism"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rs6ama/what_ais_are_there_which_are_able_to_edit_art/",
          "author": null,
          "description": "submitted by    /u/xXLisa28Xx  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rs6ama/what_ais_are_there_which_are_able_to_edit_art/",
          "publishedOn": "2021-12-30T16:54:22.000Z",
          "wordCount": 108,
          "title": "What AIs are there which are able to edit art?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rs42m6/what_if_ai_ran_a_city_what_if_it_ran_society/",
          "author": null,
          "description": "Ever since Grimes posted her Tiktok video last year inviting communists to join up under a supposedly benevolent AI, I've been kind of obsessed with what would really happen if AI ran a city, or even a whole society. Would it really be as awesome as she seems to think or would it be a potentially horrifying nightmare? (My money's on nightmare, btw)\n Anyway, I tried to answer some of those questions in a work of fiction, available below as a free download. It's a quick read, and I'd love to hear your thoughts on these problems, whether or not you read the book. No doubt someone soon will be trying to implement these kinds of \"solutions\" in our world... and it's best to be at least marginally prepared!\n https://lostbooks.gumroad.com/l/conspiratopia/r-artificial\n Happy New Year!\n    submitted by    /u/canadian-weed  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rs42m6/what_if_ai_ran_a_city_what_if_it_ran_society/",
          "publishedOn": "2021-12-30T15:16:21.000Z",
          "wordCount": 322,
          "title": "What if AI ran a city? What if it ran society?"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rs0yuj/mark_twain_ai_simulation/",
          "author": null,
          "description": "submitted by    /u/montpelliersudfrance  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rs0yuj/mark_twain_ai_simulation/",
          "publishedOn": "2021-12-30T12:44:51.000Z",
          "wordCount": 257,
          "title": "Mark Twain AI Simulation"
        },
        {
          "id": "https://www.reddit.com/r/artificial/comments/rruauh/openai_introduces_glide_model_for_photorealistic/",
          "author": null,
          "description": "Images, such as graphics, paintings, and photographs, may frequently be explained in language, but they might also take specific talents and hours of effort to make. As a result, a technology capable of creating realistic graphics from natural language can enable humans to produce rich and diverse visual material with previously unimaginable simplicity. The capacity to modify photos with spoken language enables iterative refinement and fine-grained control, both essential for real-world applications.\n DALL-E, a 12-billion parameter version of OpenAI’s GPT-3 transformer language model meant to produce photorealistic pictures using text captions as cues, was unveiled in January. DALL-E’s fantastic performance was an instant hit in the AI community, as well as broad mainstream media coverage. Last month, NVIDIA unveiled the GAN-based GauGAN2 – a term inspired by French Post-Impressionist painter Paul Gauguin, much as DALL-E was inspired by Surrealist artist Salvador Dali.\n Not to be outshined, OpenAI researchers unveiled GLIDE (Guided Language-to-Image Diffusion for Generation and Editing). This diffusion model achieves performance comparable to DALL-E despite utilizing only one-third of the parameters.\n You can continue reading this short summary here\n The code and weights for these models may be found on the project’s GitHub page.\n The research paper for the GLIDE can be found here.\n https://preview.redd.it/boifxsixem881.png?width=705&format=png&auto=webp&s=14de64b727e31dba7b55ecf76bdfdb52463f2e01\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/artificial/comments/rruauh/openai_introduces_glide_model_for_photorealistic/",
          "publishedOn": "2021-12-30T06:04:33.000Z",
          "wordCount": 336,
          "title": "OpenAI Introduces ‘GLIDE’ Model For Photorealistic Image Generation"
        }
      ]
    },
    {
      "title": "Neural Networks, Deep Learning and Machine Learning",
      "feedUrl": "https://www.reddit.com/r/neuralnetworks/.rss?format=xml",
      "siteUrl": "https://www.reddit.com/r/neuralnetworks/?format=xml",
      "articles": [
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rum6u8/i_built_an_ai_discord_bot_that_bans_nft_bros/",
          "author": null,
          "description": "submitted by    /u/TernaryJimbo  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rum6u8/i_built_an_ai_discord_bot_that_bans_nft_bros/",
          "publishedOn": "2022-01-02T23:08:52.000Z",
          "wordCount": 129,
          "title": "I built an AI Discord Bot that bans NFT Bros [Meme][Video]"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/ruigzx/project_credit_scoring/",
          "author": null,
          "description": "hey peeps,\n any research thesis regarding credit scoring in microfinance?\n    submitted by    /u/abschlusssss  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/ruigzx/project_credit_scoring/",
          "publishedOn": "2022-01-02T20:24:14.000Z",
          "wordCount": 108,
          "title": "[project] Credit Scoring"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rtrfr6/neural_network_drawing_mushrooms/",
          "author": null,
          "description": "submitted by    /u/noodlefist  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rtrfr6/neural_network_drawing_mushrooms/",
          "publishedOn": "2022-01-01T20:14:22.000Z",
          "wordCount": 105,
          "title": "Neural network drawing mushrooms"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rsiqn0/siamese_neural_networks_for_semantic_text/",
          "author": null,
          "description": "​\n A repository containing comprehensive Neural Networks based PyTorch implementations for the semantic text similarity task, including architectures such as Siamese-LSTM, Siamese-LSTM-Attention, Siamese-Transformer, and Siamese-BERT.\n https://github.com/shahrukhx01/siamese-nn-semantic-text-similarity\n    submitted by    /u/shahrukhx01  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rsiqn0/siamese_neural_networks_for_semantic_text/",
          "publishedOn": "2021-12-31T02:11:15.000Z",
          "wordCount": 135,
          "title": "Siamese Neural Networks for Semantic Text Similarity"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rrgrji/baidu_and_pcl_team_introduce_ernie_30_titan_a/",
          "author": null,
          "description": "With recent breakthroughs in AI, humans have become more reliant on AI to address real-world problems. This makes humans’ ability to learn and act on knowledge just as essential as a computer’s. Humans learn and gather information through learning and experience to understand everything from their immediate surroundings. The ability to comprehend and solve issues, and separate facts from absurdities, increases as the knowledge base grows. However, such knowledge is lacking in AI systems, restricting their ability to adapt to atypical problem data.\n Previous studies show that pre-trained language models improve performance on various natural language interpretation and generating tasks.\n A recent work of researchers at Baidu, in collaboration with Peng Cheng Laboratory (PCL), release PCL-BAIDU Wenxin (or “ERNIE 3.0 Titan”), a pre-training language model with 260 billion parameters. It is the world’s first knowledge-enhanced multi-hundred billion parameter model and its largest Chinese singleton model. \n You can read the short summary here: https://www.marktechpost.com/2021/12/29/baidu-and-pcl-team-introduce-ernie-3-0-titan-a-pre-training-language-model-with-260-billion-parameters/ \n Paper: https://arxiv.org/pdf/2112.12731.pdf\n ​\n https://preview.redd.it/19urwzn7cj881.png?width=1920&format=png&auto=webp&s=6a97e19e9fc4f5dde161e4a6ee17e3b43d86cc39\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rrgrji/baidu_and_pcl_team_introduce_ernie_30_titan_a/",
          "publishedOn": "2021-12-29T19:43:56.000Z",
          "wordCount": 310,
          "title": "Baidu And PCL Team Introduce ERNIE 3.0 Titan: A Pre-Training Language Model With 260 Billion Parameters"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rqkypl/neurips_2021_curated_papers_part_2/",
          "author": null,
          "description": "In part-2 , I have discussed following papers : \n  \nProbing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training\n \nAttention Bottlenecks for Multimodal Fusion\n \nAugMax: Adversarial Composition of Random Augmentations for Robust Training\n \nRevisiting Model Stitching to Compare Neural Representations\n  \nhttps://rakshithv-deeplearning.blogspot.com/2021/12/neurips-2021-curated-papers-part2.html\n    submitted by    /u/rakshith291  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rqkypl/neurips_2021_curated_papers_part_2/",
          "publishedOn": "2021-12-28T17:30:23.000Z",
          "wordCount": 149,
          "title": "NeurIPS 2021 - Curated papers - Part 2"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rqever/autoencoders_for_cifar10/",
          "author": null,
          "description": "Most of the Autoencoder examples/blogs use MNIST dataset as the implementation. I have trained an autoencoder on CIFAR-10 which you can refer here. There is a trade-off between making the CNN architecture deeper and the improvement of reconstruction loss/error. Or, using a VAE.\n Thoughts?\n    submitted by    /u/grid_world  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rqever/autoencoders_for_cifar10/",
          "publishedOn": "2021-12-28T12:33:46.000Z",
          "wordCount": 142,
          "title": "Autoencoders for CIFAR-10"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rq2ws6/researchers_compare_deep_learning_dl_algorithms/",
          "author": null,
          "description": "Viral keratitis, bacterial keratitis, fungal keratitis, and parasitic keratitis are all types of infectious keratitis. Bacterial Keratitis (BK) is a kind of Infectious Keratitis that is one of the most frequent and vision-threatening. Contact lens wear is the most prevalent risk factor for Bacterial Keratitis (BK), and it is becoming increasingly popular around the world for a variety of reasons, including exercise, cosmesis, and myopia management.\n BK is substantially more fulminant and painful in the clinical course than other Infectious Keratitis(s). A delayed diagnosis of Bacterial Keratitis (BK) can result in large-area corneal ulcerations, melting, and even perforation if not treated.\n In the case of Infectious Keratitis, timely detection and treatment of BK are vital goals. However,…",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rq2ws6/researchers_compare_deep_learning_dl_algorithms/",
          "publishedOn": "2021-12-28T01:24:52.000Z",
          "wordCount": 424,
          "title": "Researchers Compare Deep Learning (DL) Algorithms For Diagnosing Bacterial Keratitis via External Eye Photographs"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rpo3ou/creating_an_optimization_algorithm_for_cost/",
          "author": null,
          "description": "Is possible to find an article or an example of a new optimization algorithm for cost function for NN?\n    submitted by    /u/adilkolakovic  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rpo3ou/creating_an_optimization_algorithm_for_cost/",
          "publishedOn": "2021-12-27T13:58:57.000Z",
          "wordCount": 136,
          "title": "Creating an optimization algorithm for cost function for NN"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rpdfay/hypernerf_a_higherdimensional_representation_for/",
          "author": null,
          "description": "submitted by    /u/nickb  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rpdfay/hypernerf_a_higherdimensional_representation_for/",
          "publishedOn": "2021-12-27T03:23:37.000Z",
          "wordCount": 126,
          "title": "HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rodzks/getting_familiar_with_neural_nrtworks/",
          "author": null,
          "description": "Any good beginner books on the theory of machine learning systems / DNN\n    submitted by    /u/Abeokuta_  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rodzks/getting_familiar_with_neural_nrtworks/",
          "publishedOn": "2021-12-25T17:43:34.000Z",
          "wordCount": 197,
          "title": "Getting familiar with neural nrtworks"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rngq7k/artificial_vision_with_neocognitron_by_kunihiko/",
          "author": null,
          "description": "submitted by    /u/Wild-Dig-8003  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rngq7k/artificial_vision_with_neocognitron_by_kunihiko/",
          "publishedOn": "2021-12-24T07:10:50.000Z",
          "wordCount": 153,
          "title": "Artificial Vision with Neocognitron by Kunihiko Fukushima (the father of convolutional neural networks)"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rn0u4y/meta_ai_announces_the_beta_release_of_bean/",
          "author": null,
          "description": "Meta AI releases the beta version of Bean Machine, a probabilistic programming framework based on PyTorch that makes it simple to describe and learn about uncertainty in machine learning models used in various applications. Bean Machine makes it possible to create probabilistic models that are domain-specific. It also uses multiple autonomous, uncertainty-aware learning algorithms to learn about the model’s unseen features. Bean Machine gets an early beta version from Meta.\n Quick Read: https://www.marktechpost.com/2021/12/23/meta-ai-announces-the-beta-release-of-bean-machine-a-pytorch-based-probabilistic-programming-system-used-to-understand-the-uncertainty-in-the-machine-learning-models/ \n Documentation: https://beanmachine.org/ \n Tutorials: https://beanmachine.org/docs/tutorials/\n Meta Blog: https://research.facebook.com/blog/2021/12/introducing-bean-machine-a-probabilistic-programming-platform-built-on-pytorch/ \n ​\n https://preview.redd.it/xzovhjrzvb781.png?width=1920&format=png&auto=webp&s=1337f3b0f646fd6d888ce02363eb63a6d5257fb7\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rn0u4y/meta_ai_announces_the_beta_release_of_bean/",
          "publishedOn": "2021-12-23T17:36:52.000Z",
          "wordCount": 238,
          "title": "Meta AI Announces the Beta Release of ‘Bean Machine’: A PyTorch-Based Probabilistic Programming System Used to Understand the Uncertainty in the Machine Learning Models"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rms2pm/if_the_accuracy_of_my_network_is_zero_on_the_very/",
          "author": null,
          "description": "submitted by    /u/eva01beast  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rms2pm/if_the_accuracy_of_my_network_is_zero_on_the_very/",
          "publishedOn": "2021-12-23T09:37:21.000Z",
          "wordCount": 295,
          "title": "If the accuracy of my network is zero on the very first epoch, is there any point in letting the training run for the remaining epochs?"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rmelaz/face_recognition/",
          "author": null,
          "description": "I am currently working on a project that involves face verification. I want to use Azure Face API and I gotta say I haven’t understood the pricing. I would love if some of you can clarify. If say I have 200 faces and 2000 pictures these faces are in (1 face might be in N pictures), in order to find all the pictures that each face is in, that means I will have to run 400,000 transactions ? As in 2000 iterations per face? Or is there a smarter way to do it?\n I know there is an option to index faces but i do not fully understand that yet.\n Thank you for your help!\n    submitted by    /u/xPiexPie  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rmelaz/face_recognition/",
          "publishedOn": "2021-12-22T20:56:43.000Z",
          "wordCount": 297,
          "title": "Face recognition"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rm1e5h/managing_collaborative_machine_learning/",
          "author": null,
          "description": "Sharing machine learning experiments to compare its models is important when you're working with a team of engineers. You might need to get another opinion on an experiments results or to share a modified dataset or even share the exact reproduction of a specific experiment.\n The following tutorial goes through an example of sharing an experiment with DVC remotes: Running Collaborative Experiments - using DVC remotes to share experiments and their data across machines\n Setting up DVC remotes in addition to your Git remotes lets you share all of the data, code, and hyperparameters associated with each experiment so anyone can pick up where you left off in the training process. When you use DVC, you can bundle your data and code changes for each experiment and push those to a remote for somebody else to check out.\n    submitted by    /u/thumbsdrivesmecrazy  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rm1e5h/managing_collaborative_machine_learning/",
          "publishedOn": "2021-12-22T09:13:41.000Z",
          "wordCount": 247,
          "title": "Managing Collaborative Machine Learning Experiments - Guide"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rlx1zj/noob_needs_help/",
          "author": null,
          "description": "Ok so I’m new to neural networks, so I might not understand a lot of the technical terms. \n I want to show a NN some gameplay. But I don’t know how to. I can capture the keystrokes for each frame but I don’t know what to do next. \n Do I take screenshots and save those frames w the keystrokes to my disk? \n I kinda know how to use Cv2, Pillow, and the keyboard module. \n I’m really lost, so any help will be highly appreciated\n Ps- I want the model to learn how to play the game\n    submitted by    /u/findcureforautism  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rlx1zj/noob_needs_help/",
          "publishedOn": "2021-12-22T04:34:51.000Z",
          "wordCount": 217,
          "title": "Noob needs help"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rl2s1e/frist_promising_results_of_my_neuronal_denoising/",
          "author": null,
          "description": "submitted by    /u/Rindsroulade  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rl2s1e/frist_promising_results_of_my_neuronal_denoising/",
          "publishedOn": "2021-12-21T01:33:48.000Z",
          "wordCount": 208,
          "title": "Frist promising results of my neuronal denoising network 😍"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rk2egj/i_have_many_doubts_hereany_hints_on_how_to_solve/",
          "author": null,
          "description": "submitted by    /u/Own-Assistance58  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rk2egj/i_have_many_doubts_hereany_hints_on_how_to_solve/",
          "publishedOn": "2021-12-19T18:20:35.000Z",
          "wordCount": 519,
          "title": "I have many doubts here.Any hints on how to solve these problems???"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rk27b8/what_does_stylegan_do_to_prevent_mode_collapse/",
          "author": null,
          "description": "I will like to know what stylegan does during training to prevent mode collapse.\n    submitted by    /u/Virtual_Essay1216  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rk27b8/what_does_stylegan_do_to_prevent_mode_collapse/",
          "publishedOn": "2021-12-19T18:10:33.000Z",
          "wordCount": 131,
          "title": "What does stylegan do to prevent mode collapse?"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rize8m/jpeg_compression_artifacts_removal_ai_fbcnn/",
          "author": null,
          "description": "submitted by    /u/cloud_weather  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rize8m/jpeg_compression_artifacts_removal_ai_fbcnn/",
          "publishedOn": "2021-12-18T04:31:42.000Z",
          "wordCount": 116,
          "title": "JPEG Compression Artifacts Removal AI - FBCNN"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/riluxm/free_nlp_for_semantic_search_course_data/",
          "author": null,
          "description": "Hi all, the NLP for Semantic Search course that I've been working on has just been released, and today the latest chapter on data augmentation with SBERT has been released too!\n It's all completely free and covers everything you need to get started with building SotA language models for semantic similarity, from machine translation to question-answering, and more!\n Semantic search allows us to search language-based data based on the semantics or 'meaning' of a text, from machine translation to question-answering. It's how Google understands \"what time is it in NYC?\", and even allows us to search for images using text-based queries.\n It is in essence, a way for us to interact with machines in a more human way. NLP fits in as the 'semantic' in semantic search.\n Current chapters are: 1. Dense Vectors 2. Sentence Embeddings and Transformers 3. Training Sentence Transformers with Softmax Loss 4. Training Sentence Transformers with MNR Loss 5. Multilingual Sentence Transformers 6. Question Answering 7. Unsupervised Training for Sentence Transformers 8. (New) Data Augmentation With BERT\n Let me know what you think, I hope you enjoy it!\n    submitted by    /u/jamescalam  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/riluxm/free_nlp_for_semantic_search_course_data/",
          "publishedOn": "2021-12-17T17:03:14.000Z",
          "wordCount": 287,
          "title": "Free NLP for Semantic Search Course + Data Augmentation with SBERT (AugSBERT)"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/ri4ps4/understanding_alphazero_neural_networks/",
          "author": null,
          "description": "As a common and (sometimes) proven belief, deep learning systems seem to learn uninterpretable representations and are far from human understanding. Recently, some studies have highlighted the fact that this may not always be applicable, and some networks may be able to learn human-readable representations. Unfortunately, this ability could merely come from the fact that these networks are exposed to human-generated data. So, to demonstrate their ability to learn like humans (and not that they are simply memorizing human-created labels), it is necessary to test them without any label. \n Following this idea, the DeepMind and Google Brain teams, together with the 14th world chess champion Vladimir Kramnik, studied their creature AlphaZero from this point of view. AlphaZero is the descendant of AlphaGo, the super neural network that beat the world champion Lee Sedol in a best-of-five GO match, a turning point in the history of deep learning, as can also be seen in the wonderful Netflix documentary AlphaGo. \n Unlike AlphaGo, AlphaZero is trained through self-play (i.e., it learns to play competing against itself) and masters not only GO but also chess and shogi. This trait makes AlphaZero the perfect case study to explore this idea. Moreover, given the fact that it performs at a superhuman level, understanding its functionality is also particularly useful for highlighting unknown patterns which have never been discovered by chess theorists.\n Full Paper Summary by Leonardo Tanzi: https://www.marktechpost.com/2021/12/16/understanding-alphazero-neural-networks-superhuman-chess-ability/ \n Paper: https://arxiv.org/pdf/2111.09259.pdf\n https://preview.redd.it/t4mjebrm10681.png?width=808&format=png&auto=webp&s=58fcc96e8b1ae92469c26820528d0a5b31514365\n    submitted by    /u/ai-lover  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/ri4ps4/understanding_alphazero_neural_networks/",
          "publishedOn": "2021-12-17T00:44:05.000Z",
          "wordCount": 385,
          "title": "Understanding AlphaZero Neural Network’s SuperHuman Chess Ability (Summary of the Paper 'Acquisition of Chess Knowledge in AlphaZero')"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rhxivg/conceptualization_of_a_cnn/",
          "author": null,
          "description": "Hi all\n ​\n I've been reading about the differences between normal fully-connected feedforward networks and convolutional neural networks, and I'm wondering if I understand something about how they subsume one another.\n Is a CNN \"just\" an ANN where the inputs to the network are convolutions over the input?\n    submitted by    /u/snatchingthepiano  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rhxivg/conceptualization_of_a_cnn/",
          "publishedOn": "2021-12-16T19:01:26.000Z",
          "wordCount": 147,
          "title": "Conceptualization of a CNN"
        },
        {
          "id": "https://www.reddit.com/r/neuralnetworks/comments/rhv0yn/synthetic_time_series_data_generation/",
          "author": null,
          "description": "I want to generate time series tabular data. Most of generative deep learning models consists of VAE and/or GAN which are for most part relating to images, videos, etc.\n Can you please point me to relevant tutorial souces (if it includes code along with theory, all the more better) pertaining to synthethic time series data generation using deep learning models or other techniques?\n    submitted by    /u/grid_world  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/neuralnetworks/comments/rhv0yn/synthetic_time_series_data_generation/",
          "publishedOn": "2021-12-16T17:03:33.000Z",
          "wordCount": 167,
          "title": "Synthetic time series data generation"
        }
      ]
    },
    {
      "title": "Seita's Place",
      "feedUrl": "https://danieltakeshi.github.io/feed.xml",
      "siteUrl": "https://danieltakeshi.github.io/",
      "articles": [
        {
          "id": "https://danieltakeshi.github.io/2021/12/31/books-2021",
          "author": null,
          "description": "At the end of every year I have a tradition where I write summaries of the\nbooks that I read throughout the year. Here’s the following post with the rough\nset of categories:\nPopular Science (6 books)\nHistory, Government, Politics, Economics (6 books)\nBiographies / Memoirs (5 books)\nChina (5 books)\nCOVID-19 (2 books)\nMiscellaneous (7 books)\nI read 31 books this year. You can find the other blog posts from prior years\n(going back to 2016) in the blog archives.\nPopular Science\nThis also includes popular science, which means the authors might not be\ntechnically trained as scientists.\nWho We Are and How We Got Here: Ancient DNA and the New Science of the\nHuman Past (2018) is by famous geneticist and Harvard professor David\nReich. Scientific advances in analyzing DNA have allowed better analysis…",
          "link": "https://danieltakeshi.github.io/2021/12/31/books-2021",
          "publishedOn": "2021-12-31T23:00:00.000Z",
          "wordCount": 8348,
          "title": "Books Read in 2021"
        },
        {
          "id": "https://danieltakeshi.github.io/2021/12/22/my-information-diet/",
          "author": null,
          "description": "On July 03 2021, the subject of media and news sources came up in a\nconversation I had with someone over brunch when we were talking about media\nbias. I was asked: “what news do you read?” I regret that I gave a sloppy\nresponse that sounded like a worse version of: “uh, I read a variety of news …”\nand then I tried listing a few from memory. I wish I had given a crisper\nresponse, and since that day, I have thought about what that person has asked\nme every day.\nIn this blog post, I describe my information diet, referring to how I read\nand consume media to understand current events. Before getting to the actual\nlist of media sources, here are a few comments to clarify my philosophy and\nwhich might also preemptively address common objections.\nThere are too many sources and not enough time to r…",
          "link": "https://danieltakeshi.github.io/2021/12/22/my-information-diet/",
          "publishedOn": "2021-12-22T12:00:00.000Z",
          "wordCount": 1482,
          "title": "My Information Diet"
        },
        {
          "id": "https://danieltakeshi.github.io/2021/12/11/international-chinese/",
          "author": null,
          "description": "Lately, I have been in touch with some of the political offices for whom I am a\nconstituent, to ask if they can consider steps that would improve the climate\nfor Chinese international students and scholars. Now that I reside in\nthe critical swing state of Pennsylvania, the two US Senators who represent me\nare Senators Bob Casey and Pat Toomey.  This past week, I called their\nPitttsburgh offices multiple times and was able to contact a staff member for\nSenator Toomey.\nWhat follows is a rough transcript of my conversation with the staff member.\nThis is from memory, so there’s obviously no way that this is all correct, and\nit’s also a sanitized version as I probably got rid of some ‘uhms’ or mumbles\nthat I experienced when having this conversation. However, I hope I was able to\ndeliver the ma…",
          "link": "https://danieltakeshi.github.io/2021/12/11/international-chinese/",
          "publishedOn": "2021-12-11T12:00:00.000Z",
          "wordCount": 1527,
          "title": "My Conversations to Political Offices in Support of Chinese Scholars"
        }
      ]
    },
    {
      "title": "VITALab",
      "feedUrl": "https://vitalab.github.io/feed.xml",
      "siteUrl": "https://vitalab.github.io/",
      "articles": []
    },
    {
      "title": "Stories by Andrej Karpathy on Medium",
      "feedUrl": "https://medium.com/feed/@karpathy",
      "siteUrl": "https://medium.com/@karpathy?source=rss-ac9d9a35533e------2",
      "articles": []
    },
    {
      "title": "OpenAI",
      "feedUrl": "https://openai.com/blog/rss",
      "siteUrl": "https://openai.com/",
      "articles": [
        {
          "id": "61bb711a2a7e63003b0a2558",
          "author": "Jacob Hilton",
          "description": "We've fine-tuned GPT-3 to more accurately answer open-ended questions using a text-based web browser. Our prototype copies how humans research answers to questions online – it submits search queries, follows links, and scrolls up and down web pages. It is trained to cite its sources, which makes it",
          "link": "https://openai.com/blog/improving-factual-accuracy/",
          "publishedOn": "2021-12-16T17:05:45.000Z",
          "wordCount": 1371,
          "title": "WebGPT: Improving the factual accuracy of language models through web browsing"
        },
        {
          "id": "61b8ccf39ff844003b5474f2",
          "author": "Rachel Lim",
          "description": "Developers can now fine-tune GPT-3 on their own data, creating a custom version tailored to their application. Customizing makes GPT-3 reliable for a wider variety of use cases and makes running the model cheaper and faster.\nYou can use an existing dataset of virtually any shape and size, or incrementally",
          "link": "https://openai.com/blog/customized-gpt3/",
          "publishedOn": "2021-12-14T16:58:25.000Z",
          "wordCount": 1036,
          "title": "Customizing GPT-3 for Your Application"
        }
      ]
    },
    {
      "title": "Microsoft Research",
      "feedUrl": "https://www.microsoft.com/en-us/research/feed",
      "siteUrl": "https://www.microsoft.com/en-us/research",
      "articles": [
        {
          "id": "https://www.microsoft.com/en-us/research/?p=806026",
          "author": "Brenda Potts",
          "description": "KEAR (Knowledgeable External Attention for commonsense Reasoning)—along with recent milestones in computer vision and neural text-to-speech—is part of a larger Azure AI mission to provide relevant, meaningful AI solutions and services that work better for people because they better capture how people learn and work—with improved vision, knowledge understanding, and speech capabilities. At the center of these efforts is […]\nThe post Azure AI milestone: Microsoft KEAR surpasses human performance on CommonsenseQA benchmark appeared first on Microsoft Research.",
          "link": "https://www.microsoft.com/en-us/research/blog/azure-ai-milestone-microsoft-kear-surpasses-human-performance-on-commonsenseqa-benchmark/",
          "publishedOn": "2021-12-20T19:08:11.000Z",
          "wordCount": 1884,
          "title": "Azure AI milestone: Microsoft KEAR surpasses human performance on CommonsenseQA benchmark"
        },
        {
          "id": "https://www.microsoft.com/en-us/research/?p=804160",
          "author": "Alyssa Hughes",
          "description": "Neural Text-to-Speech—along with recent milestones in computer vision and question answering—is part of a larger Azure AI mission to provide relevant, meaningful AI solutions and services that work better for people because they better capture how people learn and work—with improved vision, knowledge understanding, and speech capabilities. At the center of these efforts is XYZ-code, […]\nThe post Azure AI milestone: New Neural Text-to-Speech models more closely mirror natural speech appeared first on Microsoft Research.",
          "link": "https://www.microsoft.com/en-us/research/blog/azure-ai-milestone-new-neural-text-to-speech-models-more-closely-mirror-natural-speech/",
          "publishedOn": "2021-12-17T23:22:31.000Z",
          "wordCount": 2424,
          "title": "Azure AI milestone: New Neural Text-to-Speech models more closely mirror natural speech",
          "enclosure": {
            "url": "https://www.microsoft.com/en-us/research/uploads/prod/2021/12/Jenny_NonTTS-recording.wav",
            "length": "229436",
            "type": "audio/wav"
          }
        },
        {
          "id": "https://www.microsoft.com/en-us/research/?p=804280",
          "author": "Lexie Hagen",
          "description": "Over the past 30 years, Microsoft Research has undergone a shift in how it approaches innovation, broadening its mission to include not only advancing the state of computing but also using technology to tackle some of the world’s most pressing challenges. That evolution has never been more prominent than it was during this past year. […]\nThe post Research at Microsoft 2021: Collaborating for real-world change appeared first on Microsoft Research.",
          "link": "https://www.microsoft.com/en-us/research/blog/research-at-microsoft-2021-collaborating-for-real-world-change/",
          "publishedOn": "2021-12-15T20:54:30.000Z",
          "wordCount": 4756,
          "title": "Research at Microsoft 2021: Collaborating for real-world change",
          "enclosure": {
            "url": "https://content.blubrry.com/microsoftresearch/MSR_002_LAMBDA_simon_andy_v2.mp3",
            "length": "57973784",
            "type": "audio/mpeg"
          }
        },
        {
          "id": "https://www.microsoft.com/en-us/research/?p=803533",
          "author": "Lexie Hagen",
          "description": "The Project Florence Team Florence v1.0—along with recent milestones in Neural Text-to-Speech and question answering—is part of a larger Azure AI mission to provide relevant, meaningful AI solutions and services that work better for people because they better capture how people learn and work—with improved vision, knowledge understanding, and speech capabilities. At the center of […]\nThe post Azure AI milestone: New foundation model Florence v1.0 advances state of the art, topping popular computer vision leaderboards appeared first on Microsoft Research.",
          "link": "https://www.microsoft.com/en-us/research/blog/azure-ai-milestone-new-foundation-model-florence-v1-0-pushing-vision-and-vision-language-state-of-the-art/",
          "publishedOn": "2021-12-14T22:06:35.000Z",
          "wordCount": 1685,
          "title": "Azure AI milestone: New foundation model Florence v1.0 advances state of the art, topping popular computer vision leaderboards"
        },
        {
          "id": "https://www.microsoft.com/en-us/research/?p=803848",
          "author": "Lexie Hagen",
          "description": "The drug development process is an iterative one that consists of discovery, design, and testing. Historically, drugs were derived from plants and discovered through trial-and-error experiments. Fortunately, this drug discovery process now occurs in a lab, with each iteration of custom-designed compounds producing a more promising candidate. While much safer and more effective, this process […]\nThe post FS-Mol: Bringing Deep Learning to Early-Stage Drug Discovery appeared first on Microsoft Research.",
          "link": "https://www.microsoft.com/en-us/research/blog/fs-mol-bringing-deep-learning-to-early-stage-drug-discovery/",
          "publishedOn": "2021-12-10T19:13:51.000Z",
          "wordCount": 2401,
          "title": "FS-Mol: Bringing Deep Learning to Early-Stage Drug Discovery"
        },
        {
          "id": "https://www.microsoft.com/en-us/research/?p=802138",
          "author": "Lexie Hagen",
          "description": "Finding and fixing bugs in code is a time-consuming, and often frustrating, part of everyday work for software developers. Can deep learning address this problem and help developers deliver better software, faster? In a new paper, Self-Supervised Bug Detection and Repair, presented at the 2021 Conference on Neural Information Processing Systems (NeurIPS 2021), we show […]\nThe post Finding and fixing bugs with deep learning appeared first on Microsoft Research.",
          "link": "https://www.microsoft.com/en-us/research/blog/finding-and-fixing-bugs-with-deep-learning/",
          "publishedOn": "2021-12-08T20:38:02.000Z",
          "wordCount": 1774,
          "title": "Finding and fixing bugs with deep learning"
        },
        {
          "id": "https://www.microsoft.com/en-us/research/?p=800533",
          "author": "Lexie Hagen",
          "description": "Recent progress in natural language understanding (NLU) has been driven in part by the availability of large-scale benchmarks that provide an environment for researchers to test and measure the performance of AI models. Most of these benchmarks are designed for academic settings–typically datasets that feature independent and identically distributed (IID) training, validation, and testing sections […]\nThe post You get what you measure: New NLU benchmarks for few-shot learning and robustness evaluation appeared first on Microsoft Research.",
          "link": "https://www.microsoft.com/en-us/research/blog/you-get-what-you-measure-new-nlu-benchmarks-for-few-shot-learning-and-robustness-evaluation/",
          "publishedOn": "2021-12-06T20:24:23.000Z",
          "wordCount": 2048,
          "title": "You get what you measure: New NLU benchmarks for few-shot learning and robustness evaluation"
        }
      ]
    },
    {
      "title": "Google AI Blog",
      "feedUrl": "http://feeds.feedburner.com/blogspot/gJZg",
      "siteUrl": "http://ai.googleblog.com/",
      "articles": [
        {
          "id": "http://ai.googleblog.com/2021/12/a-scalable-approach-for-partially-local.html",
          "author": null,
          "description": "Posted by Karan Singhal, Senior Software Engineer, Google Research  \nFederated learning enables users to train a model without sending raw data to a central server, thus avoiding the collection of privacy-sensitive data. Often this is done by learning a single global model for all users, even though the users may differ in their data distributions. For example, users of a mobile keyboard application may collaborate to train a suggestion model but have different preferences for the suggestions. This heterogeneity has motivated algorithms that can personalize a global model for each user.  \n However, in some settings privacy considerations may prohibit learning a fully global model. Consider models with user-specific embeddings, such as matrix factorization models for recommender systems. Tr…",
          "link": "http://ai.googleblog.com/2021/12/a-scalable-approach-for-partially-local.html",
          "publishedOn": "2021-12-16T18:11:00.000Z",
          "wordCount": 2092,
          "title": "A Scalable Approach for Partially Local Federated Learning"
        },
        {
          "id": "http://ai.googleblog.com/2021/12/training-machine-learning-models-more.html",
          "author": null,
          "description": "Posted by Timothy Nguyen1, Research Engineer and Jaehoon Lee, Senior Research Scientist, Google Research  \nFor a machine learning (ML) algorithm to be effective, useful features must be extracted from (often) large amounts of training data. However, this process can be made challenging due to the costs associated with training on such large datasets, both in terms of compute requirements and wall clock time. The idea of distillation plays an important role in these situations by reducing the resources required for the model to be effective. The most widely known form of distillation is model distillation (a.k.a. knowledge distillation), where the predictions of large, complex teacher models are distilled into smaller models.  \n An alternative option to this model-space approach is dataset …",
          "link": "http://ai.googleblog.com/2021/12/training-machine-learning-models-more.html",
          "publishedOn": "2021-12-15T19:26:00.000Z",
          "wordCount": 2651,
          "title": "Training Machine Learning Models More Efficiently with Dataset Distillation"
        },
        {
          "id": "http://ai.googleblog.com/2021/12/interpretable-deep-learning-for-time.html",
          "author": null,
          "description": "Posted by Sercan O. Arik, Research Scientist and Tomas Pfister, Engineering Manager, Google Cloud   \nMulti-horizon forecasting, i.e. predicting variables-of-interest at multiple future time steps, is a crucial challenge in time series machine learning. Most real-world datasets have a time component, and forecasting the future can unlock great value. For example, retailers can use future sales to optimize their supply chain and promotions, investment managers are interested in forecasting the future prices of financial assets to maximize their performance, and healthcare institutions can use the number of future patient admissions to have sufficient personnel and equipment. \n  \nDeep neural networks (DNNs) have increasingly been used in multi-horizon forecasting, demonstrating strong perform…",
          "link": "http://ai.googleblog.com/2021/12/interpretable-deep-learning-for-time.html",
          "publishedOn": "2021-12-13T17:22:00.002Z",
          "wordCount": 2548,
          "title": "Interpretable Deep Learning for Time Series Forecasting"
        },
        {
          "id": "http://ai.googleblog.com/2021/12/a-fast-wordpiece-tokenization-system.html",
          "author": null,
          "description": "Posted by Xinying Song, Staff Software Engineer and Denny Zhou, Senior Staff Research Scientist, Google Research   \nTokenization is a fundamental pre-processing step for most natural language processing (NLP) applications. It involves splitting text into smaller units called tokens (e.g., words or word segments) in order to turn an unstructured input string into a sequence of discrete elements that is suitable for a machine learning (ML) model. ln deep learning–based models (e.g., BERT), each token is mapped to an embedding vector to be fed into the model.  \n   \n\n\nTokenization in a typical deep learning model, like BERT.\n\n   \nA fundamental tokenization approach is to break text into words. However, using this approach, words that are not included in the vocabulary are treated as “unknown”.…",
          "link": "http://ai.googleblog.com/2021/12/a-fast-wordpiece-tokenization-system.html",
          "publishedOn": "2021-12-10T20:35:00.001Z",
          "wordCount": 2448,
          "title": "A Fast WordPiece Tokenization System"
        },
        {
          "id": "http://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html",
          "author": null,
          "description": "Posted by Andrew M Dai and Nan Du, Research Scientists, Google Research, Brain Team  \nLarge language models (e.g., GPT-3) have many significant capabilities, such as performing few-shot learning across a wide array of tasks, including reading comprehension and question answering with very few or no training examples. While these models can perform better by simply using more parameters, training and serving these large models can be very computationally intensive. Is it possible to train and use these models more efficiently? \n  \nIn “GLaM: Efficient Scaling of Language Models with Mixture-of-Experts”, we introduce the Generalist Language Model (GLaM), a trillion weight model that can be trained and served efficiently (in terms of computation and energy use) thanks to sparsity, and achieves…",
          "link": "http://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html",
          "publishedOn": "2021-12-09T20:22:00.004Z",
          "wordCount": 2596,
          "title": "More Efficient In-Context Learning with GLaM"
        },
        {
          "id": "http://ai.googleblog.com/2021/12/general-and-scalable-parallelization.html",
          "author": null,
          "description": "Posted by Yuanzhong Xu and Yanping Huang, Software Engineers; Google Research, Brain Team  \nScaling neural networks, whether it be the amount of training data used, the model size or the computation being utilized,  has been critical for improving model quality in many real-world machine learning applications, such as computer vision, language understanding and neural machine translation. This, in turn, has motivated recent studies to scrutinize the factors that play a critical role in the success of scaling a neural model. Although increasing model capacity can be a sound approach to improve model quality, doing so presents a number of systems and software engineering challenges that must be overcome. For instance, in order to train large models that exceed the memory capacity of an accel…",
          "link": "http://ai.googleblog.com/2021/12/general-and-scalable-parallelization.html",
          "publishedOn": "2021-12-08T19:30:00.003Z",
          "wordCount": 2554,
          "title": "General and Scalable Parallelization for Neural Networks"
        },
        {
          "id": "http://ai.googleblog.com/2021/12/improving-vision-transformer-efficiency.html",
          "author": null,
          "description": "Posted by Michael Ryoo, Research Scientist, Robotics at Google and Anurag Arnab, Research Scientist, Google Research \nTransformer models consistently obtain state-of-the-art results in computer vision tasks, including object detection and video classification. In contrast to standard convolutional approaches that process images pixel-by-pixel, the Vision Transformers (ViT) treat an image as a sequence of patch tokens (i.e., a smaller part, or “patch”, of an image made up of multiple pixels). This means that at every layer, a ViT model recombines and processes patch tokens based on relations between each pair of tokens, using multi-head self-attention. In doing so, ViT models have the capability to construct a global representation of the entire image. \nAt the input-level, the tokens are fo…",
          "link": "http://ai.googleblog.com/2021/12/improving-vision-transformer-efficiency.html",
          "publishedOn": "2021-12-07T17:35:00.000Z",
          "wordCount": 2333,
          "title": "Improving Vision Transformer Efficiency and Accuracy by Learning to Tokenize"
        },
        {
          "id": "http://ai.googleblog.com/2021/12/google-at-neurips-2021.html",
          "author": null,
          "description": "Posted by Jaqui Herman and Cat Armato, Program Managers \nThis week marks the beginning of the 35th annual Conference on Neural Information Processing Systems (NeurIPS 2021), the biggest machine learning conference of the year. NeurIPS 2021 will be held virtually and includes invited talks, demonstrations and presentations of some of the latest in machine learning research. This year, NeurIPS also announced a new Datasets and Benchmarks track, which will include publications, talks, posters, and discussions related to this research area.   \n Google will have a strong presence with more than 170 accepted papers, additionally contributing to and learning from the broader academic research community via talks, posters, workshops, and tutorials. You can learn more about our work being presented…",
          "link": "http://ai.googleblog.com/2021/12/google-at-neurips-2021.html",
          "publishedOn": "2021-12-06T19:38:00.001Z",
          "wordCount": 6082,
          "title": "Google at NeurIPS 2021"
        }
      ]
    },
    {
      "title": "fast.ai",
      "feedUrl": "https://www.fast.ai/atom.xml",
      "siteUrl": "http://www.fast.ai/atom.xml",
      "articles": []
    },
    {
      "title": "Reinforcement Learning",
      "feedUrl": "https://www.reddit.com/r/reinforcementlearning/.rss?format=xml",
      "siteUrl": "https://www.reddit.com/r/reinforcementlearning/?format=xml",
      "articles": [
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/ruvrti/how_to_change_mdp_into_pomdp_getting_pixel/",
          "author": null,
          "description": "Hi, I'm looking into OpenAI robogym (https://github.com/openai/robogym)'s rearrange environments, and want to try RL-based control on it with pixel observations.\n The default observations are not pixels, and I can't seem to find any documentation online on how to change these environments into pixel observations.\n Can anyone point me to resources where similar changes have been done? Are there preexisting wrappers or code for similar mujoco environments anywhere?\n My other option is to simply render the environment with mode rgb_array, and then train the algorithm based on these rendered observations. Is this a viable option?\n    submitted by    /u/junkQn  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/ruvrti/how_to_change_mdp_into_pomdp_getting_pixel/",
          "publishedOn": "2022-01-03T07:11:19.000Z",
          "wordCount": 221,
          "title": "How to change MDP into POMDP? (getting pixel observations)"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rup48q/discrete_actions_openai_hide_seek/",
          "author": null,
          "description": "Hello :),\n Happy new year to all of you!\n I have a question regarding the action space for the Hide & Seek paper by OpenAI. The architecture of the learning model they used is below.\n https://preview.redd.it/el54yihfjd981.png?width=794&format=png&auto=webp&s=33c5228ac8003b9bb89b6d008e2cf2aea5f7e071\n They have eventually 3 action heads, as shown in the figure. As per the authors, the movement action for an agent \"sets a discretized force along their x and y axis and torque around their z-axis\". How do these 3 forces get pulled from the \"movement action head\", given that they are discretized? Does the movement action head have 3 outputs, each corresponding to an axis? So far, in my humble knowledge, I always assumed that a network with a discrete action space gives only one action per head (which could be the result of a softmax over the action space).\n Any insight?\n  \nAfter some digging, I found that the Movement Action has a \"MultiDiscrete\" type, which is a gym class. I am struggling to understand the nature of actions produced by this class, any idea?\n  \n   submitted by    /u/AhmedNizam_  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rup48q/discrete_actions_openai_hide_seek/",
          "publishedOn": "2022-01-03T01:22:41.000Z",
          "wordCount": 303,
          "title": "Discrete Actions (OpenAI Hide & Seek)"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rui9bm/player_of_games_schmid_et_al_2021_dm_generalizing/",
          "author": null,
          "description": "submitted by    /u/gwern  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rui9bm/player_of_games_schmid_et_al_2021_dm_generalizing/",
          "publishedOn": "2022-01-02T20:14:43.000Z",
          "wordCount": 164,
          "title": "\"Player of Games\", Schmid et al 2021 {DM} (generalizing AlphaZero to imperfect-information games)"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rue5bf/what_is_the_best_methodtechnology_to_deploy_deep/",
          "author": null,
          "description": "Hello, \n I am a robotics enthusiast building a robot arm to perform mundane tasks. So far, I have only been able to deploy deep reinforcement learning models on the Jetson Nano. The process has been easy using the Nvidia TensorRT SDK. What other methods/technology exist? My friend recommended using a google coral and a raspberry pi. Any recommendations would be greatly appreciated. Thank you\n    submitted by    /u/AINerd1  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rue5bf/what_is_the_best_methodtechnology_to_deploy_deep/",
          "publishedOn": "2022-01-02T17:12:52.000Z",
          "wordCount": 290,
          "title": "What is the best method/technology to deploy deep reinforcement learning models for robotics?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rudf9k/markov_decision_process_explained_through_memes/",
          "author": null,
          "description": "Today I published my first article on medium explaining Markov Decision Process with memes. Hope someone finds this helpful. \n https://medium.com/@saminbinkarim/explaining-markov-decision-process-with-memes-af679a0af343\n    submitted by    /u/sardines_again  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rudf9k/markov_decision_process_explained_through_memes/",
          "publishedOn": "2022-01-02T16:40:02.000Z",
          "wordCount": 183,
          "title": "Markov Decision Process explained through Memes"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rubyb9/proper_way_to_count_environment_steps_frames_in/",
          "author": null,
          "description": "In classical - on-policy - vtrace/Impala algorithm env_steps are incremented every training iteration\n like this : env_steps += size_of_batch * unroll_length\n However when we add replay_buffer to vtrace (like in CLEAR or LASER) we also start to learn from off-policy data therefore some trajectories (from replay) could be sampled more than once.\n My question is - does the env_step_counting stay the same even when some trajectories from batch are from replay and some directly from workers (on-policy) ?\n Or do i count steps only on those trajectories that are on-policy => not from replay\n like : env_steps += len(batch[\"on_policy_samples\"]) * unroll_length\n And if so what happens when i decide to use only samples from replay to train - how do i count env_steps then ? Using some kind of flag to indicate if specific trajectory was sampled already and leave it from counting ?\n ​\n Have been digging through the research papers for some time now and i haven`t been able to find what is the proper way to do it. Any help or advice would be much appreciated.\n    submitted by    /u/ParradoxSVK  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rubyb9/proper_way_to_count_environment_steps_frames_in/",
          "publishedOn": "2022-01-02T15:32:09.000Z",
          "wordCount": 522,
          "title": "Proper way to count environment steps / frames in distributed RL architecture for algorithms like CLEAR or LASER => basically modified impala with replay"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/ru4glz/what_should_a_beginner_do_after_learning_some/",
          "author": null,
          "description": "Hi all, it's my first time to use reddit to ask for suggestions.\n I am a beginner in RL. I learned some basic ideas in RL (such as SARSA, PPO and A2C) recently and applied Q-network to train a Go game agent. \n Personally speaking, I am going to pursue for a position as a RL engineer in the future. But I have no idea about what should I do to improve my RL skills. So I am wondering if there are some awesome resources for a beginner like me? Or could you please give me some instructions?\n    submitted by    /u/ZavierTi2021  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/ru4glz/what_should_a_beginner_do_after_learning_some/",
          "publishedOn": "2022-01-02T07:24:05.000Z",
          "wordCount": 697,
          "title": "What should a beginner do after learning some basic ideas?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rtp5ts/nethack_2021_neurips_challenge_winning_agent/",
          "author": null,
          "description": "Hi All! I am Michał from the AutoAscend team that has won the NetHack 2021 NeurIPS Challenge.\n I have just shared some episode visualization videos:\n https://www.youtube.com/playlist?list=PLJ92BrynhLbdQVcz6-bUAeTeUo5i901RQ\n The winning agent isn't based on reinforcement learning in the end, but the victory of symbolic methods in this competition shows what RL is still missing to some extent -- so I believe this subreddit is a good place to discuss it.\n We hope that NLE will someday become a new standard benchmark for evaluation next to chess, go, Atari, etc. as it presents a set of whole new complex problems for agents to learn. Contrary to Atari, NetHack levels are procedurally generated, and therefore agents can't memorize the layout. Observations are highly partial, rewards are sparse, and episodes are usually very long.\n Here are some other useful links related to the competition:\n Full NeurIPS Session recording: https://www.youtube.com/watch?v=fVkXE330Bh0\n AutoAscend team presentation starts here: https://youtu.be/fVkXE330Bh0?t=4437\n Competition report: https://nethackchallenge.com/report.html\n AICrowd Challenge link: https://www.aicrowd.com/challenges/neurips-2021-the-nethack-challenge\n    submitted by    /u/procedural_only  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rtp5ts/nethack_2021_neurips_challenge_winning_agent/",
          "publishedOn": "2022-01-01T18:27:09.000Z",
          "wordCount": 1465,
          "title": "NetHack 2021 NeurIPS Challenge -- winning agent episode visualizations"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rtifft/some_questions_on_drl/",
          "author": null,
          "description": "Hello,\n I´m applying Deep Reinforcement Learning for the first time, and I have some questions about it (I´ve already looked for an answer but in vain):\n  \nHow to normalize objectives' values in the reward function? if we have an objective that values are in the range of 10 and another objective that values are in the range of 1000.\n During the training phase, how can we watch the weights updates of a network and the gradient calculation too?\n In a multi-agent setting and episodic task, for \"dones\" vector, it will be set to \"True\" once all the agents are finished, or once an agent finishes the task done[agent_index]=True in other words, we won´t wait the latest agent to finish to set dones = [True]*number_of_agents\n  \nThank you.\n    submitted by    /u/GuavaAgreeable208  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rtifft/some_questions_on_drl/",
          "publishedOn": "2022-01-01T12:22:45.000Z",
          "wordCount": 400,
          "title": "Some questions on DRL"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rthv34/explained_variance_suddenly_crashes_and_stay/",
          "author": null,
          "description": "Hi, all Happy New Year!\n I'm working on a project to teach my quad-rotor how to hover, and these are what my rollout graphs looks like.\n The reward graph constantly increases for some time, but suddenly during a rapid increase in reward, the entropy loss, and the train std of starts to diverge, and the explained variance just crashes.\n ​\n https://preview.redd.it/yv4qbhmbc2981.png?width=826&format=png&auto=webp&s=31322a62963b1de47ed72452dfb36498ce222dd1\n ​\n https://preview.redd.it/d38papwdc2981.png?width=1373&format=png&auto=webp&s=0ab5b98d0bdad165f0dcfd7f5ea3e54ad965842c\n From my understanding, this explained variance far from 1 is a bad sign, Could you please point out, or give comments on what would be the possible reason why this is happening in my case?\n Any kind of reply would be appreciate.\n Thanks again Happy New Year. :D\n ​\n Hp: ent_coef=0.0001, learning_rate=linear decay from 0.00004, n_steps=1024, batch_size=64 apart from these, I used the default params from stable_baselines3\n I scale the action output from [-1, 1] to [min_rpm max_rpm] to actuate the quad-rotor.\n observations states = input = [pos,vel,acc,ang,ang_vel] // Network size 64,64 // algorithm used:PPO\n REWARD FUNCTION\n ​\n https://preview.redd.it/o3w2i0r7s9981.png?width=818&format=png&auto=webp&s=f03b4f12d6c57f7b7aaec7dbd03b9fecad2c8f2c\n    submitted by    /u/GOMTAE  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rthv34/explained_variance_suddenly_crashes_and_stay/",
          "publishedOn": "2022-01-01T11:44:03.000Z",
          "wordCount": 585,
          "title": "Explained Variance suddenly crashes, and stay around zero and even sometimes go to negative values"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rtc336/help_with_ppo_model_performing_poorly/",
          "author": null,
          "description": "I am attempting to recreate the PPO algorithm to try to learn the inner workings of the algorithm better and to learn more about actor-critic reinforcement learning. So far, I have a model that seems to learn, just not very well.\n In the early stages of training, the algorithm seems more sporadic and may happen to find a pretty solid policy, but due to unstable the early parts of training are, it tends to move away from this policy. Eventually, the algorithm moves the policy toward a reward of around 30. For the past few commits in my repo where I have attempted to fix this issue, the policy always tends to the around 30 reward mark, and I'm not entirely sure why it's doing this. I'm thinking maybe I implemented the algorithm incorrectly, but I'm not certain. Can someone please help me with this issue?\n Below is a link to an image of training using the latest commit, using a previous commit, and my Github project\n current commit: https://ibb.co/JQgnq1f\n previous commit: https://ibb.co/rppVHKb\n GitHub: https://github.com/gmongaras/PPO_CartPole\n ​\n Thanks for your help!\n    submitted by    /u/gmongaras  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rtc336/help_with_ppo_model_performing_poorly/",
          "publishedOn": "2022-01-01T05:07:30.000Z",
          "wordCount": 659,
          "title": "Help With PPO Model Performing Poorly"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rt226f/agent_not_learning_any_help/",
          "author": null,
          "description": "Hello\n Can someone explain why the actor critic maps the states to the same actions, in other words why the actor outputs the same action whatever the states?\n This what makes the agent learns nothing during training phase.\n Happy New Year!\n    submitted by    /u/LeatherCredit7148  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rt226f/agent_not_learning_any_help/",
          "publishedOn": "2021-12-31T20:04:50.000Z",
          "wordCount": 515,
          "title": "Agent not learning! Any Help"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rst5go/question_about_baseline_with_value_network/",
          "author": null,
          "description": "I have a question about baseline in policy gradient method. Based on what is implemented in relation to PG, there are many shared policy and value networks. But I understood baseline should not affect the parameters of the policy. When PG with shared networks and baseline updates value function, then baseline will affect policy network. Is it okay?\n    submitted by    /u/Spiritual_Fig3632  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rst5go/question_about_baseline_with_value_network/",
          "publishedOn": "2021-12-31T12:44:12.000Z",
          "wordCount": 179,
          "title": "question about baseline with value network"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rspzc6/how_to_check_feature_importance_in_reinforcement/",
          "author": null,
          "description": "I have made a custom environment in openAi Gym using data from a csv file that contains certain features in it as column names .After training a DQN agent on the environment, i wanted to check some meaningful features used by the DQN agent to make the policy but i am unable to find some good resources regarding it.\n An example of the feature importance graph\n    submitted by    /u/EBISU1234  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rspzc6/how_to_check_feature_importance_in_reinforcement/",
          "publishedOn": "2021-12-31T09:14:19.000Z",
          "wordCount": 312,
          "title": "How to check feature importance in Reinforcement learning?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rspmiy/current_unansweredinteresting_applications_in/",
          "author": null,
          "description": "Hi,\n I am planning on doing my MSc in CS with a focus in RL. More specifically, I want to learn about multi-armed bandits and how it can be used by agents to enable them to perform actions in a diverse environment. I am new to this field and I want to know more about what questions about MAB are unanswered? Any interesting application that may be currently under research? \n I would really appreciate if anyone can help me out. \n Thank you!\n    submitted by    /u/_UNIPOOL  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rspmiy/current_unansweredinteresting_applications_in/",
          "publishedOn": "2021-12-31T08:50:45.000Z",
          "wordCount": 428,
          "title": "Current unanswered/interesting applications in Multi-armed bandits?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rse3lr/ppo_with_multiple_heads/",
          "author": null,
          "description": "Hello there,\n I am working on an implementation for PPO, and I am struggling to compute the loss when two output heads for the actor network are used (one is continuous and one is discrete). The loss in PPO has 3 components: 1) clipped surrogate, 2) squared state-value loss, and 3) Entropy.\n I thought of treating the two actions separately, and compute two different losses that I add before backpropagating, but the middle term (2) is the same in both losses. How could I do that? Do I add (1) and (3) for both actions and have one loss, or do I compute two different losses then add them? Or is it hidden option 3? :p\n ​\n Any help is appreciated.\n    submitted by    /u/AhmedNizam_  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rse3lr/ppo_with_multiple_heads/",
          "publishedOn": "2021-12-30T22:29:02.000Z",
          "wordCount": 610,
          "title": "PPO with multiple heads"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rsc59y/discrete_vs_continuous_action_space/",
          "author": null,
          "description": "Hello there,\n I noticed that in many works by OpenAI and Deepmind in RL, they deal with a continuous action space which they discretize. For example rather than having to continuous actions to determine speed and direction of movement, they would discretize speed and torque.\n Is there any reason why discretized action space is more favorable?\n    submitted by    /u/AhmedNizam_  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rsc59y/discrete_vs_continuous_action_space/",
          "publishedOn": "2021-12-30T21:03:43.000Z",
          "wordCount": 422,
          "title": "Discrete vs Continuous action space"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rs9841/combined_discrete_and_action_space/",
          "author": null,
          "description": "I need an algorithm that combines both discrete and continuous actions space, like if I wanted to choose action but I needed to also perform that action at an optimized velocity or something. Can I do this with ppo and how. I am still a beginner but must of the algorithms I played with had either separately I thought of actor cri but I don’t think the value network does what I want I can’t use that to predict how much velocity to use\n    submitted by    /u/ConsiderationCivil74  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rs9841/combined_discrete_and_action_space/",
          "publishedOn": "2021-12-30T18:58:37.000Z",
          "wordCount": 296,
          "title": "Combined discrete and action space"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rs6g73/actor_net_gives_me_the_same_reward_over_time/",
          "author": null,
          "description": "Hello\n I´ll be glad if someone could help me with this.\n I am new in DRL and when I applied it I got some issues. For example, my network freezes in the same reward from the first episode I noticed also that the agent selects always the same index from the action space. Even the same reward and the same trajectory toward this reward, the critic loss is still shrinking over time (this the weirdest thing)\n I don´t know what is the reson behind that?\n https://preview.redd.it/02zlhgnqnp881.png?width=477&format=png&auto=webp&s=093c2b22d368e33fa4853ca4f3c5b459c9e2e31d\n    submitted by    /u/LeatherCredit7148  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rs6g73/actor_net_gives_me_the_same_reward_over_time/",
          "publishedOn": "2021-12-30T17:00:54.000Z",
          "wordCount": 263,
          "title": "Actor net gives me the same reward over time"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rs5yoe/ablation_teststudy/",
          "author": null,
          "description": "I'm trying to see the effect of simulated sensor noise in training a successful policy using RL for the cart pole control problem.\n I have four states: cart position, cart velocity, pole angle, pole angular velocity, and I added Gaussian noise to all of them. However, I want to find the impact of adding noise to a specific variable to find which of the states is the most sensitive to noise.\n If I were to only test the system with noise in one of the variables at a time (e.g. noise for position, but none for the other variables), would that be considered an ablation test?\n Just caught up in the terminology, because I know typical ablation tests remove a certain component instead of adding only one (or removing all but one).\n    submitted by    /u/Cogitarius  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rs5yoe/ablation_teststudy/",
          "publishedOn": "2021-12-30T16:39:32.000Z",
          "wordCount": 363,
          "title": "Ablation test/study"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rry8ge/best_suite_for_robotics_tasks/",
          "author": null,
          "description": "Recently, OpenAI removed the robotics tasks from gym, as they are no longer being maintained and Robosuite uses mujoco-py which has been deprecated. Deepmind's control suite includes some robotics tasks but they don't seem to be first-class citizens (they are in a separate module, with less options and some bugs).\n Are there any other robotics environment suites that are well-maintained and supported?\n    submitted by    /u/escapevelocitylabs  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rry8ge/best_suite_for_robotics_tasks/",
          "publishedOn": "2021-12-30T10:00:51.000Z",
          "wordCount": 203,
          "title": "Best suite for robotics tasks"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rrx27c/how_to_map_states_into_the_code/",
          "author": null,
          "description": "Hello,\n I am new in the Reinforcement Learning field. My question is: how can I easily map my states into my code?\n F.I. when the problem is about how to move an agent in a board-like environment, such as walk from point A to B by moving UP, DOWN, LEFT or RIGHT, it is quite easy to map your states. You can create a matrix like NUMBER_OF_ROWS x NUMBER_OF_COLUMNS and that is pretty much it.\n However when it comes to other type of scenarios, such as CartPole-V0 from OpenAIGym (https://gym.openai.com/envs/CartPole-v0/), I don't understand how to map the states into my code.\n This scenario has 4 states and 2 actions:\n States:\n  \nCart Position (range -2.4 --> 2.4)\n Cart Velocity (range -Inf --> Inf)\n Pole Angle (range -12deg --> 12deg)\n Pole Angular Velocity (range -Inf --> Inf)\n  \nActions:\n  \nPush cart to the left\n Push cart to the right\n  \nThe same is true for every other scenario that is not board-like, f.i. tic-tac-toe. I don't understand how can I make every possible move in the game a state inside the code.\n ​\n Is there any technique to approach each type of problem or do I just need to figure it out by myself? \n Have a nice day,\n Gabriel.\n    submitted by    /u/gabrieloxi  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rrx27c/how_to_map_states_into_the_code/",
          "publishedOn": "2021-12-30T08:49:42.000Z",
          "wordCount": 547,
          "title": "How to map states into the code?"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rruluq/general_question_in_regards_to_understanding_the/",
          "author": null,
          "description": "Hi there, I was a graduate student previously working on a few RL-related problems a year ago. And recently I've got much interest in Policy Optimization problems, and started with reading some foundation papers like DQN, DPG.\n While reading the paper of DPG, it was very straightforward to understand the main idea of the paper, however, it was very difficult to understand the proofs of the theorems shown in the appendix(http://proceedings.mlr.press/v32/silver14-supp.pdf). I barely understood the proof of Theorem1, but proof of Theorem2 is almost impossible to understand due to the lack of my mathematics background. \n The question is that, could you tell me what I should study or learn before understanding this kind of proof? What kind of subject or material should I look for in order to understand the flow of the proofs and related definition/notation used in the proof of 'DPG'?\n I personally think that it is crucial to understand the proofs of the main Theorem because it will help me to understand the authors' intuition and motivation which can be the foundation of the paper.\n    submitted by    /u/g6ssgs  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rruluq/general_question_in_regards_to_understanding_the/",
          "publishedOn": "2021-12-30T06:21:32.000Z",
          "wordCount": 347,
          "title": "General question in regards to understanding the proofs of Deterministic Policy Gradient"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rr8thh/favorite_papers_from_2021/",
          "author": null,
          "description": "What have been your favorite reads of 2021 in terms of RL papers? I will start!\n Reward is enough (Reddit Discussion) - Four great names from RL (silver, Singh, Precup and Sutton) give their reasonings as to why using RL can create super intelligence. You might not agree with it, but it's interesting to see the standpoint of Deepmind and where they want to take RL.\n Deep Reinforcement Learning at the Edge of the Statistical Precipice (Reddit Discussion) - This is a major step towards a better model comparison in RL. Too many papers in the past have used a selection technique akin to 'average top 30 runs in a total of 100'. I have also never even heard of Munchausen RL before this paper, and was pleasantly surprised by reading it.\n Mastering Atari with Discrete World Models - Very good read and a nice path from Ha's World Models to Dream to Control to DreamerV2. This is one of the methods this year that actually seems to improve performance quite a bit without needing a large scale distributed approach.\n On the Expressivity of Markov Reward (Reddit Discussion) - The last sentence in the blog post captures it for me: \"We hope this work provides new conceptual perspectives on reward and its place in reinforcement learning\", it did.\n Open-Ended Learning Leads to Generally Capable Agents (Reddit Discussion) - Great to see the environment integrated into the learning process, seems like something we will see much more of in the future. Unfortunately, as DeepMind does, the environment is not released nor is the code. I remember positions at OpenAI for open-ended learning, perhaps we might see something next year to compete with this.\n ​\n Most of my picks are not practical algorithms. For me, it seems like PPO is still king when looking at performance and simplicity, kind of a disappointment. I probably missed some papers too. What was your favorite paper in RL 2021? Was it Player of Games (why?), something with Offline RL or perhaps Multi Agents?\n    submitted by    /u/YouAgainShmidhoobuh  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rr8thh/favorite_papers_from_2021/",
          "publishedOn": "2021-12-29T14:00:33.000Z",
          "wordCount": 587,
          "title": "Favorite papers from 2021"
        },
        {
          "id": "https://www.reddit.com/r/reinforcementlearning/comments/rr7yk6/what_happened_to_openai_rl/",
          "author": null,
          "description": "OpenAI used to do a lot of RL research, but it seems like last year and this year the only real RL related work was on benchmark competitions. They even gave away the control of OpenAI Gym. They still have great RL researchers working there, but nothing major has come out.\n Is it all due to a pivot towards large scale language models that are at least profitable? Is Sam Altman just not interested in RL?\n    submitted by    /u/YouAgainShmidhoobuh  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/reinforcementlearning/comments/rr7yk6/what_happened_to_openai_rl/",
          "publishedOn": "2021-12-29T13:15:56.000Z",
          "wordCount": 632,
          "title": "What Happened to OpenAI + RL?"
        }
      ]
    },
    {
      "title": "RL Weekly",
      "feedUrl": "https://www.getrevue.co/profile/seungjaeryanlee?format=rss",
      "siteUrl": "https://www.getrevue.co/profile/seungjaeryanlee",
      "articles": []
    },
    {
      "title": "Damian Bogunowicz - dtransposed",
      "feedUrl": "https://dtransposed.github.io/feed.xml",
      "siteUrl": "http://dtransposed.github.io/",
      "articles": [
        {
          "id": "http://dtransposed.github.io/blog/2021/12/24/DeFi-Adventure-2/",
          "author": null,
          "description": "Christmas break is a great time to catch up with the backlog of interesting things to learn and read about. I used some of this time to finish the amazing DeFi quest by Cristian Strat. This is a continuation to the first part of this series, where I document my first steps in the world of Decentralized Finance. If you have not read the first write-up, please do. Otherwise, this text will be very confusing and not too useful for you. But once you go through both blog posts, I guarantee that you will have a DeFi knowledge superior to the majority of the folks out there.\nAdventure IV\nThis adventure will be more like a short pause from more in-depth topics. I will quickly present some of the best DeFi dashboards out there. A DeFi dashboard is a central place, where you can view your assets, tr…",
          "link": "http://dtransposed.github.io/blog/2021/12/24/DeFi-Adventure-2/",
          "publishedOn": "2021-12-24T00:00:00.000Z",
          "wordCount": 3123,
          "title": "Learning by Doing - the DeFi Quest (Part 2 out of 2)"
        },
        {
          "id": "http://dtransposed.github.io/blog/2021/12/11/DeFi-adventure-1/",
          "author": null,
          "description": "I find Decentralised Finance (DeFi) truly fascinating because it promises to solve so many problems of the modern financial world. It gives hope, that the future will be much less dystopian than we think. Recent years (decades) have shown that we have a problem with harnessing the complexity of the ever-expanding, ever-global financial system. We have efficiency problems related to:\nCapital markets (Citadel, Robin Hood scandal, naked-shorts, toxic debts, and other types of harmful financial engineering)\nGlobal financial system (most of the world living on borrowed money, with the possible intention of devaluating the debt over time and thus the risk of (hyper)inflation)\nThe fragility of the paper tiger (every hiccup in the financial system spreads like a butterfly effect throughout the wor…",
          "link": "http://dtransposed.github.io/blog/2021/12/11/DeFi-adventure-1/",
          "publishedOn": "2021-12-11T00:00:00.000Z",
          "wordCount": 3344,
          "title": "Learning by Doing - the DeFi Quest (Part 1 out of 2)"
        }
      ]
    },
    {
      "title": "Featured Blog Posts - Data Science Central",
      "feedUrl": "http://feeds.feedburner.com/FeaturedBlogPosts-DataScienceCentral?format=xml",
      "siteUrl": null,
      "articles": [
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085499",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9949632861?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9949632861?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p></p>\r\n<p>Okay, so I am still confused by the concept of a Data Mesh (I’m a slow learner).</p>\r\n<p>Recently, I wrote a blog that posed the question: <a href=\"https://www.datasciencecentral.com/profiles/blogs/are-data-meshes-the-enabler-of-the-marginal-propensity-to-reuse\">Are Data Meshes the Enabler of the Marginal Propensity to Reuse</a>? The ability to…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085499",
          "publishedOn": "2021-12-27T16:30:00.000Z",
          "wordCount": 1872,
          "title": "Are Data Meshes Really Data Marts with Conformed Dimensions?"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085380",
          "author": null,
          "description": "<p><span style=\"font-weight: 400;\"><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9949663901?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9949663901?profile=RESIZE_710x\" width=\"720\"></img></a></span></p>\r\n<p><span style=\"font-weight: 400;\">Web scraping deals with HTML almost exclusively. In nearly all cases, what is required is a small sample from a very large file (e.g. pricing information from an ecommerce page). Therefore, an essential part of scraping is searching through an HTML document and finding the correct…</span></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085380",
          "publishedOn": "2021-12-27T08:30:00.000Z",
          "wordCount": 1710,
          "title": "Optimal Scraping Technique: CSS Selector, XPath, & RegEx"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085290",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9948211856?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9948211856?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p><strong>Introduction</strong></p>\r\n<p>With the explosion of digital media and the resultant avalanche of constantly increasing user data being created and collected daily, businesses, institutions, and organizations need better ways to manage that data beyond the standard suite of tools. The description of a Data Management Platform or Augmented Data…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085290",
          "publishedOn": "2021-12-27T06:00:00.000Z",
          "wordCount": 1066,
          "title": "What is a Data Management Platform?"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085364",
          "author": null,
          "description": "<h2><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9947285672?profile=original\" rel=\"noopener\" target=\"_blank\"></a></h2>\r\n<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9949734895?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9949734895?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>Late in the year, we are suddenly hearing a new term ‘<strong>web 3’</strong></p>\r\n<p>For the reasons I describe below, this trend will be significant</p>\r\n<p>In this article, I take the perspective of a neutral…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085364",
          "publishedOn": "2021-12-26T19:00:00.000Z",
          "wordCount": 1305,
          "title": "The web 3 meme"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1078655",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9942574898?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9942574898?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<ul>\r\n<li>Thousands of people are injured by turbulence every year.</li>\r\n<li>New machine learning model gives high-accuracy, 10 second warning for turbulence.</li>\r\n<li>The model may lessen in-flight injuries and save lives.</li>\r\n</ul>\r\n<p>Turbulence is one of the leading cause of injuries on passenger planes and—if you don’t have your seat belt on—those…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1078655",
          "publishedOn": "2021-12-24T15:30:00.000Z",
          "wordCount": 1383,
          "title": "Machine Learning can give a 10 second Turbulence Warning"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085080",
          "author": null,
          "description": "<p><span style=\"font-weight: 400;\"><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9939242664?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9939242664?profile=RESIZE_710x\" width=\"720\"></img></a></span></p>\r\n<p><span style=\"font-weight: 400;\">Let’s imagine that you have signed a contract with a mobile app development firm. It seems that you have agreed upon everything: terms, budget, the scope of work, and other things. But six months later, it turns out that the developers are not up to their work and the product is…</span></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085080",
          "publishedOn": "2021-12-23T12:00:00.000Z",
          "wordCount": 1232,
          "title": "How to Find a Mobile App Development Firm: Tips for Businesses"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085306",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9949742696?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9949742696?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<h2><span id=\"What_is_a_Scrum_Board\"><b>What is a Scrum Board?</b></span></h2>\r\n<p>As Scrum is one of the popular frameworks to break down complex problems into smaller tasks, Scrum board is a project management software used to visually represent these tasks and Scrum sprints. The scrum board is the center of every sprint meeting to get regular updates…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085306",
          "publishedOn": "2021-12-23T12:00:00.000Z",
          "wordCount": 1354,
          "title": "What is a Scrum Board? What is the Difference Between a Scrum & Kanban Board?"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085158",
          "author": null,
          "description": "<p><span style=\"font-weight: 400;\"><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9950166067?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9950166067?profile=RESIZE_710x\" width=\"720\"></img></a></span></p>\r\n<p><span style=\"font-weight: 400;\">The COVID-19 pandemic has thrown organizations into disarray—the increased usage of conferencing and collaboration services by employees working from home strains back-end support services. </span></p>\r\n<p><span style=\"font-weight: 400;\">And growing traffic on networks connecting…</span></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085158",
          "publishedOn": "2021-12-23T10:00:00.000Z",
          "wordCount": 1934,
          "title": "Cloud, Cost and Containers three C's in Cloud-Computing in post-COVID"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084882",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9949749054?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9949749054?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p><span>Scraping Google SERPs (search engine result pages) is as straightforward or as complicated as the tools we use. For this tutorial, we’ll be using Scrapy, a web scraping framework designed for Python. Python and Scrapy combine to create a powerful duo that we can use to scrape almost any website.</span></p>\r\n<p><span>Scrapy has many useful…</span></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084882",
          "publishedOn": "2021-12-22T16:00:00.000Z",
          "wordCount": 3001,
          "title": "Scraping Data from Google Search Using Python and Scrapy"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085049",
          "author": null,
          "description": "<p></p>\r\n<p><strong><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9936688655?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9936688655?profile=RESIZE_710x\" width=\"720\"></img></a></strong></p>\r\n<p><strong>3D Secure Authentication Industry:</strong></p>\r\n<ul>\r\n<li>3D secure authentication is a fraud-prevention security system for credit and debit card transactions processed online. During card payments, 3D secure authentication adds an additional layer of security. It provides clients with a safe authentication step before…</li>\r\n</ul>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085049",
          "publishedOn": "2021-12-22T16:00:00.000Z",
          "wordCount": 1276,
          "title": "3D Secure Authentication: A New Era For Payment Authentication And Customer Experience"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085116",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9949757081?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9949757081?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>The exchange of data between corporations is known as data monetization. It is the process of earning income or creating new revenue streams by utilizing data, which is estimated to support expansion of the global data monetization market. Direct data monetization as well as indirect data monetization is the two forms of data monetization. The sale of…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085116",
          "publishedOn": "2021-12-22T11:30:00.000Z",
          "wordCount": 1341,
          "title": "Using Cloud and AI Technologies to Make Data Driven Decisions For Monetization"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085036",
          "author": null,
          "description": "<p><span>The discipline of data annotation and labeling is growing in popularity and significance throughout the world. The global market for data annotation tools is expected to reach </span><strong class=\"js iv\">$2.57 billion by 2027</strong><span>, according to a published report.</span></p>\r\n<p class=\"jq jr iu js b jt ju iy jv jw jx jb jy jz ka kb kc kd ke kf kg kh ki kj kk kl em jp\" id=\"412f\">For robots, drones, and vehicles to gain increasing levels of au<span id=\"rmm\">t</span>onomy,…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1085036",
          "publishedOn": "2021-12-22T10:30:00.000Z",
          "wordCount": 1106,
          "title": "Outsourcing Data Annotation Work"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084865",
          "author": null,
          "description": "<p><span style=\"font-weight: 400;\"><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9949819074?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9949819074?profile=RESIZE_710x\" width=\"720\"></img></a></span></p>\r\n<p><span style=\"font-weight: 400;\">Java is the</span> <a href=\"https://www.oracle.com/ar/a/ocom/docs/java-strength-in-numbers.pdf\"><span style=\"font-weight: 400;\">#1 programming language for Big Data</span></a><span style=\"font-weight: 400;\">, Analytics, DevOps, and AI. It is consistently the first choice for…</span></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084865",
          "publishedOn": "2021-12-22T10:00:00.000Z",
          "wordCount": 1638,
          "title": "Top 7 Reasons Data Scientists Should Know Java Programming Language"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084850",
          "author": null,
          "description": "<div class=\"dsc_primaryImage\"><span style=\"font-family: arial, helvetica, sans-serif;\"><a href=\"https://www.datasciencecentral.com/profiles/blog/list?tag=dsc_newsletter\" rel=\"noopener\" target=\"_blank\"><img alt=\"Winter Solstice Shenanigans\" src=\"https://multimedia.getresponse360.com/datascience-B/photos/0fdfb61a-5192-4b3b-bf19-15bdb6d3da2b.jpg\" style=\"vertical-align: baseline;\" title=\"Winter Solstice Shenanigans\" width=\"720\"></img></a></span></div>\r\n<table style=\"width: 726px;\">\r\n<tbody><tr><td><p dir=\"ltr\">Today, the 21st of December, is the Winter Solstice, unless you happen to be in sub-Saharan Africa, Australia, Argentina, or Antarctica, or other points south of the equator. In that case, today is the first day…</p>\r\n</td>\r\n</tr>\r\n</tbody>\r\n</table>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084850",
          "publishedOn": "2021-12-22T00:30:00.000Z",
          "wordCount": 2524,
          "title": "DSC Weekly Digest 21 December 2021: Winter Solstice Shenanigans"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084676",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9933327478?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9933327478?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>We live in a world where the lines between private and public data are blurred. With access to almost every aspect of data, businesses slowly dive into the lives of consumers and surprise them by showing them the products they've been looking for at random, anywhere. Data discomfort is slowly subsiding, people are still cautious and for the right…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084676",
          "publishedOn": "2021-12-21T11:00:00.000Z",
          "wordCount": 1236,
          "title": "Steps in securing your enterprise mobile application"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084757",
          "author": null,
          "description": "<p><span style=\"font-weight: 400;\"><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9933233072?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9933233072?profile=RESIZE_710x\" width=\"720\"></img></a></span></p>\r\n<p><span style=\"font-weight: 400;\">We have noticed how the industrial revolution transformed the shape of our modernized world. We got big industries, endowed the computer, and made significant progress in the IT sector.</span></p>\r\n<p><span style=\"font-weight: 400;\">But, this change had its consequences too. With an…</span></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084757",
          "publishedOn": "2021-12-21T11:00:00.000Z",
          "wordCount": 2088,
          "title": "Ways To Make Cloud Data More Environmentally Friendly"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084740",
          "author": null,
          "description": "<p><span style=\"font-weight: 400;\"><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9933241254?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9933241254?profile=RESIZE_710x\" width=\"720\"></img></a></span></p>\r\n<p><span style=\"font-weight: 400;\">One of the most exciting sectors of the IT business is web development. There is a vast library of tools and libraries available, many of which strive to improve an application's functionality and efficiency. However, taking advantage of every fresh opportunity isn't a good idea…</span></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084740",
          "publishedOn": "2021-12-21T05:30:00.000Z",
          "wordCount": 1576,
          "title": "Building a Scalable Application with React and Redux: A Step-by-Step Guide"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084804",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9930070895?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9930070895?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>Chief Data Officers (CDO) and Chief Data Analytics Officers (CDAO) are under intense pressure to find ways to “monetize” their growing volumes of data. While some organizations seek “monetization” by trying to sell their data, as I discussed in the “…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084804",
          "publishedOn": "2021-12-20T23:00:00.000Z",
          "wordCount": 1818,
          "title": "The Economics of Data Products"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084215",
          "author": null,
          "description": "<div class=\"wpb_text_column wpb_content_element vc_custom_1636464703663 tm-animation move-up animate\"><div class=\"wpb_wrapper\"><p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9917146659?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9917146659?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>Back in 2014, Gartner placed the field of prescriptive analytics at the beginning of the ‘Peak of Inflated Expectations’ in their Hype Cycle of Emerging Technologies. And also went on to project the prescriptive analytics…</p>\r\n</div>\r\n</div>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084215",
          "publishedOn": "2021-12-20T22:54:43.000Z",
          "wordCount": 1470,
          "title": "Prescriptive Analytics – The Final Frontier"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1083447",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9929648266?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9929648266?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>I was hanging around the LinkedIn water cooler the other day, just minding my own business, when I came across this very interesting graphic (Figure 1) from <a href=\"https://www.linkedin.com/in/brentdykes/\">Brent Dykes</a> titled “…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1083447",
          "publishedOn": "2021-12-20T19:08:22.000Z",
          "wordCount": 1765,
          "title": "Using Design Thinking to Win the First and the Last Mile"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084495",
          "author": null,
          "description": "<div class=\"post-header text-fields\"><div class=\"text post-header__introduction\"><div class=\"post-header text-fields\"><div class=\"text post-header__introduction\"><p style=\"text-align: center;\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9930051081?profile=RESIZE_710x\" width=\"720\"></img></p>\r\n</div>\r\n</div>\r\n</div>\r\n</div>\r\n<div class=\"text-fields u-padding\"><div class=\"text-fields__container\"><div class=\"text text-fields__text\"><div class=\"text-fields u-padding\"><div class=\"text-fields__container\"><div class=\"text text-fields__text\"><p>In today’s data-driven reality, data…</p>\r\n</div>\r\n</div>\r\n</div>\r\n</div>\r\n</div>\r\n</div>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084495",
          "publishedOn": "2021-12-20T10:30:00.000Z",
          "wordCount": 1946,
          "title": "Most data warehouse projects fail. Here’s how not to."
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084560",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9926540062?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9926540062?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>Artificial Intelligence has a long history of oscillating between two somewhat contradictory poles. On one side, exemplified by Noam Chomsky, Marvin Minsky, Seymour Papert, and many others, is the idea that cognitive intelligence was algorithmic in nature - that there were a set of fundamental precepts that formed the foundation of language, and by…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084560",
          "publishedOn": "2021-12-20T02:00:00.000Z",
          "wordCount": 2136,
          "title": "Where Semantics and Machine Learning Converge"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084609",
          "author": null,
          "description": "<p><br></br> <a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9926326071?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9926326071?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>Its that time of the year and while I do not like predictions, I think there is one which I want to talk about</p>\r\n<p>It’s a bit specific so needs some context</p>\r\n<p>From an AI standpoint, 2021 has been the year of large language models like  <a href=\"https://en.wikipedia.org/wiki/GPT-3\">GPT-3</a></p>\r\n<p>And that has led to near-magical…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084609",
          "publishedOn": "2021-12-19T23:30:00.000Z",
          "wordCount": 964,
          "title": "Large Language Models Are Leading low-code AI Applications in the Cloud"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084383",
          "author": null,
          "description": "<p><span style=\"font-size: 12pt;\"><span><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9924034458?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9924034458?profile=RESIZE_710x\" width=\"720\"></img></a></span></span></p>\r\n<p><span style=\"font-size: 12pt;\">A labyrinth is a structure in the inner ear.  The use of the term labyrinthology might sometimes relate to this part of the human anatomy.  Alternatively, some people think of a labyrinth as a maze or a body of routes and passages.  Years ago, I purchased some land…</span></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084383",
          "publishedOn": "2021-12-19T03:30:00.000Z",
          "wordCount": 1364,
          "title": "Using Labyrinthology to Study the Progression of Experiences"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1078656",
          "author": null,
          "description": "<p> <a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9922845287?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9922845287?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<ul>\r\n<li>A new study evaluates ML models that classify fake news from fact.</li>\r\n<li>The best models can only achieve up to 77.2% accuracy.</li>\r\n<li>AI will probably never be able to fully replace the nuanced analysis of human journalists.</li>\r\n</ul>\r\n<p><strong>Many Natural Language Processing (NLP) techniques exist for detecting “fake news”.…</strong></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1078656",
          "publishedOn": "2021-12-18T15:00:00.000Z",
          "wordCount": 1396,
          "title": "Machine Learning and the Challenge of Predicting Fake News"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084370",
          "author": null,
          "description": "<p><span style=\"font-weight: 300;\"><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9926815486?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9926815486?profile=RESIZE_710x\" width=\"720\"></img></a> The benefits of 5G – delivering super-fast, low-latency broadband to consumers, wirelessly, wherever they are – promises to revolutionise the way we interact with one another and businesses. For the first time consumers are going to have immediate, portable access to the kind of speeds and connectivity that was previously only…</span></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084370",
          "publishedOn": "2021-12-18T04:00:00.000Z",
          "wordCount": 1415,
          "title": "Will 5G Bring the Death of Brick and Mortar Banking?"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084363",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9921348679?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9921348679?profile=RESIZE_710x\" width=\"720\"></img></a> <span style=\"font-size: 8pt;\">Manufacturing Line, <em>Wikimedia Commons</em>, 2014</span><br></br> <br></br> <span style=\"font-weight: 400;\">Fakery certainly isn’t limited to news and social media. The Organisation for Economic Co-operation and Development (OECD) estimated in 2016 that 6.8 percent of physical goods imported by the EU were fakes. …</span></p>\r\n<p></p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084363",
          "publishedOn": "2021-12-18T00:30:00.000Z",
          "wordCount": 2040,
          "title": "How data decentralization is already benefiting enterprises–and society"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084503",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9926827266?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9926827266?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>Almost all Software-as-a-Service (SaaS) businesses start with a \"visionary idea\" - a concept that can revolutionize the industry. Yet, the world has witnessed numerous passionate founders invest their energy, time, and money in a product that solves a problem that nobody cares about. It's the responsibility of a leader to determine if the product is…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084503",
          "publishedOn": "2021-12-17T07:00:00.000Z",
          "wordCount": 2584,
          "title": "How to Build a Minimum Viable Product (MVP): A Detailed Guide"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084194",
          "author": null,
          "description": "<p><b><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9919075296?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9919075296?profile=RESIZE_710x\" width=\"720\"></img></a></b></p>\r\n<p><b>Technology</b> is emerging as well as changing the future of the <b>education</b> <b>system</b> by turning the traditional methods of gaining knowledge into a comprehensive way of learning with the help of augmented reality and simulation tools. In the future, this will actively help the education as well as educators to manage time for…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084194",
          "publishedOn": "2021-12-17T04:30:00.000Z",
          "wordCount": 1367,
          "title": "Ways Technology is offering Innovative Solutions to the Education System"
        },
        {
          "id": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084052",
          "author": null,
          "description": "<p><a href=\"https://storage.ning.com/topology/rest/1.0/file/get/9916643293?profile=original\" rel=\"noopener\" target=\"_blank\"><img class=\"align-full\" src=\"https://storage.ning.com/topology/rest/1.0/file/get/9916643293?profile=RESIZE_710x\" width=\"720\"></img></a></p>\r\n<p>It is no secret that technology has evolved at a rapid pace and that this fast-paced evolution has brought about intense changes across the world. Today’s manufacturing companies need advanced services to maximize efficiency and productivity. Thanks to technology and innovation, today’s manufacturing industry today has undergone drastic changes. From…</p>",
          "link": "http://www.datasciencecentral.com/xn/detail/6448529:BlogPost:1084052",
          "publishedOn": "2021-12-16T13:00:00.000Z",
          "wordCount": 1199,
          "title": "Top Reasons Why Manufacturing Companies Need DevOps"
        }
      ]
    },
    {
      "title": "John D. Cook",
      "feedUrl": "https://www.johndcook.com/blog/feed",
      "siteUrl": "https://www.johndcook.com/blog",
      "articles": [
        {
          "id": "https://www.johndcook.com/blog/?p=93265",
          "author": "John",
          "description": "I recently received a review copy of Andrew Witt’s new book Formulations: Architecture, Mathematics, and Culture. The Hankel function on the cover is the first clue that this book contains some advanced mathematics. Or rather, it references some advanced mathematics. I’ve only skimmed the book so far, but I didn’t see any equations. Hankel functions […]\nArchitecture and Math first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2022/01/01/architecture-and-math/",
          "publishedOn": "2022-01-01T19:32:30.000Z",
          "wordCount": 380,
          "title": "Architecture and Math"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=93255",
          "author": "John",
          "description": "I’ve written a couple posts about the Golay problem recently, first here then here. The problem is to find all values of N and n such that is a power of 2, say 2p. Most solutions apparently fall into three categories: n = 0 or n = N, N is odd and n = (N-1)/2, […]\nTurning the Golay problem sideways first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2022/01/01/turning-the-golay-problem-sideways/",
          "publishedOn": "2022-01-01T16:46:19.000Z",
          "wordCount": 1030,
          "title": "Turning the Golay problem sideways"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=93159",
          "author": "John",
          "description": "I played around with what I’m calling the Golay problem over Christmas and wrote a blog post about it. I rewrote the post as I learned more about the problem due to experimentation and helpful feedback via comments and Twitter. In short, the Golay problem is to classify the values of N and n such […]\nUpdate on the Golay problem first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/31/update-on-the-golay-problem/",
          "publishedOn": "2021-12-31T14:53:16.000Z",
          "wordCount": 504,
          "title": "Update on the Golay problem"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=93062",
          "author": "John",
          "description": "Stanislaw Ulam once said Using a term like nonlinear science is like referring to the bulk of zoology as the study of non-elephant animals. There is only one way to be linear, but there are many ways to not be linear. A similar observation applies to non-classical logic. There are many ways to not be […]\nNames and numbers for modal logic axioms first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/30/modal-axioms/",
          "publishedOn": "2021-12-30T19:49:32.000Z",
          "wordCount": 480,
          "title": "Names and numbers for modal logic axioms"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=93026",
          "author": "John",
          "description": "The previous post looked at two of the five Lagrange points of the Sun-Earth system. These points, L1 and L2, are located on either side of Earth along a line between the Earth and the Sun. The third Lagrange point, L3, is located along that same line, but on the opposite side of the Sun. […]\nWhen do two-body systems have stable Lagrange points? first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/30/stable-lagrange-points/",
          "publishedOn": "2021-12-30T11:30:05.000Z",
          "wordCount": 763,
          "title": "When do two-body systems have stable Lagrange points?"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=92910",
          "author": "John",
          "description": "The James Webb Space Telescope (JWST) is on its way to the Lagrange point L2 of the Sun-Earth system. Objects in this location will orbit the Sun at a fixed distance from Earth. There are five Lagrange points, L1 through L5. The points L1, L2, and L3 are unstable, and points L4 and L5 are […]\nFinding Lagrange points L1 and L2 first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/28/lagrange-points-l1-and-l2/",
          "publishedOn": "2021-12-28T21:34:34.000Z",
          "wordCount": 577,
          "title": "Finding Lagrange points L1 and L2"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=92536",
          "author": "John",
          "description": "These were the most popular posts from 2021 on this site, listed in chronological order. Simple gamma approximation Floor, ceiling, bracket Evolution of random number generators Format text in twitter Where has all the productivity gone? Computing zeta(3) Morse code palindromes Much less than, much greater than Monads and macros Partial functions\nMost popular posts of 2021 first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/28/most-popular-posts-of-2021/",
          "publishedOn": "2021-12-28T13:00:33.000Z",
          "wordCount": 210,
          "title": "Most popular posts of 2021"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=92751",
          "author": "John",
          "description": "Sine and cosine have power series with simple coefficients, but tangent does not. Students are shocked when they see the power series for tangent because there is no simple expression for the power series coefficients unless you introduce Bernoulli numbers, and there’s no simple expression for Bernoulli numbers. The perception is that sine and cosine […]\nThe exception case is normal first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/27/the-exception-case-is-normal/",
          "publishedOn": "2021-12-27T19:41:17.000Z",
          "wordCount": 385,
          "title": "The exception case is normal"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=92740",
          "author": "John",
          "description": "Marcel Golay noticed that and realized that this suggested it might be possible to create a perfect code of length 23. I wrote a Twitter thread on this that explains how this relates to coding theory (and to sporadic simple groups). This made me wonder how common it is for cumulative sums of binomial coefficients […]\nBinomial sums and powers of 2 first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/27/golay/",
          "publishedOn": "2021-12-27T15:25:11.000Z",
          "wordCount": 487,
          "title": "Binomial sums and powers of 2"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=92486",
          "author": "John",
          "description": "The Christmas hymn “O Come, O Come, Emmanuel” is a summary of the seven “O Antiphons,” sung on December 17 though 23, dating back to the 8th century [1]. The seven antiphons are O Sapientia O Adonai O Radix Jesse O Clavis David O Oriens O Rex Gentium O Emmanuel The corresponding verses of “O […]\nO Come, O Come, Emmanuel: condensing seven hymns into one first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/24/o-come-o-come-emmanuel/",
          "publishedOn": "2021-12-24T16:09:46.000Z",
          "wordCount": 300,
          "title": "O Come, O Come, Emmanuel: condensing seven hymns into one"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=92157",
          "author": "John",
          "description": "Yesterday I wrote about how to multiply octets of real numbers, the octonions. Today I’ll show how to create an error correcting code from the octonions. In fact, we’ll create a perfect code in the sense explained below. We’re going to make a code out of octonions over a binary field. That is, we’re going […]\nError correcting code from octonions first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/20/octonion-ecc/",
          "publishedOn": "2021-12-20T15:41:01.000Z",
          "wordCount": 953,
          "title": "Error correcting code from octonions"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=92096",
          "author": "John",
          "description": "Yesterday I wrote about the fact that quaternions, unlike complex numbers, can form conjugates via a series of multiplications and additions. This post will show that you can do something similar with octonions. If x is an octonion x = r0 e0 + r1 e1 + … + r7 e7 where all the r‘s are […]\nConjugate theorem for octonions first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/19/conjugate-octonions/",
          "publishedOn": "2021-12-19T23:33:28.000Z",
          "wordCount": 362,
          "title": "Conjugate theorem for octonions"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=92088",
          "author": "John",
          "description": "This post will present a way of multiplying octonions that’s easy to remember. Please note that there are varying conventions for how to define multiplication for octonions [1]. Octonions The complex numbers have one imaginary unit i, and the quaternions have three: i, j, and k. The octonions have seven, and so it makes sense […]\nHow to multiply octonions first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/19/multiply-octonions/",
          "publishedOn": "2021-12-19T23:27:37.000Z",
          "wordCount": 799,
          "title": "How to multiply octonions"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=92005",
          "author": "John",
          "description": "The conjugate of a complex number is the complex number Taking the conjugate flips over a complex number, taking its reflection in the real axis. Multiplication stretches and rotates complex numbers, and addition translates complex numbers. You can’t flip the complex plane over by any series of dilatations, rotations, and translations. The situation is different […]\nComplex Conjugates versus Quaternion Conjugates first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/18/quaternion-conjugate/",
          "publishedOn": "2021-12-18T18:54:45.000Z",
          "wordCount": 341,
          "title": "Complex Conjugates versus Quaternion Conjugates"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=91983",
          "author": "John",
          "description": "“… Things fall apart; the centre cannot hold …” — Yeats, The Second Coming   Center of a group The center of a group is the set of elements that commute with everything else in the group. For example, matrix multiplication is not commutative in general. You can’t count on AB being equal to BA, […]\nThe center may not hold first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/18/the-center-may-not-hold/",
          "publishedOn": "2021-12-18T15:22:11.000Z",
          "wordCount": 803,
          "title": "The center may not hold"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=91901",
          "author": "John",
          "description": "A positive integer n is said to be congruent if there exists a right triangle with area n such that the length of all three sides is rational. You could always choose one leg to have length n and the other to have length 2. Such a triangle would have area n and two rational […]\nThe congruent number problem first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/17/congruent-number/",
          "publishedOn": "2021-12-17T16:34:43.000Z",
          "wordCount": 891,
          "title": "The congruent number problem"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=91637",
          "author": "John",
          "description": "We think they like randomness in design, but we don’t exactly. People like things that are sorta random, but not too random. When you literally scatter things randomly, they looked too clumped [1]. There are many ways around this problem, variations on randomness that people find more aesthetically pleasing. One of these ways is random […]\nAesthetic uses of Latin squares first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/14/aesthetic-latin-squares/",
          "publishedOn": "2021-12-14T15:41:40.000Z",
          "wordCount": 802,
          "title": "Aesthetic uses of Latin squares"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=91577",
          "author": "John",
          "description": "There’s a saying in the arts “Know the rules before you break the rules.” Master the classical conventions of your field before you violate them. Break the rules deliberately and not accidentally, skillfully and not due to a lack of skill. There’s a world of difference between a beginning musician playing a wrong note and […]\nKnow the rules to break the rules first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/13/break-the-rules/",
          "publishedOn": "2021-12-13T22:25:26.000Z",
          "wordCount": 698,
          "title": "Know the rules to break the rules"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=91474",
          "author": "John",
          "description": "A Latin square is an n × n grid with a number from 1 to n in each cell, such that no number appears twice in a row or twice in a column. It’s not obvious that Latin squares exist for all n, but they do, and in fact there are a lot of them. […]\nHow many Latin squares are there? first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/13/latin-squares/",
          "publishedOn": "2021-12-13T14:44:19.000Z",
          "wordCount": 715,
          "title": "How many Latin squares are there?"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=91283",
          "author": "John",
          "description": "I recently wrote about the Yule-Simon distribution. The same Yule, George Udny Yule, is also known for the statistics Yule’s Y and Yule’s Q. The former is also known as the coefficient of colligation, and the latter is also known as the Yule coefficient of association. Both measure how things are related. Given a 2 […]\nYule statistics Y and Q first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/12/yule-statistics-y-and-q/",
          "publishedOn": "2021-12-12T18:36:05.000Z",
          "wordCount": 419,
          "title": "Yule statistics Y and Q"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=91392",
          "author": "John",
          "description": "Imagine you could list the contents of a directory from a command line, and then edit the text output to make things happen. That’s sorta how Emacs dired works. It’s kind of a cross between a bash shell and the Windows File Explorer. Why would you ever want to use such a bizarre hybrid? One […]\nDeleting reproducible files in Emacs dired first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/12/deleting-files-in-dired/",
          "publishedOn": "2021-12-12T18:17:18.000Z",
          "wordCount": 645,
          "title": "Deleting reproducible files in Emacs dired"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=91360",
          "author": "John",
          "description": "A few years ago the scientific community suddenly realized that a lot of scientific papers were wrong. I imagine a lot of people knew this all along, but suddenly it became a topic of discussion and people realized the problem was bigger than imagined. The layman’s first response was “Are you saying scientists are making […]\nFraud, Sloppiness, and Statistics first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/11/fraud-sloppiness-and-statistics/",
          "publishedOn": "2021-12-11T15:17:46.000Z",
          "wordCount": 1558,
          "title": "Fraud, Sloppiness, and Statistics"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=91354",
          "author": "John",
          "description": "Yesterday I wrote about the zeta distribution and the Yule-Simon distribution and said that they, along with the Zipf distribution, are all discrete power laws. This post fills in detail for that statement. The probability mass functions for the zeta, Zipf, and Yule-Simon distributions are as follows. Here the subscripts ζ, y, and z stand for […]\nComparing three discrete power laws first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/11/zeta-zipf-yule/",
          "publishedOn": "2021-12-11T12:43:24.000Z",
          "wordCount": 513,
          "title": "Comparing three discrete power laws"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=91297",
          "author": "John",
          "description": "The previous post on the Yule-Simon distribution mentioned the zeta distribution at the end. This is a powerlaw distribution on the positive integers with normalizing constant given by the Riemann zeta distribution. That is, the zeta distribution has density f(k; s) = k–s / ζ(s). where k is a positive integer and s > 1 is […]\nThe zeta distribution first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/10/the-zeta-distribution/",
          "publishedOn": "2021-12-10T19:01:19.000Z",
          "wordCount": 460,
          "title": "The zeta distribution"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=91286",
          "author": "John",
          "description": "The Yule-Simon distribution, named after Udny Yule and Herbert Simon, is a discrete probability with pmf The semicolon in f(k; ρ) suggests that we think of f as a function of k, with a fixed parameter ρ. The way the distribution shows the connection to the beta function, but for our purposes it will be […]\nYule-Simon distribution first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/10/yule-simon-distribution/",
          "publishedOn": "2021-12-10T16:36:59.000Z",
          "wordCount": 363,
          "title": "Yule-Simon distribution"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=91232",
          "author": "John",
          "description": "I was reading a stats book that mentioned Mahalanobis distance and that made me think of Non Nobis from Henry V, a great scene in a great movie. As far as I know, there’s no connection between Mahalanobis and Non Nobis except that both end in “nobis.” Since Mahalanobis is an Indian surname and Non […]\nMahalanobis distance and Henry V first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/09/mahalanobis-distance/",
          "publishedOn": "2021-12-10T02:27:31.000Z",
          "wordCount": 432,
          "title": "Mahalanobis distance and Henry V"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=91141",
          "author": "John",
          "description": "Take a permutation of the numbers 1 through n² and lay out the elements of the permutation in a square. We will call a permutation a magic permutation if the corresponding square is a magic square. What is the probability that a permutation is a magic permutation? That is, if you fill a grid randomly […]\nProbability of a magical permutation first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/09/magic-permutation/",
          "publishedOn": "2021-12-09T13:05:50.000Z",
          "wordCount": 942,
          "title": "Probability of a magical permutation"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=91139",
          "author": "John",
          "description": "A square grid of distinct integers is a magic square if all its rows columns and full diagonals have the same sum. Otherwise it is not a magic square. Now suppose we fill a square grid with samples from a continuous random variable. The probability that the entries are distinct is 1, but the probability […]\nDegree of magic first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/08/degree-of-magic/",
          "publishedOn": "2021-12-09T03:12:57.000Z",
          "wordCount": 338,
          "title": "Degree of magic"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=91028",
          "author": "John",
          "description": "I started to write a blog post about universal properties, but ended up writing a Twitter thread instead. See also examples of universal properties.\nUniversal properties first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/08/universal-properties/",
          "publishedOn": "2021-12-08T15:48:56.000Z",
          "wordCount": 179,
          "title": "Universal properties"
        },
        {
          "id": "https://www.johndcook.com/blog/?p=91014",
          "author": "John",
          "description": "Rings made a bad first impression on me. I couldn’t remember the definitions of all the different kinds of rings, much less have an intuition for what was important about each one. As I recall, all the examples of rings in our course were variations on the integers, often artificial variations. Entire functions I’m more […]\nThe ring of entire functions first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2021/12/07/the-ring-of-entire-functions/",
          "publishedOn": "2021-12-07T17:17:47.000Z",
          "wordCount": 826,
          "title": "The ring of entire functions"
        }
      ]
    }
  ],
  "cliVersion": "1.11.3"
}