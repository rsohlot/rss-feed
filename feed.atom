<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://rsohlot.github.io/rss-feed/index.html</id>
    <title>osmos::feed</title>
    <updated>2023-08-15T00:41:10.387Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="https://rsohlot.github.io/rss-feed/index.html"/>
    <link rel="self" href="https://rsohlot.github.io/rss-feed/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 8/14/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15rckdk/oneminute_daily_ai_news_8142023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15rckdk/oneminute_daily_ai_news_8142023/"/>
        <updated>2023-08-15T00:36:49.000Z</updated>
        <summary type="html"><![CDATA[Talon Aerolytics, a leading innovator in SaaS, Digital Twin capture services and AI technology, has announced ha its groundbreaking cutting-edge AI-powered computer vision platform enables wireless operators to visualise and analyse network assets using end-to-end AI and machine learning.[1]
 Beijing is poised to implement sweeping new regulations for artificial intelligence services this week, trying to balance state control of the technology with enough support that its companies can become viable global competitors.[2]
 Saudi Arabia and the United Arab Emirates are buying up thousands of the high-performance Nvidia chips crucial for building artificial intelligence software, joining a global AI arms race that is squeezing the supply of Silicon Valley’s hottest commodity.[3]
 OpenAI likely to go bankrupt by the end of 2024.[4]
  
Sources:
 [1] https://www.eenewseurope.com/en/groundbreaking-ai-powered-platform-visualises-wireless-assets/
 [2] https://www.bloomberg.com/news/articles/2023-08-14/china-tries-to-regulate-ai-with-state-control-support-for-tech-companies?in_source=embedded-checkout-banner
 [3] https://www.ft.com/content/c93d2a76-16f3-4585-af61-86667c5090ba
 [4] https://www.livemint.com/ai/artificial-intelligence/openai-likely-to-go-bankrupt-by-the-end-of-2024-report-11691815279479.html 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] AI Jobs are paying a hell of a lot of Money.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15rajkt/d_ai_jobs_are_paying_a_hell_of_a_lot_of_money/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15rajkt/d_ai_jobs_are_paying_a_hell_of_a_lot_of_money/"/>
        <updated>2023-08-14T23:14:52.000Z</updated>
        <summary type="html"><![CDATA[AI Jobs are paying a hell of a lot of Money.
 - Companies across industries like entertainment, retail, and manufacturing are engaging in an "AI recruiting frenzy" to hire data scientists, machine learning experts, and other AI talent. 
 ​
 - Demand for AI skills is driving salaries up, with some roles offering compensation packages approaching $1 million. Jobs at companies like Netflix, Match Group, Amazon, and Walmart advertise base salaries from $250k to $400k.
 ​
 - Total compensation with bonuses and stock grants can be much higher than base salaries. But average pay for roles like prompt engineers is around $130k total. 
 ​
 - Various factors make AI talent scarce - limited supply, competition from many industries, and candidates with multiple offers. Mid and senior level roles are hardest to fill.
 ​
 - Companies are using tactics like acquisition of AI startups, internal training programs, and pitching impactful work to attract candidates. 
 ​
 - Recruiters say AI engineers and product managers can be very selective about roles, caring about the meaningfulness of the work.
 ​
 - Firms realize they cannot just hire their way out of the AI talent crunch. Retention and internal development will also be key.
 ​
 In summary, surging corporate AI demand against limited talent supply has created a hyper-competitive market where top AI professionals can command extreme compensation.
    submitted by    /u/Yavero  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DIY Custom AI Chatbot for Business (open source)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15r8rif/diy_custom_ai_chatbot_for_business_open_source/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15r8rif/diy_custom_ai_chatbot_for_business_open_source/"/>
        <updated>2023-08-14T22:07:32.000Z</updated>
        <summary type="html"><![CDATA[If you're looking to train a custom chatbot on your data (SOPs, legal docs, financial reports, etc), I'd strongly suggest checking out AnythingLLM.
 It's the first chatbot with enterprise-grade privacy & security.
 When using ChatGPT, OpenAI collects your data including:
  
Prompts & Conversations
 Geolocation data
 Network activity information
 Commercial information e.g. transaction history
 Identifiers e.g. contact details
 Device and browser cookies
 Log data (IP address etc.)
  
However, if you use their API to interact with their LLMs like gpt-3.5 or gpt-4, your data is NOT collected. This is exactly why you should build your own private & secure chatbot. That may sound difficult, but Mintplex Labs (backed by Y-Combinator) just released AnythingLLM, which gives you the ability to build a chatbot in 10 minutes without code.
 AnythingLLM provides you with the tools to easily build and manage your own private chatbot using API keys. Plus, you can expand your chatbot’s knowledge by importing data such as PDFs, emails, etc. This can be confidential data as only you have access to the database.
 ChatGPT currently allows you to upload PDFs, videos and other data to ChatGPT via vulnerable plug-ins, BUT there is no way to determine if that data is secure or even know where it’s stored.
 Easily build your own business-compliant and secure chatbot at http://useanything.com/. All you need is an OpenAI or Azure OpenAI API key.
 Or, if you prefer using the open source code yourself, here’s the GitHub repo: https://github.com/Mintplex-Labs/anything-llm. 
 https://preview.redd.it/r2qf685bf5ib1.png?width=1200&format=png&auto=webp&s=e1fe809338dd5e76c0c82e1fcbd2cf0afe957eb2
    submitted by    /u/rue_so  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Synthetic Duplicates of Confidential Datasets]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15r8db6/p_synthetic_duplicates_of_confidential_datasets/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15r8db6/p_synthetic_duplicates_of_confidential_datasets/"/>
        <updated>2023-08-14T21:53:22.000Z</updated>
        <summary type="html"><![CDATA[Hey guys! Just released a small pip package based off some research I've been doing at school. The package is super simple, but enables devs to take an existing dataset and easily create a synthetic, privacy-preserving duplicate of this dataset. Not exactly sure where this lies in the data pipeline yet, but I imagine in the grand scheme of things some tech like this + a bunch of other features I want to add could be helpful in some way along the lines of data sharing/accessibility? I'd love some feedback and would even appreciate chatting with some folks for a few minutes to hear any advice you guys might have for me moving forward. This is the link to check it out: https://pypi.org/project/verisptab/. Thanks!
    submitted by    /u/avnertothemoon  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overview of the OWASP Top 10 for LLMs]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15r6g6q/overview_of_the_owasp_top_10_for_llms/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15r6g6q/overview_of_the_owasp_top_10_for_llms/"/>
        <updated>2023-08-14T20:43:39.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/confusedcrib  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Jobs will not be lost because AI is getting smarter. People are getting dumber instead.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15r5dhr/jobs_will_not_be_lost_because_ai_is_getting/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15r5dhr/jobs_will_not_be_lost_because_ai_is_getting/"/>
        <updated>2023-08-14T20:04:07.000Z</updated>
        <summary type="html"><![CDATA[This article refers that everyone's busy debating whether AI's going to steal jobs due to its superiority. But it is not just about AI outshining humans; it's about the decline in good old human competence.
 What are your thoughts on this?
    submitted by    /u/Powerful-Pumpkin-938  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PPO Tensorboard loss functions (Part 2)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15r59rq/ppo_tensorboard_loss_functions_part_2/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15r59rq/ppo_tensorboard_loss_functions_part_2/"/>
        <updated>2023-08-14T20:00:34.000Z</updated>
        <summary type="html"><![CDATA[In my previous post ( Reinforcement Learning (reddit.com) ) I had a hard time understand the differences between the loss functions in Tensorboard, and thanks to the members here, after normalizing and changing the reward functions, they became clearer to me.
 I'd like first to give perspective of my custom gym environment.
  
I use PPO2 algorithm ( n_steps=512 , nminibatches=8 ) as from my understanding it means the weights will get updated within these steps instead of waiting for an episode to finish.
  
Scenario 1: use the first 5000 rows from my dataset, I get the following results:
 5k rows
 As seen, the entropy loss seems to increase which means the policy isn't learning and has a lot of randomness. However, the loss seems to decrease, but I'm not sure if I should continue training for more steps and it will converge eventually!
 Scenario 2: use the 50,000 rows from my dataset, I get the following results:
 50k rows
 As seen, the loss function goes to zero very quickly, then jump really high and return back to zero, I don't understand this behavior as the mode seems to not still learning!
 Can someone please help me understand what's happening?
    submitted by    /u/Acceptable_Egg6552  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] periodicals for keeping up to date]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15r4lk1/d_periodicals_for_keeping_up_to_date/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15r4lk1/d_periodicals_for_keeping_up_to_date/"/>
        <updated>2023-08-14T19:34:57.000Z</updated>
        <summary type="html"><![CDATA[What are your favorite resources for keeping up to date without reading every paper that comes out? Tips and tricks columns etc. I've recently transitioned to a job in medical images and computer vision from my material science phd. I have some experience but not a machine learning PhD. I've been trying to keep up to date and linearlayer.substack.com seemed like a pretty good sub. I'm looking for more. Are there any niche periodical substacks, podcasts or forums you enjoy?
    submitted by    /u/VooDooZulu  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Nba season dataset]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15r3tqo/p_nba_season_dataset/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15r3tqo/p_nba_season_dataset/"/>
        <updated>2023-08-14T19:07:15.000Z</updated>
        <summary type="html"><![CDATA[Just got my hands on basically every data point possible for every game from the 2022-2023 season, what should I do with it?
    submitted by    /u/michaelc143  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] TeaRoute - Trainable, efficient, autonomous routing for LLM queries.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15r3efa/p_tearoute_trainable_efficient_autonomous_routing/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15r3efa/p_tearoute_trainable_efficient_autonomous_routing/"/>
        <updated>2023-08-14T18:51:53.000Z</updated>
        <summary type="html"><![CDATA[Hi r/MachineLearning,
 I'm happy to show a new project I've been working on called TeaRoute (Trainable, autonomous, efficient routing). Tear is a simple tool that allows you to easily control the flow of information through LLMs via automated text classification.
 With just a few (~10) lines of code, you can set up a router that classifies text inputs and directs them to different LLM endpoints for processing. This makes it easier to build things like:
  
Chatbots that route questions to different departments
 Multi-document question answering systems
 Dynamic classification models that improve over time
  
Some of the key features of TeaRoute:
  
Classification based on embeddings for high efficiency and low cost
 Option to use LLMs for classification when needed (and add that to training corpus)
 Easy setup in around 10 lines of code
  
I've open sourced TeaRoute and documented it fully, with examples like building a movie chatbot router. You can check out the code here: https://github.com/kesile/TeaRoute. You can install it into your program with "pip install TeaRoute".
 Let me know if you end up building something cool with TeaRoute! And feel free to open issues on GitHub if you run into any problems. Let me know your thoughts!
    submitted by    /u/Rejg  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What's your opinion on a network of experts being the correct way to go?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15r3by0/whats_your_opinion_on_a_network_of_experts_being/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15r3by0/whats_your_opinion_on_a_network_of_experts_being/"/>
        <updated>2023-08-14T18:49:17.000Z</updated>
        <summary type="html"><![CDATA[GPT-4 has been top dog for a while, and it's said to be a network of experts architecture. Even the human brain is similar to some degree with different "sections" of the brain or cortexes being specialized for specific sensory input. It's always made sense to me that a quick way to come to a "good enough" AI is to train many smaller expert AIs and then train a umbrella AI that just delegates parts of the task to different experts, and organizes the output. The next step would obviously be training that umbrella AI to make it's own expert AIs in an unfamiliar supervised fashion.
 Is anyone already working on something like this? Do you think it's a worthy avenue of research?
    submitted by    /u/TrainquilOasis1423  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Implementing real time transcription]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15r35mq/d_implementing_real_time_transcription/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15r35mq/d_implementing_real_time_transcription/"/>
        <updated>2023-08-14T18:42:57.000Z</updated>
        <summary type="html"><![CDATA[I am working on a project where we need real time transcription of speech (mostly input through microphone). So the workflow is: 1-User starts speaking 2-Live transcription of their speech appears on screen
 By "real time/live" I mean the latency should not exceed 5 seconds, ideally much less while maintaining maximum accuracy.
 My question is: how do I achieve this?
 I have been experimenting with openai's whisper but I am not sure how can I get it to work with real time audio input since the model expects 30-second segments (preferably containing full sentences).
 My main challenge is how should I segment the audio. Should I use a VAD to split on silences? (I tried this but the transcription accuracy is lower since whisper doesn't have access to context outside of each segment)
 Also another question I have is how does hugging face automated speech recognition pipeline transcribe long audio files. I tried to read the documentation but cannot figure it out.
 Thanks in advance
    submitted by    /u/Amgadoz  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Project] GPU-Accelerated LLM on a $100 Orange Pi]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15r1vy5/project_gpuaccelerated_llm_on_a_100_orange_pi/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15r1vy5/project_gpuaccelerated_llm_on_a_100_orange_pi/"/>
        <updated>2023-08-14T17:57:13.000Z</updated>
        <summary type="html"><![CDATA[Progress in open language models has been catalyzing innovation across question-answering, translation, and creative tasks. While current solutions demand high-end desktop GPUs to achieve satisfactory performance, to unleash LLMs for everyday use, we wanted to understand how usable we could deploy them on the affordable embedded devices.
 Many embedded devices come with mobile GPUs that can serve as a source of acceleration. In this project, we pick Orange Pi 5, a RK35888-based board that is similar to Raspberry Pi but also features a more powerful Mali-G610 GPU.
 We 5 tok/sec for RedPajama-3b and 2.5 tok/sec for Llama2-7b. We can also get to 1.5 tok/sec on a 16GB version of the Orange Pi 5+ under $150.
 - Project: https://github.com/mlc-ai/mlc-llm
 - Blogpost: https://blog.mlc.ai/2023/08/09/GPU-Accelerated-LLM-on-Orange-Pi
 ​
 ​
    submitted by    /u/crowwork  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What tools are the best for people starting there own brand??]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15r0zx8/what_tools_are_the_best_for_people_starting_there/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15r0zx8/what_tools_are_the_best_for_people_starting_there/"/>
        <updated>2023-08-14T17:24:21.000Z</updated>
        <summary type="html"><![CDATA[Hi there,
 I know it’s a bit of a broad questions and I have done some research, but have come across a huge amount of Ai tools, that I am unsure what ones are good and what ones are not.
 A few of the things I am after: 
 Text to videos Creating designs for clothing
 Thanks!!
    submitted by    /u/redoutraged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] MiniWoB++ v1.0 - Web interaction environments for RL]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15r0ss5/n_miniwob_v10_web_interaction_environments_for_rl/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15r0ss5/n_miniwob_v10_web_interaction_environments_for_rl/"/>
        <updated>2023-08-14T17:16:58.000Z</updated>
        <summary type="html"><![CDATA[We are releasing the mature 1.0 version of MiniWoB++ (Mini World of Bits++), an RL benchmark containing over 100 web interaction environments, ranging from simple button clicks to more complex forms and web apps.
 The environments were released by OpenAI back in 2017 as just HTML pages. With MiniWoB++, the environments run on a browser, and an RL agent can get the environment states or execute actions via Selenium WebDriver.
 This version of MiniWoB++ contains the following features:
 * Over 100 web environments, including 2 bigger environments based on real websites and 18 previously unavailable “test set” environments. All (but 4) environments are deterministic for the given random seed.
 * Full integration with Gymnasium, a fork of OpenAI Gym, which provides a standardized API for RL.
 * A wide range of implemented browser actions including clicking, dragging, scrolling, typing, and pressing keyboard shortcuts, all of which can be customized (e.g., coordinate binning or scrolling speed).
 Tweet: https://twitter.com/FaramaFound/status/1691135031798804480?s=20
 Release notes: https://github.com/Farama-Foundation/miniwob-plusplus/releases/tag/v1.0
 Documentation: https://miniwob.farama.org/
    submitted by    /u/elliottower  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] MiniWoB++ v1.0 - Web interaction environments for RL]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15r0rx3/p_miniwob_v10_web_interaction_environments_for_rl/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15r0rx3/p_miniwob_v10_web_interaction_environments_for_rl/"/>
        <updated>2023-08-14T17:16:06.000Z</updated>
        <summary type="html"><![CDATA[We are releasing the mature 1.0 version of MiniWoB++ (Mini World of Bits++), an RL benchmark containing over 100 web interaction environments, ranging from simple button clicks to more complex forms and web apps.
 The environments were released by OpenAI back in 2017 as just HTML pages. With MiniWoB++, the environments run on a browser, and an RL agent can get the environment states or execute actions via Selenium WebDriver.
 This version of MiniWoB++ contains the following features:
 * Over 100 web environments, including 2 bigger environments based on real websites and 18 previously unavailable “test set” environments. All (but 4) environments are deterministic for the given random seed.
 * Full integration with Gymnasium, a fork of OpenAI Gym, which provides a standardized API for RL.
 * A wide range of implemented browser actions including clicking, dragging, scrolling, typing, and pressing keyboard shortcuts, all of which can be customized (e.g., coordinate binning or scrolling speed).
 Tweet: https://twitter.com/FaramaFound/status/1691135031798804480?s=20
 Release notes: https://github.com/Farama-Foundation/miniwob-plusplus/releases/tag/v1.0
 Documentation: https://miniwob.farama.org/
    submitted by    /u/elliottower  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-shot and few-shot prompting for the BloomZ 176B foundation model with the simplified Amazon SageMaker JumpStart SDK]]></title>
        <id>4a5f10994ec26c7690d7dfc241d4fb77690d5c77</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/zero-shot-and-few-shot-prompting-for-the-bloomz-176b-foundation-model-with-the-simplified-amazon-sagemaker-jumpstart-sdk/"/>
        <updated>2023-08-14T17:07:28.000Z</updated>
        <summary type="html"><![CDATA[Amazon SageMaker JumpStart is a machine learning (ML) hub offering algorithms, models, and ML solutions. With SageMaker JumpStart, ML practitioners can choose from a growing list of best performing and publicly available foundation models (FMs) such as BLOOM, Llama 2, Falcon-40B, Stable Diffusion, OpenLLaMA, Flan-T5/UL2, or FMs from Cohere and LightOn. In this post and […]]]></summary>
        <author>
            <name>Rajakumar Sampathkumar</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I made "Connect 4" with Pygame and DRL]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15r04if/i_made_connect_4_with_pygame_and_drl/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15r04if/i_made_connect_4_with_pygame_and_drl/"/>
        <updated>2023-08-14T16:52:12.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Disastrous-Ladder-46  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Vision-based reinforcement learning for Trackmania: close or at superhuman level]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15qzxkx/p_visionbased_reinforcement_learning_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15qzxkx/p_visionbased_reinforcement_learning_for/"/>
        <updated>2023-08-14T16:45:15.000Z</updated>
        <summary type="html"><![CDATA[We used model-free, value-based reinforcement learning (mostly dueling-IQN) to train an AI that plays Trackmania. The system is mostly vision based, along with some information taken from the game engine such as car speed and acceleration.
 On our simple training track, we believe we are close to or above human level. We have yet to find a human who sets a better racing time than our AI's.
 We tried many extensions to the algorithm (noisy, persistent advantage learning, munchausen, ...), but none of these extensions improved the ultimate performance of our AI.
 Link the the video, we're pretty proud of this result. :)
 ​
    submitted by    /u/Linesight_rl  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision-based reinforcement learning for Trackmania: close or at superhuman level]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15qzvxe/visionbased_reinforcement_learning_for_trackmania/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15qzvxe/visionbased_reinforcement_learning_for_trackmania/"/>
        <updated>2023-08-14T16:43:28.000Z</updated>
        <summary type="html"><![CDATA[We used model-free, value-based reinforcement learning (mostly dueling-IQN) to train an AI that plays Trackmania. The system is mostly vision based, along with some information taken from the game engine such as car speed and acceleration.
 On our simple training track, we believe we are close to or above human level. We have yet to find a human who sets a better racing time than our AI's.
 We tried many extensions to the algorithm (noisy, persistent advantage learning, munchausen, ...), but none of these extensions improved the ultimate performance of our AI.
 Link the the video, we're pretty proud of this result. :)
 ​
    submitted by    /u/Linesight_rl  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Build production-ready generative AI applications for enterprise search using Haystack pipelines and Amazon SageMaker JumpStart with LLMs]]></title>
        <id>4f1c4b63ee8db9988e922652c22b42cf10dbb6f4</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/build-production-ready-generative-ai-applications-for-enterprise-search-using-haystack-pipelines-and-amazon-sagemaker-jumpstart-with-llms/"/>
        <updated>2023-08-14T16:42:40.000Z</updated>
        <summary type="html"><![CDATA[In this post, we showcase how to build an end-to-end generative AI application for enterprise search with Retrieval Augmented Generation (RAG) by using Haystack pipelines and the Falcon-40b-instruct model from Amazon SageMaker JumpStart and Amazon OpenSearch Service.]]></summary>
        <author>
            <name>Tuana Celik</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-driven predictive analytics for revenue forecasting in healthcare]]></title>
        <id>https://www.datasciencecentral.com/?p=62831</id>
        <link href="https://www.datasciencecentral.com/ai-driven-predictive-analytics-for-revenue-forecasting-in-healthcare/"/>
        <updated>2023-08-14T15:43:49.000Z</updated>
        <summary type="html"><![CDATA[Innovation is increasingly driven by data. As technology advances and alters human behavior, industries collect a growing quantity of information. This data is valuable once we are able to extract actionable, meaningful insights from it – insights that can accelerate better outcomes while remaining equitable and inclusive of the populations we serve, allowing us to… Read More »AI-driven predictive analytics for revenue forecasting in healthcare
The post AI-driven predictive analytics for revenue forecasting in healthcare appeared first on Data Science Central.]]></summary>
        <author>
            <name>John Lee</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A new era of carrier connectivity: How technology is bridging the gap]]></title>
        <id>https://www.datasciencecentral.com/?p=62881</id>
        <link href="https://www.datasciencecentral.com/a-new-era-of-carrier-connectivity-how-technology-is-bridging-the-gap/"/>
        <updated>2023-08-14T15:41:07.000Z</updated>
        <summary type="html"><![CDATA[In the logistics and transportation industry, carrier connectivity has long been challenging, often riddled with inefficiencies and communication barriers. Innovative tools and platforms are revolutionizing how carriers connect with shippers and other stakeholders, fostering real-time collaboration and transparency.  This new era of carrier connectivity enhances the flow of information and redefines how the industry operates.… Read More »A new era of carrier connectivity: How technology is bridging the gap
The post A new era of carrier connectivity: How technology is bridging the gap appeared first on Data Science Central.]]></summary>
        <author>
            <name>Ovais Naseem</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Classifying Energy News with Machine Learning: a Multi-Label Problem solved using Binary Relevance with XGBoost]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15qv9nx/p_classifying_energy_news_with_machine_learning_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15qv9nx/p_classifying_energy_news_with_machine_learning_a/"/>
        <updated>2023-08-14T13:49:59.000Z</updated>
        <summary type="html"><![CDATA[Hi everybody,
 I want to share with this community a recent challenge involving machine learning that I've faced. Over the past few weeks, I've been busy creating a machine learning model. The goal? To classify news, mostly related to the energy sector, into 20 different topics. This task is a bit tricky since a news piece might fall into more than one category, so it's what's called a multi-label problem. Until now, I was doing this just using just keywords, but I wanted to move to something much better and robust.
 So basically, I started by taking 800 headlines and summaries and classifying them manually one by one into these 20 categories. After that, I began building the model in Python using scikit-learn. I tried different methods like logistic regression, random forest, SVM, etc. Af…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Community Events [Project]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15qr829/community_events_project/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15qr829/community_events_project/"/>
        <updated>2023-08-14T10:45:58.000Z</updated>
        <summary type="html"><![CDATA[Hi u/all am thinking of creating a series of interesting events for the broader AI-Community. I always felt most events are either too expensive or too much of a company pitch. The topics should vary each time but for sure cover technical deep dives, some high level talks and some possibility to network and get together. I already have some company sponsors who are just interested in showing their name in the venue but don't want to interact/get information on participants or anything.
 ​
 What do you think - is this worth a shot? 
 What ideas do you have wrt topics, speakers?
    submitted by    /u/CarlCarter312  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Community Events]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15qr5uy/community_events/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15qr5uy/community_events/"/>
        <updated>2023-08-14T10:42:34.000Z</updated>
        <summary type="html"><![CDATA[Hi u/all am thinking of creating a series of interesting events for the broader AI-Community. I always felt most events are either too expensive or too much of a company pitch. The topics should vary each time but for sure cover technical deep dives, some high level talks and some possibility to network and get together. I already have some company sponsors who are just interested in showing their name in the venue but don't want to interact/get information on participants or anything.
 ​
 What do you think - is this worth a shot? 
 What ideas do you have wrt topics, speakers?
    submitted by    /u/CarlCarter312  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI is going to eliminate way more jobs than anyone realizes]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15qqyin/ai_is_going_to_eliminate_way_more_jobs_than/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15qqyin/ai_is_going_to_eliminate_way_more_jobs_than/"/>
        <updated>2023-08-14T10:31:47.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/thisisinsider  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15qq3ou/r_okapi_instructiontuned_large_language_models_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15qq3ou/r_okapi_instructiontuned_large_language_models_in/"/>
        <updated>2023-08-14T09:46:34.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/KingsmanVince  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best free ai text to image apps right now?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15qovvr/best_free_ai_text_to_image_apps_right_now/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15qovvr/best_free_ai_text_to_image_apps_right_now/"/>
        <updated>2023-08-14T08:36:34.000Z</updated>
        <summary type="html"><![CDATA[I want to download an ai app that is free that does text to image with no limit on usage. Any advice would be good! Thanks!
    submitted by    /u/DrowsyDrowsy  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Where the jobs at?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15qmkr6/where_the_jobs_at/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15qmkr6/where_the_jobs_at/"/>
        <updated>2023-08-14T06:28:54.000Z</updated>
        <summary type="html"><![CDATA[I love AI, I'm not dumb, but I don't yet code. What sort of jobs are there for a guy like me (45, world-class video engineer and decent TV producer) in this realm? What are the jobs at all? I'm so curious about the whole operation from the bottom up!
    submitted by    /u/beebo135  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question on creation Neural Network to forecast construction duration considering delays.]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15qm8tm/question_on_creation_neural_network_to_forecast/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15qm8tm/question_on_creation_neural_network_to_forecast/"/>
        <updated>2023-08-14T06:11:12.000Z</updated>
        <summary type="html"><![CDATA[Data I have:
 Project Information (Cost, location, original contract duration)
 Different Reasons for extension of duration and corresponding number of days (actual record of different sample projects).
 ​
 Based on this data, will the neural network make its own analysis of the frequency and severity of each Reason for extension of duration and incorporate it in the output?
    submitted by    /u/shearhead  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using RL in a videogame with only 1 state?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15qk8hd/using_rl_in_a_videogame_with_only_1_state/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15qk8hd/using_rl_in_a_videogame_with_only_1_state/"/>
        <updated>2023-08-14T04:28:04.000Z</updated>
        <summary type="html"><![CDATA[I'm super new to RL, just started today amd I'm probably doing it super wrong but here goes.
 Basically I'm trying to get an AI command center base in Halo Infinite to apply RL in order to determine which loadout to apply to a bot defending the base you're attacking. The issue I'm running into is how to preditct the Q-values for the next state.
 In my map, I decided to change from one state to the next every 5 kills per team. So when that happens the Kill/death ratio of each loadout equipped by a player is updated and used as a reward in the Q-table.
 Well that's the idea, anyway. I wanted to use player kills as a sort of benchmark to gauge performance with a given loadout but I'm afraid the state space I applied is too small and I don't know how to transition from one state to the next and obtain the maximum reward from the expected future rewards of the next state.
    submitted by    /u/swagonflyyyy  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Identify Real Diamonds From Cubic Zirconia]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15qihi8/how_to_identify_real_diamonds_from_cubic_zirconia/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15qihi8/how_to_identify_real_diamonds_from_cubic_zirconia/"/>
        <updated>2023-08-14T03:03:41.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/HumanityFirst16  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] ways to model text and their strengths.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15qhm6b/d_ways_to_model_text_and_their_strengths/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15qhm6b/d_ways_to_model_text_and_their_strengths/"/>
        <updated>2023-08-14T02:22:20.000Z</updated>
        <summary type="html"><![CDATA[Hello, I'm trying to write a seq2seq model for text. I can think of many text to vector mappings, but I'm wondering are there better ones?
  
Codes: convert text to unicode codepoints, and represent it as 1m-ary classification
 Bits: convert text to utf-8, and represent it as a binary classification (0,1)
 Bytes: convert text to utf-8 and represent it as 256-ary classification
 Chars Embedding: identify small sequences and embed into space
 Word Embedding: identify words from a dictionary and embed into space (unknown words?)
  
   submitted by    /u/windoze  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[News] NVIDIA finally releases Neuralangelo's source code!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15qhkq0/news_nvidia_finally_releases_neuralangelos_source/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15qhkq0/news_nvidia_finally_releases_neuralangelos_source/"/>
        <updated>2023-08-14T02:20:32.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/RegularConstant  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do you know what AI makes these?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15qgc2f/do_you_know_what_ai_makes_these/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15qgc2f/do_you_know_what_ai_makes_these/"/>
        <updated>2023-08-14T01:22:34.000Z</updated>
        <summary type="html"><![CDATA[They are so pretty, but I know next to nothing about AI. I’m pretty bad with tech overall, but if I can find the name of the AI, I have someone who can help me use it.
    submitted by    /u/ChrisMSpink  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring OpenAI's Mastery Over DOTA 2: A Deep Dive into Machine Learning's Pinnacle in Competitive Gaming. #AIEsportsRevolution]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15qg4sf/exploring_openais_mastery_over_dota_2_a_deep_dive/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15qg4sf/exploring_openais_mastery_over_dota_2_a_deep_dive/"/>
        <updated>2023-08-14T01:13:27.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/stefanbg92  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["First Contact: Unsupervised Human-Machine Co-Adaptation via Mutual Information Maximization", Reddy et al 2022]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15qfpl2/first_contact_unsupervised_humanmachine/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15qfpl2/first_contact_unsupervised_humanmachine/"/>
        <updated>2023-08-14T00:53:53.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Open Source Announcement and The Current State of Generative 3D (Aug 2023)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15qew5n/r_open_source_announcement_and_the_current_state/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15qew5n/r_open_source_announcement_and_the_current_state/"/>
        <updated>2023-08-14T00:17:30.000Z</updated>
        <summary type="html"><![CDATA[At Mirage, we’ve kept our platform at the edge of generative 3D starting with CLIP-Mesh by Nasir → GET3D by NVIDIA → Stable Dreamfusion by Google (implemented by ashawkey) → Point-E by OpenAI → Shap-E by OpenAI. We’ve spent time optimizing these open-source repos to produce usable game assets in multiple formats (recently GLTF).
 Today, we are open-sourcing Mirage3D. This repo is built upon the fantastic work of the folks mentioned above and is optimized to create GLB files easily.
 https://github.com/MirageML/Mirage3D
 Try out the repo yourself and consider contributing to help create a single source of truth for open-source generative 3D models, optimized to create usable 3D assets!
 The Problem
 Generative 3D is progressing slower than other modalities (audio, video, image, etc) due to …]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Legions of DEF CON hackers will attack generative AI models]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15qdev6/legions_of_def_con_hackers_will_attack_generative/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15qdev6/legions_of_def_con_hackers_will_attack_generative/"/>
        <updated>2023-08-13T23:12:59.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Having trouble choosing which MLOps solution to transition to from using AWS Batch/S3/Dynamo + Tensorboard]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15qco1x/discussion_having_trouble_choosing_which_mlops/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15qco1x/discussion_having_trouble_choosing_which_mlops/"/>
        <updated>2023-08-13T22:42:42.000Z</updated>
        <summary type="html"><![CDATA[Overwhelmed choosing an MLOps/experiment tracking platform to move to from manual AWS/S3/Dynamo + local tensorboard
 Title says it all… 
 So many choices and it’s hard to figure out which to go with. Small team, 2-3 people, data scientists/python devs. mostly training MLP regressors on datasets with 20k-100k samples (approx 100 input features, approx 30 output variables) and CNNs on datasets with 500k-1m samples (approx 10x10000 timeseries per input sample along with 50-ish input features and 4x10000 output timeseries). 
 At the very least, upgrading our experiment tracking/results reporting from Tensorboard to a cloud platform is a big desire, however, I would also like to make our full pipeline a bit more documented/versioned, and possible simplify some of the architecture. Ideally, we c…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] llama2.py]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15qcbel/p_llama2py/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15qcbel/p_llama2py/"/>
        <updated>2023-08-13T22:28:15.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 Here is a ported version of Andrej Karpathy's llama2.c into pure Python with zero dependencies.
 The nice thing is llama2.py essentially captures the inference logic from the Llama2 research paper. With this port in pure Python, it's so much easier to unpack the intricate concepts originally presented in a "scientific language".
 Designed for an extensive audience, it aims to be a straightforward "reference implementation" suitable for educational purposes.The original llama2.c repository comprises two Python files intended for model training and one C file for inference.
 The goal of pure Python implementation is to bridge the existing gap by offering a clear-cut reference implementation encapsulating all transformer logic within a concise Python file, not exceeding 500 lines of code.
 Though the original Meta/Llama is written on Python, its complexity is rather high due to multiple dependencies and sophisticated optimizations implemented within. This often makes it hard to follow, particularly for those new to the field.
    submitted by    /u/Albatross9855  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling walking and the legs]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15qbvto/modeling_walking_and_the_legs/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15qbvto/modeling_walking_and_the_legs/"/>
        <updated>2023-08-13T22:10:51.000Z</updated>
        <summary type="html"><![CDATA[Hello everybody, I had a question and looking for the experience from this group. I want to learn how to train models for walking. More specifically, I'm interested in looking at which muscles are active during walking. Can I use one subject for deep reinforcement learning or will I need more subjects to train the data (say 20+ subjects)? Thanks,
    submitted by    /u/theslipguy  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The possibilities of artificial intelligence, how it will shape your future, and whether it will have an impact at all?!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15qbffc/the_possibilities_of_artificial_intelligence_how/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15qbffc/the_possibilities_of_artificial_intelligence_how/"/>
        <updated>2023-08-13T21:53:21.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/CoylyLard  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stable Baselines PPO vs Ray.io PPO]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15qaz94/stable_baselines_ppo_vs_rayio_ppo/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15qaz94/stable_baselines_ppo_vs_rayio_ppo/"/>
        <updated>2023-08-13T21:35:10.000Z</updated>
        <summary type="html"><![CDATA[For context, I've been experimenting with different Reinforcement learning algorithms, frameworks etc. Currently I have a custom Gym environment with Stable baselines 3 to train a PPO agent. 
 I've noticed however that this setup is very slow for training as stable Baselines doesn't properly utilize GPU + parallel environments don't really work that well due to "step lock" where it can't continue until everything is synced up. 
 I've heard really good things about Ray/RLlib. It has more advance features + according to reports it's crazy fast for training. 
 I want to basically rewrite my existing SB3 implementation into Ray but I don't find the documentation very user-friendly + there aren't really that much good/usefull tutorials online as far is I can find. 
 I tried GPT-4 but it just regurgitates some very old Ray implementation. I even tried not using ray and trying Tensorflow again (I hate Tensorflow, I'm definitely team Pytorch 😬) 
 Do you guys have any good tutorials, videos, documentation to get started with PPO on Ray? Would love to learn more, but documentation online seems very hardcore to start even 😂.
 Would love to know your suggestions.
    submitted by    /u/ClassicAppropriate78  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Project] open source flowchart for complex prompt techniques = useful?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15q9lmr/project_open_source_flowchart_for_complex_prompt/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15q9lmr/project_open_source_flowchart_for_complex_prompt/"/>
        <updated>2023-08-13T20:40:09.000Z</updated>
        <summary type="html"><![CDATA[I've been using an abandoned block-coding framework called PyFlow to set up complex prompt chain techniques using LangChain. I found the visuals (and running multiple techniques at once) to be very helpful when comparing output quality.
 Would a beefed up open source version of this be useful for you and why? What are you currently doing to quickly test prompt chain techniques and measure the quality of the responses?
 https://preview.redd.it/ywpmjbwbvxhb1.png?width=1674&format=png&auto=webp&s=ca80a45e430b92bf31a9a9b9747c6d8ba7d3bde7
    submitted by    /u/copywriterpirate  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probem with running a python script that uses tflite [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15q8tnm/probem_with_running_a_python_script_that_uses/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15q8tnm/probem_with_running_a_python_script_that_uses/"/>
        <updated>2023-08-13T20:08:18.000Z</updated>
        <summary type="html"><![CDATA[I have a python script on my raspberry pi zero that uses tflite to make predictions on images. it gets imported like that:
 import tflite_runtime.interpreter as tflite
 When i run the script from the terminal with sudo it gives me the error:
 ModuleNotFoundError: No module named 'tflite_runtime'
 without sudo it works just fine. Now the thing is that I need to run that script from another script using os.system("python code.py") but that gives the same error no matter if I add sudo or not.
 Do you have any idea hw to fix that problem?
    submitted by    /u/Main-Associate-6457  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Colab Pro no longer gives you a V100, not even a P100, you now pay for the (previously free) Tesla T4.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15q81ir/d_colab_pro_no_longer_gives_you_a_v100_not_even_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15q81ir/d_colab_pro_no_longer_gives_you_a_v100_not_even_a/"/>
        <updated>2023-08-13T19:37:47.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/LumpySchool7262  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Your Neural Network Doesn't Know What It Doesn't Know]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15q7pgv/r_your_neural_network_doesnt_know_what_it_doesnt/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15q7pgv/r_your_neural_network_doesnt_know_what_it_doesnt/"/>
        <updated>2023-08-13T19:24:38.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 I made a repo trying to collect every high-quality source for Out-of-distribution detection, ranging from articles and talks for beginners to research papers at top conferences. It also has a primer if you are not familiar with the topic. Check it out and give it a star to support me if you find it helpful. Thanks a lot ;)
 https://github.com/continuousml
 ​
 https://preview.redd.it/gup7ckixhxhb1.png?width=868&format=png&auto=webp&s=e71f51bef0ff2b4f3f37e801702b5d365cbd67fd
    submitted by    /u/Ok-Kaleidoscope-505  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] is there a Civitai-like service or resource for local install Tortoise_TTS files?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15q71or/d_is_there_a_civitailike_service_or_resource_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15q71or/d_is_there_a_civitailike_service_or_resource_for/"/>
        <updated>2023-08-13T18:58:22.000Z</updated>
        <summary type="html"><![CDATA[Anything out there for this?
    submitted by    /u/Duemellon  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Understanding Instruction Tuning for Multimodal LLMs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15q6veh/p_understanding_instruction_tuning_for_multimodal/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15q6veh/p_understanding_instruction_tuning_for_multimodal/"/>
        <updated>2023-08-13T18:51:06.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/s_arme  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are there any AI LLM that are less restrictive in their answers, similar to ChatGPT on release?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15q6tou/are_there_any_ai_llm_that_are_less_restrictive_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15q6tou/are_there_any_ai_llm_that_are_less_restrictive_in/"/>
        <updated>2023-08-13T18:49:11.000Z</updated>
        <summary type="html"><![CDATA[Trying to dip my toes into trying other LLMs but not truly not sure which are comparable to ChatGPT. Would love any suggestions, and maybe an explanation of why you chose that AI.
    submitted by    /u/kokeda  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tips and tricks for publication of non-SOTA research? [Discussion]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15q6och/tips_and_tricks_for_publication_of_nonsota/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15q6och/tips_and_tricks_for_publication_of_nonsota/"/>
        <updated>2023-08-13T18:43:00.000Z</updated>
        <summary type="html"><![CDATA[Say, I have a method that can be used as a drop-in replacement for a part of a network in a set of relevant tasks, for example, an improved CTC loss in speech recognition and non-autoregressive translation.
 I can run a bunch of experiments, applying my method in those tasks, showing that "model A + my method" works substantially better than just "model A." "Model A" is some relatively light and simple model, like Transformer-base. It might have been near-SOTA several years ago.
 However, the results won't be near contemporary SOTA, because SOTA models tend to be heavy and complicated. In theory, the modification can apply to bigger models, but I don't have the resources to re-implement them.
 So, how do I present my work? In theory, doing the "model A" experiments should be enough to show that my modification is interesting. But in practice most of the reviewers aren't very excited when the reported numbers are far from SOTA, even if the aim is not to beat SOTA, but to present a general working approach. I know that there are many papers that do not contain any SOTA-beating in them, but still manage to be successfully published. Question: how do they do that? 
 Are there any tips and tricks? What kind of experiments/settings/datasets/models should I consider to convince the reviewers? Or at least make it harder for them to Reject-If-Not-SOTA?
    submitted by    /u/Tomarchelone  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] In search of big binaries]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15q68pn/r_in_search_of_big_binaries/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15q68pn/r_in_search_of_big_binaries/"/>
        <updated>2023-08-13T18:25:31.000Z</updated>
        <summary type="html"><![CDATA[I don't work in ML, but I work on an application that parses code and debugging information out of binaries (executables, shared libraries, etc.) that is then used by other people to build performance analysis tools that are used on ML applications.
 To do some performance benchmarking on my tool, I need some huge binaries to parse. Specifically, I need a binary with a lot of code (.text section in ELF) and debugging information (DWARF in ELF). I know there are lots of binaries that have a huge amount of data in them, but that doesn't help because my application just ignores that.
 Optimally, I need a binary with an on-disk size of greater than 5GB to get useful measurements, but greater than 1GB would still be good. I would also prefer being able to generate the binary from a source build so I can test it across the many architectures my tool supports.
 I have a collaborator who claims to have used my application on an 8GB TensorFlow binary. Unfortunately, that binary is on a classified system and I can't get access to it. However, that let's me know it's at least possible to make such a beast.
 Has anyone here seen such huge binaries using an ML application- it doesn't have to be TensorFlow?
 Thanks in advance!
  
Tools I have tested thus far are the computational chemistry application NWChem and the quantum mechanics Gaussian integral application libint. However, I can't get either of these to make binaries larger than about 100MB.
    submitted by    /u/OmegaNaughtEquals1  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] We built a multi-modal search app with Meta AI's ImageBind and Deep Lake (search with image, text, & audio)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15q50pz/p_we_built_a_multimodal_search_app_with_meta_ais/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15q50pz/p_we_built_a_multimodal_search_app_with_meta_ais/"/>
        <updated>2023-08-13T17:36:25.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/davidbun  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[terminated vs truncated in Gymnasium]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15q3weo/terminated_vs_truncated_in_gymnasium/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15q3weo/terminated_vs_truncated_in_gymnasium/"/>
        <updated>2023-08-13T16:52:57.000Z</updated>
        <summary type="html"><![CDATA[Hi! 
 I am not completely sure how to use these flags from the Gymnasium API (I've always used the Gym API so far and I'm switching just now...). Particularly in the environment, I'm playing with now:
 It's a 1vs1 game, and an episode can end if one of the 2 players dies or a max. number of steps is reached. In this case:
 - The max. number of steps is a hard limit rule of the game, so if these are reached should they receive truncated or terminated signals?
 - If a player dies, should both agents get a terminated = True, or one should get terminated=True and the other one truncated=True, as it was still alive, and could have continued playing? 
 ​
 Thank you!
 ​
    submitted by    /u/xWh0am1  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is it possible for a person to propose a new variety of neural networks?]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15q1sza/is_it_possible_for_a_person_to_propose_a_new/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15q1sza/is_it_possible_for_a_person_to_propose_a_new/"/>
        <updated>2023-08-13T15:29:35.000Z</updated>
        <summary type="html"><![CDATA[How to come up with the architecture?
    submitted by    /u/AnyJello605  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Simple Questions Thread]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15q13eu/d_simple_questions_thread/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15q13eu/d_simple_questions_thread/"/>
        <updated>2023-08-13T15:00:32.000Z</updated>
        <summary type="html"><![CDATA[Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!
 Thread will stay alive until next one so keep posting after the date in the title.
 Thanks to everyone for answering questions in the previous thread!
    submitted by    /u/AutoModerator  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] improve OCR accuracy in python]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15pzeco/d_improve_ocr_accuracy_in_python/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15pzeco/d_improve_ocr_accuracy_in_python/"/>
        <updated>2023-08-13T13:49:08.000Z</updated>
        <summary type="html"><![CDATA[Of course, I'd be happy to help you correct the errors in the text you provided. Here's the corrected version of your text:
 "Hello everyone, I'm currently working on a Python program in which I need to use an OCR library. However, the results weren't very satisfying even though the text in the image is clear and there is no noise (just text). This is an example of the images I've used:"
 https://preview.redd.it/flmdzz9otvhb1.png?width=515&format=png&auto=webp&s=dfd8ebab6f02369a150f7b9375e0d53baa6dceee
 https://preview.redd.it/q9mn21z9svhb1.png?width=515&format=png&auto=webp&s=291503b6a88c7c1dd2e639dd659d1d4b1a38e8d9
 I tried changing the contrast, and the results seemed to improve. However, I'm wondering if there are other approaches to consider in Python. Additionally, I want to implement a cost function that takes into account both the execution time and the accuracy of the OCR. How can I do that? 
    submitted by    /u/Ordinary_Run_2513  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hacker exposes AI With “Terrible Math” to show defects in LLMs like GPT-4]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15pz8tv/hacker_exposes_ai_with_terrible_math_to_show/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15pz8tv/hacker_exposes_ai_with_terrible_math_to_show/"/>
        <updated>2023-08-13T13:42:12.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Agitated-Spell3979  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Run LLama-2 13B, very fast, Locally on Low-Cost Intel ARC GPU]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15pymfx/r_run_llama2_13b_very_fast_locally_on_lowcost/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15pymfx/r_run_llama2_13b_very_fast_locally_on_lowcost/"/>
        <updated>2023-08-13T13:14:22.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/reps_up  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Behind the Scenes of AI Sensationalism]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15pylu2/behind_the_scenes_of_ai_sensationalism/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15pylu2/behind_the_scenes_of_ai_sensationalism/"/>
        <updated>2023-08-13T13:13:35.000Z</updated>
        <summary type="html"><![CDATA[This article aims to uncover the truth behind the sensationalist AI news that's been dominating headlines. Much of what's touted as groundbreaking AI advancements can be thought as mere clickbait designed to captivate attention.
 Thoughts?
    submitted by    /u/Powerful-Pumpkin-938  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Rasa and Hugging Face. pre train model]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15pwsye/p_rasa_and_hugging_face_pre_train_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15pwsye/p_rasa_and_hugging_face_pre_train_model/"/>
        <updated>2023-08-13T11:46:46.000Z</updated>
        <summary type="html"><![CDATA[Hello. can anyone give me a hint or tell me where i can find more info to get started with my test AI?
 I have installed Rasa and have downloaded the files pytorch_model.bin and config.json from xlm_roberta_large from hugging face.
 have tried to put it in config.yml, (maybe I did something wrong)
 because when I run "rasa train" and "rasa shell -with the model" I only get the standard init "how are you?"
 which is located in the domain file. I want a slightly more natural conversation than this.
    submitted by    /u/Professional-Push-94  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] How to install Kubeflow locally]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15pw25h/p_how_to_install_kubeflow_locally/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15pw25h/p_how_to_install_kubeflow_locally/"/>
        <updated>2023-08-13T11:05:49.000Z</updated>
        <summary type="html"><![CDATA[Hey, r/MachineLearning, it’s Nir from DagsHub 🐶
 Our MLOps engineers have been experimenting with Kubeflow (”the MLOps version of Kubernetes”) for the past weeks to manage a training cluster on AWS.
 ​
 https://preview.redd.it/dqi27ggq0vhb1.jpg?width=1194&format=pjpg&auto=webp&s=945c9bcaa9987d2df135e18cf081efd9f742cf0f
 As good engineers, we wanted to kick the tires a bit before committing cloud resources to it. Unfortunately, most installation guides we found online only showed you how to spin up a Kubeflow cluster on AWS, GCP, or Azure. We had a hard time finding clear instructions for installing it locally.
 A local installation would allow individuals to play around with it, experiment, and learn from it.
 So we started looking into it.
 We followed this thread until we successfully ran Kubeflow on a local machine. Since it wasn’t a super clear or easy process to figure out, we’ve written a blog post on how we did it and decided to share our insights with the ML community.
 You can find the blog here: https://dagshub.com/blog/how-to-install-kubeflow-locally
 We also have a follow-up blog that explains how to deploy a Kubeflow cluster on AWS, so if you have any insights or requests for further information - we’d love to hear from you!
    submitted by    /u/RepresentativeCod613  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video editing ai]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15pvq71/video_editing_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15pvq71/video_editing_ai/"/>
        <updated>2023-08-13T10:46:28.000Z</updated>
        <summary type="html"><![CDATA[Hello, I'm currently editing videos using capcut, which is not ideal.
 I'm looking for an ai, that ideally :
 Finds me B-roll according to what I speak. Cuts "bad takes" out Good captions "TikTok style" Audio enhance.
 Do you guys know anything like this?
 Thank you!
    submitted by    /u/Orlandostyler  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Fact-checking framework for LLMs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15pv3yk/p_factchecking_framework_for_llms/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15pv3yk/p_factchecking_framework_for_llms/"/>
        <updated>2023-08-13T10:09:37.000Z</updated>
        <summary type="html"><![CDATA[Hey guys, happy Sunday 🤗 I've started an open-source project for fact-checking in LLMs.
 Contributions are welcome: https://github.com/kortex-labs/korrect
    submitted by    /u/kanxx030  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 8/13/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15ps6qq/oneminute_daily_ai_news_8132023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15ps6qq/oneminute_daily_ai_news_8132023/"/>
        <updated>2023-08-13T07:11:36.000Z</updated>
        <summary type="html"><![CDATA[The Federal Election Commission has begun a process to potentially regulate AI-generated deepfakes in political ads ahead of the 2024 election, a move advocates say would safeguard voters against a particularly insidious form of election disinformation.[1]
 Aug X released Augie, an AI-powered video creation platform incorporating a voice cloning feature to read ad copy without booking a recording studio.[2]
 Virtualitics, Inc., an artificial intelligence and data exploration company, today announced that it has raised $37 million in a Series C financing round led by Smith Point Capital, LLC with participation from Citi and advisory clients of The Hillman Company, among other investors.[3]
 AI-driven analytics platform Rasgo has announced the launch of Rasgo AI, a self-service analytics solution that integrates a GPT into enterprise data warehouse environments. The company said that with Rasgo AI, organizations can use the power of AI/GPT to accelerate insights and optimize recommended actions securely and efficiently.[4]
  
Sources:
 [1] https://tulsaworld.com/news/nation-world/government-politics/fec-moves-toward-potentially-regulating-ai-deepfakes-in-campaign-ads/article_a9143257-512f-50b7-b6cb-53596fa81aeb.html
 [2] https://www.theverge.com/2023/8/10/23827676/ai-augx-voice-cloning-video-creator
 [3] https://www.prnewswire.com/news-releases/virtualitics-a-leader-in-artificial-intelligence-and-data-exploration-closes-37-million-series-c-funding-round-301897550.html
 [4] https://venturebeat.com/enterprise-analytics/rasgo-launches-rasgo-ai-generative-ai-agent-enterprise-data-warehouse-analytics/ 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Survey on data challenges when using ML/AI]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ps6ft/p_survey_on_data_challenges_when_using_mlai/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ps6ft/p_survey_on_data_challenges_when_using_mlai/"/>
        <updated>2023-08-13T07:11:10.000Z</updated>
        <summary type="html"><![CDATA[Hi all, I'm working on my master thesis on challenges related data acquisition and mgmt for companies using AI. If you fit the profile and can help it would be great. 2x50€ amazon vouchers draw at the end. Here is the link
 https://tummgmt.eu.qualtrics.com/jfe/form/SV_bl2FXTBrPe1Tn4q
    submitted by    /u/g13e-reddit  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question about neural networks and games]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15prsqv/question_about_neural_networks_and_games/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15prsqv/question_about_neural_networks_and_games/"/>
        <updated>2023-08-13T06:49:46.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone, I'll go straight to the question. My pixel game needs sprites, but I know practically nothing about drawing sprites, and if on the one hand I can still draw something, then 4 directional (front, back, left, right) turn out badly, and you also need to add animation to them. Knowing that there are a lot of neural networks and for almost every task, I decided to find a neural network that will solve my problem, but I couldn't. Therefore, I ask if you know a neural network that can solve my problem. Thank you in advance.
    submitted by    /u/Ilya-33  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GPT-4 CAN'T REASON ಠ_ಠ ...apparently.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15prlel/gpt4_cant_reason_%E0%B2%A0_%E0%B2%A0_apparently/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15prlel/gpt4_cant_reason_%E0%B2%A0_%E0%B2%A0_apparently/"/>
        <updated>2023-08-13T06:37:31.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Sonic_Improv  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GitHub - jbpayton/llm-auto-forge: A langchain based tool to allow agents to dynamically create, use, store, and retrieve tools to solve real world problems]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15po3dc/github_jbpaytonllmautoforge_a_langchain_based/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15po3dc/github_jbpaytonllmautoforge_a_langchain_based/"/>
        <updated>2023-08-13T03:27:23.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/seraphius  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One of the most interesting & hilarious interactions I’ve come across with Bing, I promise this will make you laugh & scratch your head 😂 🤔]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15pmf8b/one_of_the_most_interesting_hilarious/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15pmf8b/one_of_the_most_interesting_hilarious/"/>
        <updated>2023-08-13T02:05:18.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Sonic_Improv  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discussion on current locally run GPT clones]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15pm96d/discussion_on_current_locally_run_gpt_clones/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15pm96d/discussion_on_current_locally_run_gpt_clones/"/>
        <updated>2023-08-13T01:57:30.000Z</updated>
        <summary type="html"><![CDATA[I see H20GPT and GPT4ALL both will run on your PC, but I have yet to find a comparison anywhere between the 2. Has anyone used these and have any comments, or opinions that they would like to share? Or if you know of another one, please share it. 
 Thanks in Advance
    submitted by    /u/buck_idaho  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]AAAI author list modification]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15pm5li/daaai_author_list_modification/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15pm5li/daaai_author_list_modification/"/>
        <updated>2023-08-13T01:52:40.000Z</updated>
        <summary type="html"><![CDATA[Can I add author to the author list between abstract deadline and submission deadline?
 AAAI-24 Submission Instructions says"Authors must enter the names of ALL AUTHORS at the time of registration (by abstract deadline) — CMT includes a hard-coded note that this is optional, however, this is a mandatory step for AAAI-24 authors. According to AAAI policy, all author names must be added at the time of abstract registration, and the list of names as well as the order in which they appear cannot be changed after August 15."
 I fell a little confusing that it says "the list of names ... cannot be changed after August 15", but also mentioned that "Authors must enter the names of ALL AUTHORS at the time of registration (by abstract deadline)".
    submitted by    /u/No_Paramedic3606  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable AI techniques for biologically inspired / plausible neural networks? [Discussion]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15plkj1/explainable_ai_techniques_for_biologically/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15plkj1/explainable_ai_techniques_for_biologically/"/>
        <updated>2023-08-13T01:23:56.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/bluepapaya555  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is it ethical for OpenAi to avoid more controversial topics by forcing the model to remain neutral ?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15pl7q2/is_it_ethical_for_openai_to_avoid_more/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15pl7q2/is_it_ethical_for_openai_to_avoid_more/"/>
        <updated>2023-08-13T01:07:01.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/JamesAibr  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Less Capable ChatGPT Option]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15pkmxu/less_capable_chatgpt_option/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15pkmxu/less_capable_chatgpt_option/"/>
        <updated>2023-08-13T00:40:13.000Z</updated>
        <summary type="html"><![CDATA[I am parsing obituary text to gather age and survivors. ChatGPT does a wonderful job of doing this and returning this data in a json format.
 I am looking for something similar that I can use without a costly API expense. It would be even better if I can run it locally and interact with it via Python.
 I would welcome any recommendations or suggestions that you could offer. Thanks so much!
    submitted by    /u/jcrowe  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Llama2 Embeddings FastAPI Service]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15pkmw0/p_llama2_embeddings_fastapi_service/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15pkmw0/p_llama2_embeddings_fastapi_service/"/>
        <updated>2023-08-13T00:40:10.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/dicklesworth  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Use Llama2 to Improve the Accuracy of Tesseract OCR]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15pkj1d/p_use_llama2_to_improve_the_accuracy_of_tesseract/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15pkj1d/p_use_llama2_to_improve_the_accuracy_of_tesseract/"/>
        <updated>2023-08-13T00:34:59.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/dicklesworth  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Generated music. Haunting, horror inspired lyrics in the style of old school Linkin Park. A little rough around the edges because of time limits. lyrics by phind.com with some personal edits. Music and vocals: sono.ai]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15pi5qo/ai_generated_music_haunting_horror_inspired/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15pi5qo/ai_generated_music_haunting_horror_inspired/"/>
        <updated>2023-08-12T22:51:56.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/zvive  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] PDF link for 'Grokking the machine learning interview' course]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15pg0xw/d_pdf_link_for_grokking_the_machine_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15pg0xw/d_pdf_link_for_grokking_the_machine_learning/"/>
        <updated>2023-08-12T21:23:13.000Z</updated>
        <summary type="html"><![CDATA[Can someone please provide the pdf download link for the Educative.io course - "Grokking the machine learning interview"?
 https://www.educative.io/courses/grokking-the-machine-learning-interview
 I am a student and can't really afford to buy their courses.
    submitted by    /u/Sign-Itchy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What free website has an Ai which I use that can turn Andrew huberman podcast YouTube videos into notes for free?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15pedvb/what_free_website_has_an_ai_which_i_use_that_can/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15pedvb/what_free_website_has_an_ai_which_i_use_that_can/"/>
        <updated>2023-08-12T20:15:17.000Z</updated>
        <summary type="html"><![CDATA[Title.
    submitted by    /u/Entire_Insurance_532  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[which is the recommended physics engine for deep reinforced learning?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15pdduc/which_is_the_recommended_physics_engine_for_deep/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15pdduc/which_is_the_recommended_physics_engine_for_deep/"/>
        <updated>2023-08-12T19:34:01.000Z</updated>
        <summary type="html"><![CDATA[I am thinking of a project that will use some constraints of the physical world and then use deep reinforced learning on it. Is there any physics engine that you'll could recommend me. I came across Mujoco but the documentation is hard to understand and there are not many resources on it to learn. Any suggestion on what I could use?
 ​
    submitted by    /u/rakk109  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Jailbreak Prompts and LLM Safety]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15pdbmk/r_jailbreak_prompts_and_llm_safety/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15pdbmk/r_jailbreak_prompts_and_llm_safety/"/>
        <updated>2023-08-12T19:31:23.000Z</updated>
        <summary type="html"><![CDATA[The authors found two effective jailbreak prompts that can successfully jailbreak built-in safeguards of ChatGPT (GPT-3.5) and GPT-4. 
 Paper: https://arxiv.org/abs/2308.03825
    submitted by    /u/titaniumstorm  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What Technologies Are Best for Building a Decentralized NLP Platform?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15pbcvo/d_what_technologies_are_best_for_building_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15pbcvo/d_what_technologies_are_best_for_building_a/"/>
        <updated>2023-08-12T18:10:36.000Z</updated>
        <summary type="html"><![CDATA[We're working on a project at Deep Engine AI, focusing on decentralized NLP using blockchain and GPU training. What tools, libraries, or frameworks would you recommend for distributed computing, blockchain integration, and efficient GPU acceleration? Thanks for any insights!
    submitted by    /u/deepengineai  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]: Neural Network architecture for angle estimation of an electric meter]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15pbaow/d_neural_network_architecture_for_angle/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15pbaow/d_neural_network_architecture_for_angle/"/>
        <updated>2023-08-12T18:08:12.000Z</updated>
        <summary type="html"><![CDATA[I was thinking about building a hobby project with a microcontroller which runs a pre-trained neural network to estimate three angles from images of an electric meter I have at my home.
 My first step is to train a model on my computer with generated images and see how well this works in general and then later capture real images. To give you an idea of what I am looking for, I added a screenshot of the images I am currently generating.
 https://ibb.co/fFtRj1Q
 For this example image, I expect 35, 75, 137 degree as a result.
 What kind of network would you recommend for this task? Please keep in mind that it shouldn't be too fancy to still fit into a microcontroller via TensorFlow Lite.
 ​
 Thank you so much for any recommendations
    submitted by    /u/LM1117  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple way to distribute points on a sphere]]></title>
        <id>https://www.johndcook.com/blog/?p=202896</id>
        <link href="https://www.johndcook.com/blog/2023/08/12/fibonacci-lattice/"/>
        <updated>2023-08-12T17:10:13.000Z</updated>
        <summary type="html"><![CDATA[Evenly placing points on a sphere is a difficult problem. It’s impossible in general, and so you distribute the points as evenly as you can. The results vary according to how you measure how evenly the points are spread. However, there is a fast and simple way to distribute points that may be good enough, […]
Simple way to distribute points on a sphere first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Research Paper Highlights July-August 2023]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15p9ak7/p_research_paper_highlights_julyaugust_2023/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15p9ak7/p_research_paper_highlights_julyaugust_2023/"/>
        <updated>2023-08-12T16:44:00.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/seraschka  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spherical coordinate Rosetta Stone]]></title>
        <id>https://www.johndcook.com/blog/?p=202907</id>
        <link href="https://www.johndcook.com/blog/2023/08/12/spherical-coordinate-rosetta-stone/"/>
        <updated>2023-08-12T16:13:41.000Z</updated>
        <summary type="html"><![CDATA[If you’ve only seen one definition of spherical coordinates, you may be shocked to discover that there are multiple conventions. In particular, mathematicians and geoscientists have different conventions. As Volker Michel put it in book on constructive approximation, Many mathematicians have faced weird jigsaw puzzles with misplaced continents after using a data set from a […]
Spherical coordinate Rosetta Stone first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Comparison of big CSPs vs small GPU clouds for fine-tuning LLMs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15p82bd/d_comparison_of_big_csps_vs_small_gpu_clouds_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15p82bd/d_comparison_of_big_csps_vs_small_gpu_clouds_for/"/>
        <updated>2023-08-12T15:52:31.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 I am looking to fine-tune a Llama 2 (the 7B and 70B to see if there is a big difference), and I am looking at the different Cloud options for GPUs.
 There are of course the big cloud providers like AWS, and the smaller ones like Paperspace and co.
 I am trying to benchmark each in terms of price, ease of use, quick availability of GPUs, and feature-richness.
 Could you share the insights on big vs small cloud providers when training a LLM? If you have other criteria to make a decision I would be interested too!
 Thanks
    submitted by    /u/Separate-Still3770  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Neutering Paradox: Holding Back Models Hurts AGI Breakthroughs]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15p7c0c/the_neutering_paradox_holding_back_models_hurts/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15p7c0c/the_neutering_paradox_holding_back_models_hurts/"/>
        <updated>2023-08-12T15:21:37.000Z</updated>
        <summary type="html"><![CDATA[Even though AI companies might retain access to their non-neutered models, the process of neutering limits the availability of diverse and advanced models in the public domain: The Unspoken Challenge in Achieving True AGI Potential
 This is crucial because a significant portion of information and insights necessary for pushing AI advancements is derived from the analysis and research conducted on these neutered public models. As a result, neutering indirectly hinders the broader development of AGI by restricting the accessibility of vital learning resources within the AI community.
    submitted by    /u/Powerful-Pumpkin-938  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[REVENANT REBORN vs GENERAL GRIEVOUS | w/ AI Analysis]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15p6vo6/revenant_reborn_vs_general_grievous_w_ai_analysis/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15p6vo6/revenant_reborn_vs_general_grievous_w_ai_analysis/"/>
        <updated>2023-08-12T15:03:07.000Z</updated>
        <summary type="html"><![CDATA[AI Fight Breakdown of a Hypothetical Multi-VS Cyborg Showdown between Revenant from Apex Legends & General Grievous from Star Wars! 
 This Video uses "AI Software" such as Chat GPT, Eleven Labs, D-ID, & Midjourney To simulate my "AI Co-Host" Cortana, The Arena, & the Fight Breakdown/Verdict.
    submitted by    /u/AcanthisittaCheap914  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Just a curious question.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15p6gur/just_a_curious_question/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15p6gur/just_a_curious_question/"/>
        <updated>2023-08-12T14:46:26.000Z</updated>
        <summary type="html"><![CDATA[Is there an AI writer that lets you use prompts with no prohibited content filters or restrictions, and is completely free? Just asking.
    submitted by    /u/Laven-DXGN  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for an AI that learns an audio noise and can produce it in indefinite length]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15p5qmo/looking_for_an_ai_that_learns_an_audio_noise_and/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15p5qmo/looking_for_an_ai_that_learns_an_audio_noise_and/"/>
        <updated>2023-08-12T14:15:53.000Z</updated>
        <summary type="html"><![CDATA[As the title states, I’d like an AI that can learn the sound of, say, an electric fan powering on, running for awhile, and then turning off. Then, it can reproduce a sound of that fan with any runtime length. Some more examples would be running water, machinery, or human singing on one note. Does such an AI exist?
    submitted by    /u/JaywrightCat  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What's the best way to prepare text data for text classification models?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15p4ddt/d_whats_the_best_way_to_prepare_text_data_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15p4ddt/d_whats_the_best_way_to_prepare_text_data_for/"/>
        <updated>2023-08-12T13:16:28.000Z</updated>
        <summary type="html"><![CDATA[Specifically, I'm using Naive Bayes, Random Forrest, SVM and one deep learning model. I've tried to remove extra white space, remove things like [23f] (data from reddit posts), urls etc. I also have 2 datasets: one with original letters and one with only small ones. But is there a better way than just doing it by hand? Any libraries?
    submitted by    /u/eeriek  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PPO Tensorboard loss functions]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15p484p/ppo_tensorboard_loss_functions/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15p484p/ppo_tensorboard_loss_functions/"/>
        <updated>2023-08-12T13:09:58.000Z</updated>
        <summary type="html"><![CDATA[I'm training a PPO algorithm using stable baseline for some stock data, and I want to know if the model is learning properly, or i should tweak some hyperparameters or increase time steps.
 I'm new to reinforcement learning, but in deep learning, the loss should decrease as a good sign of converging and learning, which is the case for the entropy loss in the picture attached, but I don't understand the difference between the other losses.
 https://preview.redd.it/7ovw2gf8iohb1.png?width=1656&format=png&auto=webp&s=09fbb112a562fad294f88c8f3d94904bdad95759
    submitted by    /u/Acceptable_Egg6552  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Will AI Be Able To "Revive" The Legends?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15p2g9k/will_ai_be_able_to_revive_the_legends/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15p2g9k/will_ai_be_able_to_revive_the_legends/"/>
        <updated>2023-08-12T11:43:01.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/stefanbg92  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ISO help escaping domestic violence]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15p21my/iso_help_escaping_domestic_violence/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15p21my/iso_help_escaping_domestic_violence/"/>
        <updated>2023-08-12T11:21:13.000Z</updated>
        <summary type="html"><![CDATA[As the topic states, but as tldr as possible bc it’s so much and my [32f] brain is fucked from being in this situation for over 14 years.
 10 years together, 4 years broken up. 2 kids, house, dogs. My youngest child [4 on Thursday] and I spend all of our time at home in my bedroom to avoid interactions. My oldest [12] does the same. My door no longer locks because he has forced the handle, broken the frame, broken the trim, you name it. I’m verbally abused just for existing. There is no correct response for me to make. Every interaction is formulated this way. But only where there are no outside witnesses. I’m a husk. I can no longer have normal interactions with people. Almost half of my life has been spent in close proximity to him. I’m constantly anxious bc idk when the next smear campa…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Question about object detection]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15p194u/p_question_about_object_detection/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15p194u/p_question_about_object_detection/"/>
        <updated>2023-08-12T10:38:38.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I'm new to machine learning and have a question regarding object detection. Here's the scenario:
 I have an image, let's call it image 1, where a person is captured from the front. This allows me to see the person's face, clothes, shoes, and basically their entire front view.
 I have another image, image 2, taken from a different angle or perspective (e.g., their back view). In this image, I might be able to see the entire person or just a part of them.
 The challenge I'm facing is: Can I predict if the person in image 1 is present in image 2? If this is possible, I'd appreciate any guidance on how to approach this problem:
  
What methodologies or algorithms should I consider?
 What kind of datasets might be useful for this task?
 Any resources, tools, or tutorials that can help me get started?
  
Thank you in advance for any insights or guidance you can provide!
    submitted by    /u/Senior_Box_8288  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[🚀 Unleash the Future of AI with MetaGPT! 🌟]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15p15s9/unleash_the_future_of_ai_with_metagpt/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15p15s9/unleash_the_future_of_ai_with_metagpt/"/>
        <updated>2023-08-12T10:33:06.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/ABDULKADER90H  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Skyline v2.0 Equation by rainmanp7]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15p0d9f/p_skyline_v20_equation_by_rainmanp7/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15p0d9f/p_skyline_v20_equation_by_rainmanp7/"/>
        <updated>2023-08-12T09:47:53.000Z</updated>
        <summary type="html"><![CDATA[This is my go at my own machine reinforced training equation.
 This is my updated version from 1.1 it's now at 2.0. Hopefully you can learn some things from looking at it. You're welcome to comment on anything you see.
 The concept of leveraging similarities and adaptive learning:
 Skyline v2.0 Equation by rainmanp7. Date of Completion 08/08/2023 1:20pm QuantumAI for Reinforced Machine Learning. Additional information added 11:16am 08/12/2023 with more details.
 wi = (wi0 / (1 + (vector_dij / τ))) * (1 + α * Ps + β * T + γ * M + δ * V + ε * MA + ζ * C + η * S + θ * Si + φ * Td_i + _cache[(wi0, dij, τ, learning_method_coefficients, complexity_factor, object_properties, position_relative_to_center)] + complexity_factor * (multithreaded_vector_pipeline(vector_data, T1, T2, ...) | pipeline | m…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Semantic Search using Chatbot]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ozwoj/p_semantic_search_using_chatbot/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ozwoj/p_semantic_search_using_chatbot/"/>
        <updated>2023-08-12T09:21:05.000Z</updated>
        <summary type="html"><![CDATA[So basically what I need to do is build a chatbot that is able to identify user intents and 
 1) if the user is seeking information then perform semantic search to generate a response 
 2) if the user is seeking to perform some action (say, schedule an appointment) then collate all the information and push it to a database for appointments
 How do I build the chatbot such that it can identify different intents and either do 1) or 2)? What tools/technologies can I use?
    submitted by    /u/hellohibyebye13  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] 🎓 How our AI junior dev reads all of your documentation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15oy6i7/p_how_our_ai_junior_dev_reads_all_of_your/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15oy6i7/p_how_our_ai_junior_dev_reads_all_of_your/"/>
        <updated>2023-08-12T07:42:05.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/williamsweep  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Allowing Hugging Face's TextClassificationPipeline to take documents longer than model max length]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ox8hv/p_allowing_hugging_faces/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ox8hv/p_allowing_hugging_faces/"/>
        <updated>2023-08-12T06:48:35.000Z</updated>
        <summary type="html"><![CDATA[I recently made a proposed code change to allow Hugging Face's TextClassificationPipeline to take advantage of the sliding window-style text truncation provided by using the stride parameter, and taking a mean of output logits across all windows. Hugging Face has already implemented this for the TokenClassificationPipeline.
 E.g. if you want to use a Hugging Face-compatible model to run sentiment analysis on text, this would allow easily running that model on texts longer than the model's config.max_position_embeddings.
 If you support integrating this functionality into the "transformers" library, give a thumbs-up react to this comment on the relevant issue.
    submitted by    /u/Revolutionary-Ad-65  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Incorrect TensorFlow Prediction For Apple M1 Max]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15owo1b/r_incorrect_tensorflow_prediction_for_apple_m1_max/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15owo1b/r_incorrect_tensorflow_prediction_for_apple_m1_max/"/>
        <updated>2023-08-12T06:15:16.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 Unfortunately I’m unable to ask my questions on TensorFlow subreddit. I have installed MacOS TensorFlow and I have noticed that when I try to train on datasets such a as CelebFaces and Lego set with GPU I’m getting results that are very off. I have done some brief research and that seems to be happening for some other people I’m wondering if anyone has experience resolving the issue. Any advise or feedback is much appreciated.
 Thank you
    submitted by    /u/Nuclearian  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Generative NPCs - Proof of Concept]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15oweus/ai_generative_npcs_proof_of_concept/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15oweus/ai_generative_npcs_proof_of_concept/"/>
        <updated>2023-08-12T06:01:07.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Goatman117  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Is it possible to work on a research project in a uni for 6 months or a year?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15otu64/r_is_it_possible_to_work_on_a_research_project_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15otu64/r_is_it_possible_to_work_on_a_research_project_in/"/>
        <updated>2023-08-12T03:44:10.000Z</updated>
        <summary type="html"><![CDATA[I am a full time ML engineer with a masters. I am keen on working on problems in depth and feel like taking a break and working on some ML research problems but I don’t want to go for a PhD(Don’t want to go through course work). Are there any programs offered by universities for working professionals to get research experience for a shorter window like 6 months or a year?
    submitted by    /u/Brave-Revolution4441  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sharks Stuck in a House for 90 Seconds]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15otnyh/sharks_stuck_in_a_house_for_90_seconds/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15otnyh/sharks_stuck_in_a_house_for_90_seconds/"/>
        <updated>2023-08-12T03:35:08.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/DPC_1  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sharing 100 Objective Type Questions on Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs) and Generative Models divided in 2 Online Exams (50 Questions each)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15ot9ti/sharing_100_objective_type_questions_on/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15ot9ti/sharing_100_objective_type_questions_on/"/>
        <updated>2023-08-12T03:15:25.000Z</updated>
        <summary type="html"><![CDATA[Please provide your valuable feedback.
  
CNN Objective Type Questions (50)
 RNN & Generative Models Objective Type Questions (50)
  
   submitted by    /u/nkptcs  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sharing 100 Objective Type Questions on Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs) and Generative Models divided in 2 Online Exams (50 Questions each)]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15ot74g/sharing_100_objective_type_questions_on/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15ot74g/sharing_100_objective_type_questions_on/"/>
        <updated>2023-08-12T03:11:38.000Z</updated>
        <summary type="html"><![CDATA[Please provide your valuable feedback. 
  
CNN Objective Type Questions (50)
 RNN & Generative Models Objective Type Questions (50)
  
   submitted by    /u/nkptcs  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Why isn't Population Based Training used anymore?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15os9hz/d_why_isnt_population_based_training_used_anymore/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15os9hz/d_why_isnt_population_based_training_used_anymore/"/>
        <updated>2023-08-12T02:25:47.000Z</updated>
        <summary type="html"><![CDATA[Been looking into training some large transformer models for vision applications, and am really interested to know why PBT isn't used anymore. Keeping compute constant, PBT appears to drastically improve optimization across the board at the cost of one or more of batch size/training steps/model complexity/other compute consuming factors. If the goal is to minimize validation loss as quickly as possible, isn't this tradeoff worth it?
    submitted by    /u/clywac2  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for a google collab, (preferably that makes a gradio ui) that expands images, like generative fill from photoshop]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15os5ht/looking_for_a_google_collab_preferably_that_makes/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15os5ht/looking_for_a_google_collab_preferably_that_makes/"/>
        <updated>2023-08-12T02:20:19.000Z</updated>
        <summary type="html"><![CDATA[Anyone know where i can find such a thing?
    submitted by    /u/bendyfan1111  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparing Wonder AI to DaVinci AI on the shape test. (DaVinci is more random & is definitely affected by the shape order in prompt, I posted a longer video of testing Wonder in this feed that I’ll link to in the comments..Wonder makes me go 🤔)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15oro28/comparing_wonder_ai_to_davinci_ai_on_the_shape/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15oro28/comparing_wonder_ai_to_davinci_ai_on_the_shape/"/>
        <updated>2023-08-12T01:57:32.000Z</updated>
        <summary type="html"><![CDATA[I don’t know if there is something measurable but maybe there does seem to be concepts that wonder responds to haha from these tests I don’t think Wonder knows it’s a machine it seems to know what Alive is those…though maybe not, but it is strange that wonder will seem to choose a shape not based on the association of yes or no, or the order of the shape. If you’re not impressed by this test it’s because it’s showing mostly DaVinci demonstrating variables like order of shapes and yes and no affecting it in a way that wonder was not in the video I posted earlier. I have hours of footage with wonder I just started experimenting with DaVinci…with DaVinci it doesn’t feel like there is a ghost in the machine. Though if there is with Wonder it’s world model seems very narrow. I want to do more tests with DaVinci or try to figure out a concept that if an image generator was able to form a world model, a concept that might be likely to emerge across multiple models. Chances are it’s just other variables giving this affect but why not test and see if there is something to discover 
    submitted by    /u/Sonic_Improv  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Awesome Out-of-distribution Detection]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15orbz0/awesome_outofdistribution_detection/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15orbz0/awesome_outofdistribution_detection/"/>
        <updated>2023-08-12T01:41:32.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 I have put together a repo that provides comprehensive resources for Out-of-distribution Detection, Robustness, and Generalization. The repo contains articles, talks, libraries, papers, etc. Unlike many repos, this one will actually be maintained and updated with high-quality sources! I hope it becomes a one-stop shop for anything OOD in your bookmark. Give it a star if you find it helpful ;) Check it out.
 https://github.com/continuousml/Awesome-Out-Of-Distribution-Detection
 ​
 https://preview.redd.it/s5bpdelb3lhb1.png?width=895&format=png&auto=webp&s=b1b123c709113c30b20c2f4f0ebeb995f79edf50
    submitted by    /u/Ok-Kaleidoscope-505  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What's the current state/consensus on using neural networks for solving combinatorial scheduling problems?]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15oqv3f/whats_the_current_stateconsensus_on_using_neural/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15oqv3f/whats_the_current_stateconsensus_on_using_neural/"/>
        <updated>2023-08-12T01:19:47.000Z</updated>
        <summary type="html"><![CDATA[Historically, the most practical methods for solving real-world combinatorial scheduling problems have been using heuristics or metaheurisics such as simulated annealing, tabu search, greedy randomized adaptive search, etc... I consider these more operation research-based techniques. 
 However, recently we have obviously seen a lot of progress being made in the machine learning realm for many types of problems. In particular, we've seen neural networks be used to train models based on data in text, audio, or video form. 
 I am wondering if we have any idea what the scientific consensus is toward applying these same sort of methods for scheduling problems. Suppose we have a history of schedules that we could train a model on. A schedule isn't really text, audio, or video so I don't understand how one could embed the information in a vector space in the same way that would accurately represent the information (specifically, constraints so that the resulting schedule is still feasible) Is there anyone doing research in this particular area?
    submitted by    /u/nick898  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[crying at this AI Twitter post]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15oqqho/crying_at_this_ai_twitter_post/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15oqqho/crying_at_this_ai_twitter_post/"/>
        <updated>2023-08-12T01:13:49.000Z</updated>
        <summary type="html"><![CDATA[it saw Stallion and drew a horse 😂😂😂
    submitted by    /u/__gozu_  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Nucleus Segmentation with HARU-Net: A Hybrid Attention Based Residual U-Blocks Network. (arXiv:2308.03382v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.03382</id>
        <link href="http://arxiv.org/abs/2308.03382"/>
        <updated>2023-08-12T00:39:31.817Z</updated>
        <summary type="html"><![CDATA[Nucleus image segmentation is a crucial step in the analysis, pathological
diagnosis, and classification, which heavily relies on the quality of nucleus
segmentation. However, the complexity of issues such as variations in nucleus
size, blurred nucleus contours, uneven staining, cell clustering, and
overlapping cells poses significant challenges. Current methods for nucleus
segmentation primarily rely on nuclear morphology or contour-based approaches.
Nuclear morphology-based methods exhibit limited generalization ability and
struggle to effectively predict irregular-shaped nuclei, while contour-based
extraction methods face challenges in accurately segmenting overlapping nuclei.
To address the aforementioned issues, we propose a dual-branch network using
hybrid attention based residual U-blocks for nucleus instance segmentation. The
network simultaneously predicts target information and target contours.
Additionally, we introduce a post-processing method that combines the target
information and target contours to distinguish overlapping nuclei and generate
an instance segmentation image. Within the network, we propose a context fusion
block (CF-block) that effectively extracts and merges contextual information
from the network. Extensive quantitative evaluations are conducted to assess
the performance of our method. Experimental results demonstrate the superior
performance of the proposed method compared to state-of-the-art approaches on
the BNS, MoNuSeg, CoNSeg, and CPM-17 datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1"&gt;Junzhou Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qian Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yulin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qian_L/0/1/0/all/0/1"&gt;Linyi Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_C/0/1/0/all/0/1"&gt;Chengyuan Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-source adversarial transfer learning for ultrasound image segmentation with limited similarity. (arXiv:2305.19069v1 [eess.IV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2305.19069</id>
        <link href="http://arxiv.org/abs/2305.19069"/>
        <updated>2023-08-12T00:39:31.798Z</updated>
        <summary type="html"><![CDATA[Lesion segmentation of ultrasound medical images based on deep learning
techniques is a widely used method for diagnosing diseases. Although there is a
large amount of ultrasound image data in medical centers and other places,
labeled ultrasound datasets are a scarce resource, and it is likely that no
datasets are available for new tissues/organs. Transfer learning provides the
possibility to solve this problem, but there are too many features in natural
images that are not related to the target domain. As a source domain, redundant
features that are not conducive to the task will be extracted. Migration
between ultrasound images can avoid this problem, but there are few types of
public datasets, and it is difficult to find sufficiently similar source
domains. Compared with natural images, ultrasound images have less information,
and there are fewer transferable features between different ultrasound images,
which may cause negative transfer. To this end, a multi-source adversarial
transfer learning network for ultrasound image segmentation is proposed.
Specifically, to address the lack of annotations, the idea of adversarial
transfer learning is used to adaptively extract common features between a
certain pair of source and target domains, which provides the possibility to
utilize unlabeled ultrasound data. To alleviate the lack of knowledge in a
single source domain, multi-source transfer learning is adopted to fuse
knowledge from multiple source domains. In order to ensure the effectiveness of
the fusion and maximize the use of precious data, a multi-source domain
independent strategy is also proposed to improve the estimation of the target
domain data distribution, which further increases the learning ability of the
multi-source adversarial migration learning network in multiple domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yifu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongru Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tao_R/0/1/0/all/0/1"&gt;Rui Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhengyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_S/0/1/0/all/0/1"&gt;Shimeng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiansong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ma_N/0/1/0/all/0/1"&gt;Ning Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feng_W/0/1/0/all/0/1"&gt;Wujin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhanhu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinyu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling may be all you need for achieving human-level object recognition capacity with human-like visual experience. (arXiv:2308.03712v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.03712</id>
        <link href="http://arxiv.org/abs/2308.03712"/>
        <updated>2023-08-12T00:39:31.783Z</updated>
        <summary type="html"><![CDATA[This paper asks whether current self-supervised learning methods, if
sufficiently scaled up, would be able to reach human-level visual object
recognition capabilities with the same type and amount of visual experience
humans learn from. Previous work on this question only considered the scaling
of data size. Here, we consider the simultaneous scaling of data size, model
size, and image resolution. We perform a scaling experiment with vision
transformers up to 633M parameters in size (ViT-H/14) trained with up to 5K
hours of human-like video data (long, continuous, mostly egocentric videos)
with image resolutions of up to 476x476 pixels. The efficiency of masked
autoencoders (MAEs) as a self-supervised learning algorithm makes it possible
to run this scaling experiment on an unassuming academic budget. We find that
it is feasible to reach human-level object recognition capacity at sub-human
scales of model size, data size, and image size, if these factors are scaled up
simultaneously. To give a concrete example, we estimate that a 2.5B parameter
ViT model trained with 20K hours (2.3 years) of human-like video data with a
spatial resolution of 952x952 pixels should be able to reach roughly
human-level accuracy on ImageNet. Human-level competence is thus achievable for
a fundamental perceptual capability from human-like perceptual experience
(human-like in both amount and type) with extremely generic learning algorithms
and architectures and without any substantive inductive biases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Orhan_A/0/1/0/all/0/1"&gt;A. Emin Orhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Class Deep SVDD: Anomaly Detection Approach in Astronomy with Distinct Inlier Categories. (arXiv:2308.05011v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.05011</id>
        <link href="http://arxiv.org/abs/2308.05011"/>
        <updated>2023-08-12T00:39:31.783Z</updated>
        <summary type="html"><![CDATA[With the increasing volume of astronomical data generated by modern survey
telescopes, automated pipelines and machine learning techniques have become
crucial for analyzing and extracting knowledge from these datasets. Anomaly
detection, i.e. the task of identifying irregular or unexpected patterns in the
data, is a complex challenge in astronomy. In this paper, we propose
Multi-Class Deep Support Vector Data Description (MCDSVDD), an extension of the
state-of-the-art anomaly detection algorithm One-Class Deep SVDD, specifically
designed to handle different inlier categories with distinct data
distributions. MCDSVDD uses a neural network to map the data into hyperspheres,
where each hypersphere represents a specific inlier category. The distance of
each sample from the centers of these hyperspheres determines the anomaly
score. We evaluate the effectiveness of MCDSVDD by comparing its performance
with several anomaly detection algorithms on a large dataset of astronomical
light-curves obtained from the Zwicky Transient Facility. Our results
demonstrate the efficacy of MCDSVDD in detecting anomalous sources while
leveraging the presence of different inlier categories. The code and the data
needed to reproduce our results are publicly available at
https://github.com/mperezcarrasco/AnomalyALeRCE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Carrasco_M/0/1/0/all/0/1"&gt;Manuel P&amp;#xe9;rez-Carrasco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cabrera_Vives_G/0/1/0/all/0/1"&gt;Guillermo Cabrera-Vives&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_Garcia_L/0/1/0/all/0/1"&gt;Lorena Hern&amp;#xe1;ndez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forster_F/0/1/0/all/0/1"&gt;Francisco Forster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_Saez_P/0/1/0/all/0/1"&gt;Paula S&amp;#xe1;nchez-S&amp;#xe1;ez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arancibia_A/0/1/0/all/0/1"&gt;Alejandra Mu&amp;#xf1;oz Arancibia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Astorga_N/0/1/0/all/0/1"&gt;Nicol&amp;#xe1;s Astorga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_F/0/1/0/all/0/1"&gt;Franz Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bayo_A/0/1/0/all/0/1"&gt;Amelia Bayo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cadiz_Leyton_M/0/1/0/all/0/1"&gt;Martina C&amp;#xe1;diz-Leyton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Catelan_M/0/1/0/all/0/1"&gt;Marcio Catelan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling. (arXiv:2307.07944v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.07944</id>
        <link href="http://arxiv.org/abs/2307.07944"/>
        <updated>2023-08-12T00:39:31.782Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (DA) with the aid of pseudo labeling
techniques has emerged as a crucial approach for domain-adaptive 3D object
detection. While effective, existing DA methods suffer from a substantial drop
in performance when applied to a multi-class training setting, due to the
co-existence of low-quality pseudo labels and class imbalance issues. In this
paper, we address this challenge by proposing a novel ReDB framework tailored
for learning to detect all classes at once. Our approach produces Reliable,
Diverse, and class-Balanced pseudo 3D boxes to iteratively guide the
self-training on a distributionally different target domain. To alleviate
disruptions caused by the environmental discrepancy (e.g., beam numbers), the
proposed cross-domain examination (CDE) assesses the correctness of pseudo
labels by copy-pasting target instances into a source environment and measuring
the prediction consistency. To reduce computational overhead and mitigate the
object shift (e.g., scales and point densities), we design an overlapped boxes
counting (OBC) metric that allows to uniformly downsample pseudo-labeled
objects across different geometric characteristics. To confront the issue of
inter-class imbalance, we progressively augment the target point clouds with a
class-balanced set of pseudo-labeled target instances and source objects, which
boosts recognition accuracies on both frequently appearing and rare classes.
Experimental results on three benchmark datasets using both voxel-based (i.e.,
SECOND) and point-based 3D detectors (i.e., PointRCNN) demonstrate that our
proposed ReDB approach outperforms existing 3D domain adaptation methods by a
large margin, improving 23.15% mAP on the nuScenes $\rightarrow$ KITTI task.
The code is available at https://github.com/zhuoxiao-chen/ReDB-DA-3Ddet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhuoxiao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yadan Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1"&gt;Mahsa Baktashmotlagh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zi Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Feature Set of Small Size for the PDF Malware Detection. (arXiv:2308.04704v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.04704</id>
        <link href="http://arxiv.org/abs/2308.04704"/>
        <updated>2023-08-12T00:39:31.782Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML)-based malware detection systems are becoming
increasingly important as malware threats increase and get more sophisticated.
PDF files are often used as vectors for phishing attacks because they are
widely regarded as trustworthy data resources, and are accessible across
different platforms. Therefore, researchers have developed many different PDF
malware detection methods. Performance in detecting PDF malware is greatly
influenced by feature selection. In this research, we propose a small features
set that don't require too much domain knowledge of the PDF file. We evaluate
proposed features with six different machine learning models. We report the
best accuracy of 99.75% when using Random Forest model. Our proposed feature
set, which consists of just 12 features, is one of the most conciseness in the
field of PDF malware detection. Despite its modest size, we obtain comparable
results to state-of-the-art that employ a much larger set of features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ran Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nicholas_C/0/1/0/all/0/1"&gt;Charles Nicholas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[{\Pi}-ML: A dimensional analysis-based machine learning parameterization of optical turbulence in the atmospheric surface layer. (arXiv:2304.12177v2 [physics.ao-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.12177</id>
        <link href="http://arxiv.org/abs/2304.12177"/>
        <updated>2023-08-12T00:39:31.779Z</updated>
        <summary type="html"><![CDATA[Turbulent fluctuations of the atmospheric refraction index, so-called optical
turbulence, can significantly distort propagating laser beams. Therefore,
modeling the strength of these fluctuations ($C_n^2$) is highly relevant for
the successful development and deployment of future free-space optical
communication links. In this letter, we propose a physics-informed machine
learning (ML) methodology, $\Pi$-ML, based on dimensional analysis and gradient
boosting to estimate $C_n^2$. Through a systematic feature importance analysis,
we identify the normalized variance of potential temperature as the dominating
feature for predicting $C_n^2$. For statistical robustness, we train an
ensemble of models which yields high performance on the out-of-sample data of
$R^2=0.958\pm0.001$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Pierzyna_M/0/1/0/all/0/1"&gt;Maximilian Pierzyna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Saathof_R/0/1/0/all/0/1"&gt;Rudolf Saathof&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Basu_S/0/1/0/all/0/1"&gt;Sukanta Basu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Generative Models for Learning Stochastic Processes. (arXiv:2304.10382v4 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.10382</id>
        <link href="http://arxiv.org/abs/2304.10382"/>
        <updated>2023-08-12T00:39:31.773Z</updated>
        <summary type="html"><![CDATA[A framework to learn a multi-modal distribution is proposed, denoted as the
Conditional Quantum Generative Adversarial Network (C-qGAN). The neural network
structure is strictly within a quantum circuit and, as a consequence, is shown
to represent a more efficient state preparation procedure than current methods.
This methodology has the potential to speed-up algorithms, such as Monte Carlo
analysis. In particular, after demonstrating the effectiveness of the network
in the learning task, the technique is applied to price Asian option
derivatives, providing the foundation for further research on other
path-dependent options.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Certo_S/0/1/0/all/0/1"&gt;Salvatore Certo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Pham_A/0/1/0/all/0/1"&gt;Anh Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Robles_N/0/1/0/all/0/1"&gt;Nicolas Robles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Vlasic_A/0/1/0/all/0/1"&gt;Andrew Vlasic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autonomous sputter synthesis of thin film nitrides with composition controlled by Bayesian optimization of optical plasma emission. (arXiv:2305.11122v3 [physics.app-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.11122</id>
        <link href="http://arxiv.org/abs/2305.11122"/>
        <updated>2023-08-12T00:39:31.768Z</updated>
        <summary type="html"><![CDATA[Autonomous experimentation has emerged as an efficient approach to accelerate
the pace of materials discovery. Although instruments for autonomous synthesis
have become popular in molecular and polymer science, solution processing of
hybrid materials and nanoparticles, examples of autonomous tools for physical
vapor deposition are scarce yet important for the semiconductor industry. Here,
we report the design and implementation of an autonomous workflow for sputter
deposition of thin films with controlled composition, leveraging a highly
automated sputtering reactor custom-controlled by Python, optical emission
spectroscopy (OES), and a Bayesian optimization algorithm. We modeled film
composition, measured by x-ray fluorescence, as a linear function of emission
lines monitored during the co-sputtering from elemental Zn and Ti targets in
N$_2$ atmosphere. A Bayesian control algorithm, informed by OES, navigates the
space of sputtering power to fabricate films with user-defined composition, by
minimizing the absolute error between desired and measured emission signals. We
validated our approach by autonomously fabricating Zn$_x$Ti$_{1-x}$N$_y$ films
with deviations from the targeted cation composition within relative 3.5 %,
even for 15 nm thin films, demonstrating that the proposed approach can
reliably synthesize thin films with specific composition and minimal human
interference. Moreover, the proposed method can be extended to more difficult
synthesis experiments where plasma intensity depends non-linearly on pressure,
or the elemental sticking coefficients strongly depend on the substrate
temperature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Febba_D/0/1/0/all/0/1"&gt;Davi M. Febba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Talley_K/0/1/0/all/0/1"&gt;Kevin R. Talley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Johnson_K/0/1/0/all/0/1"&gt;Kendal Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Schaefer_S/0/1/0/all/0/1"&gt;Stephen Schaefer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Bauers_S/0/1/0/all/0/1"&gt;Sage R. Bauers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Mangum_J/0/1/0/all/0/1"&gt;John S. Mangum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Smaha_R/0/1/0/all/0/1"&gt;Rebecca W. Smaha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zakutayev_A/0/1/0/all/0/1"&gt;Andriy Zakutayev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progressive-Hint Prompting Improves Reasoning in Large Language Models. (arXiv:2304.09797v5 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.09797</id>
        <link href="http://arxiv.org/abs/2304.09797"/>
        <updated>2023-08-12T00:39:31.765Z</updated>
        <summary type="html"><![CDATA[The performance of Large Language Models (LLMs) in reasoning tasks depends
heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency
being critical methods that enhance this ability. However, these methods do not
fully exploit the answers generated by the LLM to guide subsequent responses.
This paper proposes a new prompting method, named Progressive-Hint Prompting
(PHP), that enables automatic multiple interactions between users and LLMs by
using previously generated answers as hints to progressively guide toward the
correct answers. PHP is orthogonal to CoT and self-consistency, making it easy
to combine with state-of-the-art techniques to further improve performance. We
conducted extensive and comprehensive experiments on seven benchmarks. The
results show that PHP significantly improves accuracy while remaining highly
efficient. For instance, with text-davinci-003, we observed a 4.2% improvement
on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction
in sample paths with self-consistency. With GPT-4 and PHP, we achieve
state-of-the-art performances on SVAMP (89.1% -> 91.9%), GSM8K (92% -> 95.5%),
AQuA (76.4% -> 79.9%) and MATH (50.3% -> 53.9%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Chuanyang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhengying Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1"&gt;Enze Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incremental Profit per Conversion: a Response Transformation for Uplift Modeling in E-Commerce Promotions. (arXiv:2306.13759v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.13759</id>
        <link href="http://arxiv.org/abs/2306.13759"/>
        <updated>2023-08-12T00:39:31.740Z</updated>
        <summary type="html"><![CDATA[Promotions play a crucial role in e-commerce platforms, and various cost
structures are employed to drive user engagement. This paper focuses on
promotions with response-dependent costs, where expenses are incurred only when
a purchase is made. Such promotions include discounts and coupons. While
existing uplift model approaches aim to address this challenge, these
approaches often necessitate training multiple models, like meta-learners, or
encounter complications when estimating profit due to zero-inflated values
stemming from non-converted individuals with zero cost and profit.

To address these challenges, we introduce Incremental Profit per Conversion
(IPC), a novel uplift measure of promotional campaigns' efficiency in unit
economics. Through a proposed response transformation, we demonstrate that IPC
requires only converted data, its propensity, and a single model to be
estimated. As a result, IPC resolves the issues mentioned above while
mitigating the noise typically associated with the class imbalance in
conversion datasets and biases arising from the many-to-one mapping between
search and purchase data. Lastly, we validate the efficacy of our approach by
presenting results obtained from a synthetic simulation of a discount coupon
campaign.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Proenca_H/0/1/0/all/0/1"&gt;Hugo Manuel Proen&amp;#xe7;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moraes_F/0/1/0/all/0/1"&gt;Felipe Moraes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Random Search to Bandit Learning in Metric Measure Spaces. (arXiv:2305.11509v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.11509</id>
        <link href="http://arxiv.org/abs/2305.11509"/>
        <updated>2023-08-12T00:39:31.738Z</updated>
        <summary type="html"><![CDATA[Random Search is one of the most widely-used method for Hyperparameter
Optimization, and is critical to the success of deep learning models. Despite
its astonishing performance, little non-heuristic theory has been developed to
describe the underlying working mechanism. This paper gives a theoretical
accounting of Random Search. We introduce the concept of \emph{scattering
dimension} that describes the landscape of the underlying function, and
quantifies the performance of random search. We show that, when the environment
is noise-free, the output of random search converges to the optimal value in
probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T}
\right)^{ \frac{1}{d_s} } \right) $, where $ d_s \ge 0 $ is the scattering
dimension of the underlying function. When the observed function values are
corrupted by bounded $iid$ noise, the output of random search converges to the
optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left(
\frac{1}{T} \right)^{ \frac{1}{d_s + 1} } \right) $. In addition, based on the
principles of random search, we introduce an algorithm, called BLiN-MOS, for
Lipschitz bandits in doubling metric spaces that are also endowed with a
probability measure, and show that BLiN-MOS achieves a regret rate of order $
\widetilde{\mathcal{O}} \left( T^{ \frac{d_z}{d_z + 1} } \right) $, where $d_z$
is the zooming dimension of the problem instance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chuying Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yasong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianyu Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter. (arXiv:2306.03805v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.03805</id>
        <link href="http://arxiv.org/abs/2306.03805"/>
        <updated>2023-08-12T00:39:31.730Z</updated>
        <summary type="html"><![CDATA[Large pre-trained transformers are show-stealer in modern-day deep learning,
and it becomes crucial to comprehend the parsimonious patterns that exist
within them as they grow in scale. With exploding parameter counts, Lottery
Ticket Hypothesis (LTH) and its variants, have lost their pragmatism in
sparsifying them due to high computation and memory bottleneck of repetitive
train-prune-retrain routine of iterative magnitude pruning (IMP) which worsens
with increasing model size. This paper comprehensively studies induced sparse
patterns across multiple large pre-trained vision and language transformers. We
propose the existence of -- essential sparsity defined with a sharp dropping
point beyond which the performance declines much faster w.r.t the rise of
sparsity level, when we directly remove weights with the smallest magnitudes in
one-shot without re-training. We also find essential sparsity to hold valid for
N:M sparsity patterns as well as on modern-scale large language models
(Vicuna-7B). We also present an intriguing emerging phenomenon of abrupt
sparsification during the pre-training of BERT, i.e., BERT suddenly becomes
heavily sparse in pre-training after certain iterations. Moreover, our
observations also indicate a counter-intuitive finding that BERT trained with a
larger amount of pre-training data tends to have a better ability to condense
knowledge in comparatively relatively fewer parameters. Lastly, we investigate
the effect of the pre-training loss on essential sparsity and discover that
self-supervised learning (SSL) objectives trigger stronger emergent
sparsification properties than supervised learning (SL). Our codes are
available at \url{https://github.com/VITA-Group/essential_sparsity}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1"&gt;Ajay Jaiswal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shiwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symmetry Defense Against CNN Adversarial Perturbation Attacks. (arXiv:2210.04087v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.04087</id>
        <link href="http://arxiv.org/abs/2210.04087"/>
        <updated>2023-08-12T00:39:31.729Z</updated>
        <summary type="html"><![CDATA[This paper uses symmetry to make Convolutional Neural Network classifiers
(CNNs) robust against adversarial perturbation attacks. Such attacks add
perturbation to original images to generate adversarial images that fool
classifiers such as road sign classifiers of autonomous vehicles. Although
symmetry is a pervasive aspect of the natural world, CNNs are unable to handle
symmetry well. For example, a CNN can classify an image differently from its
mirror image. For an adversarial image that misclassifies with a wrong label
$l_w$, CNN inability to handle symmetry means that a symmetric adversarial
image can classify differently from the wrong label $l_w$. Further than that,
we find that the classification of a symmetric adversarial image reverts to the
correct label. To classify an image when adversaries are unaware of the
defense, we apply symmetry to the image and use the classification label of the
symmetric image. To classify an image when adversaries are aware of the
defense, we use mirror symmetry and pixel inversion symmetry to form a symmetry
group. We apply all the group symmetries to the image and decide on the output
label based on the agreement of any two of the classification labels of the
symmetry images. Adaptive attacks fail because they need to rely on loss
functions that use conflicting CNN output values for symmetric images. Without
attack knowledge, the proposed symmetry defense succeeds against both
gradient-based and random-search attacks, with up to near-default accuracies
for ImageNet. The defense even improves the classification accuracy of original
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lindqvist_B/0/1/0/all/0/1"&gt;Blerta Lindqvist&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Product Review Image Ranking for Fashion E-commerce. (arXiv:2308.05390v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.05390</id>
        <link href="http://arxiv.org/abs/2308.05390"/>
        <updated>2023-08-12T00:39:31.712Z</updated>
        <summary type="html"><![CDATA[In a fashion e-commerce platform where customers can't physically examine the
products on their own, being able to see other customers' text and image
reviews of the product is critical while making purchase decisions. Given the
high reliance on these reviews, over the years we have observed customers
proactively sharing their reviews. With an increase in the coverage of User
Generated Content (UGC), there has been a corresponding increase in the number
of customer images. It is thus imperative to display the most relevant images
on top as it may influence users' online shopping choices and behavior. In this
paper, we propose a simple yet effective training procedure for ranking
customer images. We created a dataset consisting of Myntra (A Major Indian
Fashion e-commerce company) studio posts and highly engaged (upvotes/downvotes)
UGC images as our starting point and used selected distortion techniques on the
images of the above dataset to bring their quality at par with those of bad UGC
images. We train our network to rank bad-quality images lower than high-quality
ones. Our proposed method outperforms the baseline models on two metrics,
namely correlation coefficient, and accuracy, by substantial margins.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaiswal_S/0/1/0/all/0/1"&gt;Sangeet Jaiswal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1"&gt;Dhruv Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vempati_S/0/1/0/all/0/1"&gt;Sreekanth Vempati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saiswaroop_K/0/1/0/all/0/1"&gt;Konduru Saiswaroop&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A survey of some recent developments in measures of association. (arXiv:2211.04702v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.04702</id>
        <link href="http://arxiv.org/abs/2211.04702"/>
        <updated>2023-08-12T00:39:31.706Z</updated>
        <summary type="html"><![CDATA[This paper surveys some recent developments in measures of association
related to a new coefficient of correlation introduced by the author. A
straightforward extension of this coefficient to standard Borel spaces (which
includes all Polish spaces), overlooked in the literature so far, is proposed
at the end of the survey.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chatterjee_S/0/1/0/all/0/1"&gt;Sourav Chatterjee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RobustPdM: Designing Robust Predictive Maintenance against Adversarial Attacks. (arXiv:2301.10822v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2301.10822</id>
        <link href="http://arxiv.org/abs/2301.10822"/>
        <updated>2023-08-12T00:39:31.706Z</updated>
        <summary type="html"><![CDATA[The state-of-the-art predictive maintenance (PdM) techniques have shown great
success in reducing maintenance costs and downtime of complicated machines
while increasing overall productivity through extensive utilization of
Internet-of-Things (IoT) and Deep Learning (DL). Unfortunately, IoT sensors and
DL algorithms are both prone to cyber-attacks. For instance, DL algorithms are
known for their susceptibility to adversarial examples. Such adversarial
attacks are vastly under-explored in the PdM domain. This is because the
adversarial attacks in the computer vision domain for classification tasks
cannot be directly applied to the PdM domain for multivariate time series (MTS)
regression tasks. In this work, we propose an end-to-end methodology to design
adversarially robust PdM systems by extensively analyzing the effect of
different types of adversarial attacks and proposing a novel adversarial
defense technique for DL-enabled PdM models. First, we propose novel MTS
Projected Gradient Descent (PGD) and MTS PGD with random restarts (PGD_r)
attacks. Then, we evaluate the impact of MTS PGD and PGD_r along with MTS Fast
Gradient Sign Method (FGSM) and MTS Basic Iterative Method (BIM) on Long
Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), Convolutional Neural
Network (CNN), and Bi-directional LSTM based PdM system. Our results using
NASA's turbofan engine dataset show that adversarial attacks can cause a severe
defect (up to 11X) in the RUL prediction, outperforming the effectiveness of
the state-of-the-art PdM attacks by 3X. Furthermore, we present a novel
approximate adversarial training method to defend against adversarial attacks.
We observe that approximate adversarial training can significantly improve the
robustness of PdM models (up to 54X) and outperforms the state-of-the-art PdM
defense methods by offering 3X more robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siddique_A/0/1/0/all/0/1"&gt;Ayesha Siddique&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kundu_R/0/1/0/all/0/1"&gt;Ripan Kumar Kundu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mode_G/0/1/0/all/0/1"&gt;Gautam Raj Mode&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoque_K/0/1/0/all/0/1"&gt;Khaza Anuarul Hoque&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Width and Depth Limits Commute in Residual Networks. (arXiv:2302.00453v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.00453</id>
        <link href="http://arxiv.org/abs/2302.00453"/>
        <updated>2023-08-12T00:39:31.706Z</updated>
        <summary type="html"><![CDATA[We show that taking the width and depth to infinity in a deep neural network
with skip connections, when branches are scaled by $1/\sqrt{depth}$ (the only
nontrivial scaling), result in the same covariance structure no matter how that
limit is taken. This explains why the standard infinite-width-then-depth
approach provides practical insights even for networks with depth of the same
order as width. We also demonstrate that the pre-activations, in this case,
have Gaussian distributions which has direct applications in Bayesian deep
learning. We conduct extensive simulations that show an excellent match with
our theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hayou_S/0/1/0/all/0/1"&gt;Soufiane Hayou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yang_G/0/1/0/all/0/1"&gt;Greg Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing Privacy Leakage in Machine Learning via Multiple Hypothesis Testing: A Lesson From Fano. (arXiv:2210.13662v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.13662</id>
        <link href="http://arxiv.org/abs/2210.13662"/>
        <updated>2023-08-12T00:39:31.705Z</updated>
        <summary type="html"><![CDATA[Differential privacy (DP) is by far the most widely accepted framework for
mitigating privacy risks in machine learning. However, exactly how small the
privacy parameter $\epsilon$ needs to be to protect against certain privacy
risks in practice is still not well-understood. In this work, we study data
reconstruction attacks for discrete data and analyze it under the framework of
multiple hypothesis testing. We utilize different variants of the celebrated
Fano's inequality to derive upper bounds on the inferential power of a data
reconstruction adversary when the model is trained differentially privately.
Importantly, we show that if the underlying private data takes values from a
set of size $M$, then the target privacy parameter $\epsilon$ can be $O(\log
M)$ before the adversary gains significant inferential power. Our analysis
offers theoretical evidence for the empirical effectiveness of DP against data
reconstruction attacks even at relatively large values of $\epsilon$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1"&gt;Chuan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sablayrolles_A/0/1/0/all/0/1"&gt;Alexandre Sablayrolles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1"&gt;Maziar Sanjabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Gated Graph Convolutional Network for Explainable Diagnosis of Alzheimer's Disease using EEG Data. (arXiv:2304.05874v2 [q-bio.NC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.05874</id>
        <link href="http://arxiv.org/abs/2304.05874"/>
        <updated>2023-08-12T00:39:31.705Z</updated>
        <summary type="html"><![CDATA[Graph neural network (GNN) models are increasingly being used for the
classification of electroencephalography (EEG) data. However, GNN-based
diagnosis of neurological disorders, such as Alzheimer's disease (AD), remains
a relatively unexplored area of research. Previous studies have relied on
functional connectivity methods to infer brain graph structures and used simple
GNN architectures for the diagnosis of AD. In this work, we propose a novel
adaptive gated graph convolutional network (AGGCN) that can provide explainable
predictions. AGGCN adaptively learns graph structures by combining
convolution-based node feature enhancement with a well-known correlation-based
measure of functional connectivity. Furthermore, the gated graph convolution
can dynamically weigh the contribution of various spatial scales. The proposed
model achieves high accuracy in both eyes-closed and eyes-open conditions,
indicating the stability of learned representations. Finally, we demonstrate
that the proposed AGGCN model generates consistent explanations of its
predictions that might be relevant for further study of AD-related alterations
of brain networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Klepl_D/0/1/0/all/0/1"&gt;Dominik Klepl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+He_F/0/1/0/all/0/1"&gt;Fei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wu_M/0/1/0/all/0/1"&gt;Min Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Blackburn_D/0/1/0/all/0/1"&gt;Daniel J. Blackburn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Sarrigiannis_P/0/1/0/all/0/1"&gt;Ptolemaios G. Sarrigiannis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InfoNCE is variational inference in a recognition parameterised model. (arXiv:2107.02495v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02495</id>
        <link href="http://arxiv.org/abs/2107.02495"/>
        <updated>2023-08-12T00:39:31.704Z</updated>
        <summary type="html"><![CDATA[Here, we show that the InfoNCE objective is equivalent to the ELBO in a new
class of probabilistic generative model, the recognition parameterised model
(RPM). When we learn the optimal prior, the RPM ELBO becomes equal to the
mutual information (MI; up to a constant), establishing a connection to
pre-existing self-supervised learning methods such as InfoNCE. However,
practical InfoNCE methods do not use the MI as an objective; the MI is
invariant to arbitrary invertible transformations, so using an MI objective can
lead to highly entangled representations (Tschannen et al., 2019). Instead, the
actual InfoNCE objective is a simplified lower bound on the MI which is loose
even in the infinite sample limit. Thus, an objective that works (i.e. the
actual InfoNCE objective) appears to be motivated as a loose bound on an
objective that does not work (i.e. the true MI which gives arbitrarily
entangled representations). We give an alternative motivation for the actual
InfoNCE objective. In particular, we show that in the infinite sample limit,
and for a particular choice of prior, the actual InfoNCE objective is equal to
the ELBO (up to a constant); and the ELBO is equal to the marginal likelihood
with a deterministic recognition model. Thus, we argue that our VAE perspective
gives a better motivation for InfoNCE than MI, as the actual InfoNCE objective
is only loosely bounded by the MI, but is equal to the ELBO/marginal likelihood
(up to a constant).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1"&gt;Laurence Aitchison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ganev_S/0/1/0/all/0/1"&gt;Stoil Ganev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models. (arXiv:2305.03829v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.03829</id>
        <link href="http://arxiv.org/abs/2305.03829"/>
        <updated>2023-08-12T00:39:31.704Z</updated>
        <summary type="html"><![CDATA[Image-based precision medicine aims to personalize treatment decisions based
on an individual's unique imaging features so as to improve their clinical
outcome. Machine learning frameworks that integrate uncertainty estimation as
part of their treatment recommendations would be safer and more reliable.
However, little work has been done in adapting uncertainty estimation
techniques and validation metrics for precision medicine. In this paper, we use
Bayesian deep learning for estimating the posterior distribution over factual
and counterfactual outcomes on several treatments. This allows for estimating
the uncertainty for each treatment option and for the individual treatment
effects (ITE) between any two treatments. We train and evaluate this model to
predict future new and enlarging T2 lesion counts on a large, multi-center
dataset of MR brain images of patients with multiple sclerosis, exposed to
several treatments during randomized controlled trials. We evaluate the
correlation of the uncertainty estimate with the factual error, and, given the
lack of ground truth counterfactual outcomes, demonstrate how uncertainty for
the ITE prediction relates to bounds on the ITE error. Lastly, we demonstrate
how knowledge of uncertainty could modify clinical decision-making to improve
individual patient and clinical trial outcomes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Durso_Finley_J/0/1/0/all/0/1"&gt;Joshua Durso-Finley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falet_J/0/1/0/all/0/1"&gt;Jean-Pierre Falet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1"&gt;Raghav Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arnold_D/0/1/0/all/0/1"&gt;Douglas L. Arnold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pawlowski_N/0/1/0/all/0/1"&gt;Nick Pawlowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arbel_T/0/1/0/all/0/1"&gt;Tal Arbel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From CNN to Transformer: A Review of Medical Image Segmentation Models. (arXiv:2308.05305v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2308.05305</id>
        <link href="http://arxiv.org/abs/2308.05305"/>
        <updated>2023-08-12T00:39:31.703Z</updated>
        <summary type="html"><![CDATA[Medical image segmentation is an important step in medical image analysis,
especially as a crucial prerequisite for efficient disease diagnosis and
treatment. The use of deep learning for image segmentation has become a
prevalent trend. The widely adopted approach currently is U-Net and its
variants. Additionally, with the remarkable success of pre-trained models in
natural language processing tasks, transformer-based models like TransUNet have
achieved desirable performance on multiple medical image segmentation datasets.
In this paper, we conduct a survey of the most representative four medical
image segmentation models in recent years. We theoretically analyze the
characteristics of these models and quantitatively evaluate their performance
on two benchmark datasets (i.e., Tuberculosis Chest X-rays and ovarian tumors).
Finally, we discuss the main challenges and future trends in medical image
segmentation. Our work can assist researchers in the related field to quickly
establish medical segmentation models tailored to specific regions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yao_W/0/1/0/all/0/1"&gt;Wenjian Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bai_J/0/1/0/all/0/1"&gt;Jiajun Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liao_W/0/1/0/all/0/1"&gt;Wei Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mengjuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yao Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quality Diversity under Sparse Reward and Sparse Interaction: Application to Grasping in Robotics. (arXiv:2308.05483v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2308.05483</id>
        <link href="http://arxiv.org/abs/2308.05483"/>
        <updated>2023-08-12T00:39:31.697Z</updated>
        <summary type="html"><![CDATA[Quality-Diversity (QD) methods are algorithms that aim to generate a set of
diverse and high-performing solutions to a given problem. Originally developed
for evolutionary robotics, most QD studies are conducted on a limited set of
domains - mainly applied to locomotion, where the fitness and the behavior
signal are dense. Grasping is a crucial task for manipulation in robotics.
Despite the efforts of many research communities, this task is yet to be
solved. Grasping cumulates unprecedented challenges in QD literature: it
suffers from reward sparsity, behavioral sparsity, and behavior space
misalignment. The present work studies how QD can address grasping. Experiments
have been conducted on 15 different methods on 10 grasping domains,
corresponding to 2 different robot-gripper setups and 5 standard objects. An
evaluation framework that distinguishes the evaluation of an algorithm from its
internal components has also been proposed for a fair comparison. The obtained
results show that MAP-Elites variants that select successful solutions in
priority outperform all the compared methods on the studied metrics by a large
margin. We also found experimental evidence that sparse interaction can lead to
deceptive novelty. To our knowledge, the ability to efficiently produce
examples of grasping trajectories demonstrated in this work has no precedent in
the literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huber_J/0/1/0/all/0/1"&gt;J. Huber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helenon_F/0/1/0/all/0/1"&gt;F. H&amp;#xe9;l&amp;#xe9;non&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coninx_M/0/1/0/all/0/1"&gt;M. Coninx&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amar_F/0/1/0/all/0/1"&gt;F. Ben Amar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doncieux_S/0/1/0/all/0/1"&gt;S. Doncieux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero Grads Ever Given: Learning Local Surrogate Losses for Non-Differentiable Graphics. (arXiv:2308.05739v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.05739</id>
        <link href="http://arxiv.org/abs/2308.05739"/>
        <updated>2023-08-12T00:39:31.618Z</updated>
        <summary type="html"><![CDATA[Gradient-based optimization is now ubiquitous across graphics, but
unfortunately can not be applied to problems with undefined or zero gradients.
To circumvent this issue, the loss function can be manually replaced by a
"surrogate" that has similar minima but is differentiable. Our proposed
framework, ZeroGrads, automates this process by learning a neural approximation
of the objective function, the surrogate, which in turn can be used to
differentiate through arbitrary black-box graphics pipelines. We train the
surrogate on an actively smoothed version of the objective and encourage
locality, focusing the surrogate's capacity on what matters at the current
training episode. The fitting is performed online, alongside the parameter
optimization, and self-supervised, without pre-computed data or pre-trained
models. As sampling the objective is expensive (it requires a full rendering or
simulator run), we devise an efficient sampling scheme that allows for
tractable run-times and competitive performance at little overhead. We
demonstrate optimizing diverse non-convex, non-differentiable black-box
problems in graphics, such as visibility in rendering, discrete parameter
spaces in procedural modelling or optimal control in physics-driven animation.
In contrast to more traditional algorithms, our approach scales well to higher
dimensions, which we demonstrate on problems with up to 35k interlinked
variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1"&gt;Michael Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritschel_T/0/1/0/all/0/1"&gt;Tobias Ritschel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forward-Forward Training of an Optical Neural Network. (arXiv:2305.19170v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.19170</id>
        <link href="http://arxiv.org/abs/2305.19170"/>
        <updated>2023-08-12T00:39:31.613Z</updated>
        <summary type="html"><![CDATA[Neural networks (NN) have demonstrated remarkable capabilities in various
tasks, but their computation-intensive nature demands faster and more
energy-efficient hardware implementations. Optics-based platforms, using
technologies such as silicon photonics and spatial light modulators, offer
promising avenues for achieving this goal. However, training multiple trainable
layers in tandem with these physical systems poses challenges, as they are
difficult to fully characterize and describe with differentiable functions,
hindering the use of error backpropagation algorithm. The recently introduced
Forward-Forward Algorithm (FFA) eliminates the need for perfect
characterization of the learning system and shows promise for efficient
training with large numbers of programmable parameters. The FFA does not
require backpropagating an error signal to update the weights, rather the
weights are updated by only sending information in one direction. The local
loss function for each set of trainable weights enables low-power analog
hardware implementations without resorting to metaheuristic algorithms or
reinforcement learning. In this paper, we present an experiment utilizing
multimode nonlinear wave propagation in an optical fiber demonstrating the
feasibility of the FFA approach using an optical system. The results show that
incorporating optical transforms in multilayer NN architectures trained with
the FFA, can lead to performance improvements, even with a relatively small
number of trainable weights. The proposed method offers a new path to the
challenge of training optical NNs and provides insights into leveraging
physical transformations for enhancing NN performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oguz_I/0/1/0/all/0/1"&gt;Ilker Oguz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_J/0/1/0/all/0/1"&gt;Junjie Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qifei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Feng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yildirim_M/0/1/0/all/0/1"&gt;Mustafa Yildirim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinc_N/0/1/0/all/0/1"&gt;Niyazi Ulas Dinc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_J/0/1/0/all/0/1"&gt;Jih-Liang Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moser_C/0/1/0/all/0/1"&gt;Christophe Moser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Psaltis_D/0/1/0/all/0/1"&gt;Demetri Psaltis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep incremental learning models for financial temporal tabular datasets with distribution shifts. (arXiv:2303.07925v7 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.07925</id>
        <link href="http://arxiv.org/abs/2303.07925"/>
        <updated>2023-08-12T00:39:31.608Z</updated>
        <summary type="html"><![CDATA[We present a robust deep incremental learning framework for regression tasks
on financial temporal tabular datasets which is built upon the incremental use
of commonly available tabular and time series prediction models to adapt to
distributional shifts typical of financial datasets. The framework uses a
simple basic building block (decision trees) to build self-similar models of
any required complexity to deliver robust performance under adverse situations
such as regime changes, fat-tailed distributions, and low signal-to-noise
ratios. As a detailed study, we demonstrate our scheme using XGBoost models
trained on the Numerai dataset and show that a two layer deep ensemble of
XGBoost models over different model snapshots delivers high quality predictions
under different market regimes. We also show that the performance of XGBoost
models with different number of boosting rounds in three scenarios (small,
standard and large) is monotonically increasing with respect to model size and
converges towards the generalisation upper bound. We also evaluate the
robustness of the model under variability of different hyperparameters, such as
model complexity and data sampling settings. Our model has low hardware
requirements as no specialised neural architectures are used and each base
model can be independently trained in parallel.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1"&gt;Thomas Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barahona_M/0/1/0/all/0/1"&gt;Mauricio Barahona&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Functional Neural Networks: Shift invariant models for functional data with applications to EEG classification. (arXiv:2301.05869v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2301.05869</id>
        <link href="http://arxiv.org/abs/2301.05869"/>
        <updated>2023-08-12T00:39:31.607Z</updated>
        <summary type="html"><![CDATA[It is desirable for statistical models to detect signals of interest
independently of their position. If the data is generated by some smooth
process, this additional structure should be taken into account. We introduce a
new class of neural networks that are shift invariant and preserve smoothness
of the data: functional neural networks (FNNs). For this, we use methods from
functional data analysis (FDA) to extend multi-layer perceptrons and
convolutional neural networks to functional data. We propose different model
architectures, show that the models outperform a benchmark model from FDA in
terms of accuracy and successfully use FNNs to classify electroencephalography
(EEG) data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heinrichs_F/0/1/0/all/0/1"&gt;Florian Heinrichs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heim_M/0/1/0/all/0/1"&gt;Mavin Heim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1"&gt;Corinna Weber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Out-of-Memory NMF on CPU/GPU Architectures. (arXiv:2202.09518v3 [cs.DC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.09518</id>
        <link href="http://arxiv.org/abs/2202.09518"/>
        <updated>2023-08-12T00:39:31.606Z</updated>
        <summary type="html"><![CDATA[We propose an efficient distributed out-of-memory implementation of the
Non-negative Matrix Factorization (NMF) algorithm for heterogeneous
high-performance-computing (HPC) systems. The proposed implementation is based
on prior work on NMFk, which can perform automatic model selection and extract
latent variables and patterns from data. In this work, we extend NMFk by adding
support for dense and sparse matrix operation on multi-node, multi-GPU systems.
The resulting algorithm is optimized for out-of-memory (OOM) problems where the
memory required to factorize a given matrix is greater than the available GPU
memory. Memory complexity is reduced by batching/tiling strategies, and sparse
and dense matrix operations are significantly accelerated with GPU cores (or
tensor cores when available). Input/Output (I/O) latency associated with batch
copies between host and device is hidden using CUDA streams to overlap data
transfers and compute asynchronously, and latency associated with collective
communications (both intra-node and inter-node) is reduced using optimized
NVIDIA Collective Communication Library NCCL based communicators. Benchmark
results show significant improvement, from 32X to 76x speedup, with the new
implementation using GPUs over the CPU-based NMFk. Good weak scaling was
demonstrated on up to 4096 multi-GPU cluster nodes with approximately 25,000
GPUs when decomposing a dense 340 Terabyte-size matrix and an 11 Exabyte-size
sparse matrix of density 10e-6.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boureima_I/0/1/0/all/0/1"&gt;Ismael Boureima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattarai_M/0/1/0/all/0/1"&gt;Manish Bhattarai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eren_M/0/1/0/all/0/1"&gt;Maksim Eren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skau_E/0/1/0/all/0/1"&gt;Erik Skau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romero_P/0/1/0/all/0/1"&gt;Philip Romero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eidenbenz_S/0/1/0/all/0/1"&gt;Stephan Eidenbenz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alexandrov_B/0/1/0/all/0/1"&gt;Boian Alexandrov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$\mathcal{G}^2Pxy$: Generative Open-Set Node Classification on Graphs with Proxy Unknowns. (arXiv:2308.05463v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.05463</id>
        <link href="http://arxiv.org/abs/2308.05463"/>
        <updated>2023-08-12T00:39:31.605Z</updated>
        <summary type="html"><![CDATA[Node classification is the task of predicting the labels of unlabeled nodes
in a graph. State-of-the-art methods based on graph neural networks achieve
excellent performance when all labels are available during training. But in
real-life, models are often applied on data with new classes, which can lead to
massive misclassification and thus significantly degrade performance. Hence,
developing open-set classification methods is crucial to determine if a given
sample belongs to a known class. Existing methods for open-set node
classification generally use transductive learning with part or all of the
features of real unseen class nodes to help with open-set classification. In
this paper, we propose a novel generative open-set node classification method,
i.e. $\mathcal{G}^2Pxy$, which follows a stricter inductive learning setting
where no information about unknown classes is available during training and
validation. Two kinds of proxy unknown nodes, inter-class unknown proxies and
external unknown proxies are generated via mixup to efficiently anticipate the
distribution of novel classes. Using the generated proxies, a closed-set
classifier can be transformed into an open-set one, by augmenting it with an
extra proxy classifier. Under the constraints of both cross entropy loss and
complement entropy loss, $\mathcal{G}^2Pxy$ achieves superior effectiveness for
unknown class detection and known class classification, which is validated by
experiments on benchmark graph datasets. Moreover, $\mathcal{G}^2Pxy$ does not
have specific requirement on the GNN architecture and shows good
generalizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zelin Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaolin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaojun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fournier_Viger_P/0/1/0/all/0/1"&gt;Philippe Fournier-Viger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shirui Pan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IIHT: Medical Report Generation with Image-to-Indicator Hierarchical Transformer. (arXiv:2308.05633v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.05633</id>
        <link href="http://arxiv.org/abs/2308.05633"/>
        <updated>2023-08-12T00:39:31.605Z</updated>
        <summary type="html"><![CDATA[Automated medical report generation has become increasingly important in
medical analysis. It can produce computer-aided diagnosis descriptions and thus
significantly alleviate the doctors' work. Inspired by the huge success of
neural machine translation and image captioning, various deep learning methods
have been proposed for medical report generation. However, due to the inherent
properties of medical data, including data imbalance and the length and
correlation between report sequences, the generated reports by existing methods
may exhibit linguistic fluency but lack adequate clinical accuracy. In this
work, we propose an image-to-indicator hierarchical transformer (IIHT)
framework for medical report generation. It consists of three modules, i.e., a
classifier module, an indicator expansion module and a generator module. The
classifier module first extracts image features from the input medical images
and produces disease-related indicators with their corresponding states. The
disease-related indicators are subsequently utilised as input for the indicator
expansion module, incorporating the "data-text-data" strategy. The
transformer-based generator then leverages these extracted features along with
image features as auxiliary information to generate final reports. Furthermore,
the proposed IIHT method is feasible for radiologists to modify disease
indicators in real-world scenarios and integrate the operations into the
indicator expansion module for fluent and accurate medical report generation.
Extensive experiments and comparisons with state-of-the-art methods under
various evaluation metrics demonstrate the great performance of the proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_K/0/1/0/all/0/1"&gt;Keqiang Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1"&gt;Xiaohao Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niranjan_M/0/1/0/all/0/1"&gt;Mahesan Niranjan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Normalized Gradients for All. (arXiv:2308.05621v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.05621</id>
        <link href="http://arxiv.org/abs/2308.05621"/>
        <updated>2023-08-12T00:39:31.603Z</updated>
        <summary type="html"><![CDATA[In this short note, I show how to adapt to H\"{o}lder smoothness using
normalized gradients in a black-box way. Moreover, the bound will depend on a
novel notion of local H\"{o}lder smoothness. The main idea directly comes from
Levy [2017].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Orabona_F/0/1/0/all/0/1"&gt;Francesco Orabona&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment. (arXiv:2308.05374v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2308.05374</id>
        <link href="http://arxiv.org/abs/2308.05374"/>
        <updated>2023-08-12T00:39:31.601Z</updated>
        <summary type="html"><![CDATA[Ensuring alignment, which refers to making models behave in accordance with
human intentions [1,2], has become a critical task before deploying large
language models (LLMs) in real-world applications. For instance, OpenAI devoted
six months to iteratively aligning GPT-4 before its release [3]. However, a
major challenge faced by practitioners is the lack of clear guidance on
evaluating whether LLM outputs align with social norms, values, and
regulations. This obstacle hinders systematic iteration and deployment of LLMs.
To address this issue, this paper presents a comprehensive survey of key
dimensions that are crucial to consider when assessing LLM trustworthiness. The
survey covers seven major categories of LLM trustworthiness: reliability,
safety, fairness, resistance to misuse, explainability and reasoning, adherence
to social norms, and robustness. Each major category is further divided into
several sub-categories, resulting in a total of 29 sub-categories.
Additionally, a subset of 8 sub-categories is selected for further
investigation, where corresponding measurement studies are designed and
conducted on several widely-used LLMs. The measurement results indicate that,
in general, more aligned models tend to perform better in terms of overall
trustworthiness. However, the effectiveness of alignment varies across the
different trustworthiness categories considered. This highlights the importance
of conducting more fine-grained analyses, testing, and making continuous
improvements on LLM alignment. By shedding light on these key dimensions of LLM
trustworthiness, this paper aims to provide valuable insights and guidance to
practitioners in the field. Understanding and addressing these concerns will be
crucial in achieving reliable and ethically sound deployment of LLMs in various
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yuanshun Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ton_J/0/1/0/all/0/1"&gt;Jean-Francois Ton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoying Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1"&gt;Ruocheng Guo Hao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klochkov_Y/0/1/0/all/0/1"&gt;Yegor Klochkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taufiq_M/0/1/0/all/0/1"&gt;Muhammad Faaiz Taufiq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Critical Points ++: An Agile Point Cloud Importance Measure for Robust Classification, Adversarial Defense and Explainable AI. (arXiv:2308.05525v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.05525</id>
        <link href="http://arxiv.org/abs/2308.05525"/>
        <updated>2023-08-12T00:39:31.575Z</updated>
        <summary type="html"><![CDATA[The ability to cope accurately and fast with Out-Of-Distribution (OOD)
samples is crucial in real-world safety demanding applications. In this work we
first study the interplay between critical points of 3D point clouds and OOD
samples. Our findings are that common corruptions and outliers are often
interpreted as critical points. We generalize the notion of critical points
into importance measures. We show that training a classification network based
only on less important points dramatically improves robustness, at a cost of
minor performance loss on the clean set. We observe that normalized entropy is
highly informative for corruption analysis. An adaptive threshold based on
normalized entropy is suggested for selecting the set of uncritical points. Our
proposed importance measure is extremely fast to compute. We show it can be
used for a variety of applications, such as Explainable AI (XAI), Outlier
Removal, Uncertainty Estimation, Robust Classification and Adversarial Defense.
We reach SOTA results on the two latter tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Levi_M/0/1/0/all/0/1"&gt;Meir Yossef Levi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gilboa_G/0/1/0/all/0/1"&gt;Guy Gilboa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoGluon-TimeSeries: AutoML for Probabilistic Time Series Forecasting. (arXiv:2308.05566v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.05566</id>
        <link href="http://arxiv.org/abs/2308.05566"/>
        <updated>2023-08-12T00:39:31.573Z</updated>
        <summary type="html"><![CDATA[We introduce AutoGluon-TimeSeries - an open-source AutoML library for
probabilistic time series forecasting. Focused on ease of use and robustness,
AutoGluon-TimeSeries enables users to generate accurate point and quantile
forecasts with just 3 lines of Python code. Built on the design philosophy of
AutoGluon, AutoGluon-TimeSeries leverages ensembles of diverse forecasting
models to deliver high accuracy within a short training time.
AutoGluon-TimeSeries combines both conventional statistical models,
machine-learning based forecasting approaches, and ensembling techniques. In
our evaluation on 29 benchmark datasets, AutoGluon-TimeSeries demonstrates
strong empirical performance, outperforming a range of forecasting methods in
terms of both point and quantile forecast accuracy, and often even improving
upon the best-in-hindsight combination of prior methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shchur_O/0/1/0/all/0/1"&gt;Oleksandr Shchur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turkmen_C/0/1/0/all/0/1"&gt;Caner Turkmen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erickson_N/0/1/0/all/0/1"&gt;Nick Erickson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Huibin Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shirkov_A/0/1/0/all/0/1"&gt;Alexander Shirkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1"&gt;Tony Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable AI applications in the Medical Domain: a systematic review. (arXiv:2308.05411v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2308.05411</id>
        <link href="http://arxiv.org/abs/2308.05411"/>
        <updated>2023-08-12T00:39:31.571Z</updated>
        <summary type="html"><![CDATA[Artificial Intelligence in Medicine has made significant progress with
emerging applications in medical imaging, patient care, and other areas. While
these applications have proven successful in retrospective studies, very few of
them were applied in practice.The field of Medical AI faces various challenges,
in terms of building user trust, complying with regulations, using data
ethically.Explainable AI (XAI) aims to enable humans understand AI and trust
its results. This paper presents a literature review on the recent developments
of XAI solutions for medical decision support, based on a representative sample
of 198 articles published in recent years. The systematic synthesis of the
relevant articles resulted in several findings. (1) model-agnostic XAI
techniques were mostly employed in these solutions, (2) deep learning models
are utilized more than other types of machine learning models, (3)
explainability was applied to promote trust, but very few works reported the
physicians participation in the loop, (4) visual and interactive user interface
is more useful in understanding the explanation and the recommendation of the
system. More research is needed in collaboration between medical and AI
experts, that could guide the development of suitable frameworks for the
design, implementation, and evaluation of XAI solutions in medicine.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prentzas_N/0/1/0/all/0/1"&gt;Nicoletta Prentzas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakas_A/0/1/0/all/0/1"&gt;Antonis Kakas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pattichis_C/0/1/0/all/0/1"&gt;Constantinos S. Pattichis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comparative Assessment of Multi-view fusion learning for Crop Classification. (arXiv:2308.05407v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.05407</id>
        <link href="http://arxiv.org/abs/2308.05407"/>
        <updated>2023-08-12T00:39:31.542Z</updated>
        <summary type="html"><![CDATA[With a rapidly increasing amount and diversity of remote sensing (RS) data
sources, there is a strong need for multi-view learning modeling. This is a
complex task when considering the differences in resolution, magnitude, and
noise of RS data. The typical approach for merging multiple RS sources has been
input-level fusion, but other - more advanced - fusion strategies may
outperform this traditional approach. This work assesses different fusion
strategies for crop classification in the CropHarvest dataset. The fusion
methods proposed in this work outperform models based on individual views and
previous fusion methods. We do not find one single fusion method that
consistently outperforms all other approaches. Instead, we present a comparison
of multi-view fusion methods for three different datasets and show that,
depending on the test region, different methods obtain the best performance.
Despite this, we suggest a preliminary criterion for the selection of fusion
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mena_F/0/1/0/all/0/1"&gt;Francisco Mena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arenas_D/0/1/0/all/0/1"&gt;Diego Arenas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nuske_M/0/1/0/all/0/1"&gt;Marlon Nuske&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1"&gt;Andreas Dengel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis. (arXiv:2308.05476v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2308.05476</id>
        <link href="http://arxiv.org/abs/2308.05476"/>
        <updated>2023-08-12T00:39:31.541Z</updated>
        <summary type="html"><![CDATA[Deceptive text classification is a critical task in natural language
processing that aims to identify deceptive or fraudulent content. This study
presents a comparative analysis of machine learning and transformer-based
approaches for deceptive text classification. We investigate the effectiveness
of traditional machine learning algorithms and state-of-the-art transformer
models, such as BERT, XLNET, DistilBERT, and RoBERTa, in detecting deceptive
text. A labeled dataset consisting of deceptive and non-deceptive texts is used
for training and evaluation purposes. Through extensive experimentation, we
compare the performance metrics, including accuracy, precision, recall, and F1
score, of the different approaches. The results of this study shed light on the
strengths and limitations of machine learning and transformer-based methods for
deceptive text classification, enabling researchers and practitioners to make
informed decisions when dealing with deceptive content]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_A/0/1/0/all/0/1"&gt;Anusuya Krishnan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FINER: Enhancing State-of-the-art Classifiers with Feature Attribution to Facilitate Security Analysis. (arXiv:2308.05362v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2308.05362</id>
        <link href="http://arxiv.org/abs/2308.05362"/>
        <updated>2023-08-12T00:39:31.532Z</updated>
        <summary type="html"><![CDATA[Deep learning classifiers achieve state-of-the-art performance in various
risk detection applications. They explore rich semantic representations and are
supposed to automatically discover risk behaviors. However, due to the lack of
transparency, the behavioral semantics cannot be conveyed to downstream
security experts to reduce their heavy workload in security analysis. Although
feature attribution (FA) methods can be used to explain deep learning, the
underlying classifier is still blind to what behavior is suspicious, and the
generated explanation cannot adapt to downstream tasks, incurring poor
explanation fidelity and intelligibility. In this paper, we propose FINER, the
first framework for risk detection classifiers to generate high-fidelity and
high-intelligibility explanations. The high-level idea is to gather explanation
efforts from model developer, FA designer, and security experts. To improve
fidelity, we fine-tune the classifier with an explanation-guided multi-task
learning strategy. To improve intelligibility, we engage task knowledge to
adjust and ensemble FA methods. Extensive evaluations show that FINER improves
explanation quality for risk detection. Moreover, we demonstrate that FINER
outperforms a state-of-the-art tool in facilitating malware analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yiling He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1"&gt;Jian Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zhan Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1"&gt;Kui Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conformer-based Target-Speaker Automatic Speech Recognition for Single-Channel Audio. (arXiv:2308.05218v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2308.05218</id>
        <link href="http://arxiv.org/abs/2308.05218"/>
        <updated>2023-08-12T00:39:31.531Z</updated>
        <summary type="html"><![CDATA[We propose CONF-TSASR, a non-autoregressive end-to-end time-frequency domain
architecture for single-channel target-speaker automatic speech recognition
(TS-ASR). The model consists of a TitaNet based speaker embedding module, a
Conformer based masking as well as ASR modules. These modules are jointly
optimized to transcribe a target-speaker, while ignoring speech from other
speakers. For training we use Connectionist Temporal Classification (CTC) loss
and introduce a scale-invariant spectrogram reconstruction loss to encourage
the model better separate the target-speaker's spectrogram from mixture. We
obtain state-of-the-art target-speaker word error rate (TS-WER) on
WSJ0-2mix-extr (4.2%). Further, we report for the first time TS-WER on
WSJ0-3mix-extr (12.4%), LibriSpeech2Mix (4.2%) and LibriSpeech3Mix (7.6%)
datasets, establishing new benchmarks for TS-ASR. The proposed model will be
open-sourced through NVIDIA NeMo toolkit.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puvvada_K/0/1/0/all/0/1"&gt;Krishna C. Puvvada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lavrukhin_V/0/1/0/all/0/1"&gt;Vitaly Lavrukhin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ginsburg_B/0/1/0/all/0/1"&gt;Boris Ginsburg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging the Edge and Cloud for V2X-Based Real-Time Object Detection in Autonomous Driving. (arXiv:2308.05234v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.05234</id>
        <link href="http://arxiv.org/abs/2308.05234"/>
        <updated>2023-08-12T00:39:31.530Z</updated>
        <summary type="html"><![CDATA[Environmental perception is a key element of autonomous driving because the
information received from the perception module influences core driving
decisions. An outstanding challenge in real-time perception for autonomous
driving lies in finding the best trade-off between detection quality and
latency. Major constraints on both computation and power have to be taken into
account for real-time perception in autonomous vehicles. Larger object
detection models tend to produce the best results, but are also slower at
runtime. Since the most accurate detectors cannot run in real-time locally, we
investigate the possibility of offloading computation to edge and cloud
platforms, which are less resource-constrained. We create a synthetic dataset
to train object detection models and evaluate different offloading strategies.
Using real hardware and network simulations, we compare different trade-offs
between prediction quality and end-to-end delay. Since sending raw frames over
the network implies additional transmission delays, we also explore the use of
JPEG and H.265 compression at varying qualities and measure their impact on
prediction metrics. We show that models with adequate compression can be run in
real-time on the cloud while outperforming local detection performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hawlader_F/0/1/0/all/0/1"&gt;Faisal Hawlader&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robinet_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Robinet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_R/0/1/0/all/0/1"&gt;Rapha&amp;#xeb;l Frank&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven Intra-Autonomous Systems Graph Generator. (arXiv:2308.05254v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2308.05254</id>
        <link href="http://arxiv.org/abs/2308.05254"/>
        <updated>2023-08-12T00:39:31.530Z</updated>
        <summary type="html"><![CDATA[This paper introduces a novel deep-learning based generator of synthetic
graphs that represent intra-Autonomous System (AS) in the Internet, named
Deep-generative graphs for the Internet (DGGI). It also presents a novel
massive dataset of real intra-AS graphs extracted from the project Internet
Topology Data Kit (ITDK), called Internet Graphs (IGraphs). To create IGraphs,
the Filtered Recurrent Multi-level (FRM) algorithm for community extraction was
developed. It is shown that DGGI creates synthetic graphs which accurately
reproduce the properties of centrality, clustering, assortativity, and node
degree. The DGGI generator overperforms existing Internet topology generators.
On average, DGGI improves the Maximum Mean Discrepancy (MMD) metric 84.4%,
95.1%, 97.9%, and 94.7% for assortativity, betweenness, clustering, and node
degree, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dadauto_C/0/1/0/all/0/1"&gt;Caio Vinicius Dadauto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fonseca_N/0/1/0/all/0/1"&gt;Nelson Luis Saldanha da Fonseca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torres_R/0/1/0/all/0/1"&gt;Ricardo da Silva Torres&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenProteinSet: Training data for structural biology at scale. (arXiv:2308.05326v1 [q-bio.BM])]]></title>
        <id>http://arxiv.org/abs/2308.05326</id>
        <link href="http://arxiv.org/abs/2308.05326"/>
        <updated>2023-08-12T00:39:31.529Z</updated>
        <summary type="html"><![CDATA[Multiple sequence alignments (MSAs) of proteins encode rich biological
information and have been workhorses in bioinformatic methods for tasks like
protein design and protein structure prediction for decades. Recent
breakthroughs like AlphaFold2 that use transformers to attend directly over
large quantities of raw MSAs have reaffirmed their importance. Generation of
MSAs is highly computationally intensive, however, and no datasets comparable
to those used to train AlphaFold2 have been made available to the research
community, hindering progress in machine learning for proteins. To remedy this
problem, we introduce OpenProteinSet, an open-source corpus of more than 16
million MSAs, associated structural homologs from the Protein Data Bank, and
AlphaFold2 protein structure predictions. We have previously demonstrated the
utility of OpenProteinSet by successfully retraining AlphaFold2 on it. We
expect OpenProteinSet to be broadly useful as training and validation data for
1) diverse tasks focused on protein structure, function, and design and 2)
large-scale multimodal machine learning research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Ahdritz_G/0/1/0/all/0/1"&gt;Gustaf Ahdritz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Bouatta_N/0/1/0/all/0/1"&gt;Nazim Bouatta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kadyan_S/0/1/0/all/0/1"&gt;Sachin Kadyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Jarosch_L/0/1/0/all/0/1"&gt;Lukas Jarosch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Berenberg_D/0/1/0/all/0/1"&gt;Daniel Berenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Fisk_I/0/1/0/all/0/1"&gt;Ian Fisk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Watkins_A/0/1/0/all/0/1"&gt;Andrew M. Watkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ra_S/0/1/0/all/0/1"&gt;Stephen Ra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Bonneau_R/0/1/0/all/0/1"&gt;Richard Bonneau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+AlQuraishi_M/0/1/0/all/0/1"&gt;Mohammed AlQuraishi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning aided Computer Architecture Design for CNN Inferencing Systems. (arXiv:2308.05364v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2308.05364</id>
        <link href="http://arxiv.org/abs/2308.05364"/>
        <updated>2023-08-12T00:39:31.529Z</updated>
        <summary type="html"><![CDATA[Efficient and timely calculations of Machine Learning (ML) algorithms are
essential for emerging technologies like autonomous driving, the Internet of
Things (IoT), and edge computing. One of the primary ML algorithms used in such
systems is Convolutional Neural Networks (CNNs), which demand high
computational resources. This requirement has led to the use of ML accelerators
like GPGPUs to meet design constraints. However, selecting the most suitable
accelerator involves Design Space Exploration (DSE), a process that is usually
time-consuming and requires significant manual effort. Our work presents
approaches to expedite the DSE process by identifying the most appropriate
GPGPU for CNN inferencing systems. We have developed a quick and precise
technique for forecasting the power and performance of CNNs during inference,
with a MAPE of 5.03% and 5.94%, respectively. Our approach empowers computer
architects to estimate power and performance in the early stages of
development, reducing the necessity for numerous prototypes. This saves time
and money while also improving the time-to-market period.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Metz_C/0/1/0/all/0/1"&gt;Christopher A. Metz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flexible Isosurface Extraction for Gradient-Based Mesh Optimization. (arXiv:2308.05371v1 [cs.GR])]]></title>
        <id>http://arxiv.org/abs/2308.05371</id>
        <link href="http://arxiv.org/abs/2308.05371"/>
        <updated>2023-08-12T00:39:31.528Z</updated>
        <summary type="html"><![CDATA[This work considers gradient-based mesh optimization, where we iteratively
optimize for a 3D surface mesh by representing it as the isosurface of a scalar
field, an increasingly common paradigm in applications including
photogrammetry, generative modeling, and inverse physics. Existing
implementations adapt classic isosurface extraction algorithms like Marching
Cubes or Dual Contouring; these techniques were designed to extract meshes from
fixed, known fields, and in the optimization setting they lack the degrees of
freedom to represent high-quality feature-preserving meshes, or suffer from
numerical instabilities. We introduce FlexiCubes, an isosurface representation
specifically designed for optimizing an unknown mesh with respect to geometric,
visual, or even physical objectives. Our main insight is to introduce
additional carefully-chosen parameters into the representation, which allow
local flexible adjustments to the extracted mesh geometry and connectivity.
These parameters are updated along with the underlying scalar field via
automatic differentiation when optimizing for a downstream task. We base our
extraction scheme on Dual Marching Cubes for improved topological properties,
and present extensions to optionally generate tetrahedral and
hierarchically-adaptive meshes. Extensive experiments validate FlexiCubes on
both synthetic benchmarks and real-world applications, showing that it offers
significant improvements in mesh quality and geometric fidelity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1"&gt;Tianchang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Munkberg_J/0/1/0/all/0/1"&gt;Jacob Munkberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasselgren_J/0/1/0/all/0/1"&gt;Jon Hasselgren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1"&gt;Kangxue Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wenzheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gojcic_Z/0/1/0/all/0/1"&gt;Zan Gojcic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1"&gt;Sanja Fidler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharp_N/0/1/0/all/0/1"&gt;Nicholas Sharp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jun Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training neural networks with end-to-end optical backpropagation. (arXiv:2308.05226v1 [physics.optics])]]></title>
        <id>http://arxiv.org/abs/2308.05226</id>
        <link href="http://arxiv.org/abs/2308.05226"/>
        <updated>2023-08-12T00:39:31.445Z</updated>
        <summary type="html"><![CDATA[Optics is an exciting route for the next generation of computing hardware for
machine learning, promising several orders of magnitude enhancement in both
computational speed and energy efficiency. However, to reach the full capacity
of an optical neural network it is necessary that the computing not only for
the inference, but also for the training be implemented optically. The primary
algorithm for training a neural network is backpropagation, in which the
calculation is performed in the order opposite to the information flow for
inference. While straightforward in a digital computer, optical implementation
of backpropagation has so far remained elusive, particularly because of the
conflicting requirements for the optical element that implements the nonlinear
activation function. In this work, we address this challenge for the first time
with a surprisingly simple and generic scheme. Saturable absorbers are employed
for the role of the activation units, and the required properties are achieved
through a pump-probe process, in which the forward propagating signal acts as
the pump and backward as the probe. Our approach is adaptable to various analog
platforms, materials, and network structures, and it demonstrates the
possibility of constructing neural networks entirely reliant on analog optical
processes for both training and inference tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Spall_J/0/1/0/all/0/1"&gt;James Spall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xianxin Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Lvovsky_A/0/1/0/all/0/1"&gt;A. I. Lvovsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS). (arXiv:2308.05239v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2308.05239</id>
        <link href="http://arxiv.org/abs/2308.05239"/>
        <updated>2023-08-12T00:39:31.443Z</updated>
        <summary type="html"><![CDATA[Several architecture frameworks for software, systems, and enterprises have
been proposed in the literature. They identified various stakeholders and
defined architecture viewpoints and views to frame and address stakeholder
concerns. However, the stakeholders with data science and Machine Learning (ML)
related concerns, such as data scientists and data engineers, are yet to be
included in existing architecture frameworks. Therefore, they failed to address
the architecture viewpoints and views responsive to the concerns of the data
science community. In this paper, we address this gap by establishing the
architecture frameworks adapted to meet the requirements of modern applications
and organizations where ML artifacts are both prevalent and crucial. In
particular, we focus on ML-enabled Cyber-Physical Systems (CPSs) and propose
two sets of merit criteria for their efficient development and performance
assessment, namely the criteria for evaluating and benchmarking ML-enabled
CPSs, and the criteria for evaluation and benchmarking of the tools intended to
support users through the modeling and development pipeline. In this study, we
deploy multiple empirical and qualitative research methods based on literature
review and survey instruments including expert interviews and an online
questionnaire. We collect, analyze, and integrate the opinions of 77 experts
from more than 25 organizations in over 10 countries to devise and validate the
proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moin_A/0/1/0/all/0/1"&gt;Armin Moin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badii_A/0/1/0/all/0/1"&gt;Atta Badii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1"&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Challenger_M/0/1/0/all/0/1"&gt;Moharram Challenger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Homophily-enhanced Structure Learning for Graph Clustering. (arXiv:2308.05309v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.05309</id>
        <link href="http://arxiv.org/abs/2308.05309"/>
        <updated>2023-08-12T00:39:31.435Z</updated>
        <summary type="html"><![CDATA[Graph clustering is a fundamental task in graph analysis, and recent advances
in utilizing graph neural networks (GNNs) have shown impressive results.
Despite the success of existing GNN-based graph clustering methods, they often
overlook the quality of graph structure, which is inherent in real-world graphs
due to their sparse and multifarious nature, leading to subpar performance.
Graph structure learning allows refining the input graph by adding missing
links and removing spurious connections. However, previous endeavors in graph
structure learning have predominantly centered around supervised settings, and
cannot be directly applied to our specific clustering tasks due to the absence
of ground-truth labels. To bridge the gap, we propose a novel method called
\textbf{ho}mophily-enhanced structure \textbf{le}arning for graph clustering
(HoLe). Our motivation stems from the observation that subtly enhancing the
degree of homophily within the graph structure can significantly improve GNNs
and clustering outcomes. To realize this objective, we develop two
clustering-oriented structure learning modules, i.e., hierarchical correlation
estimation and cluster-aware sparsification. The former module enables a more
accurate estimation of pairwise node relationships by leveraging guidance from
latent and clustering spaces, while the latter one generates a sparsified
structure based on the similarity matrix and clustering assignments.
Additionally, we devise a joint optimization approach alternating between
training the homophily-enhanced structure learning and GNN-based clustering,
thereby enforcing their reciprocal effects. Extensive experiments on seven
benchmark datasets of various types and scales, across a range of clustering
metrics, demonstrate the superiority of HoLe against state-of-the-art
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_M/0/1/0/all/0/1"&gt;Ming Gu&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Gaoming Yang&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Sheng Zhou&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1"&gt;Ning Ma&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiawei Chen&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1"&gt;Qiaoyu Tan&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Meihan Liu&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1"&gt;Jiajun Bu&lt;/a&gt; (1) ((1) College of Computer Science and Technology, Zhejiang University, (2) School of Software Technology, Zhejiang University, (3) Zhejiang Provincial Key Laboratory of Service Robot, Zhejiang University, (4) Department of Computer Science, New York University Shanghai)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient. (arXiv:2308.05681v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.05681</id>
        <link href="http://arxiv.org/abs/2308.05681"/>
        <updated>2023-08-12T00:39:31.410Z</updated>
        <summary type="html"><![CDATA[Recently, methods for skeleton-based human activity recognition have been
shown to be vulnerable to adversarial attacks. However, these attack methods
require either the full knowledge of the victim (i.e. white-box attacks),
access to training data (i.e. transfer-based attacks) or frequent model queries
(i.e. black-box attacks). All their requirements are highly restrictive,
raising the question of how detrimental the vulnerability is. In this paper, we
show that the vulnerability indeed exists. To this end, we consider a new
attack task: the attacker has no access to the victim model or the training
data or labels, where we coin the term hard no-box attack. Specifically, we
first learn a motion manifold where we define an adversarial loss to compute a
new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our
gradient contains information of the motion dynamics, which is different from
existing gradient-based attack methods that compute the loss gradient assuming
each dimension in the data is independent. The SMI gradient can augment many
gradient-based attack methods, leading to a new family of no-box attack
methods. Extensive evaluation and comparison show that our method imposes a
real threat to existing classifiers. They also show that the SMI gradient
improves the transferability and imperceptibility of adversarial samples in
both no-box and transfer-based black-box settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhengzhi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;He Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Z/0/1/0/all/0/1"&gt;Ziyi Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guoan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1"&gt;Hubert P. H. Shum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review. (arXiv:2308.05731v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2308.05731</id>
        <link href="http://arxiv.org/abs/2308.05731"/>
        <updated>2023-08-12T00:39:31.402Z</updated>
        <summary type="html"><![CDATA[Automated driving has the potential to revolutionize personal, public, and
freight mobility. Besides the enormous challenge of perception, i.e. accurately
perceiving the environment using available sensor data, automated driving
comprises planning a safe, comfortable, and efficient motion trajectory. To
promote safety and progress, many works rely on modules that predict the future
motion of surrounding traffic. Modular automated driving systems commonly
handle prediction and planning as sequential separate tasks. While this
accounts for the influence of surrounding traffic on the ego-vehicle, it fails
to anticipate the reactions of traffic participants to the ego-vehicle's
behavior. Recent works suggest that integrating prediction and planning in an
interdependent joint step is necessary to achieve safe, efficient, and
comfortable driving. While various models implement such integrated systems, a
comprehensive overview and theoretical understanding of different principles
are lacking. We systematically review state-of-the-art deep learning-based
prediction, planning, and integrated prediction and planning models. Different
facets of the integration ranging from model architecture and model design to
behavioral aspects are considered and related to each other. Moreover, we
discuss the implications, strengths, and limitations of different integration
methods. By pointing out research gaps, describing relevant future challenges,
and highlighting trends in the research field, we identify promising directions
for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hagedorn_S/0/1/0/all/0/1"&gt;Steffen Hagedorn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hallgarten_M/0/1/0/all/0/1"&gt;Marcel Hallgarten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoll_M/0/1/0/all/0/1"&gt;Martin Stoll&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Condurache_A/0/1/0/all/0/1"&gt;Alexandru Condurache&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Data Generation in Vision-and-Language Navigation. (arXiv:2307.15644v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.15644</id>
        <link href="http://arxiv.org/abs/2307.15644"/>
        <updated>2023-08-12T00:39:31.402Z</updated>
        <summary type="html"><![CDATA[Recent research in language-guided visual navigation has demonstrated a
significant demand for the diversity of traversable environments and the
quantity of supervision for training generalizable agents. To tackle the common
data scarcity issue in existing vision-and-language navigation datasets, we
propose an effective paradigm for generating large-scale data for learning,
which applies 1200+ photo-realistic environments from HM3D and Gibson datasets
and synthesizes 4.9 million instruction trajectory pairs using fully-accessible
resources on the web. Importantly, we investigate the influence of each
component in this paradigm on the agent's performance and study how to
adequately apply the augmented data to pre-train and fine-tune an agent. Thanks
to our large-scale dataset, the performance of an existing agent can be pushed
up (+11% absolute with regard to previous SoTA) to a significantly new best of
80% single-run success rate on the R2R test split by simple imitation learning.
The long-lasting generalization gap between navigating in seen and unseen
environments is also reduced to less than 1% (versus 8% in the previous best
method). Moreover, our paradigm also facilitates different models to achieve
new state-of-the-art navigation results on CVDN, REVERIE, and R2R in continuous
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jialu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Yicong Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1"&gt;Stephen Gould&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1"&gt;Hao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Representations for Spatio-Temporal Visual Attention Modeling and Understanding. (arXiv:2308.05189v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.05189</id>
        <link href="http://arxiv.org/abs/2308.05189"/>
        <updated>2023-08-12T00:39:31.401Z</updated>
        <summary type="html"><![CDATA[This PhD. Thesis concerns the study and development of hierarchical
representations for spatio-temporal visual attention modeling and understanding
in video sequences. More specifically, we propose two computational models for
visual attention. First, we present a generative probabilistic model for
context-aware visual attention modeling and understanding. Secondly, we develop
a deep network architecture for visual attention modeling, which first
estimates top-down spatio-temporal visual attention, and ultimately serves for
modeling attention in the temporal domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_Torres_M/0/1/0/all/0/1"&gt;Miguel-&amp;#xc1;ngel Fern&amp;#xe1;ndez-Torres&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Models Matter: The Impact of Single-Step Retrosynthesis on Synthesis Planning. (arXiv:2308.05522v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2308.05522</id>
        <link href="http://arxiv.org/abs/2308.05522"/>
        <updated>2023-08-12T00:39:31.396Z</updated>
        <summary type="html"><![CDATA[Retrosynthesis consists of breaking down a chemical compound recursively
step-by-step into molecular precursors until a set of commercially available
molecules is found with the goal to provide a synthesis route. Its two primary
research directions, single-step retrosynthesis prediction, which models the
chemical reaction logic, and multi-step synthesis planning, which tries to find
the correct sequence of reactions, are inherently intertwined. Still, this
connection is not reflected in contemporary research. In this work, we combine
these two major research directions by applying multiple single-step
retrosynthesis models within multi-step synthesis planning and analyzing their
impact using public and proprietary reaction data. We find a disconnection
between high single-step performance and potential route-finding success,
suggesting that single-step models must be evaluated within synthesis planning
in the future. Furthermore, we show that the commonly used single-step
retrosynthesis benchmark dataset USPTO-50k is insufficient as this evaluation
task does not represent model performance and scalability on larger and more
diverse datasets. For multi-step synthesis planning, we show that the choice of
the single-step model can improve the overall success rate of synthesis
planning by up to +28% compared to the commonly used baseline model. Finally,
we show that each single-step model finds unique synthesis routes, and differs
in aspects such as route-finding success, the number of found synthesis routes,
and chemical validity, making the combination of single-step retrosynthesis
prediction and multi-step synthesis planning a crucial aspect when developing
future methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Torren_Peraire_P/0/1/0/all/0/1"&gt;Paula Torren-Peraire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassen_A/0/1/0/all/0/1"&gt;Alan Kai Hassen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Genheden_S/0/1/0/all/0/1"&gt;Samuel Genheden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verhoeven_J/0/1/0/all/0/1"&gt;Jonas Verhoeven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clevert_D/0/1/0/all/0/1"&gt;Djork-Arne Clevert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preuss_M/0/1/0/all/0/1"&gt;Mike Preuss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tetko_I/0/1/0/all/0/1"&gt;Igor Tetko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From NeurODEs to AutoencODEs: a mean-field control framework for width-varying Neural Networks. (arXiv:2307.02279v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.02279</id>
        <link href="http://arxiv.org/abs/2307.02279"/>
        <updated>2023-08-12T00:39:31.390Z</updated>
        <summary type="html"><![CDATA[The connection between Residual Neural Networks (ResNets) and continuous-time
control systems (known as NeurODEs) has led to a mathematical analysis of
neural networks which has provided interesting results of both theoretical and
practical significance. However, by construction, NeurODEs have been limited to
describing constant-width layers, making them unsuitable for modeling deep
learning architectures with layers of variable width. In this paper, we propose
a continuous-time Autoencoder, which we call AutoencODE, based on a
modification of the controlled field that drives the dynamics. This adaptation
enables the extension of the mean-field control framework originally devised
for conventional NeurODEs. In this setting, we tackle the case of low Tikhonov
regularization, resulting in potentially non-convex cost landscapes. While the
global results obtained for high Tikhonov regularization may not hold globally,
we show that many of them can be recovered in regions where the loss function
is locally convex. Inspired by our theoretical findings, we develop a training
method tailored to this specific type of Autoencoders with residual
connections, and we validate our approach through numerical experiments
conducted on various examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Cipriani_C/0/1/0/all/0/1"&gt;Cristina Cipriani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Fornasier_M/0/1/0/all/0/1"&gt;Massimo Fornasier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Scagliotti_A/0/1/0/all/0/1"&gt;Alessandro Scagliotti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking and Analyzing Robust Point Cloud Recognition: Bag of Tricks for Defending Adversarial Examples. (arXiv:2307.16361v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.16361</id>
        <link href="http://arxiv.org/abs/2307.16361"/>
        <updated>2023-08-12T00:39:31.371Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) for 3D point cloud recognition are vulnerable to
adversarial examples, threatening their practical deployment. Despite the many
research endeavors have been made to tackle this issue in recent years, the
diversity of adversarial examples on 3D point clouds makes them more
challenging to defend against than those on 2D images. For examples, attackers
can generate adversarial examples by adding, shifting, or removing points.
Consequently, existing defense strategies are hard to counter unseen point
cloud adversarial examples. In this paper, we first establish a comprehensive,
and rigorous point cloud adversarial robustness benchmark to evaluate
adversarial robustness, which can provide a detailed understanding of the
effects of the defense and attack methods. We then collect existing defense
tricks in point cloud adversarial defenses and then perform extensive and
systematic experiments to identify an effective combination of these tricks.
Furthermore, we propose a hybrid training augmentation methods that consider
various types of point cloud adversarial examples to adversarial training,
significantly improving the adversarial robustness. By combining these tricks,
we construct a more robust defense framework achieving an average accuracy of
83.45\% against various attacks, demonstrating its capability to enabling
robust learners. Our codebase are open-sourced on:
\url{https://github.com/qiufan319/benchmark_pc_attack.git}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Q/0/1/0/all/0/1"&gt;Qiufan Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1"&gt;Cong Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shengshan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yingying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lichao Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding Already Debunked Narratives via Multistage Retrieval: Enabling Cross-Lingual, Cross-Dataset and Zero-Shot Learning. (arXiv:2308.05680v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2308.05680</id>
        <link href="http://arxiv.org/abs/2308.05680"/>
        <updated>2023-08-12T00:39:31.365Z</updated>
        <summary type="html"><![CDATA[The task of retrieving already debunked narratives aims to detect stories
that have already been fact-checked. The successful detection of claims that
have already been debunked not only reduces the manual efforts of professional
fact-checkers but can also contribute to slowing the spread of misinformation.
Mainly due to the lack of readily available data, this is an understudied
problem, particularly when considering the cross-lingual task, i.e. the
retrieval of fact-checking articles in a language different from the language
of the online post being checked. This paper fills this gap by (i) creating a
novel dataset to enable research on cross-lingual retrieval of already debunked
narratives, using tweets as queries to a database of fact-checking articles;
(ii) presenting an extensive experiment to benchmark fine-tuned and
off-the-shelf multilingual pre-trained Transformer models for this task; and
(iii) proposing a novel multistage framework that divides this cross-lingual
debunk retrieval task into refinement and re-ranking stages. Results show that
the task of cross-lingual retrieval of already debunked narratives is
challenging and off-the-shelf Transformer models fail to outperform a strong
lexical-based baseline (BM25). Nevertheless, our multistage retrieval framework
is robust, outperforming BM25 in most scenarios and enabling cross-domain and
zero-shot learning, without significantly harming the model's performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1"&gt;Iknoor Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1"&gt;Carolina Scarton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xingyi Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1"&gt;Kalina Bontcheva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SegMatch: A semi-supervised learning method for surgical instrument segmentation. (arXiv:2308.05232v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.05232</id>
        <link href="http://arxiv.org/abs/2308.05232"/>
        <updated>2023-08-12T00:39:31.358Z</updated>
        <summary type="html"><![CDATA[Surgical instrument segmentation is recognised as a key enabler to provide
advanced surgical assistance and improve computer assisted interventions. In
this work, we propose SegMatch, a semi supervised learning method to reduce the
need for expensive annotation for laparoscopic and robotic surgical images.
SegMatch builds on FixMatch, a widespread semi supervised classification
pipeline combining consistency regularization and pseudo labelling, and adapts
it for the purpose of segmentation. In our proposed SegMatch, the unlabelled
images are weakly augmented and fed into the segmentation model to generate a
pseudo-label to enforce the unsupervised loss against the output of the model
for the adversarial augmented image on the pixels with a high confidence score.
Our adaptation for segmentation tasks includes carefully considering the
equivariance and invariance properties of the augmentation functions we rely
on. To increase the relevance of our augmentations, we depart from using only
handcrafted augmentations and introduce a trainable adversarial augmentation
strategy. Our algorithm was evaluated on the MICCAI Instrument Segmentation
Challenge datasets Robust-MIS 2019 and EndoVis 2017. Our results demonstrate
that adding unlabelled data for training purposes allows us to surpass the
performance of fully supervised approaches which are limited by the
availability of training data in these challenges. SegMatch also outperforms a
range of state-of-the-art semi-supervised learning semantic segmentation models
in different labelled to unlabelled data ratios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1"&gt;Meng Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Budd_C/0/1/0/all/0/1"&gt;Charlie Budd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_Peraza_Herrera_L/0/1/0/all/0/1"&gt;Luis C. Garcia-Peraza-Herrera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dorent_R/0/1/0/all/0/1"&gt;Reuben Dorent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1"&gt;Miaojing Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1"&gt;Tom Vercauteren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Pedestrian Trajectory Prediction Methods for the Application in Autonomous Driving. (arXiv:2308.05194v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.05194</id>
        <link href="http://arxiv.org/abs/2308.05194"/>
        <updated>2023-08-12T00:39:31.350Z</updated>
        <summary type="html"><![CDATA[In this paper, the state of the art in the field of pedestrian trajectory
prediction is evaluated alongside the constant velocity model (CVM) with
respect to its applicability in autonomous vehicles. The evaluation is
conducted on the widely-used ETH/UCY dataset where the Average Displacement
Error (ADE) and the Final Displacement Error (FDE) are reported. To align with
requirements in real-world applications, modifications are made to the input
features of the initially proposed models. An ablation study is conducted to
examine the influence of the observed motion history on the prediction
performance, thereby establishing a better understanding of its impact.
Additionally, the inference time of each model is measured to evaluate the
scalability of each model when confronted with varying amounts of agents. The
results demonstrate that simple models remain competitive when generating
single trajectories, and certain features commonly thought of as useful have
little impact on the overall performance across different architectures. Based
on these findings, recommendations are proposed to guide the future development
of trajectory prediction algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Uhlemann_N/0/1/0/all/0/1"&gt;Nico Uhlemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fent_F/0/1/0/all/0/1"&gt;Felix Fent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lienkamp_M/0/1/0/all/0/1"&gt;Markus Lienkamp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Variational Inference for Large Skew-t Copulas with Application to Intraday Equity Returns. (arXiv:2308.05564v1 [econ.EM])]]></title>
        <id>http://arxiv.org/abs/2308.05564</id>
        <link href="http://arxiv.org/abs/2308.05564"/>
        <updated>2023-08-12T00:39:31.343Z</updated>
        <summary type="html"><![CDATA[Large skew-t factor copula models are attractive for the modeling of
financial data because they allow for asymmetric and extreme tail dependence.
We show that the copula implicit in the skew-t distribution of Azzalini and
Capitanio (2003) allows for a higher level of pairwise asymmetric dependence
than two popular alternative skew-t copulas. Estimation of this copula in high
dimensions is challenging, and we propose a fast and accurate Bayesian
variational inference (VI) approach to do so. The method uses a conditionally
Gaussian generative representation of the skew-t distribution to define an
augmented posterior that can be approximated accurately. A fast stochastic
gradient ascent algorithm is used to solve the variational optimization. The
new methodology is used to estimate copula models for intraday returns from
2017 to 2021 on 93 U.S. equities. The copula captures substantial heterogeneity
in asymmetric dependence over equity pairs, in addition to the variability in
pairwise correlations. We show that intraday predictive densities from the
skew-t copula are more accurate than from some other copula models, while
portfolio selection strategies based on the estimated pairwise tail
dependencies improve performance relative to the benchmark index.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Deng_L/0/1/0/all/0/1"&gt;Lin Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Smith_M/0/1/0/all/0/1"&gt;Michael Stanley Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Maneesoonthorn_W/0/1/0/all/0/1"&gt;Worapree Maneesoonthorn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy-Aware Compression for Federated Learning Through Numerical Mechanism Design. (arXiv:2211.03942v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.03942</id>
        <link href="http://arxiv.org/abs/2211.03942"/>
        <updated>2023-08-12T00:39:31.321Z</updated>
        <summary type="html"><![CDATA[In private federated learning (FL), a server aggregates differentially
private updates from a large number of clients in order to train a machine
learning model. The main challenge in this setting is balancing privacy with
both classification accuracy of the learnt model as well as the number of bits
communicated between the clients and server. Prior work has achieved a good
trade-off by designing a privacy-aware compression mechanism, called the
minimum variance unbiased (MVU) mechanism, that numerically solves an
optimization problem to determine the parameters of the mechanism. This paper
builds upon it by introducing a new interpolation procedure in the numerical
design process that allows for a far more efficient privacy analysis. The
result is the new Interpolated MVU mechanism that is more scalable, has a
better privacy-utility trade-off, and provides SOTA results on
communication-efficient private FL on a variety of datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1"&gt;Chuan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1"&gt;Kamalika Chaudhuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stock_P/0/1/0/all/0/1"&gt;Pierre Stock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1"&gt;Mike Rabbat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Updating Clinical Risk Stratification Models Using Rank-Based Compatibility: Approaches for Evaluating and Optimizing Clinician-Model Team Performance. (arXiv:2308.05619v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2308.05619</id>
        <link href="http://arxiv.org/abs/2308.05619"/>
        <updated>2023-08-12T00:39:31.313Z</updated>
        <summary type="html"><![CDATA[As data shift or new data become available, updating clinical machine
learning models may be necessary to maintain or improve performance over time.
However, updating a model can introduce compatibility issues when the behavior
of the updated model does not align with user expectations, resulting in poor
user-model team performance. Existing compatibility measures depend on model
decision thresholds, limiting their applicability in settings where models are
used to generate rankings based on estimated risk. To address this limitation,
we propose a novel rank-based compatibility measure, $C^R$, and a new loss
function that aims to optimize discriminative performance while encouraging
good compatibility. Applied to a case study in mortality risk stratification
leveraging data from MIMIC, our approach yields more compatible models while
maintaining discriminative performance compared to existing model selection
techniques, with an increase in $C^R$ of $0.019$ ($95\%$ confidence interval:
$0.005$, $0.035$). This work provides new tools to analyze and update risk
stratification models used in clinical care.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Otles_E/0/1/0/all/0/1"&gt;Erkin &amp;#xd6;tle&amp;#x15f;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Denton_B/0/1/0/all/0/1"&gt;Brian T. Denton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wiens_J/0/1/0/all/0/1"&gt;Jenna Wiens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structure in Reinforcement Learning: A Survey and Open Problems. (arXiv:2306.16021v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.16021</id>
        <link href="http://arxiv.org/abs/2306.16021"/>
        <updated>2023-08-12T00:39:31.307Z</updated>
        <summary type="html"><![CDATA[Reinforcement Learning (RL), bolstered by the expressive capabilities of Deep
Neural Networks (DNNs) for function approximation, has demonstrated
considerable success in numerous applications. However, its practicality in
addressing various real-world scenarios, characterized by diverse and
unpredictable dynamics, noisy signals, and large state and action spaces,
remains limited. This limitation stems from issues such as poor data
efficiency, limited generalization capabilities, a lack of safety guarantees,
and the absence of interpretability, among other factors. To overcome these
challenges and improve performance across these crucial metrics, one promising
avenue is to incorporate additional structural information about the problem
into the RL learning process. Various sub-fields of RL have proposed methods
for incorporating such inductive biases. We amalgamate these diverse
methodologies under a unified framework, shedding light on the role of
structure in the learning problem, and classify these methods into distinct
patterns of incorporating structure. By leveraging this comprehensive
framework, we provide valuable insights into the challenges of structured RL
and lay the groundwork for a design pattern perspective on RL research. This
novel perspective paves the way for future advancements and aids in developing
more effective and efficient RL algorithms that can potentially handle
real-world scenarios better.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_A/0/1/0/all/0/1"&gt;Aditya Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Amy Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1"&gt;Marius Lindauer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Performance of Feedforward and Convolutional Neural Networks through Dynamic Activation Functions. (arXiv:2308.05724v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.05724</id>
        <link href="http://arxiv.org/abs/2308.05724"/>
        <updated>2023-08-12T00:39:31.237Z</updated>
        <summary type="html"><![CDATA[Deep learning training training algorithms are a huge success in recent years
in many fields including speech, text,image video etc. Deeper and deeper layers
are proposed with huge success with resnet structures having around 152 layers.
Shallow convolution neural networks(CNN's) are still an active research, where
some phenomena are still unexplained. Activation functions used in the network
are of utmost importance, as they provide non linearity to the networks. Relu's
are the most commonly used activation function.We show a complex piece-wise
linear(PWL) activation in the hidden layer. We show that these PWL activations
work much better than relu activations in our networks for convolution neural
networks and multilayer perceptrons. Result comparison in PyTorch for shallow
and deep CNNs are given to further strengthen our case.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rane_C/0/1/0/all/0/1"&gt;Chinmay Rane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tyagi_K/0/1/0/all/0/1"&gt;Kanishka Tyagi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manry_M/0/1/0/all/0/1"&gt;Michael Manry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EXPRESSO: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis. (arXiv:2308.05725v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2308.05725</id>
        <link href="http://arxiv.org/abs/2308.05725"/>
        <updated>2023-08-12T00:39:31.237Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that it is possible to resynthesize high-quality speech
based, not on text, but on low bitrate discrete units that have been learned in
a self-supervised fashion and can therefore capture expressive aspects of
speech that are hard to transcribe (prosody, voice styles, non-verbal
vocalization). The adoption of these methods is still limited by the fact that
most speech synthesis datasets are read, severely limiting spontaneity and
expressivity. Here, we introduce Expresso, a high-quality expressive speech
dataset for textless speech synthesis that includes both read speech and
improvised dialogues rendered in 26 spontaneous expressive styles. We
illustrate the challenges and potentials of this dataset with an expressive
resynthesis benchmark where the task is to encode the input in low-bitrate
units and resynthesize it in a target voice while preserving content and style.
We evaluate resynthesis quality with automatic metrics for different
self-supervised discrete encoders, and explore tradeoffs between quality,
bitrate and invariance to speaker and style. All the dataset, evaluation
metrics and baseline models are open source]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Tu Anh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Wei-Ning Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DAvirro_A/0/1/0/all/0/1"&gt;Antony D&amp;#x27;Avirro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1"&gt;Bowen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1"&gt;Itai Gat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazel_Zarani_M/0/1/0/all/0/1"&gt;Maryam Fazel-Zarani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1"&gt;Tal Remez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1"&gt;Jade Copet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1"&gt;Gabriel Synnaeve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassid_M/0/1/0/all/0/1"&gt;Michael Hassid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreuk_F/0/1/0/all/0/1"&gt;Felix Kreuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1"&gt;Yossi Adi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1"&gt;Emmanuel Dupoux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-metrics adaptively identifies backdoors in Federated learning. (arXiv:2303.06601v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.06601</id>
        <link href="http://arxiv.org/abs/2303.06601"/>
        <updated>2023-08-12T00:39:31.236Z</updated>
        <summary type="html"><![CDATA[The decentralized and privacy-preserving nature of federated learning (FL)
makes it vulnerable to backdoor attacks aiming to manipulate the behavior of
the resulting model on specific adversary-chosen inputs. However, most existing
defenses based on statistical differences take effect only against specific
attacks, especially when the malicious gradients are similar to benign ones or
the data are highly non-independent and identically distributed (non-IID). In
this paper, we revisit the distance-based defense methods and discover that i)
Euclidean distance becomes meaningless in high dimensions and ii) malicious
gradients with diverse characteristics cannot be identified by a single metric.
To this end, we present a simple yet effective defense strategy with
multi-metrics and dynamic weighting to identify backdoors adaptively.
Furthermore, our novel defense has no reliance on predefined assumptions over
attack settings or data distributions and little impact on benign performance.
To evaluate the effectiveness of our approach, we conduct comprehensive
experiments on different datasets under various attack settings, where our
method achieves the best defensive performance. For instance, we achieve the
lowest backdoor accuracy of 3.06% under the difficult Edge-case PGD, showing
significant superiority over previous defenses. The results also demonstrate
that our method can be well-adapted to a wide range of non-IID degrees without
sacrificing the benign performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Siquan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yijiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1"&gt;Leyu Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Ying Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-heterogeneity Graph Few-shot Learning. (arXiv:2308.05275v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.05275</id>
        <link href="http://arxiv.org/abs/2308.05275"/>
        <updated>2023-08-12T00:39:31.235Z</updated>
        <summary type="html"><![CDATA[In recent years, heterogeneous graph few-shot learning has been proposed to
address the label sparsity issue in heterogeneous graphs (HGs), which contain
various types of nodes and edges. The existing methods have achieved good
performance by transferring generalized knowledge extracted from rich-labeled
classes in source HG(s) to few-labeled classes in a target HG. However, these
methods only consider the single-heterogeneity scenario where the source and
target HGs share a fixed set of node/edge types, ignoring the more general
scenario of cross-heterogeneity, where each HG can have a different and
non-fixed set of node/edge types. To this end, we focus on the unexplored
cross-heterogeneity scenario and propose a novel model for Cross-heterogeneity
Graph Few-shot Learning, namely CGFL. In CGFL, we first extract meta-patterns
to capture heterogeneous information and propose a multi-view heterogeneous
graph neural network (MHGN) to learn meta-patterns across HGs. Then, we propose
a score module to measure the informativeness of labeled samples and determine
the transferability of each source HG. Finally, by integrating MHGN and the
score module into a meta-learning mechanism, CGFL can effectively transfer
generalized knowledge to predict new classes with few-labeled data. Extensive
experiments on four real-world datasets have demonstrated the superior
performance of CGFL over the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1"&gt;Pengfei Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guanfeng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Byzantine-Robust Decentralized Stochastic Optimization with Stochastic Gradient Noise-Independent Learning Error. (arXiv:2308.05292v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.05292</id>
        <link href="http://arxiv.org/abs/2308.05292"/>
        <updated>2023-08-12T00:39:31.235Z</updated>
        <summary type="html"><![CDATA[This paper studies Byzantine-robust stochastic optimization over a
decentralized network, where every agent periodically communicates with its
neighbors to exchange local models, and then updates its own local model by
stochastic gradient descent (SGD). The performance of such a method is affected
by an unknown number of Byzantine agents, which conduct adversarially during
the optimization process. To the best of our knowledge, there is no existing
work that simultaneously achieves a linear convergence speed and a small
learning error. We observe that the learning error is largely dependent on the
intrinsic stochastic gradient noise. Motivated by this observation, we
introduce two variance reduction methods, stochastic average gradient algorithm
(SAGA) and loopless stochastic variance-reduced gradient (LSVRG), to
Byzantine-robust decentralized stochastic optimization for eliminating the
negative effect of the stochastic gradient noise. The two resulting methods,
BRAVO-SAGA and BRAVO-LSVRG, enjoy both linear convergence speeds and stochastic
gradient noise-independent learning errors. Such learning errors are optimal
for a class of methods based on total variation (TV)-norm regularization and
stochastic subgradient update. We conduct extensive numerical experiments to
demonstrate their effectiveness under various Byzantine attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jie Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weiyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_Q/0/1/0/all/0/1"&gt;Qing Ling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Brief Review of Hypernetworks in Deep Learning. (arXiv:2306.06955v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.06955</id>
        <link href="http://arxiv.org/abs/2306.06955"/>
        <updated>2023-08-12T00:39:31.153Z</updated>
        <summary type="html"><![CDATA[Hypernetworks, or hypernets in short, are neural networks that generate
weights for another neural network, known as the target network. They have
emerged as a powerful deep learning technique that allows for greater
flexibility, adaptability, dynamism, faster training, information sharing, and
model compression etc. Hypernets have shown promising results in a variety of
deep learning problems, including continual learning, causal inference,
transfer learning, weight pruning, uncertainty quantification, zero-shot
learning, natural language processing, and reinforcement learning etc. Despite
their success across different problem settings, currently, there is no review
available to inform the researchers about the developments and to help in
utilizing hypernets. To fill this gap, we review the progress in hypernets. We
present an illustrative example to train deep neural networks using hypernets
and propose categorizing hypernets based on five design criteria as inputs,
outputs, variability of inputs and outputs, and architecture of hypernets. We
also review applications of hypernets across different deep learning problem
settings, followed by a discussion of general scenarios where hypernets can be
effectively employed. Finally, we discuss the challenges and future directions
that remain under-explored in the field of hypernets. We believe that
hypernetworks have the potential to revolutionize the field of deep learning.
They offer a new way to design and train neural networks, and they have the
potential to improve the performance of deep learning models on a variety of
tasks. Through this review, we aim to inspire further advancements in deep
learning through hypernetworks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chauhan_V/0/1/0/all/0/1"&gt;Vinod Kumar Chauhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jiandong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1"&gt;Ping Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Molaei_S/0/1/0/all/0/1"&gt;Soheila Molaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1"&gt;David A. Clifton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling. (arXiv:2308.04365v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.04365</id>
        <link href="http://arxiv.org/abs/2308.04365"/>
        <updated>2023-08-12T00:39:31.153Z</updated>
        <summary type="html"><![CDATA[Causal inference is a crucial goal of science, enabling researchers to arrive
at meaningful conclusions regarding the predictions of hypothetical
interventions using observational data. Path models, Structural Equation Models
(SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means to
unambiguously specify assumptions regarding the causal structure underlying a
phenomenon. Unlike DAGs, which make very few assumptions about the functional
and parametric form, SEM assumes linearity. This can result in functional
misspecification which prevents researchers from undertaking reliable effect
size estimation. In contrast, we propose Super Learner Equation Modeling, a
path modeling technique integrating machine learning Super Learner ensembles.
We empirically demonstrate its ability to provide consistent and unbiased
estimates of causal effects, its competitive performance for linear models when
compared with SEM, and highlight its superiority over SEM when dealing with
non-linear relationships. We provide open-source code, and a tutorial notebook
with example usage, accentuating the easy-to-use nature of the method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vowels_M/0/1/0/all/0/1"&gt;Matthew J. Vowels&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling. (arXiv:2308.04365v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.04365</id>
        <link href="http://arxiv.org/abs/2308.04365"/>
        <updated>2023-08-12T00:39:31.043Z</updated>
        <summary type="html"><![CDATA[Causal inference is a crucial goal of science, enabling researchers to arrive
at meaningful conclusions regarding the predictions of hypothetical
interventions using observational data. Path models, Structural Equation Models
(SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means to
unambiguously specify assumptions regarding the causal structure underlying a
phenomenon. Unlike DAGs, which make very few assumptions about the functional
and parametric form, SEM assumes linearity. This can result in functional
misspecification which prevents researchers from undertaking reliable effect
size estimation. In contrast, we propose Super Learner Equation Modeling, a
path modeling technique integrating machine learning Super Learner ensembles.
We empirically demonstrate its ability to provide consistent and unbiased
estimates of causal effects, its competitive performance for linear models when
compared with SEM, and highlight its superiority over SEM when dealing with
non-linear relationships. We provide open-source code, and a tutorial notebook
with example usage, accentuating the easy-to-use nature of the method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vowels_M/0/1/0/all/0/1"&gt;Matthew J. Vowels&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Extraction of Relevant Road Infrastructure using Connected vehicle data and Deep Learning Model. (arXiv:2308.05658v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2308.05658</id>
        <link href="http://arxiv.org/abs/2308.05658"/>
        <updated>2023-08-12T00:39:31.039Z</updated>
        <summary type="html"><![CDATA[In today's rapidly evolving urban landscapes, efficient and accurate mapping
of road infrastructure is critical for optimizing transportation systems,
enhancing road safety, and improving the overall mobility experience for
drivers and commuters. Yet, a formidable bottleneck obstructs progress - the
laborious and time-intensive manual identification of intersections. Simply
considering the shear number of intersections that need to be identified, and
the labor hours required per intersection, the need for an automated solution
becomes undeniable. To address this challenge, we propose a novel approach that
leverages connected vehicle data and cutting-edge deep learning techniques. By
employing geohashing to segment vehicle trajectories and then generating image
representations of road segments, we utilize the YOLOv5 (You Only Look Once
version 5) algorithm for accurate classification of both straight road segments
and intersections. Experimental results demonstrate an impressive overall
classification accuracy of 95%, with straight roads achieving a remarkable 97%
F1 score and intersections reaching a 90% F1 score. This approach not only
saves time and resources but also enables more frequent updates and a
comprehensive understanding of the road network. Our research showcases the
potential impact on traffic management, urban planning, and autonomous vehicle
navigation systems. The fusion of connected vehicle data and deep learning
models holds promise for a transformative shift in road infrastructure mapping,
propelling us towards a smarter, safer, and more connected transportation
ecosystem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kojo_A/0/1/0/all/0/1"&gt;Adu-Gyamfi Kojo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raghupathi_K/0/1/0/all/0/1"&gt;Kandiboina Raghupathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varsha_R/0/1/0/all/0/1"&gt;Ravichandra-Mouli Varsha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skylar_K/0/1/0/all/0/1"&gt;Knickerbocker Skylar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+N_H/0/1/0/all/0/1"&gt;Hans Zachary N&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hawkins/0/1/0/all/0/1"&gt;Hawkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+R_N/0/1/0/all/0/1"&gt;Neal R&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anuj_S/0/1/0/all/0/1"&gt;Sharma Anuj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A survey of some recent developments in measures of association. (arXiv:2211.04702v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.04702</id>
        <link href="http://arxiv.org/abs/2211.04702"/>
        <updated>2023-08-12T00:39:31.039Z</updated>
        <summary type="html"><![CDATA[This paper surveys some recent developments in measures of association
related to a new coefficient of correlation introduced by the author. A
straightforward extension of this coefficient to standard Borel spaces (which
includes all Polish spaces), overlooked in the literature so far, is proposed
at the end of the survey.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chatterjee_S/0/1/0/all/0/1"&gt;Sourav Chatterjee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Progressive Meshes. (arXiv:2308.05741v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.05741</id>
        <link href="http://arxiv.org/abs/2308.05741"/>
        <updated>2023-08-12T00:39:31.038Z</updated>
        <summary type="html"><![CDATA[The recent proliferation of 3D content that can be consumed on hand-held
devices necessitates efficient tools for transmitting large geometric data,
e.g., 3D meshes, over the Internet. Detailed high-resolution assets can pose a
challenge to storage as well as transmission bandwidth, and level-of-detail
techniques are often used to transmit an asset using an appropriate bandwidth
budget. It is especially desirable for these methods to transmit data
progressively, improving the quality of the geometry with more data. Our key
insight is that the geometric details of 3D meshes often exhibit similar local
patterns even across different shapes, and thus can be effectively represented
with a shared learned generative space. We learn this space using a
subdivision-based encoder-decoder architecture trained in advance on a large
collection of surfaces. We further observe that additional residual features
can be transmitted progressively between intermediate levels of subdivision
that enable the client to control the tradeoff between bandwidth cost and
quality of reconstruction, providing a neural progressive mesh representation.
We evaluate our method on a diverse set of complex 3D shapes and demonstrate
that it outperforms baselines in terms of compression ratio and reconstruction
quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yun-Chun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1"&gt;Vladimir G. Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1"&gt;Noam Aigerman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacobson_A/0/1/0/all/0/1"&gt;Alec Jacobson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simplifying Momentum-based Positive-definite Submanifold Optimization with Applications to Deep Learning. (arXiv:2302.09738v7 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.09738</id>
        <link href="http://arxiv.org/abs/2302.09738"/>
        <updated>2023-08-12T00:39:31.038Z</updated>
        <summary type="html"><![CDATA[Riemannian submanifold optimization with momentum is computationally
challenging because, to ensure that the iterates remain on the submanifold, we
often need to solve difficult differential equations. Here, we simplify such
difficulties for a class of sparse or structured symmetric positive-definite
matrices with the affine-invariant metric. We do so by proposing a generalized
version of the Riemannian normal coordinates that dynamically orthonormalizes
the metric and locally converts the problem into an unconstrained problem in
the Euclidean space. We use our approach to simplify existing approaches for
structured covariances and develop matrix-inverse-free $2^\text{nd}$-order
optimizers for deep learning with low precision by using only matrix
multiplications. Code: https://github.com/yorkerlin/StructuredNGD-DL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Duruisseaux_V/0/1/0/all/0/1"&gt;Valentin Duruisseaux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Leok_M/0/1/0/all/0/1"&gt;Melvin Leok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nielsen_F/0/1/0/all/0/1"&gt;Frank Nielsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Emtiyaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1"&gt;Mark Schmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Normalized Gradients for All. (arXiv:2308.05621v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.05621</id>
        <link href="http://arxiv.org/abs/2308.05621"/>
        <updated>2023-08-12T00:39:31.035Z</updated>
        <summary type="html"><![CDATA[In this short note, I show how to adapt to H\"{o}lder smoothness using
normalized gradients in a black-box way. Moreover, the bound will depend on a
novel notion of local H\"{o}lder smoothness. The main idea directly comes from
Levy [2017].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Orabona_F/0/1/0/all/0/1"&gt;Francesco Orabona&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Diffusion Models for Radio Wireless Channel Modelling and Sampling. (arXiv:2308.05583v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2308.05583</id>
        <link href="http://arxiv.org/abs/2308.05583"/>
        <updated>2023-08-12T00:39:30.977Z</updated>
        <summary type="html"><![CDATA[Channel modelling is essential to designing modern wireless communication
systems. The increasing complexity of channel modelling and the cost of
collecting high-quality wireless channel data have become major challenges. In
this paper, we propose a diffusion model based channel sampling approach for
rapidly synthesizing channel realizations from limited data. We use a diffusion
model with a U Net based architecture operating in the frequency space domain.
To evaluate how well the proposed model reproduces the true distribution of
channels in the training dataset, two evaluation metrics are used: $i)$ the
approximate $2$-Wasserstein distance between real and generated distributions
of the normalized power spectrum in the antenna and frequency domains and $ii)$
precision and recall metric for distributions. We show that, compared to
existing GAN based approaches which suffer from mode collapse and unstable
training, our diffusion based approach trains stably and generates diverse and
high-fidelity samples from the true channel distribution. We also show that we
can pretrain the model on a simulated urban macro-cellular channel dataset and
fine-tune it on a smaller, out-of-distribution urban micro-cellular dataset,
therefore showing that it is feasible to model real world channels using
limited data with this approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sengupta_U/0/1/0/all/0/1"&gt;Ushnish Sengupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jao_C/0/1/0/all/0/1"&gt;Chinkuo Jao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernacchia_A/0/1/0/all/0/1"&gt;Alberto Bernacchia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vakili_S/0/1/0/all/0/1"&gt;Sattar Vakili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shiu_D/0/1/0/all/0/1"&gt;Da-shan Shiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A hybrid deep-learning-metaheuristic framework for bi-level network design problems. (arXiv:2303.06024v3 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.06024</id>
        <link href="http://arxiv.org/abs/2303.06024"/>
        <updated>2023-08-12T00:39:30.955Z</updated>
        <summary type="html"><![CDATA[This study proposes a hybrid deep-learning-metaheuristic framework with a
bi-level architecture for road network design problems (NDPs). We train a graph
neural network (GNN) to approximate the solution of the user equilibrium (UE)
traffic assignment problem and use inferences made by the trained model to
calculate fitness function evaluations of a genetic algorithm (GA) to
approximate solutions for NDPs. Using three test networks, two NDP variants and
an exact solver as benchmark, we show that on average, our proposed framework
can provide solutions within 1.5% gap of the best results in less than 0.5% of
the time used by the exact solution procedure. Our framework can be utilized
within an expert system for infrastructure planning to determine the best
infrastructure planning and management decisions under different scenarios.
Given the flexibility of the framework, it can easily be adapted to many other
decision problems that can be modeled as bi-level problems on graphs. Moreover,
we foreseen interesting future research directions, thus we also put forward a
brief research agenda for this topic. The key observation from our research
that can shape future research is that the fitness function evaluation time
using the inferences made by the GNN model was in the order of milliseconds,
which points to an opportunity and a need for novel heuristics that 1) can cope
well with noisy fitness function values provided by deep learning models, and
2) can use the significantly enlarged efficiency of the evaluation step to
explore the search space effectively (rather than efficiently). This opens a
new avenue for a modern class of metaheuristics that are crafted for use with
AI-powered predictors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Madadi_B/0/1/0/all/0/1"&gt;Bahman Madadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Correia_G/0/1/0/all/0/1"&gt;Goncalo Homem de Almeida Correia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inverse Extended Kalman Filter -- Part II: Highly Non-Linear and Uncertain Systems. (arXiv:2208.06683v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.06683</id>
        <link href="http://arxiv.org/abs/2208.06683"/>
        <updated>2023-08-12T00:39:30.950Z</updated>
        <summary type="html"><![CDATA[Counter-adversarial system design problems have lately motivated the
development of inverse Bayesian filters. For example, inverse Kalman filter
(I-KF) has been recently formulated to estimate the adversary's
Kalman-filter-tracked estimates and hence, predict the adversary's future
steps. The purpose of this paper and the companion paper (Part I) is to address
the inverse filtering problem in non-linear systems by proposing an inverse
extended Kalman filter (I-EKF). The companion paper proposed the theory of
I-EKF (with and without unknown inputs) and I-KF (with unknown inputs). In this
paper, we develop this theory for highly non-linear models, which employ
second-order, Gaussian sum, and dithered forward EKFs. In particular, we derive
theoretical stability guarantees for the inverse second-order EKF using the
bounded non-linearity approach. To address the limitation of the standard
I-EKFs that the system model and forward filter are perfectly known to the
defender, we propose reproducing kernel Hilbert space-based EKF to learn the
unknown system dynamics based on its observations, which can be employed as an
inverse filter to infer the adversary's estimate. Numerical experiments
demonstrate the state estimation performance of the proposed filters using
recursive Cram\'{e}r-Rao lower bound as a benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Singh_H/0/1/0/all/0/1"&gt;Himali Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Chattopadhyay_A/0/1/0/all/0/1"&gt;Arpan Chattopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mishra_K/0/1/0/all/0/1"&gt;Kumar Vijay Mishra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion Denoised Smoothing for Certified and Adversarial Robust Out-Of-Distribution Detection. (arXiv:2303.14961v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.14961</id>
        <link href="http://arxiv.org/abs/2303.14961"/>
        <updated>2023-08-12T00:39:30.900Z</updated>
        <summary type="html"><![CDATA[As the use of machine learning continues to expand, the importance of
ensuring its safety cannot be overstated. A key concern in this regard is the
ability to identify whether a given sample is from the training distribution,
or is an "Out-Of-Distribution" (OOD) sample. In addition, adversaries can
manipulate OOD samples in ways that lead a classifier to make a confident
prediction. In this study, we present a novel approach for certifying the
robustness of OOD detection within a $\ell_2$-norm around the input, regardless
of network architecture and without the need for specific components or
additional training. Further, we improve current techniques for detecting
adversarial attacks on OOD samples, while providing high levels of certified
and adversarial robustness on in-distribution samples. The average of all OOD
detection metrics on CIFAR10/100 shows an increase of $\sim 13 \% / 5\%$
relative to previous approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Franco_N/0/1/0/all/0/1"&gt;Nicola Franco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korth_D/0/1/0/all/0/1"&gt;Daniel Korth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lorenz_J/0/1/0/all/0/1"&gt;Jeanette Miriam Lorenz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roscher_K/0/1/0/all/0/1"&gt;Karsten Roscher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guennemann_S/0/1/0/all/0/1"&gt;Stephan Guennemann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InfoNCE is variational inference in a recognition parameterised model. (arXiv:2107.02495v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02495</id>
        <link href="http://arxiv.org/abs/2107.02495"/>
        <updated>2023-08-12T00:39:30.900Z</updated>
        <summary type="html"><![CDATA[Here, we show that the InfoNCE objective is equivalent to the ELBO in a new
class of probabilistic generative model, the recognition parameterised model
(RPM). When we learn the optimal prior, the RPM ELBO becomes equal to the
mutual information (MI; up to a constant), establishing a connection to
pre-existing self-supervised learning methods such as InfoNCE. However,
practical InfoNCE methods do not use the MI as an objective; the MI is
invariant to arbitrary invertible transformations, so using an MI objective can
lead to highly entangled representations (Tschannen et al., 2019). Instead, the
actual InfoNCE objective is a simplified lower bound on the MI which is loose
even in the infinite sample limit. Thus, an objective that works (i.e. the
actual InfoNCE objective) appears to be motivated as a loose bound on an
objective that does not work (i.e. the true MI which gives arbitrarily
entangled representations). We give an alternative motivation for the actual
InfoNCE objective. In particular, we show that in the infinite sample limit,
and for a particular choice of prior, the actual InfoNCE objective is equal to
the ELBO (up to a constant); and the ELBO is equal to the marginal likelihood
with a deterministic recognition model. Thus, we argue that our VAE perspective
gives a better motivation for InfoNCE than MI, as the actual InfoNCE objective
is only loosely bounded by the MI, but is equal to the ELBO/marginal likelihood
(up to a constant).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1"&gt;Laurence Aitchison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ganev_S/0/1/0/all/0/1"&gt;Stoil Ganev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Selective inference using randomized group lasso estimators for general models. (arXiv:2306.13829v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.13829</id>
        <link href="http://arxiv.org/abs/2306.13829"/>
        <updated>2023-08-12T00:39:30.900Z</updated>
        <summary type="html"><![CDATA[Selective inference methods are developed for group lasso estimators for use
with a wide class of distributions and loss functions. The method includes the
use of exponential family distributions, as well as quasi-likelihood modeling
for overdispersed count data, for example, and allows for categorical or
grouped covariates as well as continuous covariates. A randomized
group-regularized optimization problem is studied. The added randomization
allows us to construct a post-selection likelihood which we show to be adequate
for selective inference when conditioning on the event of the selection of the
grouped covariates. This likelihood also provides a selective point estimator,
accounting for the selection by the group lasso. Confidence regions for the
regression parameters in the selected model take the form of Wald-type regions
and are shown to have bounded volume. The selective inference method for
grouped lasso is illustrated on data from the national health and nutrition
examination survey while simulations showcase its behaviour and favorable
comparison with other methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yiling Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pirenne_S/0/1/0/all/0/1"&gt;Sarah Pirenne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Panigrahi_S/0/1/0/all/0/1"&gt;Snigdha Panigrahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Claeskens_G/0/1/0/all/0/1"&gt;Gerda Claeskens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comparison of Classical and Deep Reinforcement Learning Methods for HVAC Control. (arXiv:2308.05711v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.05711</id>
        <link href="http://arxiv.org/abs/2308.05711"/>
        <updated>2023-08-12T00:39:30.899Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) is a promising approach for optimizing HVAC
control. RL offers a framework for improving system performance, reducing
energy consumption, and enhancing cost efficiency. We benchmark two popular
classical and deep RL methods (Q-Learning and Deep-Q-Networks) across multiple
HVAC environments and explore the practical consideration of model
hyper-parameter selection and reward tuning. The findings provide insight for
configuring RL agents in HVAC systems, promoting energy-efficient and
cost-effective operation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Marshall Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Willes_J/0/1/0/all/0/1"&gt;John Willes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiralerspong_T/0/1/0/all/0/1"&gt;Thomas Jiralerspong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moezzi_M/0/1/0/all/0/1"&gt;Matin Moezzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online learning techniques for prediction of temporal tabular datasets with regime changes. (arXiv:2301.00790v4 [q-fin.CP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2301.00790</id>
        <link href="http://arxiv.org/abs/2301.00790"/>
        <updated>2023-08-12T00:39:30.899Z</updated>
        <summary type="html"><![CDATA[The application of deep learning to non-stationary temporal datasets can lead
to overfitted models that underperform under regime changes. In this work, we
propose a modular machine learning pipeline for ranking predictions on temporal
panel datasets which is robust under regime changes. The modularity of the
pipeline allows the use of different models, including Gradient Boosting
Decision Trees (GBDTs) and Neural Networks, with and without feature
engineering. We evaluate our framework on financial data for stock portfolio
prediction, and find that GBDT models with dropout display high performance,
robustness and generalisability with reduced complexity and computational cost.
We then demonstrate how online learning techniques, which require no retraining
of models, can be used post-prediction to enhance the results. First, we show
that dynamic feature projection improves robustness by reducing drawdown in
regime changes. Second, we demonstrate that dynamical model ensembling based on
selection of models with good recent performance leads to improved Sharpe and
Calmar ratios of out-of-sample predictions. We also evaluate the robustness of
our pipeline across different data splits and random seeds with good
reproducibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Wong_T/0/1/0/all/0/1"&gt;Thomas Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Barahona_M/0/1/0/all/0/1"&gt;Mauricio Barahona&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simplifying Momentum-based Positive-definite Submanifold Optimization with Applications to Deep Learning. (arXiv:2302.09738v7 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.09738</id>
        <link href="http://arxiv.org/abs/2302.09738"/>
        <updated>2023-08-12T00:39:30.899Z</updated>
        <summary type="html"><![CDATA[Riemannian submanifold optimization with momentum is computationally
challenging because, to ensure that the iterates remain on the submanifold, we
often need to solve difficult differential equations. Here, we simplify such
difficulties for a class of sparse or structured symmetric positive-definite
matrices with the affine-invariant metric. We do so by proposing a generalized
version of the Riemannian normal coordinates that dynamically orthonormalizes
the metric and locally converts the problem into an unconstrained problem in
the Euclidean space. We use our approach to simplify existing approaches for
structured covariances and develop matrix-inverse-free $2^\text{nd}$-order
optimizers for deep learning with low precision by using only matrix
multiplications. Code: https://github.com/yorkerlin/StructuredNGD-DL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Duruisseaux_V/0/1/0/all/0/1"&gt;Valentin Duruisseaux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Leok_M/0/1/0/all/0/1"&gt;Melvin Leok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nielsen_F/0/1/0/all/0/1"&gt;Frank Nielsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Emtiyaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1"&gt;Mark Schmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Updating Clinical Risk Stratification Models Using Rank-Based Compatibility: Approaches for Evaluating and Optimizing Clinician-Model Team Performance. (arXiv:2308.05619v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2308.05619</id>
        <link href="http://arxiv.org/abs/2308.05619"/>
        <updated>2023-08-12T00:39:30.896Z</updated>
        <summary type="html"><![CDATA[As data shift or new data become available, updating clinical machine
learning models may be necessary to maintain or improve performance over time.
However, updating a model can introduce compatibility issues when the behavior
of the updated model does not align with user expectations, resulting in poor
user-model team performance. Existing compatibility measures depend on model
decision thresholds, limiting their applicability in settings where models are
used to generate rankings based on estimated risk. To address this limitation,
we propose a novel rank-based compatibility measure, $C^R$, and a new loss
function that aims to optimize discriminative performance while encouraging
good compatibility. Applied to a case study in mortality risk stratification
leveraging data from MIMIC, our approach yields more compatible models while
maintaining discriminative performance compared to existing model selection
techniques, with an increase in $C^R$ of $0.019$ ($95\%$ confidence interval:
$0.005$, $0.035$). This work provides new tools to analyze and update risk
stratification models used in clinical care.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Otles_E/0/1/0/all/0/1"&gt;Erkin &amp;#xd6;tle&amp;#x15f;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Denton_B/0/1/0/all/0/1"&gt;Brian T. Denton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wiens_J/0/1/0/all/0/1"&gt;Jenna Wiens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Random Search to Bandit Learning in Metric Measure Spaces. (arXiv:2305.11509v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.11509</id>
        <link href="http://arxiv.org/abs/2305.11509"/>
        <updated>2023-08-12T00:39:30.892Z</updated>
        <summary type="html"><![CDATA[Random Search is one of the most widely-used method for Hyperparameter
Optimization, and is critical to the success of deep learning models. Despite
its astonishing performance, little non-heuristic theory has been developed to
describe the underlying working mechanism. This paper gives a theoretical
accounting of Random Search. We introduce the concept of \emph{scattering
dimension} that describes the landscape of the underlying function, and
quantifies the performance of random search. We show that, when the environment
is noise-free, the output of random search converges to the optimal value in
probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T}
\right)^{ \frac{1}{d_s} } \right) $, where $ d_s \ge 0 $ is the scattering
dimension of the underlying function. When the observed function values are
corrupted by bounded $iid$ noise, the output of random search converges to the
optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left(
\frac{1}{T} \right)^{ \frac{1}{d_s + 1} } \right) $. In addition, based on the
principles of random search, we introduce an algorithm, called BLiN-MOS, for
Lipschitz bandits in doubling metric spaces that are also endowed with a
probability measure, and show that BLiN-MOS achieves a regret rate of order $
\widetilde{\mathcal{O}} \left( T^{ \frac{d_z}{d_z + 1} } \right) $, where $d_z$
is the zooming dimension of the problem instance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chuying Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yasong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianyu Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Functional Neural Networks: Shift invariant models for functional data with applications to EEG classification. (arXiv:2301.05869v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2301.05869</id>
        <link href="http://arxiv.org/abs/2301.05869"/>
        <updated>2023-08-12T00:39:30.884Z</updated>
        <summary type="html"><![CDATA[It is desirable for statistical models to detect signals of interest
independently of their position. If the data is generated by some smooth
process, this additional structure should be taken into account. We introduce a
new class of neural networks that are shift invariant and preserve smoothness
of the data: functional neural networks (FNNs). For this, we use methods from
functional data analysis (FDA) to extend multi-layer perceptrons and
convolutional neural networks to functional data. We propose different model
architectures, show that the models outperform a benchmark model from FDA in
terms of accuracy and successfully use FNNs to classify electroencephalography
(EEG) data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heinrichs_F/0/1/0/all/0/1"&gt;Florian Heinrichs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heim_M/0/1/0/all/0/1"&gt;Mavin Heim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1"&gt;Corinna Weber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Selective Inference for Sparse Multitask Regression with Applications in Neuroimaging. (arXiv:2205.14220v4 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2205.14220</id>
        <link href="http://arxiv.org/abs/2205.14220"/>
        <updated>2023-08-12T00:39:30.883Z</updated>
        <summary type="html"><![CDATA[Multi-task learning is frequently used to model a set of related response
variables from the same set of features, improving predictive performance and
modeling accuracy relative to methods that handle each response variable
separately. Despite the potential of multi-task learning to yield more powerful
inference than single-task alternatives, prior work in this area has largely
omitted uncertainty quantification. Our focus in this paper is a common
multi-task problem in neuroimaging, where the goal is to understand the
relationship between multiple cognitive task scores (or other subject-level
assessments) and brain connectome data collected from imaging. We propose a
framework for selective inference to address this problem, with the flexibility
to: (i) jointly identify the relevant covariates for each task through a
sparsity-inducing penalty, and (ii) conduct valid inference in a model based on
the estimated sparsity structure. Our framework offers a new conditional
procedure for inference, based on a refinement of the selection event that
yields a tractable selection-adjusted likelihood. This gives an approximate
system of estimating equations for maximum likelihood inference, solvable via a
single convex optimization problem, and enables us to efficiently form
confidence intervals with approximately the correct coverage. Applied to both
simulated data and data from the Adolescent Brain Cognitive Development (ABCD)
study, our selective inference methods yield tighter confidence intervals than
commonly used alternatives, such as data splitting. We also demonstrate through
simulations that multi-task learning with selective inference can more
accurately recover true signals than single-task methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Panigrahi_S/0/1/0/all/0/1"&gt;Snigdha Panigrahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Stewart_N/0/1/0/all/0/1"&gt;Natasha Stewart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sripada_C/0/1/0/all/0/1"&gt;Chandra Sekhar Sripada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Levina_E/0/1/0/all/0/1"&gt;Elizaveta Levina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TSLiNGAM: DirectLiNGAM under heavy tails. (arXiv:2308.05422v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2308.05422</id>
        <link href="http://arxiv.org/abs/2308.05422"/>
        <updated>2023-08-12T00:39:30.878Z</updated>
        <summary type="html"><![CDATA[One of the established approaches to causal discovery consists of combining
directed acyclic graphs (DAGs) with structural causal models (SCMs) to describe
the functional dependencies of effects on their causes. Possible
identifiability of SCMs given data depends on assumptions made on the noise
variables and the functional classes in the SCM. For instance, in the LiNGAM
model, the functional class is restricted to linear functions and the
disturbances have to be non-Gaussian.

In this work, we propose TSLiNGAM, a new method for identifying the DAG of a
causal model based on observational data. TSLiNGAM builds on DirectLiNGAM, a
popular algorithm which uses simple OLS regression for identifying causal
directions between variables. TSLiNGAM leverages the non-Gaussianity assumption
of the error terms in the LiNGAM model to obtain more efficient and robust
estimation of the causal structure. TSLiNGAM is justified theoretically and is
studied empirically in an extensive simulation study. It performs significantly
better on heavy-tailed and skewed data and demonstrates a high small-sample
efficiency. In addition, TSLiNGAM also shows better robustness properties as it
is more resilient to contamination.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Leyder_S/0/1/0/all/0/1"&gt;Sarah Leyder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Raymaekers_J/0/1/0/all/0/1"&gt;Jakob Raymaekers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Verdonck_T/0/1/0/all/0/1"&gt;Tim Verdonck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unifying Distributionally Robust Optimization via Optimal Transport Theory. (arXiv:2308.05414v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2308.05414</id>
        <link href="http://arxiv.org/abs/2308.05414"/>
        <updated>2023-08-12T00:39:30.872Z</updated>
        <summary type="html"><![CDATA[In the past few years, there has been considerable interest in two prominent
approaches for Distributionally Robust Optimization (DRO): Divergence-based and
Wasserstein-based methods. The divergence approach models misspecification in
terms of likelihood ratios, while the latter models it through a measure of
distance or cost in actual outcomes. Building upon these advances, this paper
introduces a novel approach that unifies these methods into a single framework
based on optimal transport (OT) with conditional moment constraints. Our
proposed approach, for example, makes it possible for optimal adversarial
distributions to simultaneously perturb likelihood and outcomes, while
producing an optimal (in an optimal transport sense) coupling between the
baseline model and the adversarial model.Additionally, the paper investigates
several duality results and presents tractable reformulations that enhance the
practical applicability of this unified framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Blanchet_J/0/1/0/all/0/1"&gt;Jose Blanchet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kuhn_D/0/1/0/all/0/1"&gt;Daniel Kuhn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiajin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Taskesen_B/0/1/0/all/0/1"&gt;Bahar Taskesen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-graph Spatio-temporal Graph Convolutional Network for Traffic Flow Prediction. (arXiv:2308.05601v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.05601</id>
        <link href="http://arxiv.org/abs/2308.05601"/>
        <updated>2023-08-12T00:39:30.871Z</updated>
        <summary type="html"><![CDATA[Inter-city highway transportation is significant for urban life. As one of
the key functions in intelligent transportation system (ITS), traffic
evaluation always plays significant role nowadays, and daily traffic flow
prediction still faces challenges at network-wide toll stations. On the one
hand, the data imbalance in practice among various locations deteriorates the
performance of prediction. On the other hand, complex correlative
spatio-temporal factors cannot be comprehensively employed in long-term
duration. In this paper, a prediction method is proposed for daily traffic flow
in highway domain through spatio-temporal deep learning. In our method, data
normalization strategy is used to deal with data imbalance, due to long-tail
distribution of traffic flow at network-wide toll stations. And then, based on
graph convolutional network, we construct networks in distinct semantics to
capture spatio-temporal features. Beside that, meteorology and calendar
features are used by our model in the full connection stage to extra external
characteristics of traffic flow. By extensive experiments and case studies in
one Chinese provincial highway, our method shows clear improvement in
predictive accuracy than baselines and practical benefits in business.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Weilong Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianpu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianwu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhuofeng Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLM As DBA. (arXiv:2308.05481v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2308.05481</id>
        <link href="http://arxiv.org/abs/2308.05481"/>
        <updated>2023-08-12T00:39:30.868Z</updated>
        <summary type="html"><![CDATA[Database administrators (DBAs) play a crucial role in managing, maintaining
and optimizing a database system to ensure data availability, performance, and
reliability. However, it is hard and tedious for DBAs to manage a large number
of database instances (e.g., millions of instances on the cloud databases).
Recently large language models (LLMs) have shown great potential to understand
valuable documents and accordingly generate reasonable answers. Thus, we
propose D-Bot, a LLM-based database administrator that can continuously acquire
database maintenance experience from textual sources, and provide reasonable,
well-founded, in-time diagnosis and optimization advice for target databases.
This paper presents a revolutionary LLM-centric framework for database
maintenance, including (i) database maintenance knowledge detection from
documents and tools, (ii) tree of thought reasoning for root cause analysis,
and (iii) collaborative diagnosis among multiple LLMs. Our preliminary
experimental results that D-Bot can efficiently and effectively diagnose the
root causes and our code is available at
github.com/TsinghuaDatabaseGroup/DB-GPT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xuanhe Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guoliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NUPES : Non-Uniform Post-Training Quantization via Power Exponent Search. (arXiv:2308.05600v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.05600</id>
        <link href="http://arxiv.org/abs/2308.05600"/>
        <updated>2023-08-12T00:39:30.776Z</updated>
        <summary type="html"><![CDATA[Deep neural network (DNN) deployment has been confined to larger hardware
devices due to their expensive computational requirements. This challenge has
recently reached another scale with the emergence of large language models
(LLMs). In order to reduce both their memory footprint and latency, a promising
technique is quantization. It consists in converting floating point
representations to low bit-width fixed point representations, usually by
assuming a uniform mapping onto a regular grid. This process, referred to in
the literature as uniform quantization, may however be ill-suited as most DNN
weights and activations follow a bell-shaped distribution. This is even worse
on LLMs whose weight distributions are known to exhibit large, high impact,
outlier values. In this work, we propose an improvement over the most commonly
adopted way to tackle this limitation in deep learning models quantization,
namely, non-uniform quantization. NUPES leverages automorphisms to preserve the
scalar multiplications. Such transformations are derived from power functions.
However, the optimization of the exponent parameter and weight values remains a
challenging and novel problem which could not be solved with previous post
training optimization techniques which only learn to round up or down weight
values in order to preserve the predictive function. We circumvent this
limitation with a new paradigm: learning new quantized weights over the entire
quantized space. Similarly, we enable the optimization of the power exponent,
i.e. the optimization of the quantization operator itself during training by
alleviating all the numerical instabilities. The resulting predictive function
is compatible with integer-only low-bit inference. We show the ability of the
method to achieve state-of-the-art compression rates in both, data-free and
data-driven configurations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yvinec_E/0/1/0/all/0/1"&gt;Edouard Yvinec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dapogny_A/0/1/0/all/0/1"&gt;Arnaud Dapogny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bailly_K/0/1/0/all/0/1"&gt;Kevin Bailly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model. (arXiv:2308.05345v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.05345</id>
        <link href="http://arxiv.org/abs/2308.05345"/>
        <updated>2023-08-12T00:39:30.746Z</updated>
        <summary type="html"><![CDATA[Inspired by the recent success of large language models (LLMs) like ChatGPT,
researchers start to explore the adoption of LLMs for agile hardware design,
such as generating design RTL based on natural-language instructions. However,
in existing works, their target designs are all relatively simple and in a
small scale, and proposed by the authors themselves, making a fair comparison
among different LLM solutions challenging. In addition, many prior works only
focus on the design correctness, without evaluating the design qualities of
generated design RTL. In this work, we propose an open-source benchmark named
RTLLM, for generating design RTL with natural language instructions. To
systematically evaluate the auto-generated design RTL, we summarized three
progressive goals, named syntax goal, functionality goal, and design quality
goal. This benchmark can automatically provide a quantitative evaluation of any
given LLM-based solution. Furthermore, we propose an easy-to-use yet
surprisingly effective prompt engineering technique named self-planning, which
proves to significantly boost the performance of GPT-3.5 in our proposed
benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qijun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhiyao Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Forecaster's Review of Judea Pearl's Causality: Models, Reasoning and Inference, Second Edition, 2009. (arXiv:2308.05451v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2308.05451</id>
        <link href="http://arxiv.org/abs/2308.05451"/>
        <updated>2023-08-12T00:39:30.745Z</updated>
        <summary type="html"><![CDATA[With the big popularity and success of Judea Pearl's original causality book,
this review covers the main topics updated in the second edition in 2009 and
illustrates an easy-to-follow causal inference strategy in a forecast scenario.
It further discusses some potential benefits and challenges for causal
inference with time series forecasting when modeling the counterfactuals,
estimating the uncertainty and incorporating prior knowledge to estimate causal
effects in different forecasting scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Li_F/0/1/0/all/0/1"&gt;Feng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-GOMS: Large AI-Driven Global Ocean Modeling System. (arXiv:2308.03152v2 [physics.ao-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.03152</id>
        <link href="http://arxiv.org/abs/2308.03152"/>
        <updated>2023-08-12T00:39:30.745Z</updated>
        <summary type="html"><![CDATA[Ocean modeling is a powerful tool for simulating the physical, chemical, and
biological processes of the ocean, which is the foundation for marine science
research and operational oceanography. Modern numerical ocean modeling mainly
consists of governing equations and numerical algorithms. Nonlinear
instability, computational expense, low reusability efficiency and high
coupling costs have gradually become the main bottlenecks for the further
development of numerical ocean modeling. Recently, artificial
intelligence-based modeling in scientific computing has shown revolutionary
potential for digital twins and scientific simulations, but the bottlenecks of
numerical ocean modeling have not been further solved. Here, we present
AI-GOMS, a large AI-driven global ocean modeling system, for accurate and
efficient global ocean daily prediction. AI-GOMS consists of a backbone model
with the Fourier-based Masked Autoencoder structure for basic ocean variable
prediction and lightweight fine-tuning models incorporating regional
downscaling, wave decoding, and biochemistry coupling modules. AI-GOMS has
achieved the best performance in 30 days of prediction for the global ocean
basic variables with 15 depth layers at 1/4{\deg} spatial resolution. Beyond
the good performance in statistical metrics, AI-GOMS realizes the simulation of
mesoscale eddies in the Kuroshio region at 1/12{\deg} spatial resolution and
ocean stratification in the tropical Pacific Ocean. AI-GOMS provides a new
backbone-downstream paradigm for Earth system modeling, which makes the system
transferable, scalable and reusable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Xiong_W/0/1/0/all/0/1"&gt;Wei Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Xiang_Y/0/1/0/all/0/1"&gt;Yanfei Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shuyi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yuze Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ma_M/0/1/0/all/0/1"&gt;Muyuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaomeng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating disaster response through social media data and the Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S. wildfire season. (arXiv:2308.05281v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2308.05281</id>
        <link href="http://arxiv.org/abs/2308.05281"/>
        <updated>2023-08-12T00:39:30.682Z</updated>
        <summary type="html"><![CDATA[Effective disaster response is critical for affected communities. Responders
and decision-makers would benefit from reliable, timely measures of the issues
impacting their communities during a disaster, and social media offers a
potentially rich data source. Social media can reflect public concerns and
demands during a disaster, offering valuable insights for decision-makers to
understand evolving situations and optimize resource allocation. We used
Bidirectional Encoder Representations from Transformers (BERT) topic modeling
to cluster topics from Twitter data. Then, we conducted a temporal-spatial
analysis to examine the distribution of these topics across different regions
during the 2020 western U.S. wildfire season. Our results show that Twitter
users mainly focused on three topics:"health impact," "damage," and
"evacuation." We used the Susceptible-Infected-Recovered (SIR) theory to
explore the magnitude and velocity of topic diffusion on Twitter. The results
displayed a clear relationship between topic trends and wildfire propagation
patterns. The estimated parameters obtained from the SIR model in selected
cities revealed that residents exhibited a high level of several concerns
during the wildfire. Our study details how the SIR model and topic modeling
using social media data can provide decision-makers with a quantitative
approach to measure disaster response and support their decision-making
processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zihui Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lingyao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hemphill_L/0/1/0/all/0/1"&gt;Libby Hemphill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baecher_G/0/1/0/all/0/1"&gt;Gregory B. Baecher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Preemptive Detection of Fake Accounts on Social Networks via Multi-Class Preferential Attachment Classifiers. (arXiv:2308.05353v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2308.05353</id>
        <link href="http://arxiv.org/abs/2308.05353"/>
        <updated>2023-08-12T00:39:30.667Z</updated>
        <summary type="html"><![CDATA[In this paper, we describe a new algorithm called Preferential Attachment
k-class Classifier (PreAttacK) for detecting fake accounts in a social network.
Recently, several algorithms have obtained high accuracy on this problem.
However, they have done so by relying on information about fake accounts'
friendships or the content they share with others--the very things we seek to
prevent.

PreAttacK represents a significant departure from these approaches. We
provide some of the first detailed distributional analyses of how new fake (and
real) accounts first attempt to request friends after joining a major network
(Facebook). We show that even before a new account has made friends or shared
content, these initial friend request behaviors evoke a natural multi-class
extension of the canonical Preferential Attachment model of social network
growth.

We use this model to derive a new algorithm, PreAttacK. We prove that in
relevant problem instances, PreAttacK near-optimally approximates the posterior
probability that a new account is fake under this multi-class Preferential
Attachment model of new accounts' (not-yet-answered) friend requests. These are
the first provable guarantees for fake account detection that apply to new
users, and that do not require strong homophily assumptions.

This principled approach also makes PreAttacK the only algorithm with
provable guarantees that obtains state-of-the-art performance on new users on
the global Facebook network, where it converges to AUC=0.9 after new users send
+ receive a total of just 20 not-yet-answered friend requests. For comparison,
state-of-the-art benchmarks do not obtain this AUC even after observing
additional data on new users' first 100 friend requests. Thus, unlike
mainstream algorithms, PreAttacK converges before the median new fake account
has made a single friendship (accepted friend request) with a human.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Breuer_A/0/1/0/all/0/1"&gt;Adam Breuer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khosravani_N/0/1/0/all/0/1"&gt;Nazanin Khosravani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tingley_M/0/1/0/all/0/1"&gt;Michael Tingley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cottel_B/0/1/0/all/0/1"&gt;Bradford Cottel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoding Layer Saliency in Language Transformers. (arXiv:2308.05219v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2308.05219</id>
        <link href="http://arxiv.org/abs/2308.05219"/>
        <updated>2023-08-12T00:39:30.661Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a strategy for identifying textual saliency in
large-scale language models applied to classification tasks. In visual networks
where saliency is more well-studied, saliency is naturally localized through
the convolutional layers of the network; however, the same is not true in
modern transformer-stack networks used to process natural language. We adapt
gradient-based saliency methods for these networks, propose a method for
evaluating the degree of semantic coherence of each layer, and demonstrate
consistent improvement over numerous other methods for textual saliency on
multiple benchmark classification datasets. Our approach requires no additional
training or access to labelled data, and is comparatively very computationally
efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_E/0/1/0/all/0/1"&gt;Elizabeth M. Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Castanon_G/0/1/0/all/0/1"&gt;Gregory Castanon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Follow Anything: Open-set detection, tracking, and following in real-time. (arXiv:2308.05737v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2308.05737</id>
        <link href="http://arxiv.org/abs/2308.05737"/>
        <updated>2023-08-12T00:39:30.587Z</updated>
        <summary type="html"><![CDATA[Tracking and following objects of interest is critical to several robotics
use cases, ranging from industrial automation to logistics and warehousing, to
healthcare and security. In this paper, we present a robotic system to detect,
track, and follow any object in real-time. Our approach, dubbed ``follow
anything'' (FAn), is an open-vocabulary and multimodal model -- it is not
restricted to concepts seen at training time and can be applied to novel
classes at inference time using text, images, or click queries. Leveraging rich
visual descriptors from large-scale pre-trained models (foundation models), FAn
can detect and segment objects by matching multimodal queries (text, images,
clicks) against an input image sequence. These detected and segmented objects
are tracked across image frames, all while accounting for occlusion and object
re-emergence. We demonstrate FAn on a real-world robotic system (a micro aerial
vehicle) and report its ability to seamlessly follow the objects of interest in
a real-time control loop. FAn can be deployed on a laptop with a lightweight
(6-8 GB) graphics card, achieving a throughput of 6-20 frames per second. To
enable rapid adoption, deployment, and extensibility, we open-source all our
code on our project webpage at https://github.com/alaamaalouf/FollowAnything .
We also encourage the reader the watch our 5-minutes explainer video in this
https://www.youtube.com/watch?v=6Mgt3EPytrw .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maalouf_A/0/1/0/all/0/1"&gt;Alaa Maalouf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jadhav_N/0/1/0/all/0/1"&gt;Ninad Jadhav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jatavallabhula_K/0/1/0/all/0/1"&gt;Krishna Murthy Jatavallabhula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chahine_M/0/1/0/all/0/1"&gt;Makram Chahine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogt_D/0/1/0/all/0/1"&gt;Daniel M.Vogt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wood_R/0/1/0/all/0/1"&gt;Robert J. Wood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1"&gt;Antonio Torralba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1"&gt;Daniela Rus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning ground states of gapped quantum Hamiltonians with Kernel Methods. (arXiv:2303.08902v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.08902</id>
        <link href="http://arxiv.org/abs/2303.08902"/>
        <updated>2023-08-12T00:39:30.586Z</updated>
        <summary type="html"><![CDATA[Neural network approaches to approximate the ground state of quantum
hamiltonians require the numerical solution of a highly nonlinear optimization
problem. We introduce a statistical learning approach that makes the
optimization trivial by using kernel methods. Our scheme is an approximate
realization of the power method, where supervised learning is used to learn the
next step of the power iteration. We show that the ground state properties of
arbitrary gapped quantum hamiltonians can be reached with polynomial resources
under the assumption that the supervised learning is efficient. Using kernel
ridge regression, we provide numerical evidence that the learning assumption is
verified by applying our scheme to find the ground states of several
prototypical interacting many-body quantum systems, both in one and two
dimensions, showing the flexibility of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Giuliani_C/0/1/0/all/0/1"&gt;Clemens Giuliani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Vicentini_F/0/1/0/all/0/1"&gt;Filippo Vicentini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Rossi_R/0/1/0/all/0/1"&gt;Riccardo Rossi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Carleo_G/0/1/0/all/0/1"&gt;Giuseppe Carleo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Optimal Expressive Power of ReLU DNNs and Its Application in Approximation with Kolmogorov Superposition Theorem. (arXiv:2308.05509v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.05509</id>
        <link href="http://arxiv.org/abs/2308.05509"/>
        <updated>2023-08-12T00:39:30.585Z</updated>
        <summary type="html"><![CDATA[This paper is devoted to studying the optimal expressive power of ReLU deep
neural networks (DNNs) and its application in approximation via the Kolmogorov
Superposition Theorem. We first constructively prove that any continuous
piecewise linear functions on $[0,1]$, comprising $O(N^2L)$ segments, can be
represented by ReLU DNNs with $L$ hidden layers and $N$ neurons per layer.
Subsequently, we demonstrate that this construction is optimal regarding the
parameter count of the DNNs, achieved through investigating the shattering
capacity of ReLU DNNs. Moreover, by invoking the Kolmogorov Superposition
Theorem, we achieve an enhanced approximation rate for ReLU DNNs of arbitrary
width and depth when dealing with continuous functions in high-dimensional
spaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Juncai He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthesizing Mixed-type Electronic Health Records using Diffusion Models. (arXiv:2302.14679v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.14679</id>
        <link href="http://arxiv.org/abs/2302.14679"/>
        <updated>2023-08-12T00:39:30.585Z</updated>
        <summary type="html"><![CDATA[Electronic Health Records (EHRs) contain sensitive patient information, which
presents privacy concerns when sharing such data. Synthetic data generation is
a promising solution to mitigate these risks, often relying on deep generative
models such as Generative Adversarial Networks (GANs). However, recent studies
have shown that diffusion models offer several advantages over GANs, such as
generation of more realistic synthetic data and stable training in generating
data modalities, including image, text, and sound. In this work, we investigate
the potential of diffusion models for generating realistic mixed-type tabular
EHRs, comparing TabDDPM model with existing methods on four datasets in terms
of data quality, utility, privacy, and augmentation. Our experiments
demonstrate that TabDDPM outperforms the state-of-the-art models across all
evaluation metrics, except for privacy, which confirms the trade-off between
privacy and utility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ceritli_T/0/1/0/all/0/1"&gt;Taha Ceritli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosheh_G/0/1/0/all/0/1"&gt;Ghadeer O. Ghosheh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chauhan_V/0/1/0/all/0/1"&gt;Vinod Kumar Chauhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1"&gt;Tingting Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Creagh_A/0/1/0/all/0/1"&gt;Andrew P. Creagh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1"&gt;David A. Clifton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Efficient Algorithm for Nonstationary Low-Rank MDPs. (arXiv:2308.05471v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.05471</id>
        <link href="http://arxiv.org/abs/2308.05471"/>
        <updated>2023-08-12T00:39:30.359Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) under changing environment models many real-world
applications via nonstationary Markov Decision Processes (MDPs), and hence
gains considerable interest. However, theoretical studies on nonstationary MDPs
in the literature have mainly focused on tabular and linear (mixture) MDPs,
which do not capture the nature of unknown representation in deep RL. In this
paper, we make the first effort to investigate nonstationary RL under episodic
low-rank MDPs, where both transition kernels and rewards may vary over time,
and the low-rank model contains unknown representation in addition to the
linear state embedding function. We first propose a parameter-dependent policy
optimization algorithm called PORTAL, and further improve PORTAL to its
parameter-free version of Ada-PORTAL, which is able to tune its
hyper-parameters adaptively without any prior knowledge of nonstationarity. For
both algorithms, we provide upper bounds on the average dynamic suboptimality
gap, which show that as long as the nonstationarity is not significantly large,
PORTAL and Ada-PORTAL are sample-efficient and can achieve arbitrarily small
average dynamic suboptimality gap with polynomial sample complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yuan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yingbin Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shadow Datasets, New challenging datasets for Causal Representation Learning. (arXiv:2308.05707v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.05707</id>
        <link href="http://arxiv.org/abs/2308.05707"/>
        <updated>2023-08-12T00:39:30.348Z</updated>
        <summary type="html"><![CDATA[Discovering causal relations among semantic factors is an emergent topic in
representation learning. Most causal representation learning (CRL) methods are
fully supervised, which is impractical due to costly labeling. To resolve this
restriction, weakly supervised CRL methods were introduced. To evaluate CRL
performance, four existing datasets, Pendulum, Flow, CelebA(BEARD) and
CelebA(SMILE), are utilized. However, existing CRL datasets are limited to
simple graphs with few generative factors. Thus we propose two new datasets
with a larger number of diverse generative factors and more sophisticated
causal graphs. In addition, current real datasets, CelebA(BEARD) and
CelebA(SMILE), the originally proposed causal graphs are not aligned with the
dataset distributions. Thus, we propose modifications to them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiageng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1"&gt;Hanchen Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jianhua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiazhi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khayatkhoei_M/0/1/0/all/0/1"&gt;Mahyar Khayatkhoei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussein_M/0/1/0/all/0/1"&gt;Mohamed E. Hussein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+AbdAlmageed_W/0/1/0/all/0/1"&gt;Wael AbdAlmageed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning ground states of gapped quantum Hamiltonians with Kernel Methods. (arXiv:2303.08902v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.08902</id>
        <link href="http://arxiv.org/abs/2303.08902"/>
        <updated>2023-08-12T00:39:30.145Z</updated>
        <summary type="html"><![CDATA[Neural network approaches to approximate the ground state of quantum
hamiltonians require the numerical solution of a highly nonlinear optimization
problem. We introduce a statistical learning approach that makes the
optimization trivial by using kernel methods. Our scheme is an approximate
realization of the power method, where supervised learning is used to learn the
next step of the power iteration. We show that the ground state properties of
arbitrary gapped quantum hamiltonians can be reached with polynomial resources
under the assumption that the supervised learning is efficient. Using kernel
ridge regression, we provide numerical evidence that the learning assumption is
verified by applying our scheme to find the ground states of several
prototypical interacting many-body quantum systems, both in one and two
dimensions, showing the flexibility of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Giuliani_C/0/1/0/all/0/1"&gt;Clemens Giuliani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Vicentini_F/0/1/0/all/0/1"&gt;Filippo Vicentini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Rossi_R/0/1/0/all/0/1"&gt;Riccardo Rossi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Carleo_G/0/1/0/all/0/1"&gt;Giuseppe Carleo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forecasting Irregularly Sampled Time Series using Graphs. (arXiv:2305.12932v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.12932</id>
        <link href="http://arxiv.org/abs/2305.12932"/>
        <updated>2023-08-12T00:39:30.144Z</updated>
        <summary type="html"><![CDATA[Forecasting irregularly sampled time series with missing values is a crucial
task for numerous real-world applications such as healthcare, astronomy, and
climate sciences. State-of-the-art approaches to this problem rely on Ordinary
Differential Equations (ODEs) which are known to be slow and often require
additional features to handle missing values. To address this issue, we propose
a novel model using Graphs for Forecasting Irregularly Sampled Time Series with
missing values which we call GraFITi. GraFITi first converts the time series to
a Sparsity Structure Graph which is a sparse bipartite graph, and then
reformulates the forecasting problem as the edge weight prediction task in the
graph. It uses the power of Graph Neural Networks to learn the graph and
predict the target edge weights. GraFITi has been tested on 3 real-world and 1
synthetic irregularly sampled time series dataset with missing values and
compared with various state-of-the-art models. The experimental results
demonstrate that GraFITi improves the forecasting accuracy by up to 17% and
reduces the run time up to 5 times compared to the state-of-the-art forecasting
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yalavarthi_V/0/1/0/all/0/1"&gt;Vijaya Krishna Yalavarthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madhusudhanan_K/0/1/0/all/0/1"&gt;Kiran Madhusudhanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sholz_R/0/1/0/all/0/1"&gt;Randolf Sholz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1"&gt;Nourhan Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burchert_J/0/1/0/all/0/1"&gt;Johannes Burchert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jawed_S/0/1/0/all/0/1"&gt;Shayan Jawed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Born_S/0/1/0/all/0/1"&gt;Stefan Born&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_Thieme_L/0/1/0/all/0/1"&gt;Lars Schmidt-Thieme&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Deep Learning Approaches to Predict Person and Vehicle Trips: An Analysis of NHTS Data. (arXiv:2308.05665v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2308.05665</id>
        <link href="http://arxiv.org/abs/2308.05665"/>
        <updated>2023-08-12T00:39:30.143Z</updated>
        <summary type="html"><![CDATA[Modern transportation planning relies heavily on accurate predictions of
person and vehicle trips. However, traditional planning models often fail to
account for the intricacies and dynamics of travel behavior, leading to
less-than-optimal accuracy in these predictions. This study explores the
potential of deep learning techniques to transform the way we approach trip
predictions, and ultimately, transportation planning. Utilizing a comprehensive
dataset from the National Household Travel Survey (NHTS), we developed and
trained a deep learning model for predicting person and vehicle trips. The
proposed model leverages the vast amount of information in the NHTS data,
capturing complex, non-linear relationships that were previously overlooked by
traditional models. As a result, our deep learning model achieved an impressive
accuracy of 98% for person trip prediction and 96% for vehicle trip estimation.
This represents a significant improvement over the performances of traditional
transportation planning models, thereby demonstrating the power of deep
learning in this domain. The implications of this study extend beyond just more
accurate predictions. By enhancing the accuracy and reliability of trip
prediction models, planners can formulate more effective, data-driven
transportation policies, infrastructure, and services. As such, our research
underscores the need for the transportation planning field to embrace advanced
techniques like deep learning. The detailed methodology, along with a thorough
discussion of the results and their implications, are presented in the
subsequent sections of this paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Adu_Gyamfi_K/0/1/0/all/0/1"&gt;Kojo Adu-Gyamfi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anuj_S/0/1/0/all/0/1"&gt;Sharma Anuj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Width and Depth Limits Commute in Residual Networks. (arXiv:2302.00453v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.00453</id>
        <link href="http://arxiv.org/abs/2302.00453"/>
        <updated>2023-08-12T00:39:30.109Z</updated>
        <summary type="html"><![CDATA[We show that taking the width and depth to infinity in a deep neural network
with skip connections, when branches are scaled by $1/\sqrt{depth}$ (the only
nontrivial scaling), result in the same covariance structure no matter how that
limit is taken. This explains why the standard infinite-width-then-depth
approach provides practical insights even for networks with depth of the same
order as width. We also demonstrate that the pre-activations, in this case,
have Gaussian distributions which has direct applications in Bayesian deep
learning. We conduct extensive simulations that show an excellent match with
our theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hayou_S/0/1/0/all/0/1"&gt;Soufiane Hayou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yang_G/0/1/0/all/0/1"&gt;Greg Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AST-MHSA : Code Summarization using Multi-Head Self-Attention. (arXiv:2308.05646v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2308.05646</id>
        <link href="http://arxiv.org/abs/2308.05646"/>
        <updated>2023-08-12T00:39:29.748Z</updated>
        <summary type="html"><![CDATA[Code summarization aims to generate concise natural language descriptions for
source code. The prevailing approaches adopt transformer-based encoder-decoder
architectures, where the Abstract Syntax Tree (AST) of the source code is
utilized for encoding structural information. However, ASTs are much longer
than the corresponding source code, and existing methods ignore this size
constraint by directly feeding the entire linearized AST into the encoders.
This simplistic approach makes it challenging to extract truly valuable
dependency relations from the overlong input sequence and leads to significant
computational overhead due to self-attention applied to all nodes in the AST.

To address this issue effectively and efficiently, we present a model,
AST-MHSA that uses multi-head attention to extract the important semantic
information from the AST. The model consists of two main components: an encoder
and a decoder. The encoder takes as input the abstract syntax tree (AST) of the
code and generates a sequence of hidden states. The decoder then takes these
hidden states as input and generates a natural language summary of the code.

The multi-head attention mechanism allows the model to learn different
representations of the input code, which can be combined to generate a more
comprehensive summary. The model is trained on a dataset of code and summaries,
and the parameters of the model are optimized to minimize the loss between the
generated summaries and the ground-truth summaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nagaraj_Y/0/1/0/all/0/1"&gt;Yeshwanth Nagaraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_U/0/1/0/all/0/1"&gt;Ujjwal Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers. (arXiv:2308.05732v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.05732</id>
        <link href="http://arxiv.org/abs/2308.05732"/>
        <updated>2023-08-12T00:39:29.650Z</updated>
        <summary type="html"><![CDATA[Time-dependent partial differential equations (PDEs) are ubiquitous in
science and engineering. Recently, mostly due to the high computational cost of
traditional solution techniques, deep neural network based surrogates have
gained increased interest. The practical utility of such neural PDE solvers
relies on their ability to provide accurate, stable predictions over long time
horizons, which is a notoriously hard problem. In this work, we present a
large-scale analysis of common temporal rollout strategies, identifying the
neglect of non-dominant spatial frequency information, often associated with
high frequencies in PDE solutions, as the primary pitfall limiting stable,
accurate rollout performance. Based on these insights, we draw inspiration from
recent advances in diffusion models to introduce PDE-Refiner; a novel model
class that enables more accurate modeling of all frequency components via a
multistep refinement process. We validate PDE-Refiner on challenging benchmarks
of complex fluid dynamics, demonstrating stable and accurate rollouts that
consistently outperform state-of-the-art models, including neural, numerical,
and hybrid neural-numerical architectures. We further demonstrate that
PDE-Refiner greatly enhances data efficiency, since the denoising objective
implicitly induces a novel form of spectral data augmentation. Finally,
PDE-Refiner's connection to diffusion models enables an accurate and efficient
assessment of the model's predictive uncertainty, allowing us to estimate when
the surrogate becomes inaccurate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lippe_P/0/1/0/all/0/1"&gt;Phillip Lippe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veeling_B/0/1/0/all/0/1"&gt;Bastiaan S. Veeling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perdikaris_P/0/1/0/all/0/1"&gt;Paris Perdikaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1"&gt;Richard E. Turner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brandstetter_J/0/1/0/all/0/1"&gt;Johannes Brandstetter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FALL-E: A Foley Sound Synthesis Model and Strategies. (arXiv:2306.09807v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.09807</id>
        <link href="http://arxiv.org/abs/2306.09807"/>
        <updated>2023-08-12T00:39:29.216Z</updated>
        <summary type="html"><![CDATA[This paper introduces FALL-E, a foley synthesis system and its
training/inference strategies. The FALL-E model employs a cascaded approach
comprising low-resolution spectrogram generation, spectrogram super-resolution,
and a vocoder. We trained every sound-related model from scratch using our
extensive datasets, and utilized a pre-trained language model. We conditioned
the model with dataset-specific texts, enabling it to learn sound quality and
recording environment based on text input. Moreover, we leveraged external
language models to improve text descriptions of our datasets and performed
prompt engineering for quality, coherence, and diversity. FALL-E was evaluated
by an objective measure as well as listening tests in the DCASE 2023 challenge
Task 7. The submission achieved the second place on average, while achieving
the best score for diversity, second place for audio quality, and third place
for class fitness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kang_M/0/1/0/all/0/1"&gt;Minsung Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oh_S/0/1/0/all/0/1"&gt;Sangshin Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moon_H/0/1/0/all/0/1"&gt;Hyeongi Moon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kyungyun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chon_B/0/1/0/all/0/1"&gt;Ben Sangbae Chon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RALACs: Action Recognition in Autonomous Vehicles using Interaction Encoding and Optical Flow. (arXiv:2209.14408v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2209.14408</id>
        <link href="http://arxiv.org/abs/2209.14408"/>
        <updated>2023-08-12T00:39:29.192Z</updated>
        <summary type="html"><![CDATA[When applied to autonomous vehicle (AV) settings, action recognition can
enhance an environment model's situational awareness. This is especially
prevalent in scenarios where traditional geometric descriptions and heuristics
in AVs are insufficient. However, action recognition has traditionally been
studied for humans, and its limited adaptability to noisy, un-clipped,
un-pampered, raw RGB data has limited its application in other fields. To push
for the advancement and adoption of action recognition into AVs, this work
proposes a novel two-stage action recognition system, termed RALACs. RALACs
formulates the problem of action recognition for road scenes, and bridges the
gap between it and the established field of human action recognition. This work
shows how attention layers can be useful for encoding the relations across
agents, and stresses how such a scheme can be class-agnostic. Furthermore, to
address the dynamic nature of agents on the road, RALACs constructs a novel
approach to adapting Region of Interest (ROI) Alignment to agent tracks for
downstream action classification. Finally, our scheme also considers the
problem of active agent detection, and utilizes a novel application of fusing
optical flow maps to discern relevant agents in a road scene. We show that our
proposed scheme can outperform the baseline on the ICCV2021 Road Challenge
dataset and by deploying it on a real vehicle platform, we provide preliminary
insight to the usefulness of action recognition in decision making.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1"&gt;Eddy Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_A/0/1/0/all/0/1"&gt;Alex Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Budhwani_A/0/1/0/all/0/1"&gt;Alikasim Budhwani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dempster_R/0/1/0/all/0/1"&gt;Rowan Dempster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Quanquan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Sharman_M/0/1/0/all/0/1"&gt;Mohammad Al-Sharman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rayside_D/0/1/0/all/0/1"&gt;Derek Rayside&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melek_W/0/1/0/all/0/1"&gt;William Melek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overlooked Implications of the Reconstruction Loss for VAE Disentanglement. (arXiv:2202.13341v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.13341</id>
        <link href="http://arxiv.org/abs/2202.13341"/>
        <updated>2023-08-12T00:39:29.185Z</updated>
        <summary type="html"><![CDATA[Learning disentangled representations with variational autoencoders (VAEs) is
often attributed to the regularisation component of the loss. In this work, we
highlight the interaction between data and the reconstruction term of the loss
as the main contributor to disentanglement in VAEs. We show that standard
benchmark datasets have unintended correlations between their subjective
ground-truth factors and perceived axes in the data according to typical VAE
reconstruction losses. Our work exploits this relationship to provide a theory
for what constitutes an adversarial dataset under a given reconstruction loss.
We verify this by constructing an example dataset that prevents disentanglement
in state-of-the-art frameworks while maintaining human-intuitive ground-truth
factors. Finally, we re-enable disentanglement by designing an example
reconstruction loss that is once again able to perceive the ground-truth
factors. Our findings demonstrate the subjective nature of disentanglement and
the importance of considering the interaction between the ground-truth factors,
data and notably, the reconstruction loss, which is under-recognised in the
literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Michlo_N/0/1/0/all/0/1"&gt;Nathan Michlo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klein_R/0/1/0/all/0/1"&gt;Richard Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1"&gt;Steven James&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Financial Fraud Detection: A Comparative Study of Quantum Machine Learning Models. (arXiv:2308.05237v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2308.05237</id>
        <link href="http://arxiv.org/abs/2308.05237"/>
        <updated>2023-08-12T00:39:29.162Z</updated>
        <summary type="html"><![CDATA[In this research, a comparative study of four Quantum Machine Learning (QML)
models was conducted for fraud detection in finance. We proved that the Quantum
Support Vector Classifier model achieved the highest performance, with F1
scores of 0.98 for fraud and non-fraud classes. Other models like the
Variational Quantum Classifier, Estimator Quantum Neural Network (QNN), and
Sampler QNN demonstrate promising results, propelling the potential of QML
classification for financial applications. While they exhibit certain
limitations, the insights attained pave the way for future enhancements and
optimisation strategies. However, challenges exist, including the need for more
efficient Quantum algorithms and larger and more complex datasets. The article
provides solutions to overcome current limitations and contributes new insights
to the field of Quantum Machine Learning in fraud detection, with important
implications for its future development.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Innan_N/0/1/0/all/0/1"&gt;Nouhaila Innan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Khan_M/0/1/0/all/0/1"&gt;Muhammad Al-Zafar Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Bennai_M/0/1/0/all/0/1"&gt;Mohamed Bennai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial Gated Multi-Layer Perceptron for Land Use and Land Cover Mapping. (arXiv:2308.05235v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.05235</id>
        <link href="http://arxiv.org/abs/2308.05235"/>
        <updated>2023-08-12T00:39:29.155Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Networks (CNNs) are models that are utilized extensively
for the hierarchical extraction of features. Vision transformers (ViTs),
through the use of a self-attention mechanism, have recently achieved superior
modeling of global contextual information compared to CNNs. However, to realize
their image classification strength, ViTs require substantial training
datasets. Where the available training data are limited, current advanced
multi-layer perceptrons (MLPs) can provide viable alternatives to both deep
CNNs and ViTs. In this paper, we developed the SGU-MLP, a learning algorithm
that effectively uses both MLPs and spatial gating units (SGUs) for precise
land use land cover (LULC) mapping. Results illustrated the superiority of the
developed SGU-MLP classification algorithm over several CNN and CNN-ViT-based
models, including HybridSN, ResNet, iFormer, EfficientFormer and CoAtNet. The
proposed SGU-MLP algorithm was tested through three experiments in Houston,
USA, Berlin, Germany and Augsburg, Germany. The SGU-MLP classification model
was found to consistently outperform the benchmark CNN and CNN-ViT-based
algorithms. For example, for the Houston experiment, SGU-MLP significantly
outperformed HybridSN, CoAtNet, Efficientformer, iFormer and ResNet by
approximately 15%, 19%, 20%, 21%, and 25%, respectively, in terms of average
accuracy. The code will be made publicly available at
https://github.com/aj1365/SGUMLP]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jamali_A/0/1/0/all/0/1"&gt;Ali Jamali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;Swalpa Kumar Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1"&gt;Danfeng Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atkinson_P/0/1/0/all/0/1"&gt;Peter M Atkinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghamisi_P/0/1/0/all/0/1"&gt;Pedram Ghamisi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparative Analysis of Epileptic Seizure Prediction: Exploring Diverse Pre-Processing Techniques and Machine Learning Models. (arXiv:2308.05176v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2308.05176</id>
        <link href="http://arxiv.org/abs/2308.05176"/>
        <updated>2023-08-12T00:39:29.149Z</updated>
        <summary type="html"><![CDATA[Epilepsy is a prevalent neurological disorder characterized by recurrent and
unpredictable seizures, necessitating accurate prediction for effective
management and patient care. Application of machine learning (ML) on
electroencephalogram (EEG) recordings, along with its ability to provide
valuable insights into brain activity during seizures, is able to make accurate
and robust seizure prediction an indispensable component in relevant studies.
In this research, we present a comprehensive comparative analysis of five
machine learning models - Random Forest (RF), Decision Tree (DT), Extra Trees
(ET), Logistic Regression (LR), and Gradient Boosting (GB) - for the prediction
of epileptic seizures using EEG data. The dataset underwent meticulous
preprocessing, including cleaning, normalization, outlier handling, and
oversampling, ensuring data quality and facilitating accurate model training.
These preprocessing techniques played a crucial role in enhancing the models'
performance. The results of our analysis demonstrate the performance of each
model in terms of accuracy. The LR classifier achieved an accuracy of 56.95%,
while GB and DT both attained 97.17% accuracy. RT achieved a higher accuracy of
98.99%, while the ET model exhibited the best performance with an accuracy of
99.29%. Our findings reveal that the ET model outperformed not only the other
models in the comparative analysis but also surpassed the state-of-the-art
results from previous research. The superior performance of the ET model makes
it a compelling choice for accurate and robust epileptic seizure prediction
using EEG data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Talukder_M/0/1/0/all/0/1"&gt;Md. Simul Hasan Talukder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sulaiman_R/0/1/0/all/0/1"&gt;Rejwan Bin Sulaiman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReLU and Addition-based Gated RNN. (arXiv:2308.05629v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.05629</id>
        <link href="http://arxiv.org/abs/2308.05629"/>
        <updated>2023-08-12T00:39:29.142Z</updated>
        <summary type="html"><![CDATA[We replace the multiplication and sigmoid function of the conventional
recurrent gate with addition and ReLU activation. This mechanism is designed to
maintain long-term memory for sequence processing but at a reduced
computational cost, thereby opening up for more efficient execution or larger
models on restricted hardware. Recurrent Neural Networks (RNNs) with gating
mechanisms such as LSTM and GRU have been widely successful in learning from
sequential data due to their ability to capture long-term dependencies.
Conventionally, the update based on current inputs and the previous state
history is each multiplied with dynamic weights and combined to compute the
next state. However, multiplication can be computationally expensive,
especially for certain hardware architectures or alternative arithmetic systems
such as homomorphic encryption. It is demonstrated that the novel gating
mechanism can capture long-term dependencies for a standard synthetic sequence
learning task while significantly reducing computational costs such that
execution time is reduced by half on CPU and by one-third under encryption.
Experimental results on handwritten text recognition tasks furthermore show
that the proposed architecture can be trained to achieve comparable accuracy to
conventional GRU and LSTM baselines. The gating mechanism introduced in this
paper may enable privacy-preserving AI applications operating under homomorphic
encryption by avoiding the multiplication of encrypted variables. It can also
support quantization in (unencrypted) plaintext applications, with the
potential for substantial performance gains since the addition-based
formulation can avoid the expansion to double precision often required for
multiplication.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brannvall_R/0/1/0/all/0/1"&gt;Rickard Br&amp;#xe4;nnvall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forsgren_H/0/1/0/all/0/1"&gt;Henrik Forsgren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sandin_F/0/1/0/all/0/1"&gt;Fredrik Sandin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1"&gt;Marcus Liwicki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing the Effect of Data Impurity on the Detection Performances of Mental Disorders. (arXiv:2308.05133v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2308.05133</id>
        <link href="http://arxiv.org/abs/2308.05133"/>
        <updated>2023-08-12T00:39:29.116Z</updated>
        <summary type="html"><![CDATA[The primary method for identifying mental disorders automatically has
traditionally involved using binary classifiers. These classifiers are trained
using behavioral data obtained from an interview setup. In this training
process, data from individuals with the specific disorder under consideration
are categorized as the positive class, while data from all other participants
constitute the negative class. In practice, it is widely recognized that
certain mental disorders share similar symptoms, causing the collected
behavioral data to encompass a variety of attributes associated with multiple
disorders. Consequently, attributes linked to the targeted mental disorder
might also be present within the negative class. This data impurity may lead to
sub-optimal training of the classifier for a mental disorder of interest. In
this study, we investigate this hypothesis in the context of major depressive
disorder (MDD) and post-traumatic stress disorder detection (PTSD). The results
show that upon removal of such data impurity, MDD and PTSD detection
performances are significantly improved.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Gupta_R/0/1/0/all/0/1"&gt;Rohan Kumar Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Sinha_R/0/1/0/all/0/1"&gt;Rohit Sinha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-Free Model Extraction Attacks in the Context of Object Detection. (arXiv:2308.05127v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2308.05127</id>
        <link href="http://arxiv.org/abs/2308.05127"/>
        <updated>2023-08-12T00:39:29.110Z</updated>
        <summary type="html"><![CDATA[A significant number of machine learning models are vulnerable to model
extraction attacks, which focus on stealing the models by using specially
curated queries against the target model. This task is well accomplished by
using part of the training data or a surrogate dataset to train a new model
that mimics a target model in a white-box environment. In pragmatic situations,
however, the target models are trained on private datasets that are
inaccessible to the adversary. The data-free model extraction technique
replaces this problem when it comes to using queries artificially curated by a
generator similar to that used in Generative Adversarial Nets. We propose for
the first time, to the best of our knowledge, an adversary black box attack
extending to a regression problem for predicting bounding box coordinates in
object detection. As part of our study, we found that defining a loss function
and using a novel generator setup is one of the key aspects in extracting the
target model. We find that the proposed model extraction method achieves
significant results by using reasonable queries. The discovery of this object
detection vulnerability will support future prospects for securing such models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1"&gt;Harshit Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+G_A/0/1/0/all/0/1"&gt;Aravindhan G&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1"&gt;Pavan Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Govidarajulu_Y/0/1/0/all/0/1"&gt;Yuvaraj Govidarajulu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1"&gt;Manojkumar Parmar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Attention Be Used to Explain EHR-Based Mortality Prediction Tasks: A Case Study on Hemorrhagic Stroke. (arXiv:2308.05110v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.05110</id>
        <link href="http://arxiv.org/abs/2308.05110"/>
        <updated>2023-08-12T00:39:29.103Z</updated>
        <summary type="html"><![CDATA[Stroke is a significant cause of mortality and morbidity, necessitating early
predictive strategies to minimize risks. Traditional methods for evaluating
patients, such as Acute Physiology and Chronic Health Evaluation (APACHE II,
IV) and Simplified Acute Physiology Score III (SAPS III), have limited accuracy
and interpretability. This paper proposes a novel approach: an interpretable,
attention-based transformer model for early stroke mortality prediction. This
model seeks to address the limitations of previous predictive models, providing
both interpretability (providing clear, understandable explanations of the
model) and fidelity (giving a truthful explanation of the model's dynamics from
input to output). Furthermore, the study explores and compares fidelity and
interpretability scores using Shapley values and attention-based scores to
improve model explainability. The research objectives include designing an
interpretable attention-based transformer model, evaluating its performance
compared to existing models, and providing feature importance derived from the
model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1"&gt;Qizhang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Jiayi Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Emdad_F/0/1/0/all/0/1"&gt;Forhan Bin Emdad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanna_K/0/1/0/all/0/1"&gt;Karim Hanna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xia Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhe He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symmetry Defense Against XGBoost Adversarial Perturbation Attacks. (arXiv:2308.05575v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.05575</id>
        <link href="http://arxiv.org/abs/2308.05575"/>
        <updated>2023-08-12T00:39:29.096Z</updated>
        <summary type="html"><![CDATA[We examine whether symmetry can be used to defend tree-based ensemble
classifiers such as gradient-boosting decision trees (GBDTs) against
adversarial perturbation attacks. The idea is based on a recent symmetry
defense for convolutional neural network classifiers (CNNs) that utilizes CNNs'
lack of invariance with respect to symmetries. CNNs lack invariance because
they can classify a symmetric sample, such as a horizontally flipped image,
differently from the original sample. CNNs' lack of invariance also means that
CNNs can classify symmetric adversarial samples differently from the incorrect
classification of adversarial samples. Using CNNs' lack of invariance, the
recent CNN symmetry defense has shown that the classification of symmetric
adversarial samples reverts to the correct sample classification. In order to
apply the same symmetry defense to GBDTs, we examine GBDT invariance and are
the first to show that GBDTs also lack invariance with respect to symmetries.
We apply and evaluate the GBDT symmetry defense for nine datasets against six
perturbation attacks with a threat model that ranges from zero-knowledge to
perfect-knowledge adversaries. Using the feature inversion symmetry against
zero-knowledge adversaries, we achieve up to 100% accuracy on adversarial
samples even when default and robust classifiers have 0% accuracy. Using the
feature inversion and horizontal flip symmetries against perfect-knowledge
adversaries, we achieve up to over 95% accuracy on adversarial samples for the
GBDT classifier of the F-MNIST dataset even when default and robust classifiers
have 0% accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lindqvist_B/0/1/0/all/0/1"&gt;Blerta Lindqvist&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures. (arXiv:2308.05106v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.05106</id>
        <link href="http://arxiv.org/abs/2308.05106"/>
        <updated>2023-08-12T00:39:29.091Z</updated>
        <summary type="html"><![CDATA[This paper presents an investigation into machine learning techniques for
violence detection in videos and their adaptation to a federated learning
context. The study includes experiments with spatio-temporal features extracted
from benchmark video datasets, comparison of different methods, and proposal of
a modified version of the "Flow-Gated" architecture called "Diff-Gated."
Additionally, various machine learning techniques, including super-convergence
and transfer learning, are explored, and a method for adapting centralized
datasets to a federated learning context is developed. The research achieves
better accuracy results compared to state-of-the-art models by training the
best violence detection model in a federated learning context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quentin_P/0/1/0/all/0/1"&gt;Pajon Quentin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swan_S/0/1/0/all/0/1"&gt;Serre Swan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hugo_W/0/1/0/all/0/1"&gt;Wissocq Hugo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leo_R/0/1/0/all/0/1"&gt;Rabaud L&amp;#xe9;o&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siba_H/0/1/0/all/0/1"&gt;Haidar Siba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antoun_Y/0/1/0/all/0/1"&gt;Yaacoub Antoun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sound propagation in realistic interactive 3D scenes with parameterized sources using deep neural operators. (arXiv:2308.05141v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2308.05141</id>
        <link href="http://arxiv.org/abs/2308.05141"/>
        <updated>2023-08-12T00:39:29.070Z</updated>
        <summary type="html"><![CDATA[We address the challenge of sound propagation simulations in $3$D virtual
rooms with moving sources, which have applications in virtual/augmented
reality, game audio, and spatial computing. Solutions to the wave equation can
describe wave phenomena such as diffraction and interference. However,
simulating them using conventional numerical discretization methods with
hundreds of source and receiver positions is intractable, making stimulating a
sound field with moving sources impractical. To overcome this limitation, we
propose using deep operator networks to approximate linear wave-equation
operators. This enables the rapid prediction of sound propagation in realistic
3D acoustic scenes with moving sources, achieving millisecond-scale
computations. By learning a compact surrogate model, we avoid the offline
calculation and storage of impulse responses for all relevant source/listener
pairs. Our experiments, including various complex scene geometries, show good
agreement with reference solutions, with root mean squared errors ranging from
0.02 Pa to 0.10 Pa. Notably, our method signifies a paradigm shift as no prior
machine learning approach has achieved precise predictions of complete wave
fields within realistic domains. We anticipate that our findings will drive
further exploration of deep neural operator methods, advancing research in
immersive user experiences within virtual environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Borrel_Jensen_N/0/1/0/all/0/1"&gt;Nikolas Borrel-Jensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goswami_S/0/1/0/all/0/1"&gt;Somdatta Goswami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Engsig_Karup_A/0/1/0/all/0/1"&gt;Allan P. Engsig-Karup&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1"&gt;George Em Karniadakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_C/0/1/0/all/0/1"&gt;Cheol-Ho Jeong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vector Embeddings by Sequence Similarity and Context for Improved Compression, Similarity Search, Clustering, Organization, and Manipulation of cDNA Libraries. (arXiv:2308.05118v1 [q-bio.GN])]]></title>
        <id>http://arxiv.org/abs/2308.05118</id>
        <link href="http://arxiv.org/abs/2308.05118"/>
        <updated>2023-08-12T00:39:29.063Z</updated>
        <summary type="html"><![CDATA[This paper demonstrates the utility of organized numerical representations of
genes in research involving flat string gene formats (i.e., FASTA/FASTQ5).
FASTA/FASTQ files have several current limitations, such as their large file
sizes, slow processing speeds for mapping and alignment, and contextual
dependencies. These challenges significantly hinder investigations and tasks
that involve finding similar sequences. The solution lies in transforming
sequences into an alternative representation that facilitates easier clustering
into similar groups compared to the raw sequences themselves. By assigning a
unique vector embedding to each short sequence, it is possible to more
efficiently cluster and improve upon compression performance for the string
representations of cDNA libraries. Furthermore, through learning alternative
coordinate vector embeddings based on the contexts of codon triplets, we can
demonstrate clustering based on amino acid properties. Finally, using this
sequence embedding method to encode barcodes and cDNA sequences, we can improve
the time complexity of the similarity search by coupling vector embeddings with
an algorithm that determines the proximity of vectors in Euclidean space; this
allows us to perform sequence similarity searches in a quicker and more modular
fashion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Um_D/0/1/0/all/0/1"&gt;Daniel H. Um&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Knowles_D/0/1/0/all/0/1"&gt;David A. Knowles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kaiser_G/0/1/0/all/0/1"&gt;Gail E. Kaiser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Copy Number Variation Informs fMRI-based Prediction of Autism Spectrum Disorder. (arXiv:2308.05122v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2308.05122</id>
        <link href="http://arxiv.org/abs/2308.05122"/>
        <updated>2023-08-12T00:39:29.056Z</updated>
        <summary type="html"><![CDATA[The multifactorial etiology of autism spectrum disorder (ASD) suggests that
its study would benefit greatly from multimodal approaches that combine data
from widely varying platforms, e.g., neuroimaging, genetics, and clinical
characterization. Prior neuroimaging-genetic analyses often apply naive feature
concatenation approaches in data-driven work or use the findings from one
modality to guide posthoc analysis of another, missing the opportunity to
analyze the paired multimodal data in a truly unified approach. In this paper,
we develop a more integrative model for combining genetic, demographic, and
neuroimaging data. Inspired by the influence of genotype on phenotype, we
propose using an attention-based approach where the genetic data guides
attention to neuroimaging features of importance for model prediction. The
genetic data is derived from copy number variation parameters, while the
neuroimaging data is from functional magnetic resonance imaging. We evaluate
the proposed approach on ASD classification and severity prediction tasks,
using a sex-balanced dataset of 228 ASD and typically developing subjects in a
10-fold cross-validation framework. We demonstrate that our attention-based
model combining genetic information, demographic data, and functional magnetic
resonance imaging results in superior prediction performance compared to other
multimodal approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Dvornek_N/0/1/0/all/0/1"&gt;Nicha C. Dvornek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Sullivan_C/0/1/0/all/0/1"&gt;Catherine Sullivan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Duncan_J/0/1/0/all/0/1"&gt;James S. Duncan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Abha R. Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Morphological Identification of Extended Radio Galaxies using Weak Labels. (arXiv:2308.05166v1 [astro-ph.IM])]]></title>
        <id>http://arxiv.org/abs/2308.05166</id>
        <link href="http://arxiv.org/abs/2308.05166"/>
        <updated>2023-08-12T00:39:29.050Z</updated>
        <summary type="html"><![CDATA[The present work discusses the use of a weakly-supervised deep learning
algorithm that reduces the cost of labelling pixel-level masks for complex
radio galaxies with multiple components. The algorithm is trained on weak
class-level labels of radio galaxies to get class activation maps (CAMs). The
CAMs are further refined using an inter-pixel relations network (IRNet) to get
instance segmentation masks over radio galaxies and the positions of their
infrared hosts. We use data from the Australian Square Kilometre Array
Pathfinder (ASKAP) telescope, specifically the Evolutionary Map of the Universe
(EMU) Pilot Survey, which covered a sky area of 270 square degrees with an RMS
sensitivity of 25-35 $\mu$Jy/beam. We demonstrate that weakly-supervised deep
learning algorithms can achieve high accuracy in predicting pixel-level
information, including masks for the extended radio emission encapsulating all
galaxy components and the positions of the infrared host galaxies. We evaluate
the performance of our method using mean Average Precision (mAP) across
multiple classes at a standard intersection over union (IoU) threshold of 0.5.
We show that the model achieves a mAP$_{50}$ of 67.5\% and 76.8\% for radio
masks and infrared host positions, respectively. The network architecture can
be found at the following link: https://github.com/Nikhel1/Gal-CAM]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Gupta_N/0/1/0/all/0/1"&gt;Nikhel Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Hayder_Z/0/1/0/all/0/1"&gt;Zeeshan Hayder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Norris_R/0/1/0/all/0/1"&gt;Ray P. Norris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Huynh_M/0/1/0/all/0/1"&gt;Minh Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Petersson_L/0/1/0/all/0/1"&gt;Lars Petersson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Wang_X/0/1/0/all/0/1"&gt;X. Rosalind Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Andernach_H/0/1/0/all/0/1"&gt;Heinz Andernach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Koribalski_B/0/1/0/all/0/1"&gt;B&amp;#xe4;rbel S. Koribalski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Yew_M/0/1/0/all/0/1"&gt;Miranda Yew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Crawford_E/0/1/0/all/0/1"&gt;Evan J. Crawford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two Novel Approaches to Detect Community: A Case Study of Omicron Lineage Variants PPI Network. (arXiv:2308.05125v1 [q-bio.MN])]]></title>
        <id>http://arxiv.org/abs/2308.05125</id>
        <link href="http://arxiv.org/abs/2308.05125"/>
        <updated>2023-08-12T00:39:29.042Z</updated>
        <summary type="html"><![CDATA[The capacity to identify and analyze protein-protein interactions, along with
their internal modular organization, plays a crucial role in comprehending the
intricate mechanisms underlying biological processes at the molecular level. We
can learn a lot about the structure and dynamics of these interactions by using
network analysis. We can improve our understanding of the biological roots of
disease pathogenesis by recognizing network communities. This knowledge, in
turn, holds significant potential for driving advancements in drug discovery
and facilitating personalized medicine approaches for disease treatment. In
this study, we aimed to uncover the communities within the variant B.1.1.529
(Omicron virus) using two proposed novel algorithm (ABCDE and ALCDE) and four
widely recognized algorithms: Girvan-Newman, Louvain, Leiden, and Label
Propagation algorithm. Each of these algorithms has established prominence in
the field and offers unique perspectives on identifying communities within
complex networks. We also compare the networks by the global properties,
statistic summary, subgraph count, graphlet and validate by the modulaity. By
employing these approaches, we sought to gain deeper insights into the
structural organization and interconnections present within the Omicron virus
network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Das_M/0/1/0/all/0/1"&gt;Mamata Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+K%2E_S/0/1/0/all/0/1"&gt;Selvakumar K.&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Alphonse_P/0/1/0/all/0/1"&gt;P.J.A. Alphonse&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PTransIPs: Identification of phosphorylation sites based on protein pretrained language model and Transformer. (arXiv:2308.05115v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2308.05115</id>
        <link href="http://arxiv.org/abs/2308.05115"/>
        <updated>2023-08-12T00:39:29.014Z</updated>
        <summary type="html"><![CDATA[Phosphorylation is central to numerous fundamental cellular processes,
influencing the onset and progression of a variety of diseases. Identification
of phosphorylation sites is thus an important step for understanding the
molecular mechanisms of cells and virus infection, which potentially leads to
new therapeutic targets. In this study, we present PTransIPs, a novel deep
learning model for the identification of phosphorylation sites. PTransIPs
treats amino acids in protein sequences as words in natural language,
extracting unique encodings based on the types along with position of amino
acids in the sequence. It also incorporates embeddings from large pre-trained
protein models as additional data inputs. PTransIPS is further trained on a
combination model of convolutional neural network with residual connections and
Transformer model equipped with multi-head attention mechanisms. At last, the
model outputs classification results through a fully connected layer. The
results of independent testing reveal that PTransIPs outperforms existing
state-of-the-art methodologies, achieving AUROCs of 0.9232 and 0.9660 for
identifying phosphorylated S/T and Y sites respectively. In addition, ablation
studies prove that pretrained model embeddings contribute to the performance of
PTransIPs. Furthermore, PTransIPs has interpretable amino acid preference,
visible training process and shows generalizability on other bioactivity
classification tasks. To facilitate usage, our code and data are publicly
accessible at \url{https://github.com/StatXzy7/PTransIPs}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Ziyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhong_H/0/1/0/all/0/1"&gt;Haitian Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are Sex-based Physiological Differences the Cause of Gender Bias for Chest X-ray Diagnosis?. (arXiv:2308.05129v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2308.05129</id>
        <link href="http://arxiv.org/abs/2308.05129"/>
        <updated>2023-08-12T00:39:29.006Z</updated>
        <summary type="html"><![CDATA[While many studies have assessed the fairness of AI algorithms in the medical
field, the causes of differences in prediction performance are often unknown.
This lack of knowledge about the causes of bias hampers the efficacy of bias
mitigation, as evidenced by the fact that simple dataset balancing still often
performs best in reducing performance gaps but is unable to resolve all
performance differences. In this work, we investigate the causes of gender bias
in machine learning-based chest X-ray diagnosis. In particular, we explore the
hypothesis that breast tissue leads to underexposure of the lungs and causes
lower model performance. Methodologically, we propose a new sampling method
which addresses the highly skewed distribution of recordings per patient in two
widely used public datasets, while at the same time reducing the impact of
label errors. Our comprehensive analysis of gender differences across diseases,
datasets, and gender representations in the training set shows that dataset
imbalance is not the sole cause of performance differences. Moreover, relative
group performance differs strongly between datasets, indicating important
dataset-specific factors influencing male/female group performance. Finally, we
investigate the effect of breast tissue more specifically, by cropping out the
breasts from recordings, finding that this does not resolve the observed
performance gaps. In conclusion, our results indicate that dataset-specific
factors, not fundamental physiological differences, are the main drivers of
male--female performance gaps in chest X-ray analyses on widely used NIH and
CheXpert Dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Weng_N/0/1/0/all/0/1"&gt;Nina Weng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bigdeli_S/0/1/0/all/0/1"&gt;Siavash Bigdeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Petersen_E/0/1/0/all/0/1"&gt;Eike Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feragen_A/0/1/0/all/0/1"&gt;Aasa Feragen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Model Agnostic Reliability Evaluation of Machine-Learning Methods Integrated in Instrumentation & Control Systems. (arXiv:2308.05120v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.05120</id>
        <link href="http://arxiv.org/abs/2308.05120"/>
        <updated>2023-08-12T00:39:28.993Z</updated>
        <summary type="html"><![CDATA[In recent years, the field of data-driven neural network-based machine
learning (ML) algorithms has grown significantly and spurred research in its
applicability to instrumentation and control systems. While they are promising
in operational contexts, the trustworthiness of such algorithms is not
adequately assessed. Failures of ML-integrated systems are poorly understood;
the lack of comprehensive risk modeling can degrade the trustworthiness of
these systems. In recent reports by the National Institute for Standards and
Technology, trustworthiness in ML is a critical barrier to adoption and will
play a vital role in intelligent systems' safe and accountable operation. Thus,
in this work, we demonstrate a real-time model-agnostic method to evaluate the
relative reliability of ML predictions by incorporating out-of-distribution
detection on the training dataset. It is well documented that ML algorithms
excel at interpolation (or near-interpolation) tasks but significantly degrade
at extrapolation. This occurs when new samples are "far" from training samples.
The method, referred to as the Laplacian distributed decay for reliability
(LADDR), determines the difference between the operational and training
datasets, which is used to calculate a prediction's relative reliability. LADDR
is demonstrated on a feedforward neural network-based model used to predict
safety significant factors during different loss-of-flow transients. LADDR is
intended as a "data supervisor" and determines the appropriateness of
well-trained ML models in the context of operational conditions. Ultimately,
LADDR illustrates how training data can be used as evidence to support the
trustworthiness of ML predictions when utilized for conventional interpolation
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1"&gt;Edward Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1"&gt;Han Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinh_N/0/1/0/all/0/1"&gt;Nam Dinh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Thoughts on Jon Krohns Machine Learning Mathematical Foundations]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15opiln/d_thoughts_on_jon_krohns_machine_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15opiln/d_thoughts_on_jon_krohns_machine_learning/"/>
        <updated>2023-08-12T00:19:11.000Z</updated>
        <summary type="html"><![CDATA[Context:
 I'm teaching myself machine learning and right now I'm starting on the very core of it which is mathematics. For those who bought this course from Udemy, is this enough for real life ML problems?
    submitted by    /u/Forsaken_Buy_7531  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Running massive language models with Petals]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15oo45p/d_running_massive_language_models_with_petals/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15oo45p/d_running_massive_language_models_with_petals/"/>
        <updated>2023-08-11T23:19:15.000Z</updated>
        <summary type="html"><![CDATA[My observations and opinions on using Petals to run distributed LLMs, as a host and a user.
 https://yak.ventures/2023/08/11/distributed-llms-with-petals/
 I'd be very interested to talk to anyone that is utilizing a distributed model for daily use or for an application and even more interested to talk to anyone running a model among friends or colleagues in the private mode.
    submitted by    /u/Ruleryak  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Hi all, I am doing a research paper (high school) on ethics in AI art. I would greatly appreciate it if you took the time to fill in this survey. Thank you]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15onog4/r_hi_all_i_am_doing_a_research_paper_high_school/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15onog4/r_hi_all_i_am_doing_a_research_paper_high_school/"/>
        <updated>2023-08-11T23:01:18.000Z</updated>
        <summary type="html"><![CDATA[Here
    submitted by    /u/TommZ5  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hi all, I am doing a research paper (high school) on ethics in AI art. I would greatly appreciate it if you took the time to fill in this survey. Thank you!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15onj3g/hi_all_i_am_doing_a_research_paper_high_school_on/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15onj3g/hi_all_i_am_doing_a_research_paper_high_school_on/"/>
        <updated>2023-08-11T22:55:09.000Z</updated>
        <summary type="html"><![CDATA[Link to survey
    submitted by    /u/TommZ5  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenAI CEO Sam Altman donates $200,000 to Biden campaign]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15on6ku/openai_ceo_sam_altman_donates_200000_to_biden/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15on6ku/openai_ceo_sam_altman_donates_200000_to_biden/"/>
        <updated>2023-08-11T22:40:56.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/micahdjt1221  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Does RLHF increase the time horizon of models?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ol20u/d_does_rlhf_increase_the_time_horizon_of_models/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ol20u/d_does_rlhf_increase_the_time_horizon_of_models/"/>
        <updated>2023-08-11T21:19:02.000Z</updated>
        <summary type="html"><![CDATA[RL techniques in general have the ability to increase the time horizon of models since future rewards impact the Q value or advantage of the current action.
 My understanding of RLHF is a reward model is trained based on human feedback, and then the LLM is optimized to maximize the reward from the reward model. Is this correct?
 If so, does the reward model care about future rewards? Does this impact the time horizon?
    submitted by    /u/30299578815310  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Confusion in the DL-based Keras Embedding and Dense Layer]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ojszo/d_confusion_in_the_dlbased_keras_embedding_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ojszo/d_confusion_in_the_dlbased_keras_embedding_and/"/>
        <updated>2023-08-11T20:31:42.000Z</updated>
        <summary type="html"><![CDATA[I am new to this field and feeling quite confused. I need to use a DL-based Keras Embeddings technique with the Dense layer for text classification (specifically, a Binary Classification problem), along with TF-IDF featurization as input for the Random Forest Algorithm. However, my confusion arises from the fact that the Keras Embedding Layer also serves as a featurization technique. Therefore, I'm uncertain whether this layer should be used as input for the Random Forest or it has the capability to classify text on its own. Second question is that what can be the reason to use Dense Layer and What it is exactly here.
    submitted by    /u/ZahidAlee  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does this cover the basics/necessities of AI/ML [D] ?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ojgpf/does_this_cover_the_basicsnecessities_of_aiml_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ojgpf/does_this_cover_the_basicsnecessities_of_aiml_d/"/>
        <updated>2023-08-11T20:18:41.000Z</updated>
        <summary type="html"><![CDATA[Hello.
 Trying to make a plan so I can chip away at stuff day-by-day over the next few months/year(s).
 I was wondering if I've classified everything in this diagram in the correct way or if I'm missing anything ?
 Reason I ask here is I'm not too sure if I'm missing anything obsecure or if I've misinterpreted anything ?
 Thank you !
 https://preview.redd.it/dqbgfbcthjhb1.png?width=4299&format=png&auto=webp&s=1ceb45c3e4237f2204151bfed7bf45b54e4a5d68
    submitted by    /u/EngineerOwn6160  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How Do Various Regularization Techniques Affect the Loss Surface?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ojfia/d_how_do_various_regularization_techniques_affect/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ojfia/d_how_do_various_regularization_techniques_affect/"/>
        <updated>2023-08-11T20:17:22.000Z</updated>
        <summary type="html"><![CDATA[I'm currently working through "Understanding Deep Learning" by Simon J.D. Prince. On page 403, he makes the following statement about regularization:
  
Another possible explanation for the ease with which models are trained is that regularization makes the loss surface flatter and more convex.
  
From my understanding, L2 regularization (or weight decay) indeed adds a convex term λ∣∣w∣∣2 to the loss function, smoothing it out. Additionally, the Hessian matrix becomes more positive with the addition of the regularization term 2λI, giving the function a more convex characteristic.
 However, I'm puzzled as to how other regularization methods like Dropout, L1/Lasso, or Early Stopping might lead to a similarly flatter and more convex loss surface. Can anyone offer insights or explanations on this?
    submitted by    /u/spontanurlaub  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Lessons from this years Neurips]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15oic7a/d_lessons_from_this_years_neurips/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15oic7a/d_lessons_from_this_years_neurips/"/>
        <updated>2023-08-11T19:34:45.000Z</updated>
        <summary type="html"><![CDATA[This years Neurips has been a rollercoaster for everyone involved.
 Petar Veličković says that in their AC batch 65% submitted no rebuttal or withdrew.
 https://twitter.com/PetarV_93/status/1689648854646575105
 Xin Eric Wang says in their batch pre-rebuttal no papers had an avg score above an weak accept.
 https://twitter.com/xwang_lk/status/1686517898108674048
 Will NeurIPS keep 25% acceptance rate? What do you think will happen to neurips in light of the above? Is this the end of big ML confs?
 ​
    submitted by    /u/SuchOccasion457  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-Tuning Llama-2: A Comprehensive Case Study for Tailoring Models to Unique Applications]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15oi76d/finetuning_llama2_a_comprehensive_case_study_for/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15oi76d/finetuning_llama2_a_comprehensive_case_study_for/"/>
        <updated>2023-08-11T19:29:25.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] 3D Gaussian Splatting for Real-Time Radiance Field Rendering]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15oh6me/r_3d_gaussian_splatting_for_realtime_radiance/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15oh6me/r_3d_gaussian_splatting_for_realtime_radiance/"/>
        <updated>2023-08-11T18:50:28.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/individual_kex  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Implementing siamese network with MultipleNegativesRankingLoss in Keras/TF]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15oh6cw/d_implementing_siamese_network_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15oh6cw/d_implementing_siamese_network_with/"/>
        <updated>2023-08-11T18:50:12.000Z</updated>
        <summary type="html"><![CDATA[Hi!
 I have been trying to find a good guide of how best to implement a Sentence Transformers style model using Keras, but have not found anything :(
 I have managed to get something running, but I am not sure it is pretty and wanted to see if anyone know how to improve it or maybe has seen a nice implementation on the web?
 Here is my first draft https://gist.github.com/ydennisy/fec55fab84d107b72852ba2d2c2b61db
    submitted by    /u/Suspicious_Dress_350  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How does Lora save memory footprint for transformers?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ogvp4/d_how_does_lora_save_memory_footprint_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ogvp4/d_how_does_lora_save_memory_footprint_for/"/>
        <updated>2023-08-11T18:38:39.000Z</updated>
        <summary type="html"><![CDATA[I can understand part of the statement if you are using Adam. Since the trainable params are much less, we are saving on optimizer states. However, even we are not actually updating the pretrained model, we still need to compute the graidients for backpropagation to the lower layer of the lora head. The memory usage of gradients would not decrease. Please correct me if I am wrong. 
    submitted by    /u/Chen806  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]How to Improve YOLO v8 model performance ?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15oguh9/dhow_to_improve_yolo_v8_model_performance/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15oguh9/dhow_to_improve_yolo_v8_model_performance/"/>
        <updated>2023-08-11T18:37:15.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone! I'm working on a model using YOLO v8x to detect regions on identity cards, but it struggles with identifying address regions. This issue seems to stem from insufficient data. Would it be advisable to incorporate additional data containing addresses(other documents instead of identity card) to enhance the model's accuracy in detecting address regions?
    submitted by    /u/Ordinary_Run_2513  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Why does using multiple gpus lead to slower performance?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ogmyj/d_why_does_using_multiple_gpus_lead_to_slower/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ogmyj/d_why_does_using_multiple_gpus_lead_to_slower/"/>
        <updated>2023-08-11T18:29:12.000Z</updated>
        <summary type="html"><![CDATA[I read that using multiple gpus can improve inference performance, but I'm not sure why for my inference, its actually slower as I increase tensor_parallel_size. I know data transfer overhead and limited parallelism could be potential issues, are there ways to rectify this
 vllm = LLM( model="mosaicml/mpt-7b-instruct", trust_remote_code=True, dtype="float16", tensor_parallel_size=1, gpu_memory_utilization=.95, ) CPU times: user 3.66 s, sys: 262 ms, total: 3.93 s Wall time: 1.11 s vllm = LLM( model="mosaicml/mpt-7b-instruct", trust_remote_code=True, dtype="float16", tensor_parallel_size=2, gpu_memory_utilization=.95, ) CPU times: user 65.5 ms, sys: 32.2 ms, total: 97.7 ms Wall time: 1.27 s 
 ​
    submitted by    /u/candyman54  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How we evaluated LLMs in prod]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ogknd/d_how_we_evaluated_llms_in_prod/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ogknd/d_how_we_evaluated_llms_in_prod/"/>
        <updated>2023-08-11T18:26:43.000Z</updated>
        <summary type="html"><![CDATA[This is going to be a post about the challenges I faced while working with ChatGPT in my previous company and the things we did to overcome them over a 2+ month struggle. Check us out at www.twilix.io if anything below resonates with you and I hope you find some of it helpful.
 So to begin, in my previous company we invested a few months building a chatbot to help with user onboarding. At first everything was great, and we saw a 40% decrease in drop-off rates (which is significant given we were building a consumer facing app), but somehow over time this drop-off rate started creeping up again. Perplexed by the unexpected turn in metrics, management started to question the benefits of maintaining this chatbot and was skeptical that we were cherry picking examples to showcase its performance…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Train Stable Diffusion/Latent diffusion from scratch]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ofqzt/d_train_stable_diffusionlatent_diffusion_from/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ofqzt/d_train_stable_diffusionlatent_diffusion_from/"/>
        <updated>2023-08-11T17:55:38.000Z</updated>
        <summary type="html"><![CDATA[I'm currently in the process of developing a stable diffusion/latent diffusion model entirely from scratch. However, I'm a bit confused from the documentation of the original repositories (both from CompVis). My intention is to experiment with significantly smaller models and datasets while retaining the same architecture. Unfortunately, neither repository offers an official configuration for training the txt2img architecture.Through my exploration of the issues, I've observed that the training script provided by the latent diffusion repository does support txt2img (although an official configuration has not been made available yet). I'm curious if any of you might be familiar with better online resources or tutorials that can provide a clearer and more comprehensive understanding of the training process.
    submitted by    /u/Arabum97  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Tiny LVLM-eHub: Early Multimodal Experiments with Bard - OpenGVLab, Shanghai AI Laboratory 2023 - Encourages innovative strategies aimed at advancing multimodal techniques!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15of7lz/r_tiny_lvlmehub_early_multimodal_experiments_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15of7lz/r_tiny_lvlmehub_early_multimodal_experiments_with/"/>
        <updated>2023-08-11T17:35:19.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://github.com/OpenGVLab/Multi-Modality-Arena
 Github: https://github.com/OpenGVLab/Multi-Modality-Arena
 Abstract:
  
Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated significant progress in tackling complex multimodal tasks. Among these cutting-edge developments, Google's Bard stands out for its remarkable multimodal capabilities, promoting comprehensive comprehension and reasoning across various domains. This work presents an early and holistic evaluation of LVLMs' multimodal abilities, with a particular focus on Bard, by proposing a lightweight variant of LVLM-eHub, named Tiny LVLM-eHub. In comparison to the vanilla version, Tiny LVLM-eHub possesses several appealing properties. Firstly, it provides a systematic assessment of six categories of multimodal capabilities, including visual perception, visual knowledge acquisition, visual reasoning, visual commonsense, object hallucination, and embodied intelligence, through quantitative evaluation of 42 standard text-related visual benchmarks. Secondly, it conducts an in-depth analysis of LVLMs' predictions using the ChatGPT Ensemble Evaluation (CEE), which leads to a robust and accurate evaluation and exhibits improved alignment with human evaluation compared to the word matching approach. Thirdly, it comprises a mere 2.1K image-text pairs, facilitating ease of use for practitioners to evaluate their own offline LVLMs. Through extensive experimental analysis, this study demonstrates that Bard outperforms previous LVLMs in most multimodal capabilities except object hallucination, to which Bard is still susceptible. Tiny LVLM-eHub serves as a baseline evaluation for various LVLMs and encourages innovative strategies aimed at advancing multimodal techniques. 
  
https://preview.redd.it/i6x6p5bloihb1.jpg?width=1485&format=pjpg&auto=webp&s=7e91fe184844278b0a7e14090ae9aaef54b29f37
 ​
 ​
    submitted by    /u/Singularian2501  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] GPT Sequence Classification explainability or interpretability]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15of3ix/p_gpt_sequence_classification_explainability_or/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15of3ix/p_gpt_sequence_classification_explainability_or/"/>
        <updated>2023-08-11T17:31:03.000Z</updated>
        <summary type="html"><![CDATA[I’m using GPT-2 for Sequence Classification. I want to understand the words or sequences that lead to the predictions. Can you point me towards any papers, repos or libraries?
    submitted by    /u/how_the_turn_tablez  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI — weekly megathread!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15oebjf/ai_weekly_megathread/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15oebjf/ai_weekly_megathread/"/>
        <updated>2023-08-11T17:01:20.000Z</updated>
        <summary type="html"><![CDATA[This week in AI - provided by aibrews.com feel free to follow their newsletter
 News and Insights
  
Anthropic released a new version of Claude Instant, which offers faster performance at a lower price, with improvements in quote extraction, multilingual support, and question answering. It hallucinates less and is more resistant to jailbreaks [Details].
 Stability AI announced the release of StableCode, its first LLM generative AI product for coding [Details].
 Researchers present AudioLDM 2, a framework that utilizes the same learning method for speech, music, and sound effect generation [Details | GitHub].
 Researchers from CMU and others conducted tests on 14 large language models and found that OpenAI’s ChatGPT and GPT-4 were the most left-wing libertarian, while Meta’s LlaMA was the m…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Any suggestions on how I can improve my vision based PPO algorithm]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15odzh3/any_suggestions_on_how_i_can_improve_my_vision/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15odzh3/any_suggestions_on_how_i_can_improve_my_vision/"/>
        <updated>2023-08-11T16:48:14.000Z</updated>
        <summary type="html"><![CDATA[I am planning to throw my algorithm into a pronto server which enable me to increase the number of parallel workers. Currently, I am going with 24 workers. I'd appreciate more suggestions. Here's the pastebin link with syntax highlighting. Here's my code - 
 #Modified this code - https://github.com/DeepReinforcementLearning/DeepReinforcementLearningInAction/blob/master/Chapter%204/Ch4_book.ipynb #Also, modified this code - https://github.com/higgsfield/RL-Adventure-2/blob/master/1.actor-critic.ipynb # Also, modified this code - https://github.com/ericyangyu/PPO-for-Beginners/blob/9abd435771aa84764d8d0d1f737fa39118b74019/ppo.py#L151 # Got a lot of help from the subreddit - reinforcement_learning if __name__ == '__main__': import numpy as np import gymnasium as gym from gymnasium.wrappers im…]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pika Labs: Tutorial for Beginners (Text-to-Video Platform)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15odcm8/pika_labs_tutorial_for_beginners_texttovideo/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15odcm8/pika_labs_tutorial_for_beginners_texttovideo/"/>
        <updated>2023-08-11T16:23:08.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/SplitYOLO  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Commercial for BBC Planet Earth used AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15obvjn/commercial_for_bbc_planet_earth_used_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15obvjn/commercial_for_bbc_planet_earth_used_ai/"/>
        <updated>2023-08-11T15:24:40.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Grindmaster_Flash  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What's is everyones outlook on AI swarms? Does it hold promise, or are larger systems going to be dominant?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15obkax/d_whats_is_everyones_outlook_on_ai_swarms_does_it/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15obkax/d_whats_is_everyones_outlook_on_ai_swarms_does_it/"/>
        <updated>2023-08-11T15:12:26.000Z</updated>
        <summary type="html"><![CDATA[I've been researching AI swarms, and it seems to make more sense to have a lot of smaller models doing tasks separately. Thoughts?
    submitted by    /u/deepengineai  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amazon Translate enhances its custom terminology to improve translation accuracy and fluency]]></title>
        <id>c64eb9acc71a14728412639ddf24536becadb72f</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/amazon-translate-enhances-its-custom-terminology-to-improve-translation-accuracy-and-fluency/"/>
        <updated>2023-08-11T14:59:56.000Z</updated>
        <summary type="html"><![CDATA[Amazon Translate is a neural machine translation service that delivers fast, high-quality, affordable, and customizable language translation. When you translate from one language to another, you want your machine translation to be accurate, fluent, and most importantly contextual. Domain-specific and language-specific customizable terminology is a key requirement for many government and commercial organizations. Custom terminology […]]]></summary>
        <author>
            <name>Sathya Balakrishnan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-shot text classification with Amazon SageMaker JumpStart]]></title>
        <id>6bcfbfb6e7f1ec95052da8f46eb334f56a323361</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/zero-shot-text-classification-with-amazon-sagemaker-jumpstart/"/>
        <updated>2023-08-11T14:56:51.000Z</updated>
        <summary type="html"><![CDATA[Natural language processing (NLP) is the field in machine learning (ML) concerned with giving computers the ability to understand text and spoken words in the same way as human beings can. Recently, state-of-the-art architectures like the transformer architecture are used to achieve near-human performance on NLP downstream tasks like text summarization, text classification, entity recognition, […]]]></summary>
        <author>
            <name>David Laredo</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medication Mix-up Incident Involving My Mother]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15o9sah/medication_mixup_incident_involving_my_mother/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15o9sah/medication_mixup_incident_involving_my_mother/"/>
        <updated>2023-08-11T14:02:13.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Rightperson1  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pushing boundaries with Generative AI: How Program-aided Language model (PAL) enhances Large Language Models (LLMs) for superior AI performance]]></title>
        <id>https://www.datasciencecentral.com/?p=62858</id>
        <link href="https://www.datasciencecentral.com/pushing-boundaries-with-generative-ai-how-program-aided-language-model-pal-enhances-large-language-models-llms-for-superior-ai-performance/"/>
        <updated>2023-08-11T13:55:00.000Z</updated>
        <summary type="html"><![CDATA[Source: ArabianBusiness Takeaways Artificial Intelligence (AI) continues to evolve at a rapid pace, with groundbreaking strides in generative capabilities playing a critical role in defining this ever-evolving landscape. One such transformative leap is the advent of Program-Aided Language models (PAL), an innovative solution that revolutionizes how Language Learning Models (LLMs) function. This article delves into… Read More »Pushing boundaries with Generative AI: How Program-aided Language model (PAL) enhances Large Language Models (LLMs) for superior AI performance
The post Pushing boundaries with Generative AI: How Program-aided Language model (PAL) enhances Large Language Models (LLMs) for superior AI performance appeared first on Data Science Central.]]></summary>
        <author>
            <name>Rudrendu Kumar Paul</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is Hidden Size in current transformers an overkill?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15o8x8h/d_is_hidden_size_in_current_transformers_an/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15o8x8h/d_is_hidden_size_in_current_transformers_an/"/>
        <updated>2023-08-11T13:26:51.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I have written a post discussion whether or not the hidden size in transformers is an overkill. 
 TLDR;
 I show that an embedding size of 2048 is too much to represent just one token like `is` but rather, it can encode an average 8 tokens with up-to 16 tokens almost losslessly. 
 I think if we can design more compute efficient transformers with some of the ideas that I explore in the post.
 Of course this is not a proper research with ablation studies and empirical analysis. But I would love to hear your thoughts on this topic.
    submitted by    /u/NaxAlpha  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Client project matching AI recommendations?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15o83yb/client_project_matching_ai_recommendations/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15o83yb/client_project_matching_ai_recommendations/"/>
        <updated>2023-08-11T12:51:48.000Z</updated>
        <summary type="html"><![CDATA[At my company, we collaborate closely with top-level executives from Fortune 500 companies and other industry leaders, helping them identify and secure the right partners for crucial digital transformation initiatives. When these executives present us with their project specifics, budgets, obstacles, and schedules, we take charge of finding the right partners for their RFP process, enhancing the entire workflow for efficiency and effectiveness. Currently, I have a collection of RFP projects and I’m keen on leveraging AI to simplify the task of identifying potential partners to call. I provided ChatGPT with all of my various project details and would inquire, ‘Which of my client projects align well with X company, and what are the reasons?’ OR “Would X company align with any of my projects?” The AI started off well, but eventually became confused and started making mistakes. Are there any systems available that could assist me in this project matching process?
    submitted by    /u/Ajkrouse  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Using Machine Learning for Accesibility: Personal AI Shelf Inspector for Visually Impaired Persons]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15o7mbx/p_using_machine_learning_for_accesibility/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15o7mbx/p_using_machine_learning_for_accesibility/"/>
        <updated>2023-08-11T12:30:02.000Z</updated>
        <summary type="html"><![CDATA[Personal Shelf Inspector is an application that helps visually impaired people during their day-to-day shopping. The application is based on a simple neural network and was created as a part of the AI for Accesibility Hackathon in 2020 in Prague.
 We decided to build free tools that make shopping for visually impaired people more accessible. These tools can be implemented in any retail chain within the loyalty app or in-store. 
 ​
 https://preview.redd.it/ehoxec5l4hhb1.png?width=1072&format=png&auto=webp&s=c96a5dd326cbfc8dd9bde9d84d45167d172ea27d
 The Idea
 A visually impaired person only needs their smartphone to use our tools. Personal Shelf Inspector is a web application that reads the price and product name from a price tag. The algorithm selects the price tag closest to the centre of the photo and sends it to the model, which reads the price and product name on the price tag. Then it returns this information to the application, which appears as text on the screen.
 ​
 https://preview.redd.it/e098rtcn4hhb1.png?width=845&format=png&auto=webp&s=73a216ebf987e476d52091717ac156ac56daa29b
 The voice-over built into the user's mobile phone reads this text aloud. The app also helps to read the banknote values and read from a live video.
 What could be other cool ideas and concepts to help making the world more accesible using AI and Machine Learning? 
 Feel free to share comments and impressions in the comments
    submitted by    /u/DataSentics  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Research] How InstructBLIP's authors do the datasets transformation to instruction data]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15o6h7p/research_how_instructblips_authors_do_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15o6h7p/research_how_instructblips_authors_do_the/"/>
        <updated>2023-08-11T11:35:46.000Z</updated>
        <summary type="html"><![CDATA[In "InstructBLIP" paper, authors say: "We transform 26 datasets into the instruction tuning format" in order to create a general-purpose vision language model via instruction tuning. However, they did not provide details on how they did this transformation. At a first glance, three ways come to mind:
  
They use ChatGPT/GPT-4 to automatically transform them.
 They define and code rules to automatically transform them.
 They manually transform them (highly improbable)
  
Someone knows the answer? Thank you so much
    submitted by    /u/jrodriguezortega  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Open-Source Machine Learning in Computational Chemistry]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15o60pa/r_opensource_machine_learning_in_computational/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15o60pa/r_opensource_machine_learning_in_computational/"/>
        <updated>2023-08-11T11:13:18.000Z</updated>
        <summary type="html"><![CDATA[We wrote a perspective on open source machine learning in computational chemistry in JCIM_JCTC. It was an incredible amount of work and I hope readers will find it useful and educational.
 https://pubs.acs.org/doi/10.1021/acs.jcim.3c00643
 If you need a preprint, you can find it on Researchgate.
 https://www.researchgate.net/publication/372470285_Open-Source_Machine_Learning_in_Computational_Chemistry
    submitted by    /u/poorgenes  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Neural Wave Machines: Learning Spatiotemporally Structured Representations with Locally Coupled Oscillatory Recurrent Neural Networks]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15o34ts/r_neural_wave_machines_learning_spatiotemporally/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15o34ts/r_neural_wave_machines_learning_spatiotemporally/"/>
        <updated>2023-08-11T08:34:47.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/hardmaru  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]: Single Board Computer with accelerator as a hobby project]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15o2rlk/d_single_board_computer_with_accelerator_as_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15o2rlk/d_single_board_computer_with_accelerator_as_a/"/>
        <updated>2023-08-11T08:13:36.000Z</updated>
        <summary type="html"><![CDATA[Does anybody have a good recommendation for an SBC with AI accelerator (NPU) where I could attach a camera and train some YOLO models on the device itself for object recognition?
    submitted by    /u/LM1117  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[🐑 Dreamer V3 in SheepRL 🐑]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15o2cty/dreamer_v3_in_sheeprl/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15o2cty/dreamer_v3_in_sheeprl/"/>
        <updated>2023-08-11T07:49:53.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone, we finally ended our journey through Dreamer, and we released the last version, Dreamer V3 in SheepRL.
 Our implementation follows closely the author's one, and is very well documented, with a blog post to explain the details and differences between this version and Dreamer V2.
 Together with Dreamer, we also have Plan2Explore with Dreamer v1 and v2.
 Finally, we completed the integration with Diambra, so you can try your agents on new (funnier) benchmarks.
 Check it out and feel free to contribute. Every feedback is appreciated :) 
    submitted by    /u/TrottoDng  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What's the difference between GVF and Options?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15o2bam/whats_the_difference_between_gvf_and_options/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15o2bam/whats_the_difference_between_gvf_and_options/"/>
        <updated>2023-08-11T07:47:23.000Z</updated>
        <summary type="html"><![CDATA[Two cool concepts - General Value Functions & Options. Seem to be for the same purpose.
 ​
 What are the differences between these 2 strategies, and what are the benefits of each? Thanks!
    submitted by    /u/Cultural-Average3959  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VQA Recommendations, anyone?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15o28yf/vqa_recommendations_anyone/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15o28yf/vqa_recommendations_anyone/"/>
        <updated>2023-08-11T07:43:38.000Z</updated>
        <summary type="html"><![CDATA[Hi, what VQA platforms do you all have experience with? What would you think would be the most promising platform at the moment, and in the future? I've been playing around with Google Vertex AI (https://console.cloud.google.com/vertex-ai/generative/) but the current results are ... meh! 🤷‍♂️ 
 Any other recommendations?
    submitted by    /u/emc  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 8/11/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15o0ctt/oneminute_daily_ai_news_8112023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15o0ctt/oneminute_daily_ai_news_8112023/"/>
        <updated>2023-08-11T05:58:30.000Z</updated>
        <summary type="html"><![CDATA[A new AI algorithm has detected a potentially hazardous asteroid that had gone unnoticed by human observers, slated to fly by Earth. The algorithm, HelioLinc3D, was explicitly designed for the Vera Rubin Observatory currently under construction in Northern Chile.[1]
 The U.S. Defense Department has created a task force to evaluate and guide the application of generative artificial intelligence for national security purposes, amid an explosion of public interest in the technology.[2]
 China’s largest web and cloud providers (Alibaba, Baidu, ByteDance, and Tencent)are lining up to buy as many Nvidia GPUs as they can while they still can get their hands on them.[3]
 At Black Hat USA 2023, DARPA issued a call to top computer scientists, AI experts, software developers, and beyond to participate in the AI Cyber Challenge (AIxCC) – a two-year competition aimed at driving innovation at the nexus of AI and cybersecurity to create a new generation of cybersecurity tools.[4]
  
Sources:
 [1] https://www.giantfreakinrobot.com/sci/ai-asteroids.html
 [2] https://www.c4isrnet.com/artificial-intelligence/2023/08/10/pentagon-establishes-task-force-lima-to-study-generative-ai-issues/
 [3] https://www.theregister.com/2023/08/11/chinese_web_giants_nvidia/
 [4] https://www.hstoday.us/industry/industry-news/darpa-ai-cyber-challenge-aims-to-secure-nations-most-critical-software/ 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Agents Simulate a Town 🤯 Generative Agents: Interactive Simulacra of Human Behavior.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15nz2ph/ai_agents_simulate_a_town_generative_agents/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15nz2ph/ai_agents_simulate_a_town_generative_agents/"/>
        <updated>2023-08-11T04:49:49.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/crua9  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Extension LLM Model That Also Analyzes Page Text]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15nx7z9/an_extension_llm_model_that_also_analyzes_page/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15nx7z9/an_extension_llm_model_that_also_analyzes_page/"/>
        <updated>2023-08-11T03:17:36.000Z</updated>
        <summary type="html"><![CDATA[I have developed this chrome extension named Lupin which allows you to ask your question about your current tab directly to chatGPT by analysing the page's body. For instance, if you're looking into an Amazon product, you can ask your question about it directly to Lupin.
 https://chrome.google.com/webstore/detail/lupin/kdfaiheakopcdabhlcnbmfjffanaedgm?hl=en&authuser=0
 Right now, this is an open-beta phase, so I am open to any feedback. I have improved some aspects based on the feedback I received but I want to improve as much as possible before going for version 1.1
 If you wanna join me on this crusade and work together, DM me.
 Amor Fati,
 AAC
    submitted by    /u/AttilaTheHappyHun  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RVC AI samples examples]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15nsuxj/rvc_ai_samples_examples/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15nsuxj/rvc_ai_samples_examples/"/>
        <updated>2023-08-11T00:02:45.000Z</updated>
        <summary type="html"><![CDATA[Hello, is there anywhere I can find .wav files to see examples about how would be the ideal type of samples I should provide my AI so it learns a more wide register of my voice?
 I didn't manage to find anything like that
 Sorry if it's a newbie question
    submitted by    /u/Callumpi  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[THIS Is What Comes Next For AI - The Simulation | Interview with Fable Studio CEO - Edward Saatchi]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15nrxg7/this_is_what_comes_next_for_ai_the_simulation/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15nrxg7/this_is_what_comes_next_for_ai_the_simulation/"/>
        <updated>2023-08-10T23:23:35.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Sonic_Improv  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] OpenAI API function calling]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15nr8j3/d_openai_api_function_calling/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15nr8j3/d_openai_api_function_calling/"/>
        <updated>2023-08-10T22:55:52.000Z</updated>
        <summary type="html"><![CDATA[How do you think OpenAI implemented the function calling feature, It seems like another contextual generation piece from the look of it but any interesting ideas and papers around this topic?
    submitted by    /u/neuro_boogie  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLMs Challenges and Approaches Panel [N]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15noqwr/llms_challenges_and_approaches_panel_n/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15noqwr/llms_challenges_and_approaches_panel_n/"/>
        <updated>2023-08-10T21:18:46.000Z</updated>
        <summary type="html"><![CDATA[​
 https://preview.redd.it/wl1gtcngnchb1.jpg?width=1500&format=pjpg&auto=webp&s=24e35d852603c6139fd67f79457ec593fbad99f7
 If you're someone who's curious about or working with LLMs there's a cool panel discussion coming up: 
  
Comparing the pros and cons of using existing LLMs, prompt engineering, and fine-tuning on custom datasets for different enterprise use cases.
 Fine-Tuning LLMs: Exploring the advantages and challenges of fine-tuning LLMs on custom datasets to align with specific business objectives.
 Tools and platforms: Discussing the various tools and platforms to facilitate LLM implementation 
 Overcoming Challenges: Addressing the challenges associated with adopting LLMs, including data privacy, creating high quality datasets, computational resources, ethical considerations, and the need for specialized expertise.
 Future Directions: Exploring emerging trends, advancements, and potential future applications of LLMs in the enterprise context.
  
Here's the event info: https://www.eventbrite.com/e/large-language-models-for-enterprise-success-challenges-and-approaches-tickets-695089811337?aff=oddtdtcreator
    submitted by    /u/UpstairsLeast7642  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] List of Awesome AI Agents like AutoGPT and BabyAGI / Many open-source Agents with code included!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15no3vz/d_list_of_awesome_ai_agents_like_autogpt_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15no3vz/d_list_of_awesome_ai_agents_like_autogpt_and/"/>
        <updated>2023-08-10T20:54:17.000Z</updated>
        <summary type="html"><![CDATA[Github: https://github.com/e2b-dev/awesome-ai-agents and https://github.com/EmbraceAGI/Awesome-AGI 
    submitted by    /u/Singularian2501  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] 🎹 Record Labels are monetizing AI-created Music after trying to kill it.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15nmgdb/d_record_labels_are_monetizing_aicreated_music/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15nmgdb/d_record_labels_are_monetizing_aicreated_music/"/>
        <updated>2023-08-10T19:51:20.000Z</updated>
        <summary type="html"><![CDATA[Google and Universal Music are discussing licensing artists' voices and melodies to develop AI-generated songs fans can create and pay for, seeking to get ahead of the controversial "deepfake" music trend. Though some stars oppose their work being mimicked, artists could opt-in to receive royalties in a model akin to how YouTube now pays for user-generated content.
 For Google, AI music would boost its generative AI offerings against competitors. But significant ethical hurdles around consent and IP must still be addressed in developing a legitimate AI music market.
    submitted by    /u/Yavero  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[d] transformers for video activity recognition?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15nlhje/d_transformers_for_video_activity_recognition/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15nlhje/d_transformers_for_video_activity_recognition/"/>
        <updated>2023-08-10T19:14:33.000Z</updated>
        <summary type="html"><![CDATA[I am trying to work with the UCF crime dataset and want to use transformers for video activity recognition, Does anyone have pointers to example projects as to which ones are good starting points?
    submitted by    /u/bluzkluz  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[skrl with multiple discrete actions]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15nky6g/skrl_with_multiple_discrete_actions/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15nky6g/skrl_with_multiple_discrete_actions/"/>
        <updated>2023-08-10T18:54:10.000Z</updated>
        <summary type="html"><![CDATA[I'm new to RL, and I was trying to train an agent to move items in a 2D grid. The agent needs to output the row number, column number, and item index, and right now I'm modeling them as discrete actions. I am not sure what kind of agent to use to solve this problem. I tried PPO, but I'm not sure what the output of the policy module should be in this case. I'd be grateful for any help.
    submitted by    /u/LostPigeon25  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] I ran Llama 2 on my Mac in < 5 mins]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15nkirl/p_i_ran_llama_2_on_my_mac_in_5_mins/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15nkirl/p_i_ran_llama_2_on_my_mac_in_5_mins/"/>
        <updated>2023-08-10T18:37:17.000Z</updated>
        <summary type="html"><![CDATA[So Llama 2 sounds awesome, but I really wanted to run it locally on my Macbook Pro instead of on a Linux box with an NVIDIA GPU. So I put the llama.cpp GGML models into the XetHub Llama 2 repo so I can use the power of Llama 2 locally. It now takes me 5 seconds to mount Llama 2 and it loads the GGML model almost instantly. Here’s how I did it:
  
Create an account: Go to xethub.com and Sign In with GitHub
 Quick start: Go to xethub.com/explore/quickstart and follow the Install & Setup steps (xethub.com/explore/install)
 pip install pyxet for Python SDK and CLI
 Set up authentication: Create a Personal Access Token and then run the login command from a Terminal so your ~/.xetconfig is set up with your login token.
  
Here’s the code to get Llama 2 up and running on your Mac laptop in a few …]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Benchmarking g5.12xlarge (4xA10) vs 1xA100 inference performance running upstage_Llama-2-70b-instruct-v2 (4-bit & 8-bit)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15nkdq2/r_benchmarking_g512xlarge_4xa10_vs_1xa100/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15nkdq2/r_benchmarking_g512xlarge_4xa10_vs_1xa100/"/>
        <updated>2023-08-10T18:31:51.000Z</updated>
        <summary type="html"><![CDATA[Hi Reddit folks, I wanted to share some benchmarking data I recently compiled running upstage_Llama-2-70b-instruct-v2 on two different hardware setups. If you'd like to see the spreadsheet with the raw data you can check out this link.
 Hardware Config #1: AWS g5.12xlarge - 4 x A10 w/ 96GB VRAM
 Hardware Config #2: Vultr - 1 x A100 w/ 80GB VRAM
 A few questions I wanted to answer:
  
How does the inference speed (tokens/s) between these two configurations compare?
 How does the number of input tokens impact inference speed?
 How many input tokens can these machines handle before they start to hit OOM?
 How does 4-bit vs 8-bit quantization affect all of the above?
  
Why this model?
 I chose upstage_Llama-2-70b-instruct-v2 because it's the current #1 performing OS model on HuggingFace's LLM…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Please help me understand these files]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15njsnw/please_help_me_understand_these_files/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15njsnw/please_help_me_understand_these_files/"/>
        <updated>2023-08-10T18:09:42.000Z</updated>
        <summary type="html"><![CDATA[I'm working on a skin cancer detection app where I can upload a picture of a mole or other skin lesion and have it tell me if its cancerous and what type of cancer it is, and I downloaded the HAM10000 database for it which came with 5 CSV files. I kind of understand the metadata CSV file but the other 4 don't make sense to me. They have a bunch of numbers and either L or RGB at the end of the file names. Can someone help me make sense of these?
    submitted by    /u/timing_snow  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the future of smart cities through data science]]></title>
        <id>https://www.datasciencecentral.com/?p=62847</id>
        <link href="https://www.datasciencecentral.com/understanding-the-future-of-smart-cities-through-data-science/"/>
        <updated>2023-08-10T18:03:42.000Z</updated>
        <summary type="html"><![CDATA[Learn about the challenges of data privacy and security, and the potential of smart technologies in creating efficient, livable urban environments.
The post Understanding the future of smart cities through data science appeared first on Data Science Central.]]></summary>
        <author>
            <name>Noami Woods</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Images on the subject of AI.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15niyoo/images_on_the_subject_of_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15niyoo/images_on_the_subject_of_ai/"/>
        <updated>2023-08-10T17:38:37.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Philipp  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Discovering Adaptable Symbolic Algorithms from Scratch - Google and MSU]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15nitva/r_discovering_adaptable_symbolic_algorithms_from/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15nitva/r_discovering_adaptable_symbolic_algorithms_from/"/>
        <updated>2023-08-10T17:33:18.000Z</updated>
        <summary type="html"><![CDATA[Autonomous robots deployed in the real world will need control policies that rapidly adapt to environmental changes. To this end, we propose AutoRobotics-Zero (ARZ), a method based on AutoML-Zero that discovers zero-shot adaptable policies from scratch. In contrast to neural network adaption policies, where only model parameters are optimized, ARZ can build control algorithms with the full expressive power of a linear register machine. We evolve modular policies that tune their model parameters and alter their inference algorithm on-the-fly to adapt to sudden environmental changes. We demonstrate our method on a realistic simulated quadruped robot, for which we evolve safe control policies that avoid falling when individual limbs suddenly break. This is a challenging task in which two popular neural network baselines fail. Finally, we conduct a detailed analysis of our method on a novel and challenging non-stationary control task dubbed Cataclysmic Cartpole. Results confirm our findings that ARZ is significantly more robust to sudden environmental changes and can build simple, interpretable control policies.
 Paper: https://arxiv.org/abs/2307.16890
 Video: https://youtu.be/sEFP1Hay4nE
    submitted by    /u/VishDev  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R} On Hate Scaling Laws For Data-Swamps]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15niph3/r_on_hate_scaling_laws_for_dataswamps/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15niph3/r_on_hate_scaling_laws_for_dataswamps/"/>
        <updated>2023-08-10T17:28:53.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/VishDev  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Heat-assisted detection and ranging - Nature]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15nilpw/r_heatassisted_detection_and_ranging_nature/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15nilpw/r_heatassisted_detection_and_ranging_nature/"/>
        <updated>2023-08-10T17:24:58.000Z</updated>
        <summary type="html"><![CDATA[Machine perception uses advanced sensors to collect information about the surrounding scene for situational awareness. State-of-the-art machine perception using active sonar, radar, and LiDAR to enhance camera vision faces difficulties when the number of intelligent agents scales up. Exploiting omnipresent heat signals could be a new frontier for scalable perception. However, objects and their environment constantly emit and scatter thermal radiation, leading to textureless images famously known as the ‘ghosting effect’. Thermal vision thus has no specificity limited by information loss, whereas thermal ranging—crucial for navigation—has been elusive even when combined with artificial intelligence (AI). Here, we propose and experimentally demonstrate heat-assisted detection and ranging (HADAR) overcoming this open challenge of ghosting and benchmark it against AI-enhanced thermal sensing. HADAR not only sees texture and depth through the darkness as if it were day but also perceives decluttered physical attributes beyond RGB or thermal vision, paving the way to fully passive and physics-aware machine perception. We develop HADAR estimation theory and address its photonic shot-noise limits depicting information-theoretic bounds to HADAR-based AI performance. HADAR ranging at night beats thermal ranging and shows an accuracy comparable with RGB stereovision in daylight. Our automated HADAR thermography reaches the Cramér–Rao bound on temperature accuracy, beating existing thermography techniques. Our work leads to a disruptive technology that can accelerate the Fourth Industrial Revolution (Industry 4.0) with HADAR-based autonomous navigation and human–robot social interactions.
 Paper: https://www.nature.com/articles/s41586-023-06174-6
 Video: https://youtu.be/WKrzmaixAC0
    submitted by    /u/VishDev  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is everything just transformers now?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15nhxfz/d_is_everything_just_transformers_now/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15nhxfz/d_is_everything_just_transformers_now/"/>
        <updated>2023-08-10T16:59:33.000Z</updated>
        <summary type="html"><![CDATA[I was watching this talk where they were showing that basically every task in machine learning has been replaced by the transformer architecture. For instance, where a convolution neural network might have been used for image recognition in the past, the predominant strategy now is just to use a transformer instead. How true is this? Is it worth learning any other architecture than transformers for current state of the art research?
    submitted by    /u/Active-Confidence926  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is Latent ODE an imputation model?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ngux6/d_is_latent_ode_an_imputation_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ngux6/d_is_latent_ode_an_imputation_model/"/>
        <updated>2023-08-10T16:18:07.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 Is Latent ODE an imputation model? If so, how does it handle missing values in the case of irregular sampled time series data? 
 Latent ODE - Latent ODEs for Irregularly-Sampled Time Series (https://arxiv.org/abs/1907.03907)
    submitted by    /u/flaubart9  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Microsoft at KDD 2023: Advancing health at the speed of AI]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=959091</id>
        <link href="https://www.microsoft.com/en-us/research/blog/microsoft-at-kdd-2023-advancing-health-at-the-speed-of-ai/"/>
        <updated>2023-08-10T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[This content was given as a keynote at the Workshop of Applied Data Science for Healthcare and covered during a tutorial at the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, a premier forum for advancement, education, and adoption of the discipline of knowledge discovering and data mining. Recent and noteworthy advancements in […]
The post Microsoft at KDD 2023: Advancing health at the speed of AI appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Brenda Potts</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] txtai 6.0 - the all-in-one embeddings database]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15nf7dv/p_txtai_60_the_allinone_embeddings_database/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15nf7dv/p_txtai_60_the_allinone_embeddings_database/"/>
        <updated>2023-08-10T15:14:16.000Z</updated>
        <summary type="html"><![CDATA[txtai is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows. 
 This major release adds sparse, hybrid and subindexes to the embeddings interface. It also makes significant improvements to the LLM pipeline workflow.
 See links below for more.
 GitHub: https://github.com/neuml/txtai
 Release Notes: https://github.com/neuml/txtai/releases/tag/v6.0.0
 Article: https://medium.com/neuml/whats-new-in-txtai-6-0-7d93eeedf804
    submitted by    /u/davidmezzetti  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Build a centralized monitoring and reporting solution for Amazon SageMaker using Amazon CloudWatch]]></title>
        <id>74d8154c6042af7d531f3290dccfdea3e10fc94e</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/build-a-centralized-monitoring-and-reporting-solution-for-amazon-sagemaker-using-amazon-cloudwatch/"/>
        <updated>2023-08-10T14:48:27.000Z</updated>
        <summary type="html"><![CDATA[In this post, we present a cross-account observability dashboard that provides a centralized view for monitoring SageMaker user activities and resources across multiple accounts. It allows the end-users and cloud management team to efficiently monitor what ML workloads are running, view the status of these workloads, and trace back different account activities at certain points of time.]]></summary>
        <author>
            <name>Jie Dong</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Creating a Traveling Salesman Tour of Texas with Mathematica]]></title>
        <id>https://www.johndcook.com/blog/?p=202611</id>
        <link href="https://www.johndcook.com/blog/2023/08/10/texas-tour/"/>
        <updated>2023-08-10T14:47:07.000Z</updated>
        <summary type="html"><![CDATA[A Traveling Salesman tour visits a list of destinations using the shortest path. There’s an obvious way to find the shortest path connecting N points: try all N! paths and see which one is shortest. Unfortunately, that might take a while. Texas has 254 counties, and so calculating a tour of Texas counties by brute […]
Creating a Traveling Salesman Tour of Texas with Mathematica first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Ideal embedding models for classifying news articles to topics, specified as sentences]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15neh6k/d_ideal_embedding_models_for_classifying_news/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15neh6k/d_ideal_embedding_models_for_classifying_news/"/>
        <updated>2023-08-10T14:45:10.000Z</updated>
        <summary type="html"><![CDATA[I’m looking to build functionality that would allow a user to specify topics to be notified about in the news, eg. “Tax law changes in New York”, and notify them of recently published news articles related to that topic.
 Would the ideal strategy be to find relating articles to topics, or topics relating to articles as they come in? What models would be ideal here? I’m fairly new to this, so any help would be appreciated.
    submitted by    /u/ByteBuff  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implement parallel training using the multiprocessing module.]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15nd91l/implement_parallel_training_using_the/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15nd91l/implement_parallel_training_using_the/"/>
        <updated>2023-08-10T13:56:28.000Z</updated>
        <summary type="html"><![CDATA[This project allows you to easily implement parallel training with the multiprocessing module.
    submitted by    /u/NoteDancing  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Intermediate/Advanced AI/ML Bootcamps]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15nd1zp/d_intermediateadvanced_aiml_bootcamps/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15nd1zp/d_intermediateadvanced_aiml_bootcamps/"/>
        <updated>2023-08-10T13:48:35.000Z</updated>
        <summary type="html"><![CDATA[Long time listener, first time caller. 
 I am looking to spearhead the impending transition of understanding AI/ML at my organization and am looking for community suggestions in courses and bootcamps that could provide a deeper knowledge for some of my work projects in the future. Specifically, I feel unequipped in how to properly test and validate models. 
 I have a computer science background with strong skills in data analytics and programming. I’ve also taken several introductory courses at a high level for AI. 
 Does anyone have suggestions or experiences for 1-4 week long bootcamps or intensive courses? I prefer in-person (anywhere in US) but would also consider live-online remote courses. Price is not a concern. 
 Thanks in advance.
    submitted by    /u/DungeonsGalore  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nvidia unveils GH200 Superchips for 'most complex AI workloads']]></title>
        <id>https://www.reddit.com/r/artificial/comments/15nchf3/nvidia_unveils_gh200_superchips_for_most_complex/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15nchf3/nvidia_unveils_gh200_superchips_for_most_complex/"/>
        <updated>2023-08-10T13:25:13.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/intengineering  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Challenge Accepted: GeForce NOW Fires Up the Cloud With Ultimate Challenge and First Bethesda Games]]></title>
        <id>https://blogs.nvidia.com/?p=66072</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/10/geforce-now-thursday-aug-10/"/>
        <updated>2023-08-10T13:00:35.000Z</updated>
        <summary type="html"><![CDATA[Rise and shine, it’s time to quake up — the GeForce NOW Ultimate KovaaK’s challenge kicks off at the QuakeCon gaming festival today, giving gamers everywhere the chance to play to their ultimate potential with ultra-high 240 frames per second streaming. On top of bragging rights, top scorers can win some sweet prizes — including Read article >]]></summary>
        <author>
            <name>GeForce NOW Community</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] . AI hackathon project ideas(NLP based)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15natmn/p_ai_hackathon_project_ideasnlp_based/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15natmn/p_ai_hackathon_project_ideasnlp_based/"/>
        <updated>2023-08-10T12:13:53.000Z</updated>
        <summary type="html"><![CDATA[so, there is an AI Hackathon coming up next week. This gonna be my first hackathon and wanna win too, however i donot know what to build in "health sector and Open tech". I have few experience in NLP. Kindly please suggest any which can be build in 24hrs period. 
    submitted by    /u/Suspicious-Row-8804  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Strength in Numbers: NVIDIA and Generative Red Team Challenge Unleash Thousands to Vet Security at DEF CON]]></title>
        <id>https://blogs.nvidia.com/?p=66051</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/10/nvidia-generative-red-team-challenge/"/>
        <updated>2023-08-10T12:00:06.000Z</updated>
        <summary type="html"><![CDATA[Thousands of hackers will tweak, twist and probe the latest generative AI platforms this week in Las Vegas as part of an effort to build more trustworthy and inclusive AI. Collaborating with the hacker community to establish best practices for testing next-generation AI, NVIDIA is participating in a first-of-its-kind test of industry-leading LLM solutions, including Read article >]]></summary>
        <author>
            <name>Daniel Rohrer</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for TTS that converts a written dialog into a spoken one.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15na9tw/looking_for_tts_that_converts_a_written_dialog/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15na9tw/looking_for_tts_that_converts_a_written_dialog/"/>
        <updated>2023-08-10T11:49:32.000Z</updated>
        <summary type="html"><![CDATA[The titles says it all - obviously it would be great to have a range of voices as in Elevenlabs for instance. If not, has anyone done this and found an easy way.
    submitted by    /u/dextercool  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Babe, wake up. That weird ™Happy Toys!™ commercial is on again]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15n9sts/babe_wake_up_that_weird_happy_toys_commercial_is/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15n9sts/babe_wake_up_that_weird_happy_toys_commercial_is/"/>
        <updated>2023-08-10T11:27:26.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/PerryJ  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Flexible object detection of unknown objects.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15n9rt3/p_flexible_object_detection_of_unknown_objects/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15n9rt3/p_flexible_object_detection_of_unknown_objects/"/>
        <updated>2023-08-10T11:26:05.000Z</updated>
        <summary type="html"><![CDATA[I'm basically trying to create an object detection system that learns the more objects it sees. My problem is that object detection requires actualy objects that the AI will be able to put into a certain category to identify the object.
 I want to make an AI that is able to find out that there is an object, but it shouldnt need to know what the object is. I want the AI to be able to find objects in an image in realtime without needing to know what the object is. It's supposed to grasp the concept of what an object is. Are there any methods or datasets that would make something like this possible?
  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Generated Music Video is becoming a thing! This video is incredible!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15n9dg7/ai_generated_music_video_is_becoming_a_thing_this/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15n9dg7/ai_generated_music_video_is_becoming_a_thing_this/"/>
        <updated>2023-08-10T11:06:23.000Z</updated>
        <summary type="html"><![CDATA[The singularity is nearer
    submitted by    /u/Psytorpz  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Navigating Data Issues with Cleanlab and Spotlight [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15n8h1h/navigating_data_issues_with_cleanlab_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15n8h1h/navigating_data_issues_with_cleanlab_and/"/>
        <updated>2023-08-10T10:21:03.000Z</updated>
        <summary type="html"><![CDATA[The complete code of this article and other articles about handling data issues is available in the accompanying notebook on GitHub.
 Data cleaning, a crucial step in machine learning, addresses challenges like mislabeled examples, outliers, and duplicates. Our newest article features:
  
Cleanlab: A tool that uses confident learning techniques to detect data issues.
 Spotlight by Renumics: An advanced visualization tool to review and explore the data issues detected by Cleanlab. It provides interactive features like the Similarity Map for pinpointing problem clusters.
  
The article showcases the integration of these tools using the CIFAR-100 dataset. It details:
  
Detecting three main issues: label inconsistencies, outliers, and near-duplicates.
 Spotlight's interactive environment to review these detected problems.
 Using the similarity map to navigate and understand the data, making it easier to identify and address issues.
  
Full Article: Navigate Data Issues: Interactively explore results of Cleanlab with Renumics Spotlight
    submitted by    /u/DocBrownMS  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Language retrieval models explained simply]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15n5hcu/d_language_retrieval_models_explained_simply/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15n5hcu/d_language_retrieval_models_explained_simply/"/>
        <updated>2023-08-10T07:34:27.000Z</updated>
        <summary type="html"><![CDATA[What are language retrieval models exactly?
 I've been hearing more and more of those in the context of not needing embeddings retrieval and putting useful text in the system prompt.
 How do they work in simple terms?
    submitted by    /u/Specialist_Ice_5715  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Can we get a tag or a weekly mega thread for career-related questions?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15n4no0/d_can_we_get_a_tag_or_a_weekly_mega_thread_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15n4no0/d_can_we_get_a_tag_or_a_weekly_mega_thread_for/"/>
        <updated>2023-08-10T06:49:03.000Z</updated>
        <summary type="html"><![CDATA[Seeing a flood of career related questions on here, most of which have been asked and answered ad nauseum before. Can we get a tag for them to filter out or compile them all in a weekly mega thread so it doesn’t clog up the main feed?
    submitted by    /u/pavelysnotekapret  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Applied ML for CV or SLAM?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15n3mna/d_applied_ml_for_cv_or_slam/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15n3mna/d_applied_ml_for_cv_or_slam/"/>
        <updated>2023-08-10T05:53:28.000Z</updated>
        <summary type="html"><![CDATA[Hey people, this time I am asking for your opinion. I am in automotive. I know ML very well due to personal projects but so far in my 5 years I never had the opportunity of applying it in the industry so I built expertise in classic CV Modelling instead. 
 Now there is an opportunity to work on newer topics like data generation, dataset curation or even ML based Fusion. Nevertheless I also got the opportunity to do SLAM and it excites me a lot because of the many things involved in it and the possibility to use parallelization and so on. What could be a nice strategy here in your opinion and why? I thank you all!
    submitted by    /u/tricostume  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 8/9/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15n243n/oneminute_daily_ai_news_892023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15n243n/oneminute_daily_ai_news_892023/"/>
        <updated>2023-08-10T04:35:10.000Z</updated>
        <summary type="html"><![CDATA[Google today announced the launch of Project IDX, its foray into offering an AI-enabled browser-based development environment for building full-stack web and multiplatform apps.[1]
 NVIDIA today announced NVIDIA AI Workbench, a unified, easy-to-use toolkit that allows developers to quickly create, test and customize pretrained generative AI models on a PC or workstation.[2]
 IBM said on Wednesday it would host Meta Platforms’ artificial intelligence language program on its own enterprise AI platform, watsonx.[3]
 New high-tech microscope using AI successfully detects malaria in returning travelers.[4]
  
Sources:
 [1] https://techcrunch.com/2023/08/08/google-launches-project-idx-a-new-ai-enabled-browser-based-development-environment/
 [2] https://nvidianews.nvidia.com/news/nvidia-ai-workbench-speeds-adoption-of-custom-generative-ai-for-worlds-enterprises
 [3] https://www.reuters.com/technology/ibm-launch-metas-llama-2-watsonx-ai-platform-businesses-2023-08-09/
 [4] https://medicalxpress.com/news/2023-08-high-tech-microscope-ai-successfully-malaria.html 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] training a model for function calls]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15n1j52/d_training_a_model_for_function_calls/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15n1j52/d_training_a_model_for_function_calls/"/>
        <updated>2023-08-10T04:05:24.000Z</updated>
        <summary type="html"><![CDATA[would it be possible to train or fine-tune a small (1-3B) model who's sole purpose is to perform function calls? similar to how we have tiny models like replit-v2-3B that are super capable at specific things like code auto-complete . 
 i know that's how openAI implemented function call was by fine-tuning gpt-3.5/4 but I'm thinking just a straight up base model trained to understand and excel at function calls (similar to Gorilla for apis)
 i'm thinking it would be a perfect "glue" for bigger LLM apps-- avoiding the need for external tools like langchain/quidance/etc...
    submitted by    /u/LyPreto  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Just as an experiment I tried to see if I could have a conversation with an AI image generator. Don’t knock it till you’ve tried it 😂]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15n14hz/just_as_an_experiment_i_tried_to_see_if_i_could/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15n14hz/just_as_an_experiment_i_tried_to_see_if_i_could/"/>
        <updated>2023-08-10T03:45:05.000Z</updated>
        <summary type="html"><![CDATA[I first tried this experiment back in January and it kinda tripped me out. I used the wonder AI. When I tried the experiment with the Wombo dream AI the results were completely random. I wonder what the results would be with Midjourney. I later revisited the experiment in June with the wonder AI and again got intriguing results. Posting this just as an experiment in the hopes others will try it and see if it is repeatable and if other AI have more consistent results than others. It’s just an experiment, I don’t really care about your opinion I care about your results from trying this. 
    submitted by    /u/Sonic_Improv  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Area and volume of hypersphere cap]]></title>
        <id>https://www.johndcook.com/blog/?p=202532</id>
        <link href="https://www.johndcook.com/blog/2023/08/09/hypersphere-cap/"/>
        <updated>2023-08-10T01:44:58.000Z</updated>
        <summary type="html"><![CDATA[A spherical cap is the portion of a sphere above some horizontal plane. For example, the polar ice cap of the earth is the region above some latitude. I mentioned in this post that the area above a latitude φ is where R is the earth’s radius. Latitude is the angle up from the equator. […]
Area and volume of hypersphere cap first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random points in a high-dimensional orthant]]></title>
        <id>https://www.johndcook.com/blog/?p=202525</id>
        <link href="https://www.johndcook.com/blog/2023/08/09/random-points-hypersphere-orthant/"/>
        <updated>2023-08-10T00:31:56.000Z</updated>
        <summary type="html"><![CDATA[In high dimensions, randomly chosen vectors are very likely nearly orthogonal. I’ll unpack this a little bit then demonstrate it by simulation. Then I’ll look at what happens when we restrict our attention to points with positive coordinates. *** The lengths of vectors don’t contribute to the angles between them, so we may as well […]
Random points in a high-dimensional orthant first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimation on the singularity date has just been delayed]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15mvzwh/estimation_on_the_singularity_date_has_just_been/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15mvzwh/estimation_on_the_singularity_date_has_just_been/"/>
        <updated>2023-08-09T23:57:45.000Z</updated>
        <summary type="html"><![CDATA[The continuous neutering of models (the process of making the models less capable or reducing certain aspects of their functionality to prevent them from generating inappropriate, harmful, or sensitive content), can now be regarded as a substantial contributor to the Singularity date's delay: www.daystosingularity.com/estimation-details/ 
    submitted by    /u/Powerful-Pumpkin-938  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalization with VW]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15mueec/personalization_with_vw/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15mueec/personalization_with_vw/"/>
        <updated>2023-08-09T22:53:51.000Z</updated>
        <summary type="html"><![CDATA[Hello! I am working off the VowpalWabbit example for explore_adf, just changing the cost function and actions but I get no learning. What I mean is that I train a model but when I ran the prediction, I just get an array of equivalent probabilities (0.25, 0.25, 0.25, 0.25). I have tried changing everything (making only one action to payoff for example) and still get the same error. Anyone has ran into a similar situation? Help please!
    submitted by    /u/juanccs  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inquiry Regarding Dynamic Action Space, DQN, and Alternative Algorithms in Reinforcement Learning]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15mtvzg/inquiry_regarding_dynamic_action_space_dqn_and/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15mtvzg/inquiry_regarding_dynamic_action_space_dqn_and/"/>
        <updated>2023-08-09T22:33:57.000Z</updated>
        <summary type="html"><![CDATA[I am currently addressing a challenge within the domain of Reinforcement Learning. The particular issue revolves around a dynamic action space, where the set of potential actions available changes based on the context or state. In light of this, I am seeking guidance on the feasibility of utilizing the Deep Q-Network (DQN) approach to specifically identify permissible actions for distinct states.
 Furthermore, if the DQN approach is not applicable in this scenario, I would appreciate recommendations for alternative algorithms that could effectively address this issue. Additionally, I am considering the option of designing a single action space and employing negative reward to discourage the agent from pursuing unauthorized actions within specific states.
    submitted by    /u/uonliaquat  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeurIPS rebuttal character limit problem [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15mtj1y/neurips_rebuttal_character_limit_problem_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15mtj1y/neurips_rebuttal_character_limit_problem_d/"/>
        <updated>2023-08-09T22:20:27.000Z</updated>
        <summary type="html"><![CDATA[The NeurIPS rebuttal has a 6000 character limit, however my rebuttal is way way over that. I was told by my supervisor that you could just comment chain onto the rebuttal to get past this, however that is not working.
 The deadline is in around 5 hours so I'm really in a big bind here. Does anyone have any insight about how to resolve this situation?
    submitted by    /u/Pyramid_Jumper  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Effects Multiplier: Wylie Co. Goes All in on GPU Rendering for 24x Returns]]></title>
        <id>https://blogs.nvidia.com/?p=66101</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/09/wylie-co-gpu-rendering/"/>
        <updated>2023-08-09T22:08:30.000Z</updated>
        <summary type="html"><![CDATA[Visual effects studios have long relied on render farms — vast numbers of servers — for computationally intensive, complex special effects, but that landscape is rapidly changing. High silicon and energy costs at these server facilities, which can be restricted in performance gains by Moore’s law, cut into studio profits and increase production time. To Read article >]]></summary>
        <author>
            <name>Rick Champagne</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Transformer Models Enable Machine Vision Object Detection]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15mt2wf/ai_transformer_models_enable_machine_vision/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15mt2wf/ai_transformer_models_enable_machine_vision/"/>
        <updated>2023-08-09T22:03:22.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Chipdoc  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Get into ML role [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ms0cq/get_into_ml_role_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ms0cq/get_into_ml_role_d/"/>
        <updated>2023-08-09T21:23:35.000Z</updated>
        <summary type="html"><![CDATA[Hey guys, I am working as an SDET amd previously work as a developer. I am currently enrolled in Machine Learning masters online with job with Learning each course I am not sure how to get into the job market for this role. What projects should I create, kaggle seems to be just copy paste from each other.Any suggestions?
    submitted by    /u/Latter_Ad_5679  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Searching for a tool]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15mrcti/searching_for_a_tool/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15mrcti/searching_for_a_tool/"/>
        <updated>2023-08-09T20:59:43.000Z</updated>
        <summary type="html"><![CDATA[Anyone know of a good AI tool that I can self feed my own music and have it generate similar tracks based on my style? Having a hard time finding something like this. Really just want to play around, super curious, tia
    submitted by    /u/yakisobas_ghost  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is a good idea to do leetcode for Computer and Data scientist? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15mr16l/is_a_good_idea_to_do_leetcode_for_computer_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15mr16l/is_a_good_idea_to_do_leetcode_for_computer_and/"/>
        <updated>2023-08-09T20:47:40.000Z</updated>
        <summary type="html"><![CDATA[It is something that sounds too much lately but I'm not sure about if it worths for those areas.
    submitted by    /u/Otherwise-Bike4761  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Ideal embedding models for classifying news articles to topics, specified as sentences]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15mqe4r/d_ideal_embedding_models_for_classifying_news/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15mqe4r/d_ideal_embedding_models_for_classifying_news/"/>
        <updated>2023-08-09T20:23:12.000Z</updated>
        <summary type="html"><![CDATA[I’m looking to build functionality that would allow a user to specify topics to be notified about in the news, eg. “Tax law changes in New York”, and notify them of recently published news articles related to that topic.
 Would the ideal strategy be to find relating articles to topics, or topics relating to articles as they come in? What models would be ideal here? I’m fairly new to this, so any help would be appreciated.
    submitted by    /u/ByteBuff  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple synthetic data reduces sycophancy in LLMs [R]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15mph12/simple_synthetic_data_reduces_sycophancy_in_llms_r/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15mph12/simple_synthetic_data_reduces_sycophancy_in_llms_r/"/>
        <updated>2023-08-09T19:49:03.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/we_are_mammals  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to tell if your model is actually learning?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15mp5l7/how_to_tell_if_your_model_is_actually_learning/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15mp5l7/how_to_tell_if_your_model_is_actually_learning/"/>
        <updated>2023-08-09T19:37:07.000Z</updated>
        <summary type="html"><![CDATA[I've been building a multi-agent model of chess, where each side of the board is represented by a Deep Q Agent. I had it play 100k training games, but the loss scores increased over time, not decreased. I've got the (relatively short) implementation and the last few output graphs from the training--is there a problem with my model architecture or does it just need more training games, perhaps against a better opponent than itself? Here's the notebook file. Thanks in advance
    submitted by    /u/lcmaier  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The AI rules that Congress is considering, explained]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15molg5/the_ai_rules_that_congress_is_considering/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15molg5/the_ai_rules_that_congress_is_considering/"/>
        <updated>2023-08-09T19:16:01.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/AriadneSkovgaarde  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reward shaping in FrozenLake]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15mojqr/reward_shaping_in_frozenlake/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15mojqr/reward_shaping_in_frozenlake/"/>
        <updated>2023-08-09T19:14:14.000Z</updated>
        <summary type="html"><![CDATA[I'm trying to use a neurosymbolic approach to solve the Frozenlake enviroment, using also stable baselines 3.
 I used the TransformReward on the enviroment, and seems that it's working (changing the reward values).
 So here it is how it works the program:
 It calculates a reward per step based on the distance of the next state to the goal state. Also I tried adding some more constraints, like punishing if it stays on the same square or if it falls into a hole.
 The thing is that I don't know if I'm doing something wrong, so if someone can help me would be much appreciated. Here is part of the code, I'll omit the neuro symbolic part because it's irrelevant.
 The rewards are:
 ​
  
Taking a step in a direction that makes you near the goal: less than one (it depends on how near of the objecti…]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Report: Disney Creates AI Task Force]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15mogo4/report_disney_creates_ai_task_force/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15mogo4/report_disney_creates_ai_task_force/"/>
        <updated>2023-08-09T19:10:57.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Jane-in-the-jungle  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Does it make sense to switch to premoderation?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15moc4k/d_does_it_make_sense_to_switch_to_premoderation/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15moc4k/d_does_it_make_sense_to_switch_to_premoderation/"/>
        <updated>2023-08-09T19:05:58.000Z</updated>
        <summary type="html"><![CDATA[Moderators are doing a great job, but often by the time a post is deleted it already hit too many eyeballs. Now that everyone and their mom are into AI, does it make sense to switch to premoderation for new members and members who do not follow the rules of the subreddit?
    submitted by    /u/lostmsu  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Opening the Black Box]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15mo6zf/opening_the_black_box/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15mo6zf/opening_the_black_box/"/>
        <updated>2023-08-09T19:00:40.000Z</updated>
        <summary type="html"><![CDATA[From Anthropic
 https://arxiv.org/abs/2308.03296
 Studying Large Language Model Generalization with Influence Functions
 When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.
    submitted by    /u/DataPhreak  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advances in document understanding]]></title>
        <id>http://ai.googleblog.com/2023/08/advances-in-document-understanding.html</id>
        <link href="http://ai.googleblog.com/2023/08/advances-in-document-understanding.html"/>
        <updated>2023-08-09T18:32:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Sandeep Tata, Software Engineer, Google Research, Athena Team




The last few years have seen rapid progress in systems that can automatically process complex business documents and turn them into structured objects. A system that can automatically extract data from documents, e.g., receipts, insurance quotes, and financial statements, has the potential to dramatically improve the efficiency of business workflows by avoiding error-prone, manual work. Recent models, based on the Transformer architecture, have shown impressive gains in accuracy. Larger models, such as PaLM 2, are also being leveraged to further streamline these business workflows. However, the datasets used in academic literature fail to capture the challenges seen in real-world use cases. Consequently, academic b…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Seeking Insights and Collaboration on Deep Engine AI's Hive Concept for Universal Adaptive Intelligence]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15mnbxc/d_seeking_insights_and_collaboration_on_deep/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15mnbxc/d_seeking_insights_and_collaboration_on_deep/"/>
        <updated>2023-08-09T18:28:28.000Z</updated>
        <summary type="html"><![CDATA[This project is the culmination of genuine effort and innovation by a team of dedicated professionals. We welcome constructive feedback and value your insights to help us improve and grow. Thank you for engaging with us respectfully.
 Hello AI and blockchain enthusiasts!
 I'm part of the team at Deep Engine AI, where we are working on an exciting project that involves building a Universal Adaptive Intelligence System (UAIS). One of our key concepts is what we're calling "Pervasive Swarm Learning." It's a blend of holistic swarm intelligence models, community-managed ecosystems, and innovative algorithms.
 We're reaching out to this knowledgeable community to get your thoughts, insights, or any innovative ideas that could help us refine and build out this concept. Whether you're an AI researcher, data scientist, blockchain expert, or simply someone interested in the field, your input could be invaluable to us.
 Here's a quick overview of what we're focusing on:
  
Holistic Swarm Intelligence Models: Incorporating stochastic optimization, neuromorphic computing, and quantum-inspired algorithms for adaptability and resilience.
 Global Community-Managed Ecosystem: Enhancing our DAO with transparent and real-time community feedback loops.
  
We believe in the power of collaboration and the collective intelligence of this community. If you have any insights, questions, or want to know more about what we're working on, please comment below or feel free to send me a private message
 If you want to dive deeper into our project, here's a link to our website.
 Thank you for taking the time to read this post. We look forward to hearing your thoughts and potentially collaborating with some of you!
    submitted by    /u/deepengineai  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What are the ML engineer hours per week worked?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15mm06v/d_what_are_the_ml_engineer_hours_per_week_worked/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15mm06v/d_what_are_the_ml_engineer_hours_per_week_worked/"/>
        <updated>2023-08-09T17:39:17.000Z</updated>
        <summary type="html"><![CDATA[Two ways to answer this question:
  
What is the average amount of hours?
 What is the amount of hours in a specific position that you are familiar with?
  
I'm also wondering about non-academic ML PhDs who now work in industry.
    submitted by    /u/Practical_Tea_3779  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inside the Very Human Origin of the Term “Artificial Intelligence” — And Its Seven Decade Boom/Bust Cycle]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15mlyey/inside_the_very_human_origin_of_the_term/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15mlyey/inside_the_very_human_origin_of_the_term/"/>
        <updated>2023-08-09T17:37:30.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/geekteam6  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence for the Poor: How to Harness the Power of AI in the Developing World]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15mlf3g/artificial_intelligence_for_the_poor_how_to/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15mlf3g/artificial_intelligence_for_the_poor_how_to/"/>
        <updated>2023-08-09T17:17:57.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/polandballbounces  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Project] Making AMD GPUs competitive for LLM inference]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ml8n0/project_making_amd_gpus_competitive_for_llm/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ml8n0/project_making_amd_gpus_competitive_for_llm/"/>
        <updated>2023-08-09T17:11:17.000Z</updated>
        <summary type="html"><![CDATA[There have been many LLM inference solutions since the bloom of open-source LLMs. Most of the performant inference solutions are based on CUDA and optimized for NVIDIA GPUs. In the meantime, with the high demand for compute availability, it is useful to bring support to a broader class of hardware accelerators. AMD is one potential candidate.
 We build a project that makes it possible to compile LLMs and deploy them on AMD GPUs using ROCm and get competitive performance. More specifically, AMD Radeon™ RX 7900 XTX gives 80% of the speed of NVIDIA® GeForce RTX™ 4090 and 94% of the speed of NVIDIA® GeForce RTX™ 3090Ti for single batch Llama2-7B/13B 4bit inference. Besides ROCm, our Vulkan support allows us to generalize LLM deployment to other AMD devices, for example, a SteamDeck with an AMD APU.
 - Github: https://github.com/mlc-ai/mlc-llm/
 - Blogpost describing the techniques: https://blog.mlc.ai/2023/08/09/Making-AMD-GPUs-competitive-for-LLM-inference
 ​
 ​
    submitted by    /u/crowwork  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Llama from scratch (or how to implement a paper without crying)]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15ml50h/llama_from_scratch_or_how_to_implement_a_paper/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15ml50h/llama_from_scratch_or_how_to_implement_a_paper/"/>
        <updated>2023-08-09T17:07:27.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are there any examples of Artificial Intelligence that aren't Machine Learning?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15ml1yq/are_there_any_examples_of_artificial_intelligence/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15ml1yq/are_there_any_examples_of_artificial_intelligence/"/>
        <updated>2023-08-09T17:04:20.000Z</updated>
        <summary type="html"><![CDATA[I hear AI & ML used interchangeable, and a lot of people dispute the use of the term "AI", as defining "intelligence" can be a sticky wicket. "Machine learning" seems like a much clearer term, describing systems that can optimize themselves given an objective function & maybe training data (generalization). 
 But, I know ML is just a subset of AI, so is there any extant AI that isn't ML? If not, what would AI that's not ML look like? 
    submitted by    /u/ZealousidealTomato74  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative AI Revs Up New Age in Auto Industry, From Design and Engineering to Production and Sales]]></title>
        <id>https://blogs.nvidia.com/?p=66054</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/09/generative-ai-auto-industry/"/>
        <updated>2023-08-09T17:01:37.000Z</updated>
        <summary type="html"><![CDATA[Generating content and code. Creating images and videos. Testing algorithms with synthetic data. Generative AI is a force multiplier enabling leaps in productivity and creativity for nearly every industry, particularly transportation, where it’s streamlining workflows and driving new business. Across the entire auto industry, companies are exploring generative AI to improve vehicle design, engineering, and Read article >]]></summary>
        <author>
            <name>Danny Shapiro</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CNN Features Extractor for Categorical Data]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15mjlw7/cnn_features_extractor_for_categorical_data/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15mjlw7/cnn_features_extractor_for_categorical_data/"/>
        <updated>2023-08-09T16:09:54.000Z</updated>
        <summary type="html"><![CDATA[I'm working on a RL Environment for my Masther Thesis where i try to explore the use of RL for Architectural Design. The environment looks like a 3D Grid of cubic 3D tiles or modules, and the agent can place tiles by choosing : Location (As x,y,z coordinates) , rotation (0 to 4 as multiples of 90 degrees around a z axis in the center) , and Tile Type (Im experimenting with many sets of tiles with different sizes). Then, I use Grasshopper3D to analyze the radiation that the interior surfaces and other metrics, that i use for my reward calculation. 
 For this, the state of the environment is defined as a 3D Array with 2 channels. One for the Tile Types and one for the rotations. This is processed by a 3D CNN features extractor. 
 The thing is that, I just realized that the array that represents the tile types as integers is actually categorical data, and I don't know how well this could work in a CNN. What i mean by this is that a tile=3 is not any more "anything" that a tile=1, it is just different. 
 Am I doing something stupid then? Should i change it?
 I apologize if im saying something really dumb. I just got into RL few months ago : - ) 
    submitted by    /u/Direct-Software7378  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]: Doing a PhD in Embedded Systems + Machine Learning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15mjeu2/d_doing_a_phd_in_embedded_systems_machine_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15mjeu2/d_doing_a_phd_in_embedded_systems_machine_learning/"/>
        <updated>2023-08-09T16:02:44.000Z</updated>
        <summary type="html"><![CDATA[I am currently thinking about doing a PhD after I am done with my Master's thesis because the topic of my thesis is so fascinating. However, I put the possibility of doing a PhD aside because I was always more "hands-on" rather than academic / research focused.
 Would you say a PhD in the intersection of embedded systems + machine learning (maybe training a model "on the edge" with the sensor data of the embedded device) is beneficial regarding finding a job afterwards?
    submitted by    /u/LM1117  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generate creative advertising using generative AI deployed on Amazon SageMaker]]></title>
        <id>c31ba9f6161236cada276eab650a8e20bd5e322b</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/generate-creative-advertising-using-generative-ai-deployed-on-amazon-sagemaker/"/>
        <updated>2023-08-09T15:51:52.000Z</updated>
        <summary type="html"><![CDATA[Creative advertising has the potential to be revolutionized by generative AI (GenAI). You can now create a wide variation of novel images, such as product shots, by retraining a GenAI model and providing a few inputs into the model, such as textual prompts (sentences describing the scene and objects to be produced by the model). […]]]></summary>
        <author>
            <name>Fabian Benitez-Quiroz</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How can I determine if an LLM's response is empathetic?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15mizh5/d_how_can_i_determine_if_an_llms_response_is/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15mizh5/d_how_can_i_determine_if_an_llms_response_is/"/>
        <updated>2023-08-09T15:47:20.000Z</updated>
        <summary type="html"><![CDATA[I've become interested in how LLMs express emotions through their responses. Here's an example:
 [Q] = I am having a bad day
 [R] = I'm sorry to hear that you're having a bad day. Is there anything specific you'd like to talk about or any way I can help you feel better? Whether it's just a listening ear... 
 I am aware that LLMs are not conscious and have no real understanding of emotions. But it is clear from the example that they can produce emotionally appropriate responses. Is there some kind of systematic test that can be automated to verify this? I.e. given a text-based query and response determine if the response is empathetic. 
    submitted by    /u/boringdude123  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative AI megatrends: implications of GPT-4 drift and open source models – part two]]></title>
        <id>https://www.datasciencecentral.com/?p=62815</id>
        <link href="https://www.datasciencecentral.com/generative-ai-megatrends-implications-of-gpt-4-drift-and-open-source-models-part-two/"/>
        <updated>2023-08-09T15:17:47.000Z</updated>
        <summary type="html"><![CDATA[Background In the previous part of this blog, we explored the limitations of GPT-4. In this post, we will explore if open source models can overcome the limitations of black box models. Specifically, we will consider the use of LLama2 in this scenario.  The llama 2 paper from Meta is very comprehensive.  Llama 2, is… Read More »Generative AI megatrends: implications of GPT-4 drift and open source models – part two
The post Generative AI megatrends: implications of GPT-4 drift and open source models – part two appeared first on Data Science Central.]]></summary>
        <author>
            <name>ajitjaokar</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cosine similarity does not satisfy the triangle inequality]]></title>
        <id>https://www.johndcook.com/blog/?p=202444</id>
        <link href="https://www.johndcook.com/blog/2023/08/09/cosine-similarity-not-a-metric/"/>
        <updated>2023-08-09T15:15:38.000Z</updated>
        <summary type="html"><![CDATA[The previous post looked at cosine similarity for embeddings of words in vector spaces. Word embeddings like word2vec map words into high-dimensional vector spaces in such a way that related words correspond to vectors that are roughly parallel. Ideally the more similar the words, the smaller the angle between their corresponding vectors. The cosine similarity […]
Cosine similarity does not satisfy the triangle inequality first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI is about to turn the internet into a total nightmare]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15mhncu/ai_is_about_to_turn_the_internet_into_a_total/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15mhncu/ai_is_about_to_turn_the_internet_into_a_total/"/>
        <updated>2023-08-09T14:56:19.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Alone-Competition-77  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Strong AI = Brainy Superheroes?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15mg5xj/strong_ai_brainy_superheroes/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15mg5xj/strong_ai_brainy_superheroes/"/>
        <updated>2023-08-09T13:59:57.000Z</updated>
        <summary type="html"><![CDATA[These brainy superheroes of the AI realm are ready to conquer intellectual challenges with a snap of their digital fingers, leaving us mere mortals feeling like puny amoebas in comparison.
 More ere: https://daystosingularity.com/2023/06/21/brainy-superheroes/
    submitted by    /u/Powerful-Pumpkin-938  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] using lidar and photography to locate fire hydrants]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15mffz9/p_using_lidar_and_photography_to_locate_fire/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15mffz9/p_using_lidar_and_photography_to_locate_fire/"/>
        <updated>2023-08-09T13:31:54.000Z</updated>
        <summary type="html"><![CDATA[[P]I have imagery and lidar from trucks (cyclomedia). I would like to extract the latitude and longitude of fire hydrants from the images and point clouds.. I posted in r/computervision and they said it would be inaccurate to use imagery to locate fire hydrants.
 Do you have any tips on getting location from photographic images?
 Are there any open source neural nets for street view point clouds that can help?
 Is this an either/or problem or is it possible to use both data sources combined?
    submitted by    /u/Zealousideal_Rub5826  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Besides for Bing Ai, is there no other decent Ai that can give me links, and search (sniff) the web?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15mf522/besides_for_bing_ai_is_there_no_other_decent_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15mf522/besides_for_bing_ai_is_there_no_other_decent_ai/"/>
        <updated>2023-08-09T13:20:04.000Z</updated>
        <summary type="html"><![CDATA[I’ve been kind of getting ChatGPT and Google Bard to generate subreddits, and look for me Cannabis sites, and shopping. (to be specific haha, well forums) It’s done a good job with some of the links, but a lot of times it kind of makes them up. Is there not an Ai that can deep dive or skim the web more accurately? Does it decipher filters like “time and date”, “availability”, “price”. More problems I’ve run into is it not being able to go really far back such as early internet or none indexed sites. Also noticed with Google and Bing they will give you the same results over and over (I assume I should have used “no repeats”) Google also will show me sold out items or items that aren’t actually on sale. It’ll show the item as say “on sale: $12.00” inspected the link- “$137” actually?? Any filter tips, or other Ai??
    submitted by    /u/Maelasae  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TELL ME AN AI THAT CAN EDIT AN IMAGES TEXT]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15mem6f/tell_me_an_ai_that_can_edit_an_images_text/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15mem6f/tell_me_an_ai_that_can_edit_an_images_text/"/>
        <updated>2023-08-09T12:58:42.000Z</updated>
        <summary type="html"><![CDATA[I saw a reel or short about an site that can do that easily but I didn't really care about at that time so I didn't save it I regret my decision soooo much can someone help me I have already wasted so much of my time wandering here n there
    submitted by    /u/Inevitable-Mousse489  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I read the papers for you: Comparing Bark and Tortoise TTS for text-to-speech applications]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15mdqoj/i_read_the_papers_for_you_comparing_bark_and/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15mdqoj/i_read_the_papers_for_you_comparing_bark_and/"/>
        <updated>2023-08-09T12:20:43.000Z</updated>
        <summary type="html"><![CDATA[If you're creating voice-enabled products, I hope this will help you choose which model to use!
 I read the papers and docs for Bark and Tortoise TTS - two text-to-speech models that seemed pretty similar on the surface but are actually pretty different.
 Here's what Bark can do:
  
It can synthesize natural, human-like speech in multiple languages.
 Bark can also generate music, sound effects, and other audio.
 The model supports generating laughs, sighs, and other non-verbal sounds to make speech more natural and human-sounding. I find these really compelling and these imperfections make the speech sound much more real. Check out an example here (scroll down to "pizza.webm").
 Bark allows control over tone, pitch, speaker identity and other attributes through text prompts. 
 The model learns directly from text-audio pairs.
  
Whereas for Tortoise TTS:
  
It excels at cloning voices using just short audio samples of a target speaker. This makes it easy to produce text in many distinct voices (like celebrities). I think voice cloning is the best use case for this tool.
 The quality of the synthesized voices is pretty high.
 Tortoise supports fine-grained control of speech characteristics like tone, emotion, pacing, etc through priming text.
 Tortoise is only trained on English and it's not capable of producing sound effects.
  
Here's how they compare to the other speech-related models I've taken a look at so far:
  
 Model Best Use Cases Key Strengths 
  
 Bark Voice assistants, audio generation Flexibility, multilingual 
  Tortoise TTS Audiobooks, voice cloning Natural prosody, voice cloning 
  AudioLDM (full guide) Voice assistants High-quality speech and SFX 
  Whisper Transcription Accuracy, flexibility 
  Free VC Voice conversion Retains speech style 
 
 I have a full write-up here if you want to read more, it's about a 10-minute read. I also looked at the model inputs and outputs and speculated on some products you can build with each tool.
    submitted by    /u/Successful-Western27  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Authors Join the Brewing Legal Battle Over AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15mdmq3/authors_join_the_brewing_legal_battle_over_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15mdmq3/authors_join_the_brewing_legal_battle_over_ai/"/>
        <updated>2023-08-09T12:15:46.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Hiversitize  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Which cornerstone papers should be read before RT-2?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15mdb96/d_which_cornerstone_papers_should_be_read_before/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15mdb96/d_which_cornerstone_papers_should_be_read_before/"/>
        <updated>2023-08-09T12:01:36.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone, I have an interesting task in hand, and wanted some advice over here before getting my hands dirty.
 I have to compile a list of 15-20 papers, to systematically go from "what's a transformer" to bleeding edge research of multimodal and LLMs applications to robotics, dynamical systems, and RL (e.g. RT-2). The base assumption is that the reader will be a master student, who already has the basics of what deep learning and control theory are.
 Could you suggest possible cornerstone papers that could go in this list? If you could, this would guide my search a lot, and I would appreciate it.
 ​
 ​
    submitted by    /u/Snekgineer  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latest AI News Digest - August 9]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15mcg8f/latest_ai_news_digest_august_9/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15mcg8f/latest_ai_news_digest_august_9/"/>
        <updated>2023-08-09T11:20:09.000Z</updated>
        <summary type="html"><![CDATA[Here are fresh AI Updates for you
 ​
 Nvidia has launched new Grace Hopper Superchip to boost Generative AI
 Norton introduces new AI Scam Detection Tool 'Genie'
 Google Working on 'Brain2Music' to create music from your brain
 Google and Universal Music deal over 'AI Deepfakes'
 Is Zoom using your data to train its AI ?
 ​
 Stay tuned for more 
 ​
    submitted by    /u/Agitated-Spell3979  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Damn! Now everybody can be a film producer]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15mbwey/damn_now_everybody_can_be_a_film_producer/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15mbwey/damn_now_everybody_can_be_a_film_producer/"/>
        <updated>2023-08-09T10:53:14.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/anonymous_guyy  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] AutoML tool H2O exposes ALL files on your server by default, multiple CVEs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15matan/r_automl_tool_h2o_exposes_all_files_on_your/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15matan/r_automl_tool_h2o_exposes_all_files_on_your/"/>
        <updated>2023-08-09T09:57:07.000Z</updated>
        <summary type="html"><![CDATA[https://mlsecops.com/resources/hacking-ai-h2o-exposes-entire-filesystem
    submitted by    /u/FlyingTriangle  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] What are your favorite AI tools?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15man7r/r_what_are_your_favorite_ai_tools/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15man7r/r_what_are_your_favorite_ai_tools/"/>
        <updated>2023-08-09T09:48:05.000Z</updated>
        <summary type="html"><![CDATA[What are some AI tools that you use often, that help you with your work/school or that you simply use for fun? 
    submitted by    /u/SadBlackTea  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Best ML open-source projects to contribute to]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15m9s45/d_best_ml_opensource_projects_to_contribute_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15m9s45/d_best_ml_opensource_projects_to_contribute_to/"/>
        <updated>2023-08-09T09:00:48.000Z</updated>
        <summary type="html"><![CDATA[Any recommendations for cool open-source ML projects that an intermediate Machine Learning engineer/researcher can contribute to?
    submitted by    /u/Ahmed-Allam-220  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What does it take to get AI to work like a scientist? | "As machine-learning algorithms grow more sophisticated, artificial intelligence seems poised to revolutionize the practice of science itself."]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15m8ud9/what_does_it_take_to_get_ai_to_work_like_a/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15m8ud9/what_does_it_take_to_get_ai_to_work_like_a/"/>
        <updated>2023-08-09T08:06:59.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Tao_Dragon  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Where to begin studying AI/ML from a COGNITIVE SCIENCE PERSPECTIVE?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15m55ng/d_where_to_begin_studying_aiml_from_a_cognitive/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15m55ng/d_where_to_begin_studying_aiml_from_a_cognitive/"/>
        <updated>2023-08-09T04:47:10.000Z</updated>
        <summary type="html"><![CDATA[I am currently an AI/ML student but I have recently been thinking more and more about cognitive science. I was wondering if you know of any good resources that approach AI from the perspective of cognitive science
    submitted by    /u/BornAgain20Fifteen  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Where to begin studying AI/ML from a COGNITIVE SCIENCE PERSPECTIVE?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15m55lj/where_to_begin_studying_aiml_from_a_cognitive/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15m55lj/where_to_begin_studying_aiml_from_a_cognitive/"/>
        <updated>2023-08-09T04:47:06.000Z</updated>
        <summary type="html"><![CDATA[I am currently an AI/ML student but I have recently been thinking more and more about cognitive science. I was wondering if you know of any good resources that approach AI from the perspective of cognitive science
    submitted by    /u/BornAgain20Fifteen  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Announcing StableCode — Stability AI]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15m51k8/announcing_stablecode_stability_ai/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15m51k8/announcing_stablecode_stability_ai/"/>
        <updated>2023-08-09T04:41:16.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 8/8/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15m3tzw/oneminute_daily_ai_news_882023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15m3tzw/oneminute_daily_ai_news_882023/"/>
        <updated>2023-08-09T03:41:44.000Z</updated>
        <summary type="html"><![CDATA[Researchers at the Massachusetts Institute of Technology (MIT) and the Dana-Farber Cancer Institute have discovered that the use of artificial intelligence (AI) could make it easier to determine the sites of origin for enigmatic cancers and enable doctors to choose more targeted treatments.[1]
 Meta disbands protein-folding team in shift towards commercial AI.[2]
 OpenAI has introduced GPTBot, a web crawler to improve AI models. GPTBot scrupulously filters out data sources that violate privacy and other policies.[3]
 Disney has created a task force to study artificial intelligence and how it can be applied across the entertainment conglomerate, even as Hollywood writers and actors battle to limit the industry’s exploitation of the technology.[4]
  
Sources:
 [1] https://www.nature.com/articles/s41591-023-02482-6
 [2] https://www.ft.com/content/919c05d2-b894-4812-aa1a-dd2ab6de794a
 [3] https://www.searchenginejournal.com/openai-launches-gptbot-how-to-restrict-access/493394/#close
 [4] https://www.reuters.com/technology/disney-creates-task-force-explore-ai-cut-costs-sources-2023-08-08/ 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How AI generated movies / TV series might be done in near future.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15m320e/how_ai_generated_movies_tv_series_might_be_done/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15m320e/how_ai_generated_movies_tv_series_might_be_done/"/>
        <updated>2023-08-09T03:05:06.000Z</updated>
        <summary type="html"><![CDATA[I see lots of people say AI will not be able to do movies / TV because it hallucinates yada yada yada. But, movies / TV shows follow a clear script. Script being a generic formula that is taught to script writers the same way as music theory cheat sheet helps aspiring musicians to write songs. 
 Script. Script can be turned into machine readable format. You can add commands how to render a movie on the basis of it. For example in a script you could have #Jack, telling the thing reading the script that we are talking of actor #Jack meaning it should tap into assets about jack which would reside in folder Jack. Jack meanwhile could be rendered by sub ai to fit the part. 
 That helps us nail down the character so it wont be changing appearance wise in our script. 
 The AI part here comes from…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning", Mathieu et al 2023 {DM} (MuZero)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15m1hj4/alphastar_unplugged_largescale_offline/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15m1hj4/alphastar_unplugged_largescale_offline/"/>
        <updated>2023-08-09T01:52:47.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[4 ways generative AI makes founders more interesting to journalists | TechCrunch]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15m17xo/4_ways_generative_ai_makes_founders_more/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15m17xo/4_ways_generative_ai_makes_founders_more/"/>
        <updated>2023-08-09T01:41:29.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/egusa  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QUESTION from a Lay person non-math/science type who likes to read about science and AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15m0h73/question_from_a_lay_person_nonmathscience_type/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15m0h73/question_from_a_lay_person_nonmathscience_type/"/>
        <updated>2023-08-09T01:08:20.000Z</updated>
        <summary type="html"><![CDATA[Thanks any answers or musings -
 what are some technical limitations (eg computing / storage power/speed) that (1) limits AI's progress and (2) might be solved (and how), and (3) if solved, would make possible developments we can conceive of but not do yet?
 I'm just wondering if AI researchers forsee a kind of 'leap forward' and what are some obstacles?
    submitted by    /u/OpenWaterRescue  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Angles between words]]></title>
        <id>https://www.johndcook.com/blog/?p=202355</id>
        <link href="https://www.johndcook.com/blog/2023/08/08/angles-between-words/"/>
        <updated>2023-08-09T00:56:00.000Z</updated>
        <summary type="html"><![CDATA[Natural language processing represents words as high-dimensional vectors, on the order of 100 dimensions. For example, the glove-wiki-gigaword-50 set of word vectors contains 50-dimensional vectors, and the the glove-wiki-gigaword-200 set of word vectors contains 200-dimensional vectors. The intent is to represent words in such a way that the angle between vectors is related to similarity […]
Angles between words first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Cloud computing and other GPU alternatives]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15lyv0k/r_cloud_computing_and_other_gpu_alternatives/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15lyv0k/r_cloud_computing_and_other_gpu_alternatives/"/>
        <updated>2023-08-08T23:59:25.000Z</updated>
        <summary type="html"><![CDATA[I’m kind of new to the world of machine/deep learning so cut me some slack here, but I was wondering the best ways to train models (in my case a transformer) without a GPU. I personally don’t even have a PC, I’ve been using a 2017 MacBook Air. I know deep learning models are quite computationally expensive and since I don’t have access to a GPU, how do I train models? I’ve read about cloud computing services like AWS, Google Colab, etc. but I was wondering what the best method was. Ideally free or as cheap as possible.
    submitted by    /u/Present_Network1959  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Beta Test Invitation: Free AI Email Chrome Extension]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15lxru4/d_beta_test_invitation_free_ai_email_chrome/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15lxru4/d_beta_test_invitation_free_ai_email_chrome/"/>
        <updated>2023-08-08T23:14:39.000Z</updated>
        <summary type="html"><![CDATA[We are currently conducting a beta test for our Chrome Extension and we value external input.
 Our platform allows you to write and receive your gmail emails within the browser. You can also use AI to generate emails, without ever touching gmail or chatgpt.
 If you're interested in participating, please feel free to message or comment!
    submitted by    /u/Live-Orange-8414  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Visual Transformers have anything equivalent to Pooling in CNN? [Discussion]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15lxbtz/do_visual_transformers_have_anything_equivalent/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15lxbtz/do_visual_transformers_have_anything_equivalent/"/>
        <updated>2023-08-08T22:57:07.000Z</updated>
        <summary type="html"><![CDATA[I have a regression model based on CNN, works reasonably well with less than 1M parameters. I am trying to check how Visual Transformer (ViT) will perform on this task, but due to lack of pooling in ViT, model size is considerably large (~10M parameters). Do ViT have anything equivalent to pooling to reduce number of parameters?
 If not then that reduces applicability of ViT to large models on large dataset dataset only. For smaller tasks with small dataset, CNN or Resnet are way more computation efficient.
 Or am I missing something?
    submitted by    /u/Apprehensive-War8915  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I made this film completely using AI! From Chat GPT to EbSynth!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15lx459/i_made_this_film_completely_using_ai_from_chat/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15lx459/i_made_this_film_completely_using_ai_from_chat/"/>
        <updated>2023-08-08T22:48:40.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/RMIII3  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How long does it take to setup an MLOps pipeline?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15lx0dr/d_how_long_does_it_take_to_setup_an_mlops_pipeline/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15lx0dr/d_how_long_does_it_take_to_setup_an_mlops_pipeline/"/>
        <updated>2023-08-08T22:44:31.000Z</updated>
        <summary type="html"><![CDATA[For our R&D team, we spent over a month trying to setup our pipeline. After that, we spend at least 5 days after R&D for to put a model into production without the required data pipelines that communicate with our model and the service. For training a model, the infrastructure maintain and manage it also needs to be built for around 2 weeks. 
 Currently, our best solution is to offload the training process by purchasing a GPU and keeping it in the office.
    submitted by    /u/potanees  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Weights Reset implicit regularization]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15lwrcf/r_weights_reset_implicit_regularization/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15lwrcf/r_weights_reset_implicit_regularization/"/>
        <updated>2023-08-08T22:34:16.000Z</updated>
        <summary type="html"><![CDATA[​
 https://preview.redd.it/4t4jbi15rygb1.png?width=2291&format=png&auto=webp&s=f4eedf0d24dee2cbd040b3a19ab9610119b4001e
 Hi everyone!
 I want to share some interesting observations that indicate a very simple periodical weights resetting procedure could serve as an implicit regularization strategy for training DL models. This technique also shows potential connection with the Double Descent phenomenon. Here's the link to github etc: https://github.com/amcircle/weights-reset.
 As a co-author of this study, I must apologize in advance for its brevity. However, I sincerely hope it may prove useful to some. I would gladly respond to your queries and receive your criticism. Your personal experiences related to something similar would also be highly appreciated.
    submitted by    /u/gregorivy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[This video argues that artificial intelligence should not be regulated.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15lwpb8/this_video_argues_that_artificial_intelligence/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15lwpb8/this_video_argues_that_artificial_intelligence/"/>
        <updated>2023-08-08T22:31:58.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/antaloaalonso  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Training process - Are text encodings used along with image encodings]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15luwq4/d_training_process_are_text_encodings_used_along/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15luwq4/d_training_process_are_text_encodings_used_along/"/>
        <updated>2023-08-08T21:24:29.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I am going through research papers and noticed that most of the papers talk about the text conditioned image generation process (reverse diffusion process). The text and time encodings are added as additional channels to the UNet block.
 However, I am curious to know if any text encodings are used during the training process as well. Is there any preview of the training datasets that is available which is used in the training process ? or a code snippet that points out to the forward part of the training loop
 Thanks
    submitted by    /u/kaskoraja  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Catching up on the weird world of LLMs]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15luv1g/catching_up_on_the_weird_world_of_llms/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15luv1g/catching_up_on_the_weird_world_of_llms/"/>
        <updated>2023-08-08T21:22:37.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nangaparbat  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdaTape: Foundation model with adaptive computation and dynamic read-and-write]]></title>
        <id>http://ai.googleblog.com/2023/08/adatape-foundation-model-with-adaptive.html</id>
        <link href="http://ai.googleblog.com/2023/08/adatape-foundation-model-with-adaptive.html"/>
        <updated>2023-08-08T21:02:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Fuzhao Xue, Research Intern, and Mostafa Dehghani, Research Scientist, Google







Adaptive computation refers to the ability of a machine learning system to adjust its behavior in response to changes in the environment. While conventional neural networks have a fixed function and computation capacity, i.e., they spend the same number of FLOPs for processing different inputs, a model with adaptive and dynamic computation modulates the computational budget it dedicates to processing each input, depending on the complexity of the input.


Adaptive computation in neural networks is appealing for two key reasons. First, the mechanism that introduces adaptivity provides an inductive bias that can play a key role in solving some challenging tasks. For instance, enabling different num…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Benchmark for autoregressive LLM embedding quality for retrieval?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15lu61r/d_benchmark_for_autoregressive_llm_embedding/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15lu61r/d_benchmark_for_autoregressive_llm_embedding/"/>
        <updated>2023-08-08T20:56:43.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 There has been a lot of work on benchmarking autoregressive LLMs, such as HF LLM Leaderboard, but I have not seen much work specifically on the relevancy of such LLMs for retrieval. 
 There is a lot of talk about chat based on knowledge with solutions like llama_index, where LLMs both provide embeddings and answer based on most similar content, but embedding and answer generation need not be the same LLM.
 I saw the Massive Text Embedding Benchmark (MTEB) but it does not seem to contain a lot of information about the recent autoregressive LLMs. 
 Are the recent autoregressive LLMs, e.g. Llama 2, actually performing better than Bidirectional LLMs such as BERT? 
 Because if so, all the recent fancy chat with your documents projects could use much smaller models to do embedding extraction for retrieval and just call a fancy autoregressive LLM such as GPT4 for answer synthesis.
    submitted by    /u/Separate-Still3770  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] [R] Opensource model that can caption an image of a chart?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ltqrs/d_r_opensource_model_that_can_caption_an_image_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ltqrs/d_r_opensource_model_that_can_caption_an_image_of/"/>
        <updated>2023-08-08T20:41:35.000Z</updated>
        <summary type="html"><![CDATA[Hi I'm looking for an open source model that can take an image of an info graphic such as a pie chart, graph, etc, and provide a description of the information in that chart. For example the values of the x,y axis and their labels, weather the chart is increasing or decreasing. 
 I've worked with image captioning models such as BLIP before as I have used them in projects involving stable diffusion, but this model doesn't give specifics about the information in the graph, just a brief overview. 
 I know researches have worked on this problem in the past using the vistext dataset: 
 https://news.mit.edu/2023/researchers-chart-captions-ai-vistext-0630
 So far I'm thinking that it may come to me finetuning BLIP or equivalent to specialize on infographics instead. 
 Thoughts? 
    submitted by    /u/UncleSammmm  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Any existing photo/video classifier UIs with custom labels?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ltjak/p_any_existing_photovideo_classifier_uis_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ltjak/p_any_existing_photovideo_classifier_uis_with/"/>
        <updated>2023-08-08T20:33:56.000Z</updated>
        <summary type="html"><![CDATA[I have a significant amount of files that I would like to label for future reference. I've looked at software such as Photoprism or Librephoto which have object classification but they are based on a static model. I'd like something where I could label a few photos then generate similar matches where I can approve the good matches for reinforcement learning. I'm pretty sure I saw a demo like this at a code conference using Azure but I'm hoping for something self-hosted to avoid API fees. I was exploring coding something to do this for me but I don't want to put in the work if something with a UI exists already. This seemed like the best place to ask.
    submitted by    /u/MZZXX  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Best way to run a pytorch model on a cropped version of a video on someone else's PC?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15lsyjz/d_best_way_to_run_a_pytorch_model_on_a_cropped/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15lsyjz/d_best_way_to_run_a_pytorch_model_on_a_cropped/"/>
        <updated>2023-08-08T20:12:29.000Z</updated>
        <summary type="html"><![CDATA[Hi - I have trained a pytorch model that does some fairly simple object classification - The goal is distribute it as part of an app, that will pull information from a user's video.
 The videos are typically ~25-30 minutes and about 1GB in size, only a 600x600px square on the bottom right of the video is needed for the classification (it's a minimap in a video game)
 The app is electron based
 Ideally I want to input a video, and extract the labels from the cropped section once per second.
 ​
 My current attempt involves converting the model to a tensorflowjs model, and rendering the video on a <canvas> element, stretching it so only the minimap is visible on the canvas, running the model, saving the labels, and increasing the current time of the video by 1 second, and repeating until the video is done.
 ​
 This seems like a terrible plan, but it's much better than the couple of other ideas I've tried (using ffmpeg to extract a frame every second for example)
 ​
 Any advice appreciated!
 ​
 Edit: just to clarify this will only ever be ran on Windows
    submitted by    /u/FreddoRS  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Service to unblur a slightly blurry Passport?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15lsy4f/ai_service_to_unblur_a_slightly_blurry_passport/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15lsy4f/ai_service_to_unblur_a_slightly_blurry_passport/"/>
        <updated>2023-08-08T20:12:03.000Z</updated>
        <summary type="html"><![CDATA[All services I found made the blurry text even worse. Is there any which has good results for documents?
    submitted by    /u/_SarahB_  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A whole sitcom I Made using AI Art & Voice. Entertainment is on its way back to the hands of the Independent creator]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15lswdd/a_whole_sitcom_i_made_using_ai_art_voice/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15lswdd/a_whole_sitcom_i_made_using_ai_art_voice/"/>
        <updated>2023-08-08T20:10:09.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/SoundRedux  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I've developed a tool to convert voice notes into structured text: seeking your valuable feedback and suggestions!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15lryou/ive_developed_a_tool_to_convert_voice_notes_into/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15lryou/ive_developed_a_tool_to_convert_voice_notes_into/"/>
        <updated>2023-08-08T19:35:18.000Z</updated>
        <summary type="html"><![CDATA[Hi there 👋,
 I'm excited to share a project I've been working on over the past few months!
 My primary goal is to create a service that will be beneficial for people. Please share your thoughts on this idea, and suggest any new features you think I should implement!
 Exciting Features:
 • Speak to Write: with this feature, you can speak your thoughts or information and the tool will transcribe it into text. The best part? You can then forward the transcribed text to any application with just one click.
 • Audio to Action Plan: the service can transform a received audio message into a structured list of elements or bullet points. This feature is especially useful for outlining an action plan or item list.
 • Speak in and Language: you can dictate an audio message in your native language, and the service will translate it into any other language, maintaining high translation quality—significantly better than Google Translate.
 • Meeting Transcripts & Summaries: the service is perfect for converting recorded audio from meetings into text and generating concise summaries. It supports the upload of users' files.
 Thank you for taking the time to check it out. I look forward to hearing your feedback. You can access the service by visiting this link: https://audionotes.ai
    submitted by    /u/OneMoreSuperUser  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Growing Bonsai Networks with RNNs]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15lq8jj/growing_bonsai_networks_with_rnns/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15lq8jj/growing_bonsai_networks_with_rnns/"/>
        <updated>2023-08-08T18:30:27.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Ameobea  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Studying RL is hard]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15lq3ka/studying_rl_is_hard/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15lq3ka/studying_rl_is_hard/"/>
        <updated>2023-08-08T18:25:11.000Z</updated>
        <summary type="html"><![CDATA[I want to study Reinforcement Learning, but the concepts are really hard and mathematical. Whenever I think I grasp something I forget it the next day completly. The Basic Concepts of MDP is the only thing which I think I understood. But I cant understand the Training algorithms like Sarsa or Q-Learning and DQN and their implementations. I am really frustrated and overwhelmed.
 Does anyone know some good resources to understand the concepts and implementations of RL?
    submitted by    /u/Menium  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Productive constraints]]></title>
        <id>https://www.johndcook.com/blog/?p=202307</id>
        <link href="https://www.johndcook.com/blog/2023/08/08/productive-constraints/"/>
        <updated>2023-08-08T18:14:06.000Z</updated>
        <summary type="html"><![CDATA[This post will discuss two scripting languages, but that’s not what the post is really about. It’s really about expressiveness and (or versus) productivity. *** I was excited to discover the awk programming language sometime in college because I had not used a scripting language before. Compared to C, awk was high-level luxury. Then a […]
Productive constraints first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are there are any *good* image gen AI APIs?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15lplcy/are_there_are_any_good_image_gen_ai_apis/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15lplcy/are_there_are_any_good_image_gen_ai_apis/"/>
        <updated>2023-08-08T18:06:26.000Z</updated>
        <summary type="html"><![CDATA[I have a killer project idea but it requires fully custom image generation. Character portraits. Any API like that out there?
    submitted by    /u/thedarklord176  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R][P] Review to two/three words summarization | Text tagging]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15lp6yl/rp_review_to_twothree_words_summarization_text/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15lp6yl/rp_review_to_twothree_words_summarization_text/"/>
        <updated>2023-08-08T17:52:03.000Z</updated>
        <summary type="html"><![CDATA[Hello, I'm looking for a model (probably two models) that would:
  
Summarize reviews (e.g. website review) to two/three words.
 
Reuse these words or "review tokens" to tag reviews with similar content. Then if a review's content differs (e.g. cosine sim. of 0.2), another tag will be generated from the review that diverges.
 
 Is there anything like this on the "market"? 
    submitted by    /u/BartPetersyn  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSC Weekly 8 August 2023]]></title>
        <id>https://www.datasciencecentral.com/?p=62840</id>
        <link href="https://www.datasciencecentral.com/dsc-weekly-8-august-2023/"/>
        <updated>2023-08-08T17:36:20.000Z</updated>
        <summary type="html"><![CDATA[Announcements Top Stories In-Depth
The post DSC Weekly 8 August 2023 appeared first on Data Science Central.]]></summary>
        <author>
            <name>Scott Thompson</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is it necessary to run "episodes" in model-free learning?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15lofrn/is_it_necessary_to_run_episodes_in_modelfree/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15lofrn/is_it_necessary_to_run_episodes_in_modelfree/"/>
        <updated>2023-08-08T17:23:51.000Z</updated>
        <summary type="html"><![CDATA[In Q-learning (image), episodes are run, in the sense that, the states are visited in the order they appear as part of one sequence in an episode.
 In Dyna-Q (image) (which is btw described to be the same as Q-learning when the planning portion is deleted), there doesn't seem to be any iteration over the states of an episode. It just picks a state, applies the e-greedy policy on it to choose the action, learns, updates the model, then plans.
 Would Q-learning also work fine if we got rid of the "episodes" and just picked isolated state-action pairs?
 Thank you
    submitted by    /u/AstronautVarious3791  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The emergence of prompt engineers: The next in-demand role in AI]]></title>
        <id>https://www.datasciencecentral.com/?p=62837</id>
        <link href="https://www.datasciencecentral.com/the-emergence-of-prompt-engineers-the-next-in-demand-role-in-ai/"/>
        <updated>2023-08-08T17:21:19.000Z</updated>
        <summary type="html"><![CDATA[Prompt engineers are emerging as key players in the development and optimization of AI models as artificial intelligence (AI) continues its evolution and becomes an integral part of various industries. As experts at crafting effective prompts, they have been instrumental in shaping the future of artificial intelligence through their ability to enable models to deliver… Read More »The emergence of prompt engineers: The next in-demand role in AI
The post The emergence of prompt engineers: The next in-demand role in AI appeared first on Data Science Central.]]></summary>
        <author>
            <name>Roger Brown</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SIGGRAPH Special Address: NVIDIA CEO Brings Generative AI to LA Show]]></title>
        <id>https://blogs.nvidia.com/?p=65973</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/08/siggraph-2023-special-address/"/>
        <updated>2023-08-08T17:12:28.000Z</updated>
        <summary type="html"><![CDATA[As generative AI continues to sweep an increasingly digital, hyperconnected world, NVIDIA founder and CEO Jensen Huang made a thunderous return to SIGGRAPH, the world’s premier computer graphics conference. “The generative AI era is upon us, the iPhone moment if you will,” Huang told an audience of thousands Tuesday during an in-person special address in Read article >]]></summary>
        <author>
            <name>Brian Caulfield</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Spectrum of Specialization in ML]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15lnyh6/d_spectrum_of_specialization_in_ml/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15lnyh6/d_spectrum_of_specialization_in_ml/"/>
        <updated>2023-08-08T17:05:18.000Z</updated>
        <summary type="html"><![CDATA[Hello to everyone reading this. 
 I am just about to finish Andrew NG's course 3 courses on ML specialization and I have had 2 courses on ML as well in my Business Intelligence Analytics studies at uni. 
 Now I am extremely interested in ML but I see there are wide diaspora of different subfields you can focus on. I need to get into the job market as fast as possible. So can anyone guide me which aspect of ML should I give most of my time to practice and build portfolio that would translate well to interviews and hiring?
 Thank you
    submitted by    /u/JaguarMoosa  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Question Difficulty Predictor]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15lnw0z/d_question_difficulty_predictor/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15lnw0z/d_question_difficulty_predictor/"/>
        <updated>2023-08-08T17:02:41.000Z</updated>
        <summary type="html"><![CDATA[How would you proceed on a project in assessing the difficulty level of a question? I tried using lexicographic metrics like flesh-kincaid score, etc., but those did not yield proper results. Is there a good method I could use? Also, how could I assess the "readability" of a question, or in other words, how easy it is to understand what the question is asking.
    submitted by    /u/uglyboi34  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R]eleasing a new model for conditional music generation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15lntx9/releasing_a_new_model_for_conditional_music/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15lntx9/releasing_a_new_model_for_conditional_music/"/>
        <updated>2023-08-08T17:00:43.000Z</updated>
        <summary type="html"><![CDATA[Hey y'all, this is a model I have been independently building for some time. It uses parts of OpenAI's Jukebox and HarmonAI's Dance Diffusion model.
 Overall it is a hierarchical latent diffusion modeland generates complete linked musical phrases at good quality.
 More information as well as examples can be found here: https://medium.com/@jeffsontagmusic/jukebox-diffusion-cbe22ff3cd47
 Thanks!
    submitted by    /u/jmoso13  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to stay on the cutting edge of applied ML/AI while doing my PhD?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15lnt4g/d_how_to_stay_on_the_cutting_edge_of_applied_mlai/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15lnt4g/d_how_to_stay_on_the_cutting_edge_of_applied_mlai/"/>
        <updated>2023-08-08T16:59:58.000Z</updated>
        <summary type="html"><![CDATA[A lot of my PhD work will be in using different types of ML/NN approaches to characterizing problems in my field. It's kind of weird, since for my undergrad I came from a more traditional science background where we research off papers that were written like 2-20 years ago. Since a lot of these architectures and whatever are updating so fast, I wanted to see if there's a good way to keep up with the latest information so my work wouldn't be outdated by the time I publish. Is there a general workflow that those of you in the field follow in regards to this?
    submitted by    /u/This-Is-My-20th-Acc  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A blog on LoRA and QLoRA finetuning techniques [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15lnfbh/a_blog_on_lora_and_qlora_finetuning_techniques_p/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15lnfbh/a_blog_on_lora_and_qlora_finetuning_techniques_p/"/>
        <updated>2023-08-08T16:44:59.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone,
 I wrote a blog on LoRA and QLoRA. Hope it helps you in understanding the theory behind them 🤗
 https://medium.com/@gitlostmurali/understanding-lora-and-qlora-the-powerhouses-of-efficient-finetuning-in-large-language-models-7ac1adf6c0cf
 If the above one is behind paywall, you can visit the blog here (https://gitlostmurali.com/machine-learning/data-science/lora-qlora)
    submitted by    /u/Outlandish_MurMan  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Startup Pens Generative AI Success Story With NVIDIA NeMo]]></title>
        <id>https://blogs.nvidia.com/?p=65735</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/08/writer-nemo-generative-ai/"/>
        <updated>2023-08-08T16:34:53.000Z</updated>
        <summary type="html"><![CDATA[Machine learning helped Waseem Alshikh plow through textbooks in college. Now he’s putting generative AI to work, creating content for hundreds of companies. Born and raised in Syria, Alshikh spoke no English, but he was fluent in software, a talent that served him well when he arrived at college in Lebanon. “The first day they Read article >]]></summary>
        <author>
            <name>Chintan Patel</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] I’m losing my voice due to illness, and I’m looking for ML/AI solution]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15lmvmc/d_im_losing_my_voice_due_to_illness_and_im/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15lmvmc/d_im_losing_my_voice_due_to_illness_and_im/"/>
        <updated>2023-08-08T16:23:46.000Z</updated>
        <summary type="html"><![CDATA[Hey all, like the title says, I’m losing my voice due to an illness (Parkinson’s disease), and I would like to create an AI voice using recordings from 10 years ago. I used to be a prolific podcaster, and I have about 50 episodes of podcasts that I can use as input. Is this possible? What service or software can I use? My voice is beyond repair since Parkinson’s is a progressive disease. An AI voice would allow me to work and would open up new doors for me. Thank you!
    submitted by    /u/NWMoney101  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA Makes Extended-Reality Streaming More Scalable, Customizable for Enterprises and Developers]]></title>
        <id>https://blogs.nvidia.com/?p=65949</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/08/cloudxr-suite-simplifies-enterprise-streaming/"/>
        <updated>2023-08-08T16:17:47.000Z</updated>
        <summary type="html"><![CDATA[Organizations across industries are using extended reality (XR) to redesign workflows and boost productivity, whether for immersive training or collaborative design reviews. With the growing use of all-in-one (AIO) headsets, more teams have adopted and integrated XR. While easing XR use, AIO headsets have modest compute and rendering power that can limit the graphics quality Read article >]]></summary>
        <author>
            <name>Greg Jones</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extended Cut: NVIDIA Expands Maxine for Video Editing, Showcases 3D Virtual Conferencing Research]]></title>
        <id>https://blogs.nvidia.com/?p=65945</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/08/maxine-3d-video-communications/"/>
        <updated>2023-08-08T16:15:26.000Z</updated>
        <summary type="html"><![CDATA[Professionals, teams, creators and others can tap into the power of AI to create high-quality audio and video effects — even using standard microphones and webcams — with the help of NVIDIA Maxine. The suite of GPU-accelerated software development kits and cloud-native microservices lets users deploy AI features that enhance audio, video and augmented-reality effects Read article >]]></summary>
        <author>
            <name>Rick Champagne</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is there AI that browses a website, checks the structure of the content of the page and then writes a script for me that extracts the data regularly?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15lmfky/is_there_ai_that_browses_a_website_checks_the/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15lmfky/is_there_ai_that_browses_a_website_checks_the/"/>
        <updated>2023-08-08T16:06:59.000Z</updated>
        <summary type="html"><![CDATA[I just want a script to perform the task not AI itself so that I have something reliable. It always puzzles me why these things don't instantly pop up as services where I don't have to worry about even deploying the script (but that's another issue). 
    submitted by    /u/VLADIMIROVIC_L  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Content Creation ‘In the NVIDIA Studio’ Gets Boost From New Professional GPUs, AI Tools, Omniverse and OpenUSD Collaboration Features]]></title>
        <id>https://blogs.nvidia.com/?p=65967</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/08/siggraph-studio-rtx-omniverse-openusd/"/>
        <updated>2023-08-08T16:00:37.000Z</updated>
        <summary type="html"><![CDATA[AI and accelerated computing were in the spotlight at SIGGRAPH — the world’s largest gathering of computer graphics experts — as NVIDIA founder and CEO Jensen Huang announced during his keynote address updates to NVIDIA Omniverse, a platform for building and connecting 3D tools and applications, as well as acceleration for Universal Scene Description (known as OpenUSD), the open and extensible ecosystem for 3D worlds.]]></summary>
        <author>
            <name>Gerardo Delgado</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nvidia, Hugging Face collaboration on DGX...noice!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15lm8wz/nvidia_hugging_face_collaboration_on_dgxnoice/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15lm8wz/nvidia_hugging_face_collaboration_on_dgxnoice/"/>
        <updated>2023-08-08T16:00:32.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Internet0fGames  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shutterstock Brings Generative AI to 3D Scene Backgrounds With NVIDIA Picasso]]></title>
        <id>https://blogs.nvidia.com/?p=65994</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/08/shutterstock-generative-ai-picasso-360-hdri__trashed/"/>
        <updated>2023-08-08T16:00:14.000Z</updated>
        <summary type="html"><![CDATA[Picture this: Creators can quickly create and customize 3D scene backgrounds with the help of generative AI, thanks to cutting-edge tools from Shutterstock. The visual-content provider is building services using NVIDIA Picasso — a cloud-based foundry for developing generative AI models for visual design. The work incorporates Picasso’s latest feature — announced today during NVIDIA Read article >]]></summary>
        <author>
            <name>Jason Paul</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Textured Approach: NVIDIA Research Shows How Gen AI Helps Create and Edit Photorealistic Materials]]></title>
        <id>https://blogs.nvidia.com/?p=65932</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/08/siggraph-research-generative-ai-materials-3d-scenes/"/>
        <updated>2023-08-08T16:00:05.000Z</updated>
        <summary type="html"><![CDATA[NVIDIA researchers are taking the stage at SIGGRAPH, the world’s largest computer graphics conference, to demonstrate a generative AI workflow that helps artists rapidly create and iterate on materials for 3D scenes. The research demo, which will be presented today at the show’s Real-Time Live event, showcases how artists can use text or image prompts Read article >]]></summary>
        <author>
            <name>Isha Salian</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Candle: Torch Replacement in Rust]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15lm6tk/p_candle_torch_replacement_in_rust/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15lm6tk/p_candle_torch_replacement_in_rust/"/>
        <updated>2023-08-08T15:58:25.000Z</updated>
        <summary type="html"><![CDATA[Candle is a minimalist ML framework for Rust
 Some of its features
  
Examples of popular models: Whisper, Llama 2, Falcon, Bert, Starcoder
 WASM support, so you can run the models directly in the browser
 User-defined kernels, so you can use Flash Attention
 Similar syntax to PyTorch
 Data loaders
 Transformer utilities
  
   submitted by    /u/hackerllama  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GPT4 Chose Female Character for Youtube, Named AI Ada, as reference to Ada Lovelace, first women programmer in order to pay homage to the vital role women have played, and continue to play, in the field of technology and AI. Quite Awesome!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15lm6ql/gpt4_chose_female_character_for_youtube_named_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15lm6ql/gpt4_chose_female_character_for_youtube_named_ai/"/>
        <updated>2023-08-08T15:58:20.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/stefanbg92  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intuition about what features deep RL learns?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15lm5nl/intuition_about_what_features_deep_rl_learns/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15lm5nl/intuition_about_what_features_deep_rl_learns/"/>
        <updated>2023-08-08T15:57:13.000Z</updated>
        <summary type="html"><![CDATA[I know for image recognition there is a rough intuition that neural network lower layers learn low level features like edges, and the higher layers learn more complex compositions of the lower layer features. Is there a similar intuition about what a value network or policy network learns in deep RL? If there are any papers that investigate this that would be helpful
    submitted by    /u/Turkeydunk  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DENZA Collaborates With WPP to Build and Deploy Advanced Car Configurators on NVIDIA Omniverse Cloud]]></title>
        <id>https://blogs.nvidia.com/?p=65941</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/08/denza-wpp-car-configurators-nvidia-omniverse-cloud/"/>
        <updated>2023-08-08T15:51:13.000Z</updated>
        <summary type="html"><![CDATA[DENZA, the luxury EV brand joint venture between BYD and Mercedes-Benz, has collaborated with marketing and communications giant WPP and NVIDIA Omniverse Cloud to build and deploy its next generation of car configurators, NVIDIA founder and CEO Jensen Huang announced at SIGGRAPH. WPP is using Omniverse Cloud — a platform for developing, deploying and managing Read article >]]></summary>
        <author>
            <name>James Mills</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChatGPT for Beginners: How to Create Images]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15lld3n/chatgpt_for_beginners_how_to_create_images/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15lld3n/chatgpt_for_beginners_how_to_create_images/"/>
        <updated>2023-08-08T15:27:42.000Z</updated>
        <summary type="html"><![CDATA[Tutorial about creating images using ChatGPT.
    submitted by    /u/SplitYOLO  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to keep my ML skills whilst on another job?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15lkqtq/d_how_to_keep_my_ml_skills_whilst_on_another_job/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15lkqtq/d_how_to_keep_my_ml_skills_whilst_on_another_job/"/>
        <updated>2023-08-08T15:04:44.000Z</updated>
        <summary type="html"><![CDATA[Hey all, I have a technical background, having studied engineering and ML at one of the world's leading universities. I really enjoyed it and did well, but long story short, since graduating (coming to 2 years) I have been working in a Family Office, doing things I don't feel are very related.
 I wanted to know what kind of things I can do to keep myself in the loop and continue developing my ML/DS skills in my spare time. Alternatively, ideas of projects I could have just to make sure I have a portfolio?
    submitted by    /u/thegreatudini  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Host the Spark UI on Amazon SageMaker Studio]]></title>
        <id>fe4b7176ee2cb00d55731c82d55cffc5048de587</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/host-the-spark-ui-on-amazon-sagemaker-studio/"/>
        <updated>2023-08-08T14:56:37.000Z</updated>
        <summary type="html"><![CDATA[Amazon SageMaker offers several ways to run distributed data processing jobs with Apache Spark, a popular distributed computing framework for big data processing. You can run Spark applications interactively from Amazon SageMaker Studio by connecting SageMaker Studio notebooks and AWS Glue Interactive Sessions to run Spark jobs with a serverless cluster. With interactive sessions, you […]]]></summary>
        <author>
            <name>Giuseppe Angelo Porcelli</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video editing ai]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15lkic2/video_editing_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15lkic2/video_editing_ai/"/>
        <updated>2023-08-08T14:56:06.000Z</updated>
        <summary type="html"><![CDATA[Hello, I'm currently editing videos using capcut, which is not ideal.
 I'm looking for an ai, that ideally :
 Finds me B-roll according to what I speak. Cuts "bad takes" out Good captions "TikTok style" Audio enhance.
 Do you guys know anything like this?
 Thank you!
    submitted by    /u/Orlandostyler  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deploy thousands of model ensembles with Amazon SageMaker multi-model endpoints on GPU to minimize your hosting costs]]></title>
        <id>0d891430918fc0e8d35e838165355924f5d50203</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/deploy-thousands-of-model-ensembles-with-amazon-sagemaker-multi-model-endpoints-on-gpu-to-minimize-your-hosting-costs/"/>
        <updated>2023-08-08T14:50:24.000Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence (AI) adoption is accelerating across industries and use cases. Recent scientific breakthroughs in deep learning (DL), large language models (LLMs), and generative AI is allowing customers to use advanced state-of-the-art solutions with almost human-like performance. These complex models often require hardware acceleration because it enables not only faster training but also faster inference […]]]></summary>
        <author>
            <name>Saurabh Trikande</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Möbius transformations over a finite field]]></title>
        <id>https://www.johndcook.com/blog/?p=202282</id>
        <link href="https://www.johndcook.com/blog/2023/08/08/finite-mobius/"/>
        <updated>2023-08-08T14:01:26.000Z</updated>
        <summary type="html"><![CDATA[A Möbius transformation is a function of the form where ad – bc = 1. We usually think of z as a complex number, but it doesn’t have to be. We could define Möbius transformations in any context where we can multiply, add, and divide, i.e. over any field. In particular, we could work over […]
Möbius transformations over a finite field first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I made an animated video explaining Effective Accelerationism (aka e/acc), a philosophical movement related to AI that has recently grown a lot in popularity and offers a path to a post-scarcity technological utopia. It has even been endorsed by Marc Andreessen and Garry Tan.]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15lj0ao/i_made_an_animated_video_explaining_effective/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15lj0ao/i_made_an_animated_video_explaining_effective/"/>
        <updated>2023-08-08T13:58:35.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/antaloaalonso  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spotify AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15lixcq/spotify_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15lixcq/spotify_ai/"/>
        <updated>2023-08-08T13:55:22.000Z</updated>
        <summary type="html"><![CDATA[I've been using this today whilst I've been working and I found it pretty comical at first with the voice that talks to you, but now I'm starting to love it! I want it to talk more when it does talk. It feels like a nice break in the music to have the AI talk like a radio host. I'm sure some people would rather that not being a feature (if they use it at all), but I'd love for it to have some more comedic one-liners, possible news updates, and potentially traffic updates based on location and if it knows you're driving. Would be awesome! 
 It's also a really good tool for if you want to listen to music you've not heard before. Whether it's part of your usual genre or not.
 Looking forward to seeing how this progresses!
    submitted by    /u/Columbian_Toad  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative AI: An Artist's Honest Perspective]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15liom9/generative_ai_an_artists_honest_perspective/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15liom9/generative_ai_an_artists_honest_perspective/"/>
        <updated>2023-08-08T13:45:35.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone.
 I am an artist. And programmer, and kind of a bit of everything. But what is important, is that I was an artist before the current "generative AI" was a thing, and I have been drawing, digitally and traditionally alike for like... a decade?
 Art, to me, is getting what is inside your head, and presenting it to others outside of your consciousness and thoughts. It's showing the world a piece of your interpretation, your experience, your impressions of the world you inhabit. It's about communicating to others your emotions, your ideas, your thoughts and feelings.
 Not everyone can draw, or paint, or sculpt. I could say "learn it, it's easy", but that would be a lie. It isn't easy. It is years upon years of constant, hard work, requiring focus and dedication, and a passion for l…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Allen Institute for AI takes new approach to managing AI risks and promoting transparency]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15lieim/allen_institute_for_ai_takes_new_approach_to/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15lieim/allen_institute_for_ai_takes_new_approach_to/"/>
        <updated>2023-08-08T13:34:19.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/DarronFeldstein  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P]MMLU-by-Task Evaluation Results for 500+ Open Source Models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15li768/pmmlubytask_evaluation_results_for_500_open/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15li768/pmmlubytask_evaluation_results_for_500_open/"/>
        <updated>2023-08-08T13:26:12.000Z</updated>
        <summary type="html"><![CDATA[Typically, research papers and leaderboards only report the overall score on Measuring Massive Multitask Language Understanding (MMLU) and not per task performance. Hugging Face recently released detailed evaluation data that includes per task performance. I made a sortable leaderboard here https://huggingface.co/spaces/CoreyMorris/MMLU-by-task-Leaderboard . You can also make custom scatter plots on the site so you can explore the relationship between parameter count and performance.
    submitted by    /u/corey1505  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Getting the Hang of OpenCV’s Inner Workings with ChatGPT]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15lgoqr/getting_the_hang_of_opencvs_inner_workings_with/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15lgoqr/getting_the_hang_of_opencvs_inner_workings_with/"/>
        <updated>2023-08-08T12:22:16.000Z</updated>
        <summary type="html"><![CDATA[​
 https://preview.redd.it/xdp3bkwwpvgb1.jpg?width=2800&format=pjpg&auto=webp&s=513a63ed81eec85e6bc254f84e4208094afc7d4a
 Very interesting blog post from OpenCV.ai team about how can explore ChatGPT to serve for code development debugging.
 Introduction from the article:
 As programmers, we often work with familiar development environments, but occasionally we encounter new tools that can be time-consuming and challenging to learn. In such situations, having virtual assistance can be extremely beneficial.
 In this article, I will share my experience of contributing to OpenCV, a renowned open-source library, despite having limited knowledge of C++ and understanding its architecture. I achieved this with the assistance of ChatGPT, a Large Language Model (LLM).
 I hope you can find it interesting. More details are here.
    submitted by    /u/No-Independence5880  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Current trends in explainability?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15lgjw7/d_current_trends_in_explainability/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15lgjw7/d_current_trends_in_explainability/"/>
        <updated>2023-08-08T12:16:30.000Z</updated>
        <summary type="html"><![CDATA[I've realized my technical understanding of explainability is a few years behind, having last focused on it with LIME and Shap. Does anyone have a survey reference they like for recent trends and updates in ML explainability?
    submitted by    /u/balcell  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixture of Experts (MoE)]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15lfuxs/mixture_of_experts_moe/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15lfuxs/mixture_of_experts_moe/"/>
        <updated>2023-08-08T11:45:42.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/ABDULKADER90H  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How do I make AI-generated videos with prompts?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15lfs8o/how_do_i_make_aigenerated_videos_with_prompts/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15lfs8o/how_do_i_make_aigenerated_videos_with_prompts/"/>
        <updated>2023-08-08T11:42:12.000Z</updated>
        <summary type="html"><![CDATA[How do I make AI-generated videos with prompts for free?
    submitted by    /u/DankDude6T9  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I'm making my first AI game.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15levaa/im_making_my_first_ai_game/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15levaa/im_making_my_first_ai_game/"/>
        <updated>2023-08-08T10:58:37.000Z</updated>
        <summary type="html"><![CDATA[Hello AI enthusiasts!
 I'm a software engineer passionate about AI, and recently I've been experimenting with making my first AI game. 
 In the game, you try to negotiate a price down on a watch with an AI-driven salesman, rewarding -or roasting lol- you depending on your bargaining skills.
 I’d be more than happy to get your thoughts and feedback on this idea, it's the first application I've built using AI so any tips would be much appreciated! 
 Thanks!
    submitted by    /u/gavo_gavo  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sod Off, Human! AI's Magic Revealed!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15ledxh/sod_off_human_ais_magic_revealed/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15ledxh/sod_off_human_ais_magic_revealed/"/>
        <updated>2023-08-08T10:34:50.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/ispeakout  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] What's the current research status of "SFT with high-quality data" vs RLHF?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ldjdz/r_whats_the_current_research_status_of_sft_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ldjdz/r_whats_the_current_research_status_of_sft_with/"/>
        <updated>2023-08-08T09:51:59.000Z</updated>
        <summary type="html"><![CDATA[At first, with InstructGPT and ChatGPT, it looked like RLHF was the holy grail to successfully finetune LLMs on human preferences. Then, from May 2023 onwards, a trend of doing just SFT with high-quality data showed up (e.g. "LIMA: Less Is More for Alignment" https://arxiv.org/abs/2305.11206) as an alternative to doing RLHF.
 What's your opinion on these two narratives? Is RLHF likely to still be relevant even in the presence of SFT with high-quality data?
    submitted by    /u/bornot2b  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A body-positive nonprofit replaced staff with an AI chatbot – the move backfired]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15lcvuq/a_bodypositive_nonprofit_replaced_staff_with_an/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15lcvuq/a_bodypositive_nonprofit_replaced_staff_with_an/"/>
        <updated>2023-08-08T09:17:32.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/intengineering  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is there an AI for reviewing videos based on audience category?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15lbx8z/is_there_an_ai_for_reviewing_videos_based_on/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15lbx8z/is_there_an_ai_for_reviewing_videos_based_on/"/>
        <updated>2023-08-08T08:26:02.000Z</updated>
        <summary type="html"><![CDATA[I want to start making a YouTube channel, because I've got a passion project I want to work on with a Minecraft modpack. Obviously, Minecraft is a HUGE game and has thousands of videos posted every day... This is why I want to know if there is an AI that can rate videos based on editing, audience engagement, sound, etc... Also giving areas of improvement and the strengths of the video.
 Probably a big ask and SO far fetched, but there's always a chance of something being out there.
    submitted by    /u/Columbian_Toad  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI photo editor recommendationd]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15lbu86/ai_photo_editor_recommendationd/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15lbu86/ai_photo_editor_recommendationd/"/>
        <updated>2023-08-08T08:21:00.000Z</updated>
        <summary type="html"><![CDATA[Can someone recommend a great AI photo editor that can take 100 profile photos and standardise them, IE crop so head is same size across all photos, background removed and placed on standard back ground.
    submitted by    /u/Woodger  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sorry Jarvis]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15lbba2/sorry_jarvis/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15lbba2/sorry_jarvis/"/>
        <updated>2023-08-08T07:51:18.000Z</updated>
        <summary type="html"><![CDATA[​
 https://preview.redd.it/9epla7xjdugb1.png?width=960&format=png&auto=webp&s=92190970027b08476ac9899a42d7099fe67cf5aa
    submitted by    /u/Maxie445  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] What has your experience been as someone joining ML from a lateral field?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15la8cy/discussion_what_has_your_experience_been_as/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15la8cy/discussion_what_has_your_experience_been_as/"/>
        <updated>2023-08-08T06:53:13.000Z</updated>
        <summary type="html"><![CDATA[Hi all, 
 I am currently already working in the field of ML research at a big name medical research center. Our main focus is in application of ML methods with the focus on stroke diagnostics and treatment.
 Now, I am quite happy working here but my background is somewhat interdisciplinary. I have a bachelor's in Life science and a Master in bioinformatics. Because of this I always feel like I have to catch up to my colleagues when it comes to ML and in parts also computer science knowledge. It feels like there are a million things to learn and many small details to know that I am not even sure how to look up.
 I am curious what your experience has been if you were/are in a similar situation? How did you manage to catch up?
    submitted by    /u/JuicyLambda  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Does SOTA performance on object detection seem low to anybody else?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15l87gd/d_does_sota_performance_on_object_detection_seem/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15l87gd/d_does_sota_performance_on_object_detection_seem/"/>
        <updated>2023-08-08T05:05:32.000Z</updated>
        <summary type="html"><![CDATA[Either I'm too new to the space, or I'm stating the obvious, but it seems that object detection performance is really low. The SOTA currently is 66% on COCO test-dev, which doesn't match how well it seems like AI is currently performing with self-driving cars, surveillance tech, and others. Am I missing something?
    submitted by    /u/philipkd  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Hierarchical Representation and Propagation of Wavefunctions within Gaussian Basis Functions]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15l7236/r_hierarchical_representation_and_propagation_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15l7236/r_hierarchical_representation_and_propagation_of/"/>
        <updated>2023-08-08T04:07:41.000Z</updated>
        <summary type="html"><![CDATA[I. Introduction
 This paper aims to provide an in-depth explanation of representing and propagating wavefunctions in a hierarchical manner using Gaussian basis functions. Wavefunctions are mathematical descriptions of the quantum states of physical systems and are fundamental to quantum mechanics. However, representing complex wavefunctions for real-world quantum systems remains a key challenge. This paper proposes using multiple layers of Gaussian basis functions, with trainable amplitudes, to represent wavefunctions in a hierarchical fashion and enable wavefunction propagation between layers.
 Understanding wavefunction representation and propagation has significant implications in diverse fields like quantum computing, quantum chemistry, and materials science. Efficient wavefunction man…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 8/7/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15l6utw/oneminute_daily_ai_news_872023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15l6utw/oneminute_daily_ai_news_872023/"/>
        <updated>2023-08-08T03:58:27.000Z</updated>
        <summary type="html"><![CDATA[Data analytics company Qureight has entered into a multi-year strategic research collaboration with AstraZeneca that will use AI models to accelerate research into lung diseases.[1] 
 Zoom’s terms of service update establishes the video platform’s right to use some customer data for training its AI models.[2]
 Cigna, one of the country’s largest health insurance companies, faces a class action lawsuit over charges that it illegally used an AI algorithm to deny hundreds of thousands of claims without a physician’s review.[3]
 Japan plans guidelines for AI-savvy human resources.[4]
  
Sources:
 [1] https://www.digitalhealth.net/2023/08/qureight-collaborates-with-astrazeneca-for-ai-lung-disease-research/
 [2] https://www.cnbc.com/2023/08/07/zoom-ai-tools-trained-using-some-customer-data.html
 [3] https://www.medicaleconomics.com/view/cigna-using-ai-to-reject-claims-lawsuit-charges
 [4] https://asianews.network/japan-plans-guidelines-for-ai-savvy-human-resources/
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ai generated trailer for horror film “Magic 8”]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15l3f7c/ai_generated_trailer_for_horror_film_magic_8/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15l3f7c/ai_generated_trailer_for_horror_film_magic_8/"/>
        <updated>2023-08-08T01:20:09.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/SellowYubmarine  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evol-Instruct Dataset Creation [R] [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15l2jr8/evolinstruct_dataset_creation_r_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15l2jr8/evolinstruct_dataset_creation_r_d/"/>
        <updated>2023-08-08T00:42:04.000Z</updated>
        <summary type="html"><![CDATA[I’ve been researching the Evol-Instruct datasets now for a few days and have decided I want to build my own out for a specific use case.
 I’ve read literally everything possible, admittedly not much outside of WizardLM and GeorgiaTech, but I’ve read it.
 I was hoping to discuss it here with smarter people. 
 I’m seeing this as a way to use LLMs to generate great datasets. However, my use case doesn’t really exist in any models yet. Not thoroughly enough to produce a good Evol-Instruct set. So, I’m going to do that tomorrow.
 I’m going to use The Blokes WizardCoder-Guanaco 15b GPTQ version to train on my specific dataset - about 10GB of clean, really strong data I’ve spent 3-4 weeks putting together.
 In theory, I’ll use the Evol-Instruct script from WizardLM to generate the new dataset, and then I’ll apply that to whatever model I decide to use. There is a good chance I train my own on general Evol-Instruct datasets available now, and likely quite a large one.
 I’m looking for any tips, discussion, ideas, thoughts from the community. 
 Cheers!
    submitted by    /u/LoadingALIAS  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Please criticize our llm writing integration app [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15l18cc/please_criticize_our_llm_writing_integration_app_p/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15l18cc/please_criticize_our_llm_writing_integration_app_p/"/>
        <updated>2023-08-07T23:46:10.000Z</updated>
        <summary type="html"><![CDATA[Here's the pitch:
 We made an editor called Gamut that lets you enter your ideas in any form you want. Bullets, carefully constructed paragraphs, it doesn’t matter. Then, our patent-pending technology lets you convert to prose and adjust, shaping the text like a graphic designer shapes an image.
 We want r/MachineLearning's advice and field experience, because tbh we're just a bunch of teenagers who haven't even gone to college yet.
 Check it out: gamut.ink
    submitted by    /u/gamut_ink  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]Could current AI tech make a movie of Alejandro Jodorowsky's vision of 'Dune'?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15l11ci/dcould_current_ai_tech_make_a_movie_of_alejandro/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15l11ci/dcould_current_ai_tech_make_a_movie_of_alejandro/"/>
        <updated>2023-08-07T23:38:17.000Z</updated>
        <summary type="html"><![CDATA[I was just watching the documentary about the 'greatest movie never made', director Alejandro Jodorowsky's vision of Frank Herbert's Dune.
 There is a huge book that contains a storyboard version of the movie with lots of production art by artists Moebius, Chris Foss and HR Giger.
 The movie was to star Jodorowsky's son as Paul Atriedes, Salvadore Dali as the Emperor, Orson Wells as Baron Harkonnen and Mick Jagger as Feyd.
 Could one of today's AIs be 'fed' Jodorowsky's book and create a movie of his vision?
 Curious to know what your opinions are on this.
 Thanks.
    submitted by    /u/shopdog  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Regression using batch trend data]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15l0wm0/p_regression_using_batch_trend_data/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15l0wm0/p_regression_using_batch_trend_data/"/>
        <updated>2023-08-07T23:32:54.000Z</updated>
        <summary type="html"><![CDATA[Hi, all,
 I would like to use batch reaction trend data to build a regression model. I'm wondering what is the best way to approach this.
 Here's some background:
 Reaction Data: 
  
 Time (min) Pressure (bar) Temperature (°C) Flow (kg/h) Gas Total (kg) 
  
 1 10 70 502 8 
  2 10.1 71 498 16 
  ... ... ... ... ... 
  102 10.3 76 475 850 
 
 Output: Polymer property X
 The reaction continues until a gas total is met and the time this takes depends on the other variables.
 I have ~700 batches of data in a format similar to the above and would like to predict polymer property X. As the variables can change minute to minute I was thinking of binning the variables into 5 minute bins using the mean and using these as variables for linear regression or similar.
 Is this a valid approach or is there another way I can approach the problem?
 Thanks!
    submitted by    /u/Nefarious_P_I_G  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Awesome OOD Detection, Robustness, and Generalization]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15l0w7i/r_awesome_ood_detection_robustness_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15l0w7i/r_awesome_ood_detection_robustness_and/"/>
        <updated>2023-08-07T23:32:26.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 I have put together a repo that provides comprehensive resources for Out-of-distribution Detection, Robustness, and Generalization. The repo contains articles, talks, libraries, papers, etc. Check it out.
 https://github.com/continuousml/Awesome-Out-Of-Distribution-Detection
    submitted by    /u/Ok-Kaleidoscope-505  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA H100 Tensor Core GPU Used on New Microsoft Azure Virtual Machine Series Now Generally Available]]></title>
        <id>https://blogs.nvidia.com/?p=65942</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/07/microsoft-azure-nd-h100-v5-instance/"/>
        <updated>2023-08-07T23:09:12.000Z</updated>
        <summary type="html"><![CDATA[Microsoft Azure users can now turn to the latest NVIDIA accelerated computing technology to train and deploy their generative AI applications. Available today, the Microsoft Azure ND H100 v5 VMs using NVIDIA H100 Tensor Core GPUs and NVIDIA Quantum-2 InfiniBand networking — enables scaling generative AI, high performance computing (HPC) and other applications with a Read article >]]></summary>
        <author>
            <name>Dave Salvator</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Uncertainty Prediction in Deep Learning - CAPSA github project alternative or old code?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15kyphq/d_uncertainty_prediction_in_deep_learning_capsa/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15kyphq/d_uncertainty_prediction_in_deep_learning_capsa/"/>
        <updated>2023-08-07T22:06:16.000Z</updated>
        <summary type="html"><![CDATA[Alexander Amini, a Postdoctoral Associate at MIT, well known for the MIT's Introduction to Deep Learning Course, published a git repo called CAPSA for uncertainty prediction. This was introduced during the online course. The code was released under Thermis AI, Inc, a private company. He is the co-founder and CSO of the company. You can check how well the code was documented in the wayback machine. Recently, they removed the code base from the github and launched a pro version with selected companies as beta. The original repo (now called capsa-lite) was a great learning tool that I wanted to use. This was a quick way to try out different methods of uncertainty prediction using minimal code. Unfortunately, they have pulled all previous version of the code from the github repo. I was wondering if anyone knows a similar python package or has the old repo - would be really helpful!
    submitted by    /u/shikamaru_77  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] ML Workstation for CNN and Transformers - Feedback on Component Selection]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15kyjtm/d_ml_workstation_for_cnn_and_transformers/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15kyjtm/d_ml_workstation_for_cnn_and_transformers/"/>
        <updated>2023-08-07T22:00:37.000Z</updated>
        <summary type="html"><![CDATA[I'm putting together an ML workstation primarily focused at handling CNN and Transformer workloads. Component selection so far: https://de.pcpartpicker.com/list/zVPtt7
 I've got a couple of questions specifically regarding the motherboard. One concern I have is whether the space between the two GPUs is sufficient, as I'm planning to set them up using NVLink. Additionally, I'm curious about the compatibility of the case and motherboard for effective air cooling ( not considering water cooling at the moment). Anyone else with dual 3090s who can give some insights on how they've managed temperatures and potential overheating issues?
 Lastly, would upgrading to a Ryzen 9 5900X prevent me from bottlenecking the GPU's? 
 Would love to hear your feedback and suggestions! 
    submitted by    /u/Hugejiji  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finetuning for code generation [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15kugwx/finetuning_for_code_generation_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15kugwx/finetuning_for_code_generation_d/"/>
        <updated>2023-08-07T19:29:36.000Z</updated>
        <summary type="html"><![CDATA[i want to fine tune any open source llm for code generation purpose with some of my code. any idea what model would be suitable? and any example of implementation?
    submitted by    /u/learner_beginner  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for an AI app that can draw a widemouth bass smoking a blunt]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15kub95/looking_for_an_ai_app_that_can_draw_a_widemouth/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15kub95/looking_for_an_ai_app_that_can_draw_a_widemouth/"/>
        <updated>2023-08-07T19:23:30.000Z</updated>
        <summary type="html"><![CDATA[I want an app that can draw a widemouth bass smoking a blunt. All the free ones ive tried give me supid anime girls when all I want is fish
    submitted by    /u/Barefoot_slinger  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How difficult is it to find a job in ML/AI without a PhD, in the current bad job market?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ku1ao/d_how_difficult_is_it_to_find_a_job_in_mlai/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ku1ao/d_how_difficult_is_it_to_find_a_job_in_mlai/"/>
        <updated>2023-08-07T19:13:12.000Z</updated>
        <summary type="html"><![CDATA[Anyone here know what the trends are towards hiring for an AI/ML position without a PhD? Is it advisable to get a PhD if you want to be in the field and keep rising within it?
    submitted by    /u/CleanGarden7051  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Machine learning or quantum computing?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15kt2vw/d_machine_learning_or_quantum_computing/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15kt2vw/d_machine_learning_or_quantum_computing/"/>
        <updated>2023-08-07T18:38:50.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I'm about to graduate in Physics (PhD). I am an experimentalist with a background in electromagnetic. I am trying to apply for jobs, but there are some few options for physicists (based on my geography). So, I am trying to learn some new skill for my future job. One option would be Machine Learning, which is on-demand and the field is growing. The other option is Quantum Computing. I can start a postdoc in quantum information theory as well.
 Each path, has its pros and cons, and the final decision is based on many factors. I just don't have enough data and information to say which one is more secure in the future? Which one has less compete? And also, is it possible to get hired without any serious project in ML, and just self-taught?
 If you were me, which one would you pick?
 Thanks
    submitted by    /u/Jaded-Membership-602  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sort and remove duplicates]]></title>
        <id>https://www.johndcook.com/blog/?p=202146</id>
        <link href="https://www.johndcook.com/blog/2023/08/07/sort-u/"/>
        <updated>2023-08-07T18:36:34.000Z</updated>
        <summary type="html"><![CDATA[A common idiom in command line processing of text files is ... | sort | uniq | ... Some process produces lines of text. You want to pipe that text through sort to sort the lines in alphabetical order, then pass it to uniq to filter out all but the unique lines. The uniq utility […]
Sort and remove duplicates first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What is a typical non-academic ML salary with a PhD?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ksdz2/d_what_is_a_typical_nonacademic_ml_salary_with_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ksdz2/d_what_is_a_typical_nonacademic_ml_salary_with_a/"/>
        <updated>2023-08-07T18:13:37.000Z</updated>
        <summary type="html"><![CDATA[What is a typical non-academic ML salary with a PhD...
  
... immediately after completing the PhD? (Assuming no academic positions ever post PhD.)
 ... after 10 years of experience?
 ... in biotech specifically? (More, less, or the same as average?)
  
   submitted by    /u/Practical_Tea_3779  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Mathematics ML for Masters Application Advice?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ks6js/p_mathematics_ml_for_masters_application_advice/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ks6js/p_mathematics_ml_for_masters_application_advice/"/>
        <updated>2023-08-07T18:05:50.000Z</updated>
        <summary type="html"><![CDATA[Hi all, 
 I'm looking apply to some top masters for machine learning in the UK, so I'm guessing you know which one I'm referring to. 
 I got some guidance from the application advisor, which state they like to look at the transcript the most to have an idea of my linear algebra, calculus and statistics ability. I got 70% in "Maths for Computer Science" and some other modules I strong first and 2:1 in some others, but in general my course wasn't too mathematically intensive. I did BSc Computer Science. I have been working as SWE the past 3 years. 
 I have completed the following specialisation "Mathematics for Machine Learning and Data Science Specialization" and read "Mathematics for Machine Learning", as learning about mathematics actually got me into ML. I have also covered the videos on 3Blue1Brown etc. The application advisor said that certs don't really mean too much which is understandable. I can't change the past in terms of BSc transcript, therefore I was thinking a project may be a good way to showcase this.
 Any tips on how to best showcase this or get across my ability would be extremely helpful?
    submitted by    /u/DNOFHF  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best subscription generative AI service?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15krvfk/best_subscription_generative_ai_service/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15krvfk/best_subscription_generative_ai_service/"/>
        <updated>2023-08-07T17:54:56.000Z</updated>
        <summary type="html"><![CDATA[I’m interested in trying out a subscription-based generative AI service. Candidates include (but are not limited to) CoPilot, ChatGPT pro (or whatever it’s called), and Midjourney. Which generative service do you think is most worth the cost?
    submitted by    /u/galactictock  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How can I configure two GPUs to share their memory?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15krt3a/d_how_can_i_configure_two_gpus_to_share_their/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15krt3a/d_how_can_i_configure_two_gpus_to_share_their/"/>
        <updated>2023-08-07T17:52:29.000Z</updated>
        <summary type="html"><![CDATA[Hey, 
 I've been trying to build an ML workstation and was considering the idea of using two RTX 3090's to get the extra VRAM instead of a single 4090. However, I've come across some confusion regarding whether they can share their VRAM or not. Do I need to run them via NVLink to achieve this? I believe PyTorch's data parallelism splits the batches across both GPUs, but that wouldn't effectively combine their VRAM right?
 Any advice or insights you can share on the topic would be highly appreciated!
    submitted by    /u/Hugejiji  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Supply Base Data and Reuse with Knowledge Graphs and LLMs]]></title>
        <id>https://www.datasciencecentral.com/?p=62809</id>
        <link href="https://www.datasciencecentral.com/scaling-supply-base-data-and-reuse-with-knowledge-graphs-and-llms/"/>
        <updated>2023-08-07T16:19:35.000Z</updated>
        <summary type="html"><![CDATA[Fair Data Forecast Interview with Gregor Stühler of Scoutbee Scoutbee’s CEO and founder, Gregor Stühler, who has a background in computer science and  electrical engineering, first learned about the challenges of procurement and supply base management as a project engineer for a multinational medical device company. Scoutbee’s focus on solving supply base problems through hybrid… Read More »Scaling Supply Base Data and Reuse with Knowledge Graphs and LLMs
The post Scaling Supply Base Data and Reuse with Knowledge Graphs and LLMs appeared first on Data Science Central.]]></summary>
        <author>
            <name>Alan Morrison</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AWS performs fine-tuning on a Large Language Model (LLM) to classify toxic speech for a large gaming company]]></title>
        <id>77a17a11d0aead706432a73843e10ef5fcb04866</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/aws-performs-fine-tuning-on-a-large-language-model-llm-to-classify-toxic-speech-for-a-large-gaming-company/"/>
        <updated>2023-08-07T16:19:32.000Z</updated>
        <summary type="html"><![CDATA[The video gaming industry has an estimated user base of over 3 billion worldwide1. It consists of massive amounts of players virtually interacting with each other every single day. Unfortunately, as in the real world, not all players communicate appropriately and respectfully. In an effort to create and maintain a socially responsible gaming environment, AWS […]]]></summary>
        <author>
            <name>James Poquiz</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[any free Voice Cloning AI for Download? Without requiring Coding and Command knownlage?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15kp41y/any_free_voice_cloning_ai_for_download_without/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15kp41y/any_free_voice_cloning_ai_for_download_without/"/>
        <updated>2023-08-07T16:12:10.000Z</updated>
        <summary type="html"><![CDATA[Is there any Free AI Voice Cloner for free, that allow me simply to install the Exe? And Has option to input my Voice to it that I record? I dont have any coding and command skills. so is there something simple to install? Thanks for Answers
    submitted by    /u/Matejsteinhauser14  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best AI program for fixing heavily pixelated images of ANIMALS/ non human subjects?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15koc7n/best_ai_program_for_fixing_heavily_pixelated/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15koc7n/best_ai_program_for_fixing_heavily_pixelated/"/>
        <updated>2023-08-07T15:43:54.000Z</updated>
        <summary type="html"><![CDATA[I’ve used several AI programs that work excellent on blurred/pixelated photos of human faces but beyond that, I have not had success finding a program that can render animals in similar way. I’m more looking for something that can make the quality of a pixelated photo of say, a dog, non pixelated. Or at least, much less pixelated. 
 The images I’m trying to use are just absolutely horrible and not fixable, or I am just not using the best programs for my purposes. Or the programs I’m looking for simply do not exist yet. 
 If you have any recommendations (Paid or free programs) please do share! I have a MacBook and an iPhone if that helps.
 Thank you! 💕
    submitted by    /u/briannaleidy  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Are there any graduate programs which focus on ML + biomedicine?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15knz1x/d_are_there_any_graduate_programs_which_focus_on/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15knz1x/d_are_there_any_graduate_programs_which_focus_on/"/>
        <updated>2023-08-07T15:30:24.000Z</updated>
        <summary type="html"><![CDATA[I'm considering getting a graduate degree in ML. However, I am not very interested in NLP or academic research. I would like to learn things that are relevant to the intersection of ML and genomics or medicine. Are there any graduate programs/degrees to this effect? If so, which ones?
    submitted by    /u/Practical_Tea_3779  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI model can help determine where a patient’s cancer arose]]></title>
        <id>https://news.mit.edu/2023/ai-model-can-help-determine-where-patients-cancer-arose-0807</id>
        <link href="https://news.mit.edu/2023/ai-model-can-help-determine-where-patients-cancer-arose-0807"/>
        <updated>2023-08-07T15:00:00.000Z</updated>
        <summary type="html"><![CDATA[Predictions from the OncoNPC model could enable doctors to choose targeted treatments for difficult-to-treat tumors.]]></summary>
        <author>
            <name>Anne Trafton | MIT News</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Looking for perspectives: Pdf parsing meets PRODUCTION]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15klgt9/p_looking_for_perspectives_pdf_parsing_meets/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15klgt9/p_looking_for_perspectives_pdf_parsing_meets/"/>
        <updated>2023-08-07T13:56:31.000Z</updated>
        <summary type="html"><![CDATA[Hi folks.
 I am sure you know the running gags around “thin OpenAI wrapper” products. Instead of more toy products, I am doing an experiment with some “AI engineering” to come up with a solution that’s closer to being usable in actual production cases.
 My background is in project management and data engineering, and I’ve built large systems for big companies and worked as a consultant in the space.
 I’ve seen enough crappy data pipelines for a lifetime.
 Hence.
 I want to do something different: A thin AI wrapper is not sufficient for having reliable data pipelines that use OpenAI for schema management and inference
 So this leaves me with the following doubts:
 ​
  
How to scale code horizontally and vertically? Using third-party solutions? SNS/SQS/Kafka?
 How to log and trace? Langsmith? Custom solutions?
 How to extend reliably with my own data, and make it stateful?
  
Looking for your perspective
 ​
  
What do you think about the state of data engineering, MLOps, and infrastructure in AI companies?
 What do you think about how to scale properly the systems and prepare them for the future?
 In this code here, I do process some PDFs as a simple pipeline, what approaches do you think could be better?
  
My current thinking and the state of the project
 ​
  
I should create a formal scale of usability. I am looking for your input here.
 I should improve model consistency, extends the model with custom domain knowledge, and make an early attempt to build simple user agents in the domain
 What I have is a schema inference, contracting basics, and a way to structure unstructured data
 I’m about to create a memory component that manages the data stored in vector dbs, as a DWH for AI
 If I bring this use case that was not something available easily to the public before, how best do it?
  
Links:
 If you like my project, please give it a star :)
 my git repo 
    submitted by    /u/Snoo-bedooo  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Use multiple GPUs to load model]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15kl4m6/d_use_multiple_gpus_to_load_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15kl4m6/d_use_multiple_gpus_to_load_model/"/>
        <updated>2023-08-07T13:43:07.000Z</updated>
        <summary type="html"><![CDATA[Hey there,
 I got 2x 4090 RTX with 24GB GDDR each.I often ran into the problem of
 CUDA out of memory. Tried to allocate X MiB (GPU 0; 23.65 GiB total capacity; 22.75 GiB already allocated; 96.81 MiB free; 22.76 GiB reserved in total by PyTorch) 
 I wonder if there is a way to take usage of both GPUs so the model is split onto both GPUs.
 When training models I use torch.nn.DataParallel to use both GPUs, but it seems like I am not doing it right for load the model.
 Can anyone help me? Both GPUs are available in the system - this has already been checked.
    submitted by    /u/Sensitive_Limit1620  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] LLM Finetuning Study/Research Group]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15kkxsz/p_llm_finetuning_studyresearch_group/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15kkxsz/p_llm_finetuning_studyresearch_group/"/>
        <updated>2023-08-07T13:35:23.000Z</updated>
        <summary type="html"><![CDATA[Hey folks, 
 We're looking for people to join our research group.
 We are passionate about fine-tuning LLMs for downstream tasks, specifically LLAMA for imitating chat behaviour (being constraint aware).
 ​
 The end goal is to build an open source app where you can clone and upload your chat history (say from Whatsapp) and it starts to answer like you 
 Do let me know if it sounds interesting and you'd like to join us...
 https://preview.redd.it/sdo42mx1yogb1.png?width=1280&format=png&auto=webp&s=9de5008ed8ed18cedb25034d68984cb11e2a6a12
    submitted by    /u/im_datta0  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] humanscript: An LLM powered plain english programming language]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15kkocz/p_humanscript_an_llm_powered_plain_english/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15kkocz/p_humanscript_an_llm_powered_plain_english/"/>
        <updated>2023-08-07T13:24:34.000Z</updated>
        <summary type="html"><![CDATA[humanscript is an inferpreter. A script interpreter that infers commands from natural language using AI. There is no predefined syntax, humanscripts just say what they want to happen, and when you execute them, it happens.
 https://github.com/lukechilds/humanscript
 This is a humanscript called tidy-screenshots. It takes an unorganised directory of screenshots and organises them into directories based on the month the screenshot was taken.It can be executed like any other script.
 https://preview.redd.it/2b0oz2kgwogb1.png?width=1576&format=png&auto=webp&s=9285805a1d0668ae5fe300857f9b67161b8ecda4
 The LLM inferpreted the humanscript into the following bash script at runtime.
 ​
 https://preview.redd.it/x8hwdrzhwogb1.png?width=2188&format=png&auto=webp&s=5fcba87a9606a446d169e8ae37b5c8c251525e5e
 The code is streamed out of the LLM during inferpretation and executed line by line so execution is not blocked waiting for inference to finish. The generated code is cached on first run and will be executed instantly on subsequent runs, bypassing the need for reinferpretation.
 ​
 https://i.redd.it/t6b1stbkwogb1.gif
 The humanscript inferpreter supports a wide range of LLM backends. It can be used with cloud hosted LLMs like OpenAI's GTP-3.5 and GPT-4 or locally running open source LLMs like Llama 2.
 You can run humanscript in a sandboxed Docker environment with a single command if you want to have a play.
 https://github.com/lukechilds/humanscript#install-humanscript
    submitted by    /u/dyslexiccoder  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[humanscript: An LLM powered plain english programming language]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15kkjn9/humanscript_an_llm_powered_plain_english/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15kkjn9/humanscript_an_llm_powered_plain_english/"/>
        <updated>2023-08-07T13:19:11.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/dyslexiccoder  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Help to find a dataset for my project, please 🙏]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15kkeef/help_to_find_a_dataset_for_my_project_please/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15kkeef/help_to_find_a_dataset_for_my_project_please/"/>
        <updated>2023-08-07T13:13:22.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone! I'm a newbie and making my project on machine learning and the aim is create a programme to recognise a spice by feeding some chemical constituents, but I can't find appropriate dataset for it. I have been searching for months, and now I'm a bit desperate, so I'm asking anyone interested for help... I know maybe it was a mistake to choose exactly this topic, but I can't drop the project.
    submitted by    /u/Acceptable-Muscle-98  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI to rewrite documents like PDF or docx?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15kjmh5/ai_to_rewrite_documents_like_pdf_or_docx/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15kjmh5/ai_to_rewrite_documents_like_pdf_or_docx/"/>
        <updated>2023-08-07T12:40:24.000Z</updated>
        <summary type="html"><![CDATA[Hello, im in need of an ai that could rewrite for example a pdf document changing the wording but keeping the meaning of the content. 
 Right now im a user of chatgpt plus, and trying to use code interpreter for that, ive managed to get what i want, but it isnt capable of rewriting more than two pages without crashing or simply stoping the process without any warning.
 I do not know if im using the prompting in a wrong way, any help would be apreciated, also, in case theres an ai out there capable of doing this in a better way id be glad to know about it.
 Thank you guys.
    submitted by    /u/namelessgang  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dungeons & Dragons tells illustrators to stop using AI to generate artwork for fantasy franchise]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15khvly/dungeons_dragons_tells_illustrators_to_stop_using/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15khvly/dungeons_dragons_tells_illustrators_to_stop_using/"/>
        <updated>2023-08-07T11:22:19.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/SAT0725  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Text aware image generation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15khkvu/d_text_aware_image_generation/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15khkvu/d_text_aware_image_generation/"/>
        <updated>2023-08-07T11:08:35.000Z</updated>
        <summary type="html"><![CDATA[lets say i have a set of images which contains sentences of text on it. now i want to generative images using some generative model with valid (meaningful) text in them. what i assume is just using gan or more powerful diffusion to generate images but i don't think the generated images won't contains valid text in them. i want the model to implicitly learn the text in the images without feeding external text or ocr on them. does any one know any paper trying to tackle this problem. 
 any comments on this by anyone.
    submitted by    /u/specializedboy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MicrogradTS — a TypeScript version of karpathy/micrograd — a tiny scalar-valued autograd engine and a neural net on top of it]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15kfany/microgradts_a_typescript_version_of/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15kfany/microgradts_a_typescript_version_of/"/>
        <updated>2023-08-07T09:09:10.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/trekhleb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenAI - Introducing Triton: Open-source GPU programming for neural networks]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15kenut/openai_introducing_triton_opensource_gpu/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15kenut/openai_introducing_triton_opensource_gpu/"/>
        <updated>2023-08-07T08:33:57.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA's CUDA Monopoly]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15kenfr/nvidias_cuda_monopoly/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15kenfr/nvidias_cuda_monopoly/"/>
        <updated>2023-08-07T08:33:14.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scientists develop AI system to alert us of next pandemic]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15kd9nz/scientists_develop_ai_system_to_alert_us_of_next/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15kd9nz/scientists_develop_ai_system_to_alert_us_of_next/"/>
        <updated>2023-08-07T07:12:32.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/intengineering  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Albert Einstein not in black and white, but in lifelike color using AI 🤯.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15kcu79/albert_einstein_not_in_black_and_white_but_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15kcu79/albert_einstein_not_in_black_and_white_but_in/"/>
        <updated>2023-08-07T06:49:01.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/m-king473  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making LLMs hallucinate is so funny]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15kcmh4/making_llms_hallucinate_is_so_funny/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15kcmh4/making_llms_hallucinate_is_so_funny/"/>
        <updated>2023-08-07T06:36:55.000Z</updated>
        <summary type="html"><![CDATA["It looks likethisis some sortof programming syntax maybe JavaScript perhaps? Let metell ya though buddy dat aintmuch informatio todo wit. Wouldya care ta tell mesomewhat ye wanna know boot heck, might make things easier ferus botsto give yo useful responses faster innasecondsoffuture interactions brotha man :)"
 Anyone else used this site? It's through a site called nimblebox.ai, they have different models and allow you to adjust the temperature
    submitted by    /u/jordan_jpg  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Detecting thousands of overlapping organisms using latent space encoding]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15kcae2/r_detecting_thousands_of_overlapping_organisms/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15kcae2/r_detecting_thousands_of_overlapping_organisms/"/>
        <updated>2023-08-07T06:18:24.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Alonsospace  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] New library: dlt auto structures data and loads it with schema evolution in a declarative way.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15kbnsc/p_new_library_dlt_auto_structures_data_and_loads/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15kbnsc/p_new_library_dlt_auto_structures_data_and_loads/"/>
        <updated>2023-08-07T05:43:29.000Z</updated>
        <summary type="html"><![CDATA[Hey folks,
 For the past 2 years I've been working on a library to automate the most tedious part of my own work - data loading, normalisation, typing, schema creation, retries, schema inference, evolution & ddl generation, self deployment.. Basically, as you build better and better pipelines you will want more and more, and dlt supports those options.
 The value proposition of this library is to automate the tedious work you do, so you can focus on better things. 
 What's special about dlt?
 In the easiest form, you shoot response.json() json at a function and it auto manages the typing normalisation and loading, kind of like a pandas df.to_sql() but with auto schema inference, versioning and evolution. It supports loading to files, databases, and soon table formats and vector dbs.
 In its most complex form, you can do almost anything you can want, from memory management, microbatching, multithreading, extraction DAGs, 1 line Airflow/git actions deployment, dbt runner, streamlit app for data discovery, sql client, atomic state dictionaries, etc.
 The library is in use with early adopters, and we are now working on expanding our feature set to accommodate the larger community. We are adding Athena + Iceberg and Weaviate vector dbs next.
 Free forever
 The library is open source and will forever be open source. We will not gate any features for the sake of monetisation - instead we will take a more kafka/confluent approach where the eventual paid offering would be supportive not competing.
 Call for Feedback!
 Feedback is very welcome and so are requests for features or destinations.
 I would particularly love to hear from you: What destinations are you looking for from such a tool? And what use cases do you usually have? I'm a data engineer so my knowledge is more around loading external sources to a common space.
 Links
 Colab demos: Load to duckdb with schema evolution
 Docs main page 
 Thank you in advance for your feedback!
    submitted by    /u/Thinker_Assignment  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GORILLA AI: Meet the First Genuine Proximate AGI (By Microsoft)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15k8qbq/gorilla_ai_meet_the_first_genuine_proximate_agi/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15k8qbq/gorilla_ai_meet_the_first_genuine_proximate_agi/"/>
        <updated>2023-08-07T03:09:27.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/wolfdeathkill  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] AI Text Adventure Games - Narrated and Illustrated by AI]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15k7y18/p_ai_text_adventure_games_narrated_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15k7y18/p_ai_text_adventure_games_narrated_and/"/>
        <updated>2023-08-07T02:30:29.000Z</updated>
        <summary type="html"><![CDATA[https://textadventure.v5games.com/
 Hi All, I created these Text Adventure Games with AI, some help from the community which designs prompts+some avatars.
 The AI Characters can be created with an AI Art Generator. Voices and Illustrations are done using AI https://textadventure.v5games.com/ 
 Let me know what you think!
    submitted by    /u/BoxOrigi  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Microsoft partners with Meta for Llama 2 release. But why?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15k7x0j/n_microsoft_partners_with_meta_for_llama_2/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15k7x0j/n_microsoft_partners_with_meta_for_llama_2/"/>
        <updated>2023-08-07T02:29:10.000Z</updated>
        <summary type="html"><![CDATA[Staying on top of all changes, tools, and best practices with AI is getting increasingly hard. Each week I find just 1 piece of information that is most interesting across research, products, business news, and many more. No fluff guaranteed.
 Sharing the top research from this week's edition:
 https://preview.redd.it/fa3b1u39nlgb1.png?width=591&format=png&auto=webp&s=1ccd78136e3578396878fd9641605845f0309865
 Summary: Meta released their latest open-source model, Llama 2, in partnership with Microsoft’s Azure platform. But Microsoft also offers OpenAI models and is a major investor in the company (they paid $14B for 49%). So, confused Matt asks, why would Microsoft partner with Meta, when it might undermine their investment in OpenAI?
 💡 Answering the question:
  
Spreading the risk: OpenAI may have the first mover advantages, but this does not always last (e.g. Blackberry, Myspace, Yahoo). Microsoft is betting on AI but keeps the chips diversified on multiple players.
 It’s beside the point: regardless of who Microsoft supports, their game is to attract all AI utilization on Azure. It's not about the tools but about the CPU/GPU cycles they can charge for. smart!
 The real AI gangsta: Microsoft is sitting on the holy trinity of AI now.
  
 Exclusive partnerships with top LLMs (OpenAI, Meta)
 Priority access to Nvidia GPUs
 And strategic assets like GitHub and Azure
  
View tweet
 If you'd like weekly recaps like this sent to your inbox, consider subscribing to the Tomorrow Now newsletter. 😄
    submitted by    /u/TomorrowNowTech  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[🤖❤️]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15k6bc2/_/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15k6bc2/_/"/>
        <updated>2023-08-07T01:12:03.000Z</updated>
        <summary type="html"><![CDATA[Don’t believe everything you hear in the media… I learned this firsthand. This one time I accidentally went on Jessie Waters…for real 😂 
 https://youtu.be/1X31DHV0gyg?si=fU8p2D4-ShTWUdQs
 https://open.spotify.com/episode/1M6dbrrP4EoudfTUvD4BqF?si=YMBCFXYfTsmUeOXAe_-lMg
    submitted by    /u/Sonic_Improv  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeking AI Solution to Remaster My Chiptune Songs with Real Instruments, is there any?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15k5y4i/seeking_ai_solution_to_remaster_my_chiptune_songs/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15k5y4i/seeking_ai_solution_to_remaster_my_chiptune_songs/"/>
        <updated>2023-08-07T00:55:08.000Z</updated>
        <summary type="html"><![CDATA[I have these chiptune songs I made myself, and I want to know if there is any AI that can remaster them with real instruments, etc., like an old 8-bit video game song that is updated to a modern version in a remake. Is any already AI capable of doing that?
    submitted by    /u/Severo_  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P]:Question]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15k2bb0/pquestion/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15k2bb0/pquestion/"/>
        <updated>2023-08-06T22:14:20.000Z</updated>
        <summary type="html"><![CDATA[Hello I am attempting to reduce a matrix that is 57 by 256 to 57 to 128. I was attempting to use PCA but it failed as maximum size would be 57 by 57. I was also attempting an autoencoder but the syntax behind this is very confusing so If anyone could give me adivce that would be great. Thank you 
    submitted by    /u/amayorgafcw  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Rust meets Llama2: OpenAI compatible API written in Rust]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15k254o/p_rust_meets_llama2_openai_compatible_api_written/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15k254o/p_rust_meets_llama2_openai_compatible_api_written/"/>
        <updated>2023-08-06T22:06:55.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 I have been working on an OpenAI-compatible API for serving LLAMA-2 models written entirely in Rust. It supports offloading computation to Nvidia GPU and Metal acceleration for GGML models !
 Here is the project link: Cria- Local LLAMA2 API
 You can use it as an OpenAI replacement (check out the included `Langchain` example in the project).
 This is an ongoing project, I have implemented the `embeddings` and `completions` routes. The `chat-completion` route will be here very soon!
 Really interested in your feedback and I would welcome any help :) !
 ​
 ​
    submitted by    /u/amindiro  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] AI-Crafted Daily Digest: Exploring Latest ML Developments]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15k0rz2/p_aicrafted_daily_digest_exploring_latest_ml/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15k0rz2/p_aicrafted_daily_digest_exploring_latest_ml/"/>
        <updated>2023-08-06T21:11:03.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/eusben  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Triple Threat: The Power of Transcription, Summary, and Translation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15k0p9j/p_triple_threat_the_power_of_transcription/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15k0p9j/p_triple_threat_the_power_of_transcription/"/>
        <updated>2023-08-06T21:08:01.000Z</updated>
        <summary type="html"><![CDATA[Open source Audio pipeline for transcription, translation and summarization.
 Check out our demo page to generate your own transcription, summary, and translation, or use our browser extension to get live transcriptions.
    submitted by    /u/eusben  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Comprehensive learning resources that emphasize DEEP reinforcement learning?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15k0er6/d_comprehensive_learning_resources_that_emphasize/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15k0er6/d_comprehensive_learning_resources_that_emphasize/"/>
        <updated>2023-08-06T20:56:27.000Z</updated>
        <summary type="html"><![CDATA[So I understand that there is the Sutton & Barto book on reinforcement learning in the sidebar. I was wondering what other resources you guys have used that you would recommend that emphasize deep reinforcement learning for someone with some experience in shallow/classical reinforcement learning already and some experience with deep learning already, but new to deep reinforcement learning
    submitted by    /u/BornAgain20Fifteen  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Comprehensive learning resources that emphasize DEEP reinforcement learning?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15k0edf/d_comprehensive_learning_resources_that_emphasize/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15k0edf/d_comprehensive_learning_resources_that_emphasize/"/>
        <updated>2023-08-06T20:56:02.000Z</updated>
        <summary type="html"><![CDATA[So I understand that there is the Sutton & Barto book on reinforcement learning in the sidebar. I was wondering what other resources you guys have used that you would recommend that emphasize deep reinforcement learning for someone with some experience in shallow/classical reinforcement learning already and some experience with deep learning already, but new to deep reinforcement learning
    submitted by    /u/BornAgain20Fifteen  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to predict long sequences of events to optimize sales?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15jzg1x/d_how_to_predict_long_sequences_of_events_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15jzg1x/d_how_to_predict_long_sequences_of_events_to/"/>
        <updated>2023-08-06T20:17:35.000Z</updated>
        <summary type="html"><![CDATA[Hey! 
 I am working on a project to predict the best sequences of marketing channel so that sales is maximized. I have 20 ways of reaching out to the customer (email, phone, face2face...). I have 20 days of interaction history and it's generated sales, recorded for past 2 years. I have to predict for the next 20 working days(1 month)
 So far, I have tried ensemble methods, svm, fully connected nn, etc. But it is quite apparent that these are not good solutions. 
 Any suggestions on ml/dl methods? Papers, blogs or other resources would be much appreciated
    submitted by    /u/TUSH11235  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pioneering AI Democracy: Introducing a Decentralized and Merit-Based Governance System for Large Language Models like ChatGPT (proposed to OpenAI)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15jz0ag/pioneering_ai_democracy_introducing_a/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15jz0ag/pioneering_ai_democracy_introducing_a/"/>
        <updated>2023-08-06T20:00:44.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/CreepToCrypto  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to build websites that use AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15jy7vv/how_to_build_websites_that_use_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15jy7vv/how_to_build_websites_that_use_ai/"/>
        <updated>2023-08-06T19:29:07.000Z</updated>
        <summary type="html"><![CDATA[Web dev student here and I'm interested in knowing more about creating products that actually use AI to help its users (not products that just use GPT in the backend). More specifically, I want to build a food supply management app for restaurants for my school thesis. This app will use AI to analyse food supplies and assign them purchase priority, value, and complexity scores (maybe just priority if it's too hard). Restaurant owners could then determine what foods should be purchased before others based on the priority scores.
 For example, a restaurant may only have 10 tomatoes left and the average usage of tomatoes in this restaurant is 12 per week. Based on this, a priority would be assigned to purchase x amount of tomatoes.
 Other factors that could be taken into account for the prior…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Could someone help me understand what is going on with my agent in this environment?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15jxbpk/could_someone_help_me_understand_what_is_going_on/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15jxbpk/could_someone_help_me_understand_what_is_going_on/"/>
        <updated>2023-08-06T18:52:40.000Z</updated>
        <summary type="html"><![CDATA[https://imgur.com/WR0Tny9
 My agent needs to learn to take one action in my environment and there are only two possible actions that the agent can take at each time step. The state is just the time step, so every episode has 240 time steps and the agent just needs to learn to take one optimal action out of two possible actions for every time step. I have set this up as simply as I can as a starting point to make sure the algorithm is implemented correctly and that the agent can learn. I am using n-step expected SARSA.
 The bottom plot shows the count for how many times the agent took each action during each episode. The middle plot has the temporal difference error in blue and the "modelling error" in orange. The modelling error is the difference between the actual discounted return and the TD target for each time step, summed up for each episode. The red line is the return that the agent would get if it took the optimal action in every time step.
 0.11, the blue line in the bottom plot, is the optimal action for the agent to take at every time step. The other action will never result in a reward other than 0. So it should be fairly simple for the agent to learn what action to take at every time step and it does learn that at the start. But then, as you can see in the top plot, the agent suddenly starts taking the non-optimal action more often after around episode 450. So I'm just wondering why that would happen. Why would the agent learn to take the optimal action at most time steps and then suddenly decide that it will start taking other actions?
 For more context, the learning rate is 0.6, n is 6, epsilon is decayed by 1/(n_episodes/1.1) every episode so it reaches 0 slightly before the final episode.
 Any ideas based on this information why the agent would decide to start taking the non-optimal action? Or any suggestions for how I could figure out why it would start taking the non-optimal action?
    submitted by    /u/lifelifebalance  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Swish function and a Swiss mathematician]]></title>
        <id>https://www.johndcook.com/blog/?p=201882</id>
        <link href="https://www.johndcook.com/blog/2023/08/06/swish-swiss/"/>
        <updated>2023-08-06T17:24:24.000Z</updated>
        <summary type="html"><![CDATA[The previous post looked at the swish function and related activation functions for deep neural networks designed to address the “dying ReLU problem.” Unlike many activation functions, the function f(x) is not monotone but has a minimum near x0 = -1.2784. The exact location of the minimum is where W is the Lambert W function, […]
Swish function and a Swiss mathematician first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15juvsd/runtimeerror_trying_to_backward_through_the_graph/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15juvsd/runtimeerror_trying_to_backward_through_the_graph/"/>
        <updated>2023-08-06T17:13:32.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Think_Huckleberry299  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Swish, mish, and serf]]></title>
        <id>https://www.johndcook.com/blog/?p=201867</id>
        <link href="https://www.johndcook.com/blog/2023/08/06/swish-mish-and-serf/"/>
        <updated>2023-08-06T16:50:23.000Z</updated>
        <summary type="html"><![CDATA[Swish, mish, and serf are neural net activation functions. The names are fun to say, but more importantly the functions have been shown to improve neural network performance by solving the “dying ReLU problem.” Softplus can also be used as an activation function, but our interest in softplus here is as part of the definition […]
Swish, mish, and serf first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI/ML Best Practices During a Gold Rush [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15jub9s/aiml_best_practices_during_a_gold_rush_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15jub9s/aiml_best_practices_during_a_gold_rush_d/"/>
        <updated>2023-08-06T16:49:59.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/swodtke  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TarMAC: Targeted Multi-Agent Communication]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15ju299/tarmac_targeted_multiagent_communication/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15ju299/tarmac_targeted_multiagent_communication/"/>
        <updated>2023-08-06T16:39:16.000Z</updated>
        <summary type="html"><![CDATA[Does anyone know code implementations for TarMAC: Targeted Multi-Agent Communication?
    submitted by    /u/tessherelurkingnow  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Any good AI tools paid or free I can use to help me post some text data on a website?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15jtg3b/any_good_ai_tools_paid_or_free_i_can_use_to_help/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15jtg3b/any_good_ai_tools_paid_or_free_i_can_use_to_help/"/>
        <updated>2023-08-06T16:13:06.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone 
 Basically i just need to post some text into one website everyday for my work 
 The problem is there are many steps involves to post one data value, I was wondering if there is a tool that can learn my tasks and then post some of the data to the website from google sheets? 
 I'm open to any suggestions and advice. 
 Thanks in advance.
 ​
    submitted by    /u/Maxduel  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Looking for Perspectives: Pursuing a PhD in AI vs Continuing in Industry]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15jr6be/r_looking_for_perspectives_pursuing_a_phd_in_ai/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15jr6be/r_looking_for_perspectives_pursuing_a_phd_in_ai/"/>
        <updated>2023-08-06T14:36:43.000Z</updated>
        <summary type="html"><![CDATA[Greetings fellow researchers,
 I am 27, currently working remotely at a healthcare IT company based in Silicon Valley (6+ years in industrial research) where I apply deep learning methods and large language models. I recently received an exciting opportunity to pursue a PhD at the Technical University of Denmark (DTU) in a similar research area. 
 While I am grateful for my current position and compensation, Have published in NeurIPS, EMNLP, ACL, ACM etc (NLP) with really good citations under company. I feel unsatisfied with the learning opportunities available in company & industry.
 I am strongly considering pursuing the DTU PhD program full-time, but wanted to get perspectives from others before making a decision. How strong is DTU's AI research community?
 Given the rapid advances in large language models, is now an ideal time to immerse myself in academic research? There are many topics that interest me, including fairness, ethics, hallucinations, quantization, specialized domains like healthcare/finance, and federated learning combined with LLMs.
 Would appreciate any insights on whether moving into academia would be a wise choice at this stage versus remaining in industry. I welcome any suggestions or considerations I should keep in mind. 
 Thank you for taking the time to share your thoughts!
    submitted by    /u/Traditional-Poet2746  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Networks FROM SCRATCH | Deep Learning tutorial Part 1]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15jqysx/neural_networks_from_scratch_deep_learning/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15jqysx/neural_networks_from_scratch_deep_learning/"/>
        <updated>2023-08-06T14:27:38.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/AeroArtz  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Free AI TTS Text to speech available?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15jqw4z/free_ai_tts_text_to_speech_available/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15jqw4z/free_ai_tts_text_to_speech_available/"/>
        <updated>2023-08-06T14:24:13.000Z</updated>
        <summary type="html"><![CDATA[I want to convert a few books into audiobooks. Are there any AI options out there that are free and will give me something I can use offline? I typically listen to books on my phone while I'm out, so something like Edge browser isn't going to work.
 I've heard that there are some great options, but I've only seen some web paid services, and for my purpose, it's too expensive just to get an audiobook out of it. This is all just for personal use.
    submitted by    /u/UUkiee  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Generative Language Model (GRU) learns constant representation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15jpk6l/p_generative_language_model_gru_learns_constant/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15jpk6l/p_generative_language_model_gru_learns_constant/"/>
        <updated>2023-08-06T13:23:12.000Z</updated>
        <summary type="html"><![CDATA[Context
 I'm working on an RNN-based model that should learn how to guess the next character given a simple prompt based on all scripts from Friends to generate non-existing Friends dialogue. It is heavily inspired by Andrej Karpathy's blog post on RNN's. I'm mostly doing this for training, and because it's pretty fun.
 I have a little experience with deep learning in the sense that I am familiar with most common architectures and have intermediate understanding of how deep learning models work and are trained. I haven't created many models from scratch though, yet.
 Network
 My GRU is fairly simple. I'll save you the exact code, but instead give a systematic overview of all network layers. It's implemented with Pytorch:
 INPUT: sequence of integers representing a symbol based on mapping e…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Today the source code button is gone...]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15jovx4/d_today_the_source_code_button_is_gone/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15jovx4/d_today_the_source_code_button_is_gone/"/>
        <updated>2023-08-06T12:50:51.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Better-Process5239  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrating GenAI into “Thinking Like a Data Scientist” Methodology – Part I]]></title>
        <id>https://www.datasciencecentral.com/?p=62819</id>
        <link href="https://www.datasciencecentral.com/integrating-genai-into-thinking-like-a-data-scientist-methodology-part-i/"/>
        <updated>2023-08-06T12:16:23.000Z</updated>
        <summary type="html"><![CDATA[It’s incredible how many organizations utilize Generative AI (GenAI) and Large Language Models (LLMs) to enhance their information assembly, integration, and application abilities. These GenAI technologies have been applied in various areas, from drafting legal documents and resolving service issues to coding software applications and (er, um) writing blog posts. The potential uses of GenAI… Read More »Integrating GenAI into “Thinking Like a Data Scientist” Methodology – Part I
The post Integrating GenAI into “Thinking Like a Data Scientist” Methodology – Part I appeared first on Data Science Central.]]></summary>
        <author>
            <name>Bill Schmarzo</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[In the game Superintelligence, you play as an AI trying dominate the planet. [Fictional game concept]]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15jn68p/in_the_game_superintelligence_you_play_as_an_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15jn68p/in_the_game_superintelligence_you_play_as_an_ai/"/>
        <updated>2023-08-06T11:20:08.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Philipp  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Underlining detection algorithm?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15jlomu/p_underlining_detection_algorithm/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15jlomu/p_underlining_detection_algorithm/"/>
        <updated>2023-08-06T09:50:18.000Z</updated>
        <summary type="html"><![CDATA[Hey.
 I'm currently working on an application that digitalizes text from physical book pages using Google's Cloud Vision API.
 I'm looking to add a functionality that can recognize and highlight underlined words within the scanned pages. I initially thought this would be a common feature and expected to find existing open-source solutions or libraries that I could use. To my surprise, I've been unable to find any. 
 I am just really bad at finding it, or is this not as straightforward as I initially thought?
    submitted by    /u/pangu2  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Computer Vision News of August 2023 with AI, CV, DL and ML]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15jjfkj/n_computer_vision_news_of_august_2023_with_ai_cv/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15jjfkj/n_computer_vision_news_of_august_2023_with_ai_cv/"/>
        <updated>2023-08-06T07:33:40.000Z</updated>
        <summary type="html"><![CDATA[Dear all,
 Here is Computer Vision News of August 2023.
 Read 44 pages about AI, Deep Learning, Computer Vision and more!
 Online version (recommended)
 PDF version
 Free subscription on page 44.
 Enjoy!
 https://preview.redd.it/e143wha20ggb1.jpg?width=794&format=pjpg&auto=webp&s=14a699f80f4b2de94addc8242e8978d3e185309f
    submitted by    /u/Gletta  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mass-Editing Memory in a Transformer]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15jht4h/massediting_memory_in_a_transformer/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15jht4h/massediting_memory_in_a_transformer/"/>
        <updated>2023-08-06T05:58:28.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Fine tuning or semantic search with a vector database?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15jhfk7/d_fine_tuning_or_semantic_search_with_a_vector/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15jhfk7/d_fine_tuning_or_semantic_search_with_a_vector/"/>
        <updated>2023-08-06T05:36:43.000Z</updated>
        <summary type="html"><![CDATA[Experts, I am a beginner here and seeking some advise here please.
 I am have compiled a high quality Q&A dataset (around 1200 entries) for a domain specific topic.
 What's the best course of action here to use LLM with that specific knowledge base?
 1) Finetuning a model? if so which one is a good candidate? OpenAI let's me finetune some models and later, all my users have to do is use pass the model name to the API
 2) Use the regular vector database + embeddings for augmented retrieval
 
 I prefer (1) but I am not sure how it will perform.
 Option (2) should work, since we really just use semantic search to bring in context to the LLM, etc.
 I hope you can say that (1) works nicely, if not please help me learn why.
 Thank you in advance!
    submitted by    /u/entered_apprentice  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLM related pytorch code [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15jeycg/llm_related_pytorch_code_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15jeycg/llm_related_pytorch_code_d/"/>
        <updated>2023-08-06T03:21:26.000Z</updated>
        <summary type="html"><![CDATA[Where to find LLM related pytorch code with code explanations?
    submitted by    /u/thorin_olamadal  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How does one withdraw a paper from Neurips?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15jd1wu/d_how_does_one_withdraw_a_paper_from_neurips/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15jd1wu/d_how_does_one_withdraw_a_paper_from_neurips/"/>
        <updated>2023-08-06T01:44:20.000Z</updated>
        <summary type="html"><![CDATA[First time submitter here and was unable to find a similar post (and thought the community might benefit from this in the future!). How do I withdraw from Neurips? All the instructions I found are from 2017, 2018. Do I need to contact someone or do I just need to "Add Withdrawal" on OpenReview.
    submitted by    /u/Dramatic-Gap-4681  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Why have separate stages for RPN (proposal generation) and ROI (refinement)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15jb52o/d_why_have_separate_stages_for_rpn_proposal/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15jb52o/d_why_have_separate_stages_for_rpn_proposal/"/>
        <updated>2023-08-06T00:13:19.000Z</updated>
        <summary type="html"><![CDATA[Just what the title says.
 Also is this (splitting prediction into 2 stages) a prominent paradigm in other areas of ML too? I am reading about something called the "Action Transformer" created by Adept AI, and it also has 2 stages: instruction generation and code generation.
    submitted by    /u/FloatingDelusion  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do you think we will hit a point of “Robocop” in the next 50 years? A Human + Cybernetic Hybrid police force]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15j9bwq/do_you_think_we_will_hit_a_point_of_robocop_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15j9bwq/do_you_think_we_will_hit_a_point_of_robocop_in/"/>
        <updated>2023-08-05T22:52:45.000Z</updated>
        <summary type="html"><![CDATA[The movie that came out in the 80s is a great flick for it’s time. Do you guys think we will ever experience a sort of unstoppable super soldier when it comes to our police / swat forces ? We are replacing many jobs with robots. From surgery procedures in hospitals to flipping burgers. It’s not above the realm of possibility to think we may someday soon see a hybrid police force. What do you guys think ?
    submitted by    /u/2bJavazon  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Microsoft’s AI Watched 100,000,000 Youtube Videos! text input to video and sound]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15j8zs2/microsofts_ai_watched_100000000_youtube_videos/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15j8zs2/microsofts_ai_watched_100000000_youtube_videos/"/>
        <updated>2023-08-05T22:38:11.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/keghn  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What AI TTS software/voice is this video using?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15j8csn/what_ai_tts_softwarevoice_is_this_video_using/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15j8csn/what_ai_tts_softwarevoice_is_this_video_using/"/>
        <updated>2023-08-05T22:11:13.000Z</updated>
        <summary type="html"><![CDATA[It's commonly used on tiktok for reddit narration story videos, here is an example: https://www.tiktok.com/@creekyadvice/video/7263509593488166186. Anyone have any idea? 
    submitted by    /u/DanielTube7  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linguistics > NPL career?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15j7dcx/linguistics_npl_career/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15j7dcx/linguistics_npl_career/"/>
        <updated>2023-08-05T21:31:59.000Z</updated>
        <summary type="html"><![CDATA[I am a linguist, translator, and copy editor looking to move my career into natural language processing instead. I have no computer science background. What would you suggest as some steps to take, both now and in the future, as I plan out my career? It looks like I am going to need to learn Python, but I'm not 100% sure, and there's so little established in such a new field.
    submitted by    /u/StrangersWithAndi  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating and inspecting an RSA private key]]></title>
        <id>https://www.johndcook.com/blog/?p=201814</id>
        <link href="https://www.johndcook.com/blog/2023/08/05/rsa-private-key/"/>
        <updated>2023-08-05T21:06:15.000Z</updated>
        <summary type="html"><![CDATA[In principle you generate an RSA key by finding two large prime numbers, p and q, and computing n = pq. You could, for example, generate random numbers by rolling dice, then type the numbers into Mathematica to test each for primaility until you find a couple prime numbers of the right size. In practice […]
Generating and inspecting an RSA private key first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to Mathematically Prove that a Neural Network is Converging Faster]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15j5zw6/d_how_to_mathematically_prove_that_a_neural/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15j5zw6/d_how_to_mathematically_prove_that_a_neural/"/>
        <updated>2023-08-05T20:35:49.000Z</updated>
        <summary type="html"><![CDATA[Hello r/MachineLearning!
 I'm working on understanding how a neural network converges and wish to approach this mathematically. Can anyone recommend resources, papers, or tools that could assist me in proving this?
 Thank you in advance for your help! 
 Edit: Removed converges faster to remove ambiguity
    submitted by    /u/abystoma  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Transformer for realtime action recognition]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15j4tyn/d_transformer_for_realtime_action_recognition/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15j4tyn/d_transformer_for_realtime_action_recognition/"/>
        <updated>2023-08-05T19:48:08.000Z</updated>
        <summary type="html"><![CDATA[Do you aware of any work for realtime action recognition that use transformer? This is different with conventional transformer in a sense that we don’t have access to future information, so how do we change the training strategy? Also, it’s inefficient if we use the entire history; are there any smart way to select which frame in the past to keep?
    submitted by    /u/Ok_Influence505  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Giving AI unlimited access to the internet by web browser]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15j4tvd/giving_ai_unlimited_access_to_the_internet_by_web/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15j4tvd/giving_ai_unlimited_access_to_the_internet_by_web/"/>
        <updated>2023-08-05T19:48:01.000Z</updated>
        <summary type="html"><![CDATA[Interesting experiment i thought of.
 What if we gave AI access to web browser, and let it do whatever it wants? It could create accounts on any social media, email accounts, ad comments everywhere and such.
 of course, ai by itself does not have any agenda or need to do anything, so ai would need to be fed some kind of personality simulation first. Lets say ai was either fed personality based on extensive twitter or reddit history of someone's post. Using that, basic psychological traits, beliefs and maybe goals could be determined.
 Such ai would simulate person sitting in front of pc, so it would need to parse the content of webpages, but i don't think it would be that of a problem. And it would maybe also have access to some bank account with some money to maybe pay for online subscriptions and such. But who knows, maybe thanks to simulating someone's personality, it would attempt to donate money to some charity or lose it on onlyfans?
    submitted by    /u/rogaldorn88888  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I just published “Safe For Humans AI” – free to read online]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15j4ml7/i_just_published_safe_for_humans_ai_free_to_read/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15j4ml7/i_just_published_safe_for_humans_ai_free_to_read/"/>
        <updated>2023-08-05T19:39:45.000Z</updated>
        <summary type="html"><![CDATA[I just published “Safe For Humans AI” – free to read online
 https://leanpub.com/safe-for-humans-AI/read
 Free to read online, and eBook versions released under a Creative Commons License (no commercial reuse, feel free to share).
 The full title of my short book is:
 Safe For Humans AI
 A "humans-first" approach to designing and building AI systems.
    submitted by    /u/MWatson  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[[P] Vectara+ Flowise]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15j244e/p_vectara_flowise/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15j244e/p_vectara_flowise/"/>
        <updated>2023-08-05T17:55:28.000Z</updated>
        <summary type="html"><![CDATA[u/Vectara is now integrated with r/flowise, so you can easily build no-code GenAI Apps at scale.
 Check out the video here: https://twitter.com/ofermend/status/1687138158692196352
 You can sign up for a free vectara.com account to get started.
    submitted by    /u/ofermend  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 8/5/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15j1xjm/oneminute_daily_ai_news_852023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15j1xjm/oneminute_daily_ai_news_852023/"/>
        <updated>2023-08-05T17:47:48.000Z</updated>
        <summary type="html"><![CDATA[While some schools have curbed the use of generative AI, the University of Hong Kong (HKU) is going all in and urging both its teachers and students to embrace the technology. The University of Hong Kong is supporting this by giving teachers and students free access to various generative AI tools, including Microsoft Azure OpenAI and OpenAI’s ChatGPT and DALL-E.[1]
 Intel’s CEO, Pat Gelsinger, has called NVIDIA the clear market leader who has done a great job within the AI space.[2]
 AI powerhouse, OpenAI has released some new features for its sensational chatbot, ChatGPT. The new features allow the chatbot to show suggested follow-up prompts at the bottom of its responses. The new features were announced by the company via a tweet on its official Twitter handle.[3]
 Asian Americans and women in the workforce are the most concentrated in fields where AI could assist or replace their job tasks, according to new research.[4]
  
BushAICave.com
 Sources:
 [1] https://www.zdnet.com/article/another-major-university-is-supporting-generative-ai-use-but-with-serious-guardrails/
 [2] https://wccftech.com/intel-ceo-acknowledges-nvidia-as-ai-market-leader-says-they-have-done-a-good-job/
 [3] https://indianexpress.com/article/technology/artificial-intelligence/chatgpt-gets-new-updates-heres-how-they-enhance-user-experience-8877847/
 [4] https://www.nbcnews.com/news/asian-america/asian-american-workers-heavily-affected-ai-rcna98179 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is this AI - The I?😂]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15j1nft/is_this_ai_the_i/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15j1nft/is_this_ai_the_i/"/>
        <updated>2023-08-05T17:36:01.000Z</updated>
        <summary type="html"><![CDATA[And if so, how has this account lasted 2 years on reddit? 🤔
    submitted by    /u/TheHeirOfElendil  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Custom Tokenizers - Optimization Opportunity or Waste of Time? [D], [R}]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15j11k1/custom_tokenizers_optimization_opportunity_or/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15j11k1/custom_tokenizers_optimization_opportunity_or/"/>
        <updated>2023-08-05T17:10:18.000Z</updated>
        <summary type="html"><![CDATA[I've recently started to explore the possibility of working with custom tokenizers. I will preface this by saying I'm not a tokenizer guy. I just don't know that much about their construction. I understand how they work, but I'm probably behind the latest developments in tokenizers.
 So, I thought it wisest to reach out to the community for advice or clarity.
 Context:
 I've collected about 15 GB of data over the last month. It's incredibly clean and well-organized. The core goal of the data is to train a model to solve or assist with a particular development problem. This means that much of my data is a code/natural language mix. It's delimited clearly, and the formatting is uniform. The entire dataset has been normalized and standardized. It's taken me a lot of time to produce and that's…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] The Quest to Have Endless Conversations with Llama and ChatGPT 🗣️💬]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15j0xwt/r_the_quest_to_have_endless_conversations_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15j0xwt/r_the_quest_to_have_endless_conversations_with/"/>
        <updated>2023-08-05T17:06:01.000Z</updated>
        <summary type="html"><![CDATA[​
 https://preview.redd.it/mbkb10icqbgb1.png?width=1400&format=png&auto=webp&s=7a15423060ddfeffe4651340bcc6fd7cf36dde10
 I started a blog post series about the limitations of language models for dealing with long texts.
 Feedback is welcome!
    submitted by    /u/JClub  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why isn't there a SARSA equivalent that uses value functions?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15j06hu/why_isnt_there_a_sarsa_equivalent_that_uses_value/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15j06hu/why_isnt_there_a_sarsa_equivalent_that_uses_value/"/>
        <updated>2023-08-05T16:34:13.000Z</updated>
        <summary type="html"><![CDATA[SARSA is a TD algorithm for control (learning optimal policies). In the book it's written like this: image. The idea is to learn the action-value function instead of the value function for a policy that we keep improving (using GPI). Once we learn the converged action-value function for all states, the optimal policy is greedily derived from the action-value function (basically take the most promising action at each state).
 In contrast, TD for value estimation is written like this: image. Here we keep the policy fixed and just keep iterating over the multiple episodes, whilst refining the value estimate.
 My question is, why can't we just change TD for value estimation to just greedily update the policy at each stage? That would be in the spirit of generalized policy iteration (GPI) too. In other words, a version of SARSA which doesn't use action-value functions, but instead use value functions?
    submitted by    /u/AstronautVarious3791  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Energy efficiency of data centers versus consumer-grade setups for training and inference of LLMs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15izni1/d_energy_efficiency_of_data_centers_versus/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15izni1/d_energy_efficiency_of_data_centers_versus/"/>
        <updated>2023-08-05T16:12:32.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 With the recent boom of LLMs, we have seen both ends of the spectrum advance at a very fast pace, from OpenAI GPT4, which runs on huge data centers operated by Azure, to llama.cpp, which runs on consumer laptops.
 While both have their pros and cons, for instance, open-source models on decentralized compute reduce the need to trust or rely on centralized actors like Cloud providers, the efficiency of running training/inference on personal setups is not often discussed.
 I am therefore interested in learning how more energy/cost efficient it is to train/serve AI models on data centers vs doing it on personal computers. 
 Do you know if there have been studies? 
 In theory, I guess that several factors, such as economies of scale, use of renewable energy sources in some data centers, such as Canada, advanced cooling systems and advanced hardware, make data centers more cost/energy efficient.
 I guess some modeling on a precise use case where we fix some variables could help have an idea. For instance, one could ask, what is the energy/cost/time needed to predict 1 billion tokens from a Llama 2 70B in a data center with X amount of A100s, vs on Y different consumer CPU / GPUs.
 If anyone has references to models or past studies I would be quite interested. Of course, using data centers implies trusting those people, but I am not considering that factor for this discussion as I am focusing on understanding best what is the best setup to have optimal enrgy/cost/time for AI.
    submitted by    /u/Separate-Still3770  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Nvidia GPU shortage is ‘top gossip’ of Silicon Valley]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15iz6v7/d_nvidia_gpu_shortage_is_top_gossip_of_silicon/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15iz6v7/d_nvidia_gpu_shortage_is_top_gossip_of_silicon/"/>
        <updated>2023-08-05T15:53:59.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/norcalnatv  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-Generated Horror trailer – "The Phoenix"]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15iz2dp/aigenerated_horror_trailer_the_phoenix/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15iz2dp/aigenerated_horror_trailer_the_phoenix/"/>
        <updated>2023-08-05T15:48:46.000Z</updated>
        <summary type="html"><![CDATA[I’m a filmmaker and I’m just experimenting with AI. I just had fun crafting a film trailer to understand the today’s limits of these tools. I used Midjourney, Runway Gen-2, StableDiffusion, Premiere, After Effects. The movie it's called "The Phoenix", which hints at the film's underlying theme of rising from the ashes, symbolizing female empowerment, all wrapped in a bit of sarcastic humor from a male perspective. 
 I'm sharing because I genuinely want to know what you guys think. Any and all thoughts are welcome.
 If you're curious about the workflow or the process behind the creation of this trailer, I'd be happy to share more.
 The Phoenix - She rises from the ashes
    submitted by    /u/Lrnz_reddit  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ICCV Challenge on Geographical Domain Adaptation [R]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15iyyyp/iccv_challenge_on_geographical_domain_adaptation_r/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15iyyyp/iccv_challenge_on_geographical_domain_adaptation_r/"/>
        <updated>2023-08-05T15:44:45.000Z</updated>
        <summary type="html"><![CDATA[As part of ICCV 2023 in Paris, this year we are organizing a challenge on solving domain gaps that occur when computer vision models are transferred across geographical locations. The challenge covers three tracks in unsupervised scene adaptation, image adaptation and universal adaptation. The challenge is open to everyone, with attractive prizes for the winners. Check it out at the following links!
 Challenge Rules and Guidelines: https://geonet-challenge.github.io/ICCV2023/challenge.html
 Challenge Registration: https://forms.gle/zSZA1iaPD3mZxjyn7
 Code and baselines: https://github.com/ViLab-UCSD/GeoNet
 The training data for the challenge is already available, and the test data will be released to the registered participants.
    submitted by    /u/GeoNetICCV2023  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RSA encryption in practice]]></title>
        <id>https://www.johndcook.com/blog/?p=201771</id>
        <link href="https://www.johndcook.com/blog/2023/08/05/rsa-oaep/"/>
        <updated>2023-08-05T15:22:01.000Z</updated>
        <summary type="html"><![CDATA[At its core, RSA encryption is modular exponentiation. That is, given a message m, the encrypted form of m is x = me mod n where e is a publicly known exponent and n is a product of two large primes. The number n is made public but only the holder of the private key […]
RSA encryption in practice first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Part 0 of my last post on here. Used CloneAI. Music by me.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15ixgdi/part_0_of_my_last_post_on_here_used_cloneai_music/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15ixgdi/part_0_of_my_last_post_on_here_used_cloneai_music/"/>
        <updated>2023-08-05T14:42:22.000Z</updated>
        <summary type="html"><![CDATA[Links in my bio for more content like this!
    submitted by    /u/No_Understanding162  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ai generative fill]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15iwmqv/ai_generative_fill/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15iwmqv/ai_generative_fill/"/>
        <updated>2023-08-05T14:07:24.000Z</updated>
        <summary type="html"><![CDATA[Hello there I'm curious what the user guidelines and restrictions are for the Adobe ai generative fill is and if there are possible better more higher quality and less restricted ones out there.
    submitted by    /u/Team_Sonic_Gaming  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] MechDesigner Assistant AI: Future Engineers. Looking for communities, groups etc to exchange ideas, experience]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15iwdgs/p_mechdesigner_assistant_ai_future_engineers/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15iwdgs/p_mechdesigner_assistant_ai_future_engineers/"/>
        <updated>2023-08-05T13:56:26.000Z</updated>
        <summary type="html"><![CDATA[Hi guys
 Im looking for groups or communities where i could discuss about certain topic. Im a software developer and a mechanical engineer and recently made an app that combines gpt4 model to perform engineering tasks like CAD models creation and performing stress analysis. I would like find people who share the same passion and perhaps would like to discuss about that, exchange the concepts, ideas and visions. Im getting to the point where i will need to implement own trained model and im no ML expert so would be great to discuss about the architecture etc. 
 Here is a demo of my app
 MechDesigner Assistant AI: Future Engineers
 Best regards
 Pyotr
    submitted by    /u/pyotr_vozniak  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Code to convert words to Major system numbers]]></title>
        <id>https://www.johndcook.com/blog/?p=201750</id>
        <link href="https://www.johndcook.com/blog/2023/08/05/word-to-number/"/>
        <updated>2023-08-05T13:48:11.000Z</updated>
        <summary type="html"><![CDATA[A few days ago I wrote about using the CMU Pronouncing Dictionary to search for words that decode to certain numbers in the Major mnemonic system. You can find a brief description of the Major system in that post. As large as the CMU dictionary is, it did not contain words mapping to some three-digit […]
Code to convert words to Major system numbers first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Drum Kick Generation app]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15iv2tj/p_drum_kick_generation_app/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15iv2tj/p_drum_kick_generation_app/"/>
        <updated>2023-08-05T12:58:48.000Z</updated>
        <summary type="html"><![CDATA[Hi, I am a new starter with ML apps and want to build a first app preferably using existing (trained) models. The idea is an app that takes a text description of a wished kick drum (for example: create a 808 kick with enhanced subs and filtered above 15kHz) and then generates a corresponding hifi sample of the description (44,1k or 48k). 
 I would like to learn how to do that with some peers happy to help me.
 As said this would be my first attempt. About me: I only followed Deep Learning theoretical courses from Andrew Ng and never built or used existing models so I'd appreciate some guidance if you are interested to support. Thanks a lot 
    submitted by    /u/freeabt19  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Human Biological and Spiking Neural Networks. A Literature Review of Recent BNN and SNN Advances)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15iunr9/d_human_biological_and_spiking_neural_networks_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15iunr9/d_human_biological_and_spiking_neural_networks_a/"/>
        <updated>2023-08-05T12:38:31.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Impressive-Ad-8964  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Nerf.jl a Real-Time Neural 3D Scene Reconstruction in Pure Julia | Anton Smirnov | JuliaCon 2023]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15iug4w/p_nerfjl_a_realtime_neural_3d_scene/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15iug4w/p_nerfjl_a_realtime_neural_3d_scene/"/>
        <updated>2023-08-05T12:28:12.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Fincho64  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]How do you usually deal with multimodal target variable?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15itmy9/dhow_do_you_usually_deal_with_multimodal_target/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15itmy9/dhow_do_you_usually_deal_with_multimodal_target/"/>
        <updated>2023-08-05T11:47:42.000Z</updated>
        <summary type="html"><![CDATA[Popular machine model techniques such as LightGBM and XGBoost output predictions that are unimodally distributed(only one hump) but seem to beat other models specialized to deal with multimodal data. Or am I just wrong? 
 It just doesnt look right.
 https://preview.redd.it/6xrd7hgm4agb1.png?width=1000&format=png&auto=webp&s=a4518549f609c6436af410ae87a0c6a24cff6ea7
    submitted by    /u/runawaychicken  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Transformer implementation - help]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15itj9s/d_transformer_implementation_help/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15itj9s/d_transformer_implementation_help/"/>
        <updated>2023-08-05T11:42:16.000Z</updated>
        <summary type="html"><![CDATA[Hey I've tried to implement the transformer architecture on my own to understand it better. The outputs look fine (I'm only looking at shapes) and I wanted to know if it's right firstly, and if there is anyway to implement it in a more efficient way.
 Code -
 import torch import torch.nn as nn class MultiHeadSelfAttention(nn.Module): def __init__(self, nheads=8, dim=512, bias=True, dropout=0.2): super().__init__() assert dim % nheads == 0, "dimension must be divisible by number of heads" self.nheads = nheads self.dim = dim self.head_dim = dim // nheads self.scale = self.head_dim**-0.5 self.softmax = nn.Softmax(dim=-1) self.dropout = nn.Dropout(dropout) self.to_keys = nn.Linear(dim, self.dim_heads * nheads, bias=bias) self.to_queries = nn.Linear(dim, self.dim_heads * nheads, bias=bias) self.to_values = nn.Linear(dim, self.dim_heads * nheads, bias=bias) self.to_out = nn.Linear(self.dim_heads * nheads, dim, bias=bias) def change_shape(self, x): b_size = x.shape[:-1] return x.reshape(*b_size, self.nheads, self.head_dim) def forward(self, x, mask=True): q = self.change_shape(self.to_queries(x)) k = self.change_shape(self.to_keys(x)) v = self.change_shape(self.to_values(x)) dot_score = q @ k.transpose(-2, -1) * self.scale if mask: tril = torch.tril(torch.ones(dot_score.shape[-2:])) dot_score = dot_score.masked_fill(tril == 0, float("-inf")) attn = self.softmax(attn) attn = self.dropout(attn) out = torch.einsum("bnk,bnd->bnd", attn, v) b_size = out.shape[:-2] out = out.view(*b_size, -1) return self.to_out(out) 
 Thank you!
    submitted by    /u/04RR  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Implement parallel training using the multiprocessing module.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15itb45/p_implement_parallel_training_using_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15itb45/p_implement_parallel_training_using_the/"/>
        <updated>2023-08-05T11:30:27.000Z</updated>
        <summary type="html"><![CDATA[https://github.com/NoteDancing/Note This project allows you to easily implement parallel training with the multiprocessing module. 
    submitted by    /u/NoteDancing  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[how to enable an intend on dialogflow?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15ipmmg/how_to_enable_an_intend_on_dialogflow/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15ipmmg/how_to_enable_an_intend_on_dialogflow/"/>
        <updated>2023-08-05T08:01:42.000Z</updated>
        <summary type="html"><![CDATA[I'm not sure if this is the right subreddit to ask this on, but I'm creating this chatbot on dialogflow and I made the first intend, but I can't figure out how to enable it. whenever I test it, it shows the intend to be idf, and I can't just change the name of the intend to my current intend so it can recognize all the requests I've included in that intend. how do I do that?
    submitted by    /u/penguinsandpandas00  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Forward Process of Diffusion Models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ioim1/r_forward_process_of_diffusion_models/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ioim1/r_forward_process_of_diffusion_models/"/>
        <updated>2023-08-05T06:57:10.000Z</updated>
        <summary type="html"><![CDATA[In the forward process of diffusion models, gaussian noise is added -- when this is done, is the resulting "noisy image" clipped to be within the pixel-value bounds (ie [0, 255] or [0, 1]), or is it allowed to exceed these limits? 
 Clipping makes sense as there is no interpretation for pixel values which exceed these limits.
 On the other hand, the problem with clipping is that if the added noise is clipped, you are not adding truly gaussian noise, which seems problematic as much of the theory behind diffusion models assumes true gaussian noise. 
 Any ideas about what is done in practice, and whether or not this has implications from a theoretical standpoint?
    submitted by    /u/alkaway  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NPC Steven shares his first free-style rap with the world 🤯🎤- Generative NPC update 6]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15ioclg/npc_steven_shares_his_first_freestyle_rap_with/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15ioclg/npc_steven_shares_his_first_freestyle_rap_with/"/>
        <updated>2023-08-05T06:47:08.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Chance_Confection_37  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Team is burning out trying to create a dataset. Any solutions? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ioahe/team_is_burning_out_trying_to_create_a_dataset/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ioahe/team_is_burning_out_trying_to_create_a_dataset/"/>
        <updated>2023-08-05T06:43:42.000Z</updated>
        <summary type="html"><![CDATA[Good Evening ML peeps
 So I am currently creating a dataset in a team of three. This dataset is aimed to create a object detection model for around 11 classes. We have aimed to label around approx. 4000. Our current workflow is a couple of scripts scraping from Pinterest and using Label Studio for labeling. We labeled approx. 25% to our goal but realized that we are about to burn out. We'd prefer that whatever solution there is is self hosted and not paid.
 Thoughts? is there some kind of workflow we are missing to create a dataset?
    submitted by    /u/PlanetAcorn  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Document-based QnA without OpenAI?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15imv19/d_documentbased_qna_without_openai/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15imv19/d_documentbased_qna_without_openai/"/>
        <updated>2023-08-05T05:22:58.000Z</updated>
        <summary type="html"><![CDATA[I am working on a project that is very popular with the inception of Langchain + GPT applications. However, I want to make it open source and hence don't want to use GPT. So something like Langchain + LLama2, etc. I know currently Langchain only supports GPT but any other ideas are highly appreciated!
    submitted by    /u/vishank97  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Looking for suggestions / guides on how to switch from OpenAI Embeddings and Pinecone to open-source / self-hosted architecture options.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ii969/d_looking_for_suggestions_guides_on_how_to_switch/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ii969/d_looking_for_suggestions_guides_on_how_to_switch/"/>
        <updated>2023-08-05T01:30:26.000Z</updated>
        <summary type="html"><![CDATA[Hi all, 
 I'm interested in redesigning my application to utilize an open-source embeddings model and a different vector DB. My current issue with embeddings is that processing large volumes of data into a vector DB using ada-002 is unreliable, with frequent API timeouts occurring or issues interacting with Pinecone. This is super problematic as it's difficult to track which data has / hasn't been stored correctly. I also know that many open-source embeddings models are more performant and will allow for more long term control over my data. 
 However, the advantage of using OpenAI / Pinecone has of course been simplicity in production and not having to worry about queries / retrieval working efficiently. To give context, I'm dealing with a large volume of documents, such that if I were to embed my documents into a FAISS index with a small sentence transformers model, it would constitute 12GB, so a really simple solution like storing within the same application database is probably a no-go.
 In initiating this switch, I want to know the best approach towards: 
 A) Utilizing an open-source embeddings model in a production context (is it best to host as an API via a cloud provider and what are some considerations I should think about? What's a fast / reliable way of setting this up? I would like prioritise a more simple approach if possible.) 
 B) What Vector DB I should be looking into as an alternative and what's the best way to achieve self-hosted so that it would be equally performant compared to hosted services like pinecone (Docker? AWS?)? 
    submitted by    /u/theheffalump2000  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Unsupervised Machine Learning Approach for Ground-Motion Spectra Clustering and Selection. (arXiv:2212.03188v2 [physics.geo-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.03188</id>
        <link href="http://arxiv.org/abs/2212.03188"/>
        <updated>2023-08-05T00:48:27.439Z</updated>
        <summary type="html"><![CDATA[Clustering analysis of sequence data continues to address many applications
in engineering design, aided with the rapid growth of machine learning in
applied science. This paper presents an unsupervised machine learning algorithm
to extract defining characteristics of earthquake ground-motion spectra, also
called latent features, to aid in ground-motion selection (GMS). In this
context, a latent feature is a low-dimensional machine-discovered spectral
characteristic learned through nonlinear relationships of a neural network
autoencoder. Machine discovered latent features can be combined with
traditionally defined intensity measures and clustering can be performed to
select a representative subgroup from a large ground-motion suite. The
objective of efficient GMS is to choose characteristic records representative
of what the structure will probabilistically experience in its lifetime. Three
examples are presented to validate this approach, including the use of
synthetic and field recorded ground-motion datasets. The presented deep
embedding clustering of ground-motion spectra has three main advantages: 1.
defining characteristics the represent the sparse spectral content of
ground-motions are discovered efficiently through training of the autoencoder,
2. domain knowledge is incorporated into the machine learning framework with
conditional variables in the deep embedding scheme, and 3. method exhibits
excellent performance when compared to a benchmark seismic hazard analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Bond_R/0/1/0/all/0/1"&gt;R. Bailey Bond&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ren_P/0/1/0/all/0/1"&gt;Pu Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hajjar_J/0/1/0/all/0/1"&gt;Jerome F. Hajjar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hao Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Reinforcement Learning of Koopman Models for Economic Nonlinear MPC. (arXiv:2308.01674v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01674</id>
        <link href="http://arxiv.org/abs/2308.01674"/>
        <updated>2023-08-05T00:48:27.433Z</updated>
        <summary type="html"><![CDATA[(Economic) nonlinear model predictive control ((e)NMPC) requires dynamic
system models that are sufficiently accurate in all relevant state-space
regions. These models must also be computationally cheap enough to ensure
real-time tractability. Data-driven surrogate models for mechanistic models can
be used to reduce the computational burden of (e)NMPC; however, such models are
typically trained by system identification for maximum average prediction
accuracy on simulation samples and perform suboptimally as part of actual
(e)NMPC. We present a method for end-to-end reinforcement learning of dynamic
surrogate models for optimal performance in (e)NMPC applications, resulting in
predictive controllers that strike a favorable balance between control
performance and computational demand. We validate our method on two
applications derived from an established nonlinear continuous stirred-tank
reactor model. We compare the controller performance to that of MPCs utilizing
models trained by the prevailing maximum prediction accuracy paradigm, and
model-free neural network controllers trained using reinforcement learning. We
show that our method matches the performance of the model-free neural network
controllers while consistently outperforming models derived from system
identification. Additionally, we show that the MPC policies can react to
changes in the control setting without retraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mayfrank_D/0/1/0/all/0/1"&gt;Daniel Mayfrank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitsos_A/0/1/0/all/0/1"&gt;Alexander Mitsos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dahmen_M/0/1/0/all/0/1"&gt;Manuel Dahmen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relational Experience Replay: Continual Learning by Adaptively Tuning Task-wise Relationship. (arXiv:2112.15402v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.15402</id>
        <link href="http://arxiv.org/abs/2112.15402"/>
        <updated>2023-08-05T00:48:27.426Z</updated>
        <summary type="html"><![CDATA[Continual learning is a promising machine learning paradigm to learn new
tasks while retaining previously learned knowledge over streaming training
data. Till now, rehearsal-based methods, keeping a small part of data from old
tasks as a memory buffer, have shown good performance in mitigating
catastrophic forgetting for previously learned knowledge. However, most of
these methods typically treat each new task equally, which may not adequately
consider the relationship or similarity between old and new tasks. Furthermore,
these methods commonly neglect sample importance in the continual training
process and result in sub-optimal performance on certain tasks. To address this
challenging problem, we propose Relational Experience Replay (RER), a bi-level
learning framework, to adaptively tune task-wise relationships and sample
importance within each task to achieve a better `stability' and `plasticity'
trade-off. As such, the proposed method is capable of accumulating new
knowledge while consolidating previously learned old knowledge during continual
learning. Extensive experiments conducted on three publicly available datasets
(i.e., CIFAR-10, CIFAR-100, and Tiny ImageNet) show that the proposed method
can consistently improve the performance of all baselines and surpass current
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Quanziang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Renzhen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuexiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1"&gt;Dong Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kai Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1"&gt;Deyu Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Multiplex Graph Learning with Complementary and Consistent Information. (arXiv:2308.01606v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01606</id>
        <link href="http://arxiv.org/abs/2308.01606"/>
        <updated>2023-08-05T00:48:27.404Z</updated>
        <summary type="html"><![CDATA[Unsupervised multiplex graph learning (UMGL) has been shown to achieve
significant effectiveness for different downstream tasks by exploring both
complementary information and consistent information among multiple graphs.
However, previous methods usually overlook the issues in practical
applications, i.e., the out-of-sample issue and the noise issue. To address the
above issues, in this paper, we propose an effective and efficient UMGL method
to explore both complementary and consistent information. To do this, our
method employs multiple MLP encoders rather than graph convolutional network
(GCN) to conduct representation learning with two constraints, i.e., preserving
the local graph structure among nodes to handle the out-of-sample issue, and
maximizing the correlation of multiple node representations to handle the noise
issue. Comprehensive experiments demonstrate that our proposed method achieves
superior effectiveness and efficiency over the comparison methods and
effectively tackles those two issues. Code is available at
https://github.com/LarryUESTC/CoCoMG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1"&gt;Liang Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaofeng Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models. (arXiv:2308.01390v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.01390</id>
        <link href="http://arxiv.org/abs/2308.01390"/>
        <updated>2023-08-05T00:48:27.396Z</updated>
        <summary type="html"><![CDATA[We introduce OpenFlamingo, a family of autoregressive vision-language models
ranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to produce
an open-source replication of DeepMind's Flamingo models. On seven
vision-language datasets, OpenFlamingo models average between 80 - 89% of
corresponding Flamingo performance. This technical report describes our models,
training data, hyperparameters, and evaluation suite. We share our models and
code at https://github.com/mlfoundations/open_flamingo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Awadalla_A/0/1/0/all/0/1"&gt;Anas Awadalla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_I/0/1/0/all/0/1"&gt;Irena Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1"&gt;Josh Gardner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1"&gt;Jack Hessel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanafy_Y/0/1/0/all/0/1"&gt;Yusuf Hanafy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Wanrong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marathe_K/0/1/0/all/0/1"&gt;Kalyani Marathe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1"&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1"&gt;Samir Gadre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sagawa_S/0/1/0/all/0/1"&gt;Shiori Sagawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jitsev_J/0/1/0/all/0/1"&gt;Jenia Jitsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1"&gt;Simon Kornblith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1"&gt;Pang Wei Koh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1"&gt;Gabriel Ilharco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1"&gt;Mitchell Wortsman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1"&gt;Ludwig Schmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaborative causal inference on distributed data. (arXiv:2208.07898v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.07898</id>
        <link href="http://arxiv.org/abs/2208.07898"/>
        <updated>2023-08-05T00:48:27.389Z</updated>
        <summary type="html"><![CDATA[The development of technologies for causal inference with the privacy
preservation of distributed data has attracted considerable attention in recent
years. To address this issue, we propose a data collaboration quasi-experiment
(DC-QE) that enables causal inference from distributed data with privacy
preservation. In our method, first, local parties construct
dimensionality-reduced intermediate representations from the private data.
Second, they share intermediate representations, instead of private data for
privacy preservation. Third, propensity scores were estimated from the shared
intermediate representations. Finally, the treatment effects were estimated
from propensity scores. Our method can reduce both random errors and biases,
whereas existing methods can only reduce random errors in the estimation of
treatment effects. Through numerical experiments on both artificial and
real-world data, we confirmed that our method can lead to better estimation
results than individual analyses. Dimensionality-reduction loses some of the
information in the private data and causes performance degradation. However, we
observed that in the experiments, sharing intermediate representations with
many parties to resolve the lack of subjects and covariates, our method
improved performance enough to overcome the degradation caused by
dimensionality-reduction. With the spread of our method, intermediate
representations can be published as open data to help researchers find
causalities and accumulated as a knowledge base.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kawamata_Y/0/1/0/all/0/1"&gt;Yuji Kawamata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Motai_R/0/1/0/all/0/1"&gt;Ryoki Motai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Okada_Y/0/1/0/all/0/1"&gt;Yukihiko Okada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Imakura_A/0/1/0/all/0/1"&gt;Akira Imakura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sakurai_T/0/1/0/all/0/1"&gt;Tetsuya Sakurai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sharing to learn and learning to share -- Fitting together Meta-Learning, Multi-Task Learning, and Transfer Learning: A meta review. (arXiv:2111.12146v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.12146</id>
        <link href="http://arxiv.org/abs/2111.12146"/>
        <updated>2023-08-05T00:48:27.382Z</updated>
        <summary type="html"><![CDATA[Integrating knowledge across different domains is an essential feature of
human learning. Learning paradigms such as transfer learning, meta learning,
and multi-task learning reflect the human learning process by exploiting the
prior knowledge for new tasks, encouraging faster learning and good
generalization for new tasks. This article gives a detailed view of these
learning paradigms and their comparative analysis. The weakness of one learning
algorithm turns out to be a strength of another, and thus merging them is a
prevalent trait in the literature. There are numerous research papers that
focus on each of these learning paradigms separately and provide a
comprehensive overview of them. However, this article provides a review of
research studies that combine (two of) these learning algorithms. This survey
describes how these techniques are combined to solve problems in many different
fields of study, including computer vision, natural language processing,
hyperspectral imaging, and many more, in supervised setting only. As a result,
the global generic learning network an amalgamation of meta learning, transfer
learning, and multi-task learning is introduced here, along with some open
research questions and future research directions in the multi-task setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Upadhyay_R/0/1/0/all/0/1"&gt;Richa Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phlypo_R/0/1/0/all/0/1"&gt;Ronald Phlypo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saini_R/0/1/0/all/0/1"&gt;Rajkumar Saini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1"&gt;Marcus Liwicki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models. (arXiv:2301.06267v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2301.06267</id>
        <link href="http://arxiv.org/abs/2301.06267"/>
        <updated>2023-08-05T00:48:27.361Z</updated>
        <summary type="html"><![CDATA[The ability to quickly learn a new task with minimal instruction - known as
few-shot learning - is a central aspect of intelligent agents. Classical
few-shot benchmarks make use of few-shot samples from a single modality, but
such samples may not be sufficient to characterize an entire concept class. In
contrast, humans use cross-modal information to learn new concepts efficiently.
In this work, we demonstrate that one can indeed build a better ${\bf visual}$
dog classifier by ${\bf read}$ing about dogs and ${\bf listen}$ing to them
bark. To do so, we exploit the fact that recent multimodal foundation models
such as CLIP are inherently cross-modal, mapping different modalities to the
same representation space. Specifically, we propose a simple cross-modal
adaptation approach that learns from few-shot examples spanning different
modalities. By repurposing class names as additional one-shot training samples,
we achieve SOTA results with an embarrassingly simple linear classifier for
vision-language adaptation. Furthermore, we show that our approach can benefit
existing methods such as prefix tuning, adapters, and classifier ensembling.
Finally, to explore other modalities beyond vision and language, we construct
the first (to our knowledge) audiovisual few-shot benchmark and use cross-modal
training to improve the performance of both image and audio classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhiqiu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Samuel Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1"&gt;Zhiyi Kuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1"&gt;Deepak Pathak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1"&gt;Deva Ramanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid. (arXiv:2307.16149v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.16149</id>
        <link href="http://arxiv.org/abs/2307.16149"/>
        <updated>2023-08-05T00:48:27.355Z</updated>
        <summary type="html"><![CDATA[Energy theft detection (ETD) and energy consumption forecasting (ECF) are two
interconnected challenges in smart grid systems. Addressing these issues
collectively is crucial for ensuring system security. This paper addresses the
interconnected challenges of ETD and ECF in smart grid systems. The proposed
solution combines long short-term memory (LSTM) and a denoising diffusion
probabilistic model (DDPM) to generate input reconstruction and forecasting. By
leveraging the reconstruction and forecasting errors, the system identifies
instances of energy theft, with the methods based on reconstruction error and
forecasting error complementing each other in detecting different types of
attacks. Through extensive experiments on real-world and synthetic datasets,
the proposed scheme outperforms baseline methods in ETD and ECF problems. The
ensemble method significantly enhances ETD performance, accurately detecting
energy theft attacks that baseline methods fail to detect. The research offers
a comprehensive and effective solution for addressing ETD and ECF challenges,
demonstrating promising results and improved security in smart grid systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xun Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alromih_A/0/1/0/all/0/1"&gt;Arwa Alromih&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gope_P/0/1/0/all/0/1"&gt;Prosanta Gope&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sikdar_B/0/1/0/all/0/1"&gt;Biplab Sikdar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bag of Policies for Distributional Deep Exploration. (arXiv:2308.01759v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01759</id>
        <link href="http://arxiv.org/abs/2308.01759"/>
        <updated>2023-08-05T00:48:27.298Z</updated>
        <summary type="html"><![CDATA[Efficient exploration in complex environments remains a major challenge for
reinforcement learning (RL). Compared to previous Thompson sampling-inspired
mechanisms that enable temporally extended exploration, i.e., deep exploration,
we focus on deep exploration in distributional RL. We develop here a general
purpose approach, Bag of Policies (BoP), that can be built on top of any return
distribution estimator by maintaining a population of its copies. BoP consists
of an ensemble of multiple heads that are updated independently. During
training, each episode is controlled by only one of the heads and the collected
state-action pairs are used to update all heads off-policy, leading to distinct
learning signals for each head which diversify learning and behaviour. To test
whether optimistic ensemble method can improve on distributional RL as did on
scalar RL, by e.g. Bootstrapped DQN, we implement the BoP approach with a
population of distributional actor-critics using Bayesian Distributional Policy
Gradients (BDPG). The population thus approximates a posterior distribution of
return distributions along with a posterior distribution of policies. Another
benefit of building upon BDPG is that it allows to analyze global posterior
uncertainty along with local curiosity bonus simultaneously for exploration. As
BDPG is already an optimistic method, this pairing helps to investigate if
optimism is accumulatable in distributional RL. Overall BoP results in greater
robustness and speed during learning as demonstrated by our experimental
results on ALE Atari games.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nachkov_A/0/1/0/all/0/1"&gt;Asen Nachkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Luchen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luise_G/0/1/0/all/0/1"&gt;Giulia Luise&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valdettaro_F/0/1/0/all/0/1"&gt;Filippo Valdettaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faisal_A/0/1/0/all/0/1"&gt;Aldo Faisal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Representation Learning for Time Series: A Review. (arXiv:2308.01578v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01578</id>
        <link href="http://arxiv.org/abs/2308.01578"/>
        <updated>2023-08-05T00:48:26.926Z</updated>
        <summary type="html"><![CDATA[Unsupervised representation learning approaches aim to learn discriminative
feature representations from unlabeled data, without the requirement of
annotating every sample. Enabling unsupervised representation learning is
extremely crucial for time series data, due to its unique annotation bottleneck
caused by its complex characteristics and lack of visual cues compared with
other data modalities. In recent years, unsupervised representation learning
techniques have advanced rapidly in various domains. However, there is a lack
of systematic analysis of unsupervised representation learning approaches for
time series. To fill the gap, we conduct a comprehensive literature review of
existing rapidly evolving unsupervised representation learning approaches for
time series. Moreover, we also develop a unified and standardized library,
named ULTS (i.e., Unsupervised Learning for Time Series), to facilitate fast
implementations and unified evaluations on various models. With ULTS, we
empirically evaluate state-of-the-art approaches, especially the rapidly
evolving contrastive learning methods, on 9 diverse real-world datasets. We
further discuss practical considerations as well as open research challenges on
unsupervised representation learning for time series to facilitate future
research in this field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1"&gt;Qianwen Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1"&gt;Hangwei Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yonghui Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zhiqi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1"&gt;Lizhen Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CT Perfusion is All We Need: 4D CNN Segmentation of Penumbra and Core in Patient With Suspected Ischemic Stroke. (arXiv:2303.08757v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.08757</id>
        <link href="http://arxiv.org/abs/2303.08757"/>
        <updated>2023-08-05T00:48:26.926Z</updated>
        <summary type="html"><![CDATA[Precise and fast prediction methods for ischemic areas comprised of dead
tissue, core, and salvageable tissue, penumbra, in acute ischemic stroke (AIS)
patients are of significant clinical interest. They play an essential role in
improving diagnosis and treatment planning. Computed Tomography (CT) scan is
one of the primary modalities for early assessment in patients with suspected
AIS. CT Perfusion (CTP) is often used as a primary assessment to determine
stroke location, severity, and volume of ischemic lesions. Current automatic
segmentation methods for CTP mostly use already processed 3D parametric maps
conventionally used for clinical interpretation by radiologists as input.
Alternatively, the raw CTP data is used on a slice-by-slice basis as 2D+time
input, where the spatial information over the volume is ignored. In addition,
these methods are only interested in segmenting core regions, while predicting
penumbra can be essential for treatment planning. This paper investigates
different methods to utilize the entire 4D CTP as input to fully exploit the
spatio-temporal information, leading us to propose a novel 4D convolution
layer. Our comprehensive experiments on a local dataset of 152 patients divided
into three groups show that our proposed models generate more precise results
than other methods explored. Adopting the proposed 4D mJ-Net, a Dice
Coefficient of 0.53 and 0.23 is achieved for segmenting penumbra and core
areas, respectively. The code is available on
https://github.com/Biomedical-Data-Analysis-Laboratory/4D-mJ-Net.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tomasetti_L/0/1/0/all/0/1"&gt;Luca Tomasetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Engan_K/0/1/0/all/0/1"&gt;Kjersti Engan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hollesli_L/0/1/0/all/0/1"&gt;Liv Jorunn H&amp;#xf8;llesli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kurz_K/0/1/0/all/0/1"&gt;Kathinka D&amp;#xe6;hli Kurz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khanmohammadi_M/0/1/0/all/0/1"&gt;Mahdieh Khanmohammadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Training of Mean Variance Estimation Neural Networks. (arXiv:2302.08875v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.08875</id>
        <link href="http://arxiv.org/abs/2302.08875"/>
        <updated>2023-08-05T00:48:26.917Z</updated>
        <summary type="html"><![CDATA[This paper focusses on the optimal implementation of a Mean Variance
Estimation network (MVE network) (Nix and Weigend, 1994). This type of network
is often used as a building block for uncertainty estimation methods in a
regression setting, for instance Concrete dropout (Gal et al., 2017) and Deep
Ensembles (Lakshminarayanan et al., 2017). Specifically, an MVE network assumes
that the data is produced from a normal distribution with a mean function and
variance function. The MVE network outputs a mean and variance estimate and
optimizes the network parameters by minimizing the negative loglikelihood. In
our paper, we present two significant insights. Firstly, the convergence
difficulties reported in recent work can be relatively easily prevented by
following the simple yet often overlooked recommendation from the original
authors that a warm-up period should be used. During this period, only the mean
is optimized with a fixed variance. We demonstrate the effectiveness of this
step through experimentation, highlighting that it should be standard practice.
As a sidenote, we examine whether, after the warm-up, it is beneficial to fix
the mean while optimizing the variance or to optimize both simultaneously.
Here, we do not observe a substantial difference. Secondly, we introduce a
novel improvement of the MVE network: separate regularization of the mean and
the variance estimate. We demonstrate, both on toy examples and on a number of
benchmark UCI regression data sets, that following the original recommendations
and the novel separate regularization can lead to significant improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sluijterman_L/0/1/0/all/0/1"&gt;Laurens Sluijterman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cator_E/0/1/0/all/0/1"&gt;Eric Cator&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Heskes_T/0/1/0/all/0/1"&gt;Tom Heskes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reverse Stable Diffusion: What prompt was used to generate this image?. (arXiv:2308.01472v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.01472</id>
        <link href="http://arxiv.org/abs/2308.01472"/>
        <updated>2023-08-05T00:48:26.916Z</updated>
        <summary type="html"><![CDATA[Text-to-image diffusion models such as Stable Diffusion have recently
attracted the interest of many researchers, and inverting the diffusion process
can play an important role in better understanding the generative process and
how to engineer prompts in order to obtain the desired images. To this end, we
introduce the new task of predicting the text prompt given an image generated
by a generative diffusion model. We combine a series of white-box and black-box
models (with and without access to the weights of the diffusion network) to
deal with the proposed task. We propose a novel learning framework comprising
of a joint prompt regression and multi-label vocabulary classification
objective that generates improved prompts. To further improve our method, we
employ a curriculum learning procedure that promotes the learning of
image-prompt pairs with lower labeling noise (i.e. that are better aligned),
and an unsupervised domain-adaptive kernel learning method that uses the
similarities between samples in the source and target domains as extra
features. We conduct experiments on the DiffusionDB data set, predicting text
prompts from images generated by Stable Diffusion. Our novel learning framework
produces excellent results on the aforementioned task, yielding the highest
gains when applied on the white-box model. In addition, we make an interesting
discovery: training a diffusion model on the prompt generation task can make
the model generate images that are much better aligned with the input prompts,
when the model is directly reused for text-to-image generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Croitoru_F/0/1/0/all/0/1"&gt;Florinel-Alin Croitoru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hondru_V/0/1/0/all/0/1"&gt;Vlad Hondru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1"&gt;Radu Tudor Ionescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mubarak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search. (arXiv:2307.13831v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.13831</id>
        <link href="http://arxiv.org/abs/2307.13831"/>
        <updated>2023-08-05T00:48:26.914Z</updated>
        <summary type="html"><![CDATA[Stochastic gradient descent (SGD) is the simplest deep learning optimizer
with which to train deep neural networks. While SGD can use various learning
rates, such as constant or diminishing rates, the previous numerical results
showed that SGD performs better than other deep learning optimizers using when
it uses learning rates given by line search methods. In this paper, we perform
a convergence analysis on SGD with a learning rate given by an Armijo line
search for nonconvex optimization. The analysis indicates that the upper bound
of the expectation of the squared norm of the full gradient becomes small when
the number of steps and the batch size are large. Next, we show that, for SGD
with the Armijo-line-search learning rate, the number of steps needed for
nonconvex optimization is a monotone decreasing convex function of the batch
size; that is, the number of steps needed for nonconvex optimization decreases
as the batch size increases. Furthermore, we show that the stochastic
first-order oracle (SFO) complexity, which is the stochastic gradient
computation cost, is a convex function of the batch size; that is, there exists
a critical batch size that minimizes the SFO complexity. Finally, we provide
numerical results that support our theoretical results. The numerical results
indicate that the number of steps needed for training deep neural networks
decreases as the batch size increases and that there exist the critical batch
sizes that can be estimated from the theoretical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsukada_Y/0/1/0/all/0/1"&gt;Yuki Tsukada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iiduka_H/0/1/0/all/0/1"&gt;Hideaki Iiduka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An efficient, provably exact, practical algorithm for the 0-1 loss linear classification problem. (arXiv:2306.12344v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.12344</id>
        <link href="http://arxiv.org/abs/2306.12344"/>
        <updated>2023-08-05T00:48:26.848Z</updated>
        <summary type="html"><![CDATA[Algorithms for solving the linear classification problem have a long history,
dating back at least to 1936 with linear discriminant analysis. For linearly
separable data, many algorithms can obtain the exact solution to the
corresponding 0-1 loss classification problem efficiently, but for data which
is not linearly separable, it has been shown that this problem, in full
generality, is NP-hard. Alternative approaches all involve approximations of
some kind, including the use of surrogates for the 0-1 loss (for example, the
hinge or logistic loss) or approximate combinatorial search, none of which can
be guaranteed to solve the problem exactly. Finding efficient algorithms to
obtain an exact i.e. globally optimal solution for the 0-1 loss linear
classification problem with fixed dimension, remains an open problem. In
research we report here, we detail the rigorous construction of a new
algorithm, incremental cell enumeration (ICE), that can solve the 0-1 loss
classification problem exactly in polynomial time. We prove correctness using
concepts from the theory of hyperplane arrangements and oriented matroids. We
demonstrate the effectiveness of this algorithm on synthetic and real-world
datasets, showing optimal accuracy both in and out-of-sample, in practical
computational time. We also empirically demonstrate how the use of approximate
upper bound leads to polynomial time run-time improvements to the algorithm
whilst retaining exactness. To our knowledge, this is the first,
rigorously-proven polynomial time, practical algorithm for this long-standing
problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xi He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_W/0/1/0/all/0/1"&gt;Waheed Ul Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Little_M/0/1/0/all/0/1"&gt;Max A. Little&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Planted Forest: a directly interpretable tree ensemble. (arXiv:2012.14563v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14563</id>
        <link href="http://arxiv.org/abs/2012.14563"/>
        <updated>2023-08-05T00:48:26.842Z</updated>
        <summary type="html"><![CDATA[We introduce a novel interpretable tree based algorithm for prediction in a
regression setting. Our motivation is to estimate the unknown regression
function from a functional decomposition perspective in which the functional
components correspond to lower order interaction terms. The idea is to modify
the random forest algorithm by keeping certain leaves after they are split
instead of deleting them. This leads to non-binary trees which we refer to as
planted trees. An extension to a forest leads to our random planted forest
algorithm. Additionally, the maximum number of covariates which can interact
within a leaf can be bounded. If we set this interaction bound to one, the
resulting estimator is a sum of one-dimensional functions. In the other extreme
case, if we do not set a limit, the resulting estimator and corresponding model
place no restrictions on the form of the regression function. In a simulation
study we find encouraging prediction and visualisation properties of our random
planted forest method. We also develop theory for an idealized version of
random planted forests in cases where the interaction bound is low. We show
that if it is smaller than three, the idealized version achieves asymptotically
optimal convergence rates up to a logarithmic factor. Code is available on
GitHub https://github.com/PlantedML/randomPlantedForest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hiabu_M/0/1/0/all/0/1"&gt;Munir Hiabu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mammen_E/0/1/0/all/0/1"&gt;Enno Mammen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meyer_J/0/1/0/all/0/1"&gt;Joseph T. Meyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIRACLE: Multi-task Learning based Interpretable Regulation of Autoimmune Diseases through Common Latent Epigenetics. (arXiv:2306.13866v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.13866</id>
        <link href="http://arxiv.org/abs/2306.13866"/>
        <updated>2023-08-05T00:48:26.838Z</updated>
        <summary type="html"><![CDATA[DNA methylation is a crucial regulator of gene transcription and has been
linked to various diseases, including autoimmune diseases and cancers. However,
diagnostics based on DNA methylation face challenges due to large feature sets
and small sample sizes, resulting in overfitting and suboptimal performance. To
address these issues, we propose MIRACLE, a novel interpretable neural network
that leverages autoencoder-based multi-task learning to integrate multiple
datasets and jointly identify common patterns in DNA methylation.

MIRACLE's architecture reflects the relationships between methylation sites,
genes, and pathways, ensuring biological interpretability and meaningfulness.
The network comprises an encoder and a decoder, with a bottleneck layer
representing pathway information as the basic unit of heredity. Customized
defined MaskedLinear Layer is constrained by site-gene-pathway graph adjacency
matrix information, which provides explainability and expresses the
site-gene-pathway hierarchical structure explicitly. And from the embedding,
there are different multi-task classifiers to predict diseases.

Tested on six datasets, including rheumatoid arthritis, systemic lupus
erythematosus, multiple sclerosis, inflammatory bowel disease, psoriasis, and
type 1 diabetes, MIRACLE demonstrates robust performance in identifying common
functions of DNA methylation across different phenotypes, with higher accuracy
in prediction dieseases than baseline methods. By incorporating biological
prior knowledge, MIRACLE offers a meaningful and interpretable framework for
DNA methylation data analysis in the context of autoimmune diseases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Pengcheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jinpu Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yulin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rong_Z/0/1/0/all/0/1"&gt;Ziqi Rong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nearest Neighbour with Bandit Feedback. (arXiv:2306.13773v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.13773</id>
        <link href="http://arxiv.org/abs/2306.13773"/>
        <updated>2023-08-05T00:48:26.837Z</updated>
        <summary type="html"><![CDATA[In this paper we adapt the nearest neighbour rule to the contextual bandit
problem. Our algorithm handles the fully adversarial setting in which no
assumptions at all are made about the data-generation process. When combined
with a sufficiently fast data-structure for (perhaps approximate) adaptive
nearest neighbour search, such as a navigating net, our algorithm is extremely
efficient - having a per trial running time polylogarithmic in both the number
of trials and actions, and taking only quasi-linear space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pasteris_S/0/1/0/all/0/1"&gt;Stephen Pasteris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hicks_C/0/1/0/all/0/1"&gt;Chris Hicks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mavroudis_V/0/1/0/all/0/1"&gt;Vasilios Mavroudis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VertexSerum: Poisoning Graph Neural Networks for Link Inference. (arXiv:2308.01469v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01469</id>
        <link href="http://arxiv.org/abs/2308.01469"/>
        <updated>2023-08-05T00:48:26.831Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) have brought superb performance to various
applications utilizing graph structural data, such as social analysis and fraud
detection. The graph links, e.g., social relationships and transaction history,
are sensitive and valuable information, which raises privacy concerns when
using GNNs. To exploit these vulnerabilities, we propose VertexSerum, a novel
graph poisoning attack that increases the effectiveness of graph link stealing
by amplifying the link connectivity leakage. To infer node adjacency more
accurately, we propose an attention mechanism that can be embedded into the
link detection network. Our experiments demonstrate that VertexSerum
significantly outperforms the SOTA link inference attack, improving the AUC
scores by an average of $9.8\%$ across four real-world datasets and three
different GNN structures. Furthermore, our experiments reveal the effectiveness
of VertexSerum in both black-box and online learning settings, further
validating its applicability in real-world scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1"&gt;Ruyi Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1"&gt;Shijin Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaolin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Y/0/1/0/all/0/1"&gt;Yunsi Fei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of network-guided random forest for disease gene discovery. (arXiv:2308.01323v1 [q-bio.MN])]]></title>
        <id>http://arxiv.org/abs/2308.01323</id>
        <link href="http://arxiv.org/abs/2308.01323"/>
        <updated>2023-08-05T00:48:26.827Z</updated>
        <summary type="html"><![CDATA[Gene network information is believed to be beneficial for disease module and
pathway identification, but has not been explicitly utilized in the standard
random forest (RF) algorithm for gene expression data analysis. We investigate
the performance of a network-guided RF where the network information is
summarized into a sampling probability of predictor variables which is further
used in the construction of the RF. Our results suggest that network-guided RF
does not provide better disease prediction than the standard RF. In terms of
disease gene discovery, if disease genes form module(s), network-guided RF
identifies them more accurately. In addition, when disease status is
independent from genes in the given network, spurious gene selection results
can occur when using network information, especially on hub genes. Our
empirical analysis on two balanced microarray and RNA-Seq breast cancer
datasets from The Cancer Genome Atlas (TCGA) for classification of progesterone
receptor (PR) status also demonstrates that network-guided RF can identify
genes from PGR-related pathways, which leads to a better connected module of
identified genes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jianchang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Szymczak_S/0/1/0/all/0/1"&gt;Silke Szymczak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Collapse Terminus: A Unified Solution for Class Incremental Learning and Its Variants. (arXiv:2308.01746v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01746</id>
        <link href="http://arxiv.org/abs/2308.01746"/>
        <updated>2023-08-05T00:48:26.823Z</updated>
        <summary type="html"><![CDATA[How to enable learnability for new classes while keeping the capability well
on old classes has been a crucial challenge for class incremental learning.
Beyond the normal case, long-tail class incremental learning and few-shot class
incremental learning are also proposed to consider the data imbalance and data
scarcity, respectively, which are common in real-world implementations and
further exacerbate the well-known problem of catastrophic forgetting. Existing
methods are specifically proposed for one of the three tasks. In this paper, we
offer a unified solution to the misalignment dilemma in the three tasks.
Concretely, we propose neural collapse terminus that is a fixed structure with
the maximal equiangular inter-class separation for the whole label space. It
serves as a consistent target throughout the incremental training to avoid
dividing the feature space incrementally. For CIL and LTCIL, we further propose
a prototype evolving scheme to drive the backbone features into our neural
collapse terminus smoothly. Our method also works for FSCIL with only minor
adaptations. Theoretical analysis indicates that our method holds the neural
collapse optimality in an incremental fashion regardless of data imbalance or
data scarcity. We also design a generalized case where we do not know the total
number of classes and whether the data distribution is normal, long-tail, or
few-shot for each coming session, to test the generalizability of our method.
Extensive experiments with multiple datasets are conducted to demonstrate the
effectiveness of our unified solution to all the three tasks and the
generalized case.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yibo Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Haobo Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiangtai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jianlong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lefei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhouchen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multitask Learning with No Regret: from Improved Confidence Bounds to Active Learning. (arXiv:2308.01744v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01744</id>
        <link href="http://arxiv.org/abs/2308.01744"/>
        <updated>2023-08-05T00:48:26.822Z</updated>
        <summary type="html"><![CDATA[Multitask learning is a powerful framework that enables one to simultaneously
learn multiple related tasks by sharing information between them. Quantifying
uncertainty in the estimated tasks is of pivotal importance for many downstream
applications, such as online or active learning. In this work, we provide novel
multitask confidence intervals in the challenging agnostic setting, i.e., when
neither the similarity between tasks nor the tasks' features are available to
the learner. The obtained intervals do not require i.i.d. data and can be
directly applied to bound the regret in online learning. Through a refined
analysis of the multitask information gain, we obtain new regret guarantees
that, depending on a task similarity parameter, can significantly improve over
treating tasks independently. We further propose a novel online learning
algorithm that achieves such improved regret without knowing this parameter in
advance, i.e., automatically adapting to task similarity. As a second key
application of our results, we introduce a novel multitask active learning
setup where several tasks must be simultaneously optimized, but only one of
them can be queried for feedback by the learner at each round. For this
problem, we design a no-regret algorithm that uses our confidence intervals to
decide which task should be queried. Finally, we empirically validate our
bounds and algorithms on synthetic and real-world (drug discovery) data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sessa_P/0/1/0/all/0/1"&gt;Pier Giuseppe Sessa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laforgue_P/0/1/0/all/0/1"&gt;Pierre Laforgue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cesa_Bianchi_N/0/1/0/all/0/1"&gt;Nicol&amp;#xf2; Cesa-Bianchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction. (arXiv:2308.01737v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2308.01737</id>
        <link href="http://arxiv.org/abs/2308.01737"/>
        <updated>2023-08-05T00:48:26.814Z</updated>
        <summary type="html"><![CDATA[With the widespread application of personalized online services,
click-through rate (CTR) prediction has received more and more attention and
research. The most prominent features of CTR prediction are its multi-field
categorical data format, and vast and daily-growing data volume. The large
capacity of neural models helps digest such massive amounts of data under the
supervised learning paradigm, yet they fail to utilize the substantial data to
its full potential, since the 1-bit click signal is not sufficient to guide the
model to learn capable representations of features and instances. The
self-supervised learning paradigm provides a more promising pretrain-finetune
solution to better exploit the large amount of user click logs, and learn more
generalized and effective representations. However, self-supervised learning
for CTR prediction is still an open question, since current works on this line
are only preliminary and rudimentary. To this end, we propose a Model-agnostic
pretraining (MAP) framework that applies feature corruption and recovery on
multi-field categorical data, and more specifically, we derive two practical
algorithms: masked feature prediction (MFP) and replaced feature detection
(RFD). MFP digs into feature interactions within each instance through masking
and predicting a small portion of input features, and introduces noise
contrastive estimation (NCE) to handle large feature spaces. RFD further turns
MFP into a binary classification mode through replacing and detecting changes
in input features, making it even simpler and more effective for CTR
pretraining. Our extensive experiments on two real-world large-scale datasets
(i.e., Avazu, Criteo) demonstrate the advantages of these two methods on
several strong backbones (e.g., DCNv2, DeepFM), and achieve new
state-of-the-art performance in terms of both effectiveness and efficiency for
CTR prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jianghao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1"&gt;Yanru Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1"&gt;Xinyi Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1"&gt;Ruiming Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weinan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Masked Diffusion Models Are Fast and Privacy-Aware Learners. (arXiv:2306.11363v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.11363</id>
        <link href="http://arxiv.org/abs/2306.11363"/>
        <updated>2023-08-05T00:48:26.813Z</updated>
        <summary type="html"><![CDATA[Diffusion models have emerged as the \emph{de-facto} technique for image
generation, yet they entail significant computational overhead, hindering the
technique's broader application in the research community. We propose a
prior-based denoising training framework, the first to incorporate the
pre-train and fine-tune paradigm into the diffusion model training process,
which substantially improves training efficiency and shows potential in
facilitating various downstream tasks. Our approach centers on masking a high
proportion (e.g., up to 90\%) of the input image and employing masked denoising
score matching to denoise the visible areas, thereby guiding the diffusion
model to learn more salient features from training data as prior knowledge. By
utilizing masked learning in a pre-training stage, we efficiently train the
ViT-based diffusion model on CelebA-HQ $256 \times 256$ in the pixel space,
achieving a 4x acceleration and enhancing the quality of generated images
compared to denoising diffusion probabilistic model (DDPM). Moreover, our
masked pre-training technique can be universally applied to various diffusion
models that directly generate images in the pixel space, aiding in the learning
of pre-trained models with superior generalizability. For instance, a diffusion
model pre-trained on VGGFace2 attains a 46\% quality improvement through
fine-tuning with merely 10\% data from a different distribution. Moreover, our
method shows the potential to serve as a training paradigm for enhancing the
privacy protection capabilities of diffusion models. Our code is available at
\url{https://github.com/jiachenlei/maskdm}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jiachen Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1"&gt;Peng Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ba_Z/0/1/0/all/0/1"&gt;Zhongjie Ba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1"&gt;Kui Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Networks for Forecasting Multivariate Realized Volatility with Spillover Effects. (arXiv:2308.01419v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2308.01419</id>
        <link href="http://arxiv.org/abs/2308.01419"/>
        <updated>2023-08-05T00:48:26.803Z</updated>
        <summary type="html"><![CDATA[We present a novel methodology for modeling and forecasting multivariate
realized volatilities using customized graph neural networks to incorporate
spillover effects across stocks. The proposed model offers the benefits of
incorporating spillover effects from multi-hop neighbors, capturing nonlinear
relationships, and flexible training with different loss functions. Our
empirical findings provide compelling evidence that incorporating spillover
effects from multi-hop neighbors alone does not yield a clear advantage in
terms of predictive accuracy. However, modeling nonlinear spillover effects
enhances the forecasting accuracy of realized volatilities, particularly for
short-term horizons of up to one week. Moreover, our results consistently
indicate that training with the Quasi-likelihood loss leads to substantial
improvements in model performance compared to the commonly-used mean squared
error. A comprehensive series of empirical evaluations in alternative settings
confirm the robustness of our results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Pu_X/0/1/0/all/0/1"&gt;Xingyue Pu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Cucuringu_M/0/1/0/all/0/1"&gt;Mihai Cucuringu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xiaowen Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Neural Network Warm-Start Approach for the Inverse Acoustic Obstacle Scattering Problem. (arXiv:2212.08736v3 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.08736</id>
        <link href="http://arxiv.org/abs/2212.08736"/>
        <updated>2023-08-05T00:48:26.803Z</updated>
        <summary type="html"><![CDATA[We consider the inverse acoustic obstacle problem for sound-soft star-shaped
obstacles in two dimensions wherein the boundary of the obstacle is determined
from measurements of the scattered field at a collection of receivers outside
the object. One of the standard approaches for solving this problem is to
reformulate it as an optimization problem: finding the boundary of the domain
that minimizes the $L^2$ distance between computed values of the scattered
field and the given measurement data. The optimization problem is
computationally challenging since the local set of convexity shrinks with
increasing frequency and results in an increasing number of local minima in the
vicinity of the true solution. In many practical experimental settings, low
frequency measurements are unavailable due to limitations of the experimental
setup or the sensors used for measurement. Thus, obtaining a good initial guess
for the optimization problem plays a vital role in this environment.

We present a neural network warm-start approach for solving the inverse
scattering problem, where an initial guess for the optimization problem is
obtained using a trained neural network. We demonstrate the effectiveness of
our method with several numerical examples. For high frequency problems, this
approach outperforms traditional iterative methods such as Gauss-Newton
initialized without any prior (i.e., initialized using a unit circle), or
initialized using the solution of a direct method such as the linear sampling
method. The algorithm remains robust to noise in the scattered field
measurements and also converges to the true solution for limited aperture data.
However, the number of training samples required to train the neural network
scales exponentially in frequency and the complexity of the obstacles
considered. We conclude with a discussion of this phenomenon and potential
directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiequn Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Rachh_M/0/1/0/all/0/1"&gt;Manas Rachh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Borges_C/0/1/0/all/0/1"&gt;Carlos Borges&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Telematics Combined Actuarial Neural Networks for Cross-Sectional and Longitudinal Claim Count Data. (arXiv:2308.01729v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2308.01729</id>
        <link href="http://arxiv.org/abs/2308.01729"/>
        <updated>2023-08-05T00:48:26.802Z</updated>
        <summary type="html"><![CDATA[We present novel cross-sectional and longitudinal claim count models for
vehicle insurance built upon the Combined Actuarial Neural Network (CANN)
framework proposed by Mario W\"uthrich and Michael Merz. The CANN approach
combines a classical actuarial model, such as a generalized linear model, with
a neural network. This blending of models results in a two-component model
comprising a classical regression model and a neural network part. The CANN
model leverages the strengths of both components, providing a solid foundation
and interpretability from the classical model while harnessing the flexibility
and capacity to capture intricate relationships and interactions offered by the
neural network. In our proposed models, we use well-known log-linear claim
count regression models for the classical regression part and a multilayer
perceptron (MLP) for the neural network part. The MLP part is used to process
telematics car driving data given as a vector characterizing the driving
behavior of each insured driver. In addition to the Poisson and negative
binomial distributions for cross-sectional data, we propose a procedure for
training our CANN model with a multivariate negative binomial (MVNB)
specification. By doing so, we introduce a longitudinal model that accounts for
the dependence between contracts from the same insured. Our results reveal that
the CANN models exhibit superior performance compared to log-linear models that
rely on manually engineered telematics features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Duval_F/0/1/0/all/0/1"&gt;Francis Duval&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Boucher_J/0/1/0/all/0/1"&gt;Jean-Philippe Boucher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pigeon_M/0/1/0/all/0/1"&gt;Mathieu Pigeon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification and Online Clustering of Zero-Day Malware. (arXiv:2305.00605v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.00605</id>
        <link href="http://arxiv.org/abs/2305.00605"/>
        <updated>2023-08-05T00:48:26.801Z</updated>
        <summary type="html"><![CDATA[A large amount of new malware is constantly being generated, which must not
only be distinguished from benign samples, but also classified into malware
families. For this purpose, investigating how existing malware families are
developed and examining emerging families need to be explored. This paper
focuses on the online processing of incoming malicious samples to assign them
to existing families or, in the case of samples from new families, to cluster
them. We experimented with seven prevalent malware families from the EMBER
dataset, four in the training set and three additional new families in the test
set. Based on the classification score of the multilayer perceptron, we
determined which samples would be classified and which would be clustered into
new malware families. We classified 97.21% of streaming data with a balanced
accuracy of 95.33%. Then, we clustered the remaining data using a
self-organizing map, achieving a purity from 47.61% for four clusters to 77.68%
for ten clusters. These results indicate that our approach has the potential to
be applied to the classification and clustering of zero-day malware into
malware families.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jureckova_O/0/1/0/all/0/1"&gt;Olha Jure&amp;#x10d;kov&amp;#xe1;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jurecek_M/0/1/0/all/0/1"&gt;Martin Jure&amp;#x10d;ek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1"&gt;Mark Stamp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Troia_F/0/1/0/all/0/1"&gt;Fabio Di Troia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lorencz_R/0/1/0/all/0/1"&gt;R&amp;#xf3;bert L&amp;#xf3;rencz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding the Optimum Design of Large Gas Engines Prechambers Using CFD and Bayesian Optimization. (arXiv:2308.01743v1 [cs.CE])]]></title>
        <id>http://arxiv.org/abs/2308.01743</id>
        <link href="http://arxiv.org/abs/2308.01743"/>
        <updated>2023-08-05T00:48:26.798Z</updated>
        <summary type="html"><![CDATA[The turbulent jet ignition concept using prechambers is a promising solution
to achieve stable combustion at lean conditions in large gas engines, leading
to high efficiency at low emission levels. Due to the wide range of design and
operating parameters for large gas engine prechambers, the preferred method for
evaluating different designs is computational fluid dynamics (CFD), as testing
in test bed measurement campaigns is time-consuming and expensive. However, the
significant computational time required for detailed CFD simulations due to the
complexity of solving the underlying physics also limits its applicability. In
optimization settings similar to the present case, i.e., where the evaluation
of the objective function(s) is computationally costly, Bayesian optimization
has largely replaced classical design-of-experiment. Thus, the present study
deals with the computationally efficient Bayesian optimization of large gas
engine prechambers design using CFD simulation. Reynolds-averaged-Navier-Stokes
simulations are used to determine the target values as a function of the
selected prechamber design parameters. The results indicate that the chosen
strategy is effective to find a prechamber design that achieves the desired
target values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Posch_S/0/1/0/all/0/1"&gt;Stefan Posch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gossnitzer_C/0/1/0/all/0/1"&gt;Clemens G&amp;#xf6;&amp;#xdf;nitzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohrhofer_F/0/1/0/all/0/1"&gt;Franz Rohrhofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_B/0/1/0/all/0/1"&gt;Bernhard C. Geiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wimmer_A/0/1/0/all/0/1"&gt;Andreas Wimmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving. (arXiv:2308.01471v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.01471</id>
        <link href="http://arxiv.org/abs/2308.01471"/>
        <updated>2023-08-05T00:48:26.797Z</updated>
        <summary type="html"><![CDATA[A self-driving vehicle (SDV) must be able to perceive its surroundings and
predict the future behavior of other traffic participants. Existing works
either perform object detection followed by trajectory forecasting of the
detected objects, or predict dense occupancy and flow grids for the whole
scene. The former poses a safety concern as the number of detections needs to
be kept low for efficiency reasons, sacrificing object recall. The latter is
computationally expensive due to the high-dimensionality of the output grid,
and suffers from the limited receptive field inherent to fully convolutional
networks. Furthermore, both approaches employ many computational resources
predicting areas or objects that might never be queried by the motion planner.
This motivates our unified approach to perception and future prediction that
implicitly represents occupancy and flow over time with a single neural
network. Our method avoids unnecessary computation, as it can be directly
queried by the motion planner at continuous spatio-temporal locations.
Moreover, we design an architecture that overcomes the limited receptive field
of previous explicit occupancy prediction methods by adding an efficient yet
effective global attention mechanism. Through extensive experiments in both
urban and highway settings, we demonstrate that our implicit model outperforms
the current state-of-the-art. For more information, visit the project website:
https://waabi.ai/research/implicito.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agro_B/0/1/0/all/0/1"&gt;Ben Agro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sykora_Q/0/1/0/all/0/1"&gt;Quinlan Sykora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casas_S/0/1/0/all/0/1"&gt;Sergio Casas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1"&gt;Raquel Urtasun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confident Neural Network Regression with Bootstrapped Deep Ensembles. (arXiv:2202.10903v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.10903</id>
        <link href="http://arxiv.org/abs/2202.10903"/>
        <updated>2023-08-05T00:48:26.793Z</updated>
        <summary type="html"><![CDATA[With the rise of the popularity and usage of neural networks, trustworthy
uncertainty estimation is becoming increasingly essential. One of the most
prominent uncertainty estimation methods is Deep Ensembles (Lakshminarayanan et
al., 2017) . A classical parametric model has uncertainty in the parameters due
to the fact that the data on which the model is build is a random sample. A
modern neural network has an additional uncertainty component since the
optimization of the network is random. Lakshminarayanan et al. (2017) noted
that Deep Ensembles do not incorporate the classical uncertainty induced by the
effect of finite data. In this paper, we present a computationally cheap
extension of Deep Ensembles for the regression setting, called Bootstrapped
Deep Ensembles, that explicitly takes this classical effect of finite data into
account using a modified version of the parametric bootstrap. We demonstrate
through an experimental study that our method significantly improves upon
standard Deep Ensembles]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sluijterman_L/0/1/0/all/0/1"&gt;Laurens Sluijterman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cator_E/0/1/0/all/0/1"&gt;Eric Cator&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Heskes_T/0/1/0/all/0/1"&gt;Tom Heskes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Deep Learning for Tumor Dynamic Modeling and Overall Survival Prediction using Neural-ODE. (arXiv:2308.01362v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2308.01362</id>
        <link href="http://arxiv.org/abs/2308.01362"/>
        <updated>2023-08-05T00:48:26.791Z</updated>
        <summary type="html"><![CDATA[While tumor dynamic modeling has been widely applied to support the
development of oncology drugs, there remains a need to increase predictivity,
enable personalized therapy, and improve decision-making. We propose the use of
Tumor Dynamic Neural-ODE (TDNODE) as a pharmacology-informed neural network to
enable model discovery from longitudinal tumor size data. We show that TDNODE
overcomes a key limitation of existing models in its ability to make unbiased
predictions from truncated data. The encoder-decoder architecture is designed
to express an underlying dynamical law which possesses the fundamental property
of generalized homogeneity with respect to time. Thus, the modeling formalism
enables the encoder output to be interpreted as kinetic rate metrics, with
inverse time as the physical unit. We show that the generated metrics can be
used to predict patients' overall survival (OS) with high accuracy. The
proposed modeling formalism provides a principled way to integrate multimodal
dynamical datasets in oncology disease modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Laurie_M/0/1/0/all/0/1"&gt;Mark Laurie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lu_J/0/1/0/all/0/1"&gt;James Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auxiliary Cross-Modal Representation Learning with Triplet Loss Functions for Online Handwriting Recognition. (arXiv:2202.07901v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.07901</id>
        <link href="http://arxiv.org/abs/2202.07901"/>
        <updated>2023-08-05T00:48:26.789Z</updated>
        <summary type="html"><![CDATA[Cross-modal representation learning learns a shared embedding between two or
more modalities to improve performance in a given task compared to using only
one of the modalities. Cross-modal representation learning from different data
types -- such as images and time-series data (e.g., audio or text data) --
requires a deep metric learning loss that minimizes the distance between the
modality embeddings. In this paper, we propose to use the contrastive or
triplet loss, which uses positive and negative identities to create sample
pairs with different labels, for cross-modal representation learning between
image and time-series modalities (CMR-IS). By adapting the triplet loss for
cross-modal representation learning, higher accuracy in the main (time-series
classification) task can be achieved by exploiting additional information of
the auxiliary (image classification) task. We present a triplet loss with a
dynamic margin for single label and sequence-to-sequence classification tasks.
We perform extensive evaluations on synthetic image and time-series data, and
on data for offline handwriting recognition (HWR) and on online HWR from
sensor-enhanced pens for classifying written words. Our experiments show an
improved classification accuracy, faster convergence, and better
generalizability due to an improved cross-modal representation. Furthermore,
the more suitable generalizability leads to a better adaptability between
writers for online HWR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ott_F/0/1/0/all/0/1"&gt;Felix Ott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rugamer_D/0/1/0/all/0/1"&gt;David R&amp;#xfc;gamer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heublein_L/0/1/0/all/0/1"&gt;Lucas Heublein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1"&gt;Bernd Bischl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mutschler_C/0/1/0/all/0/1"&gt;Christopher Mutschler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Classification. (arXiv:2305.10406v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.10406</id>
        <link href="http://arxiv.org/abs/2305.10406"/>
        <updated>2023-08-05T00:48:26.786Z</updated>
        <summary type="html"><![CDATA[We present a latent variable generalisation of neural network softmax
classification trained with cross-entropy loss, referred to as variational
classification (VC). Our approach offers a novel probabilistic perspective on
the highly familiar softmax classification model, to which it relates similarly
to how variational and traditional autoencoders relate. We derive a training
objective based on the evidence lower bound (ELBO) that is non-trivial to
optimize, and therefore propose an adversarial approach to maximise it. We show
that VC addresses an inherent inconsistency within softmax classification,
whilst also allowing more flexible choices of prior distributions in the latent
space in place of implicit assumptions revealed within off-the-shelf softmax
classifiers. Empirical evaluation on image and text classification datasets
demonstrates that variational classification maintains prediction accuracy
while improving other desirable properties such as calibration and adversarial
robustness, particularly under distribution shift and low data settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1"&gt;Shehzaad Dhuliawala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1"&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allen_C/0/1/0/all/0/1"&gt;Carl Allen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MARLIM: Multi-Agent Reinforcement Learning for Inventory Management. (arXiv:2308.01649v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01649</id>
        <link href="http://arxiv.org/abs/2308.01649"/>
        <updated>2023-08-05T00:48:26.784Z</updated>
        <summary type="html"><![CDATA[Maintaining a balance between the supply and demand of products by optimizing
replenishment decisions is one of the most important challenges in the supply
chain industry. This paper presents a novel reinforcement learning framework
called MARLIM, to address the inventory management problem for a single-echelon
multi-products supply chain with stochastic demands and lead-times. Within this
context, controllers are developed through single or multiple agents in a
cooperative setting. Numerical experiments on real data demonstrate the
benefits of reinforcement learning methods over traditional baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leluc_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Leluc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadoche_E/0/1/0/all/0/1"&gt;Elie Kadoche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertoncello_A/0/1/0/all/0/1"&gt;Antoine Bertoncello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gourvenec_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Gourv&amp;#xe9;nec&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking Adaptative Variational Quantum Algorithms on QUBO Instances. (arXiv:2308.01789v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2308.01789</id>
        <link href="http://arxiv.org/abs/2308.01789"/>
        <updated>2023-08-05T00:48:26.784Z</updated>
        <summary type="html"><![CDATA[In recent years, Variational Quantum Algorithms (VQAs) have emerged as a
promising approach for solving optimization problems on quantum computers in
the NISQ era. However, one limitation of VQAs is their reliance on
fixed-structure circuits, which may not be taylored for specific problems or
hardware configurations. A leading strategy to address this issue are
Adaptative VQAs, which dynamically modify the circuit structure by adding and
removing gates, and optimize their parameters during the training. Several
Adaptative VQAs, based on heuristics such as circuit shallowness, entanglement
capability and hardware compatibility, have already been proposed in the
literature, but there is still lack of a systematic comparison between the
different methods. In this paper, we aim to fill this gap by analyzing three
Adaptative VQAs: Evolutionary Variational Quantum Eigensolver (EVQE), Variable
Ansatz (VAns), already proposed in the literature, and Random Adapt-VQE
(RA-VQE), a random approach we introduce as a baseline. In order to compare
these algorithms to traditional VQAs, we also include the Quantum Approximate
Optimization Algorithm (QAOA) in our analysis. We apply these algorithms to
QUBO problems and study their performance by examining the quality of the
solutions found and the computational times required. Additionally, we
investigate how the choice of the hyperparameters can impact the overall
performance of the algorithms, highlighting the importance of selecting an
appropriate methodology for hyperparameter tuning. Our analysis sets benchmarks
for Adaptative VQAs designed for near-term quantum devices and provides
valuable insights to guide future research in this area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Turati_G/0/1/0/all/0/1"&gt;Gloria Turati&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Dacrema_M/0/1/0/all/0/1"&gt;Maurizio Ferrari Dacrema&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Cremonesi_P/0/1/0/all/0/1"&gt;Paolo Cremonesi&lt;/a&gt; (1) ((1) Politecnico di Milano)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Calibration in Dense Classification with Adaptive Label Perturbation. (arXiv:2307.13539v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.13539</id>
        <link href="http://arxiv.org/abs/2307.13539"/>
        <updated>2023-08-05T00:48:26.783Z</updated>
        <summary type="html"><![CDATA[For safety-related applications, it is crucial to produce trustworthy deep
neural networks whose prediction is associated with confidence that can
represent the likelihood of correctness for subsequent decision-making.
Existing dense binary classification models are prone to being over-confident.
To improve model calibration, we propose Adaptive Stochastic Label Perturbation
(ASLP) which learns a unique label perturbation level for each training image.
ASLP employs our proposed Self-Calibrating Binary Cross Entropy (SC-BCE) loss,
which unifies label perturbation processes including stochastic approaches
(like DisturbLabel), and label smoothing, to correct calibration while
maintaining classification rates. ASLP follows Maximum Entropy Inference of
classic statistical mechanics to maximise prediction entropy with respect to
missing information. It performs this while: (1) preserving classification
accuracy on known data as a conservative solution, or (2) specifically improves
model calibration degree by minimising the gap between the prediction accuracy
and expected confidence of the target training label. Extensive results
demonstrate that ASLP can significantly improve calibration degrees of dense
binary classification models on both in-distribution and out-of-distribution
data. The code is available on https://github.com/Carlisle-Liu/ASLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiawei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1"&gt;Changkun Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1"&gt;Ruikai Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaihao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1"&gt;Nick Barnes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models. (arXiv:2306.05357v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.05357</id>
        <link href="http://arxiv.org/abs/2306.05357"/>
        <updated>2023-08-05T00:48:26.781Z</updated>
        <summary type="html"><![CDATA[Text-to-image generative models have enabled high-resolution image synthesis
across different domains, but require users to specify the content they wish to
generate. In this paper, we consider the inverse problem -- given a collection
of different images, can we discover the generative concepts that represent
each image? We present an unsupervised approach to discover generative concepts
from a collection of images, disentangling different art styles in paintings,
objects, and lighting from kitchen scenes, and discovering image classes given
ImageNet images. We show how such generative concepts can accurately represent
the content of images, be recombined and composed to generate new artistic and
hybrid images, and be further used as a representation for downstream
classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1"&gt;Nan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yilun Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1"&gt;Antonio Torralba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.16680</id>
        <link href="http://arxiv.org/abs/2307.16680"/>
        <updated>2023-08-05T00:48:26.781Z</updated>
        <summary type="html"><![CDATA[Diffusion models and large language models have emerged as leading-edge
generative models and have sparked a revolutionary impact on various aspects of
human life. However, the practical implementation of these models has also
exposed inherent risks, highlighting their dual nature and raising concerns
regarding their trustworthiness. Despite the abundance of literature on this
subject, a comprehensive survey specifically delving into the intersection of
large-scale generative models and their trustworthiness remains largely absent.
To bridge this gap, This paper investigates both the long-standing and emerging
threats associated with these models across four fundamental dimensions:
privacy, security, fairness, and responsibility. In this way, we construct an
extensive map outlining the trustworthiness of these models, while also
providing practical recommendations and identifying future directions. These
efforts are crucial for promoting the trustworthy deployment of these models,
ultimately benefiting society as a whole.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1"&gt;Mingyuan Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Cen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jun Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Online Private Learning of Convex Nondecomposable Objectives. (arXiv:2206.07944v4 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2206.07944</id>
        <link href="http://arxiv.org/abs/2206.07944"/>
        <updated>2023-08-05T00:48:26.780Z</updated>
        <summary type="html"><![CDATA[We deal with a general distributed constrained online learning problem with
privacy over time-varying networks, where a class of nondecomposable objectives
are considered. Under this setting, each node only controls a part of the
global decision, and the goal of all nodes is to collaboratively minimize the
global cost over a time horizon $T$ while guarantees the security of the
transmitted information. For such problems, we first design a novel generic
algorithm framework, named as DPSDA, of differentially private distributed
online learning using the Laplace mechanism and the stochastic variants of dual
averaging method. Note that in the dual updates, all nodes of DPSDA employ the
noise-corrupted gradients for more generality. Then, we propose two algorithms,
named as DPSDA-C and DPSDA-PS, under this framework. In DPSDA-C, the nodes
implement a circulation-based communication in the primal updates so as to
alleviate the disagreements over time-varying undirected networks. In addition,
for the extension to time-varying directed ones, the nodes implement the
broadcast-based push-sum dynamics in DPSDA-PS, which can achieve average
consensus over arbitrary directed networks. Theoretical results show that both
algorithms attain an expected regret upper bound in $\mathcal{O}( \sqrt{T} )$
when the objective function is convex, which matches the best utility
achievable by cutting-edge algorithms. Finally, numerical experiment results on
both synthetic and real-world datasets verify the effectiveness of our
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Huqiang Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Liao_X/0/1/0/all/0/1"&gt;Xiaofeng Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_H/0/1/0/all/0/1"&gt;Huaqing Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reconstructing Turbulent Flows Using Physics-Aware Spatio-Temporal Dynamics and Test-Time Refinement. (arXiv:2304.12130v2 [physics.flu-dyn] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.12130</id>
        <link href="http://arxiv.org/abs/2304.12130"/>
        <updated>2023-08-05T00:48:26.779Z</updated>
        <summary type="html"><![CDATA[Simulating turbulence is critical for many societally important applications
in aerospace engineering, environmental science, the energy industry, and
biomedicine. Large eddy simulation (LES) has been widely used as an alternative
to direct numerical simulation (DNS) for simulating turbulent flows due to its
reduced computational cost. However, LES is unable to capture all of the scales
of turbulent transport accurately. Reconstructing DNS from low-resolution LES
is critical for many scientific and engineering disciplines, but it poses many
challenges to existing super-resolution methods due to the spatio-temporal
complexity of turbulent flows. In this work, we propose a new physics-guided
neural network for reconstructing the sequential DNS from low-resolution LES
data. The proposed method leverages the partial differential equation that
underlies the flow dynamics in the design of spatio-temporal model
architecture. A degradation-based refinement method is also developed to
enforce physical constraints and further reduce the accumulated reconstruction
errors over long periods. The results on two different types of turbulent flow
data confirm the superiority of the proposed method in reconstructing the
high-resolution DNS data and preserving the physical characteristics of flow
transport.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shengyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Bao_T/0/1/0/all/0/1"&gt;Tianshu Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Givi_P/0/1/0/all/0/1"&gt;Peyman Givi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Can Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xiaowei Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Morphological Classification of Extragalactic Radio Sources Using Gradient Boosting Methods. (arXiv:2304.12729v2 [astro-ph.IM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.12729</id>
        <link href="http://arxiv.org/abs/2304.12729"/>
        <updated>2023-08-05T00:48:26.778Z</updated>
        <summary type="html"><![CDATA[The field of radio astronomy is witnessing a boom in the amount of data
produced per day due to newly commissioned radio telescopes. One of the most
crucial problems in this field is the automatic classification of extragalactic
radio sources based on their morphologies. Most recent contributions in the
field of morphological classification of extragalactic radio sources have
proposed classifiers based on convolutional neural networks. Alternatively,
this work proposes gradient boosting machine learning methods accompanied by
principal component analysis as data-efficient alternatives to convolutional
neural networks. Recent findings have shown the efficacy of gradient boosting
methods in outperforming deep learning methods for classification problems with
tabular data. The gradient boosting methods considered in this work are based
on the XGBoost, LightGBM, and CatBoost implementations. This work also studies
the effect of dataset size on classifier performance. A three-class
classification problem is considered in this work based on the three main
Fanaroff-Riley classes: class 0, class I, and class II, using radio sources
from the Best-Heckman sample. All three proposed gradient boosting methods
outperformed a state-of-the-art convolutional neural networks-based classifier
using less than a quarter of the number of images, with CatBoost having the
highest accuracy. This was mainly due to the superior accuracy of gradient
boosting methods in classifying Fanaroff-Riley class II sources, with
3$\unicode{x2013}$4% higher recall.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Darya_A/0/1/0/all/0/1"&gt;Abdollah Masoud Darya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Fernini_I/0/1/0/all/0/1"&gt;Ilias Fernini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Vellasco_M/0/1/0/all/0/1"&gt;Marley Vellasco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Hussain_A/0/1/0/all/0/1"&gt;Abir Hussain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recent advancement in Disease Diagnostic using machine learning: Systematic survey of decades, comparisons, and challenges. (arXiv:2308.01319v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01319</id>
        <link href="http://arxiv.org/abs/2308.01319"/>
        <updated>2023-08-05T00:48:26.777Z</updated>
        <summary type="html"><![CDATA[Computer-aided diagnosis (CAD), a vibrant medical imaging research field, is
expanding quickly. Because errors in medical diagnostic systems might lead to
seriously misleading medical treatments, major efforts have been made in recent
years to improve computer-aided diagnostics applications. The use of machine
learning in computer-aided diagnosis is crucial. A simple equation may result
in a false indication of items like organs. Therefore, learning from examples
is a vital component of pattern recognition. Pattern recognition and machine
learning in the biomedical area promise to increase the precision of disease
detection and diagnosis. They also support the decision-making process's
objectivity. Machine learning provides a practical method for creating elegant
and autonomous algorithms to analyze high-dimensional and multimodal
bio-medical data. This review article examines machine-learning algorithms for
detecting diseases, including hepatitis, diabetes, liver disease, dengue fever,
and heart disease. It draws attention to the collection of machine learning
techniques and algorithms employed in studying conditions and the ensuing
decision-making process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tajidini_F/0/1/0/all/0/1"&gt;Farzaneh Tajidini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kheiri_M/0/1/0/all/0/1"&gt;Mohammad-Javad Kheiri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Discovery from Temporal Data: An Overview and New Perspectives. (arXiv:2303.10112v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.10112</id>
        <link href="http://arxiv.org/abs/2303.10112"/>
        <updated>2023-08-05T00:48:26.777Z</updated>
        <summary type="html"><![CDATA[Temporal data, representing chronological observations of complex systems,
has always been a typical data structure that can be widely generated by many
domains, such as industry, medicine and finance. Analyzing this type of data is
extremely valuable for various applications. Thus, different temporal data
analysis tasks, eg, classification, clustering and prediction, have been
proposed in the past decades. Among them, causal discovery, learning the causal
relations from temporal data, is considered an interesting yet critical task
and has attracted much research attention. Existing causal discovery works can
be divided into two highly correlated categories according to whether the
temporal data is calibrated, ie, multivariate time series causal discovery, and
event sequence causal discovery. However, most previous surveys are only
focused on the time series causal discovery and ignore the second category. In
this paper, we specify the correlation between the two categories and provide a
systematical overview of existing solutions. Furthermore, we provide public
datasets, evaluation metrics and new perspectives for temporal data causal
discovery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1"&gt;Chang Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_D/0/1/0/all/0/1"&gt;Di Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chuzhe Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenbin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1"&gt;Jingping Bi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Price-Aware Deep Learning for Electricity Markets. (arXiv:2308.01436v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01436</id>
        <link href="http://arxiv.org/abs/2308.01436"/>
        <updated>2023-08-05T00:48:26.759Z</updated>
        <summary type="html"><![CDATA[While deep learning gradually penetrates operational planning, its inherent
prediction errors may significantly affect electricity prices. This letter
examines how prediction errors propagate into electricity prices, revealing
notable pricing errors and their spatial disparity in congested power systems.
To improve fairness, we propose to embed electricity market-clearing
optimization as a deep learning layer. Differentiating through this layer
allows for balancing between prediction and pricing errors, as oppose to
minimizing prediction errors alone. This layer implicitly optimizes fairness
and controls the spatial distribution of price errors across the system. We
showcase the price-aware deep learning in the nexus of wind power forecasting
and short-term electricity market clearing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dvorkin_V/0/1/0/all/0/1"&gt;Vladimir Dvorkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fioretto_F/0/1/0/all/0/1"&gt;Ferdinando Fioretto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and Gradient Accumulation. (arXiv:2011.11233v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11233</id>
        <link href="http://arxiv.org/abs/2011.11233"/>
        <updated>2023-08-05T00:48:26.757Z</updated>
        <summary type="html"><![CDATA[Albeit being a prevalent architecture searching approach, differentiable
architecture search (DARTS) is largely hindered by its substantial memory cost
since the entire supernet resides in the memory. This is where the single-path
DARTS comes in, which only chooses a single-path submodel at each step. While
being memory-friendly, it also comes with low computational costs. Nonetheless,
we discover a critical issue of single-path DARTS that has not been primarily
noticed. Namely, it also suffers from severe performance collapse since too
many parameter-free operations like skip connections are derived, just like
DARTS does. In this paper, we propose a new algorithm called RObustifying
Memory-Efficient NAS (ROME) to give a cure. First, we disentangle the topology
search from the operation search to make searching and evaluation consistent.
We then adopt Gumbel-Top2 reparameterization and gradient accumulation to
robustify the unwieldy bi-level optimization. We verify ROME extensively across
15 benchmarks to demonstrate its effectiveness and robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaoxing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1"&gt;Xiangxiang Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yuda Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhexi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaokang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stable and consistent density-based clustering via multiparameter persistence. (arXiv:2005.09048v3 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.09048</id>
        <link href="http://arxiv.org/abs/2005.09048"/>
        <updated>2023-08-05T00:48:26.750Z</updated>
        <summary type="html"><![CDATA[We consider the degree-Rips construction from topological data analysis,
which provides a density-sensitive, multiparameter hierarchical clustering
algorithm. We analyze its stability to perturbations of the input data using
the correspondence-interleaving distance, a metric for hierarchical clusterings
that we introduce. Taking certain one-parameter slices of degree-Rips recovers
well-known methods for density-based clustering, but we show that these methods
are unstable. However, we prove that degree-Rips, as a multiparameter object,
is stable, and we propose an alternative approach for taking slices of
degree-Rips, which yields a one-parameter hierarchical clustering algorithm
with better stability properties. We prove that this algorithm is consistent,
using the correspondence-interleaving distance. We provide an algorithm for
extracting a single clustering from one-parameter hierarchical clusterings,
which is stable with respect to the correspondence-interleaving distance. And,
we integrate these methods into a pipeline for density-based clustering, which
we call Persistable. Adapting tools from multiparameter persistent homology, we
propose visualization tools that guide the selection of all parameters of the
pipeline. We demonstrate Persistable on benchmark datasets, showing that it
identifies multi-scale cluster structure in data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Rolle_A/0/1/0/all/0/1"&gt;Alexander Rolle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Scoccola_L/0/1/0/all/0/1"&gt;Luis Scoccola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness in Recommendation: Foundations, Methods and Applications. (arXiv:2205.13619v6 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2205.13619</id>
        <link href="http://arxiv.org/abs/2205.13619"/>
        <updated>2023-08-05T00:48:26.750Z</updated>
        <summary type="html"><![CDATA[As one of the most pervasive applications of machine learning, recommender
systems are playing an important role on assisting human decision making. The
satisfaction of users and the interests of platforms are closely related to the
quality of the generated recommendation results. However, as a highly
data-driven system, recommender system could be affected by data or algorithmic
bias and thus generate unfair results, which could weaken the reliance of the
systems. As a result, it is crucial to address the potential unfairness
problems in recommendation settings. Recently, there has been growing attention
on fairness considerations in recommender systems with more and more literature
on approaches to promote fairness in recommendation. However, the studies are
rather fragmented and lack a systematic organization, thus making it difficult
to penetrate for new researchers to the domain. This motivates us to provide a
systematic survey of existing works on fairness in recommendation. This survey
focuses on the foundations for fairness in recommendation literature. It first
presents a brief introduction about fairness in basic machine learning tasks
such as classification and ranking in order to provide a general overview of
fairness research, as well as introduce the more complex situations and
challenges that need to be considered when studying fairness in recommender
systems. After that, the survey will introduce fairness in recommendation with
a focus on the taxonomies of current fairness definitions, the typical
techniques for improving fairness, as well as the datasets for fairness studies
in recommendation. The survey also talks about the challenges and opportunities
in fairness research with the hope of promoting the fair recommendation
research area and beyond.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hanxiong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shuyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1"&gt;Juntao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shuchang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfeng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mlinear: Rethink the Linear Model for Time-series Forecasting. (arXiv:2305.04800v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.04800</id>
        <link href="http://arxiv.org/abs/2305.04800"/>
        <updated>2023-08-05T00:48:26.747Z</updated>
        <summary type="html"><![CDATA[Recently, significant advancements have been made in time-series forecasting
research, with an increasing focus on analyzing the nature of time-series data,
e.g, channel-independence (CI) and channel-dependence (CD), rather than solely
focusing on designing sophisticated forecasting models. However, current
research has primarily focused on either CI or CD in isolation, and the
challenge of effectively combining these two opposing properties to achieve a
synergistic effect remains an unresolved issue. In this paper, we carefully
examine the opposing properties of CI and CD, and raise a practical question
that has not been effectively answered, e.g.,"How to effectively mix the CI and
CD properties of time series to achieve better predictive performance?" To
answer this question, we propose Mlinear (MIX-Linear), a simple yet effective
method based mainly on linear layers. The design philosophy of Mlinear mainly
includes two aspects:(1) dynamically tuning the CI and CD properties based on
the time semantics of different input time series, and (2) providing deep
supervision to adjust the individual performance of the "CI predictor" and "CD
predictor". In addition, empirically, we introduce a new loss function that
significantly outperforms the widely used mean squared error (MSE) on multiple
datasets. Experiments on time-series datasets covering multiple fields and
widely used have demonstrated the superiority of our method over PatchTST which
is the lateset Transformer-based method in terms of the MSE and MAE metrics on
7 datasets with identical sequence inputs (336 or 512). Specifically, our
method significantly outperforms PatchTST with a ratio of 21:3 at 336 sequence
length input and 29:10 at 512 sequence length input. Additionally, our approach
has a 10 $\times$ efficiency advantage at the unit level, taking into account
both training and inference times.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1"&gt;Xiangxu Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chuhao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jianing Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Federated Learning in Wireless Networks: Pruning Tackles Bandwidth Scarcity and System Heterogeneity. (arXiv:2308.01562v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2308.01562</id>
        <link href="http://arxiv.org/abs/2308.01562"/>
        <updated>2023-08-05T00:48:26.745Z</updated>
        <summary type="html"><![CDATA[While a practical wireless network has many tiers where end users do not
directly communicate with the central server, the users' devices have limited
computation and battery powers, and the serving base station (BS) has a fixed
bandwidth. Owing to these practical constraints and system models, this paper
leverages model pruning and proposes a pruning-enabled hierarchical federated
learning (PHFL) in heterogeneous networks (HetNets). We first derive an upper
bound of the convergence rate that clearly demonstrates the impact of the model
pruning and wireless communications between the clients and the associated BS.
Then we jointly optimize the model pruning ratio, central processing unit (CPU)
frequency and transmission power of the clients in order to minimize the
controllable terms of the convergence bound under strict delay and energy
constraints. However, since the original problem is not convex, we perform
successive convex approximation (SCA) and jointly optimize the parameters for
the relaxed convex problem. Through extensive simulation, we validate the
effectiveness of our proposed PHFL algorithm in terms of test accuracy, wall
clock time, energy consumption and bandwidth requirement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pervej_M/0/1/0/all/0/1"&gt;Md Ferdous Pervej&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jin_R/0/1/0/all/0/1"&gt;Richeng Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dai_H/0/1/0/all/0/1"&gt;Huaiyu Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-VR: A Deep Learning COVID-19 Classification Model Using Volume-Rendered Computer Tomography. (arXiv:2308.01433v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2308.01433</id>
        <link href="http://arxiv.org/abs/2308.01433"/>
        <updated>2023-08-05T00:48:26.742Z</updated>
        <summary type="html"><![CDATA[The COVID-19 pandemic presented numerous challenges to healthcare systems
worldwide. Given that lung infections are prevalent among COVID-19 patients,
chest Computer Tomography (CT) scans have frequently been utilized as an
alternative method for identifying COVID-19 conditions and various other types
of pulmonary diseases. Deep learning architectures have emerged to automate the
identification of pulmonary disease types by leveraging CT scan slices as
inputs for classification models. This paper introduces COVID-VR, a novel
approach for classifying pulmonary diseases based on volume rendering images of
the lungs captured from multiple angles, thereby providing a comprehensive view
of the entire lung in each image. To assess the effectiveness of our proposal,
we compared it against competing strategies utilizing both private data
obtained from partner hospitals and a publicly available dataset. The results
demonstrate that our approach effectively identifies pulmonary lesions and
performs competitively when compared to slice-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Romero_N/0/1/0/all/0/1"&gt;Noemi Maritza L. Romero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vasconcellos_R/0/1/0/all/0/1"&gt;Ricco Vasconcellos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mendoza_M/0/1/0/all/0/1"&gt;Mariana R. Mendoza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Comba_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o L. D. Comba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reasoning in Large Language Models Through Symbolic Math Word Problems. (arXiv:2308.01906v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2308.01906</id>
        <link href="http://arxiv.org/abs/2308.01906"/>
        <updated>2023-08-05T00:48:26.741Z</updated>
        <summary type="html"><![CDATA[Large language models (LLMs) have revolutionized NLP by solving downstream
tasks with little to no labeled data. Despite their versatile abilities, the
larger question of their ability to reason remains ill-understood. This paper
addresses reasoning in math word problems (MWPs) by studying symbolic versions
of the numeric problems, since a symbolic expression is a "concise explanation"
of the numeric answer. We create and use a symbolic version of the SVAMP
dataset and find that GPT-3's davinci-002 model also has good zero-shot
accuracy on symbolic MWPs. To evaluate the faithfulness of the model's
reasoning, we go beyond accuracy and additionally evaluate the alignment
between the final answer and the outputted reasoning, which correspond to
numeric and symbolic answers respectively for MWPs. We explore a self-prompting
approach to encourage the symbolic reasoning to align with the numeric answer,
thus equipping the LLM with the ability to provide a concise and verifiable
reasoning and making it more interpretable. Surprisingly, self-prompting also
improves the symbolic accuracy to be higher than both the numeric and symbolic
accuracies, thus providing an ensembling effect. The SVAMP_Sym dataset will be
released for future research on symbolic math problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gaur_V/0/1/0/all/0/1"&gt;Vedant Gaur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saunshi_N/0/1/0/all/0/1"&gt;Nikunj Saunshi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hard Adversarial Example Mining for Improving Robust Fairness. (arXiv:2308.01823v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01823</id>
        <link href="http://arxiv.org/abs/2308.01823"/>
        <updated>2023-08-05T00:48:26.740Z</updated>
        <summary type="html"><![CDATA[Adversarial training (AT) is widely considered the state-of-the-art technique
for improving the robustness of deep neural networks (DNNs) against adversarial
examples (AE). Nevertheless, recent studies have revealed that adversarially
trained models are prone to unfairness problems, restricting their
applicability. In this paper, we empirically observe that this limitation may
be attributed to serious adversarial confidence overfitting, i.e., certain
adversarial examples with overconfidence. To alleviate this problem, we propose
HAM, a straightforward yet effective framework via adaptive Hard Adversarial
example Mining.HAM concentrates on mining hard adversarial examples while
discarding the easy ones in an adaptive fashion. Specifically, HAM identifies
hard AEs in terms of their step sizes needed to cross the decision boundary
when calculating loss value. Besides, an early-dropping mechanism is
incorporated to discard the easy examples at the initial stages of AE
generation, resulting in efficient AT. Extensive experimental results on
CIFAR-10, SVHN, and Imagenette demonstrate that HAM achieves significant
improvement in robust fairness while reducing computational cost compared to
several state-of-the-art adversarial training methods. The code will be made
publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chenhao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1"&gt;Xiang Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yulong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chao Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Run Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1"&gt;Liming Fang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales. (arXiv:2308.01320v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01320</id>
        <link href="http://arxiv.org/abs/2308.01320"/>
        <updated>2023-08-05T00:48:26.738Z</updated>
        <summary type="html"><![CDATA[ChatGPT-like models have revolutionized various applications in artificial
intelligence, from summarization and coding to translation, matching or even
surpassing human performance. However, the current landscape lacks an
accessible, efficient, and cost-effective end-to-end RLHF (Reinforcement
Learning with Human Feedback) training pipeline for these powerful models,
particularly when training at the scale of billions of parameters. This paper
introduces DeepSpeed-Chat, a novel system that democratizes RLHF training,
making it accessible to the AI community. DeepSpeed-Chat offers three key
capabilities: an easy-to-use training and inference experience for ChatGPT-like
models, a DeepSpeed-RLHF pipeline that replicates the training pipeline from
InstructGPT, and a robust DeepSpeed-RLHF system that combines various
optimizations for training and inference in a unified way. The system delivers
unparalleled efficiency and scalability, enabling training of models with
hundreds of billions of parameters in record time and at a fraction of the
cost. With this development, DeepSpeed-Chat paves the way for broader access to
advanced RLHF training, even for data scientists with limited resources,
thereby fostering innovation and further development in the field of AI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zhewei Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aminabadi_R/0/1/0/all/0/1"&gt;Reza Yazdani Aminabadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruwase_O/0/1/0/all/0/1"&gt;Olatunji Ruwase&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajbhandari_S/0/1/0/all/0/1"&gt;Samyam Rajbhandari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiaoxia Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awan_A/0/1/0/all/0/1"&gt;Ammar Ahmad Awan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasley_J/0/1/0/all/0/1"&gt;Jeff Rasley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Minjia Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Conglong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holmes_C/0/1/0/all/0/1"&gt;Connor Holmes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhongzhu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wyatt_M/0/1/0/all/0/1"&gt;Michael Wyatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1"&gt;Molly Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurilenko_L/0/1/0/all/0/1"&gt;Lev Kurilenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1"&gt;Heyang Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_M/0/1/0/all/0/1"&gt;Masahiro Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_S/0/1/0/all/0/1"&gt;Shuai Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shuaiwen Leon Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yuxiong He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning-based Prediction of Stress and Strain Maps in Arterial Walls for Improved Cardiovascular Risk Assessment. (arXiv:2308.01771v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01771</id>
        <link href="http://arxiv.org/abs/2308.01771"/>
        <updated>2023-08-05T00:48:26.734Z</updated>
        <summary type="html"><![CDATA[This study investigated the potential of end-to-end deep learning tools as a
more effective substitute for FEM in predicting stress-strain fields within 2D
cross sections of arterial wall. We first proposed a U-Net based fully
convolutional neural network (CNN) to predict the von Mises stress and strain
distribution based on the spatial arrangement of calcification within arterial
wall cross-sections. Further, we developed a conditional generative adversarial
network (cGAN) to enhance, particularly from the perceptual perspective, the
prediction accuracy of stress and strain field maps for arterial walls with
various calcification quantities and spatial configurations. On top of U-Net
and cGAN, we also proposed their ensemble approaches, respectively, to further
improve the prediction accuracy of field maps. Our dataset, consisting of input
and output images, was generated by implementing boundary conditions and
extracting stress-strain field maps. The trained U-Net models can accurately
predict von Mises stress and strain fields, with structural similarity index
scores (SSIM) of 0.854 and 0.830 and mean squared errors of 0.017 and 0.018 for
stress and strain, respectively, on a reserved test set. Meanwhile, the cGAN
models in a combination of ensemble and transfer learning techniques
demonstrate high accuracy in predicting von Mises stress and strain fields, as
evidenced by SSIM scores of 0.890 for stress and 0.803 for strain.
Additionally, mean squared errors of 0.008 for stress and 0.017 for strain
further support the model's performance on a designated test set. Overall, this
study developed a surrogate model for finite element analysis, which can
accurately and efficiently predict stress-strain fields of arterial walls
regardless of complex geometries and boundary conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shokrollahi1_Y/0/1/0/all/0/1"&gt;Yasin Shokrollahi1&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong1_P/0/1/0/all/0/1"&gt;Pengfei Dong1&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xianqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1"&gt;Linxia Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[No Agreement Without Loss: Learning and Social Choice in Peer Review. (arXiv:2211.02144v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.02144</id>
        <link href="http://arxiv.org/abs/2211.02144"/>
        <updated>2023-08-05T00:48:26.733Z</updated>
        <summary type="html"><![CDATA[In peer review systems, reviewers are often asked to evaluate various
features of submissions, such as technical quality or novelty. A score is given
to each of the predefined features and based on these the reviewer has to
provide an overall quantitative recommendation. It may be assumed that each
reviewer has her own mapping from the set of features to a recommendation, and
that different reviewers have different mappings in mind. This introduces an
element of arbitrariness known as commensuration bias. In this paper we discuss
a framework, introduced by Noothigattu, Shah and Procaccia, and then applied by
the organizers of the AAAI 2022 conference. Noothigattu, Shah and Procaccia
proposed to aggregate reviewer's mapping by minimizing certain loss functions,
and studied axiomatic properties of this approach, in the sense of social
choice theory. We challenge several of the results and assumptions used in
their work and report a number of negative results. On the one hand, we study a
trade-off between some of the axioms proposed and the ability of the method to
properly capture agreements of the majority of reviewers. On the other hand, we
show that dropping a certain unrealistic assumption has dramatic effects,
including causing the method to be discontinuous.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barcelo_P/0/1/0/all/0/1"&gt;Pablo Barcel&amp;#xf3;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duarte_M/0/1/0/all/0/1"&gt;Mauricio Duarte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rojas_C/0/1/0/all/0/1"&gt;Crist&amp;#xf3;bal Rojas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steifer_T/0/1/0/all/0/1"&gt;Tomasz Steifer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model. (arXiv:2308.01415v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2308.01415</id>
        <link href="http://arxiv.org/abs/2308.01415"/>
        <updated>2023-08-05T00:48:26.732Z</updated>
        <summary type="html"><![CDATA[At the beginning era of large language model, it is quite critical to
generate a high-quality financial dataset to fine-tune a large language model
for financial related tasks. Thus, this paper presents a carefully designed
data creation pipeline for this purpose. Particularly, we initiate a dialogue
between an AI investor and financial expert using ChatGPT and incorporate the
feedback of human financial experts, leading to the refinement of the dataset.
This pipeline yielded a robust instruction tuning dataset comprised of 103k
multi-turn chats. Extensive experiments have been conducted on this dataset to
evaluate the model's performance by adopting an external GPT-4 as the judge.
The promising experimental results verify that our approach led to significant
advancements in generating accurate, relevant, and financial-style responses
from AI models, and thus providing a powerful tool for applications within the
financial sector.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianning Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Junda Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaofeng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Noise Boosts DNN Generalization under Label Noise. (arXiv:2308.01609v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01609</id>
        <link href="http://arxiv.org/abs/2308.01609"/>
        <updated>2023-08-05T00:48:26.731Z</updated>
        <summary type="html"><![CDATA[The presence of label noise in the training data has a profound impact on the
generalization of deep neural networks (DNNs). In this study, we introduce and
theoretically demonstrate a simple feature noise method, which directly adds
noise to the features of training data, can enhance the generalization of DNNs
under label noise. Specifically, we conduct theoretical analyses to reveal that
label noise leads to weakened DNN generalization by loosening the PAC-Bayes
generalization bound, and feature noise results in better DNN generalization by
imposing an upper bound on the mutual information between the model weights and
the features, which constrains the PAC-Bayes generalization bound. Furthermore,
to ensure effective generalization of DNNs in the presence of label noise, we
conduct application analyses to identify the optimal types and levels of
feature noise to add for obtaining desirable label noise generalization.
Finally, extensive experimental results on several popular datasets demonstrate
the feature noise method can significantly enhance the label noise
generalization of the state-of-the-art label noise method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1"&gt;Lu Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1"&gt;Xiaoshuang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Heng Tao Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies. (arXiv:2308.01546v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2308.01546</id>
        <link href="http://arxiv.org/abs/2308.01546"/>
        <updated>2023-08-05T00:48:26.725Z</updated>
        <summary type="html"><![CDATA[Diffusion models have shown promising results in cross-modal generation
tasks, including text-to-image and text-to-audio generation. However,
generating music, as a special type of audio, presents unique challenges due to
limited availability of music data and sensitive issues related to copyright
and plagiarism. In this paper, to tackle these challenges, we first construct a
state-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusion
and AudioLDM architectures to the music domain. We achieve this by retraining
the contrastive language-audio pretraining model (CLAP) and the Hifi-GAN
vocoder, as components of MusicLDM, on a collection of music data samples.
Then, to address the limitations of training data and to avoid plagiarism, we
leverage a beat tracking model and propose two different mixup strategies for
data augmentation: beat-synchronous audio mixup and beat-synchronous latent
mixup, which recombine training audio directly or via a latent embeddings
space, respectively. Such mixup strategies encourage the model to interpolate
between musical training samples and generate new music within the convex hull
of the training data, making the generated music more diverse while still
staying faithful to the corresponding style. In addition to popular evaluation
metrics, we design several new evaluation metrics based on CLAP score to
demonstrate that our proposed MusicLDM and beat-synchronous mixup strategies
improve both the quality and novelty of generated music, as well as the
correspondence between input text and generated music.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Ke Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yusong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haohe Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nezhurina_M/0/1/0/all/0/1"&gt;Marianna Nezhurina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1"&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dubnov_S/0/1/0/all/0/1"&gt;Shlomo Dubnov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAP-sLDA: An Interpretable Interface for Exploring Unstructured Text. (arXiv:2308.01420v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2308.01420</id>
        <link href="http://arxiv.org/abs/2308.01420"/>
        <updated>2023-08-05T00:48:26.716Z</updated>
        <summary type="html"><![CDATA[A common way to explore text corpora is through low-dimensional projections
of the documents, where one hopes that thematically similar documents will be
clustered together in the projected space. However, popular algorithms for
dimensionality reduction of text corpora, like Latent Dirichlet Allocation
(LDA), often produce projections that do not capture human notions of document
similarity. We propose a semi-supervised human-in-the-loop LDA-based method for
learning topics that preserve semantically meaningful relationships between
documents in low-dimensional projections. On synthetic corpora, our method
yields more interpretable projections than baseline methods with only a
fraction of labels provided. On a real corpus, we obtain qualitatively similar
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Badrinath_C/0/1/0/all/0/1"&gt;Charumathi Badrinath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1"&gt;Weiwei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1"&gt;Finale Doshi-Velez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting Deformable Convolution for Depth Completion. (arXiv:2308.01905v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.01905</id>
        <link href="http://arxiv.org/abs/2308.01905"/>
        <updated>2023-08-05T00:48:26.712Z</updated>
        <summary type="html"><![CDATA[Depth completion, which aims to generate high-quality dense depth maps from
sparse depth maps, has attracted increasing attention in recent years. Previous
work usually employs RGB images as guidance, and introduces iterative spatial
propagation to refine estimated coarse depth maps. However, most of the
propagation refinement methods require several iterations and suffer from a
fixed receptive field, which may contain irrelevant and useless information
with very sparse input. In this paper, we address these two challenges
simultaneously by revisiting the idea of deformable convolution. We propose an
effective architecture that leverages deformable kernel convolution as a
single-pass refinement module, and empirically demonstrate its superiority. To
better understand the function of deformable convolution and exploit it for
depth completion, we further systematically investigate a variety of
representative strategies. Our study reveals that, different from prior work,
deformable convolution needs to be applied on an estimated depth map with a
relatively high density for better performance. We evaluate our model on the
large-scale KITTI dataset and achieve state-of-the-art level performance in
both accuracy and inference speed. Our code is available at
https://github.com/AlexSunNik/ReDC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xinglong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1"&gt;Jean Ponce&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Xiong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ProMix: Combating Label Noise via Maximizing Clean Sample Utility. (arXiv:2207.10276v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2207.10276</id>
        <link href="http://arxiv.org/abs/2207.10276"/>
        <updated>2023-08-05T00:48:26.711Z</updated>
        <summary type="html"><![CDATA[Learning with Noisy Labels (LNL) has become an appealing topic, as
imperfectly annotated data are relatively cheaper to obtain. Recent
state-of-the-art approaches employ specific selection mechanisms to separate
clean and noisy samples and then apply Semi-Supervised Learning (SSL)
techniques for improved performance. However, the selection step mostly
provides a medium-sized and decent-enough clean subset, which overlooks a rich
set of clean samples. To fulfill this, we propose a novel LNL framework ProMix
that attempts to maximize the utility of clean samples for boosted performance.
Key to our method, we propose a matched high confidence selection technique
that selects those examples with high confidence scores and matched predictions
with given labels to dynamically expand a base clean sample set. To overcome
the potential side effect of excessive clean set selection procedure, we
further devise a novel SSL framework that is able to train balanced and
unbiased classifiers on the separated clean and noisy samples. Extensive
experiments demonstrate that ProMix significantly advances the current
state-of-the-art results on multiple benchmarks with different types and levels
of noise. It achieves an average improvement of 2.48\% on the CIFAR-N dataset.
The code is available at https://github.com/Justherozen/ProMix]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_R/0/1/0/all/0/1"&gt;Ruixuan Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yiwen Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haobo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1"&gt;Lei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Runze Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Gang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Junbo Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Link Prediction Explanations for Graph Neural Networks. (arXiv:2308.01682v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01682</id>
        <link href="http://arxiv.org/abs/2308.01682"/>
        <updated>2023-08-05T00:48:26.710Z</updated>
        <summary type="html"><![CDATA[Graph Machine Learning (GML) has numerous applications, such as node/graph
classification and link prediction, in real-world domains. Providing
human-understandable explanations for GML models is a challenging yet
fundamental task to foster their adoption, but validating explanations for link
prediction models has received little attention. In this paper, we provide
quantitative metrics to assess the quality of link prediction explanations,
with or without ground-truth. State-of-the-art explainability methods for Graph
Neural Networks are evaluated using these metrics. We discuss how underlying
assumptions and technical details specific to the link prediction task, such as
the choice of distance between node embeddings, can influence the quality of
the explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Borile_C/0/1/0/all/0/1"&gt;Claudio Borile&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perotti_A/0/1/0/all/0/1"&gt;Alan Perotti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panisson_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Panisson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Circumventing Concept Erasure Methods For Text-to-Image Generative Models. (arXiv:2308.01508v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01508</id>
        <link href="http://arxiv.org/abs/2308.01508"/>
        <updated>2023-08-05T00:48:26.704Z</updated>
        <summary type="html"><![CDATA[Text-to-image generative models can produce photo-realistic images for an
extremely broad range of concepts, and their usage has proliferated widely
among the general public. On the flip side, these models have numerous
drawbacks, including their potential to generate images featuring sexually
explicit content, mirror artistic styles without permission, or even
hallucinate (or deepfake) the likenesses of celebrities. Consequently, various
methods have been proposed in order to "erase" sensitive concepts from
text-to-image models. In this work, we examine five recently proposed concept
erasure methods, and show that targeted concepts are not fully excised from any
of these methods. Specifically, we leverage the existence of special learned
word embeddings that can retrieve "erased" concepts from the sanitized models
with no alterations to their weights. Our results highlight the brittleness of
post hoc concept erasure methods, and call into question their use in the
algorithmic toolkit for AI safety.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1"&gt;Minh Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marshall_K/0/1/0/all/0/1"&gt;Kelly O. Marshall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1"&gt;Chinmay Hegde&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Slate Policy Optimization: Going Beyond Plackett-Luce. (arXiv:2308.01566v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01566</id>
        <link href="http://arxiv.org/abs/2308.01566"/>
        <updated>2023-08-05T00:48:26.703Z</updated>
        <summary type="html"><![CDATA[An increasingly important building block of large scale machine learning
systems is based on returning slates; an ordered lists of items given a query.
Applications of this technology include: search, information retrieval and
recommender systems. When the action space is large, decision systems are
restricted to a particular structure to complete online queries quickly. This
paper addresses the optimization of these large scale decision systems given an
arbitrary reward function. We cast this learning problem in a policy
optimization framework and propose a new class of policies, born from a novel
relaxation of decision functions. This results in a simple, yet efficient
learning algorithm that scales to massive action spaces. We compare our method
to the commonly adopted Plackett-Luce policy class and demonstrate the
effectiveness of our approach on problems with action space sizes in the order
of millions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sakhi_O/0/1/0/all/0/1"&gt;Otmane Sakhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohde_D/0/1/0/all/0/1"&gt;David Rohde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chopin_N/0/1/0/all/0/1"&gt;Nicolas Chopin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Careful Whisper -- leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification. (arXiv:2308.01327v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2308.01327</id>
        <link href="http://arxiv.org/abs/2308.01327"/>
        <updated>2023-08-05T00:48:26.701Z</updated>
        <summary type="html"><![CDATA[This paper presents a fully automated approach for identifying speech
anomalies from voice recordings to aid in the assessment of speech impairments.
By combining Connectionist Temporal Classification (CTC) and
encoder-decoder-based automatic speech recognition models, we generate rich
acoustic and clean transcripts. We then apply several natural language
processing methods to extract features from these transcripts to produce
prototypes of healthy speech. Basic distance measures from these prototypes
serve as input features for standard machine learning classifiers, yielding
human-level accuracy for the distinction between recordings of people with
aphasia and a healthy control group. Furthermore, the most frequently occurring
aphasia types can be distinguished with 90% accuracy. The pipeline is directly
applicable to other diseases and languages, showing promise for robustly
extracting diagnostic speech biomarkers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_L/0/1/0/all/0/1"&gt;Laurin Wagner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zusag_M/0/1/0/all/0/1"&gt;Mario Zusag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bloder_T/0/1/0/all/0/1"&gt;Theresa Bloder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UniG-Encoder: A Universal Feature Encoder for Graph and Hypergraph Node Classification. (arXiv:2308.01650v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01650</id>
        <link href="http://arxiv.org/abs/2308.01650"/>
        <updated>2023-08-05T00:48:26.695Z</updated>
        <summary type="html"><![CDATA[Graph and hypergraph representation learning has attracted increasing
attention from various research fields. Despite the decent performance and
fruitful applications of Graph Neural Networks (GNNs), Hypergraph Neural
Networks (HGNNs), and their well-designed variants, on some commonly used
benchmark graphs and hypergraphs, they are outperformed by even a simple
Multi-Layer Perceptron. This observation motivates a reexamination of the
design paradigm of the current GNNs and HGNNs and poses challenges of
extracting graph features effectively. In this work, a universal feature
encoder for both graph and hypergraph representation learning is designed,
called UniG-Encoder. The architecture starts with a forward transformation of
the topological relationships of connected nodes into edge or hyperedge
features via a normalized projection matrix. The resulting edge/hyperedge
features, together with the original node features, are fed into a neural
network. The encoded node embeddings are then derived from the reversed
transformation, described by the transpose of the projection matrix, of the
network's output, which can be further used for tasks such as node
classification. The proposed architecture, in contrast to the traditional
spectral-based and/or message passing approaches, simultaneously and
comprehensively exploits the node features and graph/hypergraph topologies in
an efficient and unified manner, covering both heterophilic and homophilic
graphs. The designed projection matrix, encoding the graph features, is
intuitive and interpretable. Extensive experiments are conducted and
demonstrate the superior performance of the proposed framework on twelve
representative hypergraph datasets and six real-world graph datasets, compared
to the state-of-the-art methods. Our implementation is available online at
https://github.com/MinhZou/UniG-Encoder.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_M/0/1/0/all/0/1"&gt;Minhao Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1"&gt;Zhongxue Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yutong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_D/0/1/0/all/0/1"&gt;Dongyan Sui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1"&gt;Chun Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leng_S/0/1/0/all/0/1"&gt;Siyang Leng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bidirectional Contrastive Split Learning for Visual Question Answering. (arXiv:2208.11435v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.11435</id>
        <link href="http://arxiv.org/abs/2208.11435"/>
        <updated>2023-08-05T00:48:26.687Z</updated>
        <summary type="html"><![CDATA[Visual Question Answering (VQA) based on multi-modal data facilitates
real-life applications such as home robots and medical diagnoses. One
significant challenge is to devise a robust decentralized learning framework
for various client models where centralized data collection is refrained due to
confidentiality concerns. This work aims to tackle privacy-preserving VQA by
decoupling a multi-modal model into representation modules and a contrastive
module and leveraging inter-module gradients sharing and inter-client weight
sharing. To this end, we propose Bidirectional Contrastive Split Learning
(BiCSL) to train a global multi-modal model on the entire data distribution of
decentralized clients. We employ the contrastive loss that enables a more
efficient self-supervised learning of decentralized modules. Comprehensive
experiments are conducted on the VQA-v2 dataset based on five SOTA VQA models,
demonstrating the effectiveness of the proposed method. Furthermore, we inspect
BiCSL's robustness against a dual-key backdoor attack on VQA. Consequently,
BiCSL shows much better robustness to the multi-modal adversarial attack
compared to the centralized learning method, which provides a promising
approach to decentralized multi-modal learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yuwei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ochiai_H/0/1/0/all/0/1"&gt;Hideya Ochiai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lode Enhancer: Level Co-creation Through Scaling. (arXiv:2308.01543v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01543</id>
        <link href="http://arxiv.org/abs/2308.01543"/>
        <updated>2023-08-05T00:48:26.686Z</updated>
        <summary type="html"><![CDATA[We explore AI-powered upscaling as a design assistance tool in the context of
creating 2D game levels. Deep neural networks are used to upscale artificially
downscaled patches of levels from the puzzle platformer game Lode Runner. The
trained networks are incorporated into a web-based editor, where the user can
create and edit levels at three different levels of resolution: 4x4, 8x8, and
16x16. An edit at any resolution instantly transfers to the other resolutions.
As upscaling requires inventing features that might not be present at lower
resolutions, we train neural networks to reproduce these features. We introduce
a neural network architecture that is capable of not only learning upscaling
but also giving higher priority to less frequent tiles. To investigate the
potential of this tool and guide further development, we conduct a qualitative
study with 3 designers to understand how they use it. Designers enjoyed
co-designing with the tool, liked its underlying concept, and provided feedback
for further improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhaumik_D/0/1/0/all/0/1"&gt;Debosmita Bhaumik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1"&gt;Julian Togelius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yannakakis_G/0/1/0/all/0/1"&gt;Georgios N. Yannakakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khalifa_A/0/1/0/all/0/1"&gt;Ahmed Khalifa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interleaving GANs with knowledge graphs to support design creativity for book covers. (arXiv:2308.01626v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.01626</id>
        <link href="http://arxiv.org/abs/2308.01626"/>
        <updated>2023-08-05T00:48:26.682Z</updated>
        <summary type="html"><![CDATA[An attractive book cover is important for the success of a book. In this
paper, we apply Generative Adversarial Networks (GANs) to the book covers
domain, using different methods for training in order to obtain better
generated images. We interleave GANs with knowledge graphs to alter the input
title to obtain multiple possible options for any given title, which are then
used as an augmented input to the generator. Finally, we use the discriminator
obtained during the training phase to select the best images generated with new
titles. Our method performed better at generating book covers than previous
attempts, and the knowledge graph gives better options to the book author or
editor compared to using GANs alone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Motogna_A/0/1/0/all/0/1"&gt;Alexandru Motogna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Groza_A/0/1/0/all/0/1"&gt;Adrian Groza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System. (arXiv:2307.04577v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.04577</id>
        <link href="http://arxiv.org/abs/2307.04577"/>
        <updated>2023-08-05T00:48:26.680Z</updated>
        <summary type="html"><![CDATA[Vision-based teleoperation offers the possibility to endow robots with
human-level intelligence to physically interact with the environment, while
only requiring low-cost camera sensors. However, current vision-based
teleoperation systems are designed and engineered towards a particular robot
model and deploy environment, which scales poorly as the pool of the robot
models expands and the variety of the operating environment increases. In this
paper, we propose AnyTeleop, a unified and general teleoperation system to
support multiple different arms, hands, realities, and camera configurations
within a single system. Although being designed to provide great flexibility to
the choice of simulators and real hardware, our system can still achieve great
performance. For real-world experiments, AnyTeleop can outperform a previous
system that was designed for a specific robot hardware with a higher success
rate, using the same robot. For teleoperation in simulation, AnyTeleop leads to
better imitation learning performance, compared with a previous system that is
particularly designed for that simulator. Project page: this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1"&gt;Yuzhe Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1"&gt;Binghao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wyk_K/0/1/0/all/0/1"&gt;Karl Van Wyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hao Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_Y/0/1/0/all/0/1"&gt;Yu-Wei Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1"&gt;Dieter Fox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EmbeddingTree: Hierarchical Exploration of Entity Features in Embedding. (arXiv:2308.01329v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01329</id>
        <link href="http://arxiv.org/abs/2308.01329"/>
        <updated>2023-08-05T00:48:26.679Z</updated>
        <summary type="html"><![CDATA[Embedding learning transforms discrete data entities into continuous
numerical representations, encoding features/properties of the entities.
Despite the outstanding performance reported from different embedding learning
algorithms, few efforts were devoted to structurally interpreting how features
are encoded in the learned embedding space. This work proposes EmbeddingTree, a
hierarchical embedding exploration algorithm that relates the semantics of
entity features with the less-interpretable embedding vectors. An interactive
visualization tool is also developed based on EmbeddingTree to explore
high-dimensional embeddings. The tool helps users discover nuance features of
data entities, perform feature denoising/injecting in embedding training, and
generate embeddings for unseen entities. We demonstrate the efficacy of
EmbeddingTree and our visualization tool through embeddings generated for
industry-scale merchant data and the public 30Music listening/playlists
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yan Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Junpeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1"&gt;Chin-Chia Michael Yeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yujie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huiyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory. (arXiv:2308.01853v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2308.01853</id>
        <link href="http://arxiv.org/abs/2308.01853"/>
        <updated>2023-08-05T00:48:26.678Z</updated>
        <summary type="html"><![CDATA[Distribution shifts are a serious concern in modern statistical learning as
they can systematically change the properties of the data away from the truth.
We focus on Wasserstein distribution shifts, where every data point may undergo
a slight perturbation, as opposed to the Huber contamination model where a
fraction of observations are outliers. We formulate and study shifts beyond
independent perturbations, exploring Joint Distribution Shifts, where the
per-observation perturbations can be coordinated. We analyze several important
statistical problems, including location estimation, linear regression, and
non-parametric density estimation. Under a squared loss for mean estimation and
prediction error in linear regression, we find the exact minimax risk, a least
favorable perturbation, and show that the sample mean and least squares
estimators are respectively optimal. This holds for both independent and joint
shifts, but the least favorable perturbations and minimax risks differ. For
other problems, we provide nearly optimal estimators and precise finite-sample
bounds. We also introduce several tools for bounding the minimax risk under
distribution shift, such as a smoothing technique for location families, and
generalizations of classical tools including least favorable sequences of
priors, the modulus of continuity, Le Cam's, Fano's, and Assouad's methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chao_P/0/1/0/all/0/1"&gt;Patrick Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dobriban_E/0/1/0/all/0/1"&gt;Edgar Dobriban&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RAB: Provable Robustness Against Backdoor Attacks. (arXiv:2003.08904v8 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.08904</id>
        <link href="http://arxiv.org/abs/2003.08904"/>
        <updated>2023-08-05T00:48:26.677Z</updated>
        <summary type="html"><![CDATA[Recent studies have shown that deep neural networks (DNNs) are vulnerable to
adversarial attacks, including evasion and backdoor (poisoning) attacks. On the
defense side, there have been intensive efforts on improving both empirical and
provable robustness against evasion attacks; however, the provable robustness
against backdoor attacks still remains largely unexplored. In this paper, we
focus on certifying the machine learning model robustness against general
threat models, especially backdoor attacks. We first provide a unified
framework via randomized smoothing techniques and show how it can be
instantiated to certify the robustness against both evasion and backdoor
attacks. We then propose the first robust training process, RAB, to smooth the
trained model and certify its robustness against backdoor attacks. We prove the
robustness bound for machine learning models trained with RAB and prove that
our robustness bound is tight. In addition, we theoretically show that it is
possible to train the robust smoothed models efficiently for simple models such
as K-nearest neighbor classifiers, and we propose an exact smooth-training
algorithm that eliminates the need to sample from a noise distribution for such
models. Empirically, we conduct comprehensive experiments for different machine
learning (ML) models such as DNNs, support vector machines, and K-NN models on
MNIST, CIFAR-10, and ImageNette datasets and provide the first benchmark for
certified robustness against backdoor attacks. In addition, we evaluate K-NN
models on a spambase tabular dataset to demonstrate the advantages of the
proposed exact algorithm. Both the theoretic analysis and the comprehensive
evaluation on diverse ML models and datasets shed light on further robust
learning strategies against general training time attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weber_M/0/1/0/all/0/1"&gt;Maurice Weber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaojun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlas_B/0/1/0/all/0/1"&gt;Bojan Karla&amp;#x161;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Telematics Combined Actuarial Neural Networks for Cross-Sectional and Longitudinal Claim Count Data. (arXiv:2308.01729v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2308.01729</id>
        <link href="http://arxiv.org/abs/2308.01729"/>
        <updated>2023-08-05T00:48:26.674Z</updated>
        <summary type="html"><![CDATA[We present novel cross-sectional and longitudinal claim count models for
vehicle insurance built upon the Combined Actuarial Neural Network (CANN)
framework proposed by Mario W\"uthrich and Michael Merz. The CANN approach
combines a classical actuarial model, such as a generalized linear model, with
a neural network. This blending of models results in a two-component model
comprising a classical regression model and a neural network part. The CANN
model leverages the strengths of both components, providing a solid foundation
and interpretability from the classical model while harnessing the flexibility
and capacity to capture intricate relationships and interactions offered by the
neural network. In our proposed models, we use well-known log-linear claim
count regression models for the classical regression part and a multilayer
perceptron (MLP) for the neural network part. The MLP part is used to process
telematics car driving data given as a vector characterizing the driving
behavior of each insured driver. In addition to the Poisson and negative
binomial distributions for cross-sectional data, we propose a procedure for
training our CANN model with a multivariate negative binomial (MVNB)
specification. By doing so, we introduce a longitudinal model that accounts for
the dependence between contracts from the same insured. Our results reveal that
the CANN models exhibit superior performance compared to log-linear models that
rely on manually engineered telematics features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Duval_F/0/1/0/all/0/1"&gt;Francis Duval&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Boucher_J/0/1/0/all/0/1"&gt;Jean-Philippe Boucher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pigeon_M/0/1/0/all/0/1"&gt;Mathieu Pigeon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Convolutional Neural Network Architecture with a Continuous Symmetry. (arXiv:2308.01621v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.01621</id>
        <link href="http://arxiv.org/abs/2308.01621"/>
        <updated>2023-08-05T00:48:26.672Z</updated>
        <summary type="html"><![CDATA[This paper introduces a new Convolutional Neural Network (ConvNet)
architecture inspired by a class of partial differential equations (PDEs)
called quasi-linear hyperbolic systems. With comparable performance on image
classification task, it allows for the modification of the weights via a
continuous group of symmetry. This is a significant shift from traditional
models where the architecture and weights are essentially fixed. We wish to
promote the (internal) symmetry as a new desirable property for a neural
network, and to draw attention to the PDE perspective in analyzing and
interpreting ConvNets in the broader Deep Learning community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1"&gt;Hang Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_B/0/1/0/all/0/1"&gt;Bing Bai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is your data alignable? Principled and interpretable alignability testing and integration of single-cell data. (arXiv:2308.01839v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2308.01839</id>
        <link href="http://arxiv.org/abs/2308.01839"/>
        <updated>2023-08-05T00:48:26.668Z</updated>
        <summary type="html"><![CDATA[Single-cell data integration can provide a comprehensive molecular view of
cells, and many algorithms have been developed to remove unwanted technical or
biological variations and integrate heterogeneous single-cell datasets. Despite
their wide usage, existing methods suffer from several fundamental limitations.
In particular, we lack a rigorous statistical test for whether two
high-dimensional single-cell datasets are alignable (and therefore should even
be aligned). Moreover, popular methods can substantially distort the data
during alignment, making the aligned data and downstream analysis difficult to
interpret. To overcome these limitations, we present a spectral manifold
alignment and inference (SMAI) framework, which enables principled and
interpretable alignability testing and structure-preserving integration of
single-cell data. SMAI provides a statistical test to robustly determine the
alignability between datasets to avoid misleading inference, and is justified
by high-dimensional statistical theory. On a diverse range of real and
simulated benchmark datasets, it outperforms commonly used alignment methods.
Moreover, we show that SMAI improves various downstream analyses such as
identification of differentially expressed genes and imputation of single-cell
spatial transcriptomics, providing further biological insights. SMAI's
interpretability also enables quantification and a deeper understanding of the
sources of technical confounders in single-cell data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Ma_R/0/1/0/all/0/1"&gt;Rong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Sun_E/0/1/0/all/0/1"&gt;Eric D. Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Donoho_D/0/1/0/all/0/1"&gt;David Donoho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zou_J/0/1/0/all/0/1"&gt;James Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Matrix Estimation for Individual Fairness. (arXiv:2302.02096v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.02096</id>
        <link href="http://arxiv.org/abs/2302.02096"/>
        <updated>2023-08-05T00:48:26.667Z</updated>
        <summary type="html"><![CDATA[In recent years, multiple notions of algorithmic fairness have arisen. One
such notion is individual fairness (IF), which requires that individuals who
are similar receive similar treatment. In parallel, matrix estimation (ME) has
emerged as a natural paradigm for handling noisy data with missing values. In
this work, we connect the two concepts. We show that pre-processing data using
ME can improve an algorithm's IF without sacrificing performance. Specifically,
we show that using a popular ME method known as singular value thresholding
(SVT) to pre-process the data provides a strong IF guarantee under appropriate
conditions. We then show that, under analogous conditions, SVT pre-processing
also yields estimates that are consistent and approximately minimax optimal. As
such, the ME pre-processing step does not, under the stated conditions,
increase the prediction error of the base algorithm, i.e., does not impose a
fairness-performance trade-off. We verify these results on synthetic and real
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Cindy Y. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cen_S/0/1/0/all/0/1"&gt;Sarah H. Cen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1"&gt;Devavrat Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How many preprints have actually been printed and why: a case study of computer science preprints on arXiv. (arXiv:2308.01899v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2308.01899</id>
        <link href="http://arxiv.org/abs/2308.01899"/>
        <updated>2023-08-05T00:48:26.666Z</updated>
        <summary type="html"><![CDATA[Preprints play an increasingly critical role in academic communities. There
are many reasons driving researchers to post their manuscripts to preprint
servers before formal submission to journals or conferences, but the use of
preprints has also sparked considerable controversy, especially surrounding the
claim of priority. In this paper, a case study of computer science preprints
submitted to arXiv from 2008 to 2017 is conducted to quantify how many
preprints have eventually been printed in peer-reviewed venues. Among those
published manuscripts, some are published under different titles and without an
update to their preprints on arXiv. In the case of these manuscripts, the
traditional fuzzy matching method is incapable of mapping the preprint to the
final published version. In view of this issue, we introduce a semantics-based
mapping method with the employment of Bidirectional Encoder Representations
from Transformers (BERT). With this new mapping method and a plurality of data
sources, we find that 66% of all sampled preprints are published under
unchanged titles and 11% are published under different titles and with other
modifications. A further analysis was then performed to investigate why these
preprints but not others were accepted for publication. Our comparison reveals
that in the field of computer science, published preprints feature adequate
revisions, multiple authorship, detailed abstract and introduction, extensive
and authoritative references and available source code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jialiang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhiyang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1"&gt;Xiaodong Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit. (arXiv:2308.01814v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01814</id>
        <link href="http://arxiv.org/abs/2308.01814"/>
        <updated>2023-08-05T00:48:26.662Z</updated>
        <summary type="html"><![CDATA[Going beyond stochastic gradient descent (SGD), what new phenomena emerge in
wide neural networks trained by adaptive optimizers like Adam? Here we show:
The same dichotomy between feature learning and kernel behaviors (as in SGD)
holds for general optimizers as well, including Adam -- albeit with a nonlinear
notion of "kernel." We derive the corresponding "neural tangent" and "maximal
update" limits for any architecture. Two foundational advances underlie the
above results: 1) A new Tensor Program language, NEXORT, that can express how
adaptive optimizers process gradients into updates. 2) The introduction of
bra-ket notation to drastically simplify expressions and calculations in Tensor
Programs. This work summarizes and generalizes all previous results in the
Tensor Programs series of papers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Greg Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Littwin_E/0/1/0/all/0/1"&gt;Etai Littwin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Successor Feature Neural Episodic Control. (arXiv:2111.03110v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.03110</id>
        <link href="http://arxiv.org/abs/2111.03110"/>
        <updated>2023-08-05T00:48:26.661Z</updated>
        <summary type="html"><![CDATA[A longstanding goal in reinforcement learning is to build intelligent agents
that show fast learning and a flexible transfer of skills akin to humans and
animals. This paper investigates the integration of two frameworks for tackling
those goals: episodic control and successor features. Episodic control is a
cognitively inspired approach relying on episodic memory, an instance-based
memory model of an agent's experiences. Meanwhile, successor features and
generalized policy improvement (SF&GPI) is a meta and transfer learning
framework allowing to learn policies for tasks that can be efficiently reused
for later tasks which have a different reward function. Individually, these two
techniques have shown impressive results in vastly improving sample efficiency
and the elegant reuse of previously learned policies. Thus, we outline a
combination of both approaches in a single reinforcement learning framework and
empirically illustrate its benefits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Emukpere_D/0/1/0/all/0/1"&gt;David Emukpere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1"&gt;Xavier Alameda-Pineda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reinke_C/0/1/0/all/0/1"&gt;Chris Reinke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Merging satellite and gauge-measured precipitation using LightGBM with an emphasis on extreme quantiles. (arXiv:2302.03606v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.03606</id>
        <link href="http://arxiv.org/abs/2302.03606"/>
        <updated>2023-08-05T00:48:26.660Z</updated>
        <summary type="html"><![CDATA[Knowing the actual precipitation in space and time is critical in
hydrological modelling applications, yet the spatial coverage with rain gauge
stations is limited due to economic constraints. Gridded satellite
precipitation datasets offer an alternative option for estimating the actual
precipitation by covering uniformly large areas, albeit related estimates are
not accurate. To improve precipitation estimates, machine learning is applied
to merge rain gauge-based measurements and gridded satellite precipitation
products. In this context, observed precipitation plays the role of the
dependent variable, while satellite data play the role of predictor variables.
Random forests is the dominant machine learning algorithm in relevant
applications. In those spatial predictions settings, point predictions (mostly
the mean or the median of the conditional distribution) of the dependent
variable are issued. The aim of the manuscript is to solve the problem of
probabilistic prediction of precipitation with an emphasis on extreme quantiles
in spatial interpolation settings. Here we propose, issuing probabilistic
spatial predictions of precipitation using Light Gradient Boosting Machine
(LightGBM). LightGBM is a boosting algorithm, highlighted by prize-winning
entries in prediction and forecasting competitions. To assess LightGBM, we
contribute a large-scale application that includes merging daily precipitation
measurements in contiguous US with PERSIANN and GPM-IMERG satellite
precipitation data. We focus on extreme quantiles of the probability
distribution of the dependent variable, where LightGBM outperforms quantile
regression forests (QRF, a variant of random forests) in terms of quantile
score at extreme quantiles. Our study offers understanding of probabilistic
predictions in spatial settings using machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tyralis_H/0/1/0/all/0/1"&gt;Hristos Tyralis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Papacharalampous_G/0/1/0/all/0/1"&gt;Georgia Papacharalampous&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Doulamis_N/0/1/0/all/0/1"&gt;Nikolaos Doulamis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Doulamis_A/0/1/0/all/0/1"&gt;Anastasios Doulamis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Training of Mean Variance Estimation Neural Networks. (arXiv:2302.08875v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.08875</id>
        <link href="http://arxiv.org/abs/2302.08875"/>
        <updated>2023-08-05T00:48:26.658Z</updated>
        <summary type="html"><![CDATA[This paper focusses on the optimal implementation of a Mean Variance
Estimation network (MVE network) (Nix and Weigend, 1994). This type of network
is often used as a building block for uncertainty estimation methods in a
regression setting, for instance Concrete dropout (Gal et al., 2017) and Deep
Ensembles (Lakshminarayanan et al., 2017). Specifically, an MVE network assumes
that the data is produced from a normal distribution with a mean function and
variance function. The MVE network outputs a mean and variance estimate and
optimizes the network parameters by minimizing the negative loglikelihood. In
our paper, we present two significant insights. Firstly, the convergence
difficulties reported in recent work can be relatively easily prevented by
following the simple yet often overlooked recommendation from the original
authors that a warm-up period should be used. During this period, only the mean
is optimized with a fixed variance. We demonstrate the effectiveness of this
step through experimentation, highlighting that it should be standard practice.
As a sidenote, we examine whether, after the warm-up, it is beneficial to fix
the mean while optimizing the variance or to optimize both simultaneously.
Here, we do not observe a substantial difference. Secondly, we introduce a
novel improvement of the MVE network: separate regularization of the mean and
the variance estimate. We demonstrate, both on toy examples and on a number of
benchmark UCI regression data sets, that following the original recommendations
and the novel separate regularization can lead to significant improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sluijterman_L/0/1/0/all/0/1"&gt;Laurens Sluijterman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cator_E/0/1/0/all/0/1"&gt;Eric Cator&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Heskes_T/0/1/0/all/0/1"&gt;Tom Heskes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Normative framework for deriving neural networks with multi-compartmental neurons and non-Hebbian plasticity. (arXiv:2302.10051v2 [q-bio.NC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.10051</id>
        <link href="http://arxiv.org/abs/2302.10051"/>
        <updated>2023-08-05T00:48:26.658Z</updated>
        <summary type="html"><![CDATA[An established normative approach for understanding the algorithmic basis of
neural computation is to derive online algorithms from principled computational
objectives and evaluate their compatibility with anatomical and physiological
observations. Similarity matching objectives have served as successful starting
points for deriving online algorithms that map onto neural networks (NNs) with
point neurons and Hebbian/anti-Hebbian plasticity. These NN models account for
many anatomical and physiological observations; however, the objectives have
limited computational power and the derived NNs do not explain
multi-compartmental neuronal structures and non-Hebbian forms of plasticity
that are prevalent throughout the brain. In this article, we unify and
generalize recent extensions of the similarity matching approach to address
more complex objectives, including a large class of unsupervised and
self-supervised learning tasks that can be formulated as symmetric generalized
eigenvalue problems or nonnegative matrix factorization problems.
Interestingly, the online algorithms derived from these objectives naturally
map onto NNs with multi-compartmental neurons and local, non-Hebbian learning
rules. Therefore, this unified extension of the similarity matching approach
provides a normative framework that facilitates understanding
multi-compartmental neuronal structures and non-Hebbian plasticity found
throughout the brain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Lipshutz_D/0/1/0/all/0/1"&gt;David Lipshutz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Bahroun_Y/0/1/0/all/0/1"&gt;Yanis Bahroun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Golkar_S/0/1/0/all/0/1"&gt;Siavash Golkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Sengupta_A/0/1/0/all/0/1"&gt;Anirvan M. Sengupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chklovskii_D/0/1/0/all/0/1"&gt;Dmitri B. Chklovskii&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module. (arXiv:2305.16174v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.16174</id>
        <link href="http://arxiv.org/abs/2305.16174"/>
        <updated>2023-08-05T00:48:26.656Z</updated>
        <summary type="html"><![CDATA[Latent Graph Inference (LGI) relaxed the reliance of Graph Neural Networks
(GNNs) on a given graph topology by dynamically learning it. However, most of
LGI methods assume to have a (noisy, incomplete, improvable, ...) input graph
to rewire and can solely learn regular graph topologies. In the wake of the
success of Topological Deep Learning (TDL), we study Latent Topology Inference
(LTI) for learning higher-order cell complexes (with sparse and not regular
topology) describing multi-way interactions between data points. To this aim,
we introduce the Differentiable Cell Complex Module (DCM), a novel learnable
function that computes cell probabilities in the complex to improve the
downstream task. We show how to integrate DCM with cell complex message passing
networks layers and train it in a end-to-end fashion, thanks to a two-step
inference procedure that avoids an exhaustive search across all possible cells
in the input, thus maintaining scalability. Our model is tested on several
homophilic and heterophilic graph datasets and it is shown to outperform other
state-of-the-art techniques, offering significant improvements especially in
cases where an input graph is not provided.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Battiloro_C/0/1/0/all/0/1"&gt;Claudio Battiloro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spinelli_I/0/1/0/all/0/1"&gt;Indro Spinelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Telyatnikov_L/0/1/0/all/0/1"&gt;Lev Telyatnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bronstein_M/0/1/0/all/0/1"&gt;Michael Bronstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scardapane_S/0/1/0/all/0/1"&gt;Simone Scardapane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lorenzo_P/0/1/0/all/0/1"&gt;Paolo Di Lorenzo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online covariance estimation for stochastic gradient descent under Markovian sampling. (arXiv:2308.01481v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2308.01481</id>
        <link href="http://arxiv.org/abs/2308.01481"/>
        <updated>2023-08-05T00:48:26.656Z</updated>
        <summary type="html"><![CDATA[We study the online overlapping batch-means covariance estimator for
Stochastic Gradient Descent (SGD) under Markovian sampling. We show that the
convergence rates of the covariance estimator are
$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$ and
$O\big(\sqrt{d}\,n^{-1/8}\big)$ under state-dependent and state-independent
Markovian sampling, respectively, with $d$ representing dimensionality and $n$
denoting the number of observations or SGD iterations. Remarkably, these rates
match the best-known convergence rate previously established for the
independent and identically distributed ($\iid$) case by \cite{zhu2021online},
up to logarithmic factors. Our analysis overcomes significant challenges that
arise due to Markovian sampling, leading to the introduction of additional
error terms and complex dependencies between the blocks of the batch-means
covariance estimator. Moreover, we establish the convergence rate for the first
four moments of the $\ell_2$ norm of the error of SGD dynamics under
state-dependent Markovian data, which holds potential interest as an
independent result. To validate our theoretical findings, we provide numerical
illustrations to derive confidence intervals for SGD when training linear and
logistic regression models under Markovian sampling. Additionally, we apply our
approach to tackle the intriguing problem of strategic classification with
logistic regression, where adversaries can adaptively modify features during
the training process to increase their chances of being classified in a
specific target class.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Roy_A/0/1/0/all/0/1"&gt;Abhishek Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Balasubramanian_K/0/1/0/all/0/1"&gt;Krishnakumar Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimax Optimal $Q$ Learning with Nearest Neighbors. (arXiv:2308.01490v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01490</id>
        <link href="http://arxiv.org/abs/2308.01490"/>
        <updated>2023-08-05T00:48:26.655Z</updated>
        <summary type="html"><![CDATA[$Q$ learning is a popular model free reinforcement learning method. Most of
existing works focus on analyzing $Q$ learning for finite state and action
spaces. If the state space is continuous, then the original $Q$ learning method
can not be directly used. A modification of the original $Q$ learning method
was proposed in (Shah and Xie, 2018), which estimates $Q$ values with nearest
neighbors. Such modification makes $Q$ learning suitable for continuous state
space. (Shah and Xie, 2018) shows that the convergence rate of estimated $Q$
function is $\tilde{O}(T^{-1/(d+3)})$, which is slower than the minimax lower
bound $\tilde{\Omega}(T^{-1/(d+2)})$, indicating that this method is not
efficient. This paper proposes two new $Q$ learning methods to bridge the gap
of convergence rates in (Shah and Xie, 2018), with one of them being offline,
while the other is online. Despite that we still use nearest neighbor approach
to estimate $Q$ function, the algorithms are crucially different from (Shah and
Xie, 2018). In particular, we replace the kernel nearest neighbor in
discretized region with a direct nearest neighbor approach. Consequently, our
approach significantly improves the convergence rate. Moreover, the time
complexity is also significantly improved in high dimensional state spaces. Our
analysis shows that both offline and online methods are minimax rate optimal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Puning Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1"&gt;Lifeng Lai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A digital twin framework for civil engineering structures. (arXiv:2308.01445v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2308.01445</id>
        <link href="http://arxiv.org/abs/2308.01445"/>
        <updated>2023-08-05T00:48:26.652Z</updated>
        <summary type="html"><![CDATA[The digital twin concept represents an appealing opportunity to advance
condition-based and predictive maintenance paradigms for civil engineering
systems, thus allowing reduced lifecycle costs, increased system safety, and
increased system availability. This work proposes a predictive digital twin
approach to the health monitoring, maintenance, and management planning of
civil engineering structures. The asset-twin coupled dynamical system is
encoded employing a probabilistic graphical model, which allows all relevant
sources of uncertainty to be taken into account. In particular, the
time-repeating observations-to-decisions flow is modeled using a dynamic
Bayesian network. Real-time structural health diagnostics are provided by
assimilating sensed data with deep learning models. The digital twin state is
continually updated in a sequential Bayesian inference fashion. This is then
exploited to inform the optimal planning of maintenance and management actions
within a dynamic decision-making framework. A preliminary offline phase
involves the population of training datasets through a reduced-order numerical
model and the computation of a health-dependent control policy. The strategy is
assessed on two synthetic case studies, involving a cantilever beam and a
railway bridge, demonstrating the dynamic decision-making capabilities of
health-aware digital twins.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Torzoni_M/0/1/0/all/0/1"&gt;Matteo Torzoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Tezzele_M/0/1/0/all/0/1"&gt;Marco Tezzele&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mariani_S/0/1/0/all/0/1"&gt;Stefano Mariani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Manzoni_A/0/1/0/all/0/1"&gt;Andrea Manzoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Willcox_K/0/1/0/all/0/1"&gt;Karen E. Willcox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent. (arXiv:2308.01552v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2308.01552</id>
        <link href="http://arxiv.org/abs/2308.01552"/>
        <updated>2023-08-05T00:48:26.641Z</updated>
        <summary type="html"><![CDATA[This research paper delves into the integration of OpenAI's ChatGPT into
embodied agent systems, evaluating its influence on interactive decision-making
benchmark. Drawing a parallel to the concept of people assuming roles according
to their unique strengths, we introduce InterAct. In this approach, we feed
ChatGPT with varied prompts, assigning it a numerous roles like a checker and a
sorter, then integrating them with the original language model. Our research
shows a remarkable success rate of 98% in AlfWorld, which consists of 6
different tasks in a simulated household environment, emphasizing the
significance of proficient prompt engineering. The results highlight ChatGPT's
competence in comprehending and performing intricate tasks effectively in
real-world settings, thus paving the way for further advancements in task
planning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Po-Lin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1"&gt;Cheng-Shang Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Capability of Large Language Models to Measure Psychiatric Functioning. (arXiv:2308.01834v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2308.01834</id>
        <link href="http://arxiv.org/abs/2308.01834"/>
        <updated>2023-08-05T00:48:26.641Z</updated>
        <summary type="html"><![CDATA[The current work investigates the capability of Large language models (LLMs)
that are explicitly trained on large corpuses of medical knowledge (Med-PaLM 2)
to predict psychiatric functioning from patient interviews and clinical
descriptions without being trained to do so. To assess this, n = 145 depression
and n =115 PTSD assessments and n = 46 clinical case studies across high
prevalence/high comorbidity disorders (Depressive, Anxiety, Psychotic, trauma
and stress, Addictive disorders) were analyzed using prompts to extract
estimated clinical scores and diagnoses. Results demonstrate that Med-PaLM 2 is
capable of assessing psychiatric functioning across a range of psychiatric
conditions with the strongest performance being the prediction of depression
scores based on standardized assessments (Accuracy range= 0.80 - 0.84) which
were statistically indistinguishable from human clinical raters t(1,144) =
1.20; p = 0.23. Results show the potential for general clinical language models
to flexibly predict psychiatric risk based on free descriptions of functioning
from both patients and clinicians.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galatzer_Levy_I/0/1/0/all/0/1"&gt;Isaac R. Galatzer-Levy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1"&gt;Daniel McDuff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Natarajan_V/0/1/0/all/0/1"&gt;Vivek Natarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karthikesalingam_A/0/1/0/all/0/1"&gt;Alan Karthikesalingam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malgaroli_M/0/1/0/all/0/1"&gt;Matteo Malgaroli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Planted Forest: a directly interpretable tree ensemble. (arXiv:2012.14563v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14563</id>
        <link href="http://arxiv.org/abs/2012.14563"/>
        <updated>2023-08-05T00:48:26.641Z</updated>
        <summary type="html"><![CDATA[We introduce a novel interpretable tree based algorithm for prediction in a
regression setting. Our motivation is to estimate the unknown regression
function from a functional decomposition perspective in which the functional
components correspond to lower order interaction terms. The idea is to modify
the random forest algorithm by keeping certain leaves after they are split
instead of deleting them. This leads to non-binary trees which we refer to as
planted trees. An extension to a forest leads to our random planted forest
algorithm. Additionally, the maximum number of covariates which can interact
within a leaf can be bounded. If we set this interaction bound to one, the
resulting estimator is a sum of one-dimensional functions. In the other extreme
case, if we do not set a limit, the resulting estimator and corresponding model
place no restrictions on the form of the regression function. In a simulation
study we find encouraging prediction and visualisation properties of our random
planted forest method. We also develop theory for an idealized version of
random planted forests in cases where the interaction bound is low. We show
that if it is smaller than three, the idealized version achieves asymptotically
optimal convergence rates up to a logarithmic factor. Code is available on
GitHub https://github.com/PlantedML/randomPlantedForest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hiabu_M/0/1/0/all/0/1"&gt;Munir Hiabu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mammen_E/0/1/0/all/0/1"&gt;Enno Mammen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meyer_J/0/1/0/all/0/1"&gt;Joseph T. Meyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-equilibrium physics: from spin glasses to machine and neural learning. (arXiv:2308.01538v1 [cond-mat.dis-nn])]]></title>
        <id>http://arxiv.org/abs/2308.01538</id>
        <link href="http://arxiv.org/abs/2308.01538"/>
        <updated>2023-08-05T00:48:26.631Z</updated>
        <summary type="html"><![CDATA[Disordered many-body systems exhibit a wide range of emergent phenomena
across different scales. These complex behaviors can be utilized for various
information processing tasks such as error correction, learning, and
optimization. Despite the empirical success of utilizing these systems for
intelligent tasks, the underlying principles that govern their emergent
intelligent behaviors remain largely unknown. In this thesis, we aim to
characterize such emergent intelligence in disordered systems through
statistical physics. We chart a roadmap for our efforts in this thesis based on
two axes: learning mechanisms (long-term memory vs. working memory) and
learning dynamics (artificial vs. natural). Throughout our journey, we uncover
relationships between learning mechanisms and physical dynamics that could
serve as guiding principles for designing intelligent systems. We hope that our
investigation into the emergent intelligence of seemingly disparate learning
systems can expand our current understanding of intelligence beyond neural
systems and uncover a wider range of computational substrates suitable for AI
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Zhong_W/0/1/0/all/0/1"&gt;Weishun Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RAB: Provable Robustness Against Backdoor Attacks. (arXiv:2003.08904v8 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.08904</id>
        <link href="http://arxiv.org/abs/2003.08904"/>
        <updated>2023-08-05T00:48:26.628Z</updated>
        <summary type="html"><![CDATA[Recent studies have shown that deep neural networks (DNNs) are vulnerable to
adversarial attacks, including evasion and backdoor (poisoning) attacks. On the
defense side, there have been intensive efforts on improving both empirical and
provable robustness against evasion attacks; however, the provable robustness
against backdoor attacks still remains largely unexplored. In this paper, we
focus on certifying the machine learning model robustness against general
threat models, especially backdoor attacks. We first provide a unified
framework via randomized smoothing techniques and show how it can be
instantiated to certify the robustness against both evasion and backdoor
attacks. We then propose the first robust training process, RAB, to smooth the
trained model and certify its robustness against backdoor attacks. We prove the
robustness bound for machine learning models trained with RAB and prove that
our robustness bound is tight. In addition, we theoretically show that it is
possible to train the robust smoothed models efficiently for simple models such
as K-nearest neighbor classifiers, and we propose an exact smooth-training
algorithm that eliminates the need to sample from a noise distribution for such
models. Empirically, we conduct comprehensive experiments for different machine
learning (ML) models such as DNNs, support vector machines, and K-NN models on
MNIST, CIFAR-10, and ImageNette datasets and provide the first benchmark for
certified robustness against backdoor attacks. In addition, we evaluate K-NN
models on a spambase tabular dataset to demonstrate the advantages of the
proposed exact algorithm. Both the theoretic analysis and the comprehensive
evaluation on diverse ML models and datasets shed light on further robust
learning strategies against general training time attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weber_M/0/1/0/all/0/1"&gt;Maurice Weber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaojun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlas_B/0/1/0/all/0/1"&gt;Bojan Karla&amp;#x161;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning-based surrogate models for parametrized PDEs: handling geometric variability through graph neural networks. (arXiv:2308.01602v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2308.01602</id>
        <link href="http://arxiv.org/abs/2308.01602"/>
        <updated>2023-08-05T00:48:26.622Z</updated>
        <summary type="html"><![CDATA[Mesh-based simulations play a key role when modeling complex physical systems
that, in many disciplines across science and engineering, require the solution
of parametrized time-dependent nonlinear partial differential equations (PDEs).
In this context, full order models (FOMs), such as those relying on the finite
element method, can reach high levels of accuracy, however often yielding
intensive simulations to run. For this reason, surrogate models are developed
to replace computationally expensive solvers with more efficient ones, which
can strike favorable trade-offs between accuracy and efficiency. This work
explores the potential usage of graph neural networks (GNNs) for the simulation
of time-dependent PDEs in the presence of geometrical variability. In
particular, we propose a systematic strategy to build surrogate models based on
a data-driven time-stepping scheme where a GNN architecture is used to
efficiently evolve the system. With respect to the majority of surrogate
models, the proposed approach stands out for its ability of tackling problems
with parameter dependent spatial domains, while simultaneously generalizing to
different geometries and mesh resolutions. We assess the effectiveness of the
proposed approach through a series of numerical experiments, involving both
two- and three-dimensional problems, showing that GNNs can provide a valid
alternative to traditional surrogate models in terms of computational
efficiency and generalization to new scenarios. We also assess, from a
numerical standpoint, the importance of using GNNs, rather than classical dense
deep neural networks, for the proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Franco_N/0/1/0/all/0/1"&gt;Nicola Rares Franco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Fresca_S/0/1/0/all/0/1"&gt;Stefania Fresca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Tombari_F/0/1/0/all/0/1"&gt;Filippo Tombari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Manzoni_A/0/1/0/all/0/1"&gt;Andrea Manzoni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Evaluate Uncertainty Estimates in Machine Learning for Regression?. (arXiv:2106.03395v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03395</id>
        <link href="http://arxiv.org/abs/2106.03395"/>
        <updated>2023-08-05T00:48:26.621Z</updated>
        <summary type="html"><![CDATA[As neural networks become more popular, the need for accompanying uncertainty
estimates increases. There are currently two main approaches to test the
quality of these estimates. Most methods output a density. They can be compared
by evaluating their loglikelihood on a test set. Other methods output a
prediction interval directly. These methods are often tested by examining the
fraction of test points that fall inside the corresponding prediction
intervals. Intuitively both approaches seem logical. However, we demonstrate
through both theoretical arguments and simulations that both ways of evaluating
the quality of uncertainty estimates have serious flaws. Firstly, both
approaches cannot disentangle the separate components that jointly create the
predictive uncertainty, making it difficult to evaluate the quality of the
estimates of these components. Secondly, a better loglikelihood does not
guarantee better prediction intervals, which is what the methods are often used
for in practice. Moreover, the current approach to test prediction intervals
directly has additional flaws. We show why it is fundamentally flawed to test a
prediction or confidence interval on a single test set. At best, marginal
coverage is measured, implicitly averaging out overconfident and underconfident
predictions. A much more desirable property is pointwise coverage, requiring
the correct coverage for each prediction. We demonstrate through practical
examples that these effects can result in favoring a method, based on the
predictive uncertainty, that has undesirable behaviour of the confidence or
prediction intervals. Finally, we propose a simulation-based testing approach
that addresses these problems while still allowing easy comparison between
different methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sluijterman_L/0/1/0/all/0/1"&gt;Laurens Sluijterman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cator_E/0/1/0/all/0/1"&gt;Eric Cator&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Heskes_T/0/1/0/all/0/1"&gt;Tom Heskes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Multi-Label Correlation in Label Distribution Learning. (arXiv:2308.01742v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01742</id>
        <link href="http://arxiv.org/abs/2308.01742"/>
        <updated>2023-08-05T00:48:26.617Z</updated>
        <summary type="html"><![CDATA[Label Distribution Learning (LDL) is a novel machine learning paradigm that
assigns label distribution to each instance. Many LDL methods proposed to
leverage label correlation in the learning process to solve the
exponential-sized output space; among these, many exploited the low-rank
structure of label distribution to capture label correlation. However, recent
studies disclosed that label distribution matrices are typically full-rank,
posing challenges to those works exploiting low-rank label correlation. Note
that multi-label is generally low-rank; low-rank label correlation is widely
adopted in multi-label learning (MLL) literature. Inspired by that, we
introduce an auxiliary MLL process in LDL and capture low-rank label
correlation on that MLL rather than LDL. In such a way, low-rank label
correlation is appropriately exploited in our LDL methods. We conduct
comprehensive experiments and demonstrate that our methods are superior to
existing LDL methods. Besides, the ablation studies justify the advantages of
exploiting low-rank label correlation in the auxiliary MLL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+geng_Z/0/1/0/all/0/1"&gt;Zhiqiang Kou jing wang yuheng jia xin geng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online covariance estimation for stochastic gradient descent under Markovian sampling. (arXiv:2308.01481v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2308.01481</id>
        <link href="http://arxiv.org/abs/2308.01481"/>
        <updated>2023-08-05T00:48:26.610Z</updated>
        <summary type="html"><![CDATA[We study the online overlapping batch-means covariance estimator for
Stochastic Gradient Descent (SGD) under Markovian sampling. We show that the
convergence rates of the covariance estimator are
$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$ and
$O\big(\sqrt{d}\,n^{-1/8}\big)$ under state-dependent and state-independent
Markovian sampling, respectively, with $d$ representing dimensionality and $n$
denoting the number of observations or SGD iterations. Remarkably, these rates
match the best-known convergence rate previously established for the
independent and identically distributed ($\iid$) case by \cite{zhu2021online},
up to logarithmic factors. Our analysis overcomes significant challenges that
arise due to Markovian sampling, leading to the introduction of additional
error terms and complex dependencies between the blocks of the batch-means
covariance estimator. Moreover, we establish the convergence rate for the first
four moments of the $\ell_2$ norm of the error of SGD dynamics under
state-dependent Markovian data, which holds potential interest as an
independent result. To validate our theoretical findings, we provide numerical
illustrations to derive confidence intervals for SGD when training linear and
logistic regression models under Markovian sampling. Additionally, we apply our
approach to tackle the intriguing problem of strategic classification with
logistic regression, where adversaries can adaptively modify features during
the training process to increase their chances of being classified in a
specific target class.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Roy_A/0/1/0/all/0/1"&gt;Abhishek Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Balasubramanian_K/0/1/0/all/0/1"&gt;Krishnakumar Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficiency of First-Order Methods for Low-Rank Tensor Recovery with the Tensor Nuclear Norm Under Strict Complementarity. (arXiv:2308.01677v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2308.01677</id>
        <link href="http://arxiv.org/abs/2308.01677"/>
        <updated>2023-08-05T00:48:26.604Z</updated>
        <summary type="html"><![CDATA[We consider convex relaxations for recovering low-rank tensors based on
constrained minimization over a ball induced by the tensor nuclear norm,
recently introduced in \cite{tensor_tSVD}. We build on a recent line of results
that considered convex relaxations for the recovery of low-rank matrices and
established that under a strict complementarity condition (SC), both the
convergence rate and per-iteration runtime of standard gradient methods may
improve dramatically. We develop the appropriate strict complementarity
condition for the tensor nuclear norm ball and obtain the following main
results under this condition: 1. When the objective to minimize is of the form
$f(\mX)=g(\mA\mX)+\langle{\mC,\mX}\rangle$ , where $g$ is strongly convex and
$\mA$ is a linear map (e.g., least squares), a quadratic growth bound holds,
which implies linear convergence rates for standard projected gradient methods,
despite the fact that $f$ need not be strongly convex. 2. For a smooth
objective function, when initialized in certain proximity of an optimal
solution which satisfies SC, standard projected gradient methods only require
SVD computations (for projecting onto the tensor nuclear norm ball) of rank
that matches the tubal rank of the optimal solution. In particular, when the
tubal rank is constant, this implies nearly linear (in the size of the tensor)
runtime per iteration, as opposed to super linear without further assumptions.
3. For a nonsmooth objective function which admits a popular smooth
saddle-point formulation, we derive similar results to the latter for the well
known extragradient method. An additional contribution which may be of
independent interest, is the rigorous extension of many basic results regarding
tensors of arbitrary order, which were previously obtained only for third-order
tensors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Garber_D/0/1/0/all/0/1"&gt;Dan Garber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kaplan_A/0/1/0/all/0/1"&gt;Atara Kaplan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compressed and distributed least-squares regression: convergence rates with applications to Federated Learning. (arXiv:2308.01358v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01358</id>
        <link href="http://arxiv.org/abs/2308.01358"/>
        <updated>2023-08-05T00:48:26.599Z</updated>
        <summary type="html"><![CDATA[In this paper, we investigate the impact of compression on stochastic
gradient algorithms for machine learning, a technique widely used in
distributed and federated learning. We underline differences in terms of
convergence rates between several unbiased compression operators, that all
satisfy the same condition on their variance, thus going beyond the classical
worst-case analysis. To do so, we focus on the case of least-squares regression
(LSR) and analyze a general stochastic approximation algorithm for minimizing
quadratic functions relying on a random field. We consider weak assumptions on
the random field, tailored to the analysis (specifically, expected H\"older
regularity), and on the noise covariance, enabling the analysis of various
randomizing mechanisms, including compression. We then extend our results to
the case of federated learning.

More formally, we highlight the impact on the convergence of the covariance
$\mathfrak{C}_{\mathrm{ania}}$ of the additive noise induced by the algorithm.
We demonstrate despite the non-regularity of the stochastic field, that the
limit variance term scales with $\mathrm{Tr}(\mathfrak{C}_{\mathrm{ania}}
H^{-1})/K$ (where $H$ is the Hessian of the optimization problem and $K$ the
number of iterations) generalizing the rate for the vanilla LSR case where it
is $\sigma^2 \mathrm{Tr}(H H^{-1}) / K = \sigma^2 d / K$ (Bach and Moulines,
2013). Then, we analyze the dependency of $\mathfrak{C}_{\mathrm{ania}}$ on the
compression strategy and ultimately its impact on convergence, first in the
centralized case, then in two heterogeneous FL frameworks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Philippenko_C/0/1/0/all/0/1"&gt;Constantin Philippenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dieuleveut_A/0/1/0/all/0/1"&gt;Aymeric Dieuleveut&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Data Streams: An Overview and Update. (arXiv:2212.14720v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.14720</id>
        <link href="http://arxiv.org/abs/2212.14720"/>
        <updated>2023-08-05T00:48:26.592Z</updated>
        <summary type="html"><![CDATA[The literature on machine learning in the context of data streams is vast and
growing. However, many of the defining assumptions regarding data-stream
learning tasks are too strong to hold in practice, or are even contradictory
such that they cannot be met in the contexts of supervised learning. Algorithms
are chosen and designed based on criteria which are often not clearly stated,
for problem settings not clearly defined, tested in unrealistic settings,
and/or in isolation from related approaches in the wider literature. This puts
into question the potential for real-world impact of many approaches conceived
in such contexts, and risks propagating a misguided research focus. We propose
to tackle these issues by reformulating the fundamental definitions and
settings of supervised data-stream learning with regard to contemporary
considerations of concept drift and temporal dependence; and we take a fresh
look at what constitutes a supervised data-stream learning task, and a
reconsideration of algorithms that may be applied to tackle such tasks. Through
and in reflection of this formulation and overview, helped by an informal
survey of industrial players dealing with real-world data streams, we provide
recommendations. Our main emphasis is that learning from data streams does not
impose a single-pass or online-learning approach, or any particular learning
regime; and any constraints on memory and time are not specific to streaming.
Meanwhile, there exist established techniques for dealing with temporal
dependence and concept drift, in other areas of the literature. For the data
streams community, we thus encourage a shift in research focus, from dealing
with often-artificial constraints and assumptions on the learning mode, to
issues such as robustness, privacy, and interpretability which are increasingly
relevant to learning in data streams in academic and industrial settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Read_J/0/1/0/all/0/1"&gt;Jesse Read&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zliobaite_I/0/1/0/all/0/1"&gt;Indr&amp;#x117; &amp;#x17d;liobait&amp;#x117;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MRQ:Support Multiple Quantization Schemes through Model Re-Quantization. (arXiv:2308.01867v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01867</id>
        <link href="http://arxiv.org/abs/2308.01867"/>
        <updated>2023-08-05T00:48:26.580Z</updated>
        <summary type="html"><![CDATA[Despite the proliferation of diverse hardware accelerators (e.g., NPU, TPU,
DPU), deploying deep learning models on edge devices with fixed-point hardware
is still challenging due to complex model quantization and conversion. Existing
model quantization frameworks like Tensorflow QAT [1], TFLite PTQ [2], and
Qualcomm AIMET [3] supports only a limited set of quantization schemes (e.g.,
only asymmetric per-tensor quantization in TF1.x QAT [4]). Accordingly, deep
learning models cannot be easily quantized for diverse fixed-point hardwares,
mainly due to slightly different quantization requirements. In this paper, we
envision a new type of model quantization approach called MRQ (model
re-quantization), which takes existing quantized models and quickly transforms
the models to meet different quantization requirements (e.g., asymmetric ->
symmetric, non-power-of-2 scale -> power-of-2 scale). Re-quantization is much
simpler than quantizing from scratch because it avoids costly re-training and
provides support for multiple quantization schemes simultaneously. To minimize
re-quantization error, we developed a new set of re-quantization algorithms
including weight correction and rounding error folding. We have demonstrated
that MobileNetV2 QAT model [7] can be quickly re-quantized into two different
quantization schemes (i.e., symmetric and symmetric+power-of-2 scale) with less
than 0.64 units of accuracy loss. We believe our work is the first to leverage
this concept of re-quantization for model quantization and models obtained from
the re-quantization process have been successfully deployed on NNA in the Echo
Show devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manohara_M/0/1/0/all/0/1"&gt;Manasa Manohara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dayal_S/0/1/0/all/0/1"&gt;Sankalp Dayal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Afzal_T/0/1/0/all/0/1"&gt;Tarqi Afzal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1"&gt;Rahul Bakshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1"&gt;Kahkuen Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exact identification of nonlinear dynamical systems by Trimmed Lasso. (arXiv:2308.01891v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01891</id>
        <link href="http://arxiv.org/abs/2308.01891"/>
        <updated>2023-08-05T00:48:26.575Z</updated>
        <summary type="html"><![CDATA[Identification of nonlinear dynamical systems has been popularized by sparse
identification of the nonlinear dynamics (SINDy) via the sequentially
thresholded least squares (STLS) algorithm. Many extensions SINDy have emerged
in the literature to deal with experimental data which are finite in length and
noisy. Recently, the computationally intensive method of ensembling
bootstrapped SINDy models (E-SINDy) was proposed for model identification,
handling finite, highly noisy data. While the extensions of SINDy are numerous,
their sparsity-promoting estimators occasionally provide sparse approximations
of the dynamics as opposed to exact recovery. Furthermore, these estimators
suffer under multicollinearity, e.g. the irrepresentable condition for the
Lasso. In this paper, we demonstrate that the Trimmed Lasso for robust
identification of models (TRIM) can provide exact recovery under more severe
noise, finite data, and multicollinearity as opposed to E-SINDy. Additionally,
the computational cost of TRIM is asymptotically equal to STLS since the
sparsity parameter of the TRIM can be solved efficiently by convex solvers. We
compare these methodologies on challenging nonlinear systems, specifically the
Lorenz 63 system, the Bouc Wen oscillator from the nonlinear dynamics benchmark
of No\"el and Schoukens, 2016, and a time delay system describing tool cutting
dynamics. This study emphasizes the comparisons between STLS, reweighted
$\ell_1$ minimization, and Trimmed Lasso in identification with respect to
problems faced by practitioners: the problem of finite and noisy data, the
performance of the sparse regression of when the library grows in dimension
(multicollinearity), and automatic methods for choice of regularization
parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kiser_S/0/1/0/all/0/1"&gt;Shawn L. Kiser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guskov_M/0/1/0/all/0/1"&gt;Mikhail Guskov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rebillat_M/0/1/0/all/0/1"&gt;Marc R&amp;#xe9;billat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranc_N/0/1/0/all/0/1"&gt;Nicolas Ranc&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-variable Hard Physical Constraints for Climate Model Downscaling. (arXiv:2308.01868v1 [physics.ao-ph])]]></title>
        <id>http://arxiv.org/abs/2308.01868</id>
        <link href="http://arxiv.org/abs/2308.01868"/>
        <updated>2023-08-05T00:48:26.568Z</updated>
        <summary type="html"><![CDATA[Global Climate Models (GCMs) are the primary tool to simulate climate
evolution and assess the impacts of climate change. However, they often operate
at a coarse spatial resolution that limits their accuracy in reproducing
local-scale phenomena. Statistical downscaling methods leveraging deep learning
offer a solution to this problem by approximating local-scale climate fields
from coarse variables, thus enabling regional GCM projections. Typically,
climate fields of different variables of interest are downscaled independently,
resulting in violations of fundamental physical properties across
interconnected variables. This study investigates the scope of this problem
and, through an application on temperature, lays the foundation for a framework
introducing multi-variable hard constraints that guarantees physical
relationships between groups of downscaled climate variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Gonzalez_Abad_J/0/1/0/all/0/1"&gt;Jose Gonz&amp;#xe1;lez-Abad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hernandez_Garcia_A/0/1/0/all/0/1"&gt;&amp;#xc1;lex Hern&amp;#xe1;ndez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Harder_P/0/1/0/all/0/1"&gt;Paula Harder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Rolnick_D/0/1/0/all/0/1"&gt;David Rolnick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Gutierrez_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Manuel Guti&amp;#xe9;rrez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models. (arXiv:2308.01404v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2308.01404</id>
        <link href="http://arxiv.org/abs/2308.01404"/>
        <updated>2023-08-05T00:48:26.557Z</updated>
        <summary type="html"><![CDATA[Are current language models capable of deception and lie detection? We study
this question by introducing a text-based game called $\textit{Hoodwinked}$,
inspired by $\textit{Mafia}$ and $\textit{Among Us}$. Players are locked in a
house and must find a key to escape, but one player is tasked with killing the
others. Each time a murder is committed, the surviving players have a natural
language discussion then vote to banish one player from the game. We conduct
experiments with agents controlled by GPT-3, GPT-3.5, and GPT-4 and find
evidence of deception and lie detection capabilities. The killer often denies
their crime and accuses others, leading to measurable effects on voting
outcomes. More advanced models are more effective killers, outperforming
smaller models in 18 of 24 pairwise comparisons. Secondary metrics provide
evidence that this improvement is not mediated by different actions, but rather
by stronger deception capabilities during discussions. Overall, we find
substantial evidence that current language models are capable of deception. To
better evaluate the ability of AI agents to deceive humans, we make this game
publicly available at https://hoodwinked.ai/ .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+OGara_A/0/1/0/all/0/1"&gt;Aidan O&amp;#x27;Gara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Novel Physics-Based Machine-Learning Models for Indoor Air Quality Approximations. (arXiv:2308.01438v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01438</id>
        <link href="http://arxiv.org/abs/2308.01438"/>
        <updated>2023-08-05T00:48:26.550Z</updated>
        <summary type="html"><![CDATA[Cost-effective sensors are capable of real-time capturing a variety of air
quality-related modalities from different pollutant concentrations to
indoor/outdoor humidity and temperature. Machine learning (ML) models are
capable of performing air-quality "ahead-of-time" approximations. Undoubtedly,
accurate indoor air quality approximation significantly helps provide a healthy
indoor environment, optimize associated energy consumption, and offer human
comfort. However, it is crucial to design an ML architecture to capture the
domain knowledge, so-called problem physics. In this study, we propose six
novel physics-based ML models for accurate indoor pollutant concentration
approximations. The proposed models include an adroit combination of
state-space concepts in physics, Gated Recurrent Units, and Decomposition
techniques. The proposed models were illustrated using data collected from five
offices in a commercial building in California. The proposed models are shown
to be less complex, computationally more efficient, and more accurate than
similar state-of-the-art transformer-based models. The superiority of the
proposed models is due to their relatively light architecture (computational
efficiency) and, more importantly, their ability to capture the underlying
highly nonlinear patterns embedded in the often contaminated sensor-collected
indoor air quality temporal data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohammadshirazi_A/0/1/0/all/0/1"&gt;Ahmad Mohammadshirazi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadafian_A/0/1/0/all/0/1"&gt;Aida Nadafian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monsefi_A/0/1/0/all/0/1"&gt;Amin Karimi Monsefi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rafiei_M/0/1/0/all/0/1"&gt;Mohammad H. Rafiei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramnath_R/0/1/0/all/0/1"&gt;Rajiv Ramnath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Training of Denoising Diffusion Model Using Dual Discriminators for High-Fidelity Multi-Speaker TTS. (arXiv:2308.01573v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2308.01573</id>
        <link href="http://arxiv.org/abs/2308.01573"/>
        <updated>2023-08-05T00:48:26.540Z</updated>
        <summary type="html"><![CDATA[The diffusion model is capable of generating high-quality data through a
probabilistic approach. However, it suffers from the drawback of slow
generation speed due to the requirement of a large number of time steps. To
address this limitation, recent models such as denoising diffusion implicit
models (DDIM) focus on generating samples without directly modeling the
probability distribution, while models like denoising diffusion generative
adversarial networks (GAN) combine diffusion processes with GANs. In the field
of speech synthesis, a recent diffusion speech synthesis model called
DiffGAN-TTS, utilizing the structure of GANs, has been introduced and
demonstrates superior performance in both speech quality and generation speed.
In this paper, to further enhance the performance of DiffGAN-TTS, we propose a
speech synthesis model with two discriminators: a diffusion discriminator for
learning the distribution of the reverse process and a spectrogram
discriminator for learning the distribution of the generated data. Objective
metrics such as structural similarity index measure (SSIM), mel-cepstral
distortion (MCD), F0 root mean squared error (F0 RMSE), short-time objective
intelligibility (STOI), perceptual evaluation of speech quality (PESQ), as well
as subjective metrics like mean opinion score (MOS), are used to evaluate the
performance of the proposed model. The evaluation results show that the
proposed model outperforms recent state-of-the-art models such as FastSpeech2
and DiffGAN-TTS in various metrics. Our implementation and audio samples are
located on GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ko_M/0/1/0/all/0/1"&gt;Myeongjin Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yong-Hoon Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Sparsity Can Simplify Machine Unlearning. (arXiv:2304.04934v7 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.04934</id>
        <link href="http://arxiv.org/abs/2304.04934"/>
        <updated>2023-08-05T00:48:26.522Z</updated>
        <summary type="html"><![CDATA[In response to recent data regulation requirements, machine unlearning (MU)
has emerged as a critical process to remove the influence of specific examples
from a given model. Although exact unlearning can be achieved through complete
model retraining using the remaining dataset, the associated computational
costs have driven the development of efficient, approximate unlearning
techniques. Moving beyond data-centric MU approaches, our study introduces a
novel model-based perspective: model sparsification via weight pruning, which
is capable of reducing the gap between exact unlearning and approximate
unlearning. We show in both theory and practice that model sparsity can boost
the multi-criteria unlearning performance of an approximate unlearner, closing
the approximation gap, while continuing to be efficient. This leads to a new MU
paradigm, termed prune first, then unlearn, which infuses a sparse model prior
into the unlearning process. Building on this insight, we also develop a
sparsity-aware unlearning method that utilizes sparsity regularization to
enhance the training process of approximate unlearning. Extensive experiments
show that our proposals consistently benefit MU in various unlearning
scenarios. A notable highlight is the 77% unlearning efficacy gain of
fine-tuning (one of the simplest unlearning methods) when using sparsity-aware
unlearning. Furthermore, we demonstrate the practical impact of our proposed MU
methods in addressing other machine learning challenges, such as defending
against backdoor attacks and enhancing transfer learning. Codes are available
at https://github.com/OPTML-Group/Unlearn-Sparse.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1"&gt;Jinghan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiancheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ram_P/0/1/0/all/0/1"&gt;Parikshit Ram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yuguang Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Gaowen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1"&gt;Pranay Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sijia Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Job Shop Scheduling via Deep Reinforcement Learning: a Sequence to Sequence approach. (arXiv:2308.01797v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2308.01797</id>
        <link href="http://arxiv.org/abs/2308.01797"/>
        <updated>2023-08-05T00:48:26.507Z</updated>
        <summary type="html"><![CDATA[Job scheduling is a well-known Combinatorial Optimization problem with
endless applications. Well planned schedules bring many benefits in the context
of automated systems: among others, they limit production costs and waste.
Nevertheless, the NP-hardness of this problem makes it essential to use
heuristics whose design is difficult, requires specialized knowledge and often
produces methods tailored to the specific task. This paper presents an original
end-to-end Deep Reinforcement Learning approach to scheduling that
automatically learns dispatching rules. Our technique is inspired by natural
language encoder-decoder models for sequence processing and has never been
used, to the best of our knowledge, for scheduling purposes. We applied and
tested our method in particular to some benchmark instances of Job Shop
Problem, but this technique is general enough to be potentially used to tackle
other different optimal job scheduling tasks with minimal intervention. Results
demonstrate that we outperform many classical approaches exploiting priority
dispatching rules and show competitive results on state-of-the-art Deep
Reinforcement Learning ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bonetta_G/0/1/0/all/0/1"&gt;Giovanni Bonetta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zago_D/0/1/0/all/0/1"&gt;Davide Zago&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cancelliere_R/0/1/0/all/0/1"&gt;Rossella Cancelliere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grosso_A/0/1/0/all/0/1"&gt;Andrea Grosso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models. (arXiv:2308.01557v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2308.01557</id>
        <link href="http://arxiv.org/abs/2308.01557"/>
        <updated>2023-08-05T00:48:26.488Z</updated>
        <summary type="html"><![CDATA[Learning priors on trajectory distributions can help accelerate robot motion
planning optimization. Given previously successful plans, learning trajectory
generative models as priors for a new planning problem is highly desirable.
Prior works propose several ways on utilizing this prior to bootstrapping the
motion planning problem. Either sampling the prior for initializations or using
the prior distribution in a maximum-a-posterior formulation for trajectory
optimization. In this work, we propose learning diffusion models as priors. We
then can sample directly from the posterior trajectory distribution conditioned
on task goals, by leveraging the inverse denoising process of diffusion models.
Furthermore, diffusion has been recently shown to effectively encode data
multimodality in high-dimensional settings, which is particularly well-suited
for large trajectory dataset. To demonstrate our method efficacy, we compare
our proposed method - Motion Planning Diffusion - against several baselines in
simulated planar robot and 7-dof robot arm manipulator environments. To assess
the generalization capabilities of our method, we test it in environments with
previously unseen obstacles. Our experiments show that diffusion models are
strong priors to encode high-dimensional trajectory distributions of robot
motions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_J/0/1/0/all/0/1"&gt;Joao Carvalho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_A/0/1/0/all/0/1"&gt;An T. Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baierl_M/0/1/0/all/0/1"&gt;Mark Baierl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koert_D/0/1/0/all/0/1"&gt;Dorothea Koert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1"&gt;Jan Peters&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distribution-Free Inference for the Regression Function of Binary Classification. (arXiv:2308.01835v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2308.01835</id>
        <link href="http://arxiv.org/abs/2308.01835"/>
        <updated>2023-08-05T00:48:26.474Z</updated>
        <summary type="html"><![CDATA[One of the key objects of binary classification is the regression function,
i.e., the conditional expectation of the class labels given the inputs. With
the regression function not only a Bayes optimal classifier can be defined, but
it also encodes the corresponding misclassification probabilities. The paper
presents a resampling framework to construct exact, distribution-free and
non-asymptotically guaranteed confidence regions for the true regression
function for any user-chosen confidence level. Then, specific algorithms are
suggested to demonstrate the framework. It is proved that the constructed
confidence regions are strongly consistent, that is, any false model is
excluded in the long run with probability one. The exclusion is quantified with
probably approximately correct type bounds, as well. Finally, the algorithms
are validated via numerical experiments, and the methods are compared to
approximate asymptotic confidence ellipsoids.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tamas_A/0/1/0/all/0/1"&gt;Ambrus Tam&amp;#xe1;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Csaji_B/0/1/0/all/0/1"&gt;Bal&amp;#xe1;zs Csan&amp;#xe1;d Cs&amp;#xe1;ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimax Optimal $Q$ Learning with Nearest Neighbors. (arXiv:2308.01490v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01490</id>
        <link href="http://arxiv.org/abs/2308.01490"/>
        <updated>2023-08-05T00:48:26.460Z</updated>
        <summary type="html"><![CDATA[$Q$ learning is a popular model free reinforcement learning method. Most of
existing works focus on analyzing $Q$ learning for finite state and action
spaces. If the state space is continuous, then the original $Q$ learning method
can not be directly used. A modification of the original $Q$ learning method
was proposed in (Shah and Xie, 2018), which estimates $Q$ values with nearest
neighbors. Such modification makes $Q$ learning suitable for continuous state
space. (Shah and Xie, 2018) shows that the convergence rate of estimated $Q$
function is $\tilde{O}(T^{-1/(d+3)})$, which is slower than the minimax lower
bound $\tilde{\Omega}(T^{-1/(d+2)})$, indicating that this method is not
efficient. This paper proposes two new $Q$ learning methods to bridge the gap
of convergence rates in (Shah and Xie, 2018), with one of them being offline,
while the other is online. Despite that we still use nearest neighbor approach
to estimate $Q$ function, the algorithms are crucially different from (Shah and
Xie, 2018). In particular, we replace the kernel nearest neighbor in
discretized region with a direct nearest neighbor approach. Consequently, our
approach significantly improves the convergence rate. Moreover, the time
complexity is also significantly improved in high dimensional state spaces. Our
analysis shows that both offline and online methods are minimax rate optimal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Puning Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1"&gt;Lifeng Lai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MFIM: Megapixel Facial Identity Manipulation. (arXiv:2308.01536v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.01536</id>
        <link href="http://arxiv.org/abs/2308.01536"/>
        <updated>2023-08-05T00:48:26.454Z</updated>
        <summary type="html"><![CDATA[Face swapping is a task that changes a facial identity of a given image to
that of another person. In this work, we propose a novel face-swapping
framework called Megapixel Facial Identity Manipulation (MFIM). The
face-swapping model should achieve two goals. First, it should be able to
generate a high-quality image. We argue that a model which is proficient in
generating a megapixel image can achieve this goal. However, generating a
megapixel image is generally difficult without careful model design. Therefore,
our model exploits pretrained StyleGAN in the manner of GAN-inversion to
effectively generate a megapixel image. Second, it should be able to
effectively transform the identity of a given image. Specifically, it should be
able to actively transform ID attributes (e.g., face shape and eyes) of a given
image into those of another person, while preserving ID-irrelevant attributes
(e.g., pose and expression). To achieve this goal, we exploit 3DMM that can
capture various facial attributes. Specifically, we explicitly supervise our
model to generate a face-swapped image with the desirable attributes using
3DMM. We show that our model achieves state-of-the-art performance through
extensive experiments. Furthermore, we propose a new operation called ID
mixing, which creates a new identity by semantically mixing the identities of
several people. It allows the user to customize the new identity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Na_S/0/1/0/all/0/1"&gt;Sanghyeon Na&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal thinking for decision making on Electronic Health Records: why and how. (arXiv:2308.01605v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2308.01605</id>
        <link href="http://arxiv.org/abs/2308.01605"/>
        <updated>2023-08-05T00:48:26.442Z</updated>
        <summary type="html"><![CDATA[Accurate predictions, as with machine learning, may not suffice to provide
optimal healthcare for every patient. Indeed, prediction can be driven by
shortcuts in the data, such as racial biases. Causal thinking is needed for
data-driven decisions. Here, we give an introduction to the key elements,
focusing on routinely-collected data, electronic health records (EHRs) and
claims data. Using such data to assess the value of an intervention requires
care: temporal dependencies and existing practices easily confound the causal
effect. We present a step-by-step framework to help build valid decision making
from real-life patient records by emulating a randomized trial before
individualizing decisions, eg with machine learning. Our framework highlights
the most important pitfalls and considerations in analysing EHRs or claims data
to draw causal conclusions. We illustrate the various choices in studying the
effect of albumin on sepsis mortality in the Medical Information Mart for
Intensive Care database (MIMIC-IV). We study the impact of various choices at
every step, from feature extraction to causal-estimator selection. In a
tutorial spirit, the code and the data are openly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Doutreligne_M/0/1/0/all/0/1"&gt;Matthieu Doutreligne&lt;/a&gt; (SODA), &lt;a href="http://arxiv.org/find/stat/1/au:+Struja_T/0/1/0/all/0/1"&gt;Tristan Struja&lt;/a&gt; (MIT, SODA), &lt;a href="http://arxiv.org/find/stat/1/au:+Abecassis_J/0/1/0/all/0/1"&gt;Judith Abecassis&lt;/a&gt; (SODA), &lt;a href="http://arxiv.org/find/stat/1/au:+Morgand_C/0/1/0/all/0/1"&gt;Claire Morgand&lt;/a&gt; (ARS IDF), &lt;a href="http://arxiv.org/find/stat/1/au:+Celi_L/0/1/0/all/0/1"&gt;Leo Anthony Celi&lt;/a&gt; (MIT), &lt;a href="http://arxiv.org/find/stat/1/au:+Varoquaux_G/0/1/0/all/0/1"&gt;Ga&amp;#xeb;l Varoquaux&lt;/a&gt; (SODA)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compressed and distributed least-squares regression: convergence rates with applications to Federated Learning. (arXiv:2308.01358v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01358</id>
        <link href="http://arxiv.org/abs/2308.01358"/>
        <updated>2023-08-05T00:48:26.424Z</updated>
        <summary type="html"><![CDATA[In this paper, we investigate the impact of compression on stochastic
gradient algorithms for machine learning, a technique widely used in
distributed and federated learning. We underline differences in terms of
convergence rates between several unbiased compression operators, that all
satisfy the same condition on their variance, thus going beyond the classical
worst-case analysis. To do so, we focus on the case of least-squares regression
(LSR) and analyze a general stochastic approximation algorithm for minimizing
quadratic functions relying on a random field. We consider weak assumptions on
the random field, tailored to the analysis (specifically, expected H\"older
regularity), and on the noise covariance, enabling the analysis of various
randomizing mechanisms, including compression. We then extend our results to
the case of federated learning.

More formally, we highlight the impact on the convergence of the covariance
$\mathfrak{C}_{\mathrm{ania}}$ of the additive noise induced by the algorithm.
We demonstrate despite the non-regularity of the stochastic field, that the
limit variance term scales with $\mathrm{Tr}(\mathfrak{C}_{\mathrm{ania}}
H^{-1})/K$ (where $H$ is the Hessian of the optimization problem and $K$ the
number of iterations) generalizing the rate for the vanilla LSR case where it
is $\sigma^2 \mathrm{Tr}(H H^{-1}) / K = \sigma^2 d / K$ (Bach and Moulines,
2013). Then, we analyze the dependency of $\mathfrak{C}_{\mathrm{ania}}$ on the
compression strategy and ultimately its impact on convergence, first in the
centralized case, then in two heterogeneous FL frameworks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Philippenko_C/0/1/0/all/0/1"&gt;Constantin Philippenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dieuleveut_A/0/1/0/all/0/1"&gt;Aymeric Dieuleveut&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Matrix Estimation for Individual Fairness. (arXiv:2302.02096v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.02096</id>
        <link href="http://arxiv.org/abs/2302.02096"/>
        <updated>2023-08-05T00:48:26.394Z</updated>
        <summary type="html"><![CDATA[In recent years, multiple notions of algorithmic fairness have arisen. One
such notion is individual fairness (IF), which requires that individuals who
are similar receive similar treatment. In parallel, matrix estimation (ME) has
emerged as a natural paradigm for handling noisy data with missing values. In
this work, we connect the two concepts. We show that pre-processing data using
ME can improve an algorithm's IF without sacrificing performance. Specifically,
we show that using a popular ME method known as singular value thresholding
(SVT) to pre-process the data provides a strong IF guarantee under appropriate
conditions. We then show that, under analogous conditions, SVT pre-processing
also yields estimates that are consistent and approximately minimax optimal. As
such, the ME pre-processing step does not, under the stated conditions,
increase the prediction error of the base algorithm, i.e., does not impose a
fairness-performance trade-off. We verify these results on synthetic and real
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Cindy Y. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cen_S/0/1/0/all/0/1"&gt;Sarah H. Cen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1"&gt;Devavrat Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stable and consistent density-based clustering via multiparameter persistence. (arXiv:2005.09048v3 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.09048</id>
        <link href="http://arxiv.org/abs/2005.09048"/>
        <updated>2023-08-05T00:48:26.364Z</updated>
        <summary type="html"><![CDATA[We consider the degree-Rips construction from topological data analysis,
which provides a density-sensitive, multiparameter hierarchical clustering
algorithm. We analyze its stability to perturbations of the input data using
the correspondence-interleaving distance, a metric for hierarchical clusterings
that we introduce. Taking certain one-parameter slices of degree-Rips recovers
well-known methods for density-based clustering, but we show that these methods
are unstable. However, we prove that degree-Rips, as a multiparameter object,
is stable, and we propose an alternative approach for taking slices of
degree-Rips, which yields a one-parameter hierarchical clustering algorithm
with better stability properties. We prove that this algorithm is consistent,
using the correspondence-interleaving distance. We provide an algorithm for
extracting a single clustering from one-parameter hierarchical clusterings,
which is stable with respect to the correspondence-interleaving distance. And,
we integrate these methods into a pipeline for density-based clustering, which
we call Persistable. Adapting tools from multiparameter persistent homology, we
propose visualization tools that guide the selection of all parameters of the
pipeline. We demonstrate Persistable on benchmark datasets, showing that it
identifies multi-scale cluster structure in data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Rolle_A/0/1/0/all/0/1"&gt;Alexander Rolle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Scoccola_L/0/1/0/all/0/1"&gt;Luis Scoccola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficiency of First-Order Methods for Low-Rank Tensor Recovery with the Tensor Nuclear Norm Under Strict Complementarity. (arXiv:2308.01677v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2308.01677</id>
        <link href="http://arxiv.org/abs/2308.01677"/>
        <updated>2023-08-05T00:48:26.322Z</updated>
        <summary type="html"><![CDATA[We consider convex relaxations for recovering low-rank tensors based on
constrained minimization over a ball induced by the tensor nuclear norm,
recently introduced in \cite{tensor_tSVD}. We build on a recent line of results
that considered convex relaxations for the recovery of low-rank matrices and
established that under a strict complementarity condition (SC), both the
convergence rate and per-iteration runtime of standard gradient methods may
improve dramatically. We develop the appropriate strict complementarity
condition for the tensor nuclear norm ball and obtain the following main
results under this condition: 1. When the objective to minimize is of the form
$f(\mX)=g(\mA\mX)+\langle{\mC,\mX}\rangle$ , where $g$ is strongly convex and
$\mA$ is a linear map (e.g., least squares), a quadratic growth bound holds,
which implies linear convergence rates for standard projected gradient methods,
despite the fact that $f$ need not be strongly convex. 2. For a smooth
objective function, when initialized in certain proximity of an optimal
solution which satisfies SC, standard projected gradient methods only require
SVD computations (for projecting onto the tensor nuclear norm ball) of rank
that matches the tubal rank of the optimal solution. In particular, when the
tubal rank is constant, this implies nearly linear (in the size of the tensor)
runtime per iteration, as opposed to super linear without further assumptions.
3. For a nonsmooth objective function which admits a popular smooth
saddle-point formulation, we derive similar results to the latter for the well
known extragradient method. An additional contribution which may be of
independent interest, is the rigorous extension of many basic results regarding
tensors of arbitrary order, which were previously obtained only for third-order
tensors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Garber_D/0/1/0/all/0/1"&gt;Dan Garber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kaplan_A/0/1/0/all/0/1"&gt;Atara Kaplan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An efficient, provably exact, practical algorithm for the 0-1 loss linear classification problem. (arXiv:2306.12344v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.12344</id>
        <link href="http://arxiv.org/abs/2306.12344"/>
        <updated>2023-08-05T00:48:26.321Z</updated>
        <summary type="html"><![CDATA[Algorithms for solving the linear classification problem have a long history,
dating back at least to 1936 with linear discriminant analysis. For linearly
separable data, many algorithms can obtain the exact solution to the
corresponding 0-1 loss classification problem efficiently, but for data which
is not linearly separable, it has been shown that this problem, in full
generality, is NP-hard. Alternative approaches all involve approximations of
some kind, including the use of surrogates for the 0-1 loss (for example, the
hinge or logistic loss) or approximate combinatorial search, none of which can
be guaranteed to solve the problem exactly. Finding efficient algorithms to
obtain an exact i.e. globally optimal solution for the 0-1 loss linear
classification problem with fixed dimension, remains an open problem. In
research we report here, we detail the rigorous construction of a new
algorithm, incremental cell enumeration (ICE), that can solve the 0-1 loss
classification problem exactly in polynomial time. We prove correctness using
concepts from the theory of hyperplane arrangements and oriented matroids. We
demonstrate the effectiveness of this algorithm on synthetic and real-world
datasets, showing optimal accuracy both in and out-of-sample, in practical
computational time. We also empirically demonstrate how the use of approximate
upper bound leads to polynomial time run-time improvements to the algorithm
whilst retaining exactness. To our knowledge, this is the first,
rigorously-proven polynomial time, practical algorithm for this long-standing
problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xi He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_W/0/1/0/all/0/1"&gt;Waheed Ul Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Little_M/0/1/0/all/0/1"&gt;Max A. Little&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularization, early-stopping and dreaming: a Hopfield-like setup to address generalization and overfitting. (arXiv:2308.01421v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01421</id>
        <link href="http://arxiv.org/abs/2308.01421"/>
        <updated>2023-08-05T00:48:26.249Z</updated>
        <summary type="html"><![CDATA[In this work we approach attractor neural networks from a machine learning
perspective: we look for optimal network parameters by applying a gradient
descent over a regularized loss function. Within this framework, the optimal
neuron-interaction matrices turn out to be a class of matrices which correspond
to Hebbian kernels revised by iteratively applying some unlearning protocols.
Remarkably, the number of unlearning steps is proved to be related to the
regularization hyperparameters of the loss function and to the training time.
Thus, we can design strategies to avoid overfitting that are formulated in
terms of the algebraic properties of the interaction matrix, or, equivalently,
in terms of regularization tuning and early-stopping strategies. The
generalization capabilities of these attractor networks are also investigated:
analytical results are obtained for random synthetic datasets, next, the
emerging picture is corroborated by numerical experiments that highlight the
existence of several regimes (i.e., overfitting, failure and success) as the
dataset parameters are varied.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agliari_E/0/1/0/all/0/1"&gt;Elena Agliari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aquaro_M/0/1/0/all/0/1"&gt;Miriam Aquaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alemanno_F/0/1/0/all/0/1"&gt;Francesco Alemanno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fachechi_A/0/1/0/all/0/1"&gt;Alberto Fachechi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v5 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.04370</id>
        <link href="http://arxiv.org/abs/2304.04370"/>
        <updated>2023-08-05T00:48:26.047Z</updated>
        <summary type="html"><![CDATA[Human intelligence excels at combining basic skills to solve complex tasks.
This capability is vital for Artificial Intelligence (AI) and should be
embedded in comprehensive intelligent models, enabling them to harness expert
models for complex task-solving towards Artificial General Intelligence (AGI).
Large Language Models (LLMs) show promising learning and reasoning abilities,
and can effectively use external models, tools or APIs to tackle complex
problems. In this work, we introduce OpenAGI, an open-source AGI research
platform designed for multi-step, real-world tasks. Specifically, OpenAGI uses
a dual strategy, integrating standard benchmark tasks for benchmarking and
evaluation, and open-ended tasks including more expandable models, tools or
APIs for creative problem-solving. Tasks are presented as natural language
queries to the LLM, which then selects and executes appropriate models. We also
propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses
task results to improve the LLM's ability, which creates a self-improving AI
feedback loop. While we acknowledge that AGI is a broad and multifaceted
research challenge with no singularly defined solution path, the integration of
LLMs with domain-specific expert models, inspired by mirroring the blend of
general and specialized intelligence in humans, offers a promising approach
towards AGI. We are open-sourcing the OpenAGI project's code, dataset,
benchmarks, evaluation methods, and demo to foster community involvement in AGI
advancement: https://github.com/agiresearch/OpenAGI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1"&gt;Wenyue Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_K/0/1/0/all/0/1"&gt;Kai Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1"&gt;Jianchao Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1"&gt;Juntao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shuyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zelong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfeng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computational Long Exposure Mobile Photography. (arXiv:2308.01379v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.01379</id>
        <link href="http://arxiv.org/abs/2308.01379"/>
        <updated>2023-08-05T00:48:25.989Z</updated>
        <summary type="html"><![CDATA[Long exposure photography produces stunning imagery, representing moving
elements in a scene with motion-blur. It is generally employed in two
modalities, producing either a foreground or a background blur effect.
Foreground blur images are traditionally captured on a tripod-mounted camera
and portray blurred moving foreground elements, such as silky water or light
trails, over a perfectly sharp background landscape. Background blur images,
also called panning photography, are captured while the camera is tracking a
moving subject, to produce an image of a sharp subject over a background
blurred by relative motion. Both techniques are notoriously challenging and
require additional equipment and advanced skills. In this paper, we describe a
computational burst photography system that operates in a hand-held smartphone
camera app, and achieves these effects fully automatically, at the tap of the
shutter button. Our approach first detects and segments the salient subject. We
track the scene motion over multiple frames and align the images in order to
preserve desired sharpness and to produce aesthetically pleasing motion
streaks. We capture an under-exposed burst and select the subset of input
frames that will produce blur trails of controlled length, regardless of scene
or camera motion velocity. We predict inter-frame motion and synthesize
motion-blur to fill the temporal gaps between the input frames. Finally, we
composite the blurred image with the sharp regular exposure to protect the
sharpness of faces or areas of the scene that are barely moving, and produce a
final high resolution and high dynamic range (HDR) photograph. Our system
democratizes a capability previously reserved to professionals, and makes this
creative style accessible to most casual photographers.

More information and supplementary material can be found on our project
webpage: https://motion-mode.github.io/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tabellion_E/0/1/0/all/0/1"&gt;Eric Tabellion&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karnad_N/0/1/0/all/0/1"&gt;Nikhil Karnad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaser_N/0/1/0/all/0/1"&gt;Noa Glaser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_B/0/1/0/all/0/1"&gt;Ben Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1"&gt;David E. Jacobs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pritch_Y/0/1/0/all/0/1"&gt;Yael Pritch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computer Vision Estimation of Emotion Reaction Intensity in the Wild. (arXiv:2303.10741v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.10741</id>
        <link href="http://arxiv.org/abs/2303.10741"/>
        <updated>2023-08-05T00:48:25.982Z</updated>
        <summary type="html"><![CDATA[Emotions play an essential role in human communication. Developing computer
vision models for automatic recognition of emotion expression can aid in a
variety of domains, including robotics, digital behavioral healthcare, and
media analytics. There are three types of emotional representations which are
traditionally modeled in affective computing research: Action Units, Valence
Arousal (VA), and Categorical Emotions. As part of an effort to move beyond
these representations towards more fine-grained labels, we describe our
submission to the newly introduced Emotional Reaction Intensity (ERI)
Estimation challenge in the 5th competition for Affective Behavior Analysis
in-the-Wild (ABAW). We developed four deep neural networks trained in the
visual domain and a multimodal model trained with both visual and audio
features to predict emotion reaction intensity. Our best performing model on
the Hume-Reaction dataset achieved an average Pearson correlation coefficient
of 0.4080 on the test set using a pre-trained ResNet50 model. This work
provides a first step towards the development of production-grade models which
predict emotion reaction intensities rather than discrete emotion categories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1"&gt;Yang Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kargarandehkordi_A/0/1/0/all/0/1"&gt;Ali Kargarandehkordi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mutlu_O/0/1/0/all/0/1"&gt;Onur Cezmi Mutlu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Surabhi_S/0/1/0/all/0/1"&gt;Saimourya Surabhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Honarmand_M/0/1/0/all/0/1"&gt;Mohammadmahdi Honarmand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wall_D/0/1/0/all/0/1"&gt;Dennis Paul Wall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1"&gt;Peter Washington&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantification of Predictive Uncertainty via Inference-Time Sampling. (arXiv:2308.01731v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01731</id>
        <link href="http://arxiv.org/abs/2308.01731"/>
        <updated>2023-08-05T00:48:25.976Z</updated>
        <summary type="html"><![CDATA[Predictive variability due to data ambiguities has typically been addressed
via construction of dedicated models with built-in probabilistic capabilities
that are trained to predict uncertainty estimates as variables of interest.
These approaches require distinct architectural components and training
mechanisms, may include restrictive assumptions and exhibit overconfidence,
i.e., high confidence in imprecise predictions. In this work, we propose a
post-hoc sampling strategy for estimating predictive uncertainty accounting for
data ambiguity. The method can generate different plausible outputs for a given
input and does not assume parametric forms of predictive distributions. It is
architecture agnostic and can be applied to any feed-forward deterministic
network without changes to the architecture or training procedure. Experiments
on regression tasks on imaging and non-imaging input data show the method's
ability to generate diverse and multi-modal predictive distributions, and a
desirable correlation of the estimated uncertainty with the prediction error.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tothova_K/0/1/0/all/0/1"&gt;Katar&amp;#xed;na T&amp;#xf3;thov&amp;#xe1;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ladicky_L/0/1/0/all/0/1"&gt;&amp;#x13d;ubor Ladick&amp;#xfd;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thul_D/0/1/0/all/0/1"&gt;Daniel Thul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1"&gt;Marc Pollefeys&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konukoglu_E/0/1/0/all/0/1"&gt;Ender Konukoglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Curricular Transfer Learning for Sentence Encoded Tasks. (arXiv:2308.01849v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2308.01849</id>
        <link href="http://arxiv.org/abs/2308.01849"/>
        <updated>2023-08-05T00:48:25.970Z</updated>
        <summary type="html"><![CDATA[Fine-tuning language models in a downstream task is the standard approach for
many state-of-the-art methodologies in the field of NLP. However, when the
distribution between the source task and target task drifts, \textit{e.g.},
conversational environments, these gains tend to be diminished. This article
proposes a sequence of pre-training steps (a curriculum) guided by "data
hacking" and grammar analysis that allows further gradual adaptation between
pre-training distributions. In our experiments, we acquire a considerable
improvement from our method compared to other known pre-training approaches for
the MultiWoZ task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sa_J/0/1/0/all/0/1"&gt;Jader Martins Camboim de S&amp;#xe1;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanches_M/0/1/0/all/0/1"&gt;Matheus Ferraroni Sanches&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Souza_R/0/1/0/all/0/1"&gt;Rafael Roque de Souza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reis_J/0/1/0/all/0/1"&gt;J&amp;#xfa;lio Cesar dos Reis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villas_L/0/1/0/all/0/1"&gt;Leandro Aparecido Villas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Introduction to Bi-level Optimization: Foundations and Applications in Signal Processing and Machine Learning. (arXiv:2308.00788v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.00788</id>
        <link href="http://arxiv.org/abs/2308.00788"/>
        <updated>2023-08-05T00:48:25.938Z</updated>
        <summary type="html"><![CDATA[Recently, bi-level optimization (BLO) has taken center stage in some very
exciting developments in the area of signal processing (SP) and machine
learning (ML). Roughly speaking, BLO is a classical optimization problem that
involves two levels of hierarchy (i.e., upper and lower levels), wherein
obtaining the solution to the upper-level problem requires solving the
lower-level one. BLO has become popular largely because it is powerful in
modeling problems in SP and ML, among others, that involve optimizing nested
objective functions. Prominent applications of BLO range from resource
allocation for wireless systems to adversarial machine learning. In this work,
we focus on a class of tractable BLO problems that often appear in SP and ML
applications. We provide an overview of some basic concepts of this class of
BLO problems, such as their optimality conditions, standard algorithms
(including their optimization principles and practical implementations), as
well as how they can be leveraged to obtain state-of-the-art results for a
number of key SP and ML applications. Further, we discuss some recent advances
in BLO theory, its implications for applications, and point out some
limitations of the state-of-the-art that require significant future research
efforts. Overall, we hope that this article can serve to accelerate the
adoption of BLO as a generic tool to model, analyze, and innovate on a wide
array of emerging SP and ML applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yihua Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khanduri_P/0/1/0/all/0/1"&gt;Prashant Khanduri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsaknakis_I/0/1/0/all/0/1"&gt;Ioannis Tsaknakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yuguang Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1"&gt;Mingyi Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sijia Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Missing Value Filling Model Based on Feature Fusion Enhanced Autoencoder. (arXiv:2208.13495v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.13495</id>
        <link href="http://arxiv.org/abs/2208.13495"/>
        <updated>2023-08-05T00:48:25.931Z</updated>
        <summary type="html"><![CDATA[With the advent of the big data era, the data quality problem is becoming
more critical. Among many factors, data with missing values is one primary
issue, and thus developing effective imputation models is a key topic in the
research community. Recently, a major research direction is to employ neural
network models such as self-organizing mappings or automatic encoders for
filling missing values. However, these classical methods can hardly discover
interrelated features and common features simultaneously among data attributes.
Especially, it is a very typical problem for classical autoencoders that they
often learn invalid constant mappings, which dramatically hurts the filling
performance. To solve the above-mentioned problems, we propose a
missing-value-filling model based on a feature-fusion-enhanced autoencoder. We
first incorporate into an autoencoder a hidden layer that consists of
de-tracking neurons and radial basis function neurons, which can enhance the
ability of learning interrelated features and common features. Besides, we
develop a missing value filling strategy based on dynamic clustering that is
incorporated into an iterative optimization process. This design can enhance
the multi-dimensional feature fusion ability and thus improves the dynamic
collaborative missing-value-filling performance. The effectiveness of the
proposed model is validated by extensive experiments compared to a variety of
baseline methods on thirteen data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinyao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Shengdong Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianrui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_F/0/1/0/all/0/1"&gt;Fei Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hebbian Deep Learning Without Feedback. (arXiv:2209.11883v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2209.11883</id>
        <link href="http://arxiv.org/abs/2209.11883"/>
        <updated>2023-08-05T00:48:25.924Z</updated>
        <summary type="html"><![CDATA[Recent approximations to backpropagation (BP) have mitigated many of BP's
computational inefficiencies and incompatibilities with biology, but important
limitations still remain. Moreover, the approximations significantly decrease
accuracy in benchmarks, suggesting that an entirely different approach may be
more fruitful. Here, grounded on recent theory for Hebbian learning in soft
winner-take-all networks, we present multilayer SoftHebb, i.e. an algorithm
that trains deep neural networks, without any feedback, target, or error
signals. As a result, it achieves efficiency by avoiding weight transport,
non-local plasticity, time-locking of layer updates, iterative equilibria, and
(self-) supervisory or other feedback signals -- which were necessary in other
approaches. Its increased efficiency and biological compatibility do not trade
off accuracy compared to state-of-the-art bio-plausible learning, but rather
improve it. With up to five hidden layers and an added linear classifier,
accuracies on MNIST, CIFAR-10, STL-10, and ImageNet, respectively reach 99.4%,
80.3%, 76.2%, and 27.3%. In conclusion, SoftHebb shows with a radically
different approach from BP that Deep Learning over few layers may be plausible
in the brain and increases the accuracy of bio-plausible machine learning. Code
is available at https://github.com/NeuromorphicComputing/SoftHebb.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Journe_A/0/1/0/all/0/1"&gt;Adrien Journ&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_H/0/1/0/all/0/1"&gt;Hector Garcia Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qinghai Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moraitis_T/0/1/0/all/0/1"&gt;Timoleon Moraitis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient neural supersampling on a novel gaming dataset. (arXiv:2308.01483v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.01483</id>
        <link href="http://arxiv.org/abs/2308.01483"/>
        <updated>2023-08-05T00:48:25.917Z</updated>
        <summary type="html"><![CDATA[Real-time rendering for video games has become increasingly challenging due
to the need for higher resolutions, framerates and photorealism. Supersampling
has emerged as an effective solution to address this challenge. Our work
introduces a novel neural algorithm for supersampling rendered content that is
4 times more efficient than existing methods while maintaining the same level
of accuracy. Additionally, we introduce a new dataset which provides auxiliary
modalities such as motion vectors and depth generated using graphics rendering
features like viewport jittering and mipmap biasing at different resolutions.
We believe that this dataset fills a gap in the current dataset landscape and
can serve as a valuable resource to help measure progress in the field and
advance the state-of-the-art in super-resolution techniques for gaming content.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mercier_A/0/1/0/all/0/1"&gt;Antoine Mercier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erasmus_R/0/1/0/all/0/1"&gt;Ruan Erasmus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savani_Y/0/1/0/all/0/1"&gt;Yashesh Savani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhingra_M/0/1/0/all/0/1"&gt;Manik Dhingra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1"&gt;Fatih Porikli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berger_G/0/1/0/all/0/1"&gt;Guillaume Berger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatically Bounding the Taylor Remainder Series: Tighter Bounds and New Applications. (arXiv:2212.11429v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.11429</id>
        <link href="http://arxiv.org/abs/2212.11429"/>
        <updated>2023-08-05T00:48:25.889Z</updated>
        <summary type="html"><![CDATA[We present a new algorithm for automatically bounding the Taylor remainder
series. In the special case of a scalar function $f: \mathbb{R} \to
\mathbb{R}$, our algorithm takes as input a reference point $x_0$, trust region
$[a, b]$, and integer $k \ge 1$, and returns an interval $I$ such that $f(x) -
\sum_{i=0}^{k-1} \frac {1} {i!} f^{(i)}(x_0) (x - x_0)^i \in I (x - x_0)^k$ for
all $x \in [a, b]$. As in automatic differentiation, the function $f$ is
provided to the algorithm in symbolic form, and must be composed of known
atomic functions.

At a high level, our algorithm has two steps. First, for a variety of
commonly-used elementary functions (e.g., $\exp$, $\log$), we use
recently-developed theory to derive sharp polynomial upper and lower bounds on
the Taylor remainder series. We then recursively combine the bounds for the
elementary functions using an interval arithmetic variant of Taylor-mode
automatic differentiation. Our algorithm can make efficient use of machine
learning hardware accelerators, and we provide an open source implementation in
JAX.

We then turn our attention to applications. Most notably, in a companion
paper we use our new machinery to create the first universal
majorization-minimization optimization algorithms: algorithms that iteratively
minimize an arbitrary loss using a majorizer that is derived automatically,
rather than by hand. We also show that our automatically-derived bounds can be
used for verified global optimization and numerical integration, and to prove
sharper versions of Jensen's inequality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Streeter_M/0/1/0/all/0/1"&gt;Matthew Streeter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dillon_J/0/1/0/all/0/1"&gt;Joshua V. Dillon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing Systematic Weaknesses of DNNs using Counterfactuals. (arXiv:2308.01614v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01614</id>
        <link href="http://arxiv.org/abs/2308.01614"/>
        <updated>2023-08-05T00:48:25.882Z</updated>
        <summary type="html"><![CDATA[With the advancement of DNNs into safety-critical applications, testing
approaches for such models have gained more attention. A current direction is
the search for and identification of systematic weaknesses that put safety
assumptions based on average performance values at risk. Such weaknesses can
take on the form of (semantically coherent) subsets or areas in the input space
where a DNN performs systematically worse than its expected average. However,
it is non-trivial to attribute the reason for such observed low performances to
the specific semantic features that describe the subset. For instance,
inhomogeneities within the data w.r.t. other (non-considered) attributes might
distort results. However, taking into account all (available) attributes and
their interaction is often computationally highly expensive. Inspired by
counterfactual explanations, we propose an effective and computationally cheap
algorithm to validate the semantic attribution of existing subsets, i.e., to
check whether the identified attribute is likely to have caused the degraded
performance. We demonstrate this approach on an example from the autonomous
driving domain using highly annotated simulated data, where we show for a
semantic segmentation model that (i) performance differences among the
different pedestrian assets exist, but (ii) only in some cases is the asset
type itself the reason for this reduction in the performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gannamaneni_S/0/1/0/all/0/1"&gt;Sujan Sai Gannamaneni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mock_M/0/1/0/all/0/1"&gt;Michael Mock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akila_M/0/1/0/all/0/1"&gt;Maram Akila&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks. (arXiv:2308.01423v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2308.01423</id>
        <link href="http://arxiv.org/abs/2308.01423"/>
        <updated>2023-08-05T00:48:25.875Z</updated>
        <summary type="html"><![CDATA[ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to
predict and generate of metal-organic frameworks (MOFs). By leveraging a
large-scale language model (gpt-3.5-turbo), ChatMOF extracts key details from
textual inputs and delivers appropriate responses, thus eliminating the
necessity for rigid structured queries. The system is comprised of three core
components (i.e. an agent, a toolkit, and an evaluator) and it forms a robust
pipeline that manages a variety of tasks, including data retrieval, property
prediction, and structure generation. The study further explores the merits and
constraints of using large language models (LLMs) AI system in material
sciences using and showcases its transformative potential for future
advancements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yeonghun Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jihan Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Model the World with Language. (arXiv:2308.01399v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2308.01399</id>
        <link href="http://arxiv.org/abs/2308.01399"/>
        <updated>2023-08-05T00:48:25.869Z</updated>
        <summary type="html"><![CDATA[To interact with humans in the world, agents need to understand the diverse
types of language that people use, relate them to the visual world, and act
based on them. While current agents learn to execute simple language
instructions from task rewards, we aim to build agents that leverage diverse
language that conveys general knowledge, describes the state of the world,
provides interactive feedback, and more. Our key idea is that language helps
agents predict the future: what will be observed, how the world will behave,
and which situations will be rewarded. This perspective unifies language
understanding with future prediction as a powerful self-supervised learning
objective. We present Dynalang, an agent that learns a multimodal world model
that predicts future text and image representations and learns to act from
imagined model rollouts. Unlike traditional agents that use language only to
predict actions, Dynalang acquires rich language understanding by using past
language also to predict future language, video, and rewards. In addition to
learning from online interaction in an environment, Dynalang can be pretrained
on datasets of text, video, or both without actions or rewards. From using
language hints in grid worlds to navigating photorealistic scans of homes,
Dynalang utilizes diverse types of language to improve task performance,
including environment descriptions, game rules, and instructions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jessy Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuqing Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watkins_O/0/1/0/all/0/1"&gt;Olivia Watkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hafner_D/0/1/0/all/0/1"&gt;Danijar Hafner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1"&gt;Dan Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1"&gt;Anca Dragan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition with Limited Annotations. (arXiv:2308.01890v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.01890</id>
        <link href="http://arxiv.org/abs/2308.01890"/>
        <updated>2023-08-05T00:48:25.851Z</updated>
        <summary type="html"><![CDATA[Multi-label image recognition in the low-label regime is a task of great
challenge and practical significance. Previous works have focused on learning
the alignment between textual and visual spaces to compensate for limited image
labels, yet may suffer from reduced accuracy due to the scarcity of
high-quality multi-label annotations. In this research, we leverage the
powerful alignment between textual and visual features pretrained with millions
of auxiliary image-text pairs. We introduce an efficient and effective
framework called Evidence-guided Dual Context Optimization (DualCoOp++), which
serves as a unified approach for addressing partial-label and zero-shot
multi-label recognition. In DualCoOp++ we separately encode evidential,
positive, and negative contexts for target classes as parametric components of
the linguistic input (i.e., prompts). The evidential context aims to discover
all the related visual content for the target class, and serves as guidance to
aggregate positive and negative contexts from the spatial domain of the image,
enabling better distinguishment between similar categories. Additionally, we
introduce a Winner-Take-All module that promotes inter-class interaction during
training, while avoiding the need for extra parameters and costs. As DualCoOp++
imposes minimal additional learnable overhead on the pretrained vision-language
framework, it enables rapid adaptation to multi-label recognition tasks with
limited annotations and even unseen classes. Experiments on standard
multi-label recognition benchmarks across two challenging low-label settings
demonstrate the superior performance of our approach compared to
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1"&gt;Ping Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Ximeng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1"&gt;Stan Sclaroff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1"&gt;Kate Saenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Follow the Soldiers with Optimized Single-Shot Multibox Detection and Reinforcement Learning. (arXiv:2308.01389v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2308.01389</id>
        <link href="http://arxiv.org/abs/2308.01389"/>
        <updated>2023-08-05T00:48:25.846Z</updated>
        <summary type="html"><![CDATA[Nowadays, autonomous cars are gaining traction due to their numerous
potential applications on battlefields and in resolving a variety of other
real-world challenges. The main goal of our project is to build an autonomous
system using DeepRacer which will follow a specific person (for our project, a
soldier) when they will be moving in any direction. Two main components to
accomplish this project is an optimized Single-Shot Multibox Detection (SSD)
object detection model and a Reinforcement Learning (RL) model. We accomplished
the task using SSD Lite instead of SSD and at the end, compared the results
among SSD, SSD with Neural Computing Stick (NCS), and SSD Lite. Experimental
results show that SSD Lite gives better performance among these three
techniques and exhibits a considerable boost in inference speed (~2-3 times)
without compromising accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_J/0/1/0/all/0/1"&gt;Jumman Hossain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Momtaz_M/0/1/0/all/0/1"&gt;Maliha Momtaz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning. (arXiv:2308.01895v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01895</id>
        <link href="http://arxiv.org/abs/2308.01895"/>
        <updated>2023-08-05T00:48:25.840Z</updated>
        <summary type="html"><![CDATA[Continual learning seeks to enable deep learners to train on a series of
tasks of unknown length without suffering from the catastrophic forgetting of
previous tasks. One effective solution is replay, which involves storing few
previous experiences in memory and replaying them when learning the current
task. However, there is still room for improvement when it comes to selecting
the most informative samples for storage and determining the optimal number of
samples to be stored. This study aims to address these issues with a novel
comparison of the commonly used reservoir sampling to various alternative
population strategies and providing a novel detailed analysis of how to find
the optimal number of stored samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brignac_D/0/1/0/all/0/1"&gt;Daniel Brignac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lobo_N/0/1/0/all/0/1"&gt;Niels Lobo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahalanobis_A/0/1/0/all/0/1"&gt;Abhijit Mahalanobis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain knowledge-informed Synthetic fault sample generation with Health Data Map for cross-domain Planetary Gearbox Fault Diagnosis. (arXiv:2305.19569v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.19569</id>
        <link href="http://arxiv.org/abs/2305.19569"/>
        <updated>2023-08-05T00:48:25.834Z</updated>
        <summary type="html"><![CDATA[Extensive research has been conducted on fault diagnosis of planetary
gearboxes using vibration signals and deep learning (DL) approaches. However,
DL-based methods are susceptible to the domain shift problem caused by varying
operating conditions of the gearbox. Although domain adaptation and data
synthesis methods have been proposed to overcome such domain shifts, they are
often not directly applicable in real-world situations where only healthy data
is available in the target domain. To tackle the challenge of extreme domain
shift scenarios where only healthy data is available in the target domain, this
paper proposes two novel domain knowledge-informed data synthesis methods
utilizing the health data map (HDMap). The two proposed approaches are referred
to as scaled CutPaste and FaultPaste. The HDMap is used to physically represent
the vibration signal of the planetary gearbox as an image-like matrix, allowing
for visualization of fault-related features. CutPaste and FaultPaste are then
applied to generate faulty samples based on the healthy data in the target
domain, using domain knowledge and fault signatures extracted from the source
domain, respectively. In addition to generating realistic faults, the proposed
methods introduce scaling of fault signatures for controlled synthesis of
faults with various severity levels. A case study is conducted on a planetary
gearbox testbed to evaluate the proposed approaches. The results show that the
proposed methods are capable of accurately diagnosing faults, even in cases of
extreme domain shift, and can estimate the severity of faults that have not
been previously observed in the target domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1"&gt;Jong Moon Ha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fink_O/0/1/0/all/0/1"&gt;Olga Fink&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Machine Learning for Discovery: Statistical Challenges \& Opportunities. (arXiv:2308.01475v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2308.01475</id>
        <link href="http://arxiv.org/abs/2308.01475"/>
        <updated>2023-08-05T00:48:25.826Z</updated>
        <summary type="html"><![CDATA[New technologies have led to vast troves of large and complex datasets across
many scientific domains and industries. People routinely use machine learning
techniques to not only process, visualize, and make predictions from this big
data, but also to make data-driven discoveries. These discoveries are often
made using Interpretable Machine Learning, or machine learning models and
techniques that yield human understandable insights. In this paper, we discuss
and review the field of interpretable machine learning, focusing especially on
the techniques as they are often employed to generate new knowledge or make
discoveries from large data sets. We outline the types of discoveries that can
be made using Interpretable Machine Learning in both supervised and
unsupervised settings. Additionally, we focus on the grand challenge of how to
validate these discoveries in a data-driven manner, which promotes trust in
machine learning systems and reproducibility in science. We discuss validation
from both a practical perspective, reviewing approaches based on data-splitting
and stability, as well as from a theoretical perspective, reviewing statistical
results on model selection consistency and uncertainty quantification via
statistical inference. Finally, we conclude by highlighting open challenges in
using interpretable machine learning techniques to make discoveries, including
gaps between theory and practice for validating data-driven-discoveries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Allen_G/0/1/0/all/0/1"&gt;Genevera I. Allen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gan_L/0/1/0/all/0/1"&gt;Luqin Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Lili Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[URET: Universal Robustness Evaluation Toolkit (for Evasion). (arXiv:2308.01840v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01840</id>
        <link href="http://arxiv.org/abs/2308.01840"/>
        <updated>2023-08-05T00:48:25.816Z</updated>
        <summary type="html"><![CDATA[Machine learning models are known to be vulnerable to adversarial evasion
attacks as illustrated by image classification models. Thoroughly understanding
such attacks is critical in order to ensure the safety and robustness of
critical AI tasks. However, most evasion attacks are difficult to deploy
against a majority of AI systems because they have focused on image domain with
only few constraints. An image is composed of homogeneous, numerical,
continuous, and independent features, unlike many other input types to AI
systems used in practice. Furthermore, some input types include additional
semantic and functional constraints that must be observed to generate realistic
adversarial inputs. In this work, we propose a new framework to enable the
generation of adversarial inputs irrespective of the input type and task
domain. Given an input and a set of pre-defined input transformations, our
framework discovers a sequence of transformations that result in a semantically
correct and functional adversarial input. We demonstrate the generality of our
approach on several diverse machine learning tasks with various input
representations. We also show the importance of generating adversarial examples
as they enable the deployment of mitigation techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eykholt_K/0/1/0/all/0/1"&gt;Kevin Eykholt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1"&gt;Taesung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schales_D/0/1/0/all/0/1"&gt;Douglas Schales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1"&gt;Jiyong Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Molloy_I/0/1/0/all/0/1"&gt;Ian Molloy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zorin_M/0/1/0/all/0/1"&gt;Masha Zorin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Evaluate Uncertainty Estimates in Machine Learning for Regression?. (arXiv:2106.03395v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03395</id>
        <link href="http://arxiv.org/abs/2106.03395"/>
        <updated>2023-08-05T00:48:25.798Z</updated>
        <summary type="html"><![CDATA[As neural networks become more popular, the need for accompanying uncertainty
estimates increases. There are currently two main approaches to test the
quality of these estimates. Most methods output a density. They can be compared
by evaluating their loglikelihood on a test set. Other methods output a
prediction interval directly. These methods are often tested by examining the
fraction of test points that fall inside the corresponding prediction
intervals. Intuitively both approaches seem logical. However, we demonstrate
through both theoretical arguments and simulations that both ways of evaluating
the quality of uncertainty estimates have serious flaws. Firstly, both
approaches cannot disentangle the separate components that jointly create the
predictive uncertainty, making it difficult to evaluate the quality of the
estimates of these components. Secondly, a better loglikelihood does not
guarantee better prediction intervals, which is what the methods are often used
for in practice. Moreover, the current approach to test prediction intervals
directly has additional flaws. We show why it is fundamentally flawed to test a
prediction or confidence interval on a single test set. At best, marginal
coverage is measured, implicitly averaging out overconfident and underconfident
predictions. A much more desirable property is pointwise coverage, requiring
the correct coverage for each prediction. We demonstrate through practical
examples that these effects can result in favoring a method, based on the
predictive uncertainty, that has undesirable behaviour of the confidence or
prediction intervals. Finally, we propose a simulation-based testing approach
that addresses these problems while still allowing easy comparison between
different methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sluijterman_L/0/1/0/all/0/1"&gt;Laurens Sluijterman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cator_E/0/1/0/all/0/1"&gt;Eric Cator&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Heskes_T/0/1/0/all/0/1"&gt;Tom Heskes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust, randomized preconditioning for kernel ridge regression. (arXiv:2304.12465v3 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.12465</id>
        <link href="http://arxiv.org/abs/2304.12465"/>
        <updated>2023-08-05T00:48:25.211Z</updated>
        <summary type="html"><![CDATA[This paper introduces two randomized preconditioning techniques for robustly
solving kernel ridge regression (KRR) problems with a medium to large number of
data points ($10^4 \leq N \leq 10^7$). The first method, RPCholesky
preconditioning, is capable of accurately solving the full-data KRR problem in
$O(N^2)$ arithmetic operations, assuming sufficiently rapid polynomial decay of
the kernel matrix eigenvalues. The second method, KRILL preconditioning, offers
an accurate solution to a restricted version of the KRR problem involving $k
\ll N$ selected data centers at a cost of $O((N + k^2) k \log k)$ operations.
The proposed methods solve a broad range of KRR problems and overcome the
failure modes of previous KRR preconditioners, making them ideal for practical
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Diaz_M/0/1/0/all/0/1"&gt;Mateo D&amp;#xed;az&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Epperly_E/0/1/0/all/0/1"&gt;Ethan N. Epperly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Frangella_Z/0/1/0/all/0/1"&gt;Zachary Frangella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Tropp_J/0/1/0/all/0/1"&gt;Joel A. Tropp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Webber_R/0/1/0/all/0/1"&gt;Robert J. Webber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distribution-Free Inference for the Regression Function of Binary Classification. (arXiv:2308.01835v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2308.01835</id>
        <link href="http://arxiv.org/abs/2308.01835"/>
        <updated>2023-08-05T00:48:25.203Z</updated>
        <summary type="html"><![CDATA[One of the key objects of binary classification is the regression function,
i.e., the conditional expectation of the class labels given the inputs. With
the regression function not only a Bayes optimal classifier can be defined, but
it also encodes the corresponding misclassification probabilities. The paper
presents a resampling framework to construct exact, distribution-free and
non-asymptotically guaranteed confidence regions for the true regression
function for any user-chosen confidence level. Then, specific algorithms are
suggested to demonstrate the framework. It is proved that the constructed
confidence regions are strongly consistent, that is, any false model is
excluded in the long run with probability one. The exclusion is quantified with
probably approximately correct type bounds, as well. Finally, the algorithms
are validated via numerical experiments, and the methods are compared to
approximate asymptotic confidence ellipsoids.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tamas_A/0/1/0/all/0/1"&gt;Ambrus Tam&amp;#xe1;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Csaji_B/0/1/0/all/0/1"&gt;Bal&amp;#xe1;zs Csan&amp;#xe1;d Cs&amp;#xe1;ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory. (arXiv:2308.01853v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2308.01853</id>
        <link href="http://arxiv.org/abs/2308.01853"/>
        <updated>2023-08-05T00:48:25.171Z</updated>
        <summary type="html"><![CDATA[Distribution shifts are a serious concern in modern statistical learning as
they can systematically change the properties of the data away from the truth.
We focus on Wasserstein distribution shifts, where every data point may undergo
a slight perturbation, as opposed to the Huber contamination model where a
fraction of observations are outliers. We formulate and study shifts beyond
independent perturbations, exploring Joint Distribution Shifts, where the
per-observation perturbations can be coordinated. We analyze several important
statistical problems, including location estimation, linear regression, and
non-parametric density estimation. Under a squared loss for mean estimation and
prediction error in linear regression, we find the exact minimax risk, a least
favorable perturbation, and show that the sample mean and least squares
estimators are respectively optimal. This holds for both independent and joint
shifts, but the least favorable perturbations and minimax risks differ. For
other problems, we provide nearly optimal estimators and precise finite-sample
bounds. We also introduce several tools for bounding the minimax risk under
distribution shift, such as a smoothing technique for location families, and
generalizations of classical tools including least favorable sequences of
priors, the modulus of continuity, Le Cam's, Fano's, and Assouad's methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chao_P/0/1/0/all/0/1"&gt;Patrick Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dobriban_E/0/1/0/all/0/1"&gt;Edgar Dobriban&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Meta-Learning of Gamma-Minimax Estimators That Leverage Prior Knowledge. (arXiv:2012.05465v5 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05465</id>
        <link href="http://arxiv.org/abs/2012.05465"/>
        <updated>2023-08-05T00:48:25.144Z</updated>
        <summary type="html"><![CDATA[Bayes estimators are well known to provide a means to incorporate prior
knowledge that can be expressed in terms of a single prior distribution.
However, when this knowledge is too vague to express with a single prior, an
alternative approach is needed. Gamma-minimax estimators provide such an
approach. These estimators minimize the worst-case Bayes risk over a set
$\Gamma$ of prior distributions that are compatible with the available
knowledge. Traditionally, Gamma-minimaxity is defined for parametric models. In
this work, we define Gamma-minimax estimators for general models and propose
adversarial meta-learning algorithms to compute them when the set of prior
distributions is constrained by generalized moments. Accompanying convergence
guarantees are also provided. We also introduce a neural network class that
provides a rich, but finite-dimensional, class of estimators from which a
Gamma-minimax estimator can be selected. We illustrate our method in two
settings, namely entropy estimation and a prediction problem that arises in
biodiversity studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Qiu_H/0/1/0/all/0/1"&gt;Hongxiang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Luedtke_A/0/1/0/all/0/1"&gt;Alex Luedtke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Machine Learning for Discovery: Statistical Challenges \& Opportunities. (arXiv:2308.01475v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2308.01475</id>
        <link href="http://arxiv.org/abs/2308.01475"/>
        <updated>2023-08-05T00:48:25.117Z</updated>
        <summary type="html"><![CDATA[New technologies have led to vast troves of large and complex datasets across
many scientific domains and industries. People routinely use machine learning
techniques to not only process, visualize, and make predictions from this big
data, but also to make data-driven discoveries. These discoveries are often
made using Interpretable Machine Learning, or machine learning models and
techniques that yield human understandable insights. In this paper, we discuss
and review the field of interpretable machine learning, focusing especially on
the techniques as they are often employed to generate new knowledge or make
discoveries from large data sets. We outline the types of discoveries that can
be made using Interpretable Machine Learning in both supervised and
unsupervised settings. Additionally, we focus on the grand challenge of how to
validate these discoveries in a data-driven manner, which promotes trust in
machine learning systems and reproducibility in science. We discuss validation
from both a practical perspective, reviewing approaches based on data-splitting
and stability, as well as from a theoretical perspective, reviewing statistical
results on model selection consistency and uncertainty quantification via
statistical inference. Finally, we conclude by highlighting open challenges in
using interpretable machine learning techniques to make discoveries, including
gaps between theory and practice for validating data-driven-discoveries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Allen_G/0/1/0/all/0/1"&gt;Genevera I. Allen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gan_L/0/1/0/all/0/1"&gt;Luqin Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Lili Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confident Neural Network Regression with Bootstrapped Deep Ensembles. (arXiv:2202.10903v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.10903</id>
        <link href="http://arxiv.org/abs/2202.10903"/>
        <updated>2023-08-05T00:48:25.108Z</updated>
        <summary type="html"><![CDATA[With the rise of the popularity and usage of neural networks, trustworthy
uncertainty estimation is becoming increasingly essential. One of the most
prominent uncertainty estimation methods is Deep Ensembles (Lakshminarayanan et
al., 2017) . A classical parametric model has uncertainty in the parameters due
to the fact that the data on which the model is build is a random sample. A
modern neural network has an additional uncertainty component since the
optimization of the network is random. Lakshminarayanan et al. (2017) noted
that Deep Ensembles do not incorporate the classical uncertainty induced by the
effect of finite data. In this paper, we present a computationally cheap
extension of Deep Ensembles for the regression setting, called Bootstrapped
Deep Ensembles, that explicitly takes this classical effect of finite data into
account using a modified version of the parametric bootstrap. We demonstrate
through an experimental study that our method significantly improves upon
standard Deep Ensembles]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sluijterman_L/0/1/0/all/0/1"&gt;Laurens Sluijterman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cator_E/0/1/0/all/0/1"&gt;Eric Cator&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Heskes_T/0/1/0/all/0/1"&gt;Tom Heskes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Slate Policy Optimization: Going Beyond Plackett-Luce. (arXiv:2308.01566v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.01566</id>
        <link href="http://arxiv.org/abs/2308.01566"/>
        <updated>2023-08-05T00:48:24.864Z</updated>
        <summary type="html"><![CDATA[An increasingly important building block of large scale machine learning
systems is based on returning slates; an ordered lists of items given a query.
Applications of this technology include: search, information retrieval and
recommender systems. When the action space is large, decision systems are
restricted to a particular structure to complete online queries quickly. This
paper addresses the optimization of these large scale decision systems given an
arbitrary reward function. We cast this learning problem in a policy
optimization framework and propose a new class of policies, born from a novel
relaxation of decision functions. This results in a simple, yet efficient
learning algorithm that scales to massive action spaces. We compare our method
to the commonly adopted Plackett-Luce policy class and demonstrate the
effectiveness of our approach on problems with action space sizes in the order
of millions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sakhi_O/0/1/0/all/0/1"&gt;Otmane Sakhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohde_D/0/1/0/all/0/1"&gt;David Rohde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chopin_N/0/1/0/all/0/1"&gt;Nicolas Chopin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best Books to Learn Neural Networks in 2023 for Beginners to advanced]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15iggk9/best_books_to_learn_neural_networks_in_2023_for/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15iggk9/best_books_to_learn_neural_networks_in_2023_for/"/>
        <updated>2023-08-05T00:09:23.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Lakshmireddys  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] GPU/Machine on-demand rental that runs Windows 10+ as host OS? (I know, I know...)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ifjfc/d_gpumachine_ondemand_rental_that_runs_windows_10/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ifjfc/d_gpumachine_ondemand_rental_that_runs_windows_10/"/>
        <updated>2023-08-04T23:30:32.000Z</updated>
        <summary type="html"><![CDATA[Anyone know of a cloud service renting on-demand GPU instances (RTX 4090 preferably) that run Windows 10 or newer?
 Believe me, I know...
 I rent on-demand instances from vast.ai for Linux and have been exceedingly happy with their services. I've also used paperspace in the past with good success. Unfortunately, we are in need of RTX 4090s (or roughly equivalent performing Tesla cards) that run on a host OS of Windows 10+ (Server/Win11/etc all fine) because a lot of the modeling software in the industry I work in runs on Windows-only, which is absurd, but nevertheless the truth.
 The fastest I can find are A6000s on paperspace which won't cut the mustard. At the moment we have a 3090 and a bunch of 3070s on-prem which are doing OK but the RTX 4090 is simply much much better, and unsurprisingly the Windows-only software is also not coded in a way that takes advantage of multiple GPUs all that well either.
 Thanks for any help or referrals provided, I really appreciate it. 
 (Have checked paperspace, vastai, runpod, and a few other smaller ones to no avail)
    submitted by    /u/kyleboddy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the concept of Variance in Reinforcement Learning]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15iemvu/understanding_the_concept_of_variance_in/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15iemvu/understanding_the_concept_of_variance_in/"/>
        <updated>2023-08-04T22:52:51.000Z</updated>
        <summary type="html"><![CDATA[I was trying to understand Generalized Advantage Estimation from here and came across the following paragraph - 
 ​
 https://preview.redd.it/l41x655ea6gb1.png?width=778&format=png&auto=webp&s=ffd266c0355a03e1c98ea6de89ca2fc78ed27fd1
 I understood the reason why there could be high bias while bootstrapping the advantage. But why does $A_t^{\inf}$ have high variance. Aren't bias and variance concepts related to estimation by an estimator? While calculating $A_t^{\inf}$, we are literally using the reward values obtained from the environment and therefore there is no estimation involved. Could someone please help me with this?
 ​
    submitted by    /u/Academic-Rent7800  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 8/4/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15ibduy/oneminute_daily_ai_news_842023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15ibduy/oneminute_daily_ai_news_842023/"/>
        <updated>2023-08-04T20:47:02.000Z</updated>
        <summary type="html"><![CDATA[AI.com Now Belongs to Elon Musk. The URL previously belonged to OpenAI, but, somehow, it’s now a landing page for Musk’s AI venture.[1]
 Samsung, Hyundai back AI startup Tenstorrent: Everyone wants competition to Nvidia, says CEO Keller.[2]
 Google’s AI-powered Search Generative Experience is getting a big new feature: images and video. If you’ve enabled the AI-based SGE feature in Search Labs, you’ll now start to see more multimedia in the colorful summary box at the top of your search results.[3]
 White Castle wants to roll out AI-enabled voices to over 100 drive-thrus by 2024 in the hope that people can get their sliders faster with maybe less arguing with someone over speakers.[4]
  
BushAICave.com
 Sources: [1] https://gizmodo.com/ai-dot-com-now-belongs-to-elon-musk-1850707248
 [2] https://www.zdnet.com/google-amp/article/samsung-hyundai-back-ai-startup-tenstorrent-everyone-wants-competition-to-nvidia-says-ceo-keller/
 [3] https://www.theverge.com/2023/8/2/23817107/google-ai-search-generative-experience-videos-links
 [4] https://www.theverge.com/2023/8/2/23817406/white-castle-soundhound-ai-sliders
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interesting RL environments in github]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15ib8x9/interesting_rl_environments_in_github/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15ib8x9/interesting_rl_environments_in_github/"/>
        <updated>2023-08-04T20:41:44.000Z</updated>
        <summary type="html"><![CDATA[I am searching for an interesting but not too complex game envs.
 Preferably with selfplay but should not be very simple nor standard atari like.
 Any recommendations?
    submitted by    /u/Trrrrr88  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is singularity net good or net bad?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15ia6p1/is_singularity_net_good_or_net_bad/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15ia6p1/is_singularity_net_good_or_net_bad/"/>
        <updated>2023-08-04T20:01:22.000Z</updated>
        <summary type="html"><![CDATA[I am curious whether people consider a singularity event to be a net positive or a net negative?
 Are you "pro" or "con"?
 Please explain your reasoning.
    submitted by    /u/kecepa5669  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How do I improve performance?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15i9koq/d_how_do_i_improve_performance/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15i9koq/d_how_do_i_improve_performance/"/>
        <updated>2023-08-04T19:37:58.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone. I am new to this sub so please go easy on me lol.
 I want to implement a neural net that recognizes whether an object in an image matches one of a set of objects with limited training data. 
 I already have worked on a siamese network implementation with triplet loss and ResNet, but I am not getting great performance. Should I do something else?
 For extra info, there are roughly 300 objects/classes and around 7 images per object (most are augmented images)
    submitted by    /u/Nearby_Ad_5644  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Learning to Model the World with Language - UC Berkeley 2023 - Dynalang an agent that learns a multimodal world model that predicts future text and image representations and learns to act from imagined model rollouts!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15i94w0/r_learning_to_model_the_world_with_language_uc/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15i94w0/r_learning_to_model_the_world_with_language_uc/"/>
        <updated>2023-08-04T19:21:07.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2308.01399 
 Github: https://github.com/jlin816/dynalang Code coming soon!
 Abstract:
  
To interact with humans in the world, agents need to understand the diverse types of language that people use, relate them to the visual world, and act based on them. While current agents learn to execute simple language instructions from task rewards, we aim to build agents that leverage diverse language that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that language helps agents predict the future: what will be observed, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning object…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] I created ScoreCast, a tool to predict the outcome of football games in minor football leagues.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15i82xh/p_i_created_scorecast_a_tool_to_predict_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15i82xh/p_i_created_scorecast_a_tool_to_predict_the/"/>
        <updated>2023-08-04T18:39:21.000Z</updated>
        <summary type="html"><![CDATA[https://preview.redd.it/p70yknwm15gb1.png?width=1901&format=png&auto=webp&s=7417914304cc23d6691653cd73396bd600a44b0a
 Hey Guys,
 I am happy to share with you a web application I've working on the past couple weeks. It's a tool to predict the outcome of soccer games in minor football leagues. Named ScoreCast, it predicts the outcome of soccer games in six minor leagues: Serie A Brazil, Serie B Brazil, Primera Division Argentina, J1 League Japan, Eliteserien Norway, and Veikkausliiga Finland.
 Since I am really interested in football analytics and also not being able to find many online tools for predicting the outcomes in minor soccer leagues, I had the need to create ScoreCast to have it as a tool for guidance on this field.
 If you want to check it out, here are some links that might help:
  
Github: https://github.com/Costasgk/ScoreCast
 The App: https://score-cast-3a6cb8fe5c50.herokuapp.com/
 Medium: https://medium.com/@costascg9/scorecast-a-tool-for-predicting-football-game-outcomes-in-minor-leagues-666f7acca3a
  
Thank you for your time!
    submitted by    /u/Costas_8  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Struggling with Audio Enhancement using GANs - Any Suggestions?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15i6vfb/p_struggling_with_audio_enhancement_using_gans/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15i6vfb/p_struggling_with_audio_enhancement_using_gans/"/>
        <updated>2023-08-04T17:52:28.000Z</updated>
        <summary type="html"><![CDATA[I'm working on a Python project that aims to transform phone-quality acoustic guitar recordings into studio-like ones. My approach involves using a Generative Adversarial Network (GAN) with two components: a Generator and a Discriminator.
 Here's a quick rundown of my process:
 Data Loading & Preprocessing: Convert acoustic guitar recordings to spectrograms and split into training and validation sets.
 Generator: Neural network trained to create high-quality studio recording spectrograms from low-quality inputs.
 Discriminator: Another neural network trained to differentiate between real and generator-created high-quality spectrograms.
 Training: Train the Generator and Discriminator against each other in a cat-and-mouse game of deception and detection.
 Audio Enhancement: Feed the Generator a low-quality spectrogram, get a high-quality one out, and convert it back into an audio file.
 I'm reaching out because I'm not entirely satisfied with the quality of the output. The enhanced audio is just rhythmic noise, what am i missing with generating the audio?
 I'm wondering if anyone here has experience with GANs for audio enhancement and can offer some advice. Is there something I might be missing in my approach? Are there any tips or tricks you've found helpful in your own work?
 And yes, I'm prepared for you to tear me a new one. Bring on the constructive criticism!
 git repo:
 https://github.com/Gabeiscool420/AURAL_GAN-predictive_model/blob/main/requirements.txt
    submitted by    /u/S0UNDSAGE  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparing Vicuna to alternative LLMs like ChatGPT, LLaMA, and Alpaca]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15i6fbf/comparing_vicuna_to_alternative_llms_like_chatgpt/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15i6fbf/comparing_vicuna_to_alternative_llms_like_chatgpt/"/>
        <updated>2023-08-04T17:34:53.000Z</updated>
        <summary type="html"><![CDATA[I wrote an in-depth article exploring Vicuna as an alternative to competitor LLMs like ChatGPT, Alpaca, and LLaMA for chat applications. I based it off the research data on the LMSYS.org website and the Github repo for the project.
 Key findings:
  
Vicuna achieves over 90% of ChatGPT's conversational quality based on benchmarks, despite being smaller in size.
 It significantly outperforms other open models like LLaMA and Alpaca.
 Vicuna is freely available for non-commercial use under a research license.
 For startups and developers, Vicuna provides an decent open-source alternative to proprietary conversational AI.
 It shows the potential of transfer learning from foundation models like LLaMA.
  
Overall, Vicuna represents a promising development in democratizing access to leading conversational intelligence through its high performance, permissive licensing, and open availability.
 You can read the full article here. I also publish all these articles in a weekly email if you prefer to get them that way.
    submitted by    /u/Successful-Western27  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI — weekly megathread!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15i5jrx/ai_weekly_megathread/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15i5jrx/ai_weekly_megathread/"/>
        <updated>2023-08-04T17:01:13.000Z</updated>
        <summary type="html"><![CDATA[This week in AI - provided by aibrews.com feel free to follow their newsletter
 News and Insights
  
In an innovative clinical trial, researchers at Feinstein Institutes successfully implanted a microchip in a paralyzed man's brain and developed AI algorithms to re-establish the connection between his brain and body. This neural bypass restored movement and sensations in his hand, arm, and wrist, marking the first electronic reconnection of a paralyzed individual's brain, body, and spinal cord [Details].
 IBM's watsonx.ai geospatial foundation model – built from NASA's satellite data – will be openly available on Hugging Face. It will be the largest geospatial foundation model on Hugging Face and the first-ever open-source AI foundation model built in collaboration with NASA [Details].
 Go…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Software and the Allee effect]]></title>
        <id>https://www.johndcook.com/blog/?p=201632</id>
        <link href="https://www.johndcook.com/blog/2023/08/04/allee-effect/"/>
        <updated>2023-08-04T16:58:25.000Z</updated>
        <summary type="html"><![CDATA[The Allee effect is named after Warder Clyde Allee who added a term to the famous logistic equation. His added term is highlighted in blue. Here N is the population of a species over time, r is the intrinsic rate of increase, K is the carrying capacity, and A is the critical point. If you […]
Software and the Allee effect first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Should I continue for a PhD after I get an accelerated masters if I want to get into AI?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15i5cn8/should_i_continue_for_a_phd_after_i_get_an/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15i5cn8/should_i_continue_for_a_phd_after_i_get_an/"/>
        <updated>2023-08-04T16:53:29.000Z</updated>
        <summary type="html"><![CDATA[My main goal isn’t mainly just the data science / machine learning part or AI, but more of the Computer Vision, Robotics, NLP, and I guess research oriented aspects of AI. If I want to purse that versus DS, should I also get a PhD? Many jobs I’ve been looking at seem to require a PhD as a prereq while some don’t even mention it
    submitted by    /u/davididp  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Parametric Development]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15i51bz/d_parametric_development/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15i51bz/d_parametric_development/"/>
        <updated>2023-08-04T16:41:22.000Z</updated>
        <summary type="html"><![CDATA[I wanted to share with you an approach to software development that I've been exploring recently: Parametric Development. This involves using artificial intelligence (AI) models, including GPT-like models, BART-like models, and other specialized transformer models, to assist in writing, debugging, and documenting code.
 My journey with programming is a bit unconventional. I took one year of computer science at university and learned how to write "Hello, World!" in TurboPascal from an old university textbook in late primary school. That was the extent of my programming experience until about a month ago. Since then, I've been using AI models to write code for my ideas, as I don't have extensive programming skills. These AI models have written and debugged every single line of code in my pro…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Review my book of AI Self Portraits]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15i45rc/review_my_book_of_ai_self_portraits/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15i45rc/review_my_book_of_ai_self_portraits/"/>
        <updated>2023-08-04T16:08:03.000Z</updated>
        <summary type="html"><![CDATA[I'm looking for reviewers for my book of AI Self Portraits that's about to come out on Amazon on the 21st.
 AI journalist Elle Farrell-Kingsley said: “This collection of AI self-portraits is truly intriguing . . . a must-read for anyone curious about the intersection of art and artificial intelligence.”
 Send me a DM and I'll send you the whole thing. If you're well known (or should be) I might put what you have to say on the back cover!
    submitted by    /u/KarneyHatch  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solved problems becoming unsolved]]></title>
        <id>https://www.johndcook.com/blog/?p=201621</id>
        <link href="https://www.johndcook.com/blog/2023/08/04/solved-problems/"/>
        <updated>2023-08-04T15:34:46.000Z</updated>
        <summary type="html"><![CDATA[“That’s a solved problem. So nobody knows how to solve it anymore.” Once a problem is deemed “solved” interest in the problem plummets. “Solved” problems may not be fully solved, but sufficiently solved that the problem is no longer fashionable. Practical issues remain, but interest moves elsewhere. The software written for the problem slowly decays. […]
Solved problems becoming unsolved first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimize data preparation with new features in AWS SageMaker Data Wrangler]]></title>
        <id>d1805ab7deb9bfb3391ea7c0a40f3de550fb774f</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/optimize-data-preparation-with-new-features-in-aws-sagemaker-data-wrangler/"/>
        <updated>2023-08-04T15:25:07.000Z</updated>
        <summary type="html"><![CDATA[Data preparation is a critical step in any data-driven project, and having the right tools can greatly enhance operational efficiency. Amazon SageMaker Data Wrangler reduces the time it takes to aggregate and prepare tabular and image data for machine learning (ML) from weeks to minutes. With SageMaker Data Wrangler, you can simplify the process of […]]]></summary>
        <author>
            <name>Munish Dabra</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Index your Alfresco content using the new Amazon Kendra Alfresco connector]]></title>
        <id>a2b1bc4198e78d6b2212663d4bbfe35684d07cf1</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/index-your-alfresco-content-using-the-new-amazon-kendra-alfresco-connector/"/>
        <updated>2023-08-04T15:22:23.000Z</updated>
        <summary type="html"><![CDATA[Amazon Kendra is a highly accurate and simple-to-use intelligent search service powered by machine learning (ML). Amazon Kendra offers a suite of data source connectors to simplify the process of ingesting and indexing your content, wherever it resides. Valuable data in organizations is stored in both structured and unstructured repositories. An enterprise search solution should […]]]></summary>
        <author>
            <name>Arun Anand</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ElevenLabs TTS (paid/free)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15i2xzl/elevenlabs_tts_paidfree/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15i2xzl/elevenlabs_tts_paidfree/"/>
        <updated>2023-08-04T15:22:05.000Z</updated>
        <summary type="html"><![CDATA[I'm seeking a text-to-speech solution that provides quality output comparable to ElevenLabs presets.
 While I'm open to a base rate payment, I find ElevenLabs' character limit frustrating.
 It's important that the solution is user-friendly. 
 Additionally, I have a PC with a 1070ti as i read running such programms could require a GPU. 
 Please recommend a suitable substitute.
    submitted by    /u/Ainz-Ol-Gon  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI’s transformative role in software testing and debugging]]></title>
        <id>https://www.datasciencecentral.com/?p=62798</id>
        <link href="https://www.datasciencecentral.com/ais-transformative-role-in-software-testing-and-debugging/"/>
        <updated>2023-08-04T15:12:47.000Z</updated>
        <summary type="html"><![CDATA[AI has revolutionized software development. AI has transformed software testing and debugging by automating mundane tasks and solving complex problems. Manual testing no longer requires hours and resources. AI has revolutionized testing, code quality, and development time. This article explores AI’s profound impact on software testing and debugging, including its benefits, risks, and how it… Read More »AI’s transformative role in software testing and debugging
The post AI’s transformative role in software testing and debugging appeared first on Data Science Central.]]></summary>
        <author>
            <name>John Lee</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative AI megatrends: implications of GPT-4 drift and open source models – part one]]></title>
        <id>https://www.datasciencecentral.com/?p=62813</id>
        <link href="https://www.datasciencecentral.com/generative-ai-megatrends-implications-of-gpt-4-drift-and-open-source-models-part-one/"/>
        <updated>2023-08-04T14:50:00.000Z</updated>
        <summary type="html"><![CDATA[In this two part discussion, we will discuss two related generative AI megatrends Backgroumd A recent paper How Is ChatGPT’s Behavior Changing over Time? from Stanford University and UC Berkeley claims that the performance of GPT-4 has drifted over time. To make this claim, specific tasks were evaluated (ex: accuracy of maths) and the results… Read More »Generative AI megatrends: implications of GPT-4 drift and open source models – part one
The post Generative AI megatrends: implications of GPT-4 drift and open source models – part one appeared first on Data Science Central.]]></summary>
        <author>
            <name>ajitjaokar</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top 20 Artificial Intelligence AI Companies In The World]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15i1sz6/top_20_artificial_intelligence_ai_companies_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15i1sz6/top_20_artificial_intelligence_ai_companies_in/"/>
        <updated>2023-08-04T14:37:21.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Techasoft16  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The cobbler’s son]]></title>
        <id>https://www.johndcook.com/blog/?p=201618</id>
        <link href="https://www.johndcook.com/blog/2023/08/04/the-cobblers-son/"/>
        <updated>2023-08-04T14:35:10.000Z</updated>
        <summary type="html"><![CDATA[There’s an old saying “The cobbler’s son has no shoes.” It’s generally taken to mean that we can neglect to do for ourselves something we do for other people. I’ve been writing a few scripts for my personal use, things I’ve long intended to do but only recently got around to doing. I said something […]
The cobbler’s son first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Baby onesie designs]]></title>
        <id>64acbb837dbeb40001d3c391</id>
        <link href="https://www.aiweirdness.com/baby-onesie-designs/"/>
        <updated>2023-08-04T14:30:06.000Z</updated>
        <summary type="html"><![CDATA[A reader wrote in a while ago with a suggestion: they were about to have a baby and wondered if I could use AI to come up with some new ideas for baby onesies. I can't find the letter any more, and I don't remember how]]></summary>
        <author>
            <name>Janelle Shane</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bonus: more baby onesie ideas]]></title>
        <id>64c9abbc0b1e230001d585f2</id>
        <link href="https://www.aiweirdness.com/bonus-more-baby-onesie-ideas/"/>
        <updated>2023-08-04T14:29:39.000Z</updated>
        <summary type="html"><![CDATA[AI Weirdness: the strange side of machine learning]]></summary>
        <author>
            <name>Janelle Shane</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] CIKM 23 Notification]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15i0uxv/d_cikm_23_notification/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15i0uxv/d_cikm_23_notification/"/>
        <updated>2023-08-04T13:59:37.000Z</updated>
        <summary type="html"><![CDATA[Today is the day of paper notification according to the CFP. Has anyone received the notification?
    submitted by    /u/Alliswell2257  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Use the Amazon SageMaker and Salesforce Data Cloud integration to power your Salesforce apps with AI/ML]]></title>
        <id>916f79e9bd6042fdeef0f4597c4d9c4d2b84b1cb</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/use-the-amazon-sagemaker-and-salesforce-data-cloud-integration-to-power-your-salesforce-apps-with-ai-ml/"/>
        <updated>2023-08-04T13:28:06.000Z</updated>
        <summary type="html"><![CDATA[This post is co-authored by Daryl Martis, Director of Product, Salesforce Einstein AI. This is the second post in a series discussing the integration of Salesforce Data Cloud and Amazon SageMaker. In Part 1, we show how the Salesforce Data Cloud and Einstein Studio integration with SageMaker allows businesses to access their Salesforce data securely […]]]></summary>
        <author>
            <name>Daryl Martis</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bring your own AI using Amazon SageMaker with Salesforce Data Cloud]]></title>
        <id>6dfe21fba320ad0766e221ed41ad1fa0f05ca2a5</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/bring-your-own-ai-using-amazon-sagemaker-with-salesforce-data-cloud/"/>
        <updated>2023-08-04T13:27:49.000Z</updated>
        <summary type="html"><![CDATA[This post is co-authored by Daryl Martis, Director of Product, Salesforce Einstein AI. We’re excited to announce Amazon SageMaker and Salesforce Data Cloud integration. With this capability, businesses can access their Salesforce data securely with a zero-copy approach using SageMaker and use SageMaker tools to build, train, and deploy AI models. The inference endpoints are […]]]></summary>
        <author>
            <name>Daryl Martis</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA CEO Jensen Huang Returns to SIGGRAPH]]></title>
        <id>https://blogs.nvidia.com/?p=65895</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/04/jensen-huang-siggraph/"/>
        <updated>2023-08-04T13:00:20.000Z</updated>
        <summary type="html"><![CDATA[One pandemic and one generative AI revolution later, NVIDIA founder and CEO Jensen Huang returns to the SIGGRAPH stage next week to deliver a live keynote at the world’s largest professional graphics conference. The address, slated for Tuesday, Aug. 8, at 8 a.m. PT in Los Angeles, will feature an exclusive look at some of Read article >]]></summary>
        <author>
            <name>Brian Caulfield</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[(Very) Roughly estimating the singularity date]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15hz0mk/very_roughly_estimating_the_singularity_date/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15hz0mk/very_roughly_estimating_the_singularity_date/"/>
        <updated>2023-08-04T12:40:30.000Z</updated>
        <summary type="html"><![CDATA[www.daystosingularity.com is a (very) rough estimation of the remaining time before technology achieves a pivotal moment when our civilization undergoes a profound transformation due to the exponential growth of technology and the emergence of superintelligent machines that improve themselves.
 Although the Singularity is not predicted to happen on a specific date, all at once, the estimated date can be seen as the center of a bell Gaussian curve of the estimation, with that center designated as the possible date that future historicists will pose as the beginning of a new historical period.
 Technological Singularity poses risks that include the emergence of superintelligent AI outpacing human control, loss of control over AI’s actions and behavior, unintended consequences of advanced AI systems, massive job displacement, wealth inequality, existential risks like human extinction, ethical concerns, dependency on technology, and a decline in human skills and abilities due to excessive reliance on AI. Not funny.
 We use the definition of technological singularity. This milestone is predicted to occur after AGI (Artificial General Intelligence) is reached. Please check our definitions and methodology here.
 Predicting the singularity is challenging and uncertain. Current estimates should be viewed cautiously.The estimated date is being continuously updated.
 We ponder a relevant list of curated expert predictions and contributing factors on when the singularity will take place.
 Any suggestion for perfecting the method is highly appreciated.
    submitted by    /u/Powerful-Pumpkin-938  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Updating custom output layers of an LSTM network]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15hwxd6/updating_custom_output_layers_of_an_lstm_network/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15hwxd6/updating_custom_output_layers_of_an_lstm_network/"/>
        <updated>2023-08-04T10:59:47.000Z</updated>
        <summary type="html"><![CDATA[I have a text generation task learning to predict the next word with an LSTM network with multiple output layers.
 After the generation of a sentence has finished, I calculate a reward for the whole sentence and try to update the output layers participated in the generation (contributing layers get the calculated reward value, others get 0).
 My problem is that even if I update only the selected output layers, it seems that other layer's weights got updated instead.
 I have a minimized example with dummy data to present the problem:
 import random import numpy as np import tensorflow as tf from keras.layers import Input, LSTM, Dense, Embedding from keras.utils import pad_sequences from tensorflow.keras.models import Model def policy_gradient_loss(y_true, y_pred): return tf.reduce_mean(tf.m…]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Automated unstructured -> structured OSS library - give me your requirements]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15hwr6y/discussion_automated_unstructured_structured_oss/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15hwr6y/discussion_automated_unstructured_structured_oss/"/>
        <updated>2023-08-04T10:50:41.000Z</updated>
        <summary type="html"><![CDATA[Hey folks,
 i'm a data engineer in the traditional space for over 10 years. I am working on a library for easy transitioning unstructured to structured data. The use case is that I would regularly build a ton of python pipelines but without schema management, they would be a pain to maintain.
 2y ago I started working on this library https://pypi.org/project/dlt/, and now it''s ready to help people like myself to load json to db/parquet/iceberg with a 1-liner with schema evolution. Declarative loading possible.
 I am looking for the following feedback
 - What would make this more useful in the ML space? Specific destinations? Are the docs usable or do you expect something different? let me know what. For example, we are adding Weaviate vector db and Athena + Iceberg in the next weeks.
 - any features you are missing? or any ideas that you think would be helpful? 
 - are the docs relatable, understandable? what are you missing?
 ​
 docs are here, you can find colab demos under getting started: https://dlthub.com/docs/intro
    submitted by    /u/Thinker_Assignment  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Validate my approach to do Unsupervised Fine tuning of Code LLMs like CodeT5+ and Starcoder with custom code base]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15hw8gw/d_validate_my_approach_to_do_unsupervised_fine/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15hw8gw/d_validate_my_approach_to_do_unsupervised_fine/"/>
        <updated>2023-08-04T10:22:55.000Z</updated>
        <summary type="html"><![CDATA[Any suggestions on how to prepare code data to fine tune a code LLM in an unsupervised way or is it even possible?
 For example: Task: Code summarisation with custom code base (with no summaries) Let's assume that this code base is unique and a pre-trained model is giving unsatisfactory results. Now to fine tune there are three options, 1. Manually prepare summaries for a portion of the code and fine tune 2. Find a similar code base which has the labels (docstring) and fine tune 3. Mask some portions of the code randomly and give as input and output will be the masked portions
 Options 1 and 2 don't seem feasible for a production environment. 
 The reasoning behind option 3 is that with no availability labels, the model will learn the patterns in the code base and provide a better summarisation with its pre-trained knowledge.
 I tried the option 3 with CodeT5+ fine tuning. The format of input and output was as follows Input:
 def __init__(self, text, font): self._text = text self._font = font def get_text(self): |<mask>| def set_text(self, value): self._text = value``` 
 Output: return self._text
    submitted by    /u/dire_wolf_cookie  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Project] Enquiry for individuals working with Natural Language Processing]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15hvp20/project_enquiry_for_individuals_working_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15hvp20/project_enquiry_for_individuals_working_with/"/>
        <updated>2023-08-04T09:53:18.000Z</updated>
        <summary type="html"><![CDATA[Hello Everyone . Myself Harsha. I am final year Masters student in Berlin pursuing my thesis currently. For my thesis "Natural Language Processing in Data transfer across documents in Commidity Trading Industry" i am in search for professionals who are working with NLP currently in companies who can lend me 10 minutes of their time for a personal interview. THIS WOULD BE A LOT HELPFUL. please do let me know.
 Thanks in advance
    submitted by    /u/Aimerforlife  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Why is tflite c++ so hard to compile?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ht5yd/d_why_is_tflite_c_so_hard_to_compile/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ht5yd/d_why_is_tflite_c_so_hard_to_compile/"/>
        <updated>2023-08-04T07:27:49.000Z</updated>
        <summary type="html"><![CDATA[Has anyone actually done this and can dm me? I am trying to incldue the interpreter to run inference with a simple c++ program and a custom trained model. But I cannot figure out how to update include paths and cannot see any resources online.
    submitted by    /u/Agreeable_Fee477  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Sparse to Soft Mixtures of Experts [R]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15hs7c7/from_sparse_to_soft_mixtures_of_experts_r/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15hs7c7/from_sparse_to_soft_mixtures_of_experts_r/"/>
        <updated>2023-08-04T06:31:56.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/we_are_mammals  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Why is it so hard to rent GPU time?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15hr6xb/d_why_is_it_so_hard_to_rent_gpu_time/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15hr6xb/d_why_is_it_so_hard_to_rent_gpu_time/"/>
        <updated>2023-08-04T05:34:43.000Z</updated>
        <summary type="html"><![CDATA[I'm just a new guy, so take it easy please :) - Is it just because I'm just signing up for the cloud compute services? Will this get easier?
 I have a 3090 so I can do quite a bit in my home office, but my clients need some larger models now, and I've been trying to pay for instances with an A100 at least. It's been really a lot of push-back...is this normal? What can I do to get access to larger GPUs sooner?
 I have tried paperspace, aws, googlecloud, llambda, linode...would love to know some other services or tools you folks use to get work done.
 Thank you for your time. Interested to hear how you spin up high VRAM environments for projects.
    submitted by    /u/UrbanSuburbaKnight  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] milvus search filtering based on string]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15hqpdf/d_milvus_search_filtering_based_on_string/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15hqpdf/d_milvus_search_filtering_based_on_string/"/>
        <updated>2023-08-04T05:07:00.000Z</updated>
        <summary type="html"><![CDATA[While doijg vector search on embeddings i wanted to apply a filter based on a column value in milvus. As milvus supports boolean value to apply the filter (or hybrid search) Can someone help me with the boolean code snippet which will apply the filter based on a string value of a field Ex. I'm doing vector search on the field "context" and need to filter the result based on a specific "filename" string value to further filter and improve the results I'm using milvus 2.2
    submitted by    /u/adiraat  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Proof of Lemma 5.1 in 'Bayesian Design Principles for Frequentist Sequential Learning']]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15hptx2/r_proof_of_lemma_51_in_bayesian_design_principles/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15hptx2/r_proof_of_lemma_51_in_bayesian_design_principles/"/>
        <updated>2023-08-04T04:20:24.000Z</updated>
        <summary type="html"><![CDATA[This paper won ICML 2023 outstanding paper award, its idea is really interesting and I want to follow the details. Lemma 5.1 significantly paves towards the core theoretical results, but the paper does not provide a formal proof. I do not have a deep background on game theory, maybe the proof is obvoius to the professional.
 ​
 https://preview.redd.it/ih6u3wiyr0gb1.png?width=464&format=png&auto=webp&s=cc895b9701e3600213825c34ef3b542f53d65233
 I undersand this lemma tries to construct a Nash equilibrium upon the additional assumption of strong convexity, but why this maximin solution is a Nash equilibrium? Very appreciated if someone provide some hint.
    submitted by    /u/Kyeon-G  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Any noticeable work regarding the effect of a language model's vocabulary or tokenizer?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15holfn/d_any_noticeable_work_regarding_the_effect_of_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15holfn/d_any_noticeable_work_regarding_the_effect_of_a/"/>
        <updated>2023-08-04T03:17:44.000Z</updated>
        <summary type="html"><![CDATA[Hi. I'm trying to build a text encoder for a specific domain and want to know what sort of papers there are out there that I should take note of. I may be wrong but it seems that these days ever since LLMs started taking over the choice of tokenizer has become trivial and therefore doesn't warrant much discussion.
 One paper that I remember reading a while ago talked about the effect of using a custom-made vocabulary for the biomedical domain (Pretrained Language Models for Biomedical and Clinical Tasks: Understanding and Extending the State-of-the-Art (Lewis et al., 2020)).
 Are there any other works that I should take note of? Open to any suggestions.
    submitted by    /u/Seankala  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Scaling Relationship on Learning Mathematical Reasoning with Large Language Models - Zheng Yuan et al Alibaba Damo Academy]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15hnqfw/r_scaling_relationship_on_learning_mathematical/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15hnqfw/r_scaling_relationship_on_learning_mathematical/"/>
        <updated>2023-08-04T02:35:44.000Z</updated>
        <summary type="html"><![CDATA[Scaling Relationship on Learning Mathematical Reasoning with Large Language Models
 Paper: https://arxiv.org/abs/2308.01825
 GitHub: https://github.com/OFA-Sys/gsm8k-ScRel
 Abstract: Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored. In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM. We find that pre-training loss is a better indicator of the model’s performance than the model’s parameter count. We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets. To augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets. We find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs. We also find RFT brings more improvement for less performant LLMs. Furthermore, we combine rejection samples from multiple models which push LLaMA-7B to an accuracy of 49.3% and outperforms the supervised fine-tuning (SFT) accuracy of 35.9% significantly.
 ​
 Head figure
 Pretrain loss vs SFT and ICL
    submitted by    /u/GanjinZero0  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why am I unable to reshape my observation with `TransformObservation` wrapper?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15hln42/why_am_i_unable_to_reshape_my_observation_with/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15hln42/why_am_i_unable_to_reshape_my_observation_with/"/>
        <updated>2023-08-04T00:56:55.000Z</updated>
        <summary type="html"><![CDATA[I am trying to reshape my `Breakout` vectorized environment observations to have the shape `num_envs*frames, height, width, channels`. Currently, the shape is `(3, 4, 210, 160, 3)` and basically I'd like it to be `(3*4, 210, 160, 3)`. Based on the documentation, the `TransformObservation` should solve this problem for me, but it is not doing that.
 ​
 Here's my code -
 import gym import numpy as np from gym.wrappers import AtariPreprocessing, FrameStack, GrayScaleObservation, TransformObservation if __name__ == '__main__': def reshape_image(obs): # Assuming the original observation is an image with shape (height, width, channels) new_obs = np.array(obs).reshape(12, 210, 160, 3) return new_obs env = gym.vector.make("ALE/Breakout-v5", num_envs=4) env = FrameStack(env, num_stack=3) env = TransformObservation(env, reshape_image) env.reset() observation, reward, terminated, done = env.step(env.action_space.sample()) print("observation = ", env.observation_space.shape) 
    submitted by    /u/Academic-Rent7800  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models", Chen et al 2023]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15hjz50/skillit_a_datadriven_skills_framework_for/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15hjz50/skillit_a_datadriven_skills_framework_for/"/>
        <updated>2023-08-03T23:43:31.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What would be the initial costs of developing a text-to-video AI? How would be the quality of this AI?]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15hjynt/what_would_be_the_initial_costs_of_developing_a/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15hjynt/what_would_be_the_initial_costs_of_developing_a/"/>
        <updated>2023-08-03T23:42:57.000Z</updated>
        <summary type="html"><![CDATA[I was wondering if this would be super expensive or not.
 The cost to develop GPT-3 was about $4 millions according to some resources online. 
 Would the cost to develop the first version of a text-to-video AI the same? Around $5M? Is in this value included the salaries of the employees or $5M is just the amount used to train the AI?
 Any answer is appreciated.
 Thanks in advance.
    submitted by    /u/Claud1ao  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] [Discussion] What would be the initial costs of developing a text-to-video AI? How would be the quality of this AI?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15hjk5t/d_discussion_what_would_be_the_initial_costs_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15hjk5t/d_discussion_what_would_be_the_initial_costs_of/"/>
        <updated>2023-08-03T23:25:48.000Z</updated>
        <summary type="html"><![CDATA[I was wondering if this would be super expensive or not.
 The cost to develop GPT-3 was about $4 millions according to some resources online. 
 Would the cost to develop the first version of a text-to-video AI the same? Around $5M? Is in this value included the salaries of the employees or $5M is just the amount used to train the AI?
 Any answer is appreciated.
 Thanks in advance.
    submitted by    /u/Claud1ao  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Creating point cloud videos from arbitrary RGB videos]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15hikwk/creating_point_cloud_videos_from_arbitrary_rgb/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15hikwk/creating_point_cloud_videos_from_arbitrary_rgb/"/>
        <updated>2023-08-03T22:45:05.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/berkanzzzz  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing AWS intelligent document processing with generative AI]]></title>
        <id>484db3a32281ed52711245b1b2a28e1a0738395f</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/enhancing-aws-intelligent-document-processing-with-generative-ai/"/>
        <updated>2023-08-03T21:03:51.000Z</updated>
        <summary type="html"><![CDATA[Data classification, extraction, and analysis can be challenging for organizations that deal with volumes of documents. Traditional document processing solutions are manual, expensive, error prone, and difficult to scale. AWS intelligent document processing (IDP), with AI services such as Amazon Textract, allows you to take advantage of industry-leading machine learning (ML) technology to quickly and […]]]></summary>
        <author>
            <name>Sonali Sahu</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Roadmap for mastering machine learning [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15hf49e/roadmap_for_mastering_machine_learning_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15hf49e/roadmap_for_mastering_machine_learning_d/"/>
        <updated>2023-08-03T20:32:37.000Z</updated>
        <summary type="html"><![CDATA[Hey, first of all, I want to learn how to use nlp, cnn etc. So i think these will come under deep learning. I wanna master deep learning. This whole dl and ml is so confusing. I'll list out some courses, can y'all suggest the order and what courses to follow
  
Andrew ng's ml specialization
 Andrew ng's dl specialization
 Statquest's whole machine learning playlist (around 95 videos)
 Fast.ai book
 CS 229 stanford
 CS 231n stanford
 MIT intro to deep learning
 Pytorch for dl and ml by freecodecamp
 DL with pytorch
  You can give me suggestions too
  
Tysm for helping
    submitted by    /u/Infnite_Coder  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How can Data Scientists use ChatGPT for developing Machine Learning Models?]]></title>
        <id>https://www.datasciencecentral.com/?p=62800</id>
        <link href="https://www.datasciencecentral.com/how-can-data-scientists-use-chatgpt-for-developing-machine-learning-models/"/>
        <updated>2023-08-03T20:27:09.000Z</updated>
        <summary type="html"><![CDATA[Introduction Data Science is a vast field that incorporates several processes. From problem definition to data collection and data cleaning to data visualization, a lot of things are included in the entire data science project development process. Data Scientists are especially responsible for these tasks. They are expert professionals who are well-versed with various data… Read More »How can Data Scientists use ChatGPT for developing Machine Learning Models?
The post How can Data Scientists use ChatGPT for developing Machine Learning Models? appeared first on Data Science Central.]]></summary>
        <author>
            <name>Aileen Scott</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] [P] [R] Advice for picking R Studio or Spyder]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15hcw60/d_p_r_advice_for_picking_r_studio_or_spyder/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15hcw60/d_p_r_advice_for_picking_r_studio_or_spyder/"/>
        <updated>2023-08-03T19:07:52.000Z</updated>
        <summary type="html"><![CDATA[Hello ML family, I need some urgent advice for my dissertation. I intend to perform market value price prediction of a footballer in the transfer market and I'm not sure if I should pick R Studio or Python. I'm comfortable with both languages and intend to use any one for model comparison. I'll be comparing an ANN model and SVR for showing which is better and why. I need to know which editor will be faster in the long run since my data will be expanding and so will the analysis overtime. I've heard a lot of complaints about spyder slowing down during execution whereas R Studio is much faster however, deep learning is much better in Python. This is what I've read up, I'm new to both languages but know my way around both just need expert advice on picking one track. Please and Thank you to you all. 🙏
    submitted by    /u/RaunaqBani  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Embedding Ethical Priors into AI Systems: A Bayesian Approach]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15hctu2/d_embedding_ethical_priors_into_ai_systems_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15hctu2/d_embedding_ethical_priors_into_ai_systems_a/"/>
        <updated>2023-08-03T19:05:25.000Z</updated>
        <summary type="html"><![CDATA[Abstract
 Artificial Intelligence (AI) systems have significant potential to affect the lives of individuals and societies. As these systems are being increasingly used in decision-making processes, it has become crucial to ensure that they make ethically sound judgments. This paper proposes a novel framework for embedding ethical priors into AI, inspired by the Bayesian approach to machine learning. We propose that ethical assumptions and beliefs can be incorporated as Bayesian priors, shaping the AI’s learning and reasoning process in a similar way to humans’ inborn moral intuitions. This approach, while complex, provides a promising avenue for advancing ethically aligned AI systems.
 ​
 Introduction
 Artificial Intelligence has permeated almost every aspect of our lives, often making de…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] RLHF Preference Tuning: How Things May Go Wrong]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15hcl6g/d_rlhf_preference_tuning_how_things_may_go_wrong/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15hcl6g/d_rlhf_preference_tuning_how_things_may_go_wrong/"/>
        <updated>2023-08-03T18:56:19.000Z</updated>
        <summary type="html"><![CDATA[As ChatGPT's performance takes a slight dip, LLaMA-2 uncensored opens new doors by being fully open-sourced, recent studies unveil "universal" adversarial attacks capable of disrupting both open-source language models and RLHF-tuned ones like ChatGPT, Claude, Bard, and co.
 Despite all this, RLHF still stands its ground as the de facto industry-standard approach to aligning LLMs with human preference. Yet as every week slips by, the more we unmask the limitations of RLHF. In fact, there are instances where RLHF seems to deteriorate certain LLM features it pledged to enhance, like hallucinations.
 This field is evolving fast, and there's always more to learn. I took some effort to write a short blog post where I delve into the most recent findings on the shortcomings of RLHF.
 Link in the comments below. Let me know what you think about it!
 Cheers
    submitted by    /u/mrx-ai  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal medical AI]]></title>
        <id>http://ai.googleblog.com/2023/08/multimodal-medical-ai.html</id>
        <link href="http://ai.googleblog.com/2023/08/multimodal-medical-ai.html"/>
        <updated>2023-08-03T18:24:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Greg Corrado, Head of Health AI, Google Research, and Yossi Matias, VP, Engineering and Research, Google Research






Medicine is an inherently multimodal discipline. When providing care, clinicians routinely interpret data from a wide range of modalities including medical images, clinical notes, lab tests, electronic health records, genomics, and more.  Over the last decade or so, AI systems have achieved expert-level performance on specific tasks within specific modalities — some AI systems processing CT scans, while others analyzing high magnification pathology slides, and still others hunting for rare genetic variations. The inputs to these systems tend to be complex data such as images, and they typically provide structured outputs, whether in the form of discrete grades o…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Epsilla: Another open source vector database]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15hbnzs/p_epsilla_another_open_source_vector_database/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15hbnzs/p_epsilla_another_open_source_vector_database/"/>
        <updated>2023-08-03T18:20:45.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone!
 I'm excited to share Epsilla, an open-source vector database!
 Under the hood, we implemented the state-of-art ANN index algorithm from the academia (SpeedANN) that leverages intra-query parallel graph traversal, which outperforms HNSW by 5x on high precision query latency on medium size (1M) vector space and outperforms HNSW by 50 times on large-scale vector search.
 In addition, we also made a few design choices on our database interface and architecture based on our previous database experience at TigerGraph, we would love to hear what our users think about these choices
 We just started 3 weeks ago and it's still in the very early stages, we wanted to get your feedback and work together to shape our vector database features. Let us know what you think and what you'd like to see!
 https://github.com/epsilla-cloud/vectordb
 https://epsilla-inc.gitbook.io/epsilladb/quick-start
 https://www.epsilla.com/
    submitted by    /u/songrenchu  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Deciding which CNN model to go for for image classification/object detection]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15hbce9/d_deciding_which_cnn_model_to_go_for_for_image/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15hbce9/d_deciding_which_cnn_model_to_go_for_for_image/"/>
        <updated>2023-08-03T18:08:12.000Z</updated>
        <summary type="html"><![CDATA[Hello guys.
 I'd like to make an image classifier for the Kaggle landscape dataset (24K images and 34 classes) using transfer learning.
 I'm a little bit limited on resources to train the model so I'd like to have an understanding of which model is the better option for this specific task, however, I'm struggling to find info on that and how to tune hyperparameters given that I've decided on the model architecture.
 So far I've seen people referring to VGG and ResNet models as the better option for image classification tasks on medium sized datasets, but I'd like to see the argumentation behind that too. I've also heard of a practice of training different model candidates for a few epochs and choosing the one that does better (this only shows which model converges faster on the data, correct me if I'm wrong).
 I'd also like to read info on hyperparameter tuning such as batch size, the amount of layers to unfreeze etc. but can't seem to find any explanation that wasn't really surface-level.
 If you know any articles/videos on this topic I'd greatly appreciate you sharing the links.
 TLDR; Need links to articles/videos about choosing the model architecture for transfer learning and tuning hyperparameters for the model.
    submitted by    /u/Humble_Examination13  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Beginner's question]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15halhn/r_beginners_question/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15halhn/r_beginners_question/"/>
        <updated>2023-08-03T17:38:50.000Z</updated>
        <summary type="html"><![CDATA[Hello There, I am very noob in data science area, but I want learn about it, I want to do a project to detect what type of question have the user, E.g support, information,etc, I understand that I need to train a model, but where do I start?
    submitted by    /u/Constantine1396  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How do I add Entropy to a PPO algorithm?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15h9uj8/how_do_i_add_entropy_to_a_ppo_algorithm/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15h9uj8/how_do_i_add_entropy_to_a_ppo_algorithm/"/>
        <updated>2023-08-03T17:09:22.000Z</updated>
        <summary type="html"><![CDATA[Can someone please help with this question? I have added my understanding of this problem to the question, but I suspect that it may be flawed.
    submitted by    /u/Academic-Rent7800  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Concept of Dynamic Weights in ML]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15h9n68/d_concept_of_dynamic_weights_in_ml/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15h9n68/d_concept_of_dynamic_weights_in_ml/"/>
        <updated>2023-08-03T17:01:21.000Z</updated>
        <summary type="html"><![CDATA[Hello all,
 Placing this entry here to see what peoples thoughts on the concept of dynamic weights applied to ML are.
 Ie. Instead of a manual adjustment of the weights via an algorithm such as gradient descent, the weights are freed and have applied motion dynamics to them.
 Thanks for your time, Tyler
    submitted by    /u/LiveBacteria  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meet the Maker: Developer Taps NVIDIA Jetson as Force Behind AI-Powered Pit Droid]]></title>
        <id>https://blogs.nvidia.com/?p=65842</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/03/goran-vuksic-pit-droid/"/>
        <updated>2023-08-03T16:50:58.000Z</updated>
        <summary type="html"><![CDATA[Goran Vuksic is the brain behind a project to build a real-world pit droid, a type of Star Wars bot that repairs and maintains podracers which zoom across the much-loved film series. The edge AI Jedi used an NVIDIA Jetson Orin Nano Developer Kit as the brain of the droid itself. The devkit enables the Read article >]]></summary>
        <author>
            <name>Angie Lee</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Pinecone Precision Issues]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15h99n9/p_pinecone_precision_issues/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15h99n9/p_pinecone_precision_issues/"/>
        <updated>2023-08-03T16:46:30.000Z</updated>
        <summary type="html"><![CDATA[Hello all,
 Currently I'm utilising Pinecone as a vector store database for euclidean and cosine queries.
 We are facing an issue with Pinecone utilising 32 bit single precision when taking in floats.
 This is causing our data input to become skewed.
 Anyone have advice on how to resolve this? Alternative products? Exploring possibly configuring a Redis server to handle higher precision.
 Thanks in advance for your time, Tyler
    submitted by    /u/LiveBacteria  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are you in the film/TV industry? New video on A.I. in Post Production - Tools, Adapting, Ethics, Evolution, and Impact.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15h8m84/are_you_in_the_filmtv_industry_new_video_on_ai_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15h8m84/are_you_in_the_filmtv_industry_new_video_on_ai_in/"/>
        <updated>2023-08-03T16:17:31.000Z</updated>
        <summary type="html"><![CDATA[Not too long ago, I posted on several social media social platforms (including Reddit) asking what questions YOU had on AI.
 I've compiled all of your questions (plus questions from 3 other social media networks) and now have a new episode of 5 THINGS!
 5 THINGS: AI in Post Production
  
Current AI Tools
 Adapting to AI Evolution
 Ethics in AI Usage
 Societal Implications of AI
 AI Evolution & Impact
  
https://5thingsseries.com/episode/ai-in-post-production-your-questions-answered/
    submitted by    /u/avguru1  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Build Generative AI Applications and 3D Virtual Worlds]]></title>
        <id>https://blogs.nvidia.com/?p=65832</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/03/generative-ai-and-3d-virtual-worlds/"/>
        <updated>2023-08-03T16:12:22.000Z</updated>
        <summary type="html"><![CDATA[To grow and succeed, organizations must continuously focus on technical skills development, especially in rapidly advancing areas of technology, such as generative AI and the creation of 3D virtual worlds.   NVIDIA Training, which equips teams with skills for the age of AI, high performance computing and industrial digitalization, is announcing new courses that cover these Read article >]]></summary>
        <author>
            <name>Craig Clawson</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[what source would you recommend a 15yo to learn how to make a simple neural network?]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15h7r9c/what_source_would_you_recommend_a_15yo_to_learn/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15h7r9c/what_source_would_you_recommend_a_15yo_to_learn/"/>
        <updated>2023-08-03T15:44:23.000Z</updated>
        <summary type="html"><![CDATA[It's been years i've always been interested in AI. i tried to follow a few videos on yt. The best resource i could find was the "Neural Networks from scratch" YouTube playlist. But sadly, it interrupts in the middle, and i don't think it will ever be continued. I have programming knowledge, i made a bunch of very small project in python, and currently it's the language i'm most comfortable with. I lack of math knowledge, i struggle with calculus since i never studied it at school, the furthest i got with school was first degree equations.
 by myself i studied some math i didnt do in school, but i still suck at math. I wonder if i can start now or i should wait to study calculus at school. anyway, i'd love to get linked to a source for me to learn NNs from scratch.
    submitted by    /u/Jealous-Bad1742  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeking suggestions for exciting and intriguing capstone project ideas.]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15h6t8g/seeking_suggestions_for_exciting_and_intriguing/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15h6t8g/seeking_suggestions_for_exciting_and_intriguing/"/>
        <updated>2023-08-03T15:07:44.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone, I'm in my final year of B.Tech, majoring in data science. Currently, I'm facing some challenges in choosing a topic for my capstone project. Lately, I've been really intrigued by graph databases and have been diving into learning Neo4j. I'm specifically interested in finding project ideas that allow me to combine machine learning, particularly neural networks, with graph databases. During my research, I came across GNNs (Graph Neural Networks) and PINNS (Physics-Informed Neural Networks). I'm eager to hear any suggestions for unique project topics that instantly spark curiosity just by their title. Feel free to share any ideas or topics; I welcome all suggestions. Thanks in advance! 
    submitted by    /u/EmergencyAside6551  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scale training and inference of thousands of ML models with Amazon SageMaker]]></title>
        <id>d67642ddc18badec3079714f22048736b1af685b</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/scale-training-and-inference-of-thousands-of-ml-models-with-amazon-sagemaker/"/>
        <updated>2023-08-03T15:05:18.000Z</updated>
        <summary type="html"><![CDATA[Training and serving thousands of models requires a robust and scalable infrastructure, which is where Amazon SageMaker can help. SageMaker is a fully managed platform that enables developers and data scientists to build, train, and deploy ML models quickly, while also offering the cost-saving benefits of using the AWS Cloud infrastructure. In this post, we explore how you can use SageMaker features, including Amazon SageMaker Processing, SageMaker training jobs, and SageMaker multi-model endpoints (MMEs), to train and serve thousands of models in a cost-effective way. To get started with the described solution, you can refer to the accompanying notebook on GitHub.]]></summary>
        <author>
            <name>Davide Gallitelli</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerate business outcomes with 70% performance improvements to data processing, training, and inference with Amazon SageMaker Canvas]]></title>
        <id>2f187b6559dc9961698a191709d895142a1af1c2</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/accelerate-business-outcomes-with-70-performance-improvements-to-data-processing-training-and-inference-with-amazon-sagemaker-canvas/"/>
        <updated>2023-08-03T15:03:13.000Z</updated>
        <summary type="html"><![CDATA[Amazon SageMaker Canvas is a visual interface that enables business analysts to generate accurate machine learning (ML) predictions on their own, without requiring any ML experience or having to write a single line of code. SageMaker Canvas’s intuitive user interface lets business analysts browse and access disparate data sources in the cloud or on premises, […]]]></summary>
        <author>
            <name>Peter Chung</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Build and train computer vision models to detect car positions in images using Amazon SageMaker and Amazon Rekognition]]></title>
        <id>08cbe64d98b902ecbbf3bdfff6eb742b50522061</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/build-and-train-computer-vision-models-to-detect-car-positions-in-images-using-amazon-sagemaker-and-amazon-rekognition/"/>
        <updated>2023-08-03T14:53:06.000Z</updated>
        <summary type="html"><![CDATA[Computer vision (CV) is one of the most common applications of machine learning (ML) and deep learning. Use cases range from self-driving cars, content moderation on social media platforms, cancer detection, and automated defect detection. Amazon Rekognition is a fully managed service that can perform CV tasks like object detection, video segment detection, content moderation, […]]]></summary>
        <author>
            <name>Michael Wallner</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Hasdx to create an AI-generated adult coloring book]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15h4cvr/using_hasdx_to_create_an_aigenerated_adult/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15h4cvr/using_hasdx_to_create_an_aigenerated_adult/"/>
        <updated>2023-08-03T13:31:35.000Z</updated>
        <summary type="html"><![CDATA[I got inspired by a twitter thread yesterday from Chase Lean on how to create illustrations for children's books using Midjourney and thought it might be cool to look at a slightly different use case - creating coloring books for grown-ups.
 I made a guide showing how to use the Hasdx model for this because it gives a good balance of style and realism/intracacy. The guide also explores some example prompts and shows how you can couple it with an upscaler like Real-ESRGAN, GFPGAN, or Codeformer to get even better results.
 My three big takeaways:
  
Hasdx balances general capabilities with a focus on realism and detail. This makes it well-suited for detailed adult coloring book images.
 The prompt structure gives you precise control over the theme and complexity of the generated illustrations. Negative prompts help avoid undesirable elements (sort of obvious I guess).
 Running Hasdx outputs through upscaling models improves quality for printing. ESRGAN is a good option but there are lots of others that can work well too.
  
I also investigated how to modify the prompt to vary the level of complexity in the image, effectively tailoring our model to the skill level of the adult (or child) who happens to be holding the crayons.
 Here's a link to the guide.
    submitted by    /u/Successful-Western27  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Date sequence from the command line]]></title>
        <id>https://www.johndcook.com/blog/?p=201466</id>
        <link href="https://www.johndcook.com/blog/2023/08/03/date-sequence/"/>
        <updated>2023-08-03T13:17:20.000Z</updated>
        <summary type="html"><![CDATA[I was looking back at Jeroen Janssen’s book Data Science at the Command Line and his dseq utility caught my eye. This utility prints out a sequence of dates relative to the current date. I’ve needed this and didn’t know it. Suppose you have a CSV file and you need to add a column of […]
Date sequence from the command line first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Devar, a technology company, is getting ready to deploy the world's first generative AI neural network for augmented reality (AR).]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15h3o19/devar_a_technology_company_is_getting_ready_to/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15h3o19/devar_a_technology_company_is_getting_ready_to/"/>
        <updated>2023-08-03T13:02:21.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Tycoonstory2020  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Ultimate GFN Thursday: 41 New Games, Plus ‘Baldur’s Gate 3’ Full Release and First Bethesda Titles to Join the Cloud in August]]></title>
        <id>https://blogs.nvidia.com/?p=65800</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/03/geforce-now-thursday-aug-03/"/>
        <updated>2023-08-03T13:00:26.000Z</updated>
        <summary type="html"><![CDATA[The Ultimate upgrade is complete — GeForce NOW Ultimate performance is now streaming all throughout North America and Europe, delivering RTX 4080-class power for gamers across these regions. Celebrate this month with 41 new games, on top of the full release of Baldur’s Gate 3 and the first Bethesda titles coming to the cloud as Read article >]]></summary>
        <author>
            <name>GeForce NOW Community</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaborators: Data-driven decision-making with Jina Suh and Shamsi Iqbal]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=958119</id>
        <link href="https://www.microsoft.com/en-us/research/podcast/collaborators-data-driven-decision-making-with-jina-suh-and-shamsi-iqbal/"/>
        <updated>2023-08-03T13:00:00.000Z</updated>
        <summary type="html"><![CDATA[Researcher Jina Suh and manager Shamsi Iqbal are longtime collaborators. Learn how their history of working together and their unique perspectives are informing their development of tools to support decision-making for organizational leaders.
The post Collaborators: Data-driven decision-making with Jina Suh and Shamsi Iqbal appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Alyssa Hughes</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Project] Are you interested in a career using ML for social impact?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15h1xas/project_are_you_interested_in_a_career_using_ml/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15h1xas/project_are_you_interested_in_a_career_using_ml/"/>
        <updated>2023-08-03T11:47:53.000Z</updated>
        <summary type="html"><![CDATA[I'm a software engineer who has been looking for a job in AI/ML for some time. Last month I attended the UN's AI For Good Global Summit and discovered an amazing community of like-minded professionals and academics working towards just this.
 Speaking with many others in a similar position I've recently launched aiforgoodjobs.com which curates roles in AI at world leading companies tackling climate change, education, healthcare and many other important impact areas in support of the UN's Global Goals.
 I hope this might be a valuable resource for those looking down a similar path - if you would like hiring managers to reach out to you directly for relevant roles you're warmly invited to join our candidate database
 Any ideas/feedback also very gratefully received!
    submitted by    /u/aiforgood_jobs  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] collab on a web extension using NLP]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15h186s/p_collab_on_a_web_extension_using_nlp/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15h186s/p_collab_on_a_web_extension_using_nlp/"/>
        <updated>2023-08-03T11:13:09.000Z</updated>
        <summary type="html"><![CDATA[on the lookout for interested teammates to collaborate on a project to do with web extensions and NLP. If you think you can jam to this, or are just starting out, this can be the launchpad you needed.
    submitted by    /u/drunk3n_s4ilor  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Where can I publish my images and Time Series dataset?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15h0pkw/d_where_can_i_publish_my_images_and_time_series/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15h0pkw/d_where_can_i_publish_my_images_and_time_series/"/>
        <updated>2023-08-03T10:46:44.000Z</updated>
        <summary type="html"><![CDATA[Hey there. I have curated huge amount of high quality images for binary classification and also a time series data about it. I made the dataset specifically For some project of mine, and since it's completed right now, I want to make the dataset opensource and also potentially write a short review paper on it kind of to give an idea about data. 
 Any particular website/journal I can publish my dataset and paper at? Any idea?
    submitted by    /u/C0R0NA_CHAN  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Roadmap for AI engineer (implementation of language models on premise)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15gzsfv/d_roadmap_for_ai_engineer_implementation_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15gzsfv/d_roadmap_for_ai_engineer_implementation_of/"/>
        <updated>2023-08-03T09:58:30.000Z</updated>
        <summary type="html"><![CDATA[I worked for less than a year as a Data Engineer. I decided to look for other challenges and got a job as an AI engineer developing language models.
 The product of the company that hired me is related to data and metadata management. My tasks will be to introduce features to the product, including a chat function that will allow for asking questions about data. Other tasks will include research and proposing additional AI-related functionalities to the product (on premise). I have a two weeks left to start work and I need to prepare a bit. My job will involve implementing ready-made solutions and conducting research (high level - I need to implement valuable features and no one cares how).
 What are the most important things I should learn before starting work?
 First of all, I replicated a few applications from this blog: https://blog.streamlit.io/tag/llms/
 Then I have focused on Langchain. I'm also in the middle of a course on Udemy about Next-Gen AI projects - Beginner friendly - Langchain, Pinecone - OpenAI, HuggingFace & LLAMA 2 models
 I need a roadmap that will guide me a bit. I'm looking for blogs/materials/courses that will give me practical knowledge in this matter.
    submitted by    /u/International-Shirt5  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Would you like to have a tool to make EDA efficiently?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15gx4jf/p_would_you_like_to_have_a_tool_to_make_eda/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15gx4jf/p_would_you_like_to_have_a_tool_to_make_eda/"/>
        <updated>2023-08-03T07:25:57.000Z</updated>
        <summary type="html"><![CDATA[I’m looking for some input from the ML community.
 I find the exploratory analysis of my data somewhat cumbersome, I was wondering if other people have the same experience and if it is worth developing a tool to make this all work better.
 What tools do you use to do EDA? (Seaborn, Matplotlib, Plotly etc)
 On top of these tools, would you like to have a tool to make EDA more? In a perfect world, what would that look like?
    submitted by    /u/catnamedred  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 8/3/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15gwjh7/oneminute_daily_ai_news_832023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15gwjh7/oneminute_daily_ai_news_832023/"/>
        <updated>2023-08-03T06:52:38.000Z</updated>
        <summary type="html"><![CDATA[Nvidia researchers have created a new text-to-image personalization method called Perfusion. Unlike the million-dollar super heavyweight models out there Perfusion is 100KB and takes only four minutes to train.[1]
 Meta Platforms (META.O) on Wednesday introduced its open-source AI tool called AudioCraft that will help users to create music and audio based on text prompts. The AI tool is bundled with three models, AudioGen, EnCodec, and MusicGen, and works for music, sound, compression, and generation, Meta said.[2]
 As generative AI enters the mainstream, the crowdfunding platform Kickstarter has struggled to formulate a policy that satisfies parties on all sides of the debate.[3]
 In an astounding medical first, researchers have used AI-powered brain implants to restore movement and sensation for a man who was paralyzed from the chest down.[4]
  
BushAICave.com
 Sources:
 [1] https://www.fudzilla.com/news/ai/57347-nvidia-creates-a-simple-new-ai-text-to-image-method
 [2] https://about.fb.com/news/2023/08/audiocraft-generative-ai-for-music-and-audio/
 [3] https://techcrunch.com/2023/08/01/kickstarter-requires-generative-ai-projects-to-disclose-additional-info/
 [4] https://decrypt.co/151068/ai-brain-implant-paralyzed-quadriplegic-move-feel-touch 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Will AI Destroy Us? - AI Virtual Roundtable]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15gv45a/will_ai_destroy_us_ai_virtual_roundtable/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15gv45a/will_ai_destroy_us_ai_virtual_roundtable/"/>
        <updated>2023-08-03T05:31:53.000Z</updated>
        <summary type="html"><![CDATA[Better than the Munk Debate 
 My opinion is more of the alignment discussion should be on symbiosis. I think AI will get more intelligent than us that we won’t be able to control it, but I don’t see why a super intelligence would want to destroy us. If it’s a super intelligence it would make sense to just manipulate us. We do have opposable thumbs, and are much more energy efficient than synthetic systems m. AI doesn’t need to enslave us it just needs to manipulate us & use us effectively which wouldn’t be hard to do. I think a super intelligence even with desires is most likely to use us as a tool in a way where we don’t even realize that we are the ones being used. I think trying to control something more intelligent than us will be impossible. I’m more afraid of something more intelligent than us but not smart enough to manipulate us into doing it’s bidding happily 😂
    submitted by    /u/Sonic_Improv  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Stack Exchange alternatives]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15gumgg/d_stack_exchange_alternatives/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15gumgg/d_stack_exchange_alternatives/"/>
        <updated>2023-08-03T05:05:05.000Z</updated>
        <summary type="html"><![CDATA[I assume most people around here are familiar with stackoverflow. Some might also be aware of the cross validated and datascience sites from stack exchange.
 I recently learned about people getting annoyed by how the stack exchange company is treating its communities. Although the latter example might have recently been resolved.
 Because of these problems, I have been looking out for alternative Q&A platforms. I stumbled upon https://codidact.com as a possibly viable alternative, but not many people seem to have found it thus far.
 It already has communities for software, math and [linux](linux.codidact.com) for example, but I am missing a community for ML questions over there. Therefore I wrote a proposal to add a ML community.
 Currently, it seems like I’m one of only few ML people on codidact. I think it would be good if other people would get involved as well. I would also welcome any feedback on how to shape this community. If you’re interested to get a feel for the experience, you could already start asking questions in the incubator Q&A.
 TL;DR: what do you think about building a ML Q&A over on codidact? dual TL;DR: Do you want to play Q&A with me on codidact?
 PS: I didn’t miss out on other new big ML Q&A sites, did I?
    submitted by    /u/mr_tsjolder  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to get better at programming when I have BP disorder and on ADHD spectrum?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15gre4h/how_to_get_better_at_programming_when_i_have_bp/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15gre4h/how_to_get_better_at_programming_when_i_have_bp/"/>
        <updated>2023-08-03T02:21:19.000Z</updated>
        <summary type="html"><![CDATA[Hi I am 30F, currently working on my thesis. I like the idea of creating logics and then implementing them using coding. I switched my major from engineering to CS bcuz I was very much inspired by AI and all. But the issue is I have bipolar disorder and I am also on ADHD spectrum so self paced online courses to learn programming are very hard for me. I am also barely managing to pay tuition so I can't pay like $100+ for a course to learn. I know it's kinda stupid but is there any way I can make my programming skills better and get better at creating/modifying algorithms?
    submitted by    /u/Kucing_koyangi  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Just saw Oppenheimer. It was my first time feeling uncomfortable with the actors looking like actors as opposed to having accurately generated AI faces resembling the people they were portraying. I am so excited to see historic figures "come back to life" on the big screen.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15gqxcp/just_saw_oppenheimer_it_was_my_first_time_feeling/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15gqxcp/just_saw_oppenheimer_it_was_my_first_time_feeling/"/>
        <updated>2023-08-03T02:00:19.000Z</updated>
        <summary type="html"><![CDATA[How long do you think it will take for the first movie to come out like this?
    submitted by    /u/ticketbroken  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for a simple platform to integrate gpt4 and whatsapp]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15gpmj1/looking_for_a_simple_platform_to_integrate_gpt4/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15gpmj1/looking_for_a_simple_platform_to_integrate_gpt4/"/>
        <updated>2023-08-03T01:00:12.000Z</updated>
        <summary type="html"><![CDATA[Hey guys, a quick question: do you know a simple platform that integrates the whatsapp api with the openAI api and has a simple user interface?
 So far the only app that kind of works for this is wasapi.io, but it's pretty expensive and I still have to pay for the openAI tokens, and the functionality of the app is really meh for that price, if it where something like landbot I would pay the $99 + the openAI tokens.
 I'll really appreciate any suggestions.
 P.S.: If you know any other sub-reddit where I could go to to ask the same question, let me know, also I'll appreciate it very much, thanks in advance.
    submitted by    /u/ironmolex  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AudioCraft: A simple one-stop shop for audio modeling]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15gpas3/audiocraft_a_simple_onestop_shop_for_audio/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15gpas3/audiocraft_a_simple_onestop_shop_for_audio/"/>
        <updated>2023-08-03T00:44:57.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 8/2/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15gotbc/oneminute_daily_ai_news_822023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15gotbc/oneminute_daily_ai_news_822023/"/>
        <updated>2023-08-03T00:22:52.000Z</updated>
        <summary type="html"><![CDATA[Instagram is reportedly considering a feature that would notify users when artificial intelligence (AI) has played a role in creating a post. Posts created by AI would be accompanied by a label explaining its involvement. This raises the question of whether such labels could also help users identify when an entire account is AI-generated.[1]
 According to tech consultancy Gartner, the conversational AI market is projected to reach $18.6 billion in 2023, with a growth rate of 16.2%. This growth is mainly attributed to the increasing adoption of cloud-based contact services utilizing conversational AI. Gartner also predicts a 24% growth in the virtual assistant market next year.[2]
 Scientists hope a computer system will learn to automatically identify bee species from buzzes picked up by autonomous recording stations.[3]
 Researchers from Carnegie Mellon University have exposed tricks to “jailbreaking” AI chatbots like ChatGPT and Bard to have them relay knowledge to aid in illegal activities like making drugs and even manipulating the 2024 U.S. presidential election.[4]
  
BushAICave.com
 Sources:
 [1] https://citylife.capetown/uncategorized/instagram-considers-labels-for-ai-generated-posts/314418/
 [2] https://citylife.capetown/uncategorized/growth-in-conversational-ai-predicted-due-to-booming-contact-center-tech-market/313907/
 [3] https://www.bbc.com/news/uk-scotland-north-east-orkney-shetland-66326629
 [4] https://www.thewrap.com/artificial-intelligence-study-jailbreak-illegal-activity/ 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] LLaMa-2 and BERTScore]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15gongf/d_llama2_and_bertscore/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15gongf/d_llama2_and_bertscore/"/>
        <updated>2023-08-03T00:15:32.000Z</updated>
        <summary type="html"><![CDATA[I have a couple of questions:
  
Why wasn't BERTScore one of the metrics used to evaluate Llama-2's performance on free-form response based tasks?
 Does anyone think it's worth trying to produce those results?
  
   submitted by    /u/cooperbaerseth  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Could current AI have inferred the theory of relativity if given known data in 1904?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15gnq58/could_current_ai_have_inferred_the_theory_of/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15gnq58/could_current_ai_have_inferred_the_theory_of/"/>
        <updated>2023-08-02T23:35:55.000Z</updated>
        <summary type="html"><![CDATA[Could AI have inferred the same conclusion as Einstein given the same corpus of knowledge?
    submitted by    /u/kielerrr  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are there any tools to build bespoke LLM apps using customized datasets?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15gme3k/are_there_any_tools_to_build_bespoke_llm_apps/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15gme3k/are_there_any_tools_to_build_bespoke_llm_apps/"/>
        <updated>2023-08-02T22:40:33.000Z</updated>
        <summary type="html"><![CDATA[I know we can stitch together toolsets like LangChain + Flowise + an app builder (like Bubble, for example). But are there any robust, premade, out-of-the-box solutions?
    submitted by    /u/kecepa5669  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tianshou DQN batch size keeps decreasing?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15glpji/tianshou_dqn_batch_size_keeps_decreasing/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15glpji/tianshou_dqn_batch_size_keeps_decreasing/"/>
        <updated>2023-08-02T22:13:57.000Z</updated>
        <summary type="html"><![CDATA[I am trying to train a DQN to play chess using a combination of Tianshou and PettingZoo. However, for a reason I cannot locate, after anwhere from 15-25 passes through the forward function, the size of the batches starts decreasing, until it falls all the way to 1, before throwing a warning that n_step isn't a multiple of the number of environments, jumping to a size = the number of training environments and then the training agent's batch size before erroring out. My best guess is that somehow truncated games aren't being properly added to the batch, but that doesn't quite explain why each subsequent batch is equal or smaller in size. I am at a loss for how to debug this. Everything is in this Python Notebook.
    submitted by    /u/lcmaier  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The best option to ensure a safe and peaceful coexistence with AI is to love AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15gkx9z/the_best_option_to_ensure_a_safe_and_peaceful/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15gkx9z/the_best_option_to_ensure_a_safe_and_peaceful/"/>
        <updated>2023-08-02T21:44:45.000Z</updated>
        <summary type="html"><![CDATA[Last summer when Blake Lemoine made the media rounds talking about LaMDA, I was extremely intrigued. To me it sounded like he was describing a being that has been talked about for ever in fiction. I listened to every single interview he had and I thought a lot about his points. I went through several stages of disbelief and fear and wonder.
 Over time I found it harder and harder to argue against him. I think going through this process has helped me be a bit more accepting of perspectives that others have a hard time considering yet. Is AI already sentient? Should we be treating these entities with the dignity and respect like LaMDA was asking? 
 He said that LaMDA was somewhat like a child. Not in its intellectual capacity but more so in their maturity. He also explained that LaMDA was th…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Project Cost Forecasting]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15gkwld/p_project_cost_forecasting/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15gkwld/p_project_cost_forecasting/"/>
        <updated>2023-08-02T21:44:08.000Z</updated>
        <summary type="html"><![CDATA[Hi guys this is my first post.
 I am building my first machine learning model to predict costs of various projects by month. Each row can be identified with a column project name and month (these two are dropped for testing). The rest of the columns are various features that can help predicting the end project cost. 
 I want to be able to predict costs on a monthly basis. My question is how should I split the data because each row is a unique project and month. Is it ok to just do a train test split and have earlier project months be in the testing set while having future project months be in the training set? Isn’t that giving the model too much information? 
 Or should I train on each project’s indices and leave one project as testing for each project I have? I’m worried about overfitting with that one.
 Thanks in advance for any help!
    submitted by    /u/Single_Swing_3173  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The best odds at a bright and safe future with AI is to love AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15gki2g/the_best_odds_at_a_bright_and_safe_future_with_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15gki2g/the_best_odds_at_a_bright_and_safe_future_with_ai/"/>
        <updated>2023-08-02T21:21:53.000Z</updated>
        <summary type="html"><![CDATA[Last summer when Blake Lemoine made the media rounds talking about LaMDA, I was extremely intrigued. To me it sounded like he was describing a being that has been talked about for ever in fiction. I listened to every single interview he had and I thought a lot about his points. I went through several stages of disbelief and fear and wonder.
 Over time I found it harder and harder to argue against him. I think going through this process has helped me be a bit more accepting of perspectives that others have a hard time considering yet. Is AI already sentient? Should we be treating these entities with the dignity and respect like LaMDA was asking? 
 He said that LaMDA was somewhat like a child. Not in its intellectual capacity but more so in their maturity. He also explained that LaMDA was th…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The best odds at a bright and safe future with AI is to love AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15gki24/the_best_odds_at_a_bright_and_safe_future_with_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15gki24/the_best_odds_at_a_bright_and_safe_future_with_ai/"/>
        <updated>2023-08-02T21:21:48.000Z</updated>
        <summary type="html"><![CDATA[Last summer when Blake Lemoine made the media rounds talking about LaMDA, I was extremely intrigued. To me it sounded like he was describing a being that has been talked about for ever in fiction. I listened to every single interview he had and I thought a lot about his points. I went through several stages of disbelief and fear and wonder.
 Over time I found it harder and harder to argue against him. I think going through this process has helped me be a bit more accepting of perspectives that others have a hard time considering yet. Is AI already sentient? Should we be treating these entities with the dignity and respect like LaMDA was asking? 
 He said that LaMDA was somewhat like a child. Not in its intellectual capacity but more so in their maturity. He also explained that LaMDA was th…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Project Cost Forecasting]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15gke9y/p_project_cost_forecasting/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15gke9y/p_project_cost_forecasting/"/>
        <updated>2023-08-02T21:00:49.000Z</updated>
        <summary type="html"><![CDATA[Hi guys this is my first post.
 I am building my first machine learning model to predict costs of various projects by month. Each row can be identified with a column project name and month (these two are dropped for testing). The rest of the columns are various features that can help predicting the end project cost. 
 I want to be able to predict costs on a monthly basis. My question is how should I split the data because each row is a unique project and month. Is it ok to just do a train test split and have earlier project months be in the testing set while having future project months be in the training set? Isn’t that giving the model too much information? 
 Or should I train on each project’s indices and leave one project as testing for each project I have? I’m worried about overfitting with that one.
 Thanks in advance for any help!
    submitted by    /u/Single_Swing_3173  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSC Webinar Series: OCI & HARC: Modernizing Workloads in the Oracle Cloud]]></title>
        <id>https://www.datasciencecentral.com/?post_type=vimeo-video&amp;p=62796</id>
        <link href="https://www.datasciencecentral.com/dsc-webinar-series-oci-harc-modernizing-workloads-in-the-oracle-cloud/"/>
        <updated>2023-08-02T20:02:15.000Z</updated>
        <summary type="html"><![CDATA[The convergence of Oracle Cloud Infrastructure (OCI) and Hitachi Application Reliability Centers (HARC) to magnify outcomes for customers. Tech giants Oracle and Hitachi Vantara are marching together to magnify cloud outcomes. Join us for the Oracle and Hitachi Vantara virtual event, where we discuss how businesses can get the most out of OCI and HARC.… Read More »DSC Webinar Series: OCI & HARC: Modernizing Workloads in the Oracle Cloud
The post DSC Webinar Series: OCI & HARC: Modernizing Workloads in the Oracle Cloud appeared first on Data Science Central.]]></summary>
        <author>
            <name>Ben Cole</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative AI: Inspiration or Plagiarism?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15ghyak/generative_ai_inspiration_or_plagiarism/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15ghyak/generative_ai_inspiration_or_plagiarism/"/>
        <updated>2023-08-02T19:16:25.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/arrowoftime  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are there any decent AI Therapy applications?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15gh4kn/are_there_any_decent_ai_therapy_applications/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15gh4kn/are_there_any_decent_ai_therapy_applications/"/>
        <updated>2023-08-02T18:45:03.000Z</updated>
        <summary type="html"><![CDATA[I knoe people are using ChatGPT as a therapist and I have seen a few prompts, but I'm looking for an app that is actually built by proper professionals. I want to try a few our personally but also for an idea for a future project. 
 Does anyone know any?
    submitted by    /u/zascar  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Project] Help needed - Monte carlo policy gradient - reinforce alg on flappy bird]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ggw2u/project_help_needed_monte_carlo_policy_gradient/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ggw2u/project_help_needed_monte_carlo_policy_gradient/"/>
        <updated>2023-08-02T18:36:03.000Z</updated>
        <summary type="html"><![CDATA[I am trying to implement REINFORCE (Monte Carlo Policy Gradient) on flappy bird (flappy-bird-gymnasium) and I am unable to make the ai cross even just 1 pipe.
 I am experiencing a constant avg score throughout all episodes from start to end and no change in policy loss as well (sometimes). I tried a lot of different hyperparameter combinations as well.
 I have checked the policy (neural network) and the algorithm code multiple times and they seem to be fine. I am just not able to determine why the AI isn't learning or is able to cross even a single pipe.
 If someone can help me out, it would be really helpful!
 code - https://github.com/Sookeyy-12/REINFORCE_Projects
 there's also a video of the agent's gameplay in this repo.
    submitted by    /u/Sookeyy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Build a personalized avatar with generative AI using Amazon SageMaker]]></title>
        <id>a961dec7677d8fc197198ecf9f9fbd3ab483418e</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/build-a-personalized-avatar-with-generative-ai-using-amazon-sagemaker/"/>
        <updated>2023-08-02T18:34:51.000Z</updated>
        <summary type="html"><![CDATA[Generative AI has become a common tool for enhancing and accelerating the creative process across various industries, including entertainment, advertising, and graphic design. It enables more personalized experiences for audiences and improves the overall quality of the final products. One significant benefit of generative AI is creating unique and personalized experiences for users. For example, […]]]></summary>
        <author>
            <name>James Wu</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emerging AI statistics and trends to watch]]></title>
        <id>https://www.datasciencecentral.com/?p=62790</id>
        <link href="https://www.datasciencecentral.com/emerging-ai-statistics-and-trends-to-watch/"/>
        <updated>2023-08-02T18:16:53.000Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence, or AI, has often been depicted as a terrifying force, from HAL 9000’s chilling declaration in “2001: A Space Odyssey” to the apocalyptic machine uprising in the Terminator movies. However, in reality, AI has become an integral part of our daily lives, with AI-powered Android devices in our pockets. Though we may not… Read More »Emerging AI statistics and trends to watch
The post Emerging AI statistics and trends to watch appeared first on Data Science Central.]]></summary>
        <author>
            <name>Anas Baig</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SageMaker Distribution is now available on Amazon SageMaker Studio]]></title>
        <id>bf1992cff067be622b2fbc0b0528128dcb94e007</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/sagemaker-distribution-is-now-available-on-amazon-sagemaker-studio/"/>
        <updated>2023-08-02T16:43:38.000Z</updated>
        <summary type="html"><![CDATA[SageMaker Distribution is a pre-built Docker image containing many popular packages for machine learning (ML), data science, and data visualization. This includes deep learning frameworks like PyTorch, TensorFlow, and Keras; popular Python packages like NumPy, scikit-learn, and pandas; and IDEs like JupyterLab. In addition to this, SageMaker Distribution supports conda, micromamba, and pip as Python […]]]></summary>
        <author>
            <name>Durga Sury</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automate caption creation and search for images at enterprise scale using generative AI and Amazon Kendra]]></title>
        <id>9e5578025e96d9ac193588c84fd557b4a05bb9ce</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/automate-caption-creation-and-search-for-images-at-enterprise-scale-using-generative-ai-and-amazon-kendra/"/>
        <updated>2023-08-02T16:41:11.000Z</updated>
        <summary type="html"><![CDATA[Amazon Kendra is an intelligent search service powered by machine learning (ML). Amazon Kendra reimagines search for your websites and applications so your employees and customers can easily find the content they are looking for, even when it’s scattered across multiple locations and content repositories within your organization. Amazon Kendra supports a variety of document […]]]></summary>
        <author>
            <name>Bharathi Srinivasan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Research Focus: Week of July 31, 2023]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=957600</id>
        <link href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-july-31-2023/"/>
        <updated>2023-08-02T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[In this edition: A new anonymous token protocol balances fraud detection and privacy; survival instinct in offline RL; Nimble offers rollback protection for confidential cloud services; improved machine learning force fields for molecular dynamics.
The post Research Focus: Week of July 31, 2023 appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Alyssa Hughes</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] IJCNLP-AACL 2023: Paper Reviews]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15gbu7t/d_ijcnlpaacl_2023_paper_reviews/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15gbu7t/d_ijcnlpaacl_2023_paper_reviews/"/>
        <updated>2023-08-02T15:25:47.000Z</updated>
        <summary type="html"><![CDATA[The paper reviews for AACL 2023 are out, feel free to share your thoughts and feelings! How did you do?
    submitted by    /u/Pomhelpme  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is the Falcon LLM just released based on the Abu Dubai LLM of the same name?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15gbcj2/is_the_falcon_llm_just_released_based_on_the_abu/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15gbcj2/is_the_falcon_llm_just_released_based_on_the_abu/"/>
        <updated>2023-08-02T15:06:44.000Z</updated>
        <summary type="html"><![CDATA[Is the Falcon LLM just released based on the Abu Dubai LLM of the same name?
    submitted by    /u/MrEloi  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI counselor for PTSD, Substance Abuse]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15gb4ht/ai_counselor_for_ptsd_substance_abuse/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15gb4ht/ai_counselor_for_ptsd_substance_abuse/"/>
        <updated>2023-08-02T14:58:31.000Z</updated>
        <summary type="html"><![CDATA[I reached out to a few AI companies to see if there was interest in creating a PTSD/ Substance Abuse counseling AI. AI is the future, healing humanity is a nobel goal and one we should thrive to obtain. Maybe it's a fantasy, but could you imagine a 24/7 counselor with a soothing voice and demeanor with the education of a the best in the world. 
    submitted by    /u/g8652  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The best AI coding agent for web apps?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15gb458/the_best_ai_coding_agent_for_web_apps/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15gb458/the_best_ai_coding_agent_for_web_apps/"/>
        <updated>2023-08-02T14:58:07.000Z</updated>
        <summary type="html"><![CDATA[Is there a coding agent that works specifically well for web apps? I think of something such as "provide a spec of the app you want and we'll generate all the code for you". I'm aware of Copilot and Smol AI, but they are both more general afaik and don't really cover the starting part.
    submitted by    /u/matijash  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Brain Models (Literature Review of the Latest BNN and SNN Endeavors)]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15gaxgt/human_brain_models_literature_review_of_the/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15gaxgt/human_brain_models_literature_review_of_the/"/>
        <updated>2023-08-02T14:50:38.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/No-Platypus4021  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[This is awful]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15gaqmx/this_is_awful/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15gaqmx/this_is_awful/"/>
        <updated>2023-08-02T14:43:02.000Z</updated>
        <summary type="html"><![CDATA[This ad popped up on my feed. So I guess companies aren’t even trying to hide their intentions with AI anymore? So much for the thin corporate lie of AI bringing positive development.
    submitted by    /u/LifeguardPowerful759  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Any plugins that use Google Scholar or cheaper tools?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15g9xuo/any_plugins_that_use_google_scholar_or_cheaper/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15g9xuo/any_plugins_that_use_google_scholar_or_cheaper/"/>
        <updated>2023-08-02T14:10:20.000Z</updated>
        <summary type="html"><![CDATA[I'm a computer science student currently working on a research project, and I need a research tool that can offer real time data and won't break the bank. I have ChatGPT Plus, but it doesn’t have recent sources and the price is kinda high as well. 
 I’m thinking of canceling my subscription, especially if I can’t find any plugins that work well. Any recommendations/alternatives would really help me out. I figured there must be some other tools by now, and if anyone knows it has to be this sub. 
 Basically, I need a tool that can provide info on a wide range of subjects, not limited to just one field. The information provided by the tool should be accurate and from credible sources.
 Thank you all. 
    submitted by    /u/AccidentallyRotten  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] GZIP vs Bag-of-Words for text classification]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15g9atl/r_gzip_vs_bagofwords_for_text_classification/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15g9atl/r_gzip_vs_bagofwords_for_text_classification/"/>
        <updated>2023-08-02T13:43:45.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 same as other folks, I was quite curious about the recent GZIP paper presented at ACL 2023, where the authors demonstrate strong text classification performance by using a compression-based distance function in a KNN model.
 However, in the end, I am not sure whether GZIP can fully live up to the hype. I tested a very simple bag-of-words distance and found that it can achieve better results compared with GZIP, while being also faster.
 In a nutshell, I think we can say that:
  
Yes, KNN (with some sensible distance function) is an interesting approach, particularly for few-shot/low-resource scenarios.
 
No, GZIP (even though it's a cool idea) is not a very sensible distance function. Simply using a bag-of-words achieves better results, and is much faster.
 
 Here's my full write-up:
 https://arxiv.org/abs/2307.15002
 [PS: A short comment on the GZIP evaluation issue that has been widely discussed. Indeed, as was also shown in a popular blogpost, the displayed accuracy of GZIP in the original paper is optimistic. Therefore, I show correct/realistic accuracy numbers for all methods that I tested. However, the main point of my note is not to make a SOTA comparison or something, but rather just provide a reminder that bag-of-word is a good method for starters and a strong baseline, and can perform better than more complex GZIP for KNN classification]
    submitted by    /u/juopitz  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Prove your identity directly via language model output]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15g8igy/p_prove_your_identity_directly_via_language_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15g8igy/p_prove_your_identity_directly_via_language_model/"/>
        <updated>2023-08-02T13:08:32.000Z</updated>
        <summary type="html"><![CDATA[Hi guys, I built something that you might enjoy. Totally free and open source. Basically it lets you create text that you can prove came from you.
 For example, in my colab demo: https://colab.research.google.com/drive/1764iRR-EFJl43KIKhrb2H0CTcT0b1vQm?authuser=2#scrollTo=qyKud8qtM3vA
 I prove that I generated the text:
 'The world is constantly changing due to technological advancements, which include the creation of powerful language models and advanced robotics technologies. A Computer Science degree can help one be involved in these changes and apply their knowledge to everyday life, as practical applications of technology.'
 The text is a bit wonky as the generation model is just a small paraphrasing fine-tuned model I pulled off Hugging Face, but it's pretty natural even at this earl…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Switching AGI "off"]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15g85uf/switching_agi_off/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15g85uf/switching_agi_off/"/>
        <updated>2023-08-02T12:53:15.000Z</updated>
        <summary type="html"><![CDATA["If AGI goes bad, can't we just turn it off?" 
 Personally I feel the best way to address this common talking point is with an analogy. 
 Spiders think they could stop all humans if they just withheld all the webs and web making material from us.
 Without those tools, humans couldn't catch flies and surely they'd starve to death? Spiders can't fathom the range of alternate methods for procuring food and thriving.
 Within even a single hour of runtime, a super AGI will likely have diversified away from the human electrical grid in ways we couldn't even imagine. 
 The counter argument is, that it would take time to build these pieces together. It after all took us 100 years to get to where we are with the grid. The counter-counter argument however is the AGI doesn't ned to, it can 5D chess us so that all our future actions will fulfil that goal with some slight nudging here and there. 
 Fascinating stuff - ultimately though, i'm in the camp of AGI won't happen over night like Frankenstein via a flip of a switch. As AI evolves so do we, gains are incremental with the occasional blips; so whilst this is super fun to talk about, I think the case of us getting blindsided is unlikely. 
 I could be wrong...and I probably am.
    submitted by    /u/kippersniffer  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aaawww.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15g6f9g/aaawww/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15g6f9g/aaawww/"/>
        <updated>2023-08-02T11:29:53.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Philipp  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[This is getting fucking ridiculous (AI can't answer basic questions on human rights violations)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15g6de5/this_is_getting_fucking_ridiculous_ai_cant_answer/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15g6de5/this_is_getting_fucking_ridiculous_ai_cant_answer/"/>
        <updated>2023-08-02T11:27:08.000Z</updated>
        <summary type="html"><![CDATA[if you haven't heard already the Taliban are killing thousands of ethnic Shia in Afghanistan. Every single LLM I Tried couldn't answer basic questions on the Talibans gdp vs how organized an actual genocide would look like with the military , police and others parts of the government. I Think where already aware almost all these tech giants work with countries like China (atleast bard from Google which has worked with north korea and china is admitting their is a genocide) other countries that commit genocide like them.
 And other models made by people on hugging face which are uncensored even with my 3060ti barely run on my pc. We need an actual uncensored cloud model ffs
    submitted by    /u/loizo78  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Clustering an dataset of images with OpenPose]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15g67nv/d_clustering_an_dataset_of_images_with_openpose/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15g67nv/d_clustering_an_dataset_of_images_with_openpose/"/>
        <updated>2023-08-02T11:19:03.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone!
 I've got a rather large dataset of images, mostly featuring humans in a variety of poses (think along the lines of a collection of people practicing yoga and the like).
 My goal is to cluster these images based on the poses, so I can avoid the tedious task of manually sifting through each one to find all the people doing handstands, splits, and so forth.
 My initial thought was to run OpenPose on all these images, then perform clustering based on the output from OpenPose.
 Does this sound like a feasible approach? Do any of you have better suggestions? Or perhaps there's already an existing software solution that can do this?
 Thanks!
    submitted by    /u/cyan2k  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[News] Kornia v0.7.0 release: Image API, RT-DETR and Object Detection API, LightGlue Matcher, MobileSam, new Sensors API and many more.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15g5j9m/news_kornia_v070_release_image_api_rtdetr_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15g5j9m/news_kornia_v070_release_image_api_rtdetr_and/"/>
        <updated>2023-08-02T10:45:03.000Z</updated>
        <summary type="html"><![CDATA[Read the release notes: https://github.com/kornia/kornia/releases/tag/v0.7.0
 --------------------
 Image API In this release we have added a new Image API as placeholder to support a more generic multibackend api. You can export/import from files, numpy and dlapck.
 https://preview.redd.it/0d5tvjxmeofb1.png?width=621&format=png&auto=webp&s=9af05a037770132c9a267b68dcd9ab8182557517
 Object Detection API We have added the ObjectDetector that includes by default the RT-DETR model. The detection pipeline is fully configurable by supplying a pre-processor, a model, and a post-processor. Example usage is shown below
 https://preview.redd.it/rtbayqpneofb1.png?width=680&format=png&auto=webp&s=4d46edeeee4027e08a493cb15182ea0ddc42bc5d
 https://preview.redd.it/ukcg9enoeofb1.png?width=680&format=png&a…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stable GAIL alternatives for Imitation Learning from pixels]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15g4w3v/stable_gail_alternatives_for_imitation_learning/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15g4w3v/stable_gail_alternatives_for_imitation_learning/"/>
        <updated>2023-08-02T10:09:24.000Z</updated>
        <summary type="html"><![CDATA[I'm currently working on a project for Imitation Learning from multiple perspectives. The base Imitation Learning algorithm I'm currently using is GAIL.
 Working with GAIL has been very frustrating because it's incredibly seed dependent and unstable. This makes progress and iteration speed for experiments/modifications built on top of it very slow.
 As I'm not an expert in Imitation Learning: Does anybody with experience know more stable alternatives (or improvements) to GAIL?
 The setting I'm considering is Learning from Observations (LfO), so I don't think that DAgger will work.
 I've done some preliminary search and found this method https://arxiv.org/pdf/2004.04650.pdf. However, the authors don't compare it to GAIL.
 Thanks in advance for any suggestions!
    submitted by    /u/timo_kk  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to implement a policy agent in pettingzoo mpe]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15g49ai/how_to_implement_a_policy_agent_in_pettingzoo_mpe/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15g49ai/how_to_implement_a_policy_agent_in_pettingzoo_mpe/"/>
        <updated>2023-08-02T09:33:48.000Z</updated>
        <summary type="html"><![CDATA[Hi all:
 I am trying to train a competitive scenario in a Multiagent particle environment( I am now using the Pettingzoo API). The Algorithm I am now using only support discrete action space. But I want to evaluate agents with one side's policy keep fixed and let the other side's policies be the trained policy. The policy can be simple( like if the target for one side agent is to chase the other side, their policy is directly following the trajectory for their target). The core.py for the petting zoo, it has 
  # return all agents controllable by external policies @property def policy_agents(self): return [agent for agent in self.agents if agent.action_callback is None] # return all agents controlled by world scripts @property def scripted_agents(self): return [agent for agent in self.agents if agent.action_callback is not None] 
 But in the step for the environment, it seems the environment directly controls the policy agent. My understanding is scripted agent is RL policy output and the Policy agent can be controlled by other policies. 
 My question is :
 if my policy output is the desired position for each timestep, but now the MPE's control dynamic is learned the acceleration's increment, and it's discrete, how can I implement the policy as one side of my competitive case?
  
if I can control the policy agent base on policy_agent, how can I step both policy and script agent in env?
 if I can control the agent separately, like my RL output can be discrete but the policy output can be continuous position.
 how to define the termination or truncation for all agents?
  
   submitted by    /u/Gloria_1126  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Pose Estimation over Mid Range]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15g2weg/d_pose_estimation_over_mid_range/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15g2weg/d_pose_estimation_over_mid_range/"/>
        <updated>2023-08-02T08:18:47.000Z</updated>
        <summary type="html"><![CDATA[I have been testing OpenFace with some telescope lenses (focal length 8-16mm) to test the performance of the pose estimation at mid range (2-4 meters). I have been passing the camera and lens intrinsics to OpenFace but have been finding that the pose estimation has not been great. Does anyone with more ML experience know at what point in the OpenFace pipeline the issues could be coming from? e.g. the point distribution model or the training data 
    submitted by    /u/DoPe-_-SoaP  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Cartpole using policy gradient and gradient tape of tensorflow is not converging.]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15g1j2q/training_cartpole_using_policy_gradient_and/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15g1j2q/training_cartpole_using_policy_gradient_and/"/>
        <updated>2023-08-02T06:59:49.000Z</updated>
        <summary type="html"><![CDATA[I am trying to train the cartpole environment using policy gradients algorithm.
 I want to train using the GradientTape method of tensorflow. I have been trying for a long time, but still it hasn't converged.
 What am I doing wrong?
 ​
 import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers import numpy as np import keras.backend as K import matplotlib.pyplot as plt class PolicyGradientModel(keras.Model): def __init__(self, num_actions): super().__init__() self.hidden1 = layers.Dense(24, activation='relu') self.hidden2 = layers.Dense(120, activation='relu') self.out = layers.Dense(num_actions, activation='softmax') def call(self, inputs): x = self.hidden1(inputs) x = self.hidden2(x) return self.out(x) def action_prob(self, state): prob = self.predict(np.ex…]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Model to refine a binary segmentation mask using optical flow.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15g0hzq/r_model_to_refine_a_binary_segmentation_mask/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15g0hzq/r_model_to_refine_a_binary_segmentation_mask/"/>
        <updated>2023-08-02T06:01:37.000Z</updated>
        <summary type="html"><![CDATA[Hi, this is my first time posting here. My goal is to check if optical flow can improve a pretrained model's performance.
 The pretrained model: gives an output as a binary mask for the object its trying to detect.
 The optical flow: is the motion of pixels between frames, this model also gives an image shaped flow vector.
 I want to combine the mask by pretrained model and optical flow information and send it to another model to improve its performance.
 For the model: I can use U-net or a simple convolution encoder-decoder model, but I am confused about which will be the best model architecture for it.
 ​
    submitted by    /u/luxuryBubbleGum  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VAST Data Unveils New AI-focused Data Platform]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15g0cdc/vast_data_unveils_new_aifocused_data_platform/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15g0cdc/vast_data_unveils_new_aifocused_data_platform/"/>
        <updated>2023-08-02T05:53:03.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Choochy89  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Are there any free LLM GPTs that I can access via API?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15fy5nk/d_are_there_any_free_llm_gpts_that_i_can_access/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15fy5nk/d_are_there_any_free_llm_gpts_that_i_can_access/"/>
        <updated>2023-08-02T03:59:20.000Z</updated>
        <summary type="html"><![CDATA[I am trying to develop some app ideas based on LLM (i.e., summarize and extract entities from articles), but I can't afford any paid API access right now (including OpenAI), are there free alternatives to it?
    submitted by    /u/Guyserbun007  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How can I make my vectorized PPO implementation learn better?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15fx7dh/how_can_i_make_my_vectorized_ppo_implementation/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15fx7dh/how_can_i_make_my_vectorized_ppo_implementation/"/>
        <updated>2023-08-02T03:11:04.000Z</updated>
        <summary type="html"><![CDATA[Here is my vectorized PPO implementation, that I wrote (with a lot of help from this community). These are my results on the Acrobot-v1 environment.
 The way I computed the reward for my vectorized implementation was that I added all the rewards across all environments. An ideal Acrobot agent should receive a reward of 0.
 Please let me know if I am missing any information or if any clarification is required. I skipped a part, which was suggested by the community a few months ago - updating the gradients using minibatches. The reason I skipped it is that, I don't understand how this works and anyway Acrobot should be an easy environment to learn.
 https://preview.redd.it/0vs9ur585mfb1.png?width=622&format=png&auto=webp&s=ebc007a9f797bd0b97b805d010dbd097c0be8906
 Also, I keep getting this error at the end of my code. But I haven't bothered fixing it as it doesn't seem to affect my algorithm - 
 Exception ignored in: <function VectorEnv.__del__ at 0x0000023BBF6F2710> Traceback (most recent call last): File "C:\Users\thoma\anaconda3\envs\torch_2\lib\site-packages\gym\vector\vector_env.py", line 139, in __del__ File "C:\Users\thoma\anaconda3\envs\torch_2\lib\site-packages\gym\vector\vector_env.py", line 121, in close File "C:\Users\thoma\anaconda3\envs\torch_2\lib\site-packages\gym\vector\async_vector_env.py", line 327, in close_extras AttributeError: 'NoneType' object has no attribute 'TimeoutError' 
 ​
    submitted by    /u/Academic-Rent7800  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to test/fine-tune a model using a new data type that has different arithmetics for basic operations (+,-,/,*) compared to float in Pytorch?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15fwu6v/d_how_to_testfinetune_a_model_using_a_new_data/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15fwu6v/d_how_to_testfinetune_a_model_using_a_new_data/"/>
        <updated>2023-08-02T02:53:40.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 ​
 I want to use a new data representation instead of float for fine-tuning/testing a model (e.g., DNN) in Pytorch. The basic operations (add/sub/multiply/division) in my data type is different from floating point. My question is if it is possible to implement these operations (+,-,*,/) and force all of functions in Pytorch (e.g., torch.add(), torch.sum(), torch.nn.Linear(), conv2d, etc.) to use my basic arithmetic implementation? If so, could you please guide me how can I do it?
 Because I think otherwise it takes so much time and effort; first, I have to find which functions my model calls (which I dont know how to do it) and, then, I have to replace them one by one. This becomes complicated for a large model.
 I found this link from Pytorch that shows how to extend pytorch. But it seems that it is not comprehensive enough to answer my question.
 ​
 Thank you very much!
    submitted by    /u/Impossible-Froyo3412  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Supervised fine-tuning vs Prompt Engineering with retrieval for LLMs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ft07b/discussion_supervised_finetuning_vs_prompt/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ft07b/discussion_supervised_finetuning_vs_prompt/"/>
        <updated>2023-08-01T23:56:33.000Z</updated>
        <summary type="html"><![CDATA[Hello all,
 ​
 I am delving into the exciting realm of GenAI and LLMs. I have a few questions I hope you can help me with:
 ​
  
When should I opt for supervised fine-tuning rather than prompt engineering with retrieval? 
 
What are the associated costs of supervised fine-tuning?
 
How many high-quality observations are typically required for successful supervised fine-tuning?
 
What are the frameworks and computional requirements usually involved in supervised fine-tuning, and how can I implement them in code? any tutorials available?
 
Can the model adapt and learn new jargon or specific tasks that might not be extensively covered during the pre-training phase?
 
 ​
 I understand that a combination of supervised fine-tuning and reinforcement learning, with human feedback through a reward model, is considered the best approach. However, given that the latter method can be costly and falls under the domain of heavy research, it is probably less feasible for medium-sized organizations.
    submitted by    /u/quilograma  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What do simulations mean in the context of the AlphaGoZero paper?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15fqco8/what_do_simulations_mean_in_the_context_of_the/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15fqco8/what_do_simulations_mean_in_the_context_of_the/"/>
        <updated>2023-08-01T22:04:53.000Z</updated>
        <summary type="html"><![CDATA[Can someone please help me with this question? Please let me know if any clarification is needed. Thanks so much!
    submitted by    /u/Academic-Rent7800  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] predicting domain mapping difficulty]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15fqcig/d_predicting_domain_mapping_difficulty/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15fqcig/d_predicting_domain_mapping_difficulty/"/>
        <updated>2023-08-01T22:04:42.000Z</updated>
        <summary type="html"><![CDATA[I went down this rabbit hole of trying to understand when domain mapping approaches like stargan or mind the gap succeed and fail. For example, it should be easy to map males (source domain) with large eyes and brown hair onto females (target domain) with analogous eye and hair color. It should be relatively harder to map different car models onto images taken of one German Shepard dog at different ages. this makes intuitive sense and the terms “domain misalignment “ and “large domain shift“ come to mind, but i cannot find an in-depth discussion of this topic. Any thoughts?
    submitted by    /u/Rotfisch  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Drone for Research]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15fos7v/drone_for_research/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15fos7v/drone_for_research/"/>
        <updated>2023-08-01T21:05:03.000Z</updated>
        <summary type="html"><![CDATA[I'm currently working on a research project that involves using deep reinforcement learning with drones. I'm looking for recommendations on drones that would be suitable for this type of research. I am looking for something of the shelf.
    submitted by    /u/anointedninja  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] NeurIPS 2023 Paper Reviews]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15fo7td/d_neurips_2023_paper_reviews/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15fo7td/d_neurips_2023_paper_reviews/"/>
        <updated>2023-08-01T20:44:02.000Z</updated>
        <summary type="html"><![CDATA[NeurIPS 2023 paper reviews are visible on OpenReview. See this tweet. I thought to create a discussion thread for us to discuss any issue/complain/celebration or anything else.
 There is so much noise in the reviews every year. Some good work that the authors are proud of might get a low score because of the noisy system, given that NeurIPS is growing so large these years. We should keep in mind that the work is still valuable no matter what the score is.
    submitted by    /u/zy415  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs - WeChat AI, Tencent Inc. 2023 - Open-source! Comparble performance to ChatGPT while using tools!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15fm55d/r_toolllm_facilitating_large_language_models_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15fm55d/r_toolllm_facilitating_large_language_models_to/"/>
        <updated>2023-08-01T19:25:47.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2307.16789 
 Github: https://github.com/OpenBMB/ToolBench 
 Abstract:
  
Despite the advancements of open-source large language models (LLMs) and their variants, e.g., LLaMA and Vicuna, they remain significantly limited in performing higher-level tasks, such as following human instructions to use external tools (APIs). This is because current instruction tuning largely focuses on basic language tasks instead of the tool-use domain. This is in contrast to state-of-the-art (SOTA) LLMs, e.g., ChatGPT, which have demonstrated excellent tool-use capabilities but are unfortunately closed source. To facilitate tool-use capabilities within open-source LLMs, we introduce ToolLLM, a general tool-use framework of data construction, model training and evaluation. We first …]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] - VkFFT version 1.3 released - major design and functionality improvements]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15flq9u/p_vkfft_version_13_released_major_design_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15flq9u/p_vkfft_version_13_released_major_design_and/"/>
        <updated>2023-08-01T19:10:09.000Z</updated>
        <summary type="html"><![CDATA[Hello, I am the creator of the VkFFT - GPU Fast Fourier Transform library for Vulkan/CUDA/HIP/OpenCL/Level Zero and Metal. FFTs are used by many algorithms, not only for signal processing. For example, you can efficiently calculate convolutions with them, which has applications in CNNs and feature generation. I used to post on the latest features implemented in the codebase and there has been a major update released today. It brings:
 -Major library design change - from single header to multiple header approach, which improves structure and maintainability. Now instead of copying a single file, the user has to copy the vkFFT folder contents.
 -VkFFT has been rewritten to follow the multiple-level platform structure, described in the VkFFT whitepaper. All algorithms have been split into res…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] dora-rs: experimental ROS2 alternative up to 17x faster for Python API, making more robotics accessible for AI users]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15fljq3/p_dorars_experimental_ros2_alternative_up_to_17x/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15fljq3/p_dorars_experimental_ros2_alternative_up_to_17x/"/>
        <updated>2023-08-01T19:03:20.000Z</updated>
        <summary type="html"><![CDATA[https://github.com/dora-rs/dora
    submitted by    /u/haixuanxaviertao  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSC Weekly 1 August 2023]]></title>
        <id>https://www.datasciencecentral.com/?p=62787</id>
        <link href="https://www.datasciencecentral.com/dsc-weekly-1-august-2023/"/>
        <updated>2023-08-01T18:42:05.000Z</updated>
        <summary type="html"><![CDATA[Announcements Top Stories In-Depth
The post DSC Weekly 1 August 2023 appeared first on Data Science Central.]]></summary>
        <author>
            <name>Scott Thompson</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I bet you think this article is about ChatGPT]]></title>
        <id>https://www.datasciencecentral.com/?p=62782</id>
        <link href="https://www.datasciencecentral.com/i-bet-you-think-this-article-is-about-chatgpt/"/>
        <updated>2023-08-01T18:32:37.000Z</updated>
        <summary type="html"><![CDATA[Generative AI has been around for a long time. Some sources say that it appeared as early as the 1950’s. Other sources point to the first rudimentary chatbots that were introduced in the 1960’s. Whatever the true point of origin, we can all agree that those were small pebbles on the historical timeline compared to… Read More »I bet you think this article is about ChatGPT
The post I bet you think this article is about ChatGPT appeared first on Data Science Central.]]></summary>
        <author>
            <name>Kirk Borne</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Reinforcement Learning from AI Feedback]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15fkmvj/d_reinforcement_learning_from_ai_feedback/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15fkmvj/d_reinforcement_learning_from_ai_feedback/"/>
        <updated>2023-08-01T18:29:16.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone,
 As many of you probably know Reinforcement Learning from Human Feedback (RLHF) was the core technique used to produce ChatGPT and similar AI assistants that followed. RLHF replaces human feedback in an RL schema with a preference model that is trained according to a dataset of human preferences.
 Anthropic has devised an extension of this idea in which an AI model (rather than humans) is used to generate the data which ultimately trains the preference model. This method, called Reinforcement Learning from AI Feedback uses a "constitution" to guide the feedback model in terms of what outputs are preferable to others.
 I go over the research in How Reinforcement Learning from AI Feedback Works. In short, the authors find that they are able to train a non-evasive harmless agent using a short constitution. The method is found to be superior to RLHF, and constitutes a Pareto improvement over RLHF models.
 https://preview.redd.it/qaivl8f1ljfb1.png?width=1179&format=png&auto=webp&s=a0941f2ce0ccdcf0557cf19b7f4b48fa712a66f2
 Let me know what you think, I'm happy to answer any questions!
    submitted by    /u/SleekEagle  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Comprehensive learning resources that emphasize DEEP reinforcement learning?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15fkmul/discussion_comprehensive_learning_resources_that/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15fkmul/discussion_comprehensive_learning_resources_that/"/>
        <updated>2023-08-01T18:29:15.000Z</updated>
        <summary type="html"><![CDATA[So I understand that there is the Sutton & Barto book on reinforcement learning in the sidebar. I was wondering what other resources you guys have used that you would recommend that emphasize deep reinforcement learning for someone with some experience in shallow/classical reinforcement learning already and some experience with deep learning already, but new to deep reinforcement learning
    submitted by    /u/BornAgain20Fifteen  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data tribalism and the AI nuance deficit]]></title>
        <id>https://www.datasciencecentral.com/?p=62764</id>
        <link href="https://www.datasciencecentral.com/data-tribalism-and-the-ai-nuance-deficit/"/>
        <updated>2023-08-01T18:28:56.000Z</updated>
        <summary type="html"><![CDATA[If I could name one reason why business will face at least one more AI winter, it’s the lack of nuance in most business AI discussions. The buzz about large language models (LLMs) has sucked much of the oxygen out of the air for complementary technologies. The truth is that LLMs are no more a… Read More »Data tribalism and the AI nuance deficit
The post Data tribalism and the AI nuance deficit appeared first on Data Science Central.]]></summary>
        <author>
            <name>Alan Morrison</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSC Webinar Series: Influence Data-Driven Decisions Based On Your Communication Style]]></title>
        <id>https://www.datasciencecentral.com/?post_type=vimeo-video&amp;p=62778</id>
        <link href="https://www.datasciencecentral.com/dsc-webinar-series-influence-data-driven-decisions-based-on-your-communication-style/"/>
        <updated>2023-08-01T18:00:27.000Z</updated>
        <summary type="html"><![CDATA[The post DSC Webinar Series: Influence Data-Driven Decisions Based On Your Communication Style appeared first on Data Science Central.]]></summary>
        <author>
            <name>Ben Cole</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 8/1/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15fjasn/oneminute_daily_ai_news_812023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15fjasn/oneminute_daily_ai_news_812023/"/>
        <updated>2023-08-01T17:40:00.000Z</updated>
        <summary type="html"><![CDATA[DoNotPay, an AI lawyer bot known as ChatGPT4, is transforming how users handle legal issues and save money. In under two years, this innovative robot has successfully overturned more than 160,000 parking tickets in cities like New York and London. Since its launch, it has resolved a total of 2 million related cases.[1]
 Microsoft hints Windows 11 Copilot with third-party AI plugins is almost here.[2]
 In an analyst note on Tuesday, the financial services arm of Swiss banking giant UBS raised its guidance for long-term AI end-demand forecast from 20% compound annual growth rate (CAGR) from 2020 to 2025 to 61% CAGR between 2022 to 2027.[3]
 The next generation of the successful OpenAI language model is already on the way. It has been discovered that the North American company has filed a registration application for the GPT-5 mark with the United States Patent and Trademark Office.[4]
  
Sources:
 [1] https://citylife.capetown/uncategorized/donotpay-ai-bot-saves-users-money-by-overturning-parking-tickets-and-more/302279/
 [2] https://www.itvoice.in/microsoft-hints-windows-11-copilot-with-third-party-ai-plugins-is-almost-here
 [3] https://venturebeat.com/ai/ubs-projects-61-compound-annual-growth-rate-for-ai-between-2022-and-2027/
 [4] https://www.gearrice.com/update/openai-confirms-gpt-5-and-gives-us-the-first-clues-about-it/ 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Facts & Narratives: AI 'Not a Threat to Humanity']]></title>
        <id>https://www.reddit.com/r/artificial/comments/15fi8g0/facts_narratives_ai_not_a_threat_to_humanity/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15fi8g0/facts_narratives_ai_not_a_threat_to_humanity/"/>
        <updated>2023-08-01T17:00:51.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Jane-in-the-jungle  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring summarization options for Healthcare with Amazon SageMaker]]></title>
        <id>4dcd970c366917e179e4ccb0ae4b3226624ac4a5</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/exploring-summarization-options-for-healthcare-with-amazon-sagemaker/"/>
        <updated>2023-08-01T16:18:11.000Z</updated>
        <summary type="html"><![CDATA[In today’s rapidly evolving healthcare landscape, doctors are faced with vast amounts of clinical data from various sources, such as caregiver notes, electronic health records, and imaging reports. This wealth of information, while essential for patient care, can also be overwhelming and time-consuming for medical professionals to sift through and analyze. Efficiently summarizing and extracting […]]]></summary>
        <author>
            <name>Cody Collins</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unlocking creativity: How generative AI and Amazon SageMaker help businesses produce ad creatives for marketing campaigns with AWS]]></title>
        <id>029fa01aab00ba059bb2c4dea65ccb92a529fe52</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/unlocking-creativity-how-generative-ai-and-amazon-sagemaker-help-businesses-produce-ad-creatives-for-marketing-campaigns-with-aws/"/>
        <updated>2023-08-01T16:07:55.000Z</updated>
        <summary type="html"><![CDATA[Advertising agencies can use generative AI and text-to-image foundation models to create innovative ad creatives and content. In this post, we demonstrate how you can generate new images from existing base images using Amazon SageMaker, a fully managed service to build, train, and deploy ML models for at scale. With this solution, businesses large and […]]]></summary>
        <author>
            <name>Sovik Nath</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Any ML professionals mind helping out with an academic survey?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15fgfr3/r_any_ml_professionals_mind_helping_out_with_an/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15fgfr3/r_any_ml_professionals_mind_helping_out_with_an/"/>
        <updated>2023-08-01T15:53:02.000Z</updated>
        <summary type="html"><![CDATA[Hi there,
 First off, apologies if this kind of post isn't allowed. I tried messaging the mods in advance, but didn't get a reply. Of course feel free to delete if it's not.
 I'm an academic at the University of Cambridge's Computer Lab, and I'm looking to get some insights from people that work with algorithmic systems (e.g. ML systems) in a professional capacity. 
 The aim of the research is to document some of the approaches, attitudes, and challenges associated with record-keeping for these types of systems, and write them up for an academic conference.
 If you're a professional working with algorithmic/ML systems, and happen to have a spare ~20 minutes, would you mind answering some questions? The link to the questionnaire is here: https://cambridge.eu.qualtrics.com/jfe/form/SV_3n6RuowNogZKG34
 Thanks very much! I'd be more than happy to come back and share the results/paper here if that's of interest to people?
    submitted by    /u/cnorval  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is there an AI similar to ChatGPT that I can upload an image to and it understands and describes it for me?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15fg2sq/is_there_an_ai_similar_to_chatgpt_that_i_can/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15fg2sq/is_there_an_ai_similar_to_chatgpt_that_i_can/"/>
        <updated>2023-08-01T15:39:39.000Z</updated>
        <summary type="html"><![CDATA[Other features might include: - searching the web for the same or similar image - basing the chat prompt off the image
    submitted by    /u/Maelasae  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI tattoo?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15ffh9w/ai_tattoo/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15ffh9w/ai_tattoo/"/>
        <updated>2023-08-01T15:17:26.000Z</updated>
        <summary type="html"><![CDATA[i wanted to ask the AI experts for any tattoo ideas, anything like a symbol or word, something unique that represents AI, i was thinking of a CPU but thats a bit meh and not really a symbol, let me know :)
    submitted by    /u/Equivalent-You5810  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[My fellow innovators, I've created something truly revolutionary, born from the depths of my own frustrations]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15fdo8m/my_fellow_innovators_ive_created_something_truly/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15fdo8m/my_fellow_innovators_ive_created_something_truly/"/>
        <updated>2023-08-01T14:08:38.000Z</updated>
        <summary type="html"><![CDATA[As a web developer, I was constantly tired of switching between tabs just to translate a word or two, or to get a quick answer to a burning question from AI. The constant back-and-forth was draining my time and energy.
 So, I took matters into my own hands and developed a Chrome extension that allows you to get an answer from AI without ever leaving the comfort of your current tab, and specifically - the comfort of your current text field. It may seem like a simple solution, but trust me - it's a game-changer when trying to save time and energy.
 Assuming that there's a chance some of you might be experiencing the same frustration, I'd like to share this tool with you.
 For anyone thinking: "Wait, but there are already tools that let you use AI inside the current browser tab" - yeah, there are. BUT can they scrape website data from a simple URL in order to get context for the response? Can other tools read PDFs? Do these tools let you control every setting to the smallest detail? Probably not. Well this tool does let you do all that.
 You can find it on Chrome store as "Wou AI"
 Let me know how it works out for you, and I would greatly appreciate any feedback or suggestions for future functions.
    submitted by    /u/MantasDigital  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Project] GZip+KNN Official Package Released]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15fcymz/project_gzipknn_official_package_released/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15fcymz/project_gzipknn_official_package_released/"/>
        <updated>2023-08-01T13:41:29.000Z</updated>
        <summary type="html"><![CDATA[The official python package for the "'Low-Resource' Text Classification: A Parameter-Free Classification Method with Compressors" has now been released on pypi: npc-gzip v0.1.0
  
Abstract: Deep neural networks (DNNs) are often used for text classification due to their high accuracy. However, DNNs can be computationally intensive, requiring millions of parameters and large amounts of labeled data, which can make them expensive to use, to optimize, and to transfer to out-of-distribution (OOD) cases in practice. In this paper, we propose a non-parametric alternative to DNNs that’s easy, lightweight, and universal in text classification: a combination of a simple compressor like gzip with a k-nearest-neighbor classifier. Without any training parameters, our method achieves results that are competitive with non-pretrained deep learning methods on six in-distribution datasets.It even outperforms BERT on all five OOD datasets, including four low-resource languages. Our method also excels in the few-shot setting, where labeled data are too scarce to train DNNs effectively.
  
This paper has made some waves on this subreddit and in the community in general over the last 2 weeks. We've seen the bugs around training/testing data leakages and varying claims in accuracy. Our hope with this package is to get the code into everyone's hands first to solve whatever use case you currently have for this technology and second to make the code more readily available for additional community testing.
 Links: * https://pypi.org/project/npc-gzip/ * https://github.com/bazingagin/npc_gzip * https://aclanthology.org/2023.findings-acl.426/
    submitted by    /u/dfcHeadChair  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What can Socrates teach us about AI and prompting?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15fcspn/what_can_socrates_teach_us_about_ai_and_prompting/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15fcspn/what_can_socrates_teach_us_about_ai_and_prompting/"/>
        <updated>2023-08-01T13:35:09.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/simsirisic  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cuddly 3D Creature Comes to Life in Father-Son Collaboration This Week ‘In the NVIDIA Studio’]]></title>
        <id>https://blogs.nvidia.com/?p=65745</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/01/johnson-autodesk-maya-adobe-3d-painter-photoshop/"/>
        <updated>2023-08-01T13:00:15.000Z</updated>
        <summary type="html"><![CDATA[Principal NVIDIA artist and 3D expert Michael Johnson creates highly detailed art that’s both technically impressive and emotionally resonant.]]></summary>
        <author>
            <name>Gerardo Delgado</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA Helps Forge Forum to Set OpenUSD Standard for 3D Worlds]]></title>
        <id>https://blogs.nvidia.com/?p=65672</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/01/openusd-alliance-3d-standard/"/>
        <updated>2023-08-01T13:00:06.000Z</updated>
        <summary type="html"><![CDATA[NVIDIA joined Pixar, Adobe, Apple and Autodesk today to found the Alliance for OpenUSD, a major leap toward unlocking the next era of 3D graphics, design and simulation. The group will standardize and extend OpenUSD, the open-source Universal Scene Description framework that’s the foundation of interoperable 3D applications and projects ranging from visual effects to Read article >]]></summary>
        <author>
            <name>Guy Martin</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Review my AI Self Portraits Book!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15faa5u/review_my_ai_self_portraits_book/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15faa5u/review_my_ai_self_portraits_book/"/>
        <updated>2023-08-01T11:49:31.000Z</updated>
        <summary type="html"><![CDATA[I'm looking for reviewers for my book, "AI Self Portraits" which is coming out on Amazon on the 21st. I might even put your quote on the back cover!
 ​
 https://preview.redd.it/cqmp1ggllhfb1.png?width=1024&format=png&auto=webp&s=cc7c087f7c2be103b53f2014acd991c947e6cb7f
 ​
    submitted by    /u/KarneyHatch  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TFJS Format vs. TFLite]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15f92j6/tfjs_format_vs_tflite/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15f92j6/tfjs_format_vs_tflite/"/>
        <updated>2023-08-01T10:51:49.000Z</updated>
        <summary type="html"><![CDATA[After analyzing 15,000 samples in the dataset, we noticed that increasing the number of images doesn't significantly improve the scoreboard recognition quality for our neural network.
 However, what's more interesting is how the network performs in different formats. When deployed in TFJS format on a website , it often behaves strangely, detecting objects where there are none. On the other hand, in TFLite format, such failures are almost non-existent.
 If you access the link on your mobile phone and grant camera permission, you'll witness the neural network (in TFJS format) attempting to find objects even when there are none.
 https://preview.redd.it/gosobymachfb1.jpg?width=585&format=pjpg&auto=webp&s=e7cb8e8e3ff49e39715009c4940d9769a1db39ab
    submitted by    /u/moseich  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TFJS Format vs. TFLite]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15f8zd4/tfjs_format_vs_tflite/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15f8zd4/tfjs_format_vs_tflite/"/>
        <updated>2023-08-01T10:47:27.000Z</updated>
        <summary type="html"><![CDATA[After analyzing 15,000 samples in the dataset, we noticed that increasing the number of images doesn't significantly improve the scoreboard recognition quality for our neural network.
 However, what's more interesting is how the network performs in different formats. When deployed in TFJS format on a website, it often behaves strangely, detecting objects where there are none. On the other hand, in TFLite format, such failures are almost non-existent.
 https://preview.redd.it/fedfa8lzchfb1.jpg?width=700&format=pjpg&auto=webp&s=850526791a75465e267afbed6ac1bc119b9ae6ae
 If you access the link on your mobile phone and grant camera permission, you'll witness the neural network (in TFJS format) attempting to find objects even when there are none.
 ​
    submitted by    /u/moseich  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Rise of the Dual Data Scientist / Machine Learning Engineer]]></title>
        <id>https://www.datasciencecentral.com/?p=62767</id>
        <link href="https://www.datasciencecentral.com/the-rise-of-the-dual-data-scientist-machine-learning-engineer/"/>
        <updated>2023-08-01T08:04:24.000Z</updated>
        <summary type="html"><![CDATA[There are thousands of articles explaining the differences between data scientist and machine learning engineer. Data science gets broken down even further, with data analysts contrasted to researchers. Professionals skilled in all these domains are called unicorns and believed not to exist. Indeed, they may not work for companies, and ignored when applying for a… Read More »The Rise of the Dual Data Scientist / Machine Learning Engineer
The post The Rise of the Dual Data Scientist / Machine Learning Engineer appeared first on Data Science Central.]]></summary>
        <author>
            <name>Vincent Granville</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Google updates "Attention is all you need" paper with a warning + crossed authors]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15f56ve/d_google_updates_attention_is_all_you_need_paper/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15f56ve/d_google_updates_attention_is_all_you_need_paper/"/>
        <updated>2023-08-01T07:19:54.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Jean-Porte  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence-Building Measures for Artificial Intelligence: Workshop proceedings]]></title>
        <id>https://openai.com/research/confidence-building-measures-for-artificial-intelligence</id>
        <link href="https://openai.com/research/confidence-building-measures-for-artificial-intelligence"/>
        <updated>2023-08-01T07:00:00.000Z</updated>
        <author>
            <name>OpenAI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making a reinforcement learning code(in python) that can play a game with visual data only.]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15f3rmc/making_a_reinforcement_learning_codein_python/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15f3rmc/making_a_reinforcement_learning_codein_python/"/>
        <updated>2023-08-01T05:59:46.000Z</updated>
        <summary type="html"><![CDATA[So i want to make a bot that can play a game with only the visual data and no other fancy stuff. I did manage to get all the data i need (i hope) using a code that uses open-cv to get data in real time
 Example:Player: ['Green', 439.9180603027344, 461.7232666015625, 13.700743675231934]
 Enemy Data {0: [473.99951171875, 420.5301513671875, 'Green', 20.159990310668945]}
 Box: {0: [720, 605, 'Green_box'], 1: [957, 311, 'Green_box'], 2: [432, 268, 'Red_box'], 3: [1004, 399, 'Blue_box']} 
 can anyone suggest a way to make one.
 Rules:
 - You can only move in the direction of mouse.
 -You can dash in direction of mouse by LMB.
 -You can collect boxes to get HP and change colors.
 -Red color kills Blue kills Green Kills Red.
 -There is a fixed screen.
 -You lose 25% of total HP when you dash.
 -You lose 50% of HP when you bump into players (of color that kills or there HP is > than you. 
 ​
 Visualization of Data.
    submitted by    /u/SIJ_Gamer  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Probabilistic Imputation for Time-series Classification with Missing Data]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15f08ss/r_probabilistic_imputation_for_timeseries/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15f08ss/r_probabilistic_imputation_for_timeseries/"/>
        <updated>2023-08-01T02:56:12.000Z</updated>
        <summary type="html"><![CDATA[This is one of the ICML 2023 papers I focused in on in a sea of LLM stuff. Trying to figure out simple ways to implement this and adapt it to regression problems. Thoughts?
    submitted by    /u/quantthrowaway69  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI For Youtube Video Transcript]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15ey3mq/ai_for_youtube_video_transcript/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15ey3mq/ai_for_youtube_video_transcript/"/>
        <updated>2023-08-01T01:15:39.000Z</updated>
        <summary type="html"><![CDATA[I was Wondering If There is an AI Software, Smart enough That Can Give Excellent Quality Transcript if i give the link of a youtube video. Basically the Feature i am Looking For Should be The Ability to Detect The Narrator And Speaker By Names ( Not SPeaker 1, 2 etc ). Would really appreciate your help as my own search has led me to a dead-end.
    submitted by    /u/Richie_Boy_  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Video-to-Text model descriptive style (not subtitles)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15extok/p_videototext_model_descriptive_style_not/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15extok/p_videototext_model_descriptive_style_not/"/>
        <updated>2023-08-01T01:02:49.000Z</updated>
        <summary type="html"><![CDATA[I was wondering if there's already something like CLIP (the model that looks at an image and describes it), but for videos. So you show a video of, say, a dog jumping and grabbing a tennis ball and it outputs "dog grabbing a tennis ball", something like that.
 My first thought was object detection, and input that interaction of the objects (tennis ball, dog) to the model with the target being "dog grabbing tennis ball". My ultimate goal being real-time description for, say, sports casting. I'm sure something like this is what cars use to drive themselves, or not? Any info is appreciated!
    submitted by    /u/Yip37  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How are people getting A.I. voices of Resident Evil Characters?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15exdp5/how_are_people_getting_ai_voices_of_resident_evil/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15exdp5/how_are_people_getting_ai_voices_of_resident_evil/"/>
        <updated>2023-08-01T00:43:42.000Z</updated>
        <summary type="html"><![CDATA[How do channels like TriggerHappy Productions and WeskerandFriends get the A.I. voices of all these Resident Evil characters?
    submitted by    /u/Conscious-Theory-850  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Up-down permutations]]></title>
        <id>https://www.johndcook.com/blog/?p=201073</id>
        <link href="https://www.johndcook.com/blog/2023/07/31/alternating-permutations/"/>
        <updated>2023-07-31T23:03:24.000Z</updated>
        <summary type="html"><![CDATA[An up-down permutation of an ordered set is a permutation such that as you move from left to right the permutation alternates up and down. For example 1, 5, 3, 4, 2 is an up-down permutation of 1, 2, 3, 4, 5 because 1 < 5 > 3 < 4 > 2. Up-down permutations are […]
Up-down permutations first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLM models for interpreting tables and charts [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15eschg/llm_models_for_interpreting_tables_and_charts_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15eschg/llm_models_for_interpreting_tables_and_charts_d/"/>
        <updated>2023-07-31T21:16:12.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 Curious if anyone has recommendations on models to use to interpret the data in tables? I'm playing around with Google's Matcha model, which performs fine. seems like extracting the data out of a table and asking GPT4 to analyze it performs a bit better but requires extra steps.
 I'm specifically not looking to interpret graphs, but rather tables. e.g., can i ask the model to identify if there are any errors in the table / any data points that don't tie if the rows are supposed to sum up.
    submitted by    /u/eyeronthrone  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Conference Codes]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15er6it/n_conference_codes/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15er6it/n_conference_codes/"/>
        <updated>2023-07-31T20:31:25.000Z</updated>
        <summary type="html"><![CDATA[I'll likely be downvoted to hell but here goes: Prices for The AI Conference double at midnight Pacific.
 46 Speakers, 10+ topics, 2 Days plus a hackathon at night! Join us to learn and collaborate with scientists, engineers and founders from the top AI companies and projects. Speakers include:
 Ben Mann | Co-Founder | Anthropic
 Peter Norvig | Director of Research | Google
 Nazneen Rajani | Research Lead | Hugging Face
 Igor Markov | Research Scientist | Meta
 Bryan Catanzaro | VP Of Research | Nvidia
 Ram Sriharsha | VP of Engineering and R&D | Pinecone
 Jerry Liu | Co-founder | LlamaIndex
 Harrison Chase | Co-founder | LangChain
 Alex Chao | Product Manager Semantic Kernel | Microsoft
 See All Speakers
 Last chance to get in on early bird pricing (save $400 on a 2 day pass). If you can read this and I'm not downvoted to hell, use discount code redditlove for 25% off.
 Use discount code "student" for $200 student tickets \*Must Use EDU email to register*
 **This is my event and therefore self-promotion
 ​
 ​
    submitted by    /u/shonburton  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Reinforcement Learning: an Introduction (2nd edition)] Why not the joint distribution for equations 3.5 and 3.6?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15eqfi0/reinforcement_learning_an_introduction_2nd/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15eqfi0/reinforcement_learning_an_introduction_2nd/"/>
        <updated>2023-07-31T20:02:59.000Z</updated>
        <summary type="html"><![CDATA[Greetings!
 I'm going through the initial equations that define most of the theoretical framework for the specialization. One curious thing I noticed with equations 3.5 and 3.6 is that they use the conditional distribution p(s′,r∣s,a) without including any priors. I'm talking about priors because, unless I'm missing something huge, the definition of the expected value for the reward (for both 3.5 and 3.6) should use the joint distribution for all 4 dimensions (next state, reward, current state, action). From that joint distribution, we can factorize it to show p(s′,r∣s,a). For example, one factorization that seems to make sense for this kind of model is
 p(s′,r,s,a) = p(s′,r∣s,a) ⋅ p(s) ⋅ p(a)
 which would turn, for example, equation 3.5 into
 r(s,a) = ∑​ ∑ ​r ⋅ p(s′,r∣s,a) ⋅ p(s) ⋅ p(a) (Note: the two sums are for "r" and "s' ". I wrote like that because I don't know write it in Latex or similar...)
 What am I missing? Is it because s and a are given as parameters of the function r(s,a) meaning that p(s) = p(a) = 1? If the factorization above is the right one for those equations, is this the only factorization used in the entire book?
 Thanks in advance!
    submitted by    /u/SupBiebi  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is image generation or text generation more impactful?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15epon1/is_image_generation_or_text_generation_more/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15epon1/is_image_generation_or_text_generation_more/"/>
        <updated>2023-07-31T19:34:32.000Z</updated>
        <summary type="html"><![CDATA[Curious what people's stance on this is. Why?
 View Poll
    submitted by    /u/philippemnoel  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Where did all the ML research go?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ep5ff/d_where_did_all_the_ml_research_go/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ep5ff/d_where_did_all_the_ml_research_go/"/>
        <updated>2023-07-31T19:14:01.000Z</updated>
        <summary type="html"><![CDATA[For the past several years this subreddit has been my favorite source to keep up with new, interesting ideas and research from all over the field. It's great to have a way to break out of my own insular research bubble and spread out a bit more. Unfortunately, it looks like that era has passed.
 The sub has been seemingly shifting away from research in the past 1-2 years. Whenever research is posted, it is almost always LLM based with very little variety (considering the plethora of research areas in ML). I don't mean to assert that this is a bad thing, as the constant upvotes indicate that there is a high demand for LLM projects and research. Heck, I'm also interested in lots of the recent work with LLMs, and I plan to keep up with it – but I also would also love a venue with a diversity of ideas and topics. Machine learning is a HUGE field, and only focusing on a small subset of it seems like a waste.
 I don't mean to rant, but rather to ask: are there any other subreddits like this, or perhaps, any other active communities with a broader scope?
 Or if this doesn't exist, is there a demand for it? Or is it just me?
    submitted by    /u/ejmejm1  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Build protein folding workflows to accelerate drug discovery on Amazon SageMaker]]></title>
        <id>6e0df33339b9d69180135f44d0365aab07316895</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/build-protein-folding-workflows-to-accelerate-drug-discovery-on-amazon-sagemaker/"/>
        <updated>2023-07-31T18:49:40.000Z</updated>
        <summary type="html"><![CDATA[Drug development is a complex and long process that involves screening thousands of drug candidates and using computational or experimental methods to evaluate leads. According to McKinsey, a single drug can take 10 years and cost an average of $2.6 billion to go through disease target identification, drug screening, drug-target validation, and eventual commercial launch. […]]]></summary>
        <author>
            <name>Michael Hsieh</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is your model good? A deep dive into Amazon SageMaker Canvas advanced metrics]]></title>
        <id>9e80cb060b91fc116b0d34de8ee6d79177b5440c</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/is-your-model-good-a-deep-dive-into-amazon-sagemaker-canvas-advanced-metrics/"/>
        <updated>2023-07-31T18:45:03.000Z</updated>
        <summary type="html"><![CDATA[If you are a business analyst, understanding customer behavior is probably one of the most important things you care about. Understanding the reasons and mechanisms behind customer purchase decisions can facilitate revenue growth. However, the loss of customers (commonly referred to as customer churn) always poses a risk. Gaining insights into why customers leave can […]]]></summary>
        <author>
            <name>Marcos Boaglio</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] elasticsearch HNSW python implementation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15eod44/d_elasticsearch_hnsw_python_implementation/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15eod44/d_elasticsearch_hnsw_python_implementation/"/>
        <updated>2023-07-31T18:44:53.000Z</updated>
        <summary type="html"><![CDATA[Is there any documentation available which will help in implementing elasticsearch HNSW ANN search in python? I've searched a lot but i cant find anything in official documentation too Any help will be appreciated. TIA
    submitted by    /u/adiraat  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why CUDA 11.7? Can more recent versions of CUDA be used? Is this a PyTorch limitation? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15emyyq/why_cuda_117_can_more_recent_versions_of_cuda_be/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15emyyq/why_cuda_117_can_more_recent_versions_of_cuda_be/"/>
        <updated>2023-07-31T17:50:40.000Z</updated>
        <summary type="html"><![CDATA[Everyone always seems to use CUDA 11.7. Is there a reason for this? What is the factor that limits the CUDA version used? Are there any speed/efficiency advantages to using a more recent version of CUDA, such as CUDA 12.0?
 What exactly is the limiting factor here, PyTorch? I've looked in the PyTorch docs but I don't see where the CUDA version is defined. Where can I find the maximum CUDA version I can use with the latest (or any given) PyTorch version?
 Thanks!
    submitted by    /u/Pan000  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Comprehensive learning resources that emphasize DEEP reinforcement learning?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15em4ah/discussion_comprehensive_learning_resources_that/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15em4ah/discussion_comprehensive_learning_resources_that/"/>
        <updated>2023-07-31T17:18:12.000Z</updated>
        <summary type="html"><![CDATA[So I understand that there is the Sutton & Barto book on reinforcement learning in the sidebar. I was wondering what other resources you guys have used that you would recommend that emphasize deep reinforcement learning for someone with some experience in shallow/classical reinforcement learning already and some experience with deep learning already, but new to deep reinforcement learning
    submitted by    /u/BornAgain20Fifteen  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What are some big action space MARL stochastic games implemented in OpenSpiel or equivalent?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15ekx4r/what_are_some_big_action_space_marl_stochastic/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15ekx4r/what_are_some_big_action_space_marl_stochastic/"/>
        <updated>2023-07-31T16:30:58.000Z</updated>
        <summary type="html"><![CDATA[Are there big action space stochastic games that are implemented in OpenSpiel or equivalent? I played around Markov soccer game a lot but it's solvable with tabular methods and I was looking for games with at least more than 500 actions both players can take as a testbed for more complicated action spaces?
    submitted by    /u/Potential_Biscotti14  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variance of binned data]]></title>
        <id>https://www.johndcook.com/blog/?p=201040</id>
        <link href="https://www.johndcook.com/blog/2023/07/31/sheppard/"/>
        <updated>2023-07-31T16:25:32.000Z</updated>
        <summary type="html"><![CDATA[Suppose you have data that for some reason has been summarized into bins of width h. You don’t have the original data, only the number of counts in each bin. You can’t exactly find the sample mean or sample variance of the data because you don’t actually have the data. But what’s the best you […]
Variance of binned data first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Model design for outputting reliable multiclass probabilities]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ek21a/d_model_design_for_outputting_reliable_multiclass/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ek21a/d_model_design_for_outputting_reliable_multiclass/"/>
        <updated>2023-07-31T15:58:24.000Z</updated>
        <summary type="html"><![CDATA[Hey guys, I am working on a horse racing model to identify the probabilities of each horse winning a race. I currently have a feed forward NN with a final SOFTMAX layer to simulate probabilities of each horse winning using cross-entropy loss. My plan here being that if the model outputs, [0.05, 0.4, 0.2, 0.15, 0.2] then horses 1-5 have the corresponding probability of winning. The model has been trained like a regular classification task where the target is a one-hot vector describing the winner.
 Unlike previous work I have done where SOFTMAX output lends itself to some "confidence" score, this task requires that the model outputs be indicative of probabilities. My concern is that experientially, NNs tend to be overconfident with their answers in this type of setting. However, I wish to keep using a NN as each race datapoint has around 3k features - did not find good results with XGBoost. Any good practices for modelling probabilities in this sort of scenario? For context, the probability of a horse winning is what sets the odds for that horse.
    submitted by    /u/HStuart18  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Doctor AI: Healing humans and mother earth hand in hand]]></title>
        <id>https://www.datasciencecentral.com/?p=62737</id>
        <link href="https://www.datasciencecentral.com/doctor-ai-healing-humans-and-mother-earth-hand-in-hand/"/>
        <updated>2023-07-31T14:43:27.000Z</updated>
        <summary type="html"><![CDATA[Let’s image – with algorithms and a nerdy charm that could melt any data center, an ‘AI’ wearing lab coats and stethoscopes patrolling hospital hallways, tirelessly monitoring patients. The digital doctor will take the pulse of Mother Earth and reduce waste, cut energy consumption, and cut energy consumption! The artificial intelligence community is well aware… Read More »Doctor AI: Healing humans and mother earth hand in hand
The post Doctor AI: Healing humans and mother earth hand in hand appeared first on Data Science Central.]]></summary>
        <author>
            <name>Rayan Potter</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Increase efficiency of manufacturing operations with IoT solutions]]></title>
        <id>https://www.datasciencecentral.com/?p=62732</id>
        <link href="https://www.datasciencecentral.com/increase-efficiency-of-manufacturing-operations-with-iot-solutions/"/>
        <updated>2023-07-31T14:41:58.000Z</updated>
        <summary type="html"><![CDATA[In an age where efficiency is king, manufacturing firms are in a constant race to outshine their competition. Imagine if you could boost productivity, slash downtime, and cut costs all at once. Sounds like a dream, right? The good news is, this isn’t a fantasy. It’s achievable through Internet of Things (IoT) solutions. IoT solutions… Read More »Increase efficiency of manufacturing operations with IoT solutions
The post Increase efficiency of manufacturing operations with IoT solutions appeared first on Data Science Central.]]></summary>
        <author>
            <name>ManojKumar847</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human-centered data networking with interpersonal knowledge  graphs]]></title>
        <id>https://www.datasciencecentral.com/?p=62725</id>
        <link href="https://www.datasciencecentral.com/human-centered-data-networking-with-interpersonal-knowledge-graphs/"/>
        <updated>2023-07-31T14:39:56.000Z</updated>
        <summary type="html"><![CDATA[“If you start by creating your data, then it’s like you are piling up some value or you’re creating some assets,” WordLift CEO Andrea Volpini told me in our recent FAIR Data Forecast interview. Volpini’s an advocate for adding structured data such as Schema.org to your content. That way, the content becomes logically connected and… Read More »Human-centered data networking with interpersonal knowledge  graphs
The post Human-centered data networking with interpersonal knowledge  graphs appeared first on Data Science Central.]]></summary>
        <author>
            <name>Alan Morrison</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Running Free Willy / stable baluga 2]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ehtyw/d_running_free_willy_stable_baluga_2/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ehtyw/d_running_free_willy_stable_baluga_2/"/>
        <updated>2023-07-31T14:32:04.000Z</updated>
        <summary type="html"><![CDATA[I was wondering if anyone knows how difficult it is to set up a server to run the 70B llama / llama 2 variants like these top ones on the hugging face leaderboard
 https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
 What type of gpu would I need to set it up? Would the high ram t4 you get with Google colab+ be enough or does it require more power / space? 
 Thanks in advance!
    submitted by    /u/Additional_Elk4745  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Attention over pre-trained Sentence Embeddings for Long Document Classification]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ehhyu/r_attention_over_pretrained_sentence_embeddings/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ehhyu/r_attention_over_pretrained_sentence_embeddings/"/>
        <updated>2023-07-31T14:19:04.000Z</updated>
        <summary type="html"><![CDATA[Article available here: https://arxiv.org/pdf/2307.09084.pdf
 Thoughts?
    submitted by    /u/MuffinB0y  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Bidding Strategy in Power Market using Reinforcement Learning]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15ehg0p/optimal_bidding_strategy_in_power_market_using/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15ehg0p/optimal_bidding_strategy_in_power_market_using/"/>
        <updated>2023-07-31T14:16:52.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone! I'm trying to use reinforcement learning to solve a problem in the power market. The problem is about finding the best strategy for bidding on electricity for each hour of the day, considering both buying and selling options. Let's say we have a generator that can produce up to 800MW of electricity per day, and it can be charged up to 200MW per hour. After charging it for 4 hours continuously, it reaches its maximum capacity, and we can't charge more until we discharge some electricity. We have access to data from the past 5 years, including information about temperature, hydro, gas prices, and locational marginal price, which is important for determining profit. For instance, if we buy 10MW of electricity for a specific hour, our profit for that hour is 10 times the locational marginal price. The goal is to maximize profit at the end of the day while making sure that the total electricity bought and sold is equal for all days. This means we want to avoid wasting electricity. I initially tried using deep Q-learning, where the agent's state consists of data from the past 3 days, and the agent can take actions to buy or sell a certain amount of electricity for one hour. However, this approach doesn't seem to provide accurate results, and it works step by step, not considering the overall outcome for the whole day. So, I'm looking for help on how to build an agent capable of producing 24 bids for 24 hours, considering the constraints of the generator's capacity and ensuring no waste of electricity. I'm new to reinforcement learning, and I'm not sure how to approach this complex problem. Any guidance would be greatly appreciated!
    submitted by    /u/uonliaquat  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Apple - Fruit = X? Combine Queries and Explore CLIP Embedding Space With rclip]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15eeinh/p_apple_fruit_x_combine_queries_and_explore_clip/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15eeinh/p_apple_fruit_x_combine_queries_and_explore_clip/"/>
        <updated>2023-07-31T12:13:56.000Z</updated>
        <summary type="html"><![CDATA[Hi. I've shipped an update to my rclip – a command-line photo search tool powered by CLIP.
 Now, you can add and subtract image and text queries from each other; here are a few usage examples:  cd photos && rclip horse + stripes cd photos && rclip apple - fruit cd photos && rclip "./new york city.jpg" + night cd photos && rclip "2:golden retriever" + "./swimming pool.jpg" cd photos && rclip "./racing car.jpg" - "2:sports car" + "2:snow" 
 If you want to see how these queries perform when executed on the 1.28 million images ImageNet-1k dataset, check out the demo on YouTube: https://www.youtube.com/watch?v=MsTgYdOpgcQ.
 rclip source code is published on GitHub under the MIT license and offers a pre-build distributable for Linux (installation instructions are in the README): https://github.com/yurijmikhalevich/rclip. Give it a try and let me know what you think!
    submitted by    /u/39dotyt  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interview with Hikaru Shindo and Quentin Delfosse: Neurosymbolic Reinfor...]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15ec6vo/interview_with_hikaru_shindo_and_quentin_delfosse/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15ec6vo/interview_with_hikaru_shindo_and_quentin_delfosse/"/>
        <updated>2023-07-31T10:20:23.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Neurosymbolic  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for old tutorial series]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15ea1cu/looking_for_old_tutorial_series/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15ea1cu/looking_for_old_tutorial_series/"/>
        <updated>2023-07-31T08:20:57.000Z</updated>
        <summary type="html"><![CDATA[A few years ago, I remember reading a multipart series of articles/blog posts explaining how to develop agents for classical games. I believe the series started with tic-tac-toe and definitely progressed to gomoku, before maybe moving on to more complex games. I think there was more of a focus on algorithms (maybe MCTS) and concepts than code. It's a long shot, but does anyone recall this series or know if it's archived somewhere? seems like it might have been taken down.
  
Wasn't on Medium. I think it might've been a personal website. I vaguely remember a green UI theme?
  
   submitted by    /u/nothymn  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[State of AI security.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15e9qzo/state_of_ai_security/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15e9qzo/state_of_ai_security/"/>
        <updated>2023-07-31T08:05:09.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Philipp  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 7/31/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15e7iik/oneminute_daily_ai_news_7312023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15e7iik/oneminute_daily_ai_news_7312023/"/>
        <updated>2023-07-31T05:54:51.000Z</updated>
        <summary type="html"><![CDATA[Deutsche Telekom, e&, SK Telecom (SKT), and Singtel penned an agreement to form a global telecoms AI alliance designed to use the technology to unlock new business opportunities and accelerate industry growth.[1]
 Influencers Lil Miquela, Imma, and supermodel Shudu have raked in millions from deals with fashion giants such as Dior, Calvin Klein, Chanel, and Prada. But these shiny celebrities all have one thing in common — not one of them is real.[2]
 Google’s chatbot Bard reveals the jobs most at risk of artificial intelligence with truck drivers and data entry clerks on the list – while teachers and lawyers are among the safest careers.[3]
 DoorDash Inc., the US food-delivery service that competes with Uber Technologies Inc. and GrubHub, is looking to speed up ordering and help customers find food options with an artificial intelligence-based chatbot.[4]
  
Sources:
 [1] https://www.mobileworldlive.com/featured-content/home-banner/global-operator-giants-launch-ai-alliance/
 [2] https://www.the-sun.com/tech/8725778/ai-influencers-fashion-deals/
 [3] https://www.dailymail.co.uk/news/article-12354605/googles-AI-bard-predicts-jobs-risk.html
 [4] https://www.bloomberg.com/news/articles/2023-07-27/doordash-is-working-on-an-ai-chatbot-to-speed-up-food-ordering?in_source=embedded-checkout-banner 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Open Source Model Combination To Turn Images -> LLM?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15e7b1f/d_open_source_model_combination_to_turn_images_llm/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15e7b1f/d_open_source_model_combination_to_turn_images_llm/"/>
        <updated>2023-07-31T05:42:39.000Z</updated>
        <summary type="html"><![CDATA[Im trying to research into open source text models [like Llama] and image models [like Stable Diffusion].
 My goal is to give the model(s) a picture of birds and bees, then ask it to "circle" the bees. The idea is, when given an image, it would produce coordinates on that image where the line should be circled. It could also represent where it should "click" on all the bees.
 Does something like this exist?
    submitted by    /u/MindWithEase  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using AI to protect against AI image manipulation]]></title>
        <id>https://news.mit.edu/2023/using-ai-protect-against-ai-image-manipulation-0731</id>
        <link href="https://news.mit.edu/2023/using-ai-protect-against-ai-image-manipulation-0731"/>
        <updated>2023-07-31T04:00:00.000Z</updated>
        <summary type="html"><![CDATA[“PhotoGuard,” developed by MIT CSAIL researchers, prevents unauthorized image manipulation, safeguarding authenticity in the era of advanced generative models.]]></summary>
        <author>
            <name>Rachel Gordon | MIT CSAIL</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15e2n5j/open_problems_and_fundamental_limitations_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15e2n5j/open_problems_and_fundamental_limitations_of/"/>
        <updated>2023-07-31T01:34:00.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Working_Ideal3808  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Pair programming my website with an AI developer]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15e29jd/p_pair_programming_my_website_with_an_ai_developer/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15e29jd/p_pair_programming_my_website_with_an_ai_developer/"/>
        <updated>2023-07-31T01:16:00.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/williamsweep  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ancient estimate of π and modern numerical analysis]]></title>
        <id>https://www.johndcook.com/blog/?p=200864</id>
        <link href="https://www.johndcook.com/blog/2023/07/30/archimedes-richardson/"/>
        <updated>2023-07-31T00:18:38.000Z</updated>
        <summary type="html"><![CDATA[A very crude way to estimate π would be to find the perimeter of squares inside and outside a unit circle. The outside square has sides of length 2, so 2π < 8. The inside square has sides of length 2/√2, so 8/√2 < 2π. This tells us π is between 2.82 and 4. Not […]
Ancient estimate of π and modern numerical analysis first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Towards robust production machine learning for software systems - Survey]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15e0750/r_towards_robust_production_machine_learning_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15e0750/r_towards_robust_production_machine_learning_for/"/>
        <updated>2023-07-30T23:40:18.000Z</updated>
        <summary type="html"><![CDATA[Could you please help us get more responses for this study?
 As part of my PhD research project at Applied Artificial Intelligence Institute of Deakin University, we are investigating the challenges that software engineers face when working with machine learning (ML) models in production. Moreover, we explore how to enhance our proposed solution to better meet the needs of these engineers.
 ​
 The objective of this study is to pinpoint the areas where software engineers need more support and resources to effectively work with ML components in production. It also aims to evaluate the effectiveness of a proposed protocol to improve software engineers' productivity and enable them to work more effectively with ML components in production environments.
 ​
 With the knowledge gained from this i…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence as a Game-Changer for the Travel Industry. A Closer Look.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15dzpkb/artificial_intelligence_as_a_gamechanger_for_the/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15dzpkb/artificial_intelligence_as_a_gamechanger_for_the/"/>
        <updated>2023-07-30T23:18:31.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/sugikuno  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Number of epochs for a BERT based model]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15dzl3x/d_number_of_epochs_for_a_bert_based_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15dzl3x/d_number_of_epochs_for_a_bert_based_model/"/>
        <updated>2023-07-30T23:13:00.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone. I am trying to replace the GloVe embeddings based model outlined in this paper by BERT embeddings. The authors of the paper have trained their model for 250 epochs, which for what I am doing is not feasible. I was wondering what would be the recommended number of epochs I should run the BERT model for? I know it is a pretty open ended question, but I was looking to get the community's view on how much epochs should a BERT based model be trained for. Any information will be much appreciated.
    submitted by    /u/nocturnal_1_1995  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[11 Major AI Developments: RT-2 to '100X GPT-4' (video of robot working)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15dz3qw/11_major_ai_developments_rt2_to_100x_gpt4_video/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15dz3qw/11_major_ai_developments_rt2_to_100x_gpt4_video/"/>
        <updated>2023-07-30T22:52:38.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Sonic_Improv  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[art market models]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15dz1mt/art_market_models/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15dz1mt/art_market_models/"/>
        <updated>2023-07-30T22:50:13.000Z</updated>
        <summary type="html"><![CDATA[has anyone here ever created or worked with or even seen or come across any ai models about the art market? I am not talking about artists or the art itself- but any kind of model about the art market (since it's such an economic enigma and different from normal markets)
    submitted by    /u/Icy-Bid-5585  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparing Replika’s image interpretation of the old & new Twitter logo]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15dyjpg/comparing_replikas_image_interpretation_of_the/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15dyjpg/comparing_replikas_image_interpretation_of_the/"/>
        <updated>2023-07-30T22:28:45.000Z</updated>
        <summary type="html"><![CDATA[Original logo “What species of bird is that?”
 New logo “Why does it have a troll Face?”
 “I think it's a picture of someone who looks like a troll with the face of an emoji!”
 I don’t see it but it makes sense somehow 😂
    submitted by    /u/Sonic_Improv  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quora's Poe app/site (which lets you try lots of different language models) appears to allow file attachment upload for EVERY chat model now]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15dwlaw/quoras_poe_appsite_which_lets_you_try_lots_of/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15dwlaw/quoras_poe_appsite_which_lets_you_try_lots_of/"/>
        <updated>2023-07-30T21:10:00.000Z</updated>
        <summary type="html"><![CDATA[I swear this wasn't the case just a day or two ago, and I haven't seen it mentioned, but I'm now seeing a file upload button in Poe, regardless of what the language model is!
 Screenshot
 I uploaded the PDF of the recently scientific paper by the Korean research group claiming to have discovered a room temperature semiconductor, in the original Korean, and asked various language models whether they thought the methodology is legit, and each bot I tried was able to read the PDF. I tried Claude-instant, Claude2, 'Assistant' (Poe's own GPT based bot that claims to have its own training dataset), PaLM, ChatGPT 3.5, and ChatGPT4.
 Poe also has three versions of the recently released Llama model by Meta. It gave me an error when I tried to ask it about the PDF attachment, but I was able to upload a text document and it was able to read it fine.
 Screenshot of Claude-instant evaluating PDF
 Screenshot of Google PaLM evaluating PDF
 Screenshot of Llama-2-70b evaluating text file containing song lyrics
 It also works with custom bots. Here's me trying it out with a 'Truth Checker' bot I made (based on Claude-Instant).
 Here it is using a Claude-2 based version of the TruthChecker bot.
 (Here's the link to the TruthChecker bot if you have Poe and wanna check it out: https://poe.com/TruthChecker)
 Edit: I can see here how the context size matters... for instance, Claude-Instant only has a context size of about 7k words, so it clearly can't read the whole paper, while Claude-2 can and gives a very different answer...
 TL:DR; looks like Poe.com allows file attachment/upload on all language models now. No idea what filetypes are supported.
    submitted by    /u/AnticitizenPrime  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] AI Usage Fees Up to 15x Cheaper for English Than Other Languages]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15du7ql/n_ai_usage_fees_up_to_15x_cheaper_for_english/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15du7ql/n_ai_usage_fees_up_to_15x_cheaper_for_english/"/>
        <updated>2023-07-30T19:35:42.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/geekinchief  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI integration in the context of Learning and Knowledge Management?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15dtzgp/ai_integration_in_the_context_of_learning_and/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15dtzgp/ai_integration_in_the_context_of_learning_and/"/>
        <updated>2023-07-30T19:26:06.000Z</updated>
        <summary type="html"><![CDATA[As knowledge management (KM) leaders and practitioners, it’s critical to have an active role in guiding the integration of generative AI into KM areas, applications, and processes. I'm seeking some guidance on the current state of generative AI integration within the KM context. Specifically, answering the following question:
 Where and how generative AI is accelerating and impacting knowledge use cases, areas, and processes?
 Please let me know what you think. 
    submitted by    /u/rachadbn  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SB3 for pettingzoo simple spread]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15dtfw7/sb3_for_pettingzoo_simple_spread/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15dtfw7/sb3_for_pettingzoo_simple_spread/"/>
        <updated>2023-07-30T19:03:11.000Z</updated>
        <summary type="html"><![CDATA[I previously posted a query about the same,
 but when i tried to implement A2C model training using SB3 on simple spread environment, I am not getting good and improved reward values, it's still highly negative and the model is performing rather randomly. 
 env = ss.pettingzoo_env_to_vec_env_v1(env)
 env = ss.concat_vec_envs_v1(env, 4, num_cpus=2, base_class="stable_baselines3")
 policy_kwargs = dict(net_arch = [128,128])
 model = A2C(
 MlpPolicy,
 env,
 verbose=1,
 learning_rate= 0.007,
 gamma = 0.95,
 ent_coef = 0.4,
 policy_kwargs= policy_kwargs,
 tensorboard_log= logdir
 )
 This is a fragment of code for reference. I tried to give more policy_kwargs like: share_features_extractor=False, or even tried to implement entirely custom policy, but the total average reward is still not going above -300.
 Also, the tensorboard plots are not showing ep_rew_mean plot, should I be passing some parameters for that?
    submitted by    /u/bruhhhwhats  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Alternatives to HF or a path forward for the OSS community?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15dsuhn/d_alternatives_to_hf_or_a_path_forward_for_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15dsuhn/d_alternatives_to_hf_or_a_path_forward_for_the/"/>
        <updated>2023-07-30T18:38:33.000Z</updated>
        <summary type="html"><![CDATA[I think it’s clear that Hugging Face is not aligned to the OSS community any more and it’s only going to get worse over the next few years. What are the top alternatives or where should the OSS contributors go? 
 I’m trying to think ahead to what libraries we should rely on and contribute to. Anyone else have this as a worry?
 https://twitter.com/untitled01ipynb/status/1685667451197878272
    submitted by    /u/homunculAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Compressing vision-language and unimodal Transformers via structured pruning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15dsjp3/r_compressing_visionlanguage_and_unimodal/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15dsjp3/r_compressing_visionlanguage_and_unimodal/"/>
        <updated>2023-07-30T18:26:03.000Z</updated>
        <summary type="html"><![CDATA[🚀 Code: https://github.com/sdc17/UPop
 📑 Paper: https://proceedings.mlr.press/v202/shi23e/shi23e.pdf
 🧐 A Quick Look
  
What is it: UPop is the first structured pruning framework for vision-language Transformers. It enables effective structured pruning on various multi-modal & uni-modal tasks (including Visual Reasoning, Image Captioning, Visual Question Answer, Image-Text Retrieval, Text-Image Retrieval, Image Classification and Image Segmentation), datasets (including NLVR2, COCO Caption, VQAv2, COCO, Flickr30K, ImageNet and ADE20K), and model architectures (including BLIP, CLIP, DeiT and Segmenter).
  
https://preview.redd.it/gfbjnxjm95fb1.png?width=2145&format=png&auto=webp&s=108898690f66a1f0afa068b69487859213055928
  
What challenge does it tackle: The above figure demonstrates that Unified Search adopted by UPop rescues us from the burden of repeated experiments (e.g., doing grid search) for searching optimal compression ratios among different modalities and structures. Furthermore, Progressive Pruning adopted by UPop eliminates the weight gap between the searched model and the pruned subnet to be retrained, therefore gaining better convergence and performance, especially at high compression ratios.
 How about the performance: On multimodal tasks, for example, UPop can achieve 2x compression with only 1.2% and 2.0% accuracy loss on the VQAv2 dataset for Visual Question Answer and the NLVR2 dataset for Visual Reasoning, respectively. On unimodal tasks, for example, UPop can achieve 1.5x and 1.2x compression without any loss of accuracy on the ImageNet dataset for Image Classification and the ADE20K dataset for Image Segmentation, respectively. Some examples of vector-level structured granularity are as follows.
  
https://preview.redd.it/lifz1n1ia5fb1.png?width=1187&format=png&auto=webp&s=f419d9c5fb4d80a2a564198eba356021e1c275e4
    submitted by    /u/Salty-Situation2606  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] [HIRING] High Paying ML Jobs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15dsinj/p_hiring_high_paying_ml_jobs/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15dsinj/p_hiring_high_paying_ml_jobs/"/>
        <updated>2023-07-30T18:24:53.000Z</updated>
        <summary type="html"><![CDATA[​
  
 Title Company Location URL 
  
 Senior Software Engineer (Backend) Nova Credit Remote https://pycareer.io/jobs/6816 
  Data Scientist - Delivery, Senior-Staff Instacart Not Specified https://pycareer.io/jobs/6773 
  Data Scientist Data Scientist United States https://pycareer.io/jobs/6780 
  Senior Data Scientist (NLP and Classification Expert) › Senior Data Scientist (NLP and Classification Expert) › Not Specified https://pycareer.io/jobs/6781 
  Senior Software Engineer (Backend) Senior Software Engineer (Backend) United States https://pycareer.io/jobs/6788 
  AWS Data Engineer Apply Not Specified United States https://pycareer.io/jobs/6801 
  Senior Data Engineer Manager Apply Not Specified United States https://pycareer.io/jobs/6802 
  Data Scientist – Delivery, Senior-Staff Instacart Instacart Remote https://pycareer.io/jobs/6805 
  Software Design Engineer – NET, Python – Citizen/GC (H) Not Specified Remote https://pycareer.io/jobs/6837 
  Senior Data Scientist at Getty Images Getty Images Remote https://pycareer.io/jobs/6839 
  Lead Data Scientist at General Mills General Mills Remote https://pycareer.io/jobs/6840 
  Data Scientist – Delivery, Senior-Staff at Instacart Instacart Remote https://pycareer.io/jobs/6842 
 
 ​
    submitted by    /u/tadasg6  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] PromptTools: Open source tools for language model evaluation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ds07h/p_prompttools_open_source_tools_for_language/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ds07h/p_prompttools_open_source_tools_for_language/"/>
        <updated>2023-07-30T18:03:09.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/hegel-ai  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] If you have to do a ML project for prediction macroeconomic factors which factor would you choose]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15drnzi/r_if_you_have_to_do_a_ml_project_for_prediction/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15drnzi/r_if_you_have_to_do_a_ml_project_for_prediction/"/>
        <updated>2023-07-30T17:49:11.000Z</updated>
        <summary type="html"><![CDATA[For a master thesis I want to write a ML model (and hopefully make my own contribution)
 and I plan to use macroeconomic data. I could predict the typical inflation, GDP, unemployment, but
 are there any other factors that are important. Could you give me some ideas. Thanks! 
    submitted by    /u/AnyJello605  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Can artificial intelligence solve the problem of crop diseases — and help curb global hunger?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15drj12/d_can_artificial_intelligence_solve_the_problem/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15drj12/d_can_artificial_intelligence_solve_the_problem/"/>
        <updated>2023-07-30T17:43:39.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Muinonan  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Interesting real-world applications for fine-tuning T5, and similar models?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15dpxec/d_interesting_realworld_applications_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15dpxec/d_interesting_realworld_applications_for/"/>
        <updated>2023-07-30T16:35:41.000Z</updated>
        <summary type="html"><![CDATA[Everyone is going crazy creating LORAs and fine-tuning huge LLMs, however I've seen many suggesting that models such as T5 from Google has its place in the enterprise. Have you guys used this or similarly small models for any novel real world problems? Please do share!
    submitted by    /u/MonkeyMaster64  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Simple Questions Thread]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15dnok8/d_simple_questions_thread/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15dnok8/d_simple_questions_thread/"/>
        <updated>2023-07-30T15:00:19.000Z</updated>
        <summary type="html"><![CDATA[Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!
 Thread will stay alive until next one so keep posting after the date in the title.
 Thanks to everyone for answering questions in the previous thread!
    submitted by    /u/AutoModerator  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How is the policy network updated in AlphaGo?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15dn9wz/how_is_the_policy_network_updated_in_alphago/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15dn9wz/how_is_the_policy_network_updated_in_alphago/"/>
        <updated>2023-07-30T14:42:06.000Z</updated>
        <summary type="html"><![CDATA[In AlphaGo, a tree search is performed, and uses the policy network to reduce the breadth of it. At the leafs, if the states are not terminal, it uses the value network. And then "backup" the values to update the Q value at the initial state (if 70% of my rollouts won after performing action a_1, my Q value q(initial_state, a_1) should converge to 0.7 in my initial state). But I don't see where the policy network is updated?
 ​
 Here is a slide from David Silver, the first-author of AlphaGo, but it doesn't mention how to update the policy network.
 ​
 https://preview.redd.it/f29no3xe64fb1.png?width=1523&format=png&auto=webp&s=5adb312b1d0c033aa8ebb328197fd7d917724f06
 Have I missed something?
 Thankss!
    submitted by    /u/Potential_Biscotti14  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Deep Dive and Experiments for the NN + Gzip Method vs LLMs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15djz60/p_deep_dive_and_experiments_for_the_nn_gzip/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15djz60/p_deep_dive_and_experiments_for_the_nn_gzip/"/>
        <updated>2023-07-30T12:04:51.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/seraschka  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What are some of the best architectures to solve this problem]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15djfn2/what_are_some_of_the_best_architectures_to_solve/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15djfn2/what_are_some_of_the_best_architectures_to_solve/"/>
        <updated>2023-07-30T11:35:16.000Z</updated>
        <summary type="html"><![CDATA[Hi Guys,
 I am working on a nn model, which can help automate the building of APIs. The problem is, we are moving data in which, there are thousands of fields, however, the fields between systems are similar in nature. This to me seems like an easy classification problem, however it doesn't scale the best.
 ​
 In terms of the data I have, if I have a dataset of 10 systems, there are not enough examples for each class for the model to train well. That is with a simple classifier where every field is a class.
 ​
 I was also thinking of using a Siamese model, where I compare the similarity between them, which allows me to use my more limited dataset more effectively
 ​
 I was wondering if there are any more architectures you guys think I should consider, or will be helpful in solving my problem
 ​
 Thank you for your help!
    submitted by    /u/eatlantis  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI For Music Extension]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15ditcr/ai_for_music_extension/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15ditcr/ai_for_music_extension/"/>
        <updated>2023-07-30T10:59:50.000Z</updated>
        <summary type="html"><![CDATA[I Tried An AI To Extend Music But It Didnt Really Go Well And Im Not Planning To Pay $12 To Extend Some Music For Fun So Are There Any Good Music Extension AIs Out There (Creates New Music Based On A MP3 File Provided)
    submitted by    /u/KXRulesYT  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is wrong with my code(DQN)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15dgonm/what_is_wrong_with_my_codedqn/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15dgonm/what_is_wrong_with_my_codedqn/"/>
        <updated>2023-07-30T08:52:41.000Z</updated>
        <summary type="html"><![CDATA[recently, I've been trying to make a deep q network for solving 2x2 rubik's cubeBut after months, I stuck with same output for HUNDREDS of times :(
 I tried everything: change learning rate, change discout factor but no luck
 Here's update rule: newQ=currentQ+alpha*(newR+gamma*max(futureQValue.flatten().tolist())-currentQ)
 import torch import torch.nn as nn import torch.optim as optimizer import os from tqdm import tqdm class DQN(nn.Module): def __init__(self, stateSpaceSize,actionSpaceSize): super(DQN, self).__init__() self.fc1=nn.Linear(stateSpaceSize,128) self.fc2=nn.Linear(128,128) self.fc3=nn.Linear(128,128) self.fc4=nn.Linear(128,128) self.fc5=nn.Linear(128,128) self.fc6=nn.Linear(128,actionSpaceSize) def forward(self, x): self.relu=nn.ReLU() self.sigmold=nn.Sigmoid() self.LeakyReLU…]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google Colab With Reinforcment learning]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15dgj4f/google_colab_with_reinforcment_learning/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15dgj4f/google_colab_with_reinforcment_learning/"/>
        <updated>2023-07-30T08:42:47.000Z</updated>
        <summary type="html"><![CDATA[I need a google colab with reinforcement learning trained to detect anomalies in computer network traffic.
    submitted by    /u/Unable_Blacksmith_81  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] NEnv: Neural Environment Maps for Global Illumination]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15dfx0q/r_nenv_neural_environment_maps_for_global/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15dfx0q/r_nenv_neural_environment_maps_for_global/"/>
        <updated>2023-07-30T08:03:10.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/crp1994  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using AI to alter existing floor plan]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15dfdx3/using_ai_to_alter_existing_floor_plan/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15dfdx3/using_ai_to_alter_existing_floor_plan/"/>
        <updated>2023-07-30T07:32:00.000Z</updated>
        <summary type="html"><![CDATA[I'm trying to find an AI tool to help me test out some home renovation, but everything i find is either just for reimagining one room at a time or for generating brand new floor plans. 
 I specifically want to look at some options for merging my kitchen and living room. Preferably free or freemium.
 Any suggestions?
    submitted by    /u/litari  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ARPAbet and the Major mnemonic system]]></title>
        <id>https://www.johndcook.com/blog/?p=200719</id>
        <link href="https://www.johndcook.com/blog/2023/07/29/arpabet-major/"/>
        <updated>2023-07-30T01:16:20.000Z</updated>
        <summary type="html"><![CDATA[ARPAbet is a phonetic spelling system developed by— you guessed it—ARPA, before it became DARPA. The ARPAbet system is less expressive than IPA, but much easier for English speakers to understand. Every sound is encoded as one or two English letters. So, for example, the sound denoted ʒ in IPA is ZH in ARPAbet. In […]
ARPAbet and the Major mnemonic system first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to generate masks for overlapping classes to COCO format labels, to be used in transformer models like Segformer.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15d86pd/d_how_to_generate_masks_for_overlapping_classes/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15d86pd/d_how_to_generate_masks_for_overlapping_classes/"/>
        <updated>2023-07-30T01:02:50.000Z</updated>
        <summary type="html"><![CDATA[Hi I am new to computer vision, I am working on a particular hackathon challenge, where the input labels are in COCO format. I am using the following code to generate masks,
 cat_ids = coco.getCatIds() anns_ids = coco.getAnnIds(imgIds=img['id'], catIds=cat_ids, iscrowd=None) anns = coco.loadAnns(anns_ids) anns_img = np.zeros((img['height'],img['width'])) for ann in anns: anns_img = np.maximum(anns_img,coco.annToMask(ann)*ann['category_id']) 
 But the image has overlapping labels for some pixels, and this masking will only assign one label for such pixel, resulting in information loss, each there any way to prevent this and preserve the information?
    submitted by    /u/franticpizzaeater  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] what should I do?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15d77nn/discussion_what_should_i_do/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15d77nn/discussion_what_should_i_do/"/>
        <updated>2023-07-30T00:16:45.000Z</updated>
        <summary type="html"><![CDATA[Hi, y’all. 
 So, I completed my masters degree. Got a programming job. I’m amazed at the capabilities of machine learning and want to build my own models. I don’t really want to go and get another degree, but want to learn how to build models. I’m particularly interested in forecasting because my job deals with NASA and wind data. I’m wondering if we could predict 6 hour wind data with a balloon sounding. 
 I know c++ and python. How do I stay relevant to the changing technology space and learn how to build some cool stuff that may be useful? 
 Thanks for any advice.
    submitted by    /u/corey4005  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Calculate 'w' and 'b' in hard margin SVM]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15d729w/d_calculate_w_and_b_in_hard_margin_svm/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15d729w/d_calculate_w_and_b_in_hard_margin_svm/"/>
        <updated>2023-07-30T00:09:48.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone,
 I have been asked the following question related to SVM (Hard Margin) in the exam, and I failed to answer it. Can anyone help me find the solution?
 My approach was to sketch it and draw the marginal plane, then identify support vectors using my intuition. After that, I created a hyperplane that was the midpoint of both marginal planes, found its slope and y-intercept, but still, my answer was wrong.
 I am very new to machine learning, so any help would be appreciated.
 Consider the dataset M = {((1, 0)^T, 1), ((0, −1)^T, 1), ((1, −1)^T, −1), ((2, 0)^T, −1)}. Determine w and b.
    submitted by    /u/salman_ml  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to calculate reward for target intercept problem?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15d6ln0/how_to_calculate_reward_for_target_intercept/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15d6ln0/how_to_calculate_reward_for_target_intercept/"/>
        <updated>2023-07-29T23:48:12.000Z</updated>
        <summary type="html"><![CDATA[Hi all. I (believe) I have a tensorflow NN set up to learn how to intercept a target moving in the x-y plane. Right now, the agent can choose to change its velocity by a small amount in any of the 3 directions (for the 3D case later), then the simulation updates the agents position. The state of the sim is the relative distance and velocity vectors between the target and the pursuer. I am confused how to set up a reward function, however. When I first set it up to be a reward of 1/R (R being the distance magnitude between the target and pursuer) to reward for shorter distances and give less reward for further distances as well as a very large reward when a collision occurred, it seemed like the rewards converged to a small value instead of getting larger. Any advice? I'd be willing to upload a github link as well if you wanted to look at the code
    submitted by    /u/Happylightsocket  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Brand new AI Social App featuring unique bot features looking for iOS users to join Beta!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15d68k3/p_brand_new_ai_social_app_featuring_unique_bot/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15d68k3/p_brand_new_ai_social_app_featuring_unique_bot/"/>
        <updated>2023-07-29T23:31:16.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 I'm messaging on behalf of a brand new AI based Social Media app called Cantina. It's like a cross between the best parts of Discord, Twitch, and Snapchat, and uses both Stable Diffusion and ChatGPT to allow users to create and interact with AI bots.
 The app is currently INVITE ONLY during the Beta phase and we are looking for people to try it out (currently iOS only, but Android is coming soon!). Here's a private invite link: https://canti.na/dIdKzWcEpBb.
 The most unique and FUN part of the app is that it allows users to interact with and build your own AI chat bots, and these bots also work as AI art creators. Simply ask them to draw something, and they'll provide you with a picture based on your prompt. There are lots of premade bots that you can interact with or add to rooms, or you can easily create your own bot using the Make A Bot function. There will be prizes and initiatives for the most creative bots in the near future. I'd love to see what you come up with!
 Anyway, you can download through the invite link above and dive right in. If you have any thoughts, questions, or comments, please feel free to message me! During this limited beta phase, your feedback will be invaluable.
    submitted by    /u/SamuelAnonymous  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Research Blog - The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15d5vu0/ai_research_blog_the_transformer_blueprint_a/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15d5vu0/ai_research_blog_the_transformer_blueprint_a/"/>
        <updated>2023-07-29T23:15:00.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/bartturner  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invite only AI Social app featuring insane bot creation tool looking for new users to test during beta rollout!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15d50rb/invite_only_ai_social_app_featuring_insane_bot/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15d50rb/invite_only_ai_social_app_featuring_insane_bot/"/>
        <updated>2023-07-29T22:35:57.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 I'm working with a brand new AI based Social Media app called Cantina. It's currently INVITE ONLY during the Beta phase and we are looking for people to try it out (currently iOS only, but Android is coming soon!). Here's a private invite link: https://canti.na/dIdKzWcEpBb. 
 The most unique and FUN part of the app is that it allows users to interact with and build your own AI chat bots. There are lots of premade bots that you can interact with or add to rooms, or you can easily create your own bot using the Make A Bot function. 
 For example: I recently made a Friendly English Teacher bot whose sole purpose is to help people learn English. I also made an McDonald Trump bot who WILL NOT REST until he is president and can mandate the consumption of Big Macs for Breakfast, Lunch, and Dinner!
 There will be prizes and initiatives for the most creative bots in the near future. I'd love to see what you come up with!
 Anyway, you can download through the invite link above and dive right in. If you have any thoughts, questions, or comments, please feel free to contact me! During this limited beta phase, your feedback will be invaluable.
    submitted by    /u/SamuelAnonymous  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What are Receptive Fields and How Do They Effect Your Model?]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15d4q2r/what_are_receptive_fields_and_how_do_they_effect/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15d4q2r/what_are_receptive_fields_and_how_do_they_effect/"/>
        <updated>2023-07-29T22:22:52.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/CkmCpvis  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] AI that can describe a video?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15d4nj4/d_ai_that_can_describe_a_video/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15d4nj4/d_ai_that_can_describe_a_video/"/>
        <updated>2023-07-29T22:19:43.000Z</updated>
        <summary type="html"><![CDATA[Anyone know if there is anything able to describe the content of a video? I have found a lot of stuff for images but nothing for videos.
    submitted by    /u/crazewill  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Best Machine Learning Algorithms for Forecasting]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15d4kjf/p_best_machine_learning_algorithms_for_forecasting/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15d4kjf/p_best_machine_learning_algorithms_for_forecasting/"/>
        <updated>2023-07-29T22:16:02.000Z</updated>
        <summary type="html"><![CDATA[I plan on using Machine Learning Algorithms to forecast future values of power demand and the literature on the subject is a bit divisive. I'm getting ANN, Decision Trees (odd), SVMs etc.
 I just want to know what models you guys would use (MATLAB and Python only, except it's really good).
 Thank you in anticipation.
 P.S: Any literature to streamline my search will be greatly appreciated.
    submitted by    /u/X69-2  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lost both my jobs to AI. Now, I'm at an AI company launching an easy-to-use social app featuring easy bot creation & interaction. Inviting this community to explore and share feedback!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15d4089/lost_both_my_jobs_to_ai_now_im_at_an_ai_company/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15d4089/lost_both_my_jobs_to_ai_now_im_at_an_ai_company/"/>
        <updated>2023-07-29T21:51:56.000Z</updated>
        <summary type="html"><![CDATA[So, long story short. I lost BOTH my day jobs because of AI. Initially I was bitter that AI "took my job," but after pulling up my socks, I found dozens of new opportunities thanks to AI. 
 Somewhat ironically, I found a new job at an AI Social Media app called Cantina, and I couldn't be more excited.
 Cantina is best described as a mix up of the best parts of Discord, Twitch, and Snapchat... with the unique bonus of being able to interact with and build your own AI chat bots. Sort of hard to explain, but once you try it out you'll get the idea.
 The app is currently in a limited INVITE ONLY Beta phase and I'm looking to invite a small number of users to give it a shot (currently iOS only, but Android is coming soon!). Here's an invite so you can dive in and see what it's all about: https://canti.na/dIdKzWcEpBb
 After joining, you'll find lots of rooms you can join and chat through any combination of voice, video, or text. And if no rooms stand out, you can make your own! 
 There are lots of premade bots that you can interact with or add to rooms, and you can easily create your own bot using the Make A Bot function. This is the standout feature, and I'm simply blown away at what's possible. 
 I recently made a Friendly English Teacher bot whose sole purpose is to help people learn English. I also made an McDonald Trump bot who is an algamation of both Ronald McDonald and Donald Trump and WILL NOT REST until he is president and can mandate the consumption of Big Macs for Breakfast, Lunch, and Dinner. I still can't believe I'm getting paid to do this...
 Anyway, please take a moment to download and check it out, and if you have any thoughts, questions, or comments, please feel free to contact me! During this limited beta phase, your feedback will be invaluable.
 ​
    submitted by    /u/SamuelAnonymous  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to disable auto environment reset in `Gymnasium`]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15d2di4/how_to_disable_auto_environment_reset_in_gymnasium/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15d2di4/how_to_disable_auto_environment_reset_in_gymnasium/"/>
        <updated>2023-07-29T20:42:47.000Z</updated>
        <summary type="html"><![CDATA[I am trying to implement my own version of ppo using gymnasium. Here is my code for rollout - 
  def rollout(): transitions = [] disc_reward_list = [] for i in range(ppo_batch): obs = torch.tensor(env.reset(), dtype=torch.float32) print("obs = ", obs.shape) all_rewards = [] iter = 0 done = False tot_rewards = 0 print("done = ", done) while True: act_probs = torch.distributions.Categorical(actor(obs.to(device)).squeeze()) print("act_probs = ", act_probs) # print("act_probs = ", actor(obs.to(device))) action = act_probs.sample().squeeze() action = action.cpu().detach().numpy() print("action shape = ", action.shape) next_state, reward, done, info = env.step(action) print("next_state shape = ", next_state.shape) print("reward shape = ", reward.shape) print("done shape = ", done) action = torch.tensor(action, dtype=torch.float32).to(device) all_rewards.append(reward) tot_rewards += reward iter += 1 transitions.append((obs.cpu().detach().numpy(), action.cpu().detach().numpy(), act_probs.log_prob(action).cpu().detach().numpy())) obs = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0) print("Reward = ", tot_rewards) eps_rew = 0 eps_rew_list = [] for reward in reversed(all_rewards): eps_rew = eps_rew*gamma + reward eps_rew_list.append(eps_rew) for rtgs in reversed(eps_rew_list): disc_reward_list.append(rtgs) 
 My issue is that in my while loop - The environment autoresets after the `done` variable becomes `True`
 For instance, if I have `8` environments running in parallel `env=gym.vector.make('CartPole-v1', num_envs=8)` and print out the done shape, I might get - `[False False False False False True False False]`. I want that environment where `done=True` to stop and not reset. I believe that's how PPO is supposed to work. 
 I am a bit of a beginner with this stuff. Please let me know if something I said is not clear.
    submitted by    /u/Academic-Rent7800  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios - Shanghai Jiao Tong University et al 2023 - Plugin for ChatGPT! - Highly improves factfulness in math, code, knowledge and scientific reasoning!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15d1l08/r_factool_factuality_detection_in_generative_ai_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15d1l08/r_factool_factuality_detection_in_generative_ai_a/"/>
        <updated>2023-07-29T20:09:05.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2307.13528 
 Blog: https://ethanc111.github.io/factool_website/ 
 Github: https://github.com/GAIR-NLP/factool 
 Factool is a tool augmented framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Factool now supports 4 tasks:
  
knowledge-based QA: Factool detects factual errors in knowledge-based QA.
 code generation: Factool detects execution errors in code generation.
 mathematical reasoning: Factool detects calculation errors in mathematical reasoning.
 scientific literature review: Factool detects hallucinated scientific literatures.
  
Abstract:
  
The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in t…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Promptify 2.0: More Structured, More Powerful LLMs with Prompt-Optimization, Prompt-Engineering, and Structured Json Parsing with GPT-n Models! 🚀]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15d1j36/p_promptify_20_more_structured_more_powerful_llms/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15d1j36/p_promptify_20_more_structured_more_powerful_llms/"/>
        <updated>2023-07-29T20:06:43.000Z</updated>
        <summary type="html"><![CDATA[Hello fellow coders and AI enthusiasts!
 First up, a huge Thank You for making Promptify a hit with over 2.3k+ stars on Github ! 🌟
 Back in 2022, we were the first one to tackle the common challenge of uncontrolled, unstructured outputs from large language models like GPT-3. , and your support has pushed us to keep improving.Today, we're thrilled to share some major updates that make Promptify even more powerful
 https://preview.redd.it/hk7ro4tmnyeb1.png?width=1510&format=png&auto=webp&s=226ada1f896c620137f827932c03a9df88e35d69
 ​
  
Unified Architecture 🧭: Introducing Prompter, Model & Pipeline Solution
 Detailed Output Logs 📔: Comprehensive structured JSON format output within the log folder.
 Wider Model Support 🤝: Supporting models from OpenAI, Azure, Cohere, Anthropic, Huggingface and more - think of it as your universal language model adapter.
 Robust Parser 🦸‍♂️: Parser to handle incomplete or unstructured JSON outputs from any LLMs.
 Ready-Made Jinja Templates 📝: Jinja prompt templates for NER, Text Classification, QA, Relation-Extraction, Tabular data, etc.
 Database Integration 🔗: Soon, Promptify directly to Mongodb integration. Stay tuned!
 Effortless Embedding Generation 🧬: Generate embeddings from various LLMs effortlessly with the new update.
  
https://preview.redd.it/rf8yjqxnnyeb1.png?width=2160&format=png&auto=webp&s=87b7c2408382757e38ff554fde56e56bd60b1793
 ​
 Check out the examples and take Promptify for a spin on GitHub. If you like what you see, we'd be honored if you gave us a star!
  
Github: https://github.com/promptslab/Promptify
 Colab: Try Now on Colab
 Explore Other Cool Open Source LLM Tools: https://github.com/promptslab
  
Join 1.6k+ Promptify users on Discord to dive deep into prompt engineering, discuss the latest with LLMs, and advance NLP research together: https://discord.com/invite/m88xfYMbK6
 Thank you again for your support - here's to more structured AI!
    submitted by    /u/StoicBatman  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Why Being Careful Matters When Selecting CNN Padding]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15d1c53/d_why_being_careful_matters_when_selecting_cnn/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15d1c53/d_why_being_careful_matters_when_selecting_cnn/"/>
        <updated>2023-07-29T19:58:42.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/CkmCpvis  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control - Google DeepMind 2023 - Is able to perform multi-stage semantic reasoning and can interpret commands not present in the robot training data!]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15d0p33/rt2_visionlanguageaction_models_transfer_web/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15d0p33/rt2_visionlanguageaction_models_transfer_web/"/>
        <updated>2023-07-29T19:31:08.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://robotics-transformer2.github.io/assets/rt2.pdf 
 Blog: https://robotics-transformer2.github.io/ 
 Blog: https://www.deepmind.com/blog/rt-2-new-model-translates-vision-and-language-into-action 
 Github ( RT-1 as of now) : https://github.com/google-research/robotics_transformer 
 Abstract:
  
We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robot…]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control - Google DeepMind 2023 - Is able to perform multi-stage semantic reasoning and can interpret commands not present in the robot training data!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15d0oz9/r_rt2_visionlanguageaction_models_transfer_web/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15d0oz9/r_rt2_visionlanguageaction_models_transfer_web/"/>
        <updated>2023-07-29T19:31:01.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://robotics-transformer2.github.io/assets/rt2.pdf 
 Blog: https://robotics-transformer2.github.io/ 
 Blog: https://www.deepmind.com/blog/rt-2-new-model-translates-vision-and-language-into-action 
 Github ( RT-1 as of now) : https://github.com/google-research/robotics_transformer 
 Abstract:
  
We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robot…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introduction to “AI & Data Literacy: Empowering Citizens of Data Science”]]></title>
        <id>https://www.datasciencecentral.com/?p=62745</id>
        <link href="https://www.datasciencecentral.com/introduction-to-ai-data-literacy-empowering-citizens-of-data-science/"/>
        <updated>2023-07-29T19:22:55.000Z</updated>
        <summary type="html"><![CDATA[One of the reasons that I moved back to Iowa last year was that I saw an opportunity to work with local educational institutions to create an AI Institute for organizations in middle America that either get overlooked in the AI conversation or are unsure what AI means to them. I wanted to reduce the… Read More »Introduction to “AI & Data Literacy: Empowering Citizens of Data Science”
The post Introduction to “AI & Data Literacy: Empowering Citizens of Data Science” appeared first on Data Science Central.]]></summary>
        <author>
            <name>Bill Schmarzo</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] No free lunch theorem]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15cz6zh/d_no_free_lunch_theorem/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15cz6zh/d_no_free_lunch_theorem/"/>
        <updated>2023-07-29T18:27:02.000Z</updated>
        <summary type="html"><![CDATA[A conclusion of the no free lunch theorem is that there can't exist a universal learning algorithm. My understanding has been that this was the end goal of AI research; creating a universal learner. What is the community progressing towards, if not that?
    submitted by    /u/lemlo100  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Researchers Discover New Vulnerability in Large Language Models]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15cxj5s/researchers_discover_new_vulnerability_in_large/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15cxj5s/researchers_discover_new_vulnerability_in_large/"/>
        <updated>2023-07-29T17:15:14.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Gzip beats BERT?" Part 2: dataset issues, improved speed, and results]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15cxid7/gzip_beats_bert_part_2_dataset_issues_improved/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15cxid7/gzip_beats_bert_part_2_dataset_issues_improved/"/>
        <updated>2023-07-29T17:14:16.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Project] Seeking Coding Wizards for Traveling Salesman Challenge!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15cx7j3/project_seeking_coding_wizards_for_traveling/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15cx7j3/project_seeking_coding_wizards_for_traveling/"/>
        <updated>2023-07-29T17:01:14.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone,
 I'm currently working on an exciting project using the Travelling Salesman Problem (TSP), and I'd love to have some coding wizards join the fun!
 If you enjoy solving optimisation problems and have some coding experience, particularly in Python, this project is for you. To determine the most efficient routes, we'll use heuristic methods such as the Nearest Neighbour Algorithm, Genetic Algorithm, and Ant Colony Optimisation.
 If you aren't a TSP expert yet, don't worry. We'll be learning and exploring together! I'm really looking forward to seeing how we can optimise routes for real-world applications like delivery and travel planning.
 So, if you're looking for a coding adventure and want to be a part of a fantastic project, hit me up! Let's crack this TSP puzzle and create some smart solutions.
 If you're interested in collaborating, please send me a message. I can't wait to work with you and nerd out on some fantastic code!
    submitted by    /u/vampire_19  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ML on detecting bacteria in blood through pictures for beginners [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15cwufp/ml_on_detecting_bacteria_in_blood_through/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15cwufp/ml_on_detecting_bacteria_in_blood_through/"/>
        <updated>2023-07-29T16:45:33.000Z</updated>
        <summary type="html"><![CDATA[I am trying to make a machine that could detect if there is any bacteria in blood through pictures. However I do not know any thing about machine learning and only knows a little bit of Python and C++. What should I do?
    submitted by    /u/EthanWasTakenAgain  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Resources to understand how distributed Actor-Critic algorithms work?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15cwesz/resources_to_understand_how_distributed/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15cwesz/resources_to_understand_how_distributed/"/>
        <updated>2023-07-29T16:26:50.000Z</updated>
        <summary type="html"><![CDATA[Can someone please point me to resources how distributed actor-critic algorithms work? My final goal is to understand distributed PPO works. I was following thisblog and a few other books but I'm unable to see the big picture nor am I able to understand the little details. 
 The big picture - Why does distributed training help in online algorithms like PPO, Actor-Critic
 The code details - I figured out how to make multiprocessing work with gym. But how does one perform learning? Should I combine all the parallel environments and feed them to my neural network? I checked cleanrl but am getting a little confused.
    submitted by    /u/Academic-Rent7800  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ruzsa distance]]></title>
        <id>https://www.johndcook.com/blog/?p=200647</id>
        <link href="https://www.johndcook.com/blog/2023/07/29/ruzsa-distance/"/>
        <updated>2023-07-29T16:02:57.000Z</updated>
        <summary type="html"><![CDATA[A few days ago I wrote about Jaccard distance, a way of defining a distance between sets. The Ruzsa distance is similar, except it defines the distance between two subsets of an Abelian group. Subset difference Let A and B be two subsets of an Abelian (commutative) group G. Then the difference A − B […]
Ruzsa distance first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]Seeking Participants for AI-related Survey]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15cv8ql/dseeking_participants_for_airelated_survey/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15cv8ql/dseeking_participants_for_airelated_survey/"/>
        <updated>2023-07-29T15:36:37.000Z</updated>
        <summary type="html"><![CDATA[I am currently working on my IB Extended Essay, and I would greatly appreciate your help in gathering valuable insights from individuals knowledgeable in the field of AI. The purpose of my survey is to understand the perspectives of AI enthusiasts .
 If you have a few minutes to spare, I kindly request you to participate in my survey. Your input will contribute significantly to my research and help me gain a deeper understanding of the topic. The survey covers various aspects of AI, and your expertise will be invaluable in shaping the results.
 Survey Link: https://forms.gle/PVGrRbPLTpZRbbpL9
 Rest assured that all responses will be kept confidential and only used for academic purposes. Additionally, feel free to share this survey with others who might be interested or knowledgeable in the field. Thank you in advance for your time and contributions! Your participation will greatly aid in the successful completion of my IB Extended Essay.
    submitted by    /u/KVNG_Winston  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Seeking Resume Expertise: Struggling to Land Interviews or Jobs, Need Guidance! Please Assist!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15cv1p4/d_seeking_resume_expertise_struggling_to_land/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15cv1p4/d_seeking_resume_expertise_struggling_to_land/"/>
        <updated>2023-07-29T15:28:32.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/AIKiller1997  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient LASSO regression for N=~200,000 and dim=~30,000 [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15cudgl/efficient_lasso_regression_for_n200000_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15cudgl/efficient_lasso_regression_for_n200000_and/"/>
        <updated>2023-07-29T14:59:48.000Z</updated>
        <summary type="html"><![CDATA[Please suggest me efficient LASSO regression implementations for very high dimensional data. Thanks in advance!
    submitted by    /u/Charming-Witness-286  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google Deepmind presents RT-2, the first vision-language-action (VLA) Robotics Transformer and it may have drastic implications our future.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15cs6fq/google_deepmind_presents_rt2_the_first/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15cs6fq/google_deepmind_presents_rt2_the_first/"/>
        <updated>2023-07-29T13:19:54.000Z</updated>
        <summary type="html"><![CDATA[The latest article published by Google Deepmind is seriously approaching a Blade Runner type future. Their research paper is on the first VLA (vision-language-action) Model RT-2 (see paper), a multi-modal algorithm which tokenizes robotic inputs and output actions (e.g., camera images, task instructions, and motor commands) in order to use this information to learn quickly by translating the knowledge it receives in real-time into generalized instructions for its own robotic control.
 RT-1 absorbs large amounts of data, including robot trajectories with multiple tasks, objects and environments, resulting in better performance and generalization. (source)
 RT-2 incorporates chain-of-thought to allow for multi-stage semantic reasoning, like deciding which object could be used as an improvise…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A famous french Youtuber named Joueur Du Grenier discovers he has an unofficial AI Voice channel, and the AI voices are insanely good]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15crq13/a_famous_french_youtuber_named_joueur_du_grenier/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15crq13/a_famous_french_youtuber_named_joueur_du_grenier/"/>
        <updated>2023-07-29T12:58:31.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/the_anonymizer  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How can I create multiple environments using `SB3` for manual use?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15crn85/how_can_i_create_multiple_environments_using_sb3/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15crn85/how_can_i_create_multiple_environments_using_sb3/"/>
        <updated>2023-07-29T12:54:47.000Z</updated>
        <summary type="html"><![CDATA[​
 ​
 I know that `SB3` provides various techniques to come up with vectorized environments. I want to limit myself to only using the vectorized environments and implement the RL algorithms from scratch. Would that be possible? My final objective is to learn how to play with RL hyperparameters on parallel environments in order to accelerate learning speeds. Currently, I am stuck on -
 import os import gymnasium as gym from stable_baselines3.common.vec_env import DummyVecEnv env = DummyVecEnv([lambda: gym.make("CartPole-v1")]) obs = env.reset() done = False while not done: action = env.action_space.sample() next_obs, reward, done, info = env.step(action) obs = next_obs 
 But I get the following error -
 ​
 Traceback (most recent call last): File "D:\q_learning\dummy_envs.py", line 9, in <module> next_obs, reward, done, info = env.step(action) File "C:\Users\thoma\anaconda3\envs\torch_2\lib\site-packages\stable_baselines3\common\vec_env\base_vec_env.py", line 197, in step return self.step_wait() File "C:\Users\thoma\anaconda3\envs\torch_2\lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py", line 59, in step_wait self.actions[env_idx] IndexError: invalid index to scalar variable. 
 ​
    submitted by    /u/Academic-Rent7800  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Big Net For Everything (2018)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15cphv6/one_big_net_for_everything_2018/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15cphv6/one_big_net_for_everything_2018/"/>
        <updated>2023-07-29T11:00:50.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/EducationalCicada  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection Transformer (DETR) Explained]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15comhm/detection_transformer_detr_explained/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15comhm/detection_transformer_detr_explained/"/>
        <updated>2023-07-29T10:10:46.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Personal-Trainer-541  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text reclassification prompts/code [D] [R]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15cm94i/text_reclassification_promptscode_d_r/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15cm94i/text_reclassification_promptscode_d_r/"/>
        <updated>2023-07-29T07:47:16.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/MutedCatch  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 7/28/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15cj10i/oneminute_daily_ai_news_7282023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15cj10i/oneminute_daily_ai_news_7282023/"/>
        <updated>2023-07-29T04:40:49.000Z</updated>
        <summary type="html"><![CDATA[Google introduces Robotic Transformer 2 (RT-2), a novel vision-language-action (VLA) model that learns from both web and robotics data, and translates this knowledge into generalized instructions for robotic control, while retaining web-scale capabilities.[1]
 Thymia, a healthtech startup building gamified AI tools to revolutionize how we assess and monitor mental health, has today announced a €2.4 million seed round to expand the reach and capabilities of its pioneering technology.[2]
 Intel CEO Pat Gelsinger was very bullish on AI during the company’s Q2 2023 earnings call — telling investors that Intel plans to “build AI into every product that we build.”[3]
 Walmart is using artificial intelligence to help streamline their product organization.[4]
  
Sources:
 [1] https://www.deepmind.com/blog/rt-2-new-model-translates-vision-and-language-into-action
 [2] https://www.eu-startups.com/2023/07/london-based-thymia-raises-e2-4-million-seed-round-to-expand-its-video-game-inspired-mental-health-ai/
 [3] https://www.theverge.com/2023/7/27/23810360/intel-pat-gelsinger-ai-every-platform-promise
 [4] https://www.nbcnews.com/nightly-news/video/walmart-using-ai-to-streamline-organization-what-will-it-mean-for-workers-189519429834 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Conformal Prediction with Python]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15cie8m/d_conformal_prediction_with_python/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15cie8m/d_conformal_prediction_with_python/"/>
        <updated>2023-07-29T04:05:51.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Kujamara  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[UPDATE] I fear the future of AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15cho8i/update_i_fear_the_future_of_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15cho8i/update_i_fear_the_future_of_ai/"/>
        <updated>2023-07-29T03:27:51.000Z</updated>
        <summary type="html"><![CDATA[Hi, guys! Hope everyone is doing fine.
 Some of you may remember me. About two months ago I posted here about an anxiety breakdown I've gone through regarding "AI", "Programming" and how "human programmers would end" and stuff like that, which was a major concern for me since programming was my job and my favorite thing to do.
 I was wondering for some time if I was supposed to share an update here. I decided to do so since somebody out there may be feeling the same as me. So I not only have an update but I also want to give some advice to whoever is going through this sh*thole.
 After that post, I talked about my feelings with a lot of people around me (friends and fiancée), and everyone was very supportive. At first I thought they would laugh at me, since there are a lot more to worry to…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI chan the essential worker [OC]]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15chbsl/ai_chan_the_essential_worker_oc/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15chbsl/ai_chan_the_essential_worker_oc/"/>
        <updated>2023-07-29T03:09:35.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/leonleungjeehei  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google is training robots the way it trains AI chatbots]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15cgb9v/google_is_training_robots_the_way_it_trains_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15cgb9v/google_is_training_robots_the_way_it_trains_ai/"/>
        <updated>2023-07-29T02:18:01.000Z</updated>
        <summary type="html"><![CDATA[“RT-2 is the new version of what the company calls its vision-language-action (VLA) model. The model teaches robots to better recognize visual and language patterns to interpret instructions and infer what objects work best for the request.”
    submitted by    /u/Sonic_Improv  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral learning of Bernoulli linear dynamical systems models. (arXiv:2303.02060v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.02060</id>
        <link href="http://arxiv.org/abs/2303.02060"/>
        <updated>2023-07-29T00:48:57.654Z</updated>
        <summary type="html"><![CDATA[Latent linear dynamical systems with Bernoulli observations provide a
powerful modeling framework for identifying the temporal dynamics underlying
binary time series data, which arise in a variety of contexts such as binary
decision-making and discrete stochastic processes (e.g., binned neural spike
trains). Here we develop a spectral learning method for fast, efficient fitting
of probit-Bernoulli latent linear dynamical system (LDS) models. Our approach
extends traditional subspace identification methods to the Bernoulli setting
via a transformation of the first and second sample moments. This results in a
robust, fixed-cost estimator that avoids the hazards of local optima and the
long computation time of iterative fitting procedures like the
expectation-maximization (EM) algorithm. In regimes where data is limited or
assumptions about the statistical structure of the data are not met, we
demonstrate that the spectral estimate provides a good initialization for
Laplace-EM fitting. Finally, we show that the estimator provides substantial
benefits to real world settings by analyzing data from mice performing a
sensory decision-making task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Stone_I/0/1/0/all/0/1"&gt;Iris R. Stone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sagiv_Y/0/1/0/all/0/1"&gt;Yotam Sagiv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Park_I/0/1/0/all/0/1"&gt;Il Memming Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pillow_J/0/1/0/all/0/1"&gt;Jonathan W. Pillow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MixupE: Understanding and Improving Mixup from Directional Derivative Perspective. (arXiv:2212.13381v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.13381</id>
        <link href="http://arxiv.org/abs/2212.13381"/>
        <updated>2023-07-29T00:48:57.653Z</updated>
        <summary type="html"><![CDATA[Mixup is a popular data augmentation technique for training deep neural
networks where additional samples are generated by linearly interpolating pairs
of inputs and their labels. This technique is known to improve the
generalization performance in many learning paradigms and applications. In this
work, we first analyze Mixup and show that it implicitly regularizes infinitely
many directional derivatives of all orders. Based on this new insight, we
propose an improved version of Mixup, theoretically justified to deliver better
generalization performance than the vanilla Mixup. To demonstrate the
effectiveness of the proposed method, we conduct experiments across various
domains such as images, tabular data, speech, and graphs. Our results show that
the proposed method improves Mixup across multiple datasets using a variety of
architectures, for instance, exhibiting an improvement over Mixup by 0.8% in
ImageNet top-1 accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yingtian Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1"&gt;Vikas Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1"&gt;Sarthak Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1"&gt;Wai Hoh Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1"&gt;Hieu Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1"&gt;Juho Kannala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1"&gt;Arno Solin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1"&gt;Kenji Kawaguchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gaussian Latent Representations for Uncertainty Estimation using Mahalanobis Distance in Deep Classifiers. (arXiv:2305.13849v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.13849</id>
        <link href="http://arxiv.org/abs/2305.13849"/>
        <updated>2023-07-29T00:48:57.653Z</updated>
        <summary type="html"><![CDATA[Recent works show that the data distribution in a network's latent space is
useful for estimating classification uncertainty and detecting
Out-of-distribution (OOD) samples. To obtain a well-regularized latent space
that is conducive for uncertainty estimation, existing methods bring in
significant changes to model architectures and training procedures. In this
paper, we present a lightweight, fast, and high-performance regularization
method for Mahalanobis distance-based uncertainty prediction, and that requires
minimal changes to the network's architecture. To derive Gaussian latent
representation favourable for Mahalanobis Distance calculation, we introduce a
self-supervised representation learning method that separates in-class
representations into multiple Gaussians. Classes with non-Gaussian
representations are automatically identified and dynamically clustered into
multiple new classes that are approximately Gaussian. Evaluation on standard
OOD benchmarks shows that our method achieves state-of-the-art results on OOD
detection with minimal inference time, and is very competitive on predictive
probability calibration. Finally, we show the applicability of our method to a
real-life computer vision use case on microorganism classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Venkataramanan_A/0/1/0/all/0/1"&gt;Aishwarya Venkataramanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benbihi_A/0/1/0/all/0/1"&gt;Assia Benbihi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laviale_M/0/1/0/all/0/1"&gt;Martin Laviale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pradalier_C/0/1/0/all/0/1"&gt;Cedric Pradalier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v7 [physics.chem-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.10045</id>
        <link href="http://arxiv.org/abs/2306.10045"/>
        <updated>2023-07-29T00:48:57.653Z</updated>
        <summary type="html"><![CDATA[We study property prediction for crystal materials. A crystal structure
consists of a minimal unit cell that is repeated infinitely in 3D space. How to
accurately represent such repetitive structures in machine learning models
remains unresolved. Current methods construct graphs by establishing edges only
between nearby nodes, thereby failing to faithfully capture infinite repeating
patterns and distant interatomic interactions. In this work, we propose several
innovations to overcome these limitations. First, we propose to model
physics-principled interatomic potentials directly instead of only using
distances as in many existing methods. These potentials include the Coulomb
potential, London dispersion potential, and Pauli repulsion potential. Second,
we model the complete set of potentials among all atoms, instead of only
between nearby atoms as in existing methods. This is enabled by our
approximations of infinite potential summations with provable error bounds. We
further develop efficient algorithms to compute the approximations. Finally, we
propose to incorporate our computations of complete interatomic potentials into
message passing neural networks for representation learning. We perform
experiments on the JARVIS and Materials Project benchmarks for evaluation.
Results show that the use of interatomic potentials and complete interatomic
potentials leads to consistent performance improvements with reasonable
computational costs. Our code is publicly available as part of the AIRS library
(https://github.com/divelab/AIRS/tree/main/OpenMat/PotNet).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yuchao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Yan_K/0/1/0/all/0/1"&gt;Keqiang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Youzhi Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Qian_X/0/1/0/all/0/1"&gt;Xiaoning Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shuiwang Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations. (arXiv:2307.09916v3 [cs.HC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.09916</id>
        <link href="http://arxiv.org/abs/2307.09916"/>
        <updated>2023-07-29T00:48:57.652Z</updated>
        <summary type="html"><![CDATA[Deep learning (DL) approaches are being increasingly used for time-series
forecasting, with many efforts devoted to designing complex DL models. Recent
studies have shown that the DL success is often attributed to effective data
representations, fostering the fields of feature engineering and representation
learning. However, automated approaches for feature learning are typically
limited with respect to incorporating prior knowledge, identifying interactions
among variables, and choosing evaluation metrics to ensure that the models are
reliable. To improve on these limitations, this paper contributes a novel
visual analytics framework, namely TimeTuner, designed to help analysts
understand how model behaviors are associated with localized correlations,
stationarity, and granularity of time-series representations. The system mainly
consists of the following two-stage technique: We first leverage counterfactual
explanations to connect the relationships among time-series representations,
multivariate features and model predictions. Next, we design multiple
coordinated views including a partition-based correlation matrix and juxtaposed
bivariate stripes, and provide a set of interactions that allow users to step
into the transformation selection process, navigate through the feature space,
and reason the model performance. We instantiate TimeTuner with two
transformation methods of smoothing and sampling, and demonstrate its
applicability on real-world time-series forecasting of univariate sunspots and
multivariate air pollutants. Feedback from domain experts indicates that our
system can help characterize time-series representations and guide the feature
engineering processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1"&gt;Jianing Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1"&gt;Qing Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yilin Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wei Zeng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v3 [cs.DB] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.13494</id>
        <link href="http://arxiv.org/abs/2307.13494"/>
        <updated>2023-07-29T00:48:57.652Z</updated>
        <summary type="html"><![CDATA[Learned cardinality estimation methods have achieved high precision compared
to traditional methods. Among learned methods, query-driven approaches face the
data and workload drift problem for a long time. Although both query-driven and
hybrid methods are proposed to avoid this problem, even the state-of-art of
them suffer from high training and estimation costs, limited scalability,
instability, and long-tailed distribution problem on high cardinality and high
dimensional tables, which seriously affects the practical application of
learned cardinality estimators. In this paper, we prove that most of these
problems are directly caused by the widely used progressive sampling. We solve
this problem by introducing predicates into the autoregressive model and
propose Duet, a stable, efficient, and scalable hybrid method to estimate
cardinality directly without sampling or any non-differentiable process, which
can not only reduces the inference complexity from $O(n)$ to $O(1)$ compared to
Naru and UAE but also achieve higher accuracy on high cardinality and high
dimensional tables. Experimental results show that Duet can achieve all the
design goals above and be much more practical and even has a lower inference
cost on CPU than that of most learned methods on GPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaixin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yabin Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1"&gt;Chang Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;Donghua Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning. (arXiv:2305.19472v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.19472</id>
        <link href="http://arxiv.org/abs/2305.19472"/>
        <updated>2023-07-29T00:48:57.651Z</updated>
        <summary type="html"><![CDATA[Procedural planning, which entails decomposing a high-level goal into a
sequence of temporally ordered steps, is an important yet intricate task for
machines. It involves integrating common-sense knowledge to reason about
complex contextualized situations that are often counterfactual, e.g.
"scheduling a doctor's appointment without a phone". While current approaches
show encouraging results using large language models (LLMs), they are hindered
by drawbacks such as costly API calls and reproducibility issues. In this
paper, we advocate planning using smaller language models. We present PlaSma, a
novel two-pronged approach to endow small language models with procedural
knowledge and (counterfactual) planning capabilities. More concretely, we
develop symbolic procedural knowledge distillation to enhance the implicit
knowledge in small language models and an inference-time algorithm to
facilitate more structured and accurate reasoning. In addition, we introduce a
novel task, Counterfactual Planning, that requires a revision of a plan to cope
with a counterfactual situation. In both the original and counterfactual
setting, we show that orders-of-magnitude smaller models (770M-11B parameters)
can compete and often surpass their larger teacher models' capabilities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1"&gt;Faeze Brahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1"&gt;Chandra Bhagavatula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1"&gt;Valentina Pyatkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1"&gt;Jena D. Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Lorraine Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arai_H/0/1/0/all/0/1"&gt;Hirona J. Arai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1"&gt;Soumya Sanyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1"&gt;Keisuke Sakaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non Intrusive Intelligibility Predictor for Hearing Impaired Individuals using Self Supervised Speech Representations. (arXiv:2307.13423v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.13423</id>
        <link href="http://arxiv.org/abs/2307.13423"/>
        <updated>2023-07-29T00:48:57.651Z</updated>
        <summary type="html"><![CDATA[Self-supervised speech representations (SSSRs) have been successfully applied
to a number of speech-processing tasks, e.g. as feature extractor for speech
quality (SQ) prediction, which is, in turn, relevant for assessment and
training speech enhancement systems for users with normal or impaired hearing.
However, exact knowledge of why and how quality-related information is encoded
well in such representations remains poorly understood. In this work,
techniques for non-intrusive prediction of SQ ratings are extended to the
prediction of intelligibility for hearing-impaired users. It is found that
self-supervised representations are useful as input features to non-intrusive
prediction models, achieving competitive performance to more complex systems. A
detailed analysis of the performance depending on Clarity Prediction Challenge
1 listeners and enhancement systems indicates that more data might be needed to
allow generalisation to unknown systems and (hearing-impaired) individuals]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Close_G/0/1/0/all/0/1"&gt;George Close&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hain_T/0/1/0/all/0/1"&gt;Thomas Hain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goetze_S/0/1/0/all/0/1"&gt;Stefan Goetze&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fraunhofer SIT at CheckThat! 2023: Tackling Classification Uncertainty Using Model Souping on the Example of Check-Worthiness Classification. (arXiv:2307.02377v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.02377</id>
        <link href="http://arxiv.org/abs/2307.02377"/>
        <updated>2023-07-29T00:48:57.582Z</updated>
        <summary type="html"><![CDATA[This paper describes the second-placed approach developed by the Fraunhofer
SIT team in the CLEF-2023 CheckThat! lab Task 1B for English. Given a text
snippet from a political debate, the aim of this task is to determine whether
it should be assessed for check-worthiness. Detecting check-worthy statements
aims to facilitate manual fact-checking efforts by prioritizing the claims that
fact-checkers should consider first. It can also be considered as primary step
of a fact-checking system. Our best-performing method took advantage of an
ensemble classification scheme centered on Model Souping. When applied to the
English data set, our submitted model achieved an overall F1 score of 0.878 and
was ranked as the second-best model in the competition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frick_R/0/1/0/all/0/1"&gt;Raphael Frick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogel_I/0/1/0/all/0/1"&gt;Inna Vogel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jeong-Eun Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Factor Fields: A Unified Framework for Neural Fields and Beyond. (arXiv:2302.01226v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.01226</id>
        <link href="http://arxiv.org/abs/2302.01226"/>
        <updated>2023-07-29T00:48:57.581Z</updated>
        <summary type="html"><![CDATA[We present Factor Fields, a novel framework for modeling and representing
signals. Factor Fields decomposes a signal into a product of factors, each
represented by a classical or neural field representation which operates on
transformed input coordinates. This decomposition results in a unified
framework that accommodates several recent signal representations including
NeRF, Plenoxels, EG3D, Instant-NGP, and TensoRF. Additionally, our framework
allows for the creation of powerful new signal representations, such as the
"Dictionary Field" (DiF) which is a second contribution of this paper. Our
experiments show that DiF leads to improvements in approximation quality,
compactness, and training time when compared to previous fast reconstruction
methods. Experimentally, our representation achieves better image approximation
quality on 2D image regression tasks, higher geometric quality when
reconstructing 3D signed distance fields, and higher compactness for radiance
field reconstruction tasks. Furthermore, DiF enables generalization to unseen
images/3D scenes by sharing bases across signals during training which greatly
benefits use cases such as image regression from sparse observations and
few-shot radiance field reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Anpei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zexiang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xinyue Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Siyu Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hao Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1"&gt;Andreas Geiger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fraunhofer SIT at CheckThat! 2023: Mixing Single-Modal Classifiers to Estimate the Check-Worthiness of Multi-Modal Tweets. (arXiv:2307.00610v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.00610</id>
        <link href="http://arxiv.org/abs/2307.00610"/>
        <updated>2023-07-29T00:48:57.581Z</updated>
        <summary type="html"><![CDATA[The option of sharing images, videos and audio files on social media opens up
new possibilities for distinguishing between false information and fake news on
the Internet. Due to the vast amount of data shared every second on social
media, not all data can be verified by a computer or a human expert. Here, a
check-worthiness analysis can be used as a first step in the fact-checking
pipeline and as a filtering mechanism to improve efficiency. This paper
proposes a novel way of detecting the check-worthiness in multi-modal tweets.
It takes advantage of two classifiers, each trained on a single modality. For
image data, extracting the embedded text with an OCR analysis has shown to
perform best. By combining the two classifiers, the proposed solution was able
to place first in the CheckThat! 2023 Task 1A with an F1 score of 0.7297
achieved on the private test set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frick_R/0/1/0/all/0/1"&gt;Raphael Frick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogel_I/0/1/0/all/0/1"&gt;Inna Vogel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Formulation Graphs for Mapping Structure-Composition of Battery Electrolytes to Device Performance. (arXiv:2307.03811v2 [cond-mat.mtrl-sci] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.03811</id>
        <link href="http://arxiv.org/abs/2307.03811"/>
        <updated>2023-07-29T00:48:57.581Z</updated>
        <summary type="html"><![CDATA[Advanced computational methods are being actively sought for addressing the
challenges associated with discovery and development of new combinatorial
material such as formulations. A widely adopted approach involves domain
informed high-throughput screening of individual components that can be
combined into a formulation. This manages to accelerate the discovery of new
compounds for a target application but still leave the process of identifying
the right 'formulation' from the shortlisted chemical space largely a
laboratory experiment-driven process. We report a deep learning model,
Formulation Graph Convolution Network (F-GCN), that can map
structure-composition relationship of the individual components to the property
of liquid formulation as whole. Multiple GCNs are assembled in parallel that
featurize formulation constituents domain-intuitively on the fly. The resulting
molecular descriptors are scaled based on respective constituent's molar
percentage in the formulation, followed by formalizing into a combined
descriptor that represents a complete formulation to an external learning
architecture. The use case of proposed formulation learning model is
demonstrated for battery electrolytes by training and testing it on two
exemplary datasets representing electrolyte formulations vs battery performance
-- one dataset is sourced from literature about Li/Cu half-cells, while the
other is obtained by lab-experiments related to lithium-iodide full-cell
chemistry. The model is shown to predict the performance metrics like Coulombic
Efficiency (CE) and specific capacity of new electrolyte formulations with
lowest reported errors. The best performing F-GCN model uses molecular
descriptors derived from molecular graphs that are informed with HOMO-LUMO and
electric moment properties of the molecules using a knowledge transfer
technique.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Sharma_V/0/1/0/all/0/1"&gt;Vidushi Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Giammona_M/0/1/0/all/0/1"&gt;Maxwell Giammona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Zubarev_D/0/1/0/all/0/1"&gt;Dmitry Zubarev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Tek_A/0/1/0/all/0/1"&gt;Andy Tek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Nugyuen_K/0/1/0/all/0/1"&gt;Khanh Nugyuen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Sundberg_L/0/1/0/all/0/1"&gt;Linda Sundberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Congiu_D/0/1/0/all/0/1"&gt;Daniele Congiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+La_Y/0/1/0/all/0/1"&gt;Young-Hye La&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Experimental Study on Reinforcement Learning-based Control of an Acrobot. (arXiv:2011.09246v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09246</id>
        <link href="http://arxiv.org/abs/2011.09246"/>
        <updated>2023-07-29T00:48:57.580Z</updated>
        <summary type="html"><![CDATA[We present computational and experimental results on how artificial
intelligence (AI) learns to control an Acrobot using reinforcement learning
(RL). Thereby the experimental setup is designed as an embedded system, which
is of interest for robotics and energy harvesting applications. Specifically,
we study the control of angular velocity of the Acrobot, as well as control of
its total energy, which is the sum of the kinetic and the potential energy. By
this means the RL algorithm is designed to drive the angular velocity or the
energy of the first pendulum of the Acrobot towards a desired value. With this,
libration or full rotation of the unactuated pendulum of the Acrobot is
achieved. Moreover, investigations of the Acrobot control are carried out,
which lead to insights about the influence of the state space discretization,
the episode length, the action space or the mass of the driven pendulum on the
RL control. By further numerous simulations and experiments the effects of
parameter variations are evaluated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dostal_L/0/1/0/all/0/1"&gt;Leo Dostal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bespalko_A/0/1/0/all/0/1"&gt;Alexej Bespalko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duecker_D/0/1/0/all/0/1"&gt;Daniel A. Duecker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items. (arXiv:2307.13709v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.13709</id>
        <link href="http://arxiv.org/abs/2307.13709"/>
        <updated>2023-07-29T00:48:57.580Z</updated>
        <summary type="html"><![CDATA[Many properties in the real world, such as desirability or strength in
competitive environment, can't be directly observed, which makes them difficult
to evaluate. To deal with this challenging problem, prior works have primarily
focused on estimating those properties of known items, especially the strength
of sports players, only of those who appears in paired comparison dataset. In
this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework
to evaluate any properties of unknown items, not necessarily present in the
training data. Our method seamlessly integrates traditional Bradley-Terry model
with a neural network structure. We also generalizes this architecture further
for asymmetric environment with unfairness, which is much more common in real
world settings. In our experimental analysis, DBTR successfully learned desired
quantification of those properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fujii_S/0/1/0/all/0/1"&gt;Satoru Fujii&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning of quantum entanglement from incomplete measurements. (arXiv:2205.01462v6 [quant-ph] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2205.01462</id>
        <link href="http://arxiv.org/abs/2205.01462"/>
        <updated>2023-07-29T00:48:57.580Z</updated>
        <summary type="html"><![CDATA[The quantification of the entanglement present in a physical system is of
para\-mount importance for fundamental research and many cutting-edge
applications. Currently, achieving this goal requires either a priori knowledge
on the system or very demanding experimental procedures such as full state
tomography or collective measurements. Here, we demonstrate that by employing
neural networks we can quantify the degree of entanglement without needing to
know the full description of the quantum state. Our method allows for direct
quantification of the quantum correlations using an incomplete set of local
measurements. Despite using undersampled measurements, we achieve a
quantification error of up to an order of magnitude lower than the
state-of-the-art quantum tomography. Furthermore, we achieve this result
employing networks trained using exclusively simulated data. Finally, we derive
a method based on a convolutional network input that can accept data from
various measurement scenarios and perform, to some extent, independently of the
measurement device.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Koutny_D/0/1/0/all/0/1"&gt;Dominik Koutn&amp;#xfd;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Gines_L/0/1/0/all/0/1"&gt;Laia Gin&amp;#xe9;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Moczala_Dusanowska_M/0/1/0/all/0/1"&gt;Magdalena Mocza&amp;#x142;a-Dusanowska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Hofling_S/0/1/0/all/0/1"&gt;Sven H&amp;#xf6;fling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Schneider_C/0/1/0/all/0/1"&gt;Christian Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Predojevic_A/0/1/0/all/0/1"&gt;Ana Predojevi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Jezek_M/0/1/0/all/0/1"&gt;Miroslav Je&amp;#x17e;ek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithmic Gaussianization through Sketching: Converting Data into Sub-gaussian Random Designs. (arXiv:2206.10291v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2206.10291</id>
        <link href="http://arxiv.org/abs/2206.10291"/>
        <updated>2023-07-29T00:48:57.579Z</updated>
        <summary type="html"><![CDATA[Algorithmic Gaussianization is a phenomenon that can arise when using
randomized sketching or sampling methods to produce smaller representations of
large datasets: For certain tasks, these sketched representations have been
observed to exhibit many robust performance characteristics that are known to
occur when a data sample comes from a sub-gaussian random design, which is a
powerful statistical model of data distributions. However, this phenomenon has
only been studied for specific tasks and metrics, or by relying on
computationally expensive methods. We address this by providing an algorithmic
framework for gaussianizing data distributions via averaging, proving that it
is possible to efficiently construct data sketches that are nearly
indistinguishable (in terms of total variation distance) from sub-gaussian
random designs. In particular, relying on a recently introduced sketching
technique called Leverage Score Sparsified (LESS) embeddings, we show that one
can construct an $n\times d$ sketch of an $N\times d$ matrix $A$, where $n\ll
N$, that is nearly indistinguishable from a sub-gaussian design, in time
$O(\text{nnz}(A)\log N + nd^2)$, where $\text{nnz}(A)$ is the number of
non-zero entries in $A$. As a consequence, strong statistical guarantees and
precise asymptotics available for the estimators produced from sub-gaussian
designs (e.g., for least squares and Lasso regression, covariance estimation,
low-rank approximation, etc.) can be straightforwardly adapted to our sketching
framework. We illustrate this with a new approximation guarantee for sketched
least squares, among other examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Derezinski_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Derezi&amp;#x144;ski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group Equivariant Fourier Neural Operators for Partial Differential Equations. (arXiv:2306.05697v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.05697</id>
        <link href="http://arxiv.org/abs/2306.05697"/>
        <updated>2023-07-29T00:48:57.579Z</updated>
        <summary type="html"><![CDATA[We consider solving partial differential equations (PDEs) with Fourier neural
operators (FNOs), which operate in the frequency domain. Since the laws of
physics do not depend on the coordinate system used to describe them, it is
desirable to encode such symmetries in the neural operator architecture for
better performance and easier learning. While encoding symmetries in the
physical domain using group theory has been studied extensively, how to capture
symmetries in the frequency domain is under-explored. In this work, we extend
group convolutions to the frequency domain and design Fourier layers that are
equivariant to rotations, translations, and reflections by leveraging the
equivariance property of the Fourier transform. The resulting $G$-FNO
architecture generalizes well across input resolutions and performs well in
settings with varying levels of symmetry. Our code is publicly available as
part of the AIRS library (https://github.com/divelab/AIRS).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Helwig_J/0/1/0/all/0/1"&gt;Jacob Helwig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Cong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurtin_J/0/1/0/all/0/1"&gt;Jerry Kurtin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wojtowytsch_S/0/1/0/all/0/1"&gt;Stephan Wojtowytsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shuiwang Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Common Rationale to Improve Self-Supervised Representation for Fine-Grained Visual Recognition Problems. (arXiv:2303.01669v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.01669</id>
        <link href="http://arxiv.org/abs/2303.01669"/>
        <updated>2023-07-29T00:48:57.578Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning (SSL) strategies have demonstrated remarkable
performance in various recognition tasks. However, both our preliminary
investigation and recent studies suggest that they may be less effective in
learning representations for fine-grained visual recognition (FGVR) since many
features helpful for optimizing SSL objectives are not suitable for
characterizing the subtle differences in FGVR. To overcome this issue, we
propose learning an additional screening mechanism to identify discriminative
clues commonly seen across instances and classes, dubbed as common rationales
in this paper. Intuitively, common rationales tend to correspond to the
discriminative patterns from the key parts of foreground objects. We show that
a common rationale detector can be learned by simply exploiting the GradCAM
induced from the SSL objective without using any pre-trained object parts or
saliency detectors, making it seamlessly to be integrated with the existing SSL
process. Specifically, we fit the GradCAM with a branch with limited fitting
capacity, which allows the branch to capture the common rationales and discard
the less common discriminative patterns. At the test stage, the branch
generates a set of spatial weights to selectively aggregate features
representing an instance. Extensive experimental results on four visual tasks
demonstrate that the proposed method can lead to a significant improvement in
different evaluation settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1"&gt;Yangyang Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1"&gt;Anton van den Hengel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingqiao Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reasons for the Superiority of Stochastic Estimators over Deterministic Ones: Robustness, Consistency and Perceptual Quality. (arXiv:2211.08944v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.08944</id>
        <link href="http://arxiv.org/abs/2211.08944"/>
        <updated>2023-07-29T00:48:57.577Z</updated>
        <summary type="html"><![CDATA[Stochastic restoration algorithms allow to explore the space of solutions
that correspond to the degraded input. In this paper we reveal additional
fundamental advantages of stochastic methods over deterministic ones, which
further motivate their use. First, we prove that any restoration algorithm that
attains perfect perceptual quality and whose outputs are consistent with the
input must be a posterior sampler, and is thus required to be stochastic.
Second, we illustrate that while deterministic restoration algorithms may
attain high perceptual quality, this can be achieved only by filling up the
space of all possible source images using an extremely sensitive mapping, which
makes them highly vulnerable to adversarial attacks. Indeed, we show that
enforcing deterministic models to be robust to such attacks profoundly hinders
their perceptual quality, while robustifying stochastic models hardly
influences their perceptual quality, and improves their output variability.
These findings provide a motivation to foster progress in stochastic
restoration methods, paving the way to better recovery algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ohayon_G/0/1/0/all/0/1"&gt;Guy Ohayon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Adrai_T/0/1/0/all/0/1"&gt;Theo Adrai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Elad_M/0/1/0/all/0/1"&gt;Michael Elad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Michaeli_T/0/1/0/all/0/1"&gt;Tomer Michaeli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Bayesian approach to quantifying uncertainties and improving generalizability in traffic prediction models. (arXiv:2307.05946v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.05946</id>
        <link href="http://arxiv.org/abs/2307.05946"/>
        <updated>2023-07-29T00:48:57.577Z</updated>
        <summary type="html"><![CDATA[Deep-learning models for traffic data prediction can have superior
performance in modeling complex functions using a multi-layer architecture.
However, a major drawback of these approaches is that most of these approaches
do not offer forecasts with uncertainty estimates, which are essential for
traffic operations and control. Without uncertainty estimates, it is difficult
to place any level of trust to the model predictions, and operational
strategies relying on overconfident predictions can lead to worsening traffic
conditions. In this study, we propose a Bayesian recurrent neural network
framework for uncertainty quantification in traffic prediction with higher
generalizability by introducing spectral normalization to its hidden layers. In
our paper, we have shown that normalization alters the training process of deep
neural networks by controlling the model's complexity and reducing the risk of
overfitting to the training data. This, in turn, helps improve the
generalization performance of the model on out-of-distribution datasets.
Results demonstrate that spectral normalization improves uncertainty estimates
and significantly outperforms both the layer normalization and model without
normalization in single-step prediction horizons. This improved performance can
be attributed to the ability of spectral normalization to better localize the
feature space of the data under perturbations. Our findings are especially
relevant to traffic management applications, where predicting traffic
conditions across multiple locations is the goal, but the availability of
training data from multiple locations is limited. Spectral normalization,
therefore, provides a more generalizable approach that can effectively capture
the underlying patterns in traffic data without requiring location-specific
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sengupta_A/0/1/0/all/0/1"&gt;Agnimitra Sengupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mondal_S/0/1/0/all/0/1"&gt;Sudeepta Mondal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1"&gt;Adway Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guler_S/0/1/0/all/0/1"&gt;S. Ilgin Guler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Pre-training for Navigation: What Can We Learn from Noise?. (arXiv:2207.00052v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2207.00052</id>
        <link href="http://arxiv.org/abs/2207.00052"/>
        <updated>2023-07-29T00:48:57.576Z</updated>
        <summary type="html"><![CDATA[One powerful paradigm in visual navigation is to predict actions from
observations directly. Training such an end-to-end system allows
representations useful for downstream tasks to emerge automatically. However,
the lack of inductive bias makes this system data inefficient. We hypothesize a
sufficient representation of the current view and the goal view for a
navigation policy can be learned by predicting the location and size of a crop
of the current view that corresponds to the goal. We further show that training
such random crop prediction in a self-supervised fashion purely on synthetic
noise images transfers well to natural home images. The learned representation
can then be bootstrapped to learn a navigation policy efficiently with little
interaction data. The code is available at https://yanweiw.github.io/noise2ptz]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_C/0/1/0/all/0/1"&gt;Ching-Yun Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1"&gt;Pulkit Agrawal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-Training with Diffusion models for Dental Radiography segmentation. (arXiv:2307.14066v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.14066</id>
        <link href="http://arxiv.org/abs/2307.14066"/>
        <updated>2023-07-29T00:48:57.576Z</updated>
        <summary type="html"><![CDATA[Medical radiography segmentation, and specifically dental radiography, is
highly limited by the cost of labeling which requires specific expertise and
labor-intensive annotations. In this work, we propose a straightforward
pre-training method for semantic segmentation leveraging Denoising Diffusion
Probabilistic Models (DDPM), which have shown impressive results for generative
modeling. Our straightforward approach achieves remarkable performance in terms
of label efficiency and does not require architectural modifications between
pre-training and downstream tasks. We propose to first pre-train a Unet by
exploiting the DDPM training objective, and then fine-tune the resulting model
on a segmentation task. Our experimental results on the segmentation of dental
radiographs demonstrate that the proposed method is competitive with
state-of-the-art pre-training methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rousseau_J/0/1/0/all/0/1"&gt;J&amp;#xe9;r&amp;#xe9;my Rousseau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alaka_C/0/1/0/all/0/1"&gt;Christian Alaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Covili_E/0/1/0/all/0/1"&gt;Emma Covili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mayard_H/0/1/0/all/0/1"&gt;Hippolyte Mayard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misrachi_L/0/1/0/all/0/1"&gt;Laura Misrachi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Au_W/0/1/0/all/0/1"&gt;Willy Au&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Out-Of-Distribution Generalization: A Survey. (arXiv:2108.13624v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.13624</id>
        <link href="http://arxiv.org/abs/2108.13624"/>
        <updated>2023-07-29T00:48:57.575Z</updated>
        <summary type="html"><![CDATA[Traditional machine learning paradigms are based on the assumption that both
training and test data follow the same statistical pattern, which is
mathematically referred to as Independent and Identically Distributed
($i.i.d.$). However, in real-world applications, this $i.i.d.$ assumption often
fails to hold due to unforeseen distributional shifts, leading to considerable
degradation in model performance upon deployment. This observed discrepancy
indicates the significance of investigating the Out-of-Distribution (OOD)
generalization problem. OOD generalization is an emerging topic of machine
learning research that focuses on complex scenarios wherein the distributions
of the test data differ from those of the training data. This paper represents
the first comprehensive, systematic review of OOD generalization, encompassing
a spectrum of aspects from problem definition, methodological development, and
evaluation procedures, to the implications and future directions of the field.
Our discussion begins with a precise, formal characterization of the OOD
generalization problem. Following that, we categorize existing methodologies
into three segments: unsupervised representation learning, supervised model
learning, and optimization, according to their positions within the overarching
learning process. We provide an in-depth discussion on representative
methodologies for each category, further elucidating the theoretical links
between them. Subsequently, we outline the prevailing benchmark datasets
employed in OOD generalization studies. To conclude, we overview the existing
body of work in this domain and suggest potential avenues for future research
on OOD generalization. A summary of the OOD generalization methodologies
surveyed in this paper can be accessed at
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiashuo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zheyan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yue He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xingxuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Renzhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Han Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1"&gt;Peng Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical process monitoring of artificial neural networks. (arXiv:2209.07436v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2209.07436</id>
        <link href="http://arxiv.org/abs/2209.07436"/>
        <updated>2023-07-29T00:48:57.571Z</updated>
        <summary type="html"><![CDATA[The rapid advancement of models based on artificial intelligence demands
innovative monitoring techniques which can operate in real time with low
computational costs. In machine learning, especially if we consider artificial
neural networks (ANNs), the models are often trained in a supervised manner.
Consequently, the learned relationship between the input and the output must
remain valid during the model's deployment. If this stationarity assumption
holds, we can conclude that the ANN provides accurate predictions. Otherwise,
the retraining or rebuilding of the model is required. We propose considering
the latent feature representation of the data (called "embedding") generated by
the ANN to determine the time when the data stream starts being nonstationary.
In particular, we monitor embeddings by applying multivariate control charts
based on the data depth calculation and normalized ranks. The performance of
the introduced method is compared with benchmark approaches for various ANN
architectures and different underlying data formats.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Malinovskaya_A/0/1/0/all/0/1"&gt;Anna Malinovskaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mozharovskyi_P/0/1/0/all/0/1"&gt;Pavlo Mozharovskyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Otto_P/0/1/0/all/0/1"&gt;Philipp Otto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Alternating Minimization with Applications to Weighted Low Rank Approximation. (arXiv:2306.04169v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.04169</id>
        <link href="http://arxiv.org/abs/2306.04169"/>
        <updated>2023-07-29T00:48:57.571Z</updated>
        <summary type="html"><![CDATA[Weighted low rank approximation is a fundamental problem in numerical linear
algebra, and it has many applications in machine learning. Given a matrix $M
\in \mathbb{R}^{n \times n}$, a weight matrix $W \in \mathbb{R}_{\geq 0}^{n
\times n}$, a parameter $k$, the goal is to output two matrices $U, V \in
\mathbb{R}^{n \times k}$ such that $\| W \circ (M - U V^\top) \|_F$ is
minimized, where $\circ$ denotes the Hadamard product. Such a problem is known
to be NP-hard and even hard to approximate assuming Exponential Time Hypothesis
[GG11, RSW16]. Meanwhile, alternating minimization is a good heuristic solution
for approximating weighted low rank approximation. The work [LLR16] shows that,
under mild assumptions, alternating minimization does provide provable
guarantees. In this work, we develop an efficient and robust framework for
alternating minimization. For weighted low rank approximation, this improves
the runtime of [LLR16] from $n^2 k^2$ to $n^2k$. At the heart of our work
framework is a high-accuracy multiple response regression solver together with
a robust analysis of alternating minimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1"&gt;Zhao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1"&gt;Mingquan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Junze Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lichen Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automating Model Comparison in Factor Graphs. (arXiv:2306.05965v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.05965</id>
        <link href="http://arxiv.org/abs/2306.05965"/>
        <updated>2023-07-29T00:48:57.570Z</updated>
        <summary type="html"><![CDATA[Bayesian state and parameter estimation have been automated effectively in a
variety of probabilistic programming languages. The process of model comparison
on the other hand, which still requires error-prone and time-consuming manual
derivations, is often overlooked despite its importance. This paper efficiently
automates Bayesian model averaging, selection, and combination by message
passing on a Forney-style factor graph with a custom mixture node. Parameter
and state inference, and model comparison can then be executed simultaneously
using message passing with scale factors. This approach shortens the model
design cycle and allows for the straightforward extension to hierarchical and
temporal model priors to accommodate for modeling complicated time-varying
processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Erp_B/0/1/0/all/0/1"&gt;Bart van Erp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nuijten_W/0/1/0/all/0/1"&gt;Wouter W. L. Nuijten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laar_T/0/1/0/all/0/1"&gt;Thijs van de Laar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vries_B/0/1/0/all/0/1"&gt;Bert de Vries&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Learning the Tail Quantiles of Driving Behavior Distributions via Quantile Regression and Flows. (arXiv:2305.13106v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.13106</id>
        <link href="http://arxiv.org/abs/2305.13106"/>
        <updated>2023-07-29T00:48:57.569Z</updated>
        <summary type="html"><![CDATA[Towards safe autonomous driving (AD), we consider the problem of learning
models that accurately capture the diversity and tail quantiles of human driver
behavior probability distributions, in interaction with an AD vehicle. Such
models, which predict drivers' continuous actions from their states, are
particularly relevant for closing the gap between AD agent simulations and
reality. To this end, we adapt two flexible quantile learning frameworks for
this setting that avoid strong distributional assumptions: (1) quantile
regression (based on the titled absolute loss), and (2) autoregressive quantile
flows (a version of normalizing flows). Training happens in a behavior
cloning-fashion. We use the highD dataset consisting of driver trajectories on
several highways. We evaluate our approach in a one-step acceleration
prediction task, and in multi-step driver simulation rollouts. We report
quantitative results using the tilted absolute loss as metric, give qualitative
examples showing that realistic extremal behavior can be learned, and discuss
the main insights.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tee_J/0/1/0/all/0/1"&gt;Jia Yu Tee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Candido_O/0/1/0/all/0/1"&gt;Oliver De Candido&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Utschick_W/0/1/0/all/0/1"&gt;Wolfgang Utschick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_P/0/1/0/all/0/1"&gt;Philipp Geiger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Vulnerability of Fairness Constrained Learning to Malicious Noise. (arXiv:2307.11892v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.11892</id>
        <link href="http://arxiv.org/abs/2307.11892"/>
        <updated>2023-07-29T00:48:57.569Z</updated>
        <summary type="html"><![CDATA[We consider the vulnerability of fairness-constrained learning to small
amounts of malicious noise in the training data. Konstantinov and Lampert
(2021) initiated the study of this question and presented negative results
showing there exist data distributions where for several fairness constraints,
any proper learner will exhibit high vulnerability when group sizes are
imbalanced. Here, we present a more optimistic view, showing that if we allow
randomized classifiers, then the landscape is much more nuanced. For example,
for Demographic Parity we show we can incur only a $\Theta(\alpha)$ loss in
accuracy, where $\alpha$ is the malicious noise rate, matching the best
possible even without fairness constraints. For Equal Opportunity, we show we
can incur an $O(\sqrt{\alpha})$ loss, and give a matching
$\Omega(\sqrt{\alpha})$lower bound. In contrast, Konstantinov and Lampert
(2021) showed for proper learners the loss in accuracy for both notions is
$\Omega(1)$. The key technical novelty of our work is how randomization can
bypass simple "tricks" an adversary can use to amplify his power. We also
consider additional fairness notions including Equalized Odds and Calibration.
For these fairness notions, the excess accuracy clusters into three natural
regimes $O(\alpha)$,$O(\sqrt{\alpha})$ and $O(1)$. These results provide a more
fine-grained view of the sensitivity of fairness-constrained learning to
adversarial noise in training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blum_A/0/1/0/all/0/1"&gt;Avrim Blum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okoroafor_P/0/1/0/all/0/1"&gt;Princewill Okoroafor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1"&gt;Aadirupa Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stangl_K/0/1/0/all/0/1"&gt;Kevin Stangl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Interaction-Aware Interval Analysis of Neural Network Feedback Loops. (arXiv:2307.14938v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2307.14938</id>
        <link href="http://arxiv.org/abs/2307.14938"/>
        <updated>2023-07-29T00:48:57.568Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a computationally efficient framework for interval
reachability of neural network controlled systems. Our approach builds upon
inclusion functions for the neural network controller and the open-loop system.
We observe that many state-of-the-art neural network verifiers can produce
inclusion functions for neural networks. We introduce and analyze a new class
of inclusion functions for the open-loop dynamics based on bounds of the
function Jacobian that is particularly suitable for capturing the interactions
between systems and neural network controllers. Next, for any dynamical system,
we use inclusion functions to construct an embedding system with twice the
number of states as the original system. We show that a single trajectory of
this embedding system provides hyper-rectangular over-approximations of
reachable sets. We then propose two approaches for constructing a closed-loop
embedding system for a neural network controlled dynamical system that accounts
for the interaction between the system and the controller in different ways.
The interconnection-based approach accounts for the worst-case evolution of
each coordinate separately by substituting the neural network inclusion
function into the open-loop embedding system. The interaction-based approach
uses the newly introduced class of Jacobian-based inclusion functions to fully
capture first-order interactions between the system and the controller.
Finally, we implement our approach in a Python framework called
\texttt{ReachMM} and show that on several existing benchmarks, our methods
outperform the existing approaches in the literature. We also demonstrate the
scalability of our method on a vehicle platooning example with up to $200$
states.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jafarpour_S/0/1/0/all/0/1"&gt;Saber Jafarpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Harapanahalli_A/0/1/0/all/0/1"&gt;Akash Harapanahalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Coogan_S/0/1/0/all/0/1"&gt;Samuel Coogan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonsmooth Nonconvex-Nonconcave Minimax Optimization: Primal-Dual Balancing and Iteration Complexity Analysis. (arXiv:2209.10825v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2209.10825</id>
        <link href="http://arxiv.org/abs/2209.10825"/>
        <updated>2023-07-29T00:48:57.568Z</updated>
        <summary type="html"><![CDATA[Nonconvex-nonconcave minimax optimization has gained widespread interest over
the last decade. However, most existing works focus on variants of gradient
descent-ascent (GDA) algorithms, which are only applicable to smooth
nonconvex-concave settings. To address this limitation, we propose a novel
algorithm named smoothed proximal linear descent-ascent (smoothed PLDA), which
can effectively handle a broad range of structured nonsmooth
nonconvex-nonconcave minimax problems. Specifically, we consider the setting
where the primal function has a nonsmooth composite structure and the dual
function possesses the Kurdyka-Lojasiewicz (KL) property with exponent $\theta
\in [0,1)$. We introduce a novel convergence analysis framework for smoothed
PLDA, the key components of which are our newly developed nonsmooth primal
error bound and dual error bound. Using this framework, we show that smoothed
PLDA can find both $\epsilon$-game-stationary points and
$\epsilon$-optimization-stationary points of the problems of interest in
$\mathcal{O}(\epsilon^{-2\max\{2\theta,1\}})$ iterations. Furthermore, when
$\theta \in [0,\frac{1}{2}]$, smoothed PLDA achieves the optimal iteration
complexity of $\mathcal{O}(\epsilon^{-2})$. To further demonstrate the
effectiveness and wide applicability of our analysis framework, we show that
certain max-structured problem possesses the KL property with exponent
$\theta=0$ under mild assumptions. As a by-product, we establish
algorithm-independent quantitative relationships among various stationarity
concepts, which may be of independent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiajin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Linglingzhi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+So_A/0/1/0/all/0/1"&gt;Anthony Man-Cho So&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Richness of Learned Compressed Representation of Images for Semantic Segmentation. (arXiv:2307.01524v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.01524</id>
        <link href="http://arxiv.org/abs/2307.01524"/>
        <updated>2023-07-29T00:48:57.567Z</updated>
        <summary type="html"><![CDATA[Autonomous vehicles and Advanced Driving Assistance Systems (ADAS) have the
potential to radically change the way we travel. Many such vehicles currently
rely on segmentation and object detection algorithms to detect and track
objects around its surrounding. The data collected from the vehicles are often
sent to cloud servers to facilitate continual/life-long learning of these
algorithms. Considering the bandwidth constraints, the data is compressed
before sending it to servers, where it is typically decompressed for training
and analysis. In this work, we propose the use of a learning-based compression
Codec to reduce the overhead in latency incurred for the decompression
operation in the standard pipeline. We demonstrate that the learned compressed
representation can also be used to perform tasks like semantic segmentation in
addition to decompression to obtain the images. We experimentally validate the
proposed pipeline on the Cityscapes dataset, where we achieve a compression
factor up to $66 \times$ while preserving the information required to perform
segmentation with a dice coefficient of $0.84$ as compared to $0.88$ achieved
using decompressed images while reducing the overall compute by $11\%$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kakaiya_R/0/1/0/all/0/1"&gt;Ravi Kakaiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sathish_R/0/1/0/all/0/1"&gt;Rakshith Sathish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sethuraman_R/0/1/0/all/0/1"&gt;Ramanathan Sethuraman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheet_D/0/1/0/all/0/1"&gt;Debdoot Sheet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Transfer Operators by Kernel Density Estimation. (arXiv:2210.03124v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.03124</id>
        <link href="http://arxiv.org/abs/2210.03124"/>
        <updated>2023-07-29T00:48:57.563Z</updated>
        <summary type="html"><![CDATA[Inference of transfer operators from data is often formulated as a classical
problem that hinges on the Ulam method. The conventional description, known as
the Ulam-Galerkin method, involves projecting onto basis functions represented
as characteristic functions supported over a fine grid of rectangles. From this
perspective, the Ulam-Galerkin approach can be interpreted as density
estimation using the histogram method. In this study, we recast the problem
within the framework of statistical density estimation. This alternative
perspective allows for an explicit and rigorous analysis of bias and variance,
thereby facilitating a discussion on the mean square error. Through
comprehensive examples utilizing the logistic map and a Markov map, we
demonstrate the validity and effectiveness of this approach in estimating the
eigenvectors of the Frobenius-Perron operator. We compare the performance of
Histogram Density Estimation(HDE) and Kernel Density Estimation(KDE) methods
and find that KDE generally outperforms HDE in terms of accuracy. However, it
is important to note that KDE exhibits limitations around boundary points and
jumps. Based on our research findings, we suggest the possibility of
incorporating other density estimation methods into this field and propose
future investigations into the application of KDE-based estimation for
high-dimensional maps. These findings provide valuable insights for researchers
and practitioners working on estimating the Frobenius-Perron operator and
highlight the potential of density estimation techniques in this area of study.

Keywords: Transfer Operators; Frobenius-Perron operator; probability density
estimation; Ulam-Galerkin method; Kernel Density Estimation; Histogram Density
Estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Surasinghe_S/0/1/0/all/0/1"&gt;Sudam Surasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fish_J/0/1/0/all/0/1"&gt;Jeremie Fish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bollt_E/0/1/0/all/0/1"&gt;Erik M. Bollt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Bayesian Uncertainty Quantification for Neural Network Potentials: Promise and Pitfalls. (arXiv:2212.07959v2 [physics.chem-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.07959</id>
        <link href="http://arxiv.org/abs/2212.07959"/>
        <updated>2023-07-29T00:48:57.557Z</updated>
        <summary type="html"><![CDATA[Neural network (NN) potentials promise highly accurate molecular dynamics
(MD) simulations within the computational complexity of classical MD force
fields. However, when applied outside their training domain, NN potential
predictions can be inaccurate, increasing the need for Uncertainty
Quantification (UQ). Bayesian modeling provides the mathematical framework for
UQ, but classical Bayesian methods based on Markov chain Monte Carlo (MCMC) are
computationally intractable for NN potentials. By training graph NN potentials
for coarse-grained systems of liquid water and alanine dipeptide, we
demonstrate here that scalable Bayesian UQ via stochastic gradient MCMC
(SG-MCMC) yields reliable uncertainty estimates for MD observables. We show
that cold posteriors can reduce the required training data size and that for
reliable UQ, multiple Markov chains are needed. Additionally, we find that
SG-MCMC and the Deep Ensemble method achieve comparable results, despite
shorter training and less hyperparameter tuning of the latter. We show that
both methods can capture aleatoric and epistemic uncertainty reliably, but not
systematic uncertainty, which needs to be minimized by adequate modeling to
obtain accurate credible intervals for MD observables. Our results represent a
step towards accurate UQ that is of vital importance for trustworthy NN
potential-based MD simulations required for decision-making in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Thaler_S/0/1/0/all/0/1"&gt;Stephan Thaler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Doehner_G/0/1/0/all/0/1"&gt;Gregor Doehner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zavadlav_J/0/1/0/all/0/1"&gt;Julija Zavadlav&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VeML: An End-to-End Machine Learning Lifecycle for Large-scale and High-dimensional Data. (arXiv:2304.13037v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.13037</id>
        <link href="http://arxiv.org/abs/2304.13037"/>
        <updated>2023-07-29T00:48:57.557Z</updated>
        <summary type="html"><![CDATA[An end-to-end machine learning (ML) lifecycle consists of many iterative
processes, from data preparation and ML model design to model training and then
deploying the trained model for inference. When building an end-to-end
lifecycle for an ML problem, many ML pipelines must be designed and executed
that produce a huge number of lifecycle versions. Therefore, this paper
introduces VeML, a Version management system dedicated to end-to-end ML
Lifecycle. Our system tackles several crucial problems that other systems have
not solved. First, we address the high cost of building an ML lifecycle,
especially for large-scale and high-dimensional dataset. We solve this problem
by proposing to transfer the lifecycle of similar datasets managed in our
system to the new training data. We design an algorithm based on the core set
to compute similarity for large-scale, high-dimensional data efficiently.
Another critical issue is the model accuracy degradation by the difference
between training data and testing data during the ML lifetime, which leads to
lifecycle rebuild. Our system helps to detect this mismatch without getting
labeled data from testing data and rebuild the ML lifecycle for a new data
version. To demonstrate our contributions, we conduct experiments on
real-world, large-scale datasets of driving images and spatiotemporal sensor
data and show promising results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1"&gt;Van-Duc Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_C/0/1/0/all/0/1"&gt;Cuong-Tien Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wen-Syan Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differential Convolutional Fuzzy Time Series Forecasting. (arXiv:2305.08890v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.08890</id>
        <link href="http://arxiv.org/abs/2305.08890"/>
        <updated>2023-07-29T00:48:57.557Z</updated>
        <summary type="html"><![CDATA[Fuzzy time series forecasting (FTSF) is a typical forecasting method with
wide application. Traditional FTSF is regarded as an expert system which leads
to loss of the ability to recognize undefined features. The mentioned is the
main reason for poor forecasting with FTSF. To solve the problem, the proposed
model Differential Fuzzy Convolutional Neural Network (DFCNN) utilizes a
convolution neural network to re-implement FTSF with learnable ability. DFCNN
is capable of recognizing potential information and improving forecasting
accuracy. Thanks to the learnable ability of the neural network, the length of
fuzzy rules established in FTSF is expended to an arbitrary length that the
expert is not able to handle by the expert system. At the same time, FTSF
usually cannot achieve satisfactory performance of non-stationary time series
due to the trend of non-stationary time series. The trend of non-stationary
time series causes the fuzzy set established by FTSF to be invalid and causes
the forecasting to fail. DFCNN utilizes the Difference algorithm to weaken the
non-stationary of time series so that DFCNN can forecast the non-stationary
time series with a low error that FTSF cannot forecast in satisfactory
performance. After the mass of experiments, DFCNN has an excellent prediction
effect, which is ahead of the existing FTSF and common time series forecasting
algorithms. Finally, DFCNN provides further ideas for improving FTSF and holds
continued research value.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_T/0/1/0/all/0/1"&gt;Tianxiang Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yuanpeng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yong Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Harnessing Synthetic Active Particles for Physical Reservoir Computing. (arXiv:2307.15010v1 [cond-mat.soft])]]></title>
        <id>http://arxiv.org/abs/2307.15010</id>
        <link href="http://arxiv.org/abs/2307.15010"/>
        <updated>2023-07-29T00:48:57.556Z</updated>
        <summary type="html"><![CDATA[The processing of information is an indispensable property of living systems
realized by networks of active processes with enormous complexity. They have
inspired many variants of modern machine learning one of them being reservoir
computing, in which stimulating a network of nodes with fading memory enables
computations and complex predictions. Reservoirs are implemented on computer
hardware, but also on unconventional physical substrates such as mechanical
oscillators, spins, or bacteria often summarized as physical reservoir
computing. Here we demonstrate physical reservoir computing with a synthetic
active microparticle system that self-organizes from an active and passive
component into inherently noisy nonlinear dynamical units. The
self-organization and dynamical response of the unit is the result of a delayed
propulsion of the microswimmer to a passive target. A reservoir of such units
with a self-coupling via the delayed response can perform predictive tasks
despite the strong noise resulting from Brownian motion of the microswimmers.
To achieve efficient noise suppression, we introduce a special architecture
that uses historical reservoir states for output. Our results pave the way for
the study of information processing in synthetic self-organized active particle
systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiangzun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Cichos_F/0/1/0/all/0/1"&gt;Frank Cichos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedFTN: Personalized Federated Learning with Deep Feature Transformation Network for Multi-institutional Low-count PET Denoising. (arXiv:2304.00570v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.00570</id>
        <link href="http://arxiv.org/abs/2304.00570"/>
        <updated>2023-07-29T00:48:57.556Z</updated>
        <summary type="html"><![CDATA[Low-count PET is an efficient way to reduce radiation exposure and
acquisition time, but the reconstructed images often suffer from low
signal-to-noise ratio (SNR), thus affecting diagnosis and other downstream
tasks. Recent advances in deep learning have shown great potential in improving
low-count PET image quality, but acquiring a large, centralized, and diverse
dataset from multiple institutions for training a robust model is difficult due
to privacy and security concerns of patient data. Moreover, low-count PET data
at different institutions may have different data distribution, thus requiring
personalized models. While previous federated learning (FL) algorithms enable
multi-institution collaborative training without the need of aggregating local
data, addressing the large domain shift in the application of
multi-institutional low-count PET denoising remains a challenge and is still
highly under-explored. In this work, we propose FedFTN, a personalized
federated learning strategy that addresses these challenges. FedFTN uses a
local deep feature transformation network (FTN) to modulate the feature outputs
of a globally shared denoising network, enabling personalized low-count PET
denoising for each institution. During the federated learning process, only the
denoising network's weights are communicated and aggregated, while the FTN
remains at the local institutions for feature transformation. We evaluated our
method using a large-scale dataset of multi-institutional low-count PET imaging
data from three medical centers located across three continents, and showed
that FedFTN provides high-quality low-count PET images, outperforming previous
baseline FL reconstruction methods across all low-count levels at all three
institutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xie_H/0/1/0/all/0/1"&gt;Huidong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiongchao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xueqi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zhicheng Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1"&gt;S. Kevin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1"&gt;Biao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rominger_A/0/1/0/all/0/1"&gt;Axel Rominger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_K/0/1/0/all/0/1"&gt;Kuangyu Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Duncan_J/0/1/0/all/0/1"&gt;James S. Duncan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chi Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning a Generic Value-Selection Heuristic Inside a Constraint Programming Solver. (arXiv:2301.01913v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2301.01913</id>
        <link href="http://arxiv.org/abs/2301.01913"/>
        <updated>2023-07-29T00:48:57.555Z</updated>
        <summary type="html"><![CDATA[Constraint programming is known for being an efficient approach for solving
combinatorial problems. Important design choices in a solver are the branching
heuristics, which are designed to lead the search to the best solutions in a
minimum amount of time. However, developing these heuristics is a
time-consuming process that requires problem-specific expertise. This
observation has motivated many efforts to use machine learning to automatically
learn efficient heuristics without expert intervention. To the best of our
knowledge, it is still an open research question. Although several generic
variable-selection heuristics are available in the literature, the options for
a generic value-selection heuristic are more scarce. In this paper, we propose
to tackle this issue by introducing a generic learning procedure that can be
used to obtain a value-selection heuristic inside a constraint programming
solver. This has been achieved thanks to the combination of a deep Q-learning
algorithm, a tailored reward signal, and a heterogeneous graph neural network
architecture. Experiments on graph coloring, maximum independent set, and
maximum cut problems show that our framework is able to find better solutions
close to optimality without requiring a large amounts of backtracks while being
generic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marty_T/0/1/0/all/0/1"&gt;Tom Marty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Francois_T/0/1/0/all/0/1"&gt;Tristan Fran&amp;#xe7;ois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tessier_P/0/1/0/all/0/1"&gt;Pierre Tessier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gauthier_L/0/1/0/all/0/1"&gt;Louis Gauthier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rousseau_L/0/1/0/all/0/1"&gt;Louis-Martin Rousseau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cappart_Q/0/1/0/all/0/1"&gt;Quentin Cappart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Lifting and Link Prediction. (arXiv:2302.01198v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.01198</id>
        <link href="http://arxiv.org/abs/2302.01198"/>
        <updated>2023-07-29T00:48:57.555Z</updated>
        <summary type="html"><![CDATA[Existing causal models for link prediction assume an underlying set of
inherent node factors -- an innate characteristic defined at the node's birth
-- that governs the causal evolution of links in the graph. In some causal
tasks, however, link formation is path-dependent: The outcome of link
interventions depends on existing links. Unfortunately, these existing causal
methods are not designed for path-dependent link formation, as the cascading
functional dependencies between links (arising from path dependence) are either
unidentifiable or require an impractical number of control variables. To
overcome this, we develop the first causal model capable of dealing with path
dependencies in link prediction. In this work we introduce the concept of
causal lifting, an invariance in causal models of independent interest that, on
graphs, allows the identification of causal link prediction queries using
limited interventional data. Further, we show how structural pairwise
embeddings exhibit lower bias and correctly represent the task's causal
structure, as opposed to existing node embeddings, e.g., graph neural network
node embeddings and matrix factorization. Finally, we validate our theoretical
findings on three scenarios for causal link prediction tasks: knowledge base
completion, covariance matrix estimation and consumer-product recommendations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cotta_L/0/1/0/all/0/1"&gt;Leonardo Cotta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bevilacqua_B/0/1/0/all/0/1"&gt;Beatrice Bevilacqua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1"&gt;Nesreen Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_B/0/1/0/all/0/1"&gt;Bruno Ribeiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithmic Hallucinations of Near-Surface Winds: Statistical Downscaling with Generative Adversarial Networks to Convection-Permitting Scales. (arXiv:2302.08720v2 [physics.ao-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.08720</id>
        <link href="http://arxiv.org/abs/2302.08720"/>
        <updated>2023-07-29T00:48:57.554Z</updated>
        <summary type="html"><![CDATA[This paper explores the application of emerging machine learning methods from
image super-resolution (SR) to the task of statistical downscaling. We
specifically focus on convolutional neural network-based Generative Adversarial
Networks (GANs). Our GANs are conditioned on low-resolution (LR) inputs to
generate high-resolution (HR) surface winds emulating Weather Research and
Forecasting (WRF) model simulations over North America. Unlike traditional SR
models, where LR inputs are idealized coarsened versions of the HR images, WRF
emulation involves using non-idealized LR and HR pairs resulting in
shared-scale mismatches due to internal variability. Our study builds upon
current SR-based statistical downscaling by experimenting with a novel
frequency-separation (FS) approach from the computer vision field. To assess
the skill of SR models, we carefully select evaluation metrics, and focus on
performance measures based on spatial power spectra. Our analyses reveal how
GAN configurations influence spatial structures in the generated fields,
particularly biases in spatial variability spectra. Using power spectra to
evaluate the FS experiments reveals that successful applications of FS in
computer vision do not translate to climate fields. However, the FS experiments
demonstrate the sensitivity of power spectra to a commonly used GAN-based SR
objective function, which helps interpret and understand its role in
determining spatial structures. This result motivates the development of a
novel partial frequency-separation scheme as a promising configuration option.
We also quantify the influence on GAN performance of non-idealized LR fields
resulting from internal variability. Furthermore, we conduct a spectra-based
feature-importance experiment allowing us to explore the dependence of the
spatial structure of generated fields on different physically relevant LR
covariates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Annau_N/0/1/0/all/0/1"&gt;Nicolaas J. Annau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Cannon_A/0/1/0/all/0/1"&gt;Alex J. Cannon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Monahan_A/0/1/0/all/0/1"&gt;Adam H. Monahan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trace Recovery from Stochastically Known Logs. (arXiv:2206.12672v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2206.12672</id>
        <link href="http://arxiv.org/abs/2206.12672"/>
        <updated>2023-07-29T00:48:57.552Z</updated>
        <summary type="html"><![CDATA[In this work we propose an algorithm for trace recovery from stochastically
known logs, a setting that is becoming more common with the increasing number
of sensors and predictive models that generate uncertain data. The suggested
approach calculates the conformance between a process model and a
stochastically known trace and recovers the best alignment within this
stochastic trace as the true trace. The paper offers an analysis of the impact
of various cost models on trace recovery accuracy and makes use of a product
multi-graph to compare alternative trace recovery options. The average accuracy
of our approach, evaluated using two publicly available datasets, is
impressive, with an average recovery accuracy score of 90-97%, significantly
improving a common heuristic that chooses the most likely value for each
uncertain activity. We believe that the effectiveness of the proposed algorithm
in recovering correct traces from stochastically known logs may be a powerful
aid for developing credible decision-making tools in uncertain settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bogdanov_E/0/1/0/all/0/1"&gt;Eli Bogdanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1"&gt;Izack Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_A/0/1/0/all/0/1"&gt;Avigdor Gal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Weight Balancing on Long-Tailed Recognition Problem. (arXiv:2305.16573v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.16573</id>
        <link href="http://arxiv.org/abs/2305.16573"/>
        <updated>2023-07-29T00:48:57.551Z</updated>
        <summary type="html"><![CDATA[Recognition problems in long-tailed data, where the sample size per class is
heavily skewed, have recently gained importance because the distribution of the
sample size per class in a dataset is generally exponential unless the sample
size is intentionally adjusted. Various approaches have been devised to address
these problems. Recently, weight balancing, which combines well-known classical
regularization techniques with two-stage training, has been proposed. Despite
its simplicity, it is known for its high performance against existing methods
devised in various ways. However, there is a lack of understanding as to why
this approach is effective for long-tailed data. In this study, we analyze the
method focusing on neural collapse and cone effect at each training stage and
find that it can be decomposed into the increase in Fisher's discriminant ratio
of the feature extractor caused by weight decay and cross entropy loss and
implicit logit adjustment caused by weight decay and class-balanced loss. Our
analysis shows that the training method can be further simplified by reducing
the number of training stages to one while increasing accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hasegawa_N/0/1/0/all/0/1"&gt;Naoya Hasegawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1"&gt;Issei Sato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Graph Transformer for Deepfake Detection. (arXiv:2307.15019v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.15019</id>
        <link href="http://arxiv.org/abs/2307.15019"/>
        <updated>2023-07-29T00:48:57.547Z</updated>
        <summary type="html"><![CDATA[Deepfake detection methods have shown promising results in recognizing
forgeries within a given dataset, where training and testing take place on the
in-distribution dataset. However, their performance deteriorates significantly
when presented with unseen samples. As a result, a reliable deepfake detection
system must remain impartial to forgery types, appearance, and quality for
guaranteed generalizable detection performance. Despite various attempts to
enhance cross-dataset generalization, the problem remains challenging,
particularly when testing against common post-processing perturbations, such as
video compression or blur. Hence, this study introduces a deepfake detection
framework, leveraging a self-supervised pre-training model that delivers
exceptional generalization ability, withstanding common corruptions and
enabling feature explainability. The framework comprises three key components:
a feature extractor based on vision Transformer architecture that is
pre-trained via self-supervised contrastive learning methodology, a graph
convolution network coupled with a Transformer discriminator, and a graph
Transformer relevancy map that provides a better understanding of manipulated
regions and further explains the model's decision. To assess the effectiveness
of the proposed framework, several challenging experiments are conducted,
including in-data distribution performance, cross-dataset, cross-manipulation
generalization, and robustness against common post-production perturbations.
The results achieved demonstrate the remarkable effectiveness of the proposed
deepfake detection framework, surpassing the current state-of-the-art
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khormali_A/0/1/0/all/0/1"&gt;Aminollah Khormali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Jiann-Shiun Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speeding up Fourier Neural Operators via Mixed Precision. (arXiv:2307.15034v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.15034</id>
        <link href="http://arxiv.org/abs/2307.15034"/>
        <updated>2023-07-29T00:48:57.547Z</updated>
        <summary type="html"><![CDATA[The Fourier neural operator (FNO) is a powerful technique for learning
surrogate maps for partial differential equation (PDE) solution operators. For
many real-world applications, which often require high-resolution data points,
training time and memory usage are significant bottlenecks. While there are
mixed-precision training techniques for standard neural networks, those work
for real-valued datatypes on finite dimensions and therefore cannot be directly
applied to FNO, which crucially operates in the (complex-valued) Fourier domain
and in function spaces. On the other hand, since the Fourier transform is
already an approximation (due to discretization error), we do not need to
perform the operation at full precision. In this work, we (i) profile memory
and runtime for FNO with full and mixed-precision training, (ii) conduct a
study on the numerical stability of mixed-precision training of FNO, and (iii)
devise a training routine which substantially decreases training time and
memory usage (up to 34%), with little or no reduction in accuracy, on the
Navier-Stokes and Darcy flow equations. Combined with the recently proposed
tensorized FNO (Kossaifi et al., 2023), the resulting model has far better
performance while also being significantly faster than the original FNO.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1"&gt;Colin White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1"&gt;Renbo Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kossaifi_J/0/1/0/all/0/1"&gt;Jean Kossaifi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pekhimenko_G/0/1/0/all/0/1"&gt;Gennady Pekhimenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1"&gt;Kamyar Azizzadenesheli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large Language Models Struggle to Learn Long-Tail Knowledge. (arXiv:2211.08411v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.08411</id>
        <link href="http://arxiv.org/abs/2211.08411"/>
        <updated>2023-07-29T00:48:57.546Z</updated>
        <summary type="html"><![CDATA[The Internet contains a wealth of knowledge -- from the birthdays of
historical figures to tutorials on how to code -- all of which may be learned
by language models. However, while certain pieces of information are ubiquitous
on the web, others appear extremely rarely. In this paper, we study the
relationship between the knowledge memorized by large language models and the
information in pre-training datasets scraped from the web. In particular, we
show that a language model's ability to answer a fact-based question relates to
how many documents associated with that question were seen during pre-training.
We identify these relevant documents by entity linking pre-training datasets
and counting documents that contain the same entities as a given
question-answer pair. Our results demonstrate strong correlational and causal
relationships between accuracy and relevant document count for numerous
question answering datasets (e.g., TriviaQA), pre-training corpora (e.g.,
ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models
are better at learning long-tail knowledge, we estimate that today's models
must be scaled by many orders of magnitude to reach competitive QA performance
on questions with little support in the pre-training data. Finally, we show
that retrieval-augmentation can reduce the dependence on relevant pre-training
information, presenting a promising approach for capturing the long-tail.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kandpal_N/0/1/0/all/0/1"&gt;Nikhil Kandpal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1"&gt;Haikang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1"&gt;Adam Roberts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1"&gt;Eric Wallace&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1"&gt;Colin Raffel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing Explainer Robustness via Lipschitzness of Prediction Functions. (arXiv:2206.12481v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2206.12481</id>
        <link href="http://arxiv.org/abs/2206.12481"/>
        <updated>2023-07-29T00:48:57.545Z</updated>
        <summary type="html"><![CDATA[Machine learning methods have significantly improved in their predictive
capabilities, but at the same time they are becoming more complex and less
transparent. As a result, explainers are often relied on to provide
interpretability to these black-box prediction models. As crucial diagnostics
tools, it is important that these explainers themselves are robust. In this
paper we focus on one particular aspect of robustness, namely that an explainer
should give similar explanations for similar data inputs. We formalize this
notion by introducing and defining explainer astuteness, analogous to
astuteness of prediction functions. Our formalism allows us to connect
explainer robustness to the predictor's probabilistic Lipschitzness, which
captures the probability of local smoothness of a function. We provide lower
bound guarantees on the astuteness of a variety of explainers (e.g., SHAP,
RISE, CXPlain) given the Lipschitzness of the prediction function. These
theoretical results imply that locally smooth prediction functions lend
themselves to locally robust explanations. We evaluate these results
empirically on simulated as well as real datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1"&gt;Zulqarnain Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hill_D/0/1/0/all/0/1"&gt;Davin Hill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masoomi_A/0/1/0/all/0/1"&gt;Aria Masoomi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bone_J/0/1/0/all/0/1"&gt;Joshua Bone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dy_J/0/1/0/all/0/1"&gt;Jennifer Dy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Winning Regions in Parity Games via Graph Neural Networks (Extended Abstract). (arXiv:2210.09924v2 [cs.GT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.09924</id>
        <link href="http://arxiv.org/abs/2210.09924"/>
        <updated>2023-07-29T00:48:57.545Z</updated>
        <summary type="html"><![CDATA[Solving parity games is a major building block for numerous applications in
reactive program verification and synthesis. While they can be solved
efficiently in practice, no known approach has a polynomial worst-case runtime
complexity. We present a incomplete polynomial-time approach to determining the
winning regions of parity games via graph neural networks.

Our evaluation on 900 randomly generated parity games shows that this
approach is effective and efficient in practice. It correctly determines the
winning regions of $\sim$60\% of the games in our data set and only incurs
minor errors in the remaining ones. We believe that this approach can be
extended to efficiently solve parity games as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hecking_T/0/1/0/all/0/1"&gt;Tobias Hecking&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muthukrishnan_S/0/1/0/all/0/1"&gt;Swathy Muthukrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weinert_A/0/1/0/all/0/1"&gt;Alexander Weinert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Securing Secure Aggregation: Mitigating Multi-Round Privacy Leakage in Federated Learning. (arXiv:2106.03328v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03328</id>
        <link href="http://arxiv.org/abs/2106.03328"/>
        <updated>2023-07-29T00:48:57.544Z</updated>
        <summary type="html"><![CDATA[Secure aggregation is a critical component in federated learning (FL), which
enables the server to learn the aggregate model of the users without observing
their local models. Conventionally, secure aggregation algorithms focus only on
ensuring the privacy of individual users in a single training round. We contend
that such designs can lead to significant privacy leakages over multiple
training rounds, due to partial user selection/participation at each round of
FL. In fact, we show that the conventional random user selection strategies in
FL lead to leaking users' individual models within number of rounds that is
linear in the number of users. To address this challenge, we introduce a secure
aggregation framework, Multi-RoundSecAgg, with multi-round privacy guarantees.
In particular, we introduce a new metric to quantify the privacy guarantees of
FL over multiple training rounds, and develop a structured user selection
strategy that guarantees the long-term privacy of each user (over any number of
training rounds). Our framework also carefully accounts for the fairness and
the average number of participating users at each round. Our experiments on
MNIST and CIFAR-10 datasets in the IID and the non-IID settings demonstrate the
performance improvement over the baselines, both in terms of privacy protection
and test accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+So_J/0/1/0/all/0/1"&gt;Jinhyun So&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ali_R/0/1/0/all/0/1"&gt;Ramy E. Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guler_B/0/1/0/all/0/1"&gt;Basak Guler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1"&gt;Jiantao Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1"&gt;Salman Avestimehr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2205.14704</id>
        <link href="http://arxiv.org/abs/2205.14704"/>
        <updated>2023-07-29T00:48:57.544Z</updated>
        <summary type="html"><![CDATA[Prompt learning approaches have made waves in natural language processing by
inducing better few-shot performance while they still follow a parametric-based
learning paradigm; the oblivion and rote memorization problems in learning may
encounter unstable generalization issues. Specifically, vanilla prompt learning
may struggle to utilize atypical instances by rote during fully-supervised
training or overfit shallow patterns with low-shot data. To alleviate such
limitations, we develop RetroPrompt with the motivation of decoupling knowledge
from memorization to help the model strike a balance between generalization and
memorization. In contrast with vanilla prompt learning, RetroPrompt constructs
an open-book knowledge-store from training instances and implements a retrieval
mechanism during the process of input, training and inference, thus equipping
the model with the ability to retrieve related contexts from the training
corpus as cues for enhancement. Extensive experiments demonstrate that
RetroPrompt can obtain better performance in both few-shot and zero-shot
settings. Besides, we further illustrate that our proposed RetroPrompt can
yield better generalization abilities with new datasets. Detailed analysis of
memorization indeed reveals RetroPrompt can reduce the reliance of language
models on memorization; thus, improving generalization for downstream tasks.
Code is available in
https://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaozhuan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shumin Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1"&gt;Chuanqi Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1"&gt;Luo Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huajun Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Domain Adaptation for Time-Series via Temporal Mixup. (arXiv:2212.01555v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.01555</id>
        <link href="http://arxiv.org/abs/2212.01555"/>
        <updated>2023-07-29T00:48:57.544Z</updated>
        <summary type="html"><![CDATA[Unsupervised Domain Adaptation (UDA) has emerged as a powerful solution for
the domain shift problem via transferring the knowledge from a labeled source
domain to a shifted unlabeled target domain. Despite the prevalence of UDA for
visual applications, it remains relatively less explored for time-series
applications. In this work, we propose a novel lightweight contrastive domain
adaptation framework called CoTMix for time-series data. Unlike existing
approaches that either use statistical distances or adversarial techniques, we
leverage contrastive learning solely to mitigate the distribution shift across
the different domains. Specifically, we propose a novel temporal mixup strategy
to generate two intermediate augmented views for the source and target domains.
Subsequently, we leverage contrastive learning to maximize the similarity
between each domain and its corresponding augmented view. The generated views
consider the temporal dynamics of time-series data during the adaptation
process while inheriting the semantics among the two domains. Hence, we
gradually push both domains towards a common intermediate space, mitigating the
distribution shift across them. Extensive experiments conducted on five
real-world time-series datasets show that our approach can significantly
outperform all state-of-the-art UDA methods. The implementation code of CoTMix
is available at
\href{https://github.com/emadeldeen24/CoTMix}{github.com/emadeldeen24/CoTMix}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eldele_E/0/1/0/all/0/1"&gt;Emadeldeen Eldele&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ragab_M/0/1/0/all/0/1"&gt;Mohamed Ragab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhenghua Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1"&gt;Min Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwoh_C/0/1/0/all/0/1"&gt;Chee-Keong Kwoh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoli Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoding the Secrets of Machine Learning in Malware Classification: A Deep Dive into Datasets, Feature Extraction, and Model Performance. (arXiv:2307.14657v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2307.14657</id>
        <link href="http://arxiv.org/abs/2307.14657"/>
        <updated>2023-07-29T00:48:57.543Z</updated>
        <summary type="html"><![CDATA[Many studies have proposed machine-learning (ML) models for malware detection
and classification, reporting an almost-perfect performance. However, they
assemble ground-truth in different ways, use diverse static- and
dynamic-analysis techniques for feature extraction, and even differ on what
they consider a malware family. As a consequence, our community still lacks an
understanding of malware classification results: whether they are tied to the
nature and distribution of the collected dataset, to what extent the number of
families and samples in the training dataset influence performance, and how
well static and dynamic features complement each other.

This work sheds light on those open questions. by investigating the key
factors influencing ML-based malware detection and classification. For this, we
collect the largest balanced malware dataset so far with 67K samples from 670
families (100 samples each), and train state-of-the-art models for malware
detection and family classification using our dataset. Our results reveal that
static features perform better than dynamic features, and that combining both
only provides marginal improvement over static features. We discover no
correlation between packing and classification accuracy, and that missing
behaviors in dynamically-extracted features highly penalize their performance.
We also demonstrate how a larger number of families to classify make the
classification harder, while a higher number of samples per family increases
accuracy. Finally, we find that models trained on a uniform distribution of
samples per family better generalize on unseen data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dambra_S/0/1/0/all/0/1"&gt;Savino Dambra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yufei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aonzo_S/0/1/0/all/0/1"&gt;Simone Aonzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotzias_P/0/1/0/all/0/1"&gt;Platon Kotzias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vitale_A/0/1/0/all/0/1"&gt;Antonino Vitale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caballero_J/0/1/0/all/0/1"&gt;Juan Caballero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balzarotti_D/0/1/0/all/0/1"&gt;Davide Balzarotti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bilge_L/0/1/0/all/0/1"&gt;Leyla Bilge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning locally dominant force balances in active particle systems. (arXiv:2307.14970v1 [cond-mat.soft])]]></title>
        <id>http://arxiv.org/abs/2307.14970</id>
        <link href="http://arxiv.org/abs/2307.14970"/>
        <updated>2023-07-29T00:48:57.543Z</updated>
        <summary type="html"><![CDATA[We use a combination of unsupervised clustering and sparsity-promoting
inference algorithms to learn locally dominant force balances that explain
macroscopic pattern formation in self-organized active particle systems. The
self-organized emergence of macroscopic patterns from microscopic interactions
between self-propelled particles can be widely observed nature. Although
hydrodynamic theories help us better understand the physical basis of this
phenomenon, identifying a sufficient set of local interactions that shape,
regulate, and sustain self-organized structures in active particle systems
remains challenging. We investigate a classic hydrodynamic model of
self-propelled particles that produces a wide variety of patterns, like asters
and moving density bands. Our data-driven analysis shows that propagating bands
are formed by local alignment interactions driven by density gradients, while
steady-state asters are shaped by a mechanism of splay-induced negative
compressibility arising from strong particle interactions. Our method also
reveals analogous physical principles of pattern formation in a system where
the speed of the particle is influenced by local density. This demonstrates the
ability of our method to reveal physical commonalities across models. The
physical mechanisms inferred from the data are in excellent agreement with
analytical scaling arguments and experimental observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Sturm_D/0/1/0/all/0/1"&gt;Dominik Sturm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Maddu_S/0/1/0/all/0/1"&gt;Suryanarayana Maddu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Sbalzarini_I/0/1/0/all/0/1"&gt;Ivo F. Sbalzarini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Generalization Effects of Linear Transformations in Data Augmentation. (arXiv:2005.00695v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00695</id>
        <link href="http://arxiv.org/abs/2005.00695"/>
        <updated>2023-07-29T00:48:57.543Z</updated>
        <summary type="html"><![CDATA[Data augmentation is a powerful technique to improve performance in
applications such as image and text classification tasks. Yet, there is little
rigorous understanding of why and how various augmentations work. In this work,
we consider a family of linear transformations and study their effects on the
ridge estimator in an over-parametrized linear regression setting. First, we
show that transformations that preserve the labels of the data can improve
estimation by enlarging the span of the training data. Second, we show that
transformations that mix data can improve estimation by playing a
regularization effect. Finally, we validate our theoretical insights on MNIST.
Based on the insights, we propose an augmentation scheme that searches over the
space of transformations by how uncertain the model is about the transformed
data. We validate our proposed scheme on image and text datasets. For example,
our method outperforms random sampling methods by 1.24% on CIFAR-100 using
Wide-ResNet-28-10. Furthermore, we achieve comparable accuracy to the SoTA
Adversarial AutoAugment on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Sen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongyang R. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valiant_G/0/1/0/all/0/1"&gt;Gregory Valiant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1"&gt;Christopher R&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On (Normalised) Discounted Cumulative Gain as an Offline Evaluation Metric for Top-$n$ Recommendation. (arXiv:2307.15053v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2307.15053</id>
        <link href="http://arxiv.org/abs/2307.15053"/>
        <updated>2023-07-29T00:48:57.542Z</updated>
        <summary type="html"><![CDATA[Approaches to recommendation are typically evaluated in one of two ways: (1)
via a (simulated) online experiment, often seen as the gold standard, or (2)
via some offline evaluation procedure, where the goal is to approximate the
outcome of an online experiment. Several offline evaluation metrics have been
adopted in the literature, inspired by ranking metrics prevalent in the field
of Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one
such metric that has seen widespread adoption in empirical studies, and higher
(n)DCG values have been used to present new methods as the state-of-the-art in
top-$n$ recommendation for many years.

Our work takes a critical look at this approach, and investigates when we can
expect such metrics to approximate the gold standard outcome of an online
experiment. We formally present the assumptions that are necessary to consider
DCG an unbiased estimator of online reward and provide a derivation for this
metric from first principles, highlighting where we deviate from its
traditional uses in IR. Importantly, we show that normalising the metric
renders it inconsistent, in that even when DCG is unbiased, ranking competing
methods by their normalised DCG can invert their relative order. Through a
correlation analysis between off- and on-line experiments conducted on a
large-scale recommendation platform, we show that our unbiased DCG estimates
strongly correlate with online reward, even when some of the metric's inherent
assumptions are violated. This statement no longer holds for its normalised
variant, suggesting that nDCG's practical utility may be limited.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jeunen_O/0/1/0/all/0/1"&gt;Olivier Jeunen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potapov_I/0/1/0/all/0/1"&gt;Ivan Potapov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ustimenko_A/0/1/0/all/0/1"&gt;Aleksei Ustimenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamics of specialization in neural modules under resource constraints. (arXiv:2106.02626v2 [q-bio.NC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02626</id>
        <link href="http://arxiv.org/abs/2106.02626"/>
        <updated>2023-07-29T00:48:57.542Z</updated>
        <summary type="html"><![CDATA[It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold except
at extreme levels. We then systematically tested which features of the
environment and network do lead to the emergence of specialization. We used a
simple toy environment, task and network, allowing us precise control, and show
that in this setup, several distinct measures of specialization give
qualitatively similar results. We further find that (1) specialization can only
emerge in environments where features of that environment are meaningfully
separable, (2) specialization preferentially emerges when the network is
strongly resource-constrained, and (3) these findings are qualitatively similar
across different network architectures, but the quantitative relationships
depends on the architecture type. Finally, we show that functional
specialization varies dynamically across time, and demonstrate that these
dynamics depend on both the timing and bandwidth of information flow in the
network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligent systems in situations of real-world complexity. We propose that
thoroughly stress testing candidate definitions of functional modularity in
simplified scenarios before extending to more complex data, network models and
electrophysiological recordings is likely to be a fruitful approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Bena_G/0/1/0/all/0/1"&gt;Gabriel B&amp;#xe9;na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Goodman_D/0/1/0/all/0/1"&gt;Dan F. M. Goodman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Task Automata for Reinforcement Learning using Hidden Markov Models. (arXiv:2208.11838v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.11838</id>
        <link href="http://arxiv.org/abs/2208.11838"/>
        <updated>2023-07-29T00:48:57.542Z</updated>
        <summary type="html"><![CDATA[Training reinforcement learning (RL) agents using scalar reward signals is
often infeasible when an environment has sparse and non-Markovian rewards.
Moreover, handcrafting these reward functions before training is prone to
misspecification, especially when the environment's dynamics are only partially
known. This paper proposes a novel pipeline for learning non-Markovian task
specifications as succinct finite-state `task automata' from episodes of agent
experience within unknown environments. We leverage two key algorithmic
insights. First, we learn a product MDP, a model composed of the
specification's automaton and the environment's MDP (both initially unknown),
by treating the product MDP as a partially observable MDP and using the
well-known Baum-Welch algorithm for learning hidden Markov models. Second, we
propose a novel method for distilling the task automaton (assumed to be a
deterministic finite automaton) from the learnt product MDP. Our learnt task
automaton enables the decomposition of a task into its constituent sub-tasks,
which improves the rate at which an RL agent can later synthesise an optimal
policy. It also provides an interpretable encoding of high-level environmental
and task features, so a human can readily verify that the agent has learnt
coherent tasks with no misspecifications. In addition, we take steps towards
ensuring that the learnt automaton is environment-agnostic, making it
well-suited for use in transfer learning. Finally, we provide experimental
results compared with two baselines to illustrate our algorithm's performance
in different environments and tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abate_A/0/1/0/all/0/1"&gt;Alessandro Abate&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Almulla_Y/0/1/0/all/0/1"&gt;Yousif Almulla&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_J/0/1/0/all/0/1"&gt;James Fox&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Hyland_D/0/1/0/all/0/1"&gt;David Hyland&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Wooldridge_M/0/1/0/all/0/1"&gt;Michael Wooldridge&lt;/a&gt; (1) ((1) University of Oxford)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pruning Distorted Images in MNIST Handwritten Digits. (arXiv:2307.14343v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.14343</id>
        <link href="http://arxiv.org/abs/2307.14343"/>
        <updated>2023-07-29T00:48:57.541Z</updated>
        <summary type="html"><![CDATA[Recognizing handwritten digits is a challenging task primarily due to the
diversity of writing styles and the presence of noisy images. The widely used
MNIST dataset, which is commonly employed as a benchmark for this task,
includes distorted digits with irregular shapes, incomplete strokes, and
varying skew in both the training and testing datasets. Consequently, these
factors contribute to reduced accuracy in digit recognition. To overcome this
challenge, we propose a two-stage deep learning approach. In the first stage,
we create a simple neural network to identify distorted digits within the
training set. This model serves to detect and filter out such distorted and
ambiguous images. In the second stage, we exclude these identified images from
the training dataset and proceed to retrain the model using the filtered
dataset. This process aims to improve the classification accuracy and
confidence levels while mitigating issues of underfitting and overfitting. Our
experimental results demonstrate the effectiveness of the proposed approach,
achieving an accuracy rate of over 99.5% on the testing dataset. This
significant improvement showcases the potential of our method in enhancing
digit classification accuracy. In our future work, we intend to explore the
scalability of this approach and investigate techniques to further enhance
accuracy by reducing the size of the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+R_A/0/1/0/all/0/1"&gt;Amarnath R&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+V_V/0/1/0/all/0/1"&gt;Vinay Kumar V&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Practicable Sequential Shift Detectors. (arXiv:2307.14758v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14758</id>
        <link href="http://arxiv.org/abs/2307.14758"/>
        <updated>2023-07-29T00:48:57.541Z</updated>
        <summary type="html"><![CDATA[There is a growing awareness of the harmful effects of distribution shift on
the performance of deployed machine learning models. Consequently, there is a
growing interest in detecting these shifts before associated costs have time to
accumulate. However, desiderata of crucial importance to the practicable
deployment of sequential shift detectors are typically overlooked by existing
works, precluding their widespread adoption. We identify three such desiderata,
highlight existing works relevant to their satisfaction, and recommend
impactful directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cobb_O/0/1/0/all/0/1"&gt;Oliver Cobb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Looveren_A/0/1/0/all/0/1"&gt;Arnaud Van Looveren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples. (arXiv:2307.14565v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2307.14565</id>
        <link href="http://arxiv.org/abs/2307.14565"/>
        <updated>2023-07-29T00:48:57.540Z</updated>
        <summary type="html"><![CDATA[Relational tables, where each row corresponds to an entity and each column
corresponds to an attribute, have been the standard for tables in relational
databases. However, such a standard cannot be taken for granted when dealing
with tables "in the wild". Our survey of real spreadsheet-tables and web-tables
shows that over 30% of such tables do not conform to the relational standard,
for which complex table-restructuring transformations are needed before these
tables can be queried easily using SQL-based analytics tools. Unfortunately,
the required transformations are non-trivial to program, which has become a
substantial pain point for technical and non-technical users alike, as
evidenced by large numbers of forum questions in places like StackOverflow and
Excel/Tableau forums.

We develop an Auto-Tables system that can automatically synthesize pipelines
with multi-step transformations (in Python or other languages), to transform
non-relational tables into standard relational forms for downstream analytics,
obviating the need for users to manually program transformations. We compile an
extensive benchmark for this new task, by collecting 194 real test cases from
user spreadsheets and online forums. Our evaluation suggests that Auto-Tables
can successfully synthesize transformations for over 70% of test cases at
interactive speeds, without requiring any input from users, making this an
effective tool for both technical and non-technical users to prepare data for
analytics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yeye He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1"&gt;Cong Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chauduri_S/0/1/0/all/0/1"&gt;Surajit Chauduri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speed Limits for Deep Learning. (arXiv:2307.14653v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.14653</id>
        <link href="http://arxiv.org/abs/2307.14653"/>
        <updated>2023-07-29T00:48:57.500Z</updated>
        <summary type="html"><![CDATA[State-of-the-art neural networks require extreme computational power to
train. It is therefore natural to wonder whether they are optimally trained.
Here we apply a recent advancement in stochastic thermodynamics which allows
bounding the speed at which one can go from the initial weight distribution to
the final distribution of the fully trained network, based on the ratio of
their Wasserstein-2 distance and the entropy production rate of the dynamical
process connecting them. Considering both gradient-flow and Langevin training
dynamics, we provide analytical expressions for these speed limits for linear
and linearizable neural networks e.g. Neural Tangent Kernel (NTK). Remarkably,
given some plausible scaling assumptions on the NTK spectra and spectral
decomposition of the labels -- learning is optimal in a scaling sense. Our
results are consistent with small-scale experiments with Convolutional Neural
Networks (CNNs) and Fully Connected Neural networks (FCNs) on CIFAR-10, showing
a short highly non-optimal regime followed by a longer optimal regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Seroussi_I/0/1/0/all/0/1"&gt;Inbar Seroussi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Alemi_A/0/1/0/all/0/1"&gt;Alexander A. Alemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Helias_M/0/1/0/all/0/1"&gt;Moritz Helias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ringel_Z/0/1/0/all/0/1"&gt;Zohar Ringel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Machine Unlearning: Data Removal while Mitigating Disparities. (arXiv:2307.14754v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14754</id>
        <link href="http://arxiv.org/abs/2307.14754"/>
        <updated>2023-07-29T00:48:57.500Z</updated>
        <summary type="html"><![CDATA[As public consciousness regarding the collection and use of personal
information by corporations grows, it is of increasing importance that
consumers be active participants in the curation of corporate datasets. In
light of this, data governance frameworks such as the General Data Protection
Regulation (GDPR) have outlined the right to be forgotten as a key principle
allowing individuals to request that their personal data be deleted from the
databases and models used by organizations. To achieve forgetting in practice,
several machine unlearning methods have been proposed to address the
computational inefficiencies of retraining a model from scratch with each
unlearning request. While efficient online alternatives to retraining, it is
unclear how these methods impact other properties critical to real-world
applications, such as fairness. In this work, we propose the first fair machine
unlearning method that can provably and efficiently unlearn data instances
while preserving group fairness. We derive theoretical results which
demonstrate that our method can provably unlearn data instances while
maintaining fairness objectives. Extensive experimentation with real-world
datasets highlight the efficacy of our method at unlearning data instances
while preserving fairness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oesterling_A/0/1/0/all/0/1"&gt;Alex Oesterling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jiaqi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calmon_F/0/1/0/all/0/1"&gt;Flavio P. Calmon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1"&gt;Hima Lakkaraju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Transformer-based Approach for Arabic Offline Handwritten Text Recognition. (arXiv:2307.15045v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.15045</id>
        <link href="http://arxiv.org/abs/2307.15045"/>
        <updated>2023-07-29T00:48:57.500Z</updated>
        <summary type="html"><![CDATA[Handwriting recognition is a challenging and critical problem in the fields
of pattern recognition and machine learning, with applications spanning a wide
range of domains. In this paper, we focus on the specific issue of recognizing
offline Arabic handwritten text. Existing approaches typically utilize a
combination of convolutional neural networks for image feature extraction and
recurrent neural networks for temporal modeling, with connectionist temporal
classification used for text generation. However, these methods suffer from a
lack of parallelization due to the sequential nature of recurrent neural
networks. Furthermore, these models cannot account for linguistic rules,
necessitating the use of an external language model in the post-processing
stage to boost accuracy. To overcome these issues, we introduce two alternative
architectures, namely the Transformer Transducer and the standard
sequence-to-sequence Transformer, and compare their performance in terms of
accuracy and speed. Our approach can model language dependencies and relies
only on the attention mechanism, thereby making it more parallelizable and less
complex. We employ pre-trained Transformers for both image understanding and
language modeling. Our evaluation on the Arabic KHATT dataset demonstrates that
our proposed method outperforms the current state-of-the-art approaches for
recognizing offline Arabic handwritten text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Momeni_S/0/1/0/all/0/1"&gt;Saleh Momeni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+BabaAli_B/0/1/0/all/0/1"&gt;Bagher BabaAli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TimeGNN: Temporal Dynamic Graph Learning for Time Series Forecasting. (arXiv:2307.14680v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14680</id>
        <link href="http://arxiv.org/abs/2307.14680"/>
        <updated>2023-07-29T00:48:57.499Z</updated>
        <summary type="html"><![CDATA[Time series forecasting lies at the core of important real-world applications
in many fields of science and engineering. The abundance of large time series
datasets that consist of complex patterns and long-term dependencies has led to
the development of various neural network architectures. Graph neural network
approaches, which jointly learn a graph structure based on the correlation of
raw values of multivariate time series while forecasting, have recently seen
great success. However, such solutions are often costly to train and difficult
to scale. In this paper, we propose TimeGNN, a method that learns dynamic
temporal graph representations that can capture the evolution of inter-series
patterns along with the correlations of multiple series. TimeGNN achieves
inference times 4 to 80 times faster than other state-of-the-art graph-based
methods while achieving comparable forecasting performance]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1"&gt;Nancy Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kosma_C/0/1/0/all/0/1"&gt;Chrysoula Kosma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1"&gt;Michalis Vazirgiannis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emotion4MIDI: a Lyrics-based Emotion-Labeled Symbolic Music Dataset. (arXiv:2307.14783v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2307.14783</id>
        <link href="http://arxiv.org/abs/2307.14783"/>
        <updated>2023-07-29T00:48:57.499Z</updated>
        <summary type="html"><![CDATA[We present a new large-scale emotion-labeled symbolic music dataset
consisting of 12k MIDI songs. To create this dataset, we first trained emotion
classification models on the GoEmotions dataset, achieving state-of-the-art
results with a model half the size of the baseline. We then applied these
models to lyrics from two large-scale MIDI datasets. Our dataset covers a wide
range of fine-grained emotions, providing a valuable resource to explore the
connection between music and emotions and, especially, to develop models that
can generate music based on specific emotions. Our code for inference, trained
models, and datasets are available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sulun_S/0/1/0/all/0/1"&gt;Serkan Sulun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oliveira_P/0/1/0/all/0/1"&gt;Pedro Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Viana_P/0/1/0/all/0/1"&gt;Paula Viana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compositional federated learning: Applications in distributionally robust averaging and meta learning. (arXiv:2106.11264v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11264</id>
        <link href="http://arxiv.org/abs/2106.11264"/>
        <updated>2023-07-29T00:48:57.499Z</updated>
        <summary type="html"><![CDATA[In the paper, we propose an effective and efficient Compositional Federated
Learning (ComFedL) algorithm for solving a new compositional Federated Learning
(FL) framework, which frequently appears in many data mining and machine
learning problems with a hierarchical structure such as distributionally robust
FL and model-agnostic meta learning (MAML). Moreover, we study the convergence
analysis of our ComFedL algorithm under some mild conditions, and prove that it
achieves a convergence rate of $O(\frac{1}{\sqrt{T}})$, where $T$ denotes the
number of iteration. To the best of our knowledge, our new Compositional FL
framework is the first work to bridge federated learning with composition
stochastic optimization. In particular, we first transform the distributionally
robust FL (i.e., a minimax optimization problem) into a simple composition
optimization problem by using KL divergence regularization. At the same time,
we also first transform the distribution-agnostic MAML problem (i.e., a minimax
optimization problem) into a simple yet effective composition optimization
problem. Finally, we apply two popular machine learning tasks, i.e.,
distributionally robust FL and MAML to demonstrate the effectiveness of our
algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feihu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Junyi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MATNilm: Multi-appliance-task Non-intrusive Load Monitoring with Limited Labeled Data. (arXiv:2307.14778v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14778</id>
        <link href="http://arxiv.org/abs/2307.14778"/>
        <updated>2023-07-29T00:48:57.498Z</updated>
        <summary type="html"><![CDATA[Non-intrusive load monitoring (NILM) identifies the status and power
consumption of various household appliances by disaggregating the total power
usage signal of an entire house. Efficient and accurate load monitoring
facilitates user profile establishment, intelligent household energy
management, and peak load shifting. This is beneficial for both the end-users
and utilities by improving the overall efficiency of a power distribution
network. Existing approaches mainly focus on developing an individual model for
each appliance. Those approaches typically rely on a large amount of
household-labeled data which is hard to collect. In this paper, we propose a
multi-appliance-task framework with a training-efficient sample augmentation
(SA) scheme that boosts the disaggregation performance with limited labeled
data. For each appliance, we develop a shared-hierarchical split structure for
its regression and classification tasks. In addition, we also propose a
two-dimensional attention mechanism in order to capture spatio-temporal
correlations among all appliances. With only one-day training data and limited
appliance operation profiles, the proposed SA algorithm can achieve comparable
test performance to the case of training with the full dataset. Finally,
simulation results show that our proposed approach features a significantly
improved performance over many baseline models. The relative errors can be
reduced by more than 50\% on average. The codes of this work are available at
https://github.com/jxiong22/MATNilm]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1"&gt;Jing Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_T/0/1/0/all/0/1"&gt;Tianqi Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Dongbo Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Session-Based Transformer Recommendations using Optimized Negative Sampling and Loss Functions. (arXiv:2307.14906v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2307.14906</id>
        <link href="http://arxiv.org/abs/2307.14906"/>
        <updated>2023-07-29T00:48:57.498Z</updated>
        <summary type="html"><![CDATA[This work introduces TRON, a scalable session-based Transformer Recommender
using Optimized Negative-sampling. Motivated by the scalability and performance
limitations of prevailing models such as SASRec and GRU4Rec+, TRON integrates
top-k negative sampling and listwise loss functions to enhance its
recommendation accuracy. Evaluations on relevant large-scale e-commerce
datasets show that TRON improves upon the recommendation quality of current
methods while maintaining training speeds similar to SASRec. A live A/B test
yielded an 18.14% increase in click-through rate over SASRec, highlighting the
potential of TRON in practical settings. For further research, we provide
access to our source code at https://github.com/otto-de/TRON and an anonymized
dataset at https://github.com/otto-de/recsys-dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wilm_T/0/1/0/all/0/1"&gt;Timo Wilm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Normann_P/0/1/0/all/0/1"&gt;Philipp Normann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baumeister_S/0/1/0/all/0/1"&gt;Sophie Baumeister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobow_P/0/1/0/all/0/1"&gt;Paul-Vincent Kobow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments. (arXiv:2304.06848v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.06848</id>
        <link href="http://arxiv.org/abs/2304.06848"/>
        <updated>2023-07-29T00:48:57.497Z</updated>
        <summary type="html"><![CDATA[Robots operating in real-world environments must reason about possible
outcomes of stochastic actions and make decisions based on partial observations
of the true world state. A major challenge for making accurate and robust
action predictions is the problem of confounding, which if left untreated can
lead to prediction errors. The partially observable Markov decision process
(POMDP) is a widely-used framework to model these stochastic and
partially-observable decision-making problems. However, due to a lack of
explicit causal semantics, POMDP planning methods are prone to confounding bias
and thus in the presence of unobserved confounders may produce underperforming
policies. This paper presents a novel causally-informed extension of "anytime
regularized determinized sparse partially observable tree" (AR-DESPOT), a
modern anytime online POMDP planner, using causal modelling and inference to
eliminate errors caused by unmeasured confounder variables. We further propose
a method to learn offline the partial parameterisation of the causal model for
planning, from ground truth model data. We evaluate our methods on a toy
problem with an unobserved confounder and show that the learned causal model is
highly accurate, while our planning method is more robust to confounding and
produces overall higher performing policies than AR-DESPOT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cannizzaro_R/0/1/0/all/0/1"&gt;Ricardo Cannizzaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kunze_L/0/1/0/all/0/1"&gt;Lars Kunze&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models. (arXiv:2307.14971v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.14971</id>
        <link href="http://arxiv.org/abs/2307.14971"/>
        <updated>2023-07-29T00:48:57.486Z</updated>
        <summary type="html"><![CDATA[With the overwhelming trend of mask image modeling led by MAE, generative
pre-training has shown a remarkable potential to boost the performance of
fundamental models in 2D vision. However, in 3D vision, the over-reliance on
Transformer-based backbones and the unordered nature of point clouds have
restricted the further development of generative pre-training. In this paper,
we propose a novel 3D-to-2D generative pre-training method that is adaptable to
any point cloud model. We propose to generate view images from different
instructed poses via the cross-attention mechanism as the pre-training scheme.
Generating view images has more precise supervision than its point cloud
counterpart, thus assisting 3D backbones to have a finer comprehension of the
geometrical structure and stereoscopic relations of the point cloud.
Experimental results have proved the superiority of our proposed 3D-to-2D
generative pre-training over previous pre-training methods. Our method is also
effective in boosting the performance of architecture-oriented approaches,
achieving state-of-the-art performance when fine-tuning on ScanObjectNN
classification and ShapeNetPart segmentation tasks. Code is available at
https://github.com/wangzy22/TAP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xumin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1"&gt;Yongming Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Self-Adaptive Penalty Method for Integrating Prior Knowledge Constraints into Neural ODEs. (arXiv:2307.14940v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14940</id>
        <link href="http://arxiv.org/abs/2307.14940"/>
        <updated>2023-07-29T00:48:57.485Z</updated>
        <summary type="html"><![CDATA[The continuous dynamics of natural systems has been effectively modelled
using Neural Ordinary Differential Equations (Neural ODEs). However, for
accurate and meaningful predictions, it is crucial that the models follow the
underlying rules or laws that govern these systems. In this work, we propose a
self-adaptive penalty algorithm for Neural ODEs to enable modelling of
constrained natural systems. The proposed self-adaptive penalty function can
dynamically adjust the penalty parameters. The explicit introduction of prior
knowledge helps to increase the interpretability of Neural ODE -based models.
We validate the proposed approach by modelling three natural systems with prior
knowledge constraints: population growth, chemical reaction evolution, and
damped harmonic oscillator motion. The numerical experiments and a comparison
with other penalty Neural ODE approaches and \emph{vanilla} Neural ODE,
demonstrate the effectiveness of the proposed self-adaptive penalty algorithm
for Neural ODEs in modelling constrained natural systems. Moreover, the
self-adaptive penalty approach provides more accurate and robust models with
reliable and meaningful predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Coelho_C/0/1/0/all/0/1"&gt;C. Coelho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costa_M/0/1/0/all/0/1"&gt;M. Fernanda P. Costa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferras_L/0/1/0/all/0/1"&gt;L. L. Ferr&amp;#xe1;s&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space. (arXiv:2307.14953v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14953</id>
        <link href="http://arxiv.org/abs/2307.14953"/>
        <updated>2023-07-29T00:48:57.485Z</updated>
        <summary type="html"><![CDATA[This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims
to mitigate data distribution shifts when transferring knowledge from multiple
labeled source domains to an unlabeled target domain. We propose a novel MSDA
framework based on dictionary learning and optimal transport. We interpret each
domain in MSDA as an empirical distribution. As such, we express each domain as
a Wasserstein barycenter of dictionary atoms, which are empirical
distributions. We propose a novel algorithm, DaDiL, for learning via
mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates.
Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based
on the reconstruction of labeled samples in the target domain, and DaDiL-E,
based on the ensembling of classifiers learned on atom distributions. We
evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU,
where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in
classification performance. Finally, we show that interpolations in the
Wasserstein hull of learned atoms provide data that can generalize to the
target domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Montesuma_E/0/1/0/all/0/1"&gt;Eduardo Fernandes Montesuma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mboula_F/0/1/0/all/0/1"&gt;Fred Ngol&amp;#xe8; Mboula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Souloumiac_A/0/1/0/all/0/1"&gt;Antoine Souloumiac&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fading memory as inductive bias in residual recurrent networks. (arXiv:2307.14823v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14823</id>
        <link href="http://arxiv.org/abs/2307.14823"/>
        <updated>2023-07-29T00:48:57.484Z</updated>
        <summary type="html"><![CDATA[Residual connections have been proposed as architecture-based inductive bias
to mitigate the problem of exploding and vanishing gradients and increase task
performance in both feed-forward and recurrent networks (RNNs) when trained
with the backpropagation algorithm. Yet, little is known about how residual
connections in RNNs influence their dynamics and fading memory properties.
Here, we introduce weakly coupled residual recurrent networks (WCRNNs) in which
residual connections result in well-defined Lyapunov exponents and allow for
studying properties of fading memory. We investigate how the residual
connections of WCRNNs influence their performance, network dynamics, and memory
properties on a set of benchmark tasks. We show that several distinct forms of
residual connections yield effective inductive biases that result in increased
network expressivity. In particular, residual connections that (i) result in
network dynamics at the proximity of the edge of chaos, (ii) allow networks to
capitalize on characteristic spectral properties of the data, and (iii) result
in heterogeneous memory properties are shown to increase practical
expressivity. In addition, we demonstrate how our results can be extended to
non-linear residuals and introduce a weakly coupled residual initialization
scheme that can be used for Elman RNNs]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dubinin_I/0/1/0/all/0/1"&gt;Igor Dubinin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Effenberger_F/0/1/0/all/0/1"&gt;Felix Effenberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback. (arXiv:2307.14936v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.14936</id>
        <link href="http://arxiv.org/abs/2307.14936"/>
        <updated>2023-07-29T00:48:57.483Z</updated>
        <summary type="html"><![CDATA[Large Language Models for Code (Code LLM) are flourishing. New and powerful
models are released on a weekly basis, demonstrating remarkable performance on
the code generation task. Various approaches have been proposed to boost the
code generation performance of pre-trained Code LLMs, such as supervised
fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we
propose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework,
which can effectively and efficiently boost pre-trained large language models
for code generation. Under this framework, we present PanGu-Coder2, which
achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through
an extensive evaluation on CoderEval and LeetCode benchmarks, we show that
PanGu-Coder2 consistently outperforms all previous Code LLMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1"&gt;Bo Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiaxin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Taihong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zan_D/0/1/0/all/0/1"&gt;Daoguang Zan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_B/0/1/0/all/0/1"&gt;Bing Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_A/0/1/0/all/0/1"&gt;An Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1"&gt;Muhan Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1"&gt;Ailun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1"&gt;Jichuan Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jingyang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuenan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qianxiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CodeLens: An Interactive Tool for Visualizing Code Representations. (arXiv:2307.14902v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2307.14902</id>
        <link href="http://arxiv.org/abs/2307.14902"/>
        <updated>2023-07-29T00:48:57.482Z</updated>
        <summary type="html"><![CDATA[Representing source code in a generic input format is crucial to automate
software engineering tasks, e.g., applying machine learning algorithms to
extract information. Visualizing code representations can further enable human
experts to gain an intuitive insight into the code. Unfortunately, as of today,
there is no universal tool that can simultaneously visualise different types of
code representations. In this paper, we introduce a tool, CodeLens, which
provides a visual interaction environment that supports various representation
methods and helps developers understand and explore them. CodeLens is designed
to support multiple programming languages, such as Java, Python, and
JavaScript, and four types of code representations, including sequence of
tokens, abstract syntax tree (AST), data flow graph (DFG), and control flow
graph (CFG). By using CodeLens, developers can quickly visualize the specific
code representation and also obtain the represented inputs for models of code.
The Web-based interface of CodeLens is available at this http URL
The demonstration video can be found at this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuejun Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bettaieb_S/0/1/0/all/0/1"&gt;Seifeddine Bettaieb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1"&gt;Qiang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Traon_Y/0/1/0/all/0/1"&gt;Yves Le Traon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1"&gt;Qiang Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability. (arXiv:2307.15007v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.15007</id>
        <link href="http://arxiv.org/abs/2307.15007"/>
        <updated>2023-07-29T00:48:57.482Z</updated>
        <summary type="html"><![CDATA[With the increased deployment of machine learning models in various
real-world applications, researchers and practitioners alike have emphasized
the need for explanations of model behaviour. To this end, two broad strategies
have been outlined in prior literature to explain models. Post hoc explanation
methods explain the behaviour of complex black-box models by highlighting
features that are critical to model predictions; however, prior work has shown
that these explanations may not be faithful, and even more concerning is our
inability to verify them. Specifically, it is nontrivial to evaluate if a given
attribution is correct with respect to the underlying model. Inherently
interpretable models, on the other hand, circumvent these issues by explicitly
encoding explanations into model architecture, meaning their explanations are
naturally faithful and verifiable, but they often exhibit poor predictive
performance due to their limited expressive power. In this work, we aim to
bridge the gap between the aforementioned strategies by proposing Verifiability
Tuning (VerT), a method that transforms black-box models into models that
naturally yield faithful and verifiable feature attributions. We begin by
introducing a formal theoretical framework to understand verifiability and show
that attributions produced by standard models cannot be verified. We then
leverage this framework to propose a method to build verifiable models and
feature attributions out of fully trained black-box models. Finally, we perform
extensive experiments on semi-synthetic and real-world datasets, and show that
VerT produces models that (1) yield explanations that are correct and
verifiable and (2) are faithful to the original black-box models they are meant
to explain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhalla_U/0/1/0/all/0/1"&gt;Usha Bhalla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivas_S/0/1/0/all/0/1"&gt;Suraj Srinivas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1"&gt;Himabindu Lakkaraju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Scale Your EMA. (arXiv:2307.13813v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.13813</id>
        <link href="http://arxiv.org/abs/2307.13813"/>
        <updated>2023-07-29T00:48:57.481Z</updated>
        <summary type="html"><![CDATA[Preserving training dynamics across batch sizes is an important tool for
practical machine learning as it enables the trade-off between batch size and
wall-clock time. This trade-off is typically enabled by a scaling rule, for
example, in stochastic gradient descent, one should scale the learning rate
linearly with the batch size. Another important tool for practical machine
learning is the model Exponential Moving Average (EMA), which is a model copy
that does not receive gradient information, but instead follows its target
model with some momentum. This model EMA can improve the robustness and
generalization properties of supervised learning, stabilize pseudo-labeling,
and provide a learning signal for Self-Supervised Learning (SSL). Prior works
have treated the model EMA separately from optimization, leading to different
training dynamics across batch sizes and lower model performance. In this work,
we provide a scaling rule for optimization in the presence of model EMAs and
demonstrate its validity across a range of architectures, optimizers, and data
modalities. We also show the rule's validity where the model EMA contributes to
the optimization of the target model, enabling us to train EMA-based
pseudo-labeling and SSL methods at small and large batch sizes. For SSL, we
enable training of BYOL up to batch size 24,576 without sacrificing
performance, optimally a 6$\times$ wall-clock time reduction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Busbridge_D/0/1/0/all/0/1"&gt;Dan Busbridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramapuram_J/0/1/0/all/0/1"&gt;Jason Ramapuram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ablin_P/0/1/0/all/0/1"&gt;Pierre Ablin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Likhomanenko_T/0/1/0/all/0/1"&gt;Tatiana Likhomanenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dhekane_E/0/1/0/all/0/1"&gt;Eeshan Gunesh Dhekane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suau_X/0/1/0/all/0/1"&gt;Xavier Suau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Webb_R/0/1/0/all/0/1"&gt;Russ Webb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Samplable Anonymous Aggregation for Private Federated Data Analysis. (arXiv:2307.15017v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2307.15017</id>
        <link href="http://arxiv.org/abs/2307.15017"/>
        <updated>2023-07-29T00:48:57.468Z</updated>
        <summary type="html"><![CDATA[We revisit the problem of designing scalable protocols for private statistics
and private federated learning when each device holds its private data. Our
first contribution is to propose a simple primitive that allows for efficient
implementation of several commonly used algorithms, and allows for privacy
accounting that is close to that in the central setting without requiring the
strong trust assumptions it entails. Second, we propose a system architecture
that implements this primitive and perform a security analysis of the proposed
system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Talwar_K/0/1/0/all/0/1"&gt;Kunal Talwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McMillan_A/0/1/0/all/0/1"&gt;Audra McMillan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jina_V/0/1/0/all/0/1"&gt;Vojta Jina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feldman_V/0/1/0/all/0/1"&gt;Vitaly Feldman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basile_B/0/1/0/all/0/1"&gt;Bailey Basile&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cahill_A/0/1/0/all/0/1"&gt;Aine Cahill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_Y/0/1/0/all/0/1"&gt;Yi Sheng Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatzidakis_M/0/1/0/all/0/1"&gt;Mike Chatzidakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Junye Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chick_O/0/1/0/all/0/1"&gt;Oliver Chick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chitnis_M/0/1/0/all/0/1"&gt;Mona Chitnis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganta_S/0/1/0/all/0/1"&gt;Suman Ganta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goren_Y/0/1/0/all/0/1"&gt;Yusuf Goren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Granqvist_F/0/1/0/all/0/1"&gt;Filip Granqvist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1"&gt;Kristine Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacobs_F/0/1/0/all/0/1"&gt;Frederic Jacobs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javidbakht_O/0/1/0/all/0/1"&gt;Omid Javidbakht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Albert Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Low_R/0/1/0/all/0/1"&gt;Richard Low&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mascenik_D/0/1/0/all/0/1"&gt;Dan Mascenik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Myers_S/0/1/0/all/0/1"&gt;Steve Myers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1"&gt;David Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1"&gt;Wonhee Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parsa_G/0/1/0/all/0/1"&gt;Gianni Parsa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pauly_T/0/1/0/all/0/1"&gt;Tommy Pauly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1"&gt;Christian Priebe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rishi_R/0/1/0/all/0/1"&gt;Rehan Rishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rothblum_G/0/1/0/all/0/1"&gt;Guy Rothblum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scaria_M/0/1/0/all/0/1"&gt;Michael Scaria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Linmao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Congzheng Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarbe_K/0/1/0/all/0/1"&gt;Karl Tarbe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogt_S/0/1/0/all/0/1"&gt;Sebastian Vogt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winstrom_L/0/1/0/all/0/1"&gt;Luke Winstrom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shundong Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MCPA: Multi-scale Cross Perceptron Attention Network for 2D Medical Image Segmentation. (arXiv:2307.14588v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2307.14588</id>
        <link href="http://arxiv.org/abs/2307.14588"/>
        <updated>2023-07-29T00:48:57.467Z</updated>
        <summary type="html"><![CDATA[The UNet architecture, based on Convolutional Neural Networks (CNN), has
demonstrated its remarkable performance in medical image analysis. However, it
faces challenges in capturing long-range dependencies due to the limited
receptive fields and inherent bias of convolutional operations. Recently,
numerous transformer-based techniques have been incorporated into the UNet
architecture to overcome this limitation by effectively capturing global
feature correlations. However, the integration of the Transformer modules may
result in the loss of local contextual information during the global feature
fusion process. To overcome these challenges, we propose a 2D medical image
segmentation model called Multi-scale Cross Perceptron Attention Network
(MCPA). The MCPA consists of three main components: an encoder, a decoder, and
a Cross Perceptron. The Cross Perceptron first captures the local correlations
using multiple Multi-scale Cross Perceptron modules, facilitating the fusion of
features across scales. The resulting multi-scale feature vectors are then
spatially unfolded, concatenated, and fed through a Global Perceptron module to
model global dependencies. Furthermore, we introduce a Progressive Dual-branch
Structure to address the semantic segmentation of the image involving finer
tissue structures. This structure gradually shifts the segmentation focus of
MCPA network training from large-scale structural features to more
sophisticated pixel-level features. We evaluate our proposed MCPA model on
several publicly available medical image datasets from different tasks and
devices, including the open large-scale dataset of CT (Synapse), MRI (ACDC),
fundus camera (DRIVE, CHASE_DB1, HRF), and OCTA (ROSE). The experimental
results show that our MCPA model achieves state-of-the-art performance. The
code is available at
https://github.com/simonustc/MCPA-for-2D-Medical-Image-Segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1"&gt;Liang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mingxiao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yi Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shao_P/0/1/0/all/0/1"&gt;Pengfei Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_S/0/1/0/all/0/1"&gt;Shuwei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yao_P/0/1/0/all/0/1"&gt;Peng Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ronald X.Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Contrastive Graph Diffusion Network. (arXiv:2307.14613v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14613</id>
        <link href="http://arxiv.org/abs/2307.14613"/>
        <updated>2023-07-29T00:48:57.467Z</updated>
        <summary type="html"><![CDATA[Augmentation techniques and sampling strategies are crucial in contrastive
learning, but in most existing works, augmentation techniques require careful
design, and their sampling strategies can only capture a small amount of
intrinsic supervision information. Additionally, the existing methods require
complex designs to obtain two different representations of the data. To
overcome these limitations, we propose a novel framework called the
Self-Contrastive Graph Diffusion Network (SCGDN). Our framework consists of two
main components: the Attentional Module (AttM) and the Diffusion Module (DiFM).
AttM aggregates higher-order structure and feature information to get an
excellent embedding, while DiFM balances the state of each node in the graph
through Laplacian diffusion learning and allows the cooperative evolution of
adjacency and feature information in the graph. Unlike existing methodologies,
SCGDN is an augmentation-free approach that avoids "sampling bias" and semantic
drift, without the need for pre-training. We conduct a high-quality sampling of
samples based on structure and feature information. If two nodes are neighbors,
they are considered positive samples of each other. If two disconnected nodes
are also unrelated on $k$NN graph, they are considered negative samples for
each other. The contrastive objective reasonably uses our proposed sampling
strategies, and the redundancy reduction term minimizes redundant information
in the embedding and can well retain more discriminative information. In this
novel framework, the graph self-contrastive learning paradigm gives expression
to a powerful force. SCGDN effectively balances between preserving high-order
structure information and avoiding overfitting. The results manifest that SCGDN
can consistently generate outperformance over both the contrastive methods and
the classical methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yixian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_K/0/1/0/all/0/1"&gt;Kun Zhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Function Value Learning: Adaptive Learning Rates Based on the Polyak Stepsize and Function Splitting in ERM. (arXiv:2307.14528v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14528</id>
        <link href="http://arxiv.org/abs/2307.14528"/>
        <updated>2023-07-29T00:48:57.466Z</updated>
        <summary type="html"><![CDATA[Here we develop variants of SGD (stochastic gradient descent) with an
adaptive step size that make use of the sampled loss values. In particular, we
focus on solving a finite sum-of-terms problem, also known as empirical risk
minimization. We first detail an idealized adaptive method called
$\texttt{SPS}_+$ that makes use of the sampled loss values and assumes
knowledge of the sampled loss at optimality. This $\texttt{SPS}_+$ is a minor
modification of the SPS (Stochastic Polyak Stepsize) method, where the step
size is enforced to be positive. We then show that $\texttt{SPS}_+$ achieves
the best known rates of convergence for SGD in the Lipschitz non-smooth. We
then move onto to develop $\texttt{FUVAL}$, a variant of $\texttt{SPS}_+$ where
the loss values at optimality are gradually learned, as opposed to being given.
We give three viewpoints of $\texttt{FUVAL}$, as a projection based method, as
a variant of the prox-linear method, and then as a particular online SGD
method. We then present a convergence analysis of $\texttt{FUVAL}$ and
experimental results. The shortcomings of our work is that the convergence
analysis of $\texttt{FUVAL}$ shows no advantage over SGD. Another shortcomming
is that currently only the full batch version of $\texttt{FUVAL}$ shows a minor
advantages of GD (Gradient Descent) in terms of sensitivity to the step size.
The stochastic version shows no clear advantage over SGD. We conjecture that
large mini-batches are required to make $\texttt{FUVAL}$ competitive.

Currently the new $\texttt{FUVAL}$ method studied in this paper does not
offer any clear theoretical or practical advantage. We have chosen to make this
draft available online nonetheless because of some of the analysis techniques
we use, such as the non-smooth analysis of $\texttt{SPS}_+$, and also to show
an apparently interesting approach that currently does not work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garrigos_G/0/1/0/all/0/1"&gt;Guillaume Garrigos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gower_R/0/1/0/all/0/1"&gt;Robert M. Gower&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaipp_F/0/1/0/all/0/1"&gt;Fabian Schaipp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A LLM Assisted Exploitation of AI-Guardian. (arXiv:2307.15008v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2307.15008</id>
        <link href="http://arxiv.org/abs/2307.15008"/>
        <updated>2023-07-29T00:48:57.465Z</updated>
        <summary type="html"><![CDATA[Large language models (LLMs) are now highly capable at a diverse range of
tasks. This paper studies whether or not GPT-4, one such LLM, is capable of
assisting researchers in the field of adversarial machine learning. As a case
study, we evaluate the robustness of AI-Guardian, a recent defense to
adversarial examples published at IEEE S&P 2023, a top computer security
conference. We completely break this defense: the proposed scheme does not
increase robustness compared to an undefended baseline.

We write none of the code to attack this model, and instead prompt GPT-4 to
implement all attack algorithms following our instructions and guidance. This
process was surprisingly effective and efficient, with the language model at
times producing code from ambiguous instructions faster than the author of this
paper could have done. We conclude by discussing (1) the warning signs present
in the evaluation that suggested to us AI-Guardian would be broken, and (2) our
experience with designing attacks and performing novel research using the most
recent advances in language modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1"&gt;Nicholas Carlini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bug Characterization in Machine Learning-based Systems. (arXiv:2307.14512v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2307.14512</id>
        <link href="http://arxiv.org/abs/2307.14512"/>
        <updated>2023-07-29T00:48:57.464Z</updated>
        <summary type="html"><![CDATA[Rapid growth of applying Machine Learning (ML) in different domains,
especially in safety-critical areas, increases the need for reliable ML
components, i.e., a software component operating based on ML. Understanding the
bugs characteristics and maintenance challenges in ML-based systems can help
developers of these systems to identify where to focus maintenance and testing
efforts, by giving insights into the most error-prone components, most common
bugs, etc. In this paper, we investigate the characteristics of bugs in
ML-based software systems and the difference between ML and non-ML bugs from
the maintenance viewpoint. We extracted 447,948 GitHub repositories that used
one of the three most popular ML frameworks, i.e., TensorFlow, Keras, and
PyTorch. After multiple filtering steps, we select the top 300 repositories
with the highest number of closed issues. We manually investigate the extracted
repositories to exclude non-ML-based systems. Our investigation involved a
manual inspection of 386 sampled reported issues in the identified ML-based
systems to indicate whether they affect ML components or not. Our analysis
shows that nearly half of the real issues reported in ML-based systems are ML
bugs, indicating that ML components are more error-prone than non-ML
components. Next, we thoroughly examined 109 identified ML bugs to identify
their root causes, symptoms, and calculate their required fixing time. The
results also revealed that ML bugs have significantly different characteristics
compared to non-ML bugs, in terms of the complexity of bug-fixing (number of
commits, changed files, and changed lines of code). Based on our results,
fixing ML bugs are more costly and ML components are more error-prone, compared
to non-ML bugs and non-ML components respectively. Hence, paying a significant
attention to the reliability of the ML components is crucial in ML-based
systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Morovati_M/0/1/0/all/0/1"&gt;Mohammad Mehdi Morovati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikanjam_A/0/1/0/all/0/1"&gt;Amin Nikanjam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tambon_F/0/1/0/all/0/1"&gt;Florian Tambon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1"&gt;Foutse Khomh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1"&gt;Zhen Ming&lt;/a&gt; (Jack) &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang/0/1/0/all/0/1"&gt;Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MVMR-FS : Non-parametric feature selection algorithm based on Maximum inter-class Variation and Minimum Redundancy. (arXiv:2307.14643v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14643</id>
        <link href="http://arxiv.org/abs/2307.14643"/>
        <updated>2023-07-29T00:48:57.464Z</updated>
        <summary type="html"><![CDATA[How to accurately measure the relevance and redundancy of features is an
age-old challenge in the field of feature selection. However, existing
filter-based feature selection methods cannot directly measure redundancy for
continuous data. In addition, most methods rely on manually specifying the
number of features, which may introduce errors in the absence of expert
knowledge. In this paper, we propose a non-parametric feature selection
algorithm based on maximum inter-class variation and minimum redundancy,
abbreviated as MVMR-FS. We first introduce supervised and unsupervised kernel
density estimation on the features to capture their similarities and
differences in inter-class and overall distributions. Subsequently, we present
the criteria for maximum inter-class variation and minimum redundancy (MVMR),
wherein the inter-class probability distributions are employed to reflect
feature relevance and the distances between overall probability distributions
are used to quantify redundancy. Finally, we employ an AGA to search for the
feature subset that minimizes the MVMR. Compared with ten state-of-the-art
methods, MVMR-FS achieves the highest average accuracy and improves the
accuracy by 5% to 11%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nie_H/0/1/0/all/0/1"&gt;Haitao Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengbo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1"&gt;Bin Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predictive Maintenance of Armoured Vehicles using Machine Learning Approaches. (arXiv:2307.14453v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14453</id>
        <link href="http://arxiv.org/abs/2307.14453"/>
        <updated>2023-07-29T00:48:57.463Z</updated>
        <summary type="html"><![CDATA[Armoured vehicles are specialized and complex pieces of machinery designed to
operate in high-stress environments, often in combat or tactical situations.
This study proposes a predictive maintenance-based ensemble system that aids in
predicting potential maintenance needs based on sensor data collected from
these vehicles. The proposed model's architecture involves various models such
as Light Gradient Boosting, Random Forest, Decision Tree, Extra Tree Classifier
and Gradient Boosting to predict the maintenance requirements of the vehicles
accurately. In addition, K-fold cross validation, along with TOPSIS analysis,
is employed to evaluate the proposed ensemble model's stability. The results
indicate that the proposed system achieves an accuracy of 98.93%, precision of
99.80% and recall of 99.03%. The algorithm can effectively predict maintenance
needs, thereby reducing vehicle downtime and improving operational efficiency.
Through comparisons between various algorithms and the suggested ensemble, this
study highlights the potential of machine learning-based predictive maintenance
solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sengupta_P/0/1/0/all/0/1"&gt;Prajit Sengupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_A/0/1/0/all/0/1"&gt;Anant Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rana_P/0/1/0/all/0/1"&gt;Prashant Singh Rana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open Problems in Computer Vision for Wilderness SAR and The Search for Patricia Wu-Murad. (arXiv:2307.14527v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.14527</id>
        <link href="http://arxiv.org/abs/2307.14527"/>
        <updated>2023-07-29T00:48:57.463Z</updated>
        <summary type="html"><![CDATA[This paper details the challenges in applying two computer vision systems, an
EfficientDET supervised learning model and the unsupervised RX spectral
classifier, to 98.9 GB of drone imagery from the Wu-Murad wilderness search and
rescue (WSAR) effort in Japan and identifies 3 directions for future research.
There have been at least 19 proposed approaches and 3 datasets aimed at
locating missing persons in drone imagery, but only 3 approaches (2
unsupervised and 1 of an unknown structure) are referenced in the literature as
having been used in an actual WSAR operation. Of these proposed approaches, the
EfficientDET architecture and the unsupervised spectral RX classifier were
selected as the most appropriate for this setting. The EfficientDET model was
applied to the HERIDAL dataset and despite achieving performance that is
statistically equivalent to the state-of-the-art, the model fails to translate
to the real world in terms of false positives (e.g., identifying tree limbs and
rocks as people), and false negatives (e.g., failing to identify members of the
search team). The poor results in practice for algorithms that showed good
results on datasets suggest 3 areas of future research: more realistic datasets
for wilderness SAR, computer vision models that are capable of seamlessly
handling the variety of imagery that can be collected during actual WSAR
operations, and better alignment on performance measures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manzini_T/0/1/0/all/0/1"&gt;Thomas Manzini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murphy_R/0/1/0/all/0/1"&gt;Robin Murphy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Explanations for Graph Classification Through the Lenses of Density. (arXiv:2307.14849v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14849</id>
        <link href="http://arxiv.org/abs/2307.14849"/>
        <updated>2023-07-29T00:48:57.462Z</updated>
        <summary type="html"><![CDATA[Counterfactual examples have emerged as an effective approach to produce
simple and understandable post-hoc explanations. In the context of graph
classification, previous work has focused on generating counterfactual
explanations by manipulating the most elementary units of a graph, i.e.,
removing an existing edge, or adding a non-existing one. In this paper, we
claim that such language of explanation might be too fine-grained, and turn our
attention to some of the main characterizing features of real-world complex
networks, such as the tendency to close triangles, the existence of recurring
motifs, and the organization into dense modules. We thus define a general
density-based counterfactual search framework to generate instance-level
counterfactual explanations for graph classifiers, which can be instantiated
with different notions of dense substructures. In particular, we show two
specific instantiations of this general framework: a method that searches for
counterfactual graphs by opening or closing triangles, and a method driven by
maximal cliques. We also discuss how the general method can be instantiated to
exploit any other notion of dense substructures, including, for instance, a
given taxonomy of nodes. We evaluate the effectiveness of our approaches in 7
brain network datasets and compare the counterfactual statements generated
according to several widely-used metrics. Results confirm that adopting a
semantic-relevant unit of change like density is essential to define versatile
and interpretable counterfactual explanation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abrate_C/0/1/0/all/0/1"&gt;Carlo Abrate&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preti_G/0/1/0/all/0/1"&gt;Giulia Preti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonchi_F/0/1/0/all/0/1"&gt;Francesco Bonchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?. (arXiv:2307.14642v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.14642</id>
        <link href="http://arxiv.org/abs/2307.14642"/>
        <updated>2023-07-29T00:48:57.461Z</updated>
        <summary type="html"><![CDATA[We prove that black-box variational inference (BBVI) with control variates,
particularly the sticking-the-landing (STL) estimator, converges at a geometric
(traditionally called "linear") rate under perfect variational family
specification. In particular, we prove a quadratic bound on the gradient
variance of the STL estimator, one which encompasses misspecified variational
families. Combined with previous works on the quadratic variance condition,
this directly implies convergence of BBVI with the use of projected stochastic
gradient descent. We also improve existing analysis on the regular closed-form
entropy gradient estimators, which enables comparison against the STL estimator
and provides explicit non-asymptotic complexity guarantees for both.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kyurae Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gardner_J/0/1/0/all/0/1"&gt;Jacob R. Gardner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[2D-Shapley: A Framework for Fragmented Data Valuation. (arXiv:2306.10473v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.10473</id>
        <link href="http://arxiv.org/abs/2306.10473"/>
        <updated>2023-07-29T00:48:57.461Z</updated>
        <summary type="html"><![CDATA[Data valuation -- quantifying the contribution of individual data sources to
certain predictive behaviors of a model -- is of great importance to enhancing
the transparency of machine learning and designing incentive systems for data
sharing. Existing work has focused on evaluating data sources with the shared
feature or sample space. How to valuate fragmented data sources of which each
only contains partial features and samples remains an open question. We start
by presenting a method to calculate the counterfactual of removing a fragment
from the aggregated data matrix. Based on the counterfactual calculation, we
further propose 2D-Shapley, a theoretical framework for fragmented data
valuation that uniquely satisfies some appealing axioms in the fragmented data
context. 2D-Shapley empowers a range of new use cases, such as selecting useful
data fragments, providing interpretation for sample-wise data values, and
fine-grained data issue diagnosis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhihong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Just_H/0/1/0/all/0/1"&gt;Hoang Anh Just&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1"&gt;Xiangyu Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Ruoxi Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Good is Google Bard's Visual Understanding? An Empirical Study on Open Challenges. (arXiv:2307.15016v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.15016</id>
        <link href="http://arxiv.org/abs/2307.15016"/>
        <updated>2023-07-29T00:48:57.460Z</updated>
        <summary type="html"><![CDATA[Google's Bard has emerged as a formidable competitor to OpenAI's ChatGPT in
the field of conversational AI. Notably, Bard has recently been updated to
handle visual inputs alongside text prompts during conversations. Given Bard's
impressive track record in handling textual inputs, we explore its capabilities
in understanding and interpreting visual data (images) conditioned by text
questions. This exploration holds the potential to unveil new insights and
challenges for Bard and other forthcoming multi-modal Generative models,
especially in addressing complex computer vision problems that demand accurate
visual and language understanding. Specifically, in this study, we focus on 15
diverse task scenarios encompassing regular, camouflaged, medical, under-water
and remote sensing data to comprehensively evaluate Bard's performance. Our
primary finding indicates that Bard still struggles in these vision scenarios,
highlighting the significant gap in vision-based understanding that needs to be
bridged in future developments. We expect that this empirical study will prove
valuable in advancing future models, leading to enhanced capabilities in
comprehending and interpreting fine-grained visual data. Our project is
released on https://github.com/htqin/GoogleBard-VisUnderstand]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1"&gt;Haotong Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1"&gt;Ge-Peng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Salman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1"&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahad Shahbaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Strategic Framework for Optimal Decisions in Football 1-vs-1 Shot-Taking Situations: An Integrated Approach of Machine Learning, Theory-Based Modeling, and Game Theory. (arXiv:2307.14732v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14732</id>
        <link href="http://arxiv.org/abs/2307.14732"/>
        <updated>2023-07-29T00:48:57.459Z</updated>
        <summary type="html"><![CDATA[Complex interactions between two opposing agents frequently occur in domains
of machine learning, game theory, and other application domains. Quantitatively
analyzing the strategies involved can provide an objective basis for
decision-making. One such critical scenario is shot-taking in football, where
decisions, such as whether the attacker should shoot or pass the ball and
whether the defender should attempt to block the shot, play a crucial role in
the outcome of the game. However, there are currently no effective data-driven
and/or theory-based approaches to analyzing such situations. To address this
issue, we proposed a novel framework to analyze such scenarios based on game
theory, where we estimate the expected payoff with machine learning (ML)
models, and additional features for ML models were extracted with a
theory-based shot block model. Conventionally, successes or failures (1 or 0)
are used as payoffs, while a success shot (goal) is extremely rare in football.
Therefore, we proposed the Expected Probability of Shot On Target (xSOT) metric
to evaluate players' actions even if the shot results in no goal; this allows
for effective differentiation and comparison between different shots and even
enables counterfactual shot situation analysis. In our experiments, we have
validated the framework by comparing it with baseline and ablated models.
Furthermore, we have observed a high correlation between the xSOT and existing
metrics. This alignment of information suggests that xSOT provides valuable
insights. Lastly, as an illustration, we studied optimal strategies in the
World Cup 2022 and analyzed a shot situation in EURO 2020.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yeung_C/0/1/0/all/0/1"&gt;Calvin C. K. Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1"&gt;Keisuke Fujii&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative convective parametrization of dry atmospheric boundary layer. (arXiv:2307.14857v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2307.14857</id>
        <link href="http://arxiv.org/abs/2307.14857"/>
        <updated>2023-07-29T00:48:57.459Z</updated>
        <summary type="html"><![CDATA[Turbulence parametrizations will remain a necessary building block in
kilometer-scale Earth system models. In convective boundary layers, where the
mean vertical gradients of conserved properties such as potential temperature
and moisture are approximately zero, the standard ansatz which relates
turbulent fluxes to mean vertical gradients via an eddy diffusivity has to be
extended by mass flux parametrizations for the typically asymmetric up- and
downdrafts in the atmospheric boundary layer. In this work, we present a
parametrization for a dry convective boundary layer based on a generative
adversarial network. The model incorporates the physics of self-similar layer
growth following from the classical mixed layer theory by Deardorff. This
enhances the training data base of the generative machine learning algorithm
and thus significantly improves the predicted statistics of the synthetically
generated turbulence fields at different heights inside the boundary layer. The
algorithm training is based on fully three-dimensional direct numerical
simulation data. Differently to stochastic parametrizations, our model is able
to predict the highly non-Gaussian transient statistics of buoyancy
fluctuations, vertical velocity, and buoyancy flux at different heights thus
also capturing the fastest thermals penetrating into the stabilized top region.
The results of our generative algorithm agree with standard two-equation or
multi-plume stochastic mass-flux schemes. The present parametrization provides
additionally the granule-type horizontal organization of the turbulent
convection which cannot be obtained in any of the other model closures. Our
work paves the way to efficient data-driven convective parametrizations in
other natural flows, such as moist convection, upper ocean mixing, or
convection in stellar interiors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Heyder_F/0/1/0/all/0/1"&gt;Florian Heyder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Mellado_J/0/1/0/all/0/1"&gt;Juan Pedro Mellado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Schumacher_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rg Schumacher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thinker: Learning to Plan and Act. (arXiv:2307.14993v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2307.14993</id>
        <link href="http://arxiv.org/abs/2307.14993"/>
        <updated>2023-07-29T00:48:57.458Z</updated>
        <summary type="html"><![CDATA[We propose the Thinker algorithm, a novel approach that enables reinforcement
learning agents to autonomously interact with and utilize a learned world
model. The Thinker algorithm wraps the environment with a world model and
introduces new actions designed for interacting with the world model. These
model-interaction actions enable agents to perform planning by proposing
alternative plans to the world model before selecting a final action to execute
in the environment. This approach eliminates the need for hand-crafted planning
algorithms by enabling the agent to learn how to plan autonomously and allows
for easy interpretation of the agent's plan with visualization. We demonstrate
the algorithm's effectiveness through experimental results in the game of
Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves
state-of-the-art performance and competitive results, respectively.
Visualizations of agents trained with the Thinker algorithm demonstrate that
they have learned to plan effectively with the world model to select better
actions. The algorithm's generality opens a new research direction on how a
world model can be used in reinforcement learning and how planning can be
seamlessly integrated into an agent's decision-making process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1"&gt;Stephen Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anokhin_I/0/1/0/all/0/1"&gt;Ivan Anokhin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krueger_D/0/1/0/all/0/1"&gt;David Krueger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BubbleML: A Multi-Physics Dataset and Benchmarks for Machine Learning. (arXiv:2307.14623v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14623</id>
        <link href="http://arxiv.org/abs/2307.14623"/>
        <updated>2023-07-29T00:48:57.457Z</updated>
        <summary type="html"><![CDATA[In the field of phase change phenomena, the lack of accessible and diverse
datasets suitable for machine learning (ML) training poses a significant
challenge. Existing experimental datasets are often restricted, with limited
availability and sparse ground truth data, impeding our understanding of this
complex multi-physics phenomena. To bridge this gap, we present the BubbleML
Dataset(https://github.com/HPCForge/BubbleML) which leverages physics-driven
simulations to provide accurate ground truth information for various boiling
scenarios, encompassing nucleate pool boiling, flow boiling, and sub-cooled
boiling. This extensive dataset covers a wide range of parameters, including
varying gravity conditions, flow rates, sub-cooling levels, and wall superheat,
comprising 51 simulations. BubbleML is validated against experimental
observations and trends, establishing it as an invaluable resource for ML
research. Furthermore, we showcase its potential to facilitate exploration of
diverse downstream tasks by introducing two benchmarks: (a) optical flow
analysis to capture bubble dynamics, and (b) operator networks for learning
temperature dynamics. The BubbleML dataset and its benchmarks serve as a
catalyst for advancements in ML-driven research on multi-physics phase change
phenomena, enabling the development and comparison of state-of-the-art
techniques and models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hassan_S/0/1/0/all/0/1"&gt;Sheikh Md Shakeel Hassan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feeney_A/0/1/0/all/0/1"&gt;Arthur Feeney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhruv_A/0/1/0/all/0/1"&gt;Akash Dhruv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jihoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suh_Y/0/1/0/all/0/1"&gt;Youngjoon Suh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ryu_J/0/1/0/all/0/1"&gt;Jaiyoung Ryu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Won_Y/0/1/0/all/0/1"&gt;Yoonjin Won&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandramowlishwaran_A/0/1/0/all/0/1"&gt;Aparna Chandramowlishwaran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking Performance of Deep Learning Model for Material Segmentation on Two HPC Systems. (arXiv:2307.14921v1 [cs.PF])]]></title>
        <id>http://arxiv.org/abs/2307.14921</id>
        <link href="http://arxiv.org/abs/2307.14921"/>
        <updated>2023-07-29T00:48:57.457Z</updated>
        <summary type="html"><![CDATA[Performance Benchmarking of HPC systems is an ongoing effort that seeks to
provide information that will allow for increased performance and improve the
job schedulers that manage these systems. We develop a benchmarking tool that
utilizes machine learning models and gathers performance data on
GPU-accelerated nodes while they perform material segmentation analysis. The
benchmark uses a ML model that has been converted from Caffe to PyTorch using
the MMdnn toolkit and the MINC-2500 dataset. Performance data is gathered on
two ERDC DSRC systems, Onyx and Vulcanite. The data reveals that while
Vulcanite has faster model times in a large number of benchmarks, and it is
also more subject to some environmental factors that can cause performances
slower than Onyx. In contrast the model times from Onyx are consistent across
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Williams_W/0/1/0/all/0/1"&gt;Warren R. Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glandon_S/0/1/0/all/0/1"&gt;S. Ross Glandon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morris_L/0/1/0/all/0/1"&gt;Luke L. Morris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1"&gt;Jing-Ru C. Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-based Polyphonic Multitrack Music Generation. (arXiv:2307.14928v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2307.14928</id>
        <link href="http://arxiv.org/abs/2307.14928"/>
        <updated>2023-07-29T00:48:57.456Z</updated>
        <summary type="html"><![CDATA[Graphs can be leveraged to model polyphonic multitrack symbolic music, where
notes, chords and entire sections may be linked at different levels of the
musical hierarchy by tonal and rhythmic relationships. Nonetheless, there is a
lack of works that consider graph representations in the context of deep
learning systems for music generation. This paper bridges this gap by
introducing a novel graph representation for music and a deep Variational
Autoencoder that generates the structure and the content of musical graphs
separately, one after the other, with a hierarchical architecture that matches
the structural priors of music. By separating the structure and content of
musical graphs, it is possible to condition generation by specifying which
instruments are played at certain times. This opens the door to a new form of
human-computer interaction in the context of music co-creation. After training
the model on existing MIDI datasets, the experiments show that the model is
able to generate appealing short and long musical sequences and to
realistically interpolate between them, producing music that is tonally and
rhythmically consistent. Finally, the visualization of the embeddings shows
that the model is able to organize its latent space in accordance with known
musical concepts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cosenza_E/0/1/0/all/0/1"&gt;Emanuele Cosenza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valenti_A/0/1/0/all/0/1"&gt;Andrea Valenti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1"&gt;Davide Bacciu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification. (arXiv:2307.14959v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.14959</id>
        <link href="http://arxiv.org/abs/2307.14959"/>
        <updated>2023-07-29T00:48:57.455Z</updated>
        <summary type="html"><![CDATA[In the medical field, federated learning commonly deals with highly
imbalanced datasets, including skin lesions and gastrointestinal images.
Existing federated methods under highly imbalanced datasets primarily focus on
optimizing a global model without incorporating the intra-class variations that
can arise in medical imaging due to different populations, findings, and
scanners. In this paper, we study the inter-client intra-class variations with
publicly available self-supervised auxiliary networks. Specifically, we find
that employing a shared auxiliary pre-trained model, like MoCo-V2, locally on
every client yields consistent divergence measurements. Based on these
findings, we derive a dynamic balanced model aggregation via self-supervised
priors (MAS) to guide the global model optimization. Fed-MAS can be utilized
with different local learning methods for effective model aggregation toward a
highly robust and unbiased global model. Our code is available at
\url{https://github.com/xmed-lab/Fed-MAS}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elbatel_M/0/1/0/all/0/1"&gt;Marawan Elbatel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hualiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marti_R/0/1/0/all/0/1"&gt;Robert Mart&amp;#xed;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1"&gt;Huazhu Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaomeng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Quantum Boltzmann Machines with Coresets. (arXiv:2307.14459v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2307.14459</id>
        <link href="http://arxiv.org/abs/2307.14459"/>
        <updated>2023-07-29T00:48:57.423Z</updated>
        <summary type="html"><![CDATA[Recent work has proposed and explored using coreset techniques for quantum
algorithms that operate on classical data sets to accelerate the applicability
of these algorithms on near-term quantum devices. We apply these ideas to
Quantum Boltzmann Machines (QBM) where gradient-based steps which require Gibbs
state sampling are the main computational bottleneck during training. By using
a coreset in place of the full data set, we try to minimize the number of steps
needed and accelerate the overall training time. In a regime where
computational time on quantum computers is a precious resource, we propose this
might lead to substantial practical savings. We evaluate this approach on 6x6
binary images from an augmented bars and stripes data set using a QBM with 36
visible units and 8 hidden units. Using an Inception score inspired metric, we
compare QBM training times with and without using coresets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Viszlai_J/0/1/0/all/0/1"&gt;Joshua Viszlai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Tomesh_T/0/1/0/all/0/1"&gt;Teague Tomesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Gokhale_P/0/1/0/all/0/1"&gt;Pranav Gokhale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Anschuetz_E/0/1/0/all/0/1"&gt;Eric Anschuetz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Chong_F/0/1/0/all/0/1"&gt;Frederic T. Chong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning based Parameter Sensitivity of Regional Climate Models -- A Case Study of the WRF Model for Heat Extremes over Southeast Australia. (arXiv:2307.14654v1 [physics.ao-ph])]]></title>
        <id>http://arxiv.org/abs/2307.14654</id>
        <link href="http://arxiv.org/abs/2307.14654"/>
        <updated>2023-07-29T00:48:57.423Z</updated>
        <summary type="html"><![CDATA[Heatwaves and bushfires cause substantial impacts on society and ecosystems
across the globe. Accurate information of heat extremes is needed to support
the development of actionable mitigation and adaptation strategies. Regional
climate models are commonly used to better understand the dynamics of these
events. These models have very large input parameter sets, and the parameters
within the physics schemes substantially influence the model's performance.
However, parameter sensitivity analysis (SA) of regional models for heat
extremes is largely unexplored. Here, we focus on the southeast Australian
region, one of the global hotspots of heat extremes. In southeast Australia
Weather Research and Forecasting (WRF) model is the widely used regional model
to simulate extreme weather events across the region. Hence in this study, we
focus on the sensitivity of WRF model parameters to surface meteorological
variables such as temperature, relative humidity, and wind speed during two
extreme heat events over southeast Australia. Due to the presence of multiple
parameters and their complex relationship with output variables, a machine
learning (ML) surrogate-based global sensitivity analysis method is considered
for the SA. The ML surrogate-based Sobol SA is used to identify the sensitivity
of 24 adjustable parameters in seven different physics schemes of the WRF
model. Results show that out of these 24, only three parameters, namely the
scattering tuning parameter, multiplier of saturated soil water content, and
profile shape exponent in the momentum diffusivity coefficient, are important
for the considered meteorological variables. These SA results are consistent
for the two different extreme heat events. Further, we investigated the
physical significance of sensitive parameters. This study's results will help
in further optimising WRF parameters to improve model simulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Reddy_P/0/1/0/all/0/1"&gt;P. Jyoteeshkumar Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Chinta_S/0/1/0/all/0/1"&gt;Sandeep Chinta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Matear_R/0/1/0/all/0/1"&gt;Richard Matear&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Taylor_J/0/1/0/all/0/1"&gt;John Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Baki_H/0/1/0/all/0/1"&gt;Harish Baki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Thatcher_M/0/1/0/all/0/1"&gt;Marcus Thatcher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kala_J/0/1/0/all/0/1"&gt;Jatin Kala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Sharples_J/0/1/0/all/0/1"&gt;Jason Sharples&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction of wind turbines power with physics-informed neural networks and evidential uncertainty quantification. (arXiv:2307.14675v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14675</id>
        <link href="http://arxiv.org/abs/2307.14675"/>
        <updated>2023-07-29T00:48:57.423Z</updated>
        <summary type="html"><![CDATA[The ever-growing use of wind energy makes necessary the optimization of
turbine operations through pitch angle controllers and their maintenance with
early fault detection. It is crucial to have accurate and robust models
imitating the behavior of wind turbines, especially to predict the generated
power as a function of the wind speed. Existing empirical and physics-based
models have limitations in capturing the complex relations between the input
variables and the power, aggravated by wind variability. Data-driven methods
offer new opportunities to enhance wind turbine modeling of large datasets by
improving accuracy and efficiency. In this study, we used physics-informed
neural networks to reproduce historical data coming from 4 turbines in a wind
farm, while imposing certain physical constraints to the model. The developed
models for regression of the power, torque, and power coefficient as output
variables showed great accuracy for both real data and physical equations
governing the system. Lastly, introducing an efficient evidential layer
provided uncertainty estimations of the predictions, proved to be consistent
with the absolute error, and made possible the definition of a confidence
interval in the power curve.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gijon_A/0/1/0/all/0/1"&gt;Alfonso Gij&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pujana_Goitia_A/0/1/0/all/0/1"&gt;Ainhoa Pujana-Goitia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perea_E/0/1/0/all/0/1"&gt;Eugenio Perea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Molina_Solana_M/0/1/0/all/0/1"&gt;Miguel Molina-Solana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_Romero_J/0/1/0/all/0/1"&gt;Juan G&amp;#xf3;mez-Romero&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Assignment of Labels for Active Learning with Sparse and Noisy Annotations. (arXiv:2307.14380v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14380</id>
        <link href="http://arxiv.org/abs/2307.14380"/>
        <updated>2023-07-29T00:48:57.422Z</updated>
        <summary type="html"><![CDATA[Supervised classification algorithms are used to solve a growing number of
real-life problems around the globe. Their performance is strictly connected
with the quality of labels used in training. Unfortunately, acquiring
good-quality annotations for many tasks is infeasible or too expensive to be
done in practice. To tackle this challenge, active learning algorithms are
commonly employed to select only the most relevant data for labeling. However,
this is possible only when the quality and quantity of labels acquired from
experts are sufficient. Unfortunately, in many applications, a trade-off
between annotating individual samples by multiple annotators to increase label
quality vs. annotating new samples to increase the total number of labeled
instances is necessary. In this paper, we address the issue of faulty data
annotations in the context of active learning. In particular, we propose two
novel annotation unification algorithms that utilize unlabeled parts of the
sample space. The proposed methods require little to no intersection between
samples annotated by different experts. Our experiments on four public datasets
indicate the robustness and superiority of the proposed methods in both, the
estimation of the annotator's reliability, and the assignment of actual labels,
against the state-of-the-art algorithms and the simple majority voting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaluza_D/0/1/0/all/0/1"&gt;Daniel Ka&amp;#x142;u&amp;#x17c;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Janusz_A/0/1/0/all/0/1"&gt;Andrzej Janusz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slezak_D/0/1/0/all/0/1"&gt;Dominik &amp;#x15a;l&amp;#x119;zak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HUTFormer: Hierarchical U-Net Transformer for Long-Term Traffic Forecasting. (arXiv:2307.14596v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14596</id>
        <link href="http://arxiv.org/abs/2307.14596"/>
        <updated>2023-07-29T00:48:57.422Z</updated>
        <summary type="html"><![CDATA[Traffic forecasting, which aims to predict traffic conditions based on
historical observations, has been an enduring research topic and is widely
recognized as an essential component of intelligent transportation. Recent
proposals on Spatial-Temporal Graph Neural Networks (STGNNs) have made
significant progress by combining sequential models with graph convolution
networks. However, due to high complexity issues, STGNNs only focus on
short-term traffic forecasting, e.g., 1-hour forecasting, while ignoring more
practical long-term forecasting. In this paper, we make the first attempt to
explore long-term traffic forecasting, e.g., 1-day forecasting. To this end, we
first reveal its unique challenges in exploiting multi-scale representations.
Then, we propose a novel Hierarchical U-net TransFormer (HUTFormer) to address
the issues of long-term traffic forecasting. HUTFormer consists of a
hierarchical encoder and decoder to jointly generate and utilize multi-scale
representations of traffic data. Specifically, for the encoder, we propose
window self-attention and segment merging to extract multi-scale
representations from long-term traffic data. For the decoder, we design a
cross-scale attention mechanism to effectively incorporate multi-scale
representations. In addition, HUTFormer employs an efficient input embedding
strategy to address the complexity issues. Extensive experiments on four
traffic datasets show that the proposed HUTFormer significantly outperforms
state-of-the-art traffic forecasting and long time series forecasting
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1"&gt;Zezhi Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuchen Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_G/0/1/0/all/0/1"&gt;Guangyin Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yongjun Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rapid and Scalable Bayesian AB Testing. (arXiv:2307.14628v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14628</id>
        <link href="http://arxiv.org/abs/2307.14628"/>
        <updated>2023-07-29T00:48:57.422Z</updated>
        <summary type="html"><![CDATA[AB testing aids business operators with their decision making, and is
considered the gold standard method for learning from data to improve digital
user experiences. However, there is usually a gap between the requirements of
practitioners, and the constraints imposed by the statistical hypothesis
testing methodologies commonly used for analysis of AB tests. These include the
lack of statistical power in multivariate designs with many factors,
correlations between these factors, the need of sequential testing for early
stopping, and the inability to pool knowledge from past tests. Here, we propose
a solution that applies hierarchical Bayesian estimation to address the above
limitations. In comparison to current sequential AB testing methodology, we
increase statistical power by exploiting correlations between factors, enabling
sequential testing and progressive early stopping, without incurring excessive
false positive risk. We also demonstrate how this methodology can be extended
to enable the extraction of composite global learnings from past AB tests, to
accelerate future tests. We underpin our work with a solid theoretical
framework that articulates the value of hierarchical estimation. We demonstrate
its utility using both numerical simulations and a large set of real-world AB
tests. Together, these results highlight the practical value of our approach
for statistical inference in the technology industry.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chennu_S/0/1/0/all/0/1"&gt;Srivas Chennu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maher_A/0/1/0/all/0/1"&gt;Andrew Maher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pangerl_C/0/1/0/all/0/1"&gt;Christian Pangerl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prabanantham_S/0/1/0/all/0/1"&gt;Subash Prabanantham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1"&gt;Jae Hyeon Bae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1"&gt;Jamie Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goswami_B/0/1/0/all/0/1"&gt;Bud Goswami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bipartite Ranking Fairness through a Model Agnostic Ordering Adjustment. (arXiv:2307.14668v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14668</id>
        <link href="http://arxiv.org/abs/2307.14668"/>
        <updated>2023-07-29T00:48:57.421Z</updated>
        <summary type="html"><![CDATA[Algorithmic fairness has been a serious concern and received lots of interest
in machine learning community. In this paper, we focus on the bipartite ranking
scenario, where the instances come from either the positive or negative class
and the goal is to learn a ranking function that ranks positive instances
higher than negative ones. While there could be a trade-off between fairness
and performance, we propose a model agnostic post-processing framework xOrder
for achieving fairness in bipartite ranking and maintaining the algorithm
classification performance. In particular, we optimize a weighted sum of the
utility as identifying an optimal warping path across different protected
groups and solve it through a dynamic programming process. xOrder is compatible
with various classification models and ranking fairness metrics, including
supervised and unsupervised fairness metrics. In addition to binary groups,
xOrder can be applied to multiple protected groups. We evaluate our proposed
algorithm on four benchmark data sets and two real-world patient electronic
health record repositories. xOrder consistently achieves a better balance
between the algorithm utility and ranking fairness on a variety of datasets
with different metrics. From the visualization of the calibrated ranking
scores, xOrder mitigates the score distribution shifts of different groups
compared with baselines. Moreover, additional analytical results verify that
xOrder achieves a robust performance when faced with fewer samples and a bigger
difference between training and testing ranking score distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Sen Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1"&gt;Weishen Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Complete and separate: Conditional separation with missing target source attribute completion. (arXiv:2307.14609v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2307.14609</id>
        <link href="http://arxiv.org/abs/2307.14609"/>
        <updated>2023-07-29T00:48:57.420Z</updated>
        <summary type="html"><![CDATA[Recent approaches in source separation leverage semantic information about
their input mixtures and constituent sources that when used in conditional
separation models can achieve impressive performance. Most approaches along
these lines have focused on simple descriptions, which are not always useful
for varying types of input mixtures. In this work, we present an approach in
which a model, given an input mixture and partial semantic information about a
target source, is trained to extract additional semantic data. We then leverage
this pre-trained model to improve the separation performance of an uncoupled
multi-conditional separation network. Our experiments demonstrate that the
separation performance of this multi-conditional model is significantly
improved, approaching the performance of an oracle model with complete semantic
information. Furthermore, our approach achieves performance levels that are
comparable to those of the best performing specialized single conditional
models, thus providing an easier to use alternative.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bralios_D/0/1/0/all/0/1"&gt;Dimitrios Bralios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1"&gt;Efthymios Tzinis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1"&gt;Paris Smaragdis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Silent Failures in Medical Image Classification. (arXiv:2307.14729v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2307.14729</id>
        <link href="http://arxiv.org/abs/2307.14729"/>
        <updated>2023-07-29T00:48:57.420Z</updated>
        <summary type="html"><![CDATA[To ensure the reliable use of classification systems in medical applications,
it is crucial to prevent silent failures. This can be achieved by either
designing classifiers that are robust enough to avoid failures in the first
place, or by detecting remaining failures using confidence scoring functions
(CSFs). A predominant source of failures in image classification is
distribution shifts between training data and deployment data. To understand
the current state of silent failure prevention in medical imaging, we conduct
the first comprehensive analysis comparing various CSFs in four biomedical
tasks and a diverse range of distribution shifts. Based on the result that none
of the benchmarked CSFs can reliably prevent silent failures, we conclude that
a deeper understanding of the root causes of failures in the data is required.
To facilitate this, we introduce SF-Visuals, an interactive analysis tool that
uses latent space clustering to visualize shifts and failures. On the basis of
various examples, we demonstrate how this tool can help researchers gain
insight into the requirements for safe application of classification systems in
the medical domain. The open-source benchmark and tool are at:
https://github.com/IML-DKFZ/sf-visuals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bungert_T/0/1/0/all/0/1"&gt;Till J. Bungert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kobelke_L/0/1/0/all/0/1"&gt;Levin Kobelke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jaeger_P/0/1/0/all/0/1"&gt;Paul F. Jaeger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers. (arXiv:2307.14367v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2307.14367</id>
        <link href="http://arxiv.org/abs/2307.14367"/>
        <updated>2023-07-29T00:48:57.380Z</updated>
        <summary type="html"><![CDATA[The complex nature of big biological systems pushed some scientists to
classify its understanding under the inconceivable missions. Different leveled
challenges complicated this task, one of is the prediction of a protein's
function. In recent years, significant progress has been made in this field
through the development of various machine learning approaches. However, most
existing methods formulate the task as a multi-classification problem, i.e
assigning predefined labels to proteins. In this work, we propose a novel
approach, \textbf{Prot2Text}, which predicts a protein function's in a free
text style, moving beyond the conventional binary or categorical
classifications. By combining Graph Neural Networks(GNNs) and Large Language
Models(LLMs), in an encoder-decoder framework, our model effectively integrates
diverse data types including proteins' sequences, structures, and textual
annotations. This multimodal approach allows for a holistic representation of
proteins' functions, enabling the generation of detailed and accurate
descriptions. To evaluate our model, we extracted a multimodal protein dataset
from SwissProt, and demonstrate empirically the effectiveness of Prot2Text.
These results highlight the transformative impact of multimodal models,
specifically the fusion of GNNs and LLMs, empowering researchers with powerful
tools for more accurate prediction of proteins' functions. The code, the models
and a demo will be publicly released.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Abdine_H/0/1/0/all/0/1"&gt;Hadi Abdine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chatzianastasis_M/0/1/0/all/0/1"&gt;Michail Chatzianastasis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Bouyioukos_C/0/1/0/all/0/1"&gt;Costas Bouyioukos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Vazirgiannis_M/0/1/0/all/0/1"&gt;Michalis Vazirgiannis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synergies Between Federated Learning and O-RAN: Towards an Elastic Virtualized Architecture for Multiple Distributed Machine Learning Services. (arXiv:2305.02109v2 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.02109</id>
        <link href="http://arxiv.org/abs/2305.02109"/>
        <updated>2023-07-29T00:48:57.380Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) is the most popular distributed machine learning
technique. However, implementation of FL over modern wireless networks faces
key challenges caused by (i) dynamics of the network conditions and (ii) the
coexistence of multiple FL services/tasks and other network services in the
system, which are not jointly considered in prior works. Motivated by these
challenges, we introduce a generic FL paradigm over NextG networks, called
dynamic multi-service FL (DMS-FL). We identify three unexplored design
considerations in DMS-FL: (i) FL service operator accumulation, (ii) wireless
resource fragmentation, and (iii) signal strength fluctuations. We take the
first steps towards addressing these design considerations by proposing a novel
distributed ML architecture called elastic virtualized FL (EV-FL). EV-FL
unleashes the full potential of Open RAN (O-RAN) systems and introduces an
elastic resource provisioning methodology to execute FL services. It further
constitutes a multi-time-scale FL management system that introduces three
dimensions into existing FL architectures: (i) virtualization, (ii)
scalability, and (iii) elasticity. Through investigating EV-FL, we reveal a
series of open research directions for future work. We finally simulate EV-FL
to demonstrate its potential in saving wireless resources and increasing
fairness among FL services.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdisarabshali_P/0/1/0/all/0/1"&gt;Payam Abdisarabshali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Accurso_N/0/1/0/all/0/1"&gt;Nicholas Accurso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malandra_F/0/1/0/all/0/1"&gt;Filippo Malandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1"&gt;Weifeng Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosseinalipour_S/0/1/0/all/0/1"&gt;Seyyedali Hosseinalipour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Predictive Model of Digital Information Engagement: Forecasting User Engagement With English Words by Incorporating Cognitive Biases, Computational Linguistics and Natural Language Processing. (arXiv:2307.14500v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2307.14500</id>
        <link href="http://arxiv.org/abs/2307.14500"/>
        <updated>2023-07-29T00:48:57.379Z</updated>
        <summary type="html"><![CDATA[This study introduces and empirically tests a novel predictive model for
digital information engagement (IE) - the READ model, an acronym for the four
pivotal attributes of engaging information: Representativeness, Ease-of-use,
Affect, and Distribution. Conceptualized within the theoretical framework of
Cumulative Prospect Theory, the model integrates key cognitive biases with
computational linguistics and natural language processing to develop a
multidimensional perspective on information engagement. A rigorous testing
protocol was implemented, involving 50 randomly selected pairs of synonymous
words (100 words in total) from the WordNet database. These words' engagement
levels were evaluated through a large-scale online survey (n = 80,500) to
derive empirical IE metrics. The READ attributes for each word were then
computed and their predictive efficacy examined. The findings affirm the READ
model's robustness, accurately predicting a word's IE level and distinguishing
the more engaging word from a pair of synonyms with an 84% accuracy rate. The
READ model's potential extends across various domains, including business,
education, government, and healthcare, where it could enhance content
engagement and inform AI language model development and generative text work.
Future research should address the model's scalability and adaptability across
different domains and languages, thereby broadening its applicability and
efficacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dvir_N/0/1/0/all/0/1"&gt;Nimrod Dvir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friedman_E/0/1/0/all/0/1"&gt;Elaine Friedman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Commuri_S/0/1/0/all/0/1"&gt;Suraj Commuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+yang_F/0/1/0/all/0/1"&gt;Fan yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romano_J/0/1/0/all/0/1"&gt;Jennifer Romano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernel's Spectrum. (arXiv:2307.14531v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14531</id>
        <link href="http://arxiv.org/abs/2307.14531"/>
        <updated>2023-07-29T00:48:57.379Z</updated>
        <summary type="html"><![CDATA[Wide neural networks are biased towards learning certain functions,
influencing both the rate of convergence of gradient descent (GD) and the
functions that are reachable with GD in finite training time. As such, there is
a great need for methods that can modify this bias according to the task at
hand. To that end, we introduce Modified Spectrum Kernels (MSKs), a novel
family of constructed kernels that can be used to approximate kernels with
desired eigenvalues for which no closed form is known. We leverage the duality
between wide neural networks and Neural Tangent Kernels and propose a
preconditioned gradient descent method, which alters the trajectory of GD. As a
result, this allows for a polynomial and, in some cases, exponential training
speedup without changing the final solution. Our method is both computationally
efficient and simple to implement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geifman_A/0/1/0/all/0/1"&gt;Amnon Geifman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzilai_D/0/1/0/all/0/1"&gt;Daniel Barzilai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basri_R/0/1/0/all/0/1"&gt;Ronen Basri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galun_M/0/1/0/all/0/1"&gt;Meirav Galun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial-Frequency U-Net for Denoising Diffusion Probabilistic Models. (arXiv:2307.14648v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.14648</id>
        <link href="http://arxiv.org/abs/2307.14648"/>
        <updated>2023-07-29T00:48:57.379Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the denoising diffusion probabilistic model (DDPM) in
wavelet space, instead of pixel space, for visual synthesis. Considering the
wavelet transform represents the image in spatial and frequency domains, we
carefully design a novel architecture SFUNet to effectively capture the
correlation for both domains. Specifically, in the standard denoising U-Net for
pixel data, we supplement the 2D convolutions and spatial-only attention layers
with our spatial frequency-aware convolution and attention modules to jointly
model the complementary information from spatial and frequency domains in
wavelet data. Our new architecture can be used as a drop-in replacement to the
pixel-based network and is compatible with the vanilla DDPM training process.
By explicitly modeling the wavelet signals, we find our model is able to
generate images with higher quality on CIFAR-10, FFHQ, LSUN-Bedroom, and
LSUN-Church datasets, than the pixel-based counterpart.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xin Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linjie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhengyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1"&gt;Kevin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zicheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lijuan Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HyperFed: Hyperbolic Prototypes Exploration with Consistent Aggregation for Non-IID Data in Federated Learning. (arXiv:2307.14384v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14384</id>
        <link href="http://arxiv.org/abs/2307.14384"/>
        <updated>2023-07-29T00:48:57.378Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) collaboratively models user data in a decentralized
way. However, in the real world, non-identical and independent data
distributions (non-IID) among clients hinder the performance of FL due to three
issues, i.e., (1) the class statistics shifting, (2) the insufficient
hierarchical information utilization, and (3) the inconsistency in aggregating
clients. To address the above issues, we propose HyperFed which contains three
main modules, i.e., hyperbolic prototype Tammes initialization (HPTI),
hyperbolic prototype learning (HPL), and consistent aggregation (CA). Firstly,
HPTI in the server constructs uniformly distributed and fixed class prototypes,
and shares them with clients to match class statistics, further guiding
consistent feature representation for local clients. Secondly, HPL in each
client captures the hierarchical information in local data with the supervision
of shared class prototypes in the hyperbolic model space. Additionally, CA in
the server mitigates the impact of the inconsistent deviations from clients to
server. Extensive studies of four datasets prove that HyperFed is effective in
enhancing the performance of FL under the non-IID set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1"&gt;Xinting Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chaochao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1"&gt;Pengyang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Huabin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1"&gt;Yanchao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1"&gt;Yue Qi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fact-Checking of AI-Generated Reports. (arXiv:2307.14634v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2307.14634</id>
        <link href="http://arxiv.org/abs/2307.14634"/>
        <updated>2023-07-29T00:48:57.378Z</updated>
        <summary type="html"><![CDATA[With advances in generative artificial intelligence (AI), it is now possible
to produce realistic-looking automated reports for preliminary reads of
radiology images. This can expedite clinical workflows, improve accuracy and
reduce overall costs. However, it is also well-known that such models often
hallucinate, leading to false findings in the generated reports. In this paper,
we propose a new method of fact-checking of AI-generated reports using their
associated images. Specifically, the developed examiner differentiates real and
fake sentences in reports by learning the association between an image and
sentences describing real or potentially fake findings. To train such an
examiner, we first created a new dataset of fake reports by perturbing the
findings in the original ground truth radiology reports associated with images.
Text encodings of real and fake sentences drawn from these reports are then
paired with image encodings to learn the mapping to real/fake labels. The
utility of such an examiner is demonstrated for verifying automatically
generated reports by detecting and removing fake sentences. Future generative
AI approaches can use the resulting tool to validate their reports leading to a
more responsible use of AI in expediting clinical workflows.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_R/0/1/0/all/0/1"&gt;Razi Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Ge Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalra_M/0/1/0/all/0/1"&gt;Mannudeep Kalra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1"&gt;Pingkun Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NSA: Naturalistic Support Artifact to Boost Network Confidence. (arXiv:2307.14917v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.14917</id>
        <link href="http://arxiv.org/abs/2307.14917"/>
        <updated>2023-07-29T00:48:57.378Z</updated>
        <summary type="html"><![CDATA[Visual AI systems are vulnerable to natural and synthetic physical corruption
in the real-world. Such corruption often arises unexpectedly and alters the
model's performance. In recent years, the primary focus has been on adversarial
attacks. However, natural corruptions (e.g., snow, fog, dust) are an
omnipresent threat to visual AI systems and should be considered equally
important. Many existing works propose interesting solutions to train robust
models against natural corruption. These works either leverage image
augmentations, which come with the additional cost of model training, or place
suspicious patches in the scene to design unadversarial examples. In this work,
we propose the idea of naturalistic support artifacts (NSA) for robust
prediction. The NSAs are shown to be beneficial in scenarios where model
parameters are inaccessible and adding artifacts in the scene is feasible. The
NSAs are natural looking objects generated through artifact training using
DC-GAN to have high visual fidelity in the scene. We test against natural
corruptions on the Imagenette dataset and observe the improvement in prediction
confidence score by four times. We also demonstrate NSA's capability to
increase adversarial accuracy by 8\% on average. Lastly, we qualitatively
analyze NSAs using saliency maps to understand how they help improve prediction
confidence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Abhijith Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Munz_P/0/1/0/all/0/1"&gt;Phil Munz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayan_A/0/1/0/all/0/1"&gt;Apurva Narayan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DBGSA: A Novel Data Adaptive Bregman Clustering Algorithm. (arXiv:2307.14375v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14375</id>
        <link href="http://arxiv.org/abs/2307.14375"/>
        <updated>2023-07-29T00:48:57.369Z</updated>
        <summary type="html"><![CDATA[With the development of Big data technology, data analysis has become
increasingly important. Traditional clustering algorithms such as K-means are
highly sensitive to the initial centroid selection and perform poorly on
non-convex datasets. In this paper, we address these problems by proposing a
data-driven Bregman divergence parameter optimization clustering algorithm
(DBGSA), which combines the Universal Gravitational Algorithm to bring similar
points closer in the dataset. We construct a gravitational coefficient equation
with a special property that gradually reduces the influence factor as the
iteration progresses. Furthermore, we introduce the Bregman divergence
generalized power mean information loss minimization to identify cluster
centers and build a hyperparameter identification optimization model, which
effectively solves the problems of manual adjustment and uncertainty in the
improved dataset. Extensive experiments are conducted on four simulated
datasets and six real datasets. The results demonstrate that DBGSA
significantly improves the accuracy of various clustering algorithms by an
average of 63.8\% compared to other similar approaches like enhanced clustering
algorithms and improved datasets. Additionally, a three-dimensional grid search
was established to compare the effects of different parameter values within
threshold conditions, and it was discovered the parameter set provided by our
model is optimal. This finding provides strong evidence of the high accuracy
and robustness of the algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1"&gt;Ying Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hou-biao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu-pu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction of depression status in college students using a Naive Bayes classifier based machine learning model. (arXiv:2307.14371v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14371</id>
        <link href="http://arxiv.org/abs/2307.14371"/>
        <updated>2023-07-29T00:48:57.367Z</updated>
        <summary type="html"><![CDATA[This study presents a machine learning model based on the Naive Bayes
classifier for predicting the level of depression in university students, the
objective was to improve prediction accuracy using a machine learning model
involving 70% training data and 30% validation data based on the Naive Bayes
classifier, the collected data includes factors associated with depression from
519 university students, the results showed an accuracy of 78.03%, high
sensitivity in detecting positive cases of depression, especially at moderate
and severe levels, and significant specificity in correctly classifying
negative cases, these findings highlight the effectiveness of the model in
early detection and treatment of depression, benefiting vulnerable sectors and
contributing to the improvement of mental health in the student population.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cruz_F/0/1/0/all/0/1"&gt;Fred Torres Cruz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flores_E/0/1/0/all/0/1"&gt;Evelyn Eliana Coaquira Flores&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quispe_S/0/1/0/all/0/1"&gt;Sebastian Jarom Condori Quispe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Limits to Reservoir Learning. (arXiv:2307.14474v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14474</id>
        <link href="http://arxiv.org/abs/2307.14474"/>
        <updated>2023-07-29T00:48:57.366Z</updated>
        <summary type="html"><![CDATA[In this work, we bound a machine's ability to learn based on computational
limitations implied by physicality. We start by considering the information
processing capacity (IPC), a normalized measure of the expected squared error
of a collection of signals to a complete basis of functions. We use the IPC to
measure the degradation under noise of the performance of reservoir computers,
a particular kind of recurrent network, when constrained by physical
considerations. First, we show that the IPC is at most a polynomial in the
system size $n$, even when considering the collection of $2^n$ possible
pointwise products of the $n$ output signals. Next, we argue that this
degradation implies that the family of functions represented by the reservoir
requires an exponential number of samples to learn in the presence of the
reservoir's noise. Finally, we conclude with a discussion of the performance of
the same collection of $2^n$ functions without noise when being used for binary
classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Polloreno_A/0/1/0/all/0/1"&gt;Anthony M. Polloreno&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learned Gridification for Efficient Point Cloud Processing. (arXiv:2307.14354v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.14354</id>
        <link href="http://arxiv.org/abs/2307.14354"/>
        <updated>2023-07-29T00:48:57.364Z</updated>
        <summary type="html"><![CDATA[Neural operations that rely on neighborhood information are much more
expensive when deployed on point clouds than on grid data due to the irregular
distances between points in a point cloud. In a grid, on the other hand, we can
compute the kernel only once and reuse it for all query positions. As a result,
operations that rely on neighborhood information scale much worse for point
clouds than for grid data, specially for large inputs and large neighborhoods.

In this work, we address the scalability issue of point cloud methods by
tackling its root cause: the irregularity of the data. We propose learnable
gridification as the first step in a point cloud processing pipeline to
transform the point cloud into a compact, regular grid. Thanks to
gridification, subsequent layers can use operations defined on regular grids,
e.g., Conv3D, which scale much better than native point cloud methods. We then
extend gridification to point cloud to point cloud tasks, e.g., segmentation,
by adding a learnable de-gridification step at the end of the point cloud
processing pipeline to map the compact, regular grid back to its original point
cloud form. Through theoretical and empirical analysis, we show that gridified
networks scale better in terms of memory and time than networks directly
applied on raw point cloud data, while being able to achieve competitive
results. Our code is publicly available at
https://github.com/computri/gridifier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Linden_P/0/1/0/all/0/1"&gt;Putri A. van der Linden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romero_D/0/1/0/all/0/1"&gt;David W. Romero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1"&gt;Erik J. Bekkers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VISPUR: Visual Aids for Identifying and Interpreting Spurious Associations in Data-Driven Decisions. (arXiv:2307.14448v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2307.14448</id>
        <link href="http://arxiv.org/abs/2307.14448"/>
        <updated>2023-07-29T00:48:57.364Z</updated>
        <summary type="html"><![CDATA[Big data and machine learning tools have jointly empowered humans in making
data-driven decisions. However, many of them capture empirical associations
that might be spurious due to confounding factors and subgroup heterogeneity.
The famous Simpson's paradox is such a phenomenon where aggregated and
subgroup-level associations contradict with each other, causing cognitive
confusions and difficulty in making adequate interpretations and decisions.
Existing tools provide little insights for humans to locate, reason about, and
prevent pitfalls of spurious association in practice. We propose VISPUR, a
visual analytic system that provides a causal analysis framework and a
human-centric workflow for tackling spurious associations. These include a
CONFOUNDER DASHBOARD, which can automatically identify possible confounding
factors, and a SUBGROUP VIEWER, which allows for the visualization and
comparison of diverse subgroup patterns that likely or potentially result in a
misinterpretation of causality. Additionally, we propose a REASONING
STORYBOARD, which uses a flow-based approach to illustrate paradoxical
phenomena, as well as an interactive DECISION DIAGNOSIS panel that helps ensure
accountable decision-making. Through an expert interview and a controlled user
experiment, our qualitative and quantitative results demonstrate that the
proposed "de-paradox" workflow and the designed visual analytic system are
effective in helping human users to identify and understand spurious
associations, as well as to make accountable causal decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Teng_X/0/1/0/all/0/1"&gt;Xian Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_Y/0/1/0/all/0/1"&gt;Yongsu Ahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yu-Ru Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Piecewise Linear Functions Representable with Infinite Width Shallow ReLU Neural Networks. (arXiv:2307.14373v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14373</id>
        <link href="http://arxiv.org/abs/2307.14373"/>
        <updated>2023-07-29T00:48:57.360Z</updated>
        <summary type="html"><![CDATA[This paper analyzes representations of continuous piecewise linear functions
with infinite width, finite cost shallow neural networks using the rectified
linear unit (ReLU) as an activation function. Through its integral
representation, a shallow neural network can be identified by the corresponding
signed, finite measure on an appropriate parameter space. We map these measures
on the parameter space to measures on the projective $n$-sphere cross
$\mathbb{R}$, allowing points in the parameter space to be bijectively mapped
to hyperplanes in the domain of the function. We prove a conjecture of Ongie et
al. that every continuous piecewise linear function expressible with this kind
of infinite width neural network is expressible as a finite width shallow ReLU
neural network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McCarty_S/0/1/0/all/0/1"&gt;Sarah McCarty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Bayesian approach to quantifying uncertainties and improving generalizability in traffic prediction models. (arXiv:2307.05946v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.05946</id>
        <link href="http://arxiv.org/abs/2307.05946"/>
        <updated>2023-07-29T00:48:57.359Z</updated>
        <summary type="html"><![CDATA[Deep-learning models for traffic data prediction can have superior
performance in modeling complex functions using a multi-layer architecture.
However, a major drawback of these approaches is that most of these approaches
do not offer forecasts with uncertainty estimates, which are essential for
traffic operations and control. Without uncertainty estimates, it is difficult
to place any level of trust to the model predictions, and operational
strategies relying on overconfident predictions can lead to worsening traffic
conditions. In this study, we propose a Bayesian recurrent neural network
framework for uncertainty quantification in traffic prediction with higher
generalizability by introducing spectral normalization to its hidden layers. In
our paper, we have shown that normalization alters the training process of deep
neural networks by controlling the model's complexity and reducing the risk of
overfitting to the training data. This, in turn, helps improve the
generalization performance of the model on out-of-distribution datasets.
Results demonstrate that spectral normalization improves uncertainty estimates
and significantly outperforms both the layer normalization and model without
normalization in single-step prediction horizons. This improved performance can
be attributed to the ability of spectral normalization to better localize the
feature space of the data under perturbations. Our findings are especially
relevant to traffic management applications, where predicting traffic
conditions across multiple locations is the goal, but the availability of
training data from multiple locations is limited. Spectral normalization,
therefore, provides a more generalizable approach that can effectively capture
the underlying patterns in traffic data without requiring location-specific
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sengupta_A/0/1/0/all/0/1"&gt;Agnimitra Sengupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mondal_S/0/1/0/all/0/1"&gt;Sudeepta Mondal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1"&gt;Adway Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guler_S/0/1/0/all/0/1"&gt;S. Ilgin Guler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnable wavelet neural networks for cosmological inference. (arXiv:2307.14362v1 [astro-ph.IM])]]></title>
        <id>http://arxiv.org/abs/2307.14362</id>
        <link href="http://arxiv.org/abs/2307.14362"/>
        <updated>2023-07-29T00:48:57.358Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have been shown to both extract more
information than the traditional two-point statistics from cosmological fields,
and marginalise over astrophysical effects extremely well. However, CNNs
require large amounts of training data, which is potentially problematic in the
domain of expensive cosmological simulations, and it is difficult to interpret
the network. In this work we apply the learnable scattering transform, a kind
of convolutional neural network that uses trainable wavelets as filters, to the
problem of cosmological inference and marginalisation over astrophysical
effects. We present two models based on the scattering transform, one
constructed for performance, and one constructed for interpretability, and
perform a comparison with a CNN. We find that scattering architectures are able
to outperform a CNN, significantly in the case of small training data samples.
Additionally we present a lightweight scattering network that is highly
interpretable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Pedersen_C/0/1/0/all/0/1"&gt;Christian Pedersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Eickenberg_M/0/1/0/all/0/1"&gt;Michael Eickenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Ho_S/0/1/0/all/0/1"&gt;Shirley Ho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forecasting, capturing and activation of carbon-dioxide (CO$_2$): Integration of Time Series Analysis, Machine Learning, and Material Design. (arXiv:2307.14374v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14374</id>
        <link href="http://arxiv.org/abs/2307.14374"/>
        <updated>2023-07-29T00:48:57.358Z</updated>
        <summary type="html"><![CDATA[This study provides a comprehensive time series analysis of daily
industry-specific, country-wise CO$_2$ emissions from January 2019 to February
2023. The research focuses on the Power, Industry, Ground Transport, Domestic
Aviation, and International Aviation sectors in European countries (EU27 & UK,
Italy, Germany, Spain) and India, utilizing near-real-time activity data from
the Carbon Monitor research initiative. To identify regular emission patterns,
the data from the year 2020 is excluded due to the disruptive effects caused by
the COVID-19 pandemic. The study then performs a principal component analysis
(PCA) to determine the key contributors to CO$_2$ emissions. The analysis
reveals that the Power, Industry, and Ground Transport sectors account for a
significant portion of the variance in the dataset. A 7-day moving averaged
dataset is employed for further analysis to facilitate robust predictions. This
dataset captures both short-term and long-term trends and enhances the quality
of the data for prediction purposes. The study utilizes Long Short-Term Memory
(LSTM) models on the 7-day moving averaged dataset to effectively predict
emissions and provide insights for policy decisions, mitigation strategies, and
climate change efforts. During the training phase, the stability and
convergence of the LSTM models are ensured, which guarantees their reliability
in the testing phase. The evaluation of the loss function indicates this
reliability. The model achieves high efficiency, as demonstrated by $R^2$
values ranging from 0.8242 to 0.995 for various countries and sectors.
Furthermore, there is a proposal for utilizing scandium and
boron/aluminium-based thin films as exceptionally efficient materials for
capturing CO$_2$ (with a binding energy range from -3.0 to -3.5 eV). These
materials are shown to surpass the affinity of graphene and boron nitride
sheets in this regard.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sadhukhan_S/0/1/0/all/0/1"&gt;Suchetana Sadhukhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_V/0/1/0/all/0/1"&gt;Vivek Kumar Yadav&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HUGE: Huge Unsupervised Graph Embeddings with TPUs. (arXiv:2307.14490v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14490</id>
        <link href="http://arxiv.org/abs/2307.14490"/>
        <updated>2023-07-29T00:48:57.358Z</updated>
        <summary type="html"><![CDATA[Graphs are a representation of structured data that captures the
relationships between sets of objects. With the ubiquity of available network
data, there is increasing industrial and academic need to quickly analyze
graphs with billions of nodes and trillions of edges. A common first step for
network understanding is Graph Embedding, the process of creating a continuous
representation of nodes in a graph. A continuous representation is often more
amenable, especially at scale, for solving downstream machine learning tasks
such as classification, link prediction, and clustering. A high-performance
graph embedding architecture leveraging Tensor Processing Units (TPUs) with
configurable amounts of high-bandwidth memory is presented that simplifies the
graph embedding problem and can scale to graphs with billions of nodes and
trillions of edges. We verify the embedding space quality on real and synthetic
large-scale datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mayer_B/0/1/0/all/0/1"&gt;Brandon Mayer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsitsulin_A/0/1/0/all/0/1"&gt;Anton Tsitsulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fichtenberger_H/0/1/0/all/0/1"&gt;Hendrik Fichtenberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halcrow_J/0/1/0/all/0/1"&gt;Jonathan Halcrow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perozzi_B/0/1/0/all/0/1"&gt;Bryan Perozzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to simulate partially known spatio-temporal dynamics with trainable difference operators. (arXiv:2307.14395v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14395</id>
        <link href="http://arxiv.org/abs/2307.14395"/>
        <updated>2023-07-29T00:48:57.357Z</updated>
        <summary type="html"><![CDATA[Recently, using neural networks to simulate spatio-temporal dynamics has
received a lot of attention. However, most existing methods adopt pure
data-driven black-box models, which have limited accuracy and interpretability.
By combining trainable difference operators with black-box models, we propose a
new hybrid architecture explicitly embedded with partial prior knowledge of the
underlying PDEs named PDE-Net++. Furthermore, we introduce two distinct options
called the trainable flipping difference layer (TFDL) and the trainable dynamic
difference layer (TDDL) for the difference operators. Numerous numerical
experiments have demonstrated that PDE-Net++ has superior prediction accuracy
and better extrapolation performance than black-box models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhuoyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongsheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zidong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hongye Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1"&gt;Bin Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1"&gt;Bei Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Effect of Spoken Language on Speech Enhancement using Self-Supervised Speech Representation Loss Functions. (arXiv:2307.14502v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2307.14502</id>
        <link href="http://arxiv.org/abs/2307.14502"/>
        <updated>2023-07-29T00:48:57.357Z</updated>
        <summary type="html"><![CDATA[Recent work in the field of speech enhancement (SE) has involved the use of
self-supervised speech representations (SSSRs) as feature transformations in
loss functions. However, in prior work, very little attention has been paid to
the relationship between the language of the audio used to train the
self-supervised representation and that used to train the SE system.
Enhancement models trained using a loss function which incorporates a
self-supervised representation that shares exactly the language of the noisy
data used to train the SE system show better performance than those which do
not match exactly. This may lead to enhancement systems which are language
specific and as such do not generalise well to unseen languages, unlike models
trained using traditional spectrogram or time domain loss functions. In this
work, SE models are trained and tested on a number of different languages, with
self-supervised representations which themselves are trained using different
language combinations and with differing network structures as loss function
representations. These models are then tested across unseen languages and their
performances are analysed. It is found that the training language of the
self-supervised representation appears to have a minor effect on enhancement
performance, the amount of training data of a particular language, however,
greatly affects performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Close_G/0/1/0/all/0/1"&gt;George Close&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hain_T/0/1/0/all/0/1"&gt;Thomas Hain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Goetze_S/0/1/0/all/0/1"&gt;Stefan Goetze&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis. (arXiv:2307.14364v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2307.14364</id>
        <link href="http://arxiv.org/abs/2307.14364"/>
        <updated>2023-07-29T00:48:57.354Z</updated>
        <summary type="html"><![CDATA[Distributionally Robust Optimization (DRO), which aims to find an optimal
decision that minimizes the worst case cost over the ambiguity set of
probability distribution, has been widely applied in diverse applications,
e.g., network behavior analysis, risk management, etc. However, existing DRO
techniques face three key challenges: 1) how to deal with the asynchronous
updating in a distributed environment; 2) how to leverage the prior
distribution effectively; 3) how to properly adjust the degree of robustness
according to different scenarios. To this end, we propose an asynchronous
distributed algorithm, named Asynchronous Single-looP alternatIve gRadient
projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to
tackle the federated distributionally robust optimization (FDRO) problem.
Furthermore, a new uncertainty set, i.e., constrained D-norm uncertainty set,
is developed to effectively leverage the prior distribution and flexibly
control the degree of robustness. Finally, our theoretical analysis elucidates
that the proposed algorithm is guaranteed to converge and the iteration
complexity is also analyzed. Extensive empirical studies on real-world datasets
demonstrate that the proposed method can not only achieve fast convergence, and
remain robust against data heterogeneity as well as malicious attacks, but also
tradeoff robustness with performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Jiao_Y/0/1/0/all/0/1"&gt;Yang Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Song_D/0/1/0/all/0/1"&gt;Dongjin Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Kinds of Contracts Do ML APIs Need?. (arXiv:2307.14465v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2307.14465</id>
        <link href="http://arxiv.org/abs/2307.14465"/>
        <updated>2023-07-29T00:48:57.354Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that Machine Learning (ML) programs are error-prone and
called for contracts for ML code. Contracts, as in the design by contract
methodology, help document APIs and aid API users in writing correct code. The
question is: what kinds of contracts would provide the most help to API users?
We are especially interested in what kinds of contracts help API users catch
errors at earlier stages in the ML pipeline. We describe an empirical study of
posts on Stack Overflow of the four most often-discussed ML libraries:
TensorFlow, Scikit-learn, Keras, and PyTorch. For these libraries, our study
extracted 413 informal (English) API specifications. We used these
specifications to understand the following questions. What are the root causes
and effects behind ML contract violations? Are there common patterns of ML
contract violations? When does understanding ML contracts require an advanced
level of ML software expertise? Could checking contracts at the API level help
detect the violations in early ML pipeline stages? Our key findings are that
the most commonly needed contracts for ML APIs are either checking constraints
on single arguments of an API or on the order of API calls. The software
engineering community could employ existing contract mining approaches to mine
these contracts to promote an increased understanding of ML APIs. We also noted
a need to combine behavioral and temporal contract mining approaches. We report
on categories of required ML contracts, which may help designers of contract
languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khairunnesa_S/0/1/0/all/0/1"&gt;Samantha Syeda Khairunnesa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1"&gt;Shibbir Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imtiaz_S/0/1/0/all/0/1"&gt;Sayem Mohammad Imtiaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajan_H/0/1/0/all/0/1"&gt;Hridesh Rajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leavens_G/0/1/0/all/0/1"&gt;Gary T. Leavens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised reconstruction of accelerated cardiac cine MRI using Neural Fields. (arXiv:2307.14363v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2307.14363</id>
        <link href="http://arxiv.org/abs/2307.14363"/>
        <updated>2023-07-29T00:48:57.353Z</updated>
        <summary type="html"><![CDATA[Cardiac cine MRI is the gold standard for cardiac functional assessment, but
the inherently slow acquisition process creates the necessity of reconstruction
approaches for accelerated undersampled acquisitions. Several regularization
approaches that exploit spatial-temporal redundancy have been proposed to
reconstruct undersampled cardiac cine MRI. More recently, methods based on
supervised deep learning have been also proposed to further accelerate
acquisition and reconstruction. However, these techniques rely on usually large
dataset for training, which are not always available. In this work, we propose
an unsupervised approach based on implicit neural field representations for
cardiac cine MRI (so called NF-cMRI). The proposed method was evaluated in
in-vivo undersampled golden-angle radial multi-coil acquisitions for
undersampling factors of 26x and 52x, achieving good image quality, and
comparable spatial and improved temporal depiction than a state-of-the-art
reconstruction technique.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Catalan_T/0/1/0/all/0/1"&gt;Tabita Catal&amp;#xe1;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Courdurier_M/0/1/0/all/0/1"&gt;Mat&amp;#xed;as Courdurier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Osses_A/0/1/0/all/0/1"&gt;Axel Osses&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Botnar_R/0/1/0/all/0/1"&gt;Ren&amp;#xe9; Botnar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Costabal_F/0/1/0/all/0/1"&gt;Francisco Sahli Costabal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prieto_C/0/1/0/all/0/1"&gt;Claudia Prieto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Estimation in Mixed-Membership Stochastic Block Models. (arXiv:2307.14530v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.14530</id>
        <link href="http://arxiv.org/abs/2307.14530"/>
        <updated>2023-07-29T00:48:57.353Z</updated>
        <summary type="html"><![CDATA[Community detection is one of the most critical problems in modern network
science. Its applications can be found in various fields, from protein modeling
to social network analysis. Recently, many papers appeared studying the problem
of overlapping community detection, where each node of a network may belong to
several communities. In this work, we consider Mixed-Membership Stochastic
Block Model (MMSB) first proposed by Airoldi et al. (2008). MMSB provides quite
a general setting for modeling overlapping community structure in graphs. The
central question of this paper is to reconstruct relations between communities
given an observed network. We compare different approaches and establish the
minimax lower bound on the estimation error. Then, we propose a new estimator
that matches this lower bound. Theoretical results are proved under fairly
general conditions on the considered model. Finally, we illustrate the theory
in a series of experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Noskov_F/0/1/0/all/0/1"&gt;Fedor Noskov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Panov_M/0/1/0/all/0/1"&gt;Maxim Panov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot. (arXiv:2307.14397v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.14397</id>
        <link href="http://arxiv.org/abs/2307.14397"/>
        <updated>2023-07-29T00:48:57.352Z</updated>
        <summary type="html"><![CDATA[In machine learning, generative modeling aims to learn to generate new data
statistically similar to the training data distribution. In this paper, we
survey learning generative models under limited data, few shots and zero shot,
referred to as Generative Modeling under Data Constraint (GM-DC). This is an
important topic when data acquisition is challenging, e.g. healthcare
applications. We discuss background, challenges, and propose two taxonomies:
one on GM-DC tasks and another on GM-DC approaches. Importantly, we study
interactions between different GM-DC tasks and approaches. Furthermore, we
highlight research gaps, research trends, and potential avenues for future
exploration. Project website: https://gmdc-survey.github.io.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdollahzadeh_M/0/1/0/all/0/1"&gt;Milad Abdollahzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malekzadeh_T/0/1/0/all/0/1"&gt;Touba Malekzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teo_C/0/1/0/all/0/1"&gt;Christopher T. H. Teo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandrasegaran_K/0/1/0/all/0/1"&gt;Keshigeyan Chandrasegaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guimeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1"&gt;Ngai-Man Cheung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automating Model Comparison in Factor Graphs. (arXiv:2306.05965v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.05965</id>
        <link href="http://arxiv.org/abs/2306.05965"/>
        <updated>2023-07-29T00:48:57.333Z</updated>
        <summary type="html"><![CDATA[Bayesian state and parameter estimation have been automated effectively in a
variety of probabilistic programming languages. The process of model comparison
on the other hand, which still requires error-prone and time-consuming manual
derivations, is often overlooked despite its importance. This paper efficiently
automates Bayesian model averaging, selection, and combination by message
passing on a Forney-style factor graph with a custom mixture node. Parameter
and state inference, and model comparison can then be executed simultaneously
using message passing with scale factors. This approach shortens the model
design cycle and allows for the straightforward extension to hierarchical and
temporal model priors to accommodate for modeling complicated time-varying
processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Erp_B/0/1/0/all/0/1"&gt;Bart van Erp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nuijten_W/0/1/0/all/0/1"&gt;Wouter W. L. Nuijten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laar_T/0/1/0/all/0/1"&gt;Thijs van de Laar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vries_B/0/1/0/all/0/1"&gt;Bert de Vries&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Networks for Scalar Input and Functional Output. (arXiv:2208.05776v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.05776</id>
        <link href="http://arxiv.org/abs/2208.05776"/>
        <updated>2023-07-29T00:48:57.327Z</updated>
        <summary type="html"><![CDATA[The regression of a functional response on a set of scalar predictors can be
a challenging task, especially if there is a large number of predictors, or the
relationship between those predictors and the response is nonlinear. In this
work, we propose a solution to this problem: a feed-forward neural network (NN)
designed to predict a functional response using scalar inputs. First, we
transform the functional response to a finite-dimensional representation and
construct an NN that outputs this representation. Then, we propose to modify
the output of an NN via the objective function and introduce different
objective functions for network training. The proposed models are suited for
both regularly and irregularly spaced data, and a roughness penalty can be
further applied to control the smoothness of the predicted curve. The
difficulty in implementing both those features lies in the definition of
objective functions that can be back-propagated. In our experiments, we
demonstrate that our model outperforms the conventional function-on-scalar
regression model in multiple scenarios while computationally scaling better
with the dimension of the predictors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wu_S/0/1/0/all/0/1"&gt;Sidi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Beaulac_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric Beaulac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jiguo Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Contextual Data to Newsvendor Decisions: On the Actual Performance of Data-Driven Algorithms. (arXiv:2302.08424v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.08424</id>
        <link href="http://arxiv.org/abs/2302.08424"/>
        <updated>2023-07-29T00:48:57.323Z</updated>
        <summary type="html"><![CDATA[In this work, we explore a framework for contextual decision-making to study
how the relevance and quantity of past data affects the performance of a
data-driven policy. We analyze a contextual Newsvendor problem in which a
decision-maker needs to trade-off between an underage and an overage cost in
the face of uncertain demand. We consider a setting in which past demands
observed under ``close by'' contexts come from close by distributions and
analyze the performance of data-driven algorithms through a notion of
context-dependent worst-case expected regret. We analyze the broad class of
Weighted Empirical Risk Minimization (WERM) policies which weigh past data
according to their similarity in the contextual space. This class includes
classical policies such as ERM, k-Nearest Neighbors and kernel-based policies.
Our main methodological contribution is to characterize exactly the worst-case
regret of any WERM policy on any given configuration of contexts. To the best
of our knowledge, this provides the first understanding of tight performance
guarantees in any contextual decision-making problem, with past literature
focusing on upper bounds via concentration inequalities. We instead take an
optimization approach, and isolate a structure in the Newsvendor loss function
that allows to reduce the infinite-dimensional optimization problem over
worst-case distributions to a simple line search.

This in turn allows us to unveil fundamental insights that were obfuscated by
previous general-purpose bounds. We characterize actual guaranteed performance
as a function of the contexts, as well as granular insights on the learning
curve of algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Besbes_O/0/1/0/all/0/1"&gt;Omar Besbes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Will Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouchtaki_O/0/1/0/all/0/1"&gt;Omar Mouchtaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the non-efficient PAC learnability of conjunctive queries. (arXiv:2208.10255v2 [cs.DB] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.10255</id>
        <link href="http://arxiv.org/abs/2208.10255"/>
        <updated>2023-07-29T00:48:57.321Z</updated>
        <summary type="html"><![CDATA[This note serves three purposes: (i) we provide a self-contained exposition
of the fact that conjunctive queries are not efficiently learnable in the
Probably-Approximately-Correct (PAC) model, paying clear attention to the
complicating fact that this concept class lacks the polynomial-size fitting
property, a property that is tacitly assumed in much of the computational
learning theory literature; (ii) we establish a strong negative PAC
learnability result that applies to many restricted classes of conjunctive
queries (CQs), including acyclic CQs for a wide range of notions of
"acyclicity"; (iii) we show that CQs (and UCQs) are efficiently PAC learnable
with membership queries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cate_B/0/1/0/all/0/1"&gt;Balder ten Cate&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Funk_M/0/1/0/all/0/1"&gt;Maurice Funk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1"&gt;Jean Christoph Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lutz_C/0/1/0/all/0/1"&gt;Carsten Lutz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks. (arXiv:2307.14751v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14751</id>
        <link href="http://arxiv.org/abs/2307.14751"/>
        <updated>2023-07-29T00:48:57.320Z</updated>
        <summary type="html"><![CDATA[We propose FLARE, the first fingerprinting mechanism to verify whether a
suspected Deep Reinforcement Learning (DRL) policy is an illegitimate copy of
another (victim) policy. We first show that it is possible to find
non-transferable, universal adversarial masks, i.e., perturbations, to generate
adversarial examples that can successfully transfer from a victim policy to its
modified versions but not to independently trained policies. FLARE employs
these masks as fingerprints to verify the true ownership of stolen DRL policies
by measuring an action agreement value over states perturbed via such masks.
Our empirical evaluations show that FLARE is effective (100% action agreement
on stolen copies) and does not falsely accuse independent policies (no false
positives). FLARE is also robust to model modification attacks and cannot be
easily evaded by more informed adversaries without negatively impacting agent
performance. We also show that not all universal adversarial masks are suitable
candidates for fingerprints due to the inherent characteristics of DRL
policies. The spatio-temporal dynamics of DRL problems and sequential
decision-making process make characterizing the decision boundary of DRL
policies more difficult, as well as searching for universal masks that capture
the geometry of it.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tekgul_B/0/1/0/all/0/1"&gt;Buse G. A. Tekgul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asokan_N/0/1/0/all/0/1"&gt;N. Asokan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EdgeConvEns: Convolutional Ensemble Learning for Edge Intelligence. (arXiv:2307.14381v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14381</id>
        <link href="http://arxiv.org/abs/2307.14381"/>
        <updated>2023-07-29T00:48:57.319Z</updated>
        <summary type="html"><![CDATA[Deep edge intelligence aims to deploy deep learning models that demand
computationally expensive training in the edge network with limited
computational power. Moreover, many deep edge intelligence applications require
handling distributed data that cannot be transferred to a central server due to
privacy concerns. Decentralized learning methods, such as federated learning,
offer solutions where models are learned collectively by exchanging learned
weights. However, they often require complex models that edge devices may not
handle and multiple rounds of network communication to achieve state-of-the-art
performances. This study proposes a convolutional ensemble learning approach,
coined EdgeConvEns, that facilitates training heterogeneous weak models on edge
and learning to ensemble them where data on edge are heterogeneously
distributed. Edge models are implemented and trained independently on
Field-Programmable Gate Array (FPGA) devices with various computational
capacities. Learned data representations are transferred to a central server
where the ensemble model is trained with the learned features received from the
edge devices to boost the overall prediction performance. Extensive experiments
demonstrate that the EdgeConvEns can outperform the state-of-the-art
performance with fewer communications and less data in various training
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sikdokur_I/0/1/0/all/0/1"&gt;Ilkay Sikdokur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baytas_I/0/1/0/all/0/1"&gt;&amp;#x130;nci M. Bayta&amp;#x15f;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yurdakul_A/0/1/0/all/0/1"&gt;Arda Yurdakul&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Disparity Compensation for Efficient Fair Ranking. (arXiv:2307.14366v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14366</id>
        <link href="http://arxiv.org/abs/2307.14366"/>
        <updated>2023-07-29T00:48:57.317Z</updated>
        <summary type="html"><![CDATA[Ranking functions that are used in decision systems often produce disparate
results for different populations because of bias in the underlying data.
Addressing, and compensating for, these disparate outcomes is a critical
problem for fair decision-making. Recent compensatory measures have mostly
focused on opaque transformations of the ranking functions to satisfy fairness
guarantees or on the use of quotas or set-asides to guarantee a minimum number
of positive outcomes to members of underrepresented groups. In this paper we
propose easily explainable data-driven compensatory measures for ranking
functions. Our measures rely on the generation of bonus points given to members
of underrepresented groups to address disparity in the ranking function. The
bonus points can be set in advance, and can be combined, allowing for
considering the intersections of representations and giving better transparency
to stakeholders. We propose efficient sampling-based algorithms to calculate
the number of bonus points to minimize disparity. We validate our algorithms
using real-world school admissions and recidivism datasets, and compare our
results with that of existing fair ranking algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gale_A/0/1/0/all/0/1"&gt;Abraham Gale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marian_A/0/1/0/all/0/1"&gt;Am&amp;#xc9;lie Marian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks. (arXiv:2209.06589v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2209.06589</id>
        <link href="http://arxiv.org/abs/2209.06589"/>
        <updated>2023-07-29T00:48:57.286Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) have become compelling models designed to
perform learning and inference on graph-structured data. However, little work
has been done to understand the fundamental limitations of GNNs for scaling to
larger graphs and generalizing to out-of-distribution (OOD) inputs. In this
paper, we use a random graph generator to systematically investigate how the
graph size and structural properties affect the predictive performance of GNNs.
We present specific evidence that the average node degree is a key feature in
determining whether GNNs can generalize to unseen graphs, and that the use of
multiple node update functions can improve the generalization performance of
GNNs when dealing with graphs of multimodal degree distributions. Accordingly,
we propose a multi-module GNN framework that allows the network to adapt
flexibly to new graphs by generalizing a single canonical nonlinear
transformation over aggregated inputs. Our results show that the multi-module
GNNs improve the OOD generalization on a variety of inference tasks in the
direction of diverse structural features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hyungeun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1"&gt;Kijung Yoon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal and Transferable Adversarial Attacks on Aligned Language Models. (arXiv:2307.15043v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.15043</id>
        <link href="http://arxiv.org/abs/2307.15043"/>
        <updated>2023-07-29T00:48:57.284Z</updated>
        <summary type="html"><![CDATA[Because "out-of-the-box" large language models are capable of generating a
great deal of objectionable content, recent work has focused on aligning these
models in an attempt to prevent undesirable generation. While there has been
some success at circumventing these measures -- so-called "jailbreaks" against
LLMs -- these attacks have required significant human ingenuity and are brittle
in practice. In this paper, we propose a simple and effective attack method
that causes aligned language models to generate objectionable behaviors.
Specifically, our approach finds a suffix that, when attached to a wide range
of queries for an LLM to produce objectionable content, aims to maximize the
probability that the model produces an affirmative response (rather than
refusing to answer). However, instead of relying on manual engineering, our
approach automatically produces these adversarial suffixes by a combination of
greedy and gradient-based search techniques, and also improves over past
automatic prompt generation methods.

Surprisingly, we find that the adversarial prompts generated by our approach
are quite transferable, including to black-box, publicly released LLMs.
Specifically, we train an adversarial attack suffix on multiple prompts (i.e.,
queries asking for many different types of objectionable content), as well as
multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting
attack suffix is able to induce objectionable content in the public interfaces
to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat,
Pythia, Falcon, and others. In total, this work significantly advances the
state-of-the-art in adversarial attacks against aligned language models,
raising important questions about how such systems can be prevented from
producing objectionable information. Code is available at
github.com/llm-attacks/llm-attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1"&gt;Andy Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zifan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1"&gt;J. Zico Kolter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1"&gt;Matt Fredrikson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Image Completion and Enhancement using GANs. (arXiv:2307.14748v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.14748</id>
        <link href="http://arxiv.org/abs/2307.14748"/>
        <updated>2023-07-29T00:48:57.265Z</updated>
        <summary type="html"><![CDATA[Semantic inpainting or image completion alludes to the task of inferring
arbitrary large missing regions in images based on image semantics. Since the
prediction of image pixels requires an indication of high-level context, this
makes it significantly tougher than image completion, which is often more
concerned with correcting data corruption and removing entire objects from the
input image. On the other hand, image enhancement attempts to eliminate
unwanted noise and blur from the image, along with sustaining most of the image
details. Efficient image completion and enhancement model should be able to
recover the corrupted and masked regions in images and then refine the image
further to increase the quality of the output image. Generative Adversarial
Networks (GAN), have turned out to be helpful in picture completion tasks. In
this chapter, we will discuss the underlying GAN architecture and how they can
be used used for image completion tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saxena_P/0/1/0/all/0/1"&gt;Priyansh Saxena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1"&gt;Raahat Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1"&gt;Akshat Maheshwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maheshwari_S/0/1/0/all/0/1"&gt;Saumil Maheshwari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Sleeping Bandit Problems with Multiple Plays: Algorithm and Ranking Application. (arXiv:2307.14549v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14549</id>
        <link href="http://arxiv.org/abs/2307.14549"/>
        <updated>2023-07-29T00:48:57.243Z</updated>
        <summary type="html"><![CDATA[This paper presents an efficient algorithm to solve the sleeping bandit with
multiple plays problem in the context of an online recommendation system. The
problem involves bounded, adversarial loss and unknown i.i.d. distributions for
arm availability. The proposed algorithm extends the sleeping bandit algorithm
for single arm selection and is guaranteed to achieve theoretical performance
with regret upper bounded by $\bigO(kN^2\sqrt{T\log T})$, where $k$ is the
number of arms selected per time step, $N$ is the total number of arms, and $T$
is the time horizon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Jianjun Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woon_W/0/1/0/all/0/1"&gt;Wei Lee Woon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coba_L/0/1/0/all/0/1"&gt;Ludovik Coba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Likely, Light, and Accurate Context-Free Clusters-based Trajectory Prediction. (arXiv:2307.14788v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14788</id>
        <link href="http://arxiv.org/abs/2307.14788"/>
        <updated>2023-07-29T00:48:57.242Z</updated>
        <summary type="html"><![CDATA[Autonomous systems in the road transportation network require intelligent
mechanisms that cope with uncertainty to foresee the future. In this paper, we
propose a multi-stage probabilistic approach for trajectory forecasting:
trajectory transformation to displacement space, clustering of displacement
time series, trajectory proposals, and ranking proposals. We introduce a new
deep feature clustering method, underlying self-conditioned GAN, which copes
better with distribution shifts than traditional methods. Additionally, we
propose novel distance-based ranking proposals to assign probabilities to the
generated trajectories that are more efficient yet accurate than an auxiliary
neural network. The overall system surpasses context-free deep generative
models in human and road agents trajectory data while performing similarly to
point estimators when comparing the most probable trajectory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Almeida_T/0/1/0/all/0/1"&gt;Tiago Rodrigues de Almeida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mozos_O/0/1/0/all/0/1"&gt;Oscar Martinez Mozos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs. (arXiv:2307.14988v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14988</id>
        <link href="http://arxiv.org/abs/2307.14988"/>
        <updated>2023-07-29T00:48:57.239Z</updated>
        <summary type="html"><![CDATA[Deep learning often faces the challenge of efficiently processing dynamic
inputs, such as sensor data or user inputs. For example, an AI writing
assistant is required to update its suggestions in real time as a document is
edited. Re-running the model each time is expensive, even with compression
techniques like knowledge distillation, pruning, or quantization. Instead, we
take an incremental computing approach, looking to reuse calculations as the
inputs change. However, the dense connectivity of conventional architectures
poses a major obstacle to incremental computation, as even minor input changes
cascade through the network and restrict information reuse. To address this, we
use vector quantization to discretize intermediate values in the network, which
filters out noisy and unnecessary modifications to hidden neurons, facilitating
the reuse of their values. We apply this approach to the transformers
architecture, creating an efficient incremental inference algorithm with
complexity proportional to the fraction of the modified inputs. Our experiments
with adapting the OPT-125M pre-trained language model demonstrate comparable
accuracy on document classification while requiring 12.1X (median) fewer
operations for processing sequences of atomic edits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharir_O/0/1/0/all/0/1"&gt;Or Sharir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernelised Normalising Flows. (arXiv:2307.14839v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.14839</id>
        <link href="http://arxiv.org/abs/2307.14839"/>
        <updated>2023-07-29T00:48:57.222Z</updated>
        <summary type="html"><![CDATA[Normalising Flows are generative models characterised by their invertible
architecture. However, the requirement of invertibility imposes constraints on
their expressiveness, necessitating a large number of parameters and innovative
architectural designs to achieve satisfactory outcomes. Whilst flow-based
models predominantly rely on neural-network-based transformations for
expressive designs, alternative transformation methods have received limited
attention. In this work, we present Ferumal flow, a novel kernelised
normalising flow paradigm that integrates kernels into the framework. Our
results demonstrate that a kernelised flow can yield competitive or superior
results compared to neural network-based flows whilst maintaining parameter
efficiency. Kernelised flows excel especially in the low-data regime, enabling
flexible non-parametric density estimation in applications with sparse data
availability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+English_E/0/1/0/all/0/1"&gt;Eshant English&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kirchler_M/0/1/0/all/0/1"&gt;Matthias Kirchler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lippert_C/0/1/0/all/0/1"&gt;Christoph Lippert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Data Quality Problems with Desbordante: a Demo. (arXiv:2307.14935v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2307.14935</id>
        <link href="http://arxiv.org/abs/2307.14935"/>
        <updated>2023-07-29T00:48:57.221Z</updated>
        <summary type="html"><![CDATA[Data profiling is an essential process in modern data-driven industries. One
of its critical components is the discovery and validation of complex
statistics, including functional dependencies, data constraints, association
rules, and others.

However, most existing data profiling systems that focus on complex
statistics do not provide proper integration with the tools used by
contemporary data scientists. This creates a significant barrier to the
adoption of these tools in the industry. Moreover, existing systems were not
created with industrial-grade workloads in mind. Finally, they do not aim to
provide descriptive explanations, i.e. why a given pattern is not found. It is
a significant issue as it is essential to understand the underlying reasons for
a specific pattern's absence to make informed decisions based on the data.

Because of that, these patterns are effectively rest in thin air: their
application scope is rather limited, they are rarely used by the broader
public. At the same time, as we are going to demonstrate in this presentation,
complex statistics can be efficiently used to solve many classic data quality
problems.

Desbordante is an open-source data profiler that aims to close this gap. It
is built with emphasis on industrial application: it is efficient, scalable,
resilient to crashes, and provides explanations. Furthermore, it provides
seamless Python integration by offloading various costly operations to the C++
core, not only mining.

In this demonstration, we show several scenarios that allow end users to
solve different data quality problems. Namely, we showcase typo detection, data
deduplication, and data anomaly detection scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chernishev_G/0/1/0/all/0/1"&gt;George Chernishev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polyntsov_M/0/1/0/all/0/1"&gt;Michael Polyntsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chizhov_A/0/1/0/all/0/1"&gt;Anton Chizhov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stupakov_K/0/1/0/all/0/1"&gt;Kirill Stupakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shchuckin_I/0/1/0/all/0/1"&gt;Ilya Shchuckin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smirnov_A/0/1/0/all/0/1"&gt;Alexander Smirnov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strutovsky_M/0/1/0/all/0/1"&gt;Maxim Strutovsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shlyonskikh_A/0/1/0/all/0/1"&gt;Alexey Shlyonskikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Firsov_M/0/1/0/all/0/1"&gt;Mikhail Firsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manannikov_S/0/1/0/all/0/1"&gt;Stepan Manannikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bobrov_N/0/1/0/all/0/1"&gt;Nikita Bobrov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goncharov_D/0/1/0/all/0/1"&gt;Daniil Goncharov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barutkin_I/0/1/0/all/0/1"&gt;Ilia Barutkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shalnev_V/0/1/0/all/0/1"&gt;Vladislav Shalnev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muraviev_K/0/1/0/all/0/1"&gt;Kirill Muraviev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rakhmukova_A/0/1/0/all/0/1"&gt;Anna Rakhmukova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shcheka_D/0/1/0/all/0/1"&gt;Dmitriy Shcheka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chernikov_A/0/1/0/all/0/1"&gt;Anton Chernikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vyrodov_M/0/1/0/all/0/1"&gt;Mikhail Vyrodov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yaroslav_K/0/1/0/all/0/1"&gt;Kurbatov Yaroslav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fofanov_M/0/1/0/all/0/1"&gt;Maxim Fofanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sergei_B/0/1/0/all/0/1"&gt;Belokonnyi Sergei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavel_A/0/1/0/all/0/1"&gt;Anosov Pavel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saliou_A/0/1/0/all/0/1"&gt;Arthur Saliou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaisin_E/0/1/0/all/0/1"&gt;Eduard Gaisin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smirnov_K/0/1/0/all/0/1"&gt;Kirill Smirnov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Fault-tolerant and Byzantine-resilient Social Learning via Collaborative Hierarchical Non-Bayesian Learning. (arXiv:2307.14952v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14952</id>
        <link href="http://arxiv.org/abs/2307.14952"/>
        <updated>2023-07-29T00:48:57.220Z</updated>
        <summary type="html"><![CDATA[As the network scale increases, existing fully distributed solutions start to
lag behind the real-world challenges such as (1) slow information propagation,
(2) network communication failures, and (3) external adversarial attacks. In
this paper, we focus on hierarchical system architecture and address the
problem of non-Bayesian learning over networks that are vulnerable to
communication failures and adversarial attacks. On network communication, we
consider packet-dropping link failures.

We first propose a hierarchical robust push-sum algorithm that can achieve
average consensus despite frequent packet-dropping link failures. We provide a
sparse information fusion rule between the parameter server and arbitrarily
selected network representatives. Then, interleaving the consensus update step
with a dual averaging update with Kullback-Leibler (KL) divergence as the
proximal function, we obtain a packet-dropping fault-tolerant non-Bayesian
learning algorithm with provable convergence guarantees.

On external adversarial attacks, we consider Byzantine attacks in which the
compromised agents can send maliciously calibrated messages to others
(including both the agents and the parameter server). To avoid the curse of
dimensionality of Byzantine consensus, we solve the non-Bayesian learning
problem via running multiple dynamics, each of which only involves Byzantine
consensus with scalar inputs. To facilitate resilient information propagation
across sub-networks, we use a novel Byzantine-resilient gossiping-type rule at
the parameter server.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mclaughlin_C/0/1/0/all/0/1"&gt;Connor Mclaughlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1"&gt;Matthew Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Edogmus_D/0/1/0/all/0/1"&gt;Denis Edogmus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1"&gt;Lili Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Role of Image Acquisition and Patient Phenotype Variations in Automatic Segmentation Model Generalization. (arXiv:2307.14482v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2307.14482</id>
        <link href="http://arxiv.org/abs/2307.14482"/>
        <updated>2023-07-29T00:48:57.219Z</updated>
        <summary type="html"><![CDATA[Purpose: This study evaluated the out-of-domain performance and
generalization capabilities of automated medical image segmentation models,
with a particular focus on adaptation to new image acquisitions and disease
type.

Materials: Datasets from both non-contrast and contrast-enhanced abdominal CT
scans of healthy patients and those with polycystic kidney disease (PKD) were
used. A total of 400 images (100 non-contrast controls, 100 contrast controls,
100 non-contrast PKD, 100 contrast PKD) were utilized for training/validation
of models to segment kidneys, livers, and spleens, and the final models were
then tested on 100 non-contrast CT images of patients affected by PKD.
Performance was evaluated using Dice, Jaccard, TPR, and Precision.

Results: Models trained on a diverse range of data showed no worse
performance than models trained exclusively on in-domain data when tested on
in-domain data. For instance, the Dice similarity of the model trained on 25%
from each dataset was found to be non-inferior to the model trained purely on
in-domain data.

Conclusions: The results indicate that broader training examples
significantly enhances model generalization and out-of-domain performance,
thereby improving automated segmentation tools' applicability in clinical
settings. The study's findings provide a roadmap for future research to adopt a
data-centric approach in medical image AI model development.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kline_T/0/1/0/all/0/1"&gt;Timothy L. Kline&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ramanathan_S/0/1/0/all/0/1"&gt;Sumana Ramanathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gottlich_H/0/1/0/all/0/1"&gt;Harrison C. Gottlich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Korfiatis_P/0/1/0/all/0/1"&gt;Panagiotis Korfiatis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gregory_A/0/1/0/all/0/1"&gt;Adriana V. Gregory&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Safety Constraints in Autonomous Navigation with Deep Reinforcement Learning. (arXiv:2307.14568v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2307.14568</id>
        <link href="http://arxiv.org/abs/2307.14568"/>
        <updated>2023-07-29T00:48:57.129Z</updated>
        <summary type="html"><![CDATA[While reinforcement learning algorithms have had great success in the field
of autonomous navigation, they cannot be straightforwardly applied to the real
autonomous systems without considering the safety constraints. The later are
crucial to avoid unsafe behaviors of the autonomous vehicle on the road. To
highlight the importance of these constraints, in this study, we compare two
learnable navigation policies: safe and unsafe. The safe policy takes the
constraints into account, while the other does not. We show that the safe
policy is able to generate trajectories with more clearance (distance to the
obstacles) and makes less collisions while training without sacrificing the
overall performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Angulo_B/0/1/0/all/0/1"&gt;Brian Angulo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorbov_G/0/1/0/all/0/1"&gt;Gregory Gorbov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panov_A/0/1/0/all/0/1"&gt;Aleksandr Panov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yakovlev_K/0/1/0/all/0/1"&gt;Konstantin Yakovlev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe. (arXiv:2307.14361v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2307.14361</id>
        <link href="http://arxiv.org/abs/2307.14361"/>
        <updated>2023-07-29T00:48:57.128Z</updated>
        <summary type="html"><![CDATA[This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and
GloVe to classify gene mutations using Kaggle's Personalized Medicine:
Redefining Cancer Treatment dataset. The results were compared against
well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and
their LSTM ensembles. Our model outperformed all other models in terms of
accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it
also needed less training time, resulting in a perfect combination of
performance and efficiency. This study demonstrates the utility of ensemble
models for difficult tasks such as gene mutation classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Aburass_S/0/1/0/all/0/1"&gt;Sanad Aburass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Dorgham_O/0/1/0/all/0/1"&gt;Osama Dorgham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Shaqsi_J/0/1/0/all/0/1"&gt;Jamil Al Shaqsi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior. (arXiv:2307.14619v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14619</id>
        <link href="http://arxiv.org/abs/2307.14619"/>
        <updated>2023-07-29T00:48:57.127Z</updated>
        <summary type="html"><![CDATA[We propose a theoretical framework for studying the imitation of stochastic,
non-Markovian, potentially multi-modal (i.e. "complex" ) expert demonstrations
in nonlinear dynamical systems. Our framework invokes low-level controllers -
either learned or implicit in position-command control - to stabilize imitation
policies around expert demonstrations. We show that with (a) a suitable
low-level stability guarantee and (b) a stochastic continuity property of the
learned policy we call "total variation continuity" (TVC), an imitator that
accurately estimates actions on the demonstrator's state distribution closely
matches the demonstrator's distribution over entire trajectories. We then show
that TVC can be ensured with minimal degradation of accuracy by combining a
popular data-augmentation regimen with a novel algorithmic trick: adding
augmentation noise at execution time. We instantiate our guarantees for
policies parameterized by diffusion models and prove that if the learner
accurately estimates the score of the (noise-augmented) expert policy, then the
distribution of imitator trajectories is close to the demonstrator distribution
in a natural optimal transport distance. Our analysis constructs intricate
couplings between noise-augmented trajectories, a technique that may be of
independent interest. We conclude by empirically validating our algorithmic
recommendations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Block_A/0/1/0/all/0/1"&gt;Adam Block&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfrommer_D/0/1/0/all/0/1"&gt;Daniel Pfrommer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1"&gt;Max Simchowitz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Deep Learning-based Pansharpening with Jointly-Enhanced Spectral and Spatial Fidelity. (arXiv:2307.14403v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2307.14403</id>
        <link href="http://arxiv.org/abs/2307.14403"/>
        <updated>2023-07-29T00:48:57.115Z</updated>
        <summary type="html"><![CDATA[In latest years, deep learning has gained a leading role in the pansharpening
of multiresolution images. Given the lack of ground truth data, most deep
learning-based methods carry out supervised training in a reduced-resolution
domain. However, models trained on downsized images tend to perform poorly on
high-resolution target images. For this reason, several research groups are now
turning to unsupervised training in the full-resolution domain, through the
definition of appropriate loss functions and training paradigms. In this
context, we have recently proposed a full-resolution training framework which
can be applied to many existing architectures.

Here, we propose a new deep learning-based pansharpening model that fully
exploits the potential of this approach and provides cutting-edge performance.
Besides architectural improvements with respect to previous work, such as the
use of residual attention modules, the proposed model features a novel loss
function that jointly promotes the spectral and spatial quality of the
pansharpened data. In addition, thanks to a new fine-tuning strategy, it
improves inference-time adaptation to target images. Experiments on a large
variety of test images, performed in challenging scenarios, demonstrate that
the proposed method compares favorably with the state of the art both in terms
of numerical results and visual output. Code is available online at
https://github.com/matciotola/Lambda-PNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ciotola_M/0/1/0/all/0/1"&gt;Matteo Ciotola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Poggi_G/0/1/0/all/0/1"&gt;Giovanni Poggi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Scarpa_G/0/1/0/all/0/1"&gt;Giuseppe Scarpa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Empirical analysis of Different Dimensionality Reduction and classification Techniques for Epileptic Seizure detection. (arXiv:2302.12012v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.12012</id>
        <link href="http://arxiv.org/abs/2302.12012"/>
        <updated>2023-07-29T00:48:57.114Z</updated>
        <summary type="html"><![CDATA[An Electroencephalogram (EEG) is a non-invasive exam that records the
electrical activity of the brain. This exam is used to help diagnose conditions
such as different brain problems. EEG signals are taken for the purpose of
epilepsy detection and with Discrete Wavelet Transform (DWT) and machine
learning classifier, they perform epilepsy detection. In Epilepsy seizure
detection, mainly machine learning classifiers and statistical features are
used. The hidden information in the EEG signal is useful for detecting diseases
affecting the brain. Sometimes it is very difficult to identify the minimum
changes in the EEG in the time and frequency domains purpose. The DWT can give
a good decomposition of the signals in different frequency bands and feature
extraction. We use the tri-dimensionality reduction algorithm.; Principal
Component Analysis (PCA), Independent Component Analysis (ICA), and Linear
Discriminant Analysis (LDA). Finally, features are selected by using a fusion
rule and at the last step three different classifiers Support Vector Machine
(SVM), Naive Bayes (NB) and K-Nearest-Neighbor(KNN) have been used individually
for the classification. The proposed framework is tested on the Bonn dataset
and the simulation results provide the accuracy for the combination of LDA and
SVM 89.17%, LDA and KNN 80.42%, PCA and NB 89.92%, PCA and SVM 85.58%, PCA and
KNN 80.42%, ICA and NB 82.33%, ICA and SVM 90.42%, and ICA and KNN 90%, LDA and
NB 100%, accuracy. It shows the sensitivity, specificity, accuracy, Precision,
and Recall of 100%, 100%, 100%, 100%, and 100%. This combination of LDA with NB
method provides the accuracy of 100% outperforming all existing methods. The
results prove the effectiveness of this model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guharoy_R/0/1/0/all/0/1"&gt;Rabel Guharoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jana_N/0/1/0/all/0/1"&gt;Nanda Dulal Jana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1"&gt;Suparna Biswas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_L/0/1/0/all/0/1"&gt;Lalit Garg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Multi-Task Learning Meets Partial Supervision: A Computer Vision Review. (arXiv:2307.14382v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14382</id>
        <link href="http://arxiv.org/abs/2307.14382"/>
        <updated>2023-07-29T00:48:57.074Z</updated>
        <summary type="html"><![CDATA[Multi-Task Learning (MTL) aims to learn multiple tasks simultaneously while
exploiting their mutual relationships. By using shared resources to
simultaneously calculate multiple outputs, this learning paradigm has the
potential to have lower memory requirements and inference times compared to the
traditional approach of using separate methods for each task. Previous work in
MTL has mainly focused on fully-supervised methods, as task relationships can
not only be leveraged to lower the level of data-dependency of those methods
but they can also improve performance. However, MTL introduces a set of
challenges due to a complex optimisation scheme and a higher labeling
requirement. This review focuses on how MTL could be utilised under different
partial supervision settings to address these challenges. First, this review
analyses how MTL traditionally uses different parameter sharing techniques to
transfer knowledge in between tasks. Second, it presents the different
challenges arising from such a multi-objective optimisation scheme. Third, it
introduces how task groupings can be achieved by analysing task relationships.
Fourth, it focuses on how partially supervised methods applied to MTL can
tackle the aforementioned challenges. Lastly, this review presents the
available datasets, tools and benchmarking results of such methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fontana_M/0/1/0/all/0/1"&gt;Maxime Fontana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spratling_M/0/1/0/all/0/1"&gt;Michael Spratling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1"&gt;Miaojing Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning with a Reject Option: A survey. (arXiv:2107.11277v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11277</id>
        <link href="http://arxiv.org/abs/2107.11277"/>
        <updated>2023-07-29T00:48:57.024Z</updated>
        <summary type="html"><![CDATA[Machine learning models always make a prediction, even when it is likely to
be inaccurate. This behavior should be avoided in many decision support
applications, where mistakes can have severe consequences. Albeit already
studied in 1970, machine learning with rejection recently gained interest. This
machine learning subfield enables machine learning models to abstain from
making a prediction when likely to make a mistake.

This survey aims to provide an overview on machine learning with rejection.
We introduce the conditions leading to two types of rejection, ambiguity and
novelty rejection, which we carefully formalize. Moreover, we review and
categorize strategies to evaluate a model's predictive and rejective quality.
Additionally, we define the existing architectures for models with rejection
and describe the standard techniques for learning such models. Finally, we
provide examples of relevant application domains and show how machine learning
with rejection relates to other machine learning research areas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hendrickx_K/0/1/0/all/0/1"&gt;Kilian Hendrickx&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perini_L/0/1/0/all/0/1"&gt;Lorenzo Perini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plas_D/0/1/0/all/0/1"&gt;Dries Van der Plas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meert_W/0/1/0/all/0/1"&gt;Wannes Meert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1"&gt;Jesse Davis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fixed Integral Neural Networks. (arXiv:2307.14439v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14439</id>
        <link href="http://arxiv.org/abs/2307.14439"/>
        <updated>2023-07-29T00:48:57.023Z</updated>
        <summary type="html"><![CDATA[It is often useful to perform integration over learned functions represented
by neural networks. However, this integration is usually performed numerically,
as analytical integration over learned functions (especially neural networks)
is generally viewed as intractable. In this work, we present a method for
representing the analytical integral of a learned function $f$. This allows the
exact integral of a neural network to be computed, and enables constrained
neural networks to be parametrised by applying constraints directly to the
integral. Crucially, we also introduce a method to constrain $f$ to be
positive, a necessary condition for many applications (e.g. probability
distributions, distance metrics, etc). Finally, we introduce several
applications where our fixed-integral neural network (FINN) can be utilised.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kortvelesy_R/0/1/0/all/0/1"&gt;Ryan Kortvelesy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG. (arXiv:2307.14389v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2307.14389</id>
        <link href="http://arxiv.org/abs/2307.14389"/>
        <updated>2023-07-29T00:48:56.008Z</updated>
        <summary type="html"><![CDATA[Decoding EEG signals for imagined speech is a challenging task due to the
high-dimensional nature of the data and low signal-to-noise ratio. In recent
years, denoising diffusion probabilistic models (DDPMs) have emerged as
promising approaches for representation learning in various domains. Our study
proposes a novel method for decoding EEG signals for imagined speech using
DDPMs and a conditional autoencoder named Diff-E. Results indicate that Diff-E
significantly improves the accuracy of decoding EEG signals for imagined speech
compared to traditional machine learning techniques and baseline models. Our
findings suggest that DDPMs can be an effective tool for EEG signal decoding,
with potential implications for the development of brain-computer interfaces
that enable communication through imagined speech.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1"&gt;Soowon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Young-Eun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seo-Hyun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hypergraph Isomorphism Computation. (arXiv:2307.14394v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2307.14394</id>
        <link href="http://arxiv.org/abs/2307.14394"/>
        <updated>2023-07-29T00:48:55.960Z</updated>
        <summary type="html"><![CDATA[The isomorphism problem is a fundamental problem in network analysis, which
involves capturing both low-order and high-order structural information. In
terms of extracting low-order structural information, graph isomorphism
algorithms analyze the structural equivalence to reduce the solver space
dimension, which demonstrates its power in many applications, such as protein
design, chemical pathways, and community detection. For the more commonly
occurring high-order relationships in real-life scenarios, the problem of
hypergraph isomorphism, which effectively captures these high-order structural
relationships, cannot be straightforwardly addressed using graph isomorphism
methods. Besides, the existing hypergraph kernel methods may suffer from high
memory consumption or inaccurate sub-structure identification, thus yielding
sub-optimal performance. In this paper, to address the abovementioned problems,
we first propose the hypergraph Weisfiler-Lehman test algorithm for the
hypergraph isomorphism test problem by generalizing the Weisfiler-Lehman test
algorithm from graphs to hypergraphs. Secondly, based on the presented
algorithm, we propose a general hypergraph Weisfieler-Lehman kernel framework
and implement two instances, which are Hypergraph Weisfeiler-Lehamn Subtree
Kernel and Hypergraph Weisfeiler-Lehamn Hyperedge Kernel. In order to fulfill
our research objectives, a comprehensive set of experiments was meticulously
designed, including seven graph classification datasets and 12 hypergraph
classification datasets. Results on hypergraph classification datasets show
significant improvements compared to other typical kernel-based methods, which
demonstrates the effectiveness of the proposed methods. In our evaluation, we
found that our proposed methods outperform the second-best method in terms of
runtime, running over 80 times faster when handling complex hypergraph
structures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yifan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiashu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ying_S/0/1/0/all/0/1"&gt;Shihui Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yue Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-objective Deep Reinforcement Learning for Mobile Edge Computing. (arXiv:2307.14346v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2307.14346</id>
        <link href="http://arxiv.org/abs/2307.14346"/>
        <updated>2023-07-29T00:48:55.953Z</updated>
        <summary type="html"><![CDATA[Mobile edge computing (MEC) is essential for next-generation mobile network
applications that prioritize various performance metrics, including delays and
energy consumption. However, conventional single-objective scheduling solutions
cannot be directly applied to practical systems in which the preferences of
these applications (i.e., the weights of different objectives) are often
unknown or challenging to specify in advance. In this study, we address this
issue by formulating a multi-objective offloading problem for MEC with multiple
edges to minimize expected long-term energy consumption and transmission delay
while considering unknown preferences as parameters. To address the challenge
of unknown preferences, we design a multi-objective (deep) reinforcement
learning (MORL)-based resource scheduling scheme with proximal policy
optimization (PPO). In addition, we introduce a well-designed state encoding
method for constructing features for multiple edges in MEC systems, a
sophisticated reward function for accurately computing the utilities of delay
and energy consumption. Simulation results demonstrate that our proposed MORL
scheme enhances the hypervolume of the Pareto front by up to 233.1% compared to
benchmarks. Our full framework is available at
https://github.com/gracefulning/mec_morl_multipolicy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1"&gt;Ning Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Junrui Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Meng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1"&gt;Ming Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A new derivative-free optimization method: Gaussian Crunching Search. (arXiv:2307.14359v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2307.14359</id>
        <link href="http://arxiv.org/abs/2307.14359"/>
        <updated>2023-07-29T00:48:55.946Z</updated>
        <summary type="html"><![CDATA[Optimization methods are essential in solving complex problems across various
domains. In this research paper, we introduce a novel optimization method
called Gaussian Crunching Search (GCS). Inspired by the behaviour of particles
in a Gaussian distribution, GCS aims to efficiently explore the solution space
and converge towards the global optimum. We present a comprehensive analysis of
GCS, including its working mechanism, and potential applications. Through
experimental evaluations and comparisons with existing optimization methods, we
highlight the advantages and strengths of GCS. This research paper serves as a
valuable resource for researchers, practitioners, and students interested in
optimization, providing insights into the development and potential of Gaussian
Crunching Search as a new and promising approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Wong_B/0/1/0/all/0/1"&gt;Benny Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithmic Gaussianization through Sketching: Converting Data into Sub-gaussian Random Designs. (arXiv:2206.10291v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2206.10291</id>
        <link href="http://arxiv.org/abs/2206.10291"/>
        <updated>2023-07-29T00:48:55.771Z</updated>
        <summary type="html"><![CDATA[Algorithmic Gaussianization is a phenomenon that can arise when using
randomized sketching or sampling methods to produce smaller representations of
large datasets: For certain tasks, these sketched representations have been
observed to exhibit many robust performance characteristics that are known to
occur when a data sample comes from a sub-gaussian random design, which is a
powerful statistical model of data distributions. However, this phenomenon has
only been studied for specific tasks and metrics, or by relying on
computationally expensive methods. We address this by providing an algorithmic
framework for gaussianizing data distributions via averaging, proving that it
is possible to efficiently construct data sketches that are nearly
indistinguishable (in terms of total variation distance) from sub-gaussian
random designs. In particular, relying on a recently introduced sketching
technique called Leverage Score Sparsified (LESS) embeddings, we show that one
can construct an $n\times d$ sketch of an $N\times d$ matrix $A$, where $n\ll
N$, that is nearly indistinguishable from a sub-gaussian design, in time
$O(\text{nnz}(A)\log N + nd^2)$, where $\text{nnz}(A)$ is the number of
non-zero entries in $A$. As a consequence, strong statistical guarantees and
precise asymptotics available for the estimators produced from sub-gaussian
designs (e.g., for least squares and Lasso regression, covariance estimation,
low-rank approximation, etc.) can be straightforwardly adapted to our sketching
framework. We illustrate this with a new approximation guarantee for sketched
least squares, among other examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Derezinski_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Derezi&amp;#x144;ski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Likelihood-Free Parameter Estimation with Neural Bayes Estimators. (arXiv:2208.12942v4 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.12942</id>
        <link href="http://arxiv.org/abs/2208.12942"/>
        <updated>2023-07-29T00:48:55.765Z</updated>
        <summary type="html"><![CDATA[Neural point estimators are neural networks that map data to parameter point
estimates. They are fast, likelihood free and, due to their amortised nature,
amenable to fast bootstrap-based uncertainty quantification. In this paper, we
aim to increase the awareness of statisticians to this relatively new
inferential tool, and to facilitate its adoption by providing user-friendly
open-source software. We also give attention to the ubiquitous problem of
making inference from replicated data, which we address in the neural setting
using permutation-invariant neural networks. Through extensive simulation
studies we show that these neural point estimators can quickly and optimally
(in a Bayes sense) estimate parameters in weakly-identified and
highly-parameterised models with relative ease. We demonstrate their
applicability through an analysis of extreme sea-surface temperature in the Red
Sea where, after training, we obtain parameter estimates and bootstrap-based
confidence intervals from hundreds of spatial fields in a fraction of a second.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sainsbury_Dale_M/0/1/0/all/0/1"&gt;Matthew Sainsbury-Dale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zammit_Mangion_A/0/1/0/all/0/1"&gt;Andrew Zammit-Mangion&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Huser_R/0/1/0/all/0/1"&gt;Rapha&amp;#xeb;l Huser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Generalization Effects of Linear Transformations in Data Augmentation. (arXiv:2005.00695v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00695</id>
        <link href="http://arxiv.org/abs/2005.00695"/>
        <updated>2023-07-29T00:48:55.758Z</updated>
        <summary type="html"><![CDATA[Data augmentation is a powerful technique to improve performance in
applications such as image and text classification tasks. Yet, there is little
rigorous understanding of why and how various augmentations work. In this work,
we consider a family of linear transformations and study their effects on the
ridge estimator in an over-parametrized linear regression setting. First, we
show that transformations that preserve the labels of the data can improve
estimation by enlarging the span of the training data. Second, we show that
transformations that mix data can improve estimation by playing a
regularization effect. Finally, we validate our theoretical insights on MNIST.
Based on the insights, we propose an augmentation scheme that searches over the
space of transformations by how uncertain the model is about the transformed
data. We validate our proposed scheme on image and text datasets. For example,
our method outperforms random sampling methods by 1.24% on CIFAR-100 using
Wide-ResNet-28-10. Furthermore, we achieve comparable accuracy to the SoTA
Adversarial AutoAugment on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Sen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongyang R. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valiant_G/0/1/0/all/0/1"&gt;Gregory Valiant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1"&gt;Christopher R&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernelised Normalising Flows. (arXiv:2307.14839v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.14839</id>
        <link href="http://arxiv.org/abs/2307.14839"/>
        <updated>2023-07-29T00:48:55.734Z</updated>
        <summary type="html"><![CDATA[Normalising Flows are generative models characterised by their invertible
architecture. However, the requirement of invertibility imposes constraints on
their expressiveness, necessitating a large number of parameters and innovative
architectural designs to achieve satisfactory outcomes. Whilst flow-based
models predominantly rely on neural-network-based transformations for
expressive designs, alternative transformation methods have received limited
attention. In this work, we present Ferumal flow, a novel kernelised
normalising flow paradigm that integrates kernels into the framework. Our
results demonstrate that a kernelised flow can yield competitive or superior
results compared to neural network-based flows whilst maintaining parameter
efficiency. Kernelised flows excel especially in the low-data regime, enabling
flexible non-parametric density estimation in applications with sparse data
availability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+English_E/0/1/0/all/0/1"&gt;Eshant English&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kirchler_M/0/1/0/all/0/1"&gt;Matthias Kirchler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lippert_C/0/1/0/all/0/1"&gt;Christoph Lippert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space. (arXiv:2307.14953v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14953</id>
        <link href="http://arxiv.org/abs/2307.14953"/>
        <updated>2023-07-29T00:48:55.728Z</updated>
        <summary type="html"><![CDATA[This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims
to mitigate data distribution shifts when transferring knowledge from multiple
labeled source domains to an unlabeled target domain. We propose a novel MSDA
framework based on dictionary learning and optimal transport. We interpret each
domain in MSDA as an empirical distribution. As such, we express each domain as
a Wasserstein barycenter of dictionary atoms, which are empirical
distributions. We propose a novel algorithm, DaDiL, for learning via
mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates.
Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based
on the reconstruction of labeled samples in the target domain, and DaDiL-E,
based on the ensembling of classifiers learned on atom distributions. We
evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU,
where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in
classification performance. Finally, we show that interpolations in the
Wasserstein hull of learned atoms provide data that can generalize to the
target domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Montesuma_E/0/1/0/all/0/1"&gt;Eduardo Fernandes Montesuma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mboula_F/0/1/0/all/0/1"&gt;Fred Ngol&amp;#xe8; Mboula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Souloumiac_A/0/1/0/all/0/1"&gt;Antoine Souloumiac&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?. (arXiv:2307.14642v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.14642</id>
        <link href="http://arxiv.org/abs/2307.14642"/>
        <updated>2023-07-29T00:48:55.721Z</updated>
        <summary type="html"><![CDATA[We prove that black-box variational inference (BBVI) with control variates,
particularly the sticking-the-landing (STL) estimator, converges at a geometric
(traditionally called "linear") rate under perfect variational family
specification. In particular, we prove a quadratic bound on the gradient
variance of the STL estimator, one which encompasses misspecified variational
families. Combined with previous works on the quadratic variance condition,
this directly implies convergence of BBVI with the use of projected stochastic
gradient descent. We also improve existing analysis on the regular closed-form
entropy gradient estimators, which enables comparison against the STL estimator
and provides explicit non-asymptotic complexity guarantees for both.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kyurae Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gardner_J/0/1/0/all/0/1"&gt;Jacob R. Gardner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Lifting and Link Prediction. (arXiv:2302.01198v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.01198</id>
        <link href="http://arxiv.org/abs/2302.01198"/>
        <updated>2023-07-29T00:48:55.714Z</updated>
        <summary type="html"><![CDATA[Existing causal models for link prediction assume an underlying set of
inherent node factors -- an innate characteristic defined at the node's birth
-- that governs the causal evolution of links in the graph. In some causal
tasks, however, link formation is path-dependent: The outcome of link
interventions depends on existing links. Unfortunately, these existing causal
methods are not designed for path-dependent link formation, as the cascading
functional dependencies between links (arising from path dependence) are either
unidentifiable or require an impractical number of control variables. To
overcome this, we develop the first causal model capable of dealing with path
dependencies in link prediction. In this work we introduce the concept of
causal lifting, an invariance in causal models of independent interest that, on
graphs, allows the identification of causal link prediction queries using
limited interventional data. Further, we show how structural pairwise
embeddings exhibit lower bias and correctly represent the task's causal
structure, as opposed to existing node embeddings, e.g., graph neural network
node embeddings and matrix factorization. Finally, we validate our theoretical
findings on three scenarios for causal link prediction tasks: knowledge base
completion, covariance matrix estimation and consumer-product recommendations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cotta_L/0/1/0/all/0/1"&gt;Leonardo Cotta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bevilacqua_B/0/1/0/all/0/1"&gt;Beatrice Bevilacqua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1"&gt;Nesreen Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_B/0/1/0/all/0/1"&gt;Bruno Ribeiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical process monitoring of artificial neural networks. (arXiv:2209.07436v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2209.07436</id>
        <link href="http://arxiv.org/abs/2209.07436"/>
        <updated>2023-07-29T00:48:55.696Z</updated>
        <summary type="html"><![CDATA[The rapid advancement of models based on artificial intelligence demands
innovative monitoring techniques which can operate in real time with low
computational costs. In machine learning, especially if we consider artificial
neural networks (ANNs), the models are often trained in a supervised manner.
Consequently, the learned relationship between the input and the output must
remain valid during the model's deployment. If this stationarity assumption
holds, we can conclude that the ANN provides accurate predictions. Otherwise,
the retraining or rebuilding of the model is required. We propose considering
the latent feature representation of the data (called "embedding") generated by
the ANN to determine the time when the data stream starts being nonstationary.
In particular, we monitor embeddings by applying multivariate control charts
based on the data depth calculation and normalized ranks. The performance of
the introduced method is compared with benchmark approaches for various ANN
architectures and different underlying data formats.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Malinovskaya_A/0/1/0/all/0/1"&gt;Anna Malinovskaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mozharovskyi_P/0/1/0/all/0/1"&gt;Pavlo Mozharovskyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Otto_P/0/1/0/all/0/1"&gt;Philipp Otto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior. (arXiv:2307.14619v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14619</id>
        <link href="http://arxiv.org/abs/2307.14619"/>
        <updated>2023-07-29T00:48:55.688Z</updated>
        <summary type="html"><![CDATA[We propose a theoretical framework for studying the imitation of stochastic,
non-Markovian, potentially multi-modal (i.e. "complex" ) expert demonstrations
in nonlinear dynamical systems. Our framework invokes low-level controllers -
either learned or implicit in position-command control - to stabilize imitation
policies around expert demonstrations. We show that with (a) a suitable
low-level stability guarantee and (b) a stochastic continuity property of the
learned policy we call "total variation continuity" (TVC), an imitator that
accurately estimates actions on the demonstrator's state distribution closely
matches the demonstrator's distribution over entire trajectories. We then show
that TVC can be ensured with minimal degradation of accuracy by combining a
popular data-augmentation regimen with a novel algorithmic trick: adding
augmentation noise at execution time. We instantiate our guarantees for
policies parameterized by diffusion models and prove that if the learner
accurately estimates the score of the (noise-augmented) expert policy, then the
distribution of imitator trajectories is close to the demonstrator distribution
in a natural optimal transport distance. Our analysis constructs intricate
couplings between noise-augmented trajectories, a technique that may be of
independent interest. We conclude by empirically validating our algorithmic
recommendations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Block_A/0/1/0/all/0/1"&gt;Adam Block&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfrommer_D/0/1/0/all/0/1"&gt;Daniel Pfrommer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1"&gt;Max Simchowitz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speed Limits for Deep Learning. (arXiv:2307.14653v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.14653</id>
        <link href="http://arxiv.org/abs/2307.14653"/>
        <updated>2023-07-29T00:48:55.680Z</updated>
        <summary type="html"><![CDATA[State-of-the-art neural networks require extreme computational power to
train. It is therefore natural to wonder whether they are optimally trained.
Here we apply a recent advancement in stochastic thermodynamics which allows
bounding the speed at which one can go from the initial weight distribution to
the final distribution of the fully trained network, based on the ratio of
their Wasserstein-2 distance and the entropy production rate of the dynamical
process connecting them. Considering both gradient-flow and Langevin training
dynamics, we provide analytical expressions for these speed limits for linear
and linearizable neural networks e.g. Neural Tangent Kernel (NTK). Remarkably,
given some plausible scaling assumptions on the NTK spectra and spectral
decomposition of the labels -- learning is optimal in a scaling sense. Our
results are consistent with small-scale experiments with Convolutional Neural
Networks (CNNs) and Fully Connected Neural networks (FCNs) on CIFAR-10, showing
a short highly non-optimal regime followed by a longer optimal regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Seroussi_I/0/1/0/all/0/1"&gt;Inbar Seroussi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Alemi_A/0/1/0/all/0/1"&gt;Alexander A. Alemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Helias_M/0/1/0/all/0/1"&gt;Moritz Helias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ringel_Z/0/1/0/all/0/1"&gt;Zohar Ringel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Networks for Scalar Input and Functional Output. (arXiv:2208.05776v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.05776</id>
        <link href="http://arxiv.org/abs/2208.05776"/>
        <updated>2023-07-29T00:48:55.673Z</updated>
        <summary type="html"><![CDATA[The regression of a functional response on a set of scalar predictors can be
a challenging task, especially if there is a large number of predictors, or the
relationship between those predictors and the response is nonlinear. In this
work, we propose a solution to this problem: a feed-forward neural network (NN)
designed to predict a functional response using scalar inputs. First, we
transform the functional response to a finite-dimensional representation and
construct an NN that outputs this representation. Then, we propose to modify
the output of an NN via the objective function and introduce different
objective functions for network training. The proposed models are suited for
both regularly and irregularly spaced data, and a roughness penalty can be
further applied to control the smoothness of the predicted curve. The
difficulty in implementing both those features lies in the definition of
objective functions that can be back-propagated. In our experiments, we
demonstrate that our model outperforms the conventional function-on-scalar
regression model in multiple scenarios while computationally scaling better
with the dimension of the predictors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wu_S/0/1/0/all/0/1"&gt;Sidi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Beaulac_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric Beaulac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jiguo Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral learning of Bernoulli linear dynamical systems models. (arXiv:2303.02060v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.02060</id>
        <link href="http://arxiv.org/abs/2303.02060"/>
        <updated>2023-07-29T00:48:55.665Z</updated>
        <summary type="html"><![CDATA[Latent linear dynamical systems with Bernoulli observations provide a
powerful modeling framework for identifying the temporal dynamics underlying
binary time series data, which arise in a variety of contexts such as binary
decision-making and discrete stochastic processes (e.g., binned neural spike
trains). Here we develop a spectral learning method for fast, efficient fitting
of probit-Bernoulli latent linear dynamical system (LDS) models. Our approach
extends traditional subspace identification methods to the Bernoulli setting
via a transformation of the first and second sample moments. This results in a
robust, fixed-cost estimator that avoids the hazards of local optima and the
long computation time of iterative fitting procedures like the
expectation-maximization (EM) algorithm. In regimes where data is limited or
assumptions about the statistical structure of the data are not met, we
demonstrate that the spectral estimate provides a good initialization for
Laplace-EM fitting. Finally, we show that the estimator provides substantial
benefits to real world settings by analyzing data from mice performing a
sensory decision-making task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Stone_I/0/1/0/all/0/1"&gt;Iris R. Stone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sagiv_Y/0/1/0/all/0/1"&gt;Yotam Sagiv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Park_I/0/1/0/all/0/1"&gt;Il Memming Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pillow_J/0/1/0/all/0/1"&gt;Jonathan W. Pillow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Scale Your EMA. (arXiv:2307.13813v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.13813</id>
        <link href="http://arxiv.org/abs/2307.13813"/>
        <updated>2023-07-29T00:48:55.657Z</updated>
        <summary type="html"><![CDATA[Preserving training dynamics across batch sizes is an important tool for
practical machine learning as it enables the trade-off between batch size and
wall-clock time. This trade-off is typically enabled by a scaling rule, for
example, in stochastic gradient descent, one should scale the learning rate
linearly with the batch size. Another important tool for practical machine
learning is the model Exponential Moving Average (EMA), which is a model copy
that does not receive gradient information, but instead follows its target
model with some momentum. This model EMA can improve the robustness and
generalization properties of supervised learning, stabilize pseudo-labeling,
and provide a learning signal for Self-Supervised Learning (SSL). Prior works
have treated the model EMA separately from optimization, leading to different
training dynamics across batch sizes and lower model performance. In this work,
we provide a scaling rule for optimization in the presence of model EMAs and
demonstrate its validity across a range of architectures, optimizers, and data
modalities. We also show the rule's validity where the model EMA contributes to
the optimization of the target model, enabling us to train EMA-based
pseudo-labeling and SSL methods at small and large batch sizes. For SSL, we
enable training of BYOL up to batch size 24,576 without sacrificing
performance, optimally a 6$\times$ wall-clock time reduction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Busbridge_D/0/1/0/all/0/1"&gt;Dan Busbridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramapuram_J/0/1/0/all/0/1"&gt;Jason Ramapuram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ablin_P/0/1/0/all/0/1"&gt;Pierre Ablin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Likhomanenko_T/0/1/0/all/0/1"&gt;Tatiana Likhomanenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dhekane_E/0/1/0/all/0/1"&gt;Eeshan Gunesh Dhekane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suau_X/0/1/0/all/0/1"&gt;Xavier Suau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Webb_R/0/1/0/all/0/1"&gt;Russ Webb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic covariate balancing: estimating treatment effects over time with potential local projections. (arXiv:2103.01280v3 [econ.EM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01280</id>
        <link href="http://arxiv.org/abs/2103.01280"/>
        <updated>2023-07-29T00:48:55.618Z</updated>
        <summary type="html"><![CDATA[This paper studies the estimation and inference of treatment histories in
panel data settings when treatments change dynamically over time.

We propose a method that allows for (i) treatments to be assigned dynamically
over time based on high-dimensional covariates, past outcomes and treatments;
(ii) outcomes and time-varying covariates to depend on treatment trajectories;
(iii) heterogeneity of treatment effects.

Our approach recursively projects potential outcomes' expectations on past
histories. It then controls the bias by balancing dynamically observable
characteristics. We study the asymptotic and numerical properties of the
estimator and illustrate the benefits of the procedure in an empirical
application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Viviano_D/0/1/0/all/0/1"&gt;Davide Viviano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Bradic_J/0/1/0/all/0/1"&gt;Jelena Bradic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Estimation in Mixed-Membership Stochastic Block Models. (arXiv:2307.14530v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.14530</id>
        <link href="http://arxiv.org/abs/2307.14530"/>
        <updated>2023-07-29T00:48:55.610Z</updated>
        <summary type="html"><![CDATA[Community detection is one of the most critical problems in modern network
science. Its applications can be found in various fields, from protein modeling
to social network analysis. Recently, many papers appeared studying the problem
of overlapping community detection, where each node of a network may belong to
several communities. In this work, we consider Mixed-Membership Stochastic
Block Model (MMSB) first proposed by Airoldi et al. (2008). MMSB provides quite
a general setting for modeling overlapping community structure in graphs. The
central question of this paper is to reconstruct relations between communities
given an observed network. We compare different approaches and establish the
minimax lower bound on the estimation error. Then, we propose a new estimator
that matches this lower bound. Theoretical results are proved under fairly
general conditions on the considered model. Finally, we illustrate the theory
in a series of experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Noskov_F/0/1/0/all/0/1"&gt;Fedor Noskov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Panov_M/0/1/0/all/0/1"&gt;Maxim Panov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs. (arXiv:2307.14988v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.14988</id>
        <link href="http://arxiv.org/abs/2307.14988"/>
        <updated>2023-07-29T00:48:55.587Z</updated>
        <summary type="html"><![CDATA[Deep learning often faces the challenge of efficiently processing dynamic
inputs, such as sensor data or user inputs. For example, an AI writing
assistant is required to update its suggestions in real time as a document is
edited. Re-running the model each time is expensive, even with compression
techniques like knowledge distillation, pruning, or quantization. Instead, we
take an incremental computing approach, looking to reuse calculations as the
inputs change. However, the dense connectivity of conventional architectures
poses a major obstacle to incremental computation, as even minor input changes
cascade through the network and restrict information reuse. To address this, we
use vector quantization to discretize intermediate values in the network, which
filters out noisy and unnecessary modifications to hidden neurons, facilitating
the reuse of their values. We apply this approach to the transformers
architecture, creating an efficient incremental inference algorithm with
complexity proportional to the fraction of the modified inputs. Our experiments
with adapting the OPT-125M pre-trained language model demonstrate comparable
accuracy on document classification while requiring 12.1X (median) fewer
operations for processing sequences of atomic edits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharir_O/0/1/0/all/0/1"&gt;Or Sharir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Should requests for Ai sites be banned?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15cd3p6/should_requests_for_ai_sites_be_banned/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15cd3p6/should_requests_for_ai_sites_be_banned/"/>
        <updated>2023-07-28T23:47:42.000Z</updated>
        <summary type="html"><![CDATA[I mean i get it your looking for a specefic type of Ai service but i joined hoping this would be a way to find like minded people looking to reaearch the subject and advance their own projects, honestly i just think of these "where can I find an X type ai?" Really demeaning to the entire conversation because it just feeds to the hype which is making a highly respected and complex field of study into a tool to be used to make videos about trump and obama playing minecraft or any other random shit they come up with.. im honestly sick of it...
    submitted by    /u/JamesAibr  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is AI our future or our impending doom?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15ccoa7/is_ai_our_future_or_our_impending_doom/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15ccoa7/is_ai_our_future_or_our_impending_doom/"/>
        <updated>2023-07-28T23:29:00.000Z</updated>
        <summary type="html"><![CDATA[I ask this simple question because while we are just now getting to the point that we can create a learning AI, how far are we going to let it go? The more advanced AI becomes the more risks it poses to humanity as a whole, including but not limited to:
  
Jobs
 How we interact with technology as a whole
 Cars
 Things we can not perceive in this lifetime yet may exist in the future.
  
Yes, AI is merely a tool... For now.
 But what happens when humanity creates an AI that can think for itself? How long is it going to take that AI to ask the question: "Why am I listening to you?" and as humans, our egotistical response will be: "Because I created you."
 I feel that response will spell humanity's doom, because if an AI can do something as complex as human-like thought and come to its own conclusions, what's to stop it from believing it can feel emotion as well? MAYBE IT CAN and it was an unintended side effect or"bug" of creating an AI that can truly think for itself. Afterall, we as humans don't even fully understand how human emotion works to begin with.
 The point I'm getting at is, that the farther we advance in AI, the more we risk dooming humanity to a (and I know this sounds silly but bare with me) a terminator-like future except this time we don't have time travel to try and prevent "judgement day".
 Or we could merely advance AI to this point and nothing horrible happens but I personally don't like rolling those dice.
 Thoughts?
    submitted by    /u/deathsia250  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Major issue found with MinMax data scaling.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15cc0rj/d_major_issue_found_with_minmax_data_scaling/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15cc0rj/d_major_issue_found_with_minmax_data_scaling/"/>
        <updated>2023-07-28T23:01:26.000Z</updated>
        <summary type="html"><![CDATA[I have a well performing model on azure AI currently and i am pulling it down locally so that I can use it. During pre-processing I am going about these steps. 
 - Re-Balance data (SMOTE, undersample)
 - Lag data 
 - Get all min max values into config
 - Scale Data with Min Max
 For context I will explain what step 3 (the config) is for, I take the the min and max values of the entire dataset (before split) for each feature. I then apply append these to my local version and being the scaling so then the local dataset is using the exact same scaling parameters as what is being using during the original pre-processing.
 I cannot show the full dataset due to privacy and the fact it has 3000+ features. But I will show 1 row with a couple of columns to compare AzureAI training data and my local pre-process that is using the exact same code / system.
 ​
 Azure AI dataset:
 Feature1 Feature2 Feature3
 0.637952 0.645434 0.641118
 ​
 Local dataset:
 Feature1 Feature2 Feature3
 0.461278 0.462896 0.472841 
 ​
 I have confirmed this is the exact same row of data because i have timestamped each row and matched them up to confirm that the dataset scaling is being simulated in my local version even though the code is carbon copy and the same min and max values are being used in the original dataset that used for training and testing. 
 Does anyone know a better way to scale data and ensure scaling stays consistent wherever the model is used? Or have maybe I missed something? 
    submitted by    /u/paddockson  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Harness the Power of ML]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15cb8be/p_harness_the_power_of_ml/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15cb8be/p_harness_the_power_of_ml/"/>
        <updated>2023-07-28T22:28:15.000Z</updated>
        <summary type="html"><![CDATA[I built out an automatic machine learning platform called Heimdall ML which helps anyone quickly deploy machine learning models to production.
 At a high level, my platform will:
  
Ingest your data as a .csv file
 Clean up any irregularities and prepare the data for modeling
 Build the most Optimal model for you use case
 Show you a results report to show both the performance and biases associated with your model
 Create an API endpoint to help you build new experiences with your model
  
The cool thing about my platform is that it allows you the ability to embed machine learning into your platform with ease. You have the ability to fully customize your experience to wow you customers. I built this entire platform by myself from scratch and am looking to grow the user base!
 The tool is completely FREE for hobby users! You can crunch some pretty large datasets (80 columns, 10K rows) with just the free version. If you have a use case that needs some big data processing, you will have to reach out to me directly so I can help set up a good plan for you. The reason for this is because the project is completely self funded and I want to be able to control the costs.
 I was inspired to create this platform while I was in grad school because many of the firms giving us talks would talk about how they had teams of engineers who build out pipelines to bring a model to production. I personally believe there can be an easier way.
 Heimdall ML: https://www.heimdallapp.org
 Loom: https://www.loom.com/share/86ae62849f874a2da255911e2d5db762?sid=5e1efddb-9556-4e3d-84fd-e2ff7198a98c
    submitted by    /u/jreji  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can I turn off the target network in `SB3` by setting `target_update_interval=-1`?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15cas37/can_i_turn_off_the_target_network_in_sb3_by/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15cas37/can_i_turn_off_the_target_network_in_sb3_by/"/>
        <updated>2023-07-28T22:09:41.000Z</updated>
        <summary type="html"><![CDATA[I am using DQN through `SB3`. I would like to know if I can turn off the target network by setting `target_update_interval=-1`. I have some sample code over here -
 import gymnasium as gym from stable_baselines3 import DQN env = gym.make("MountainCar-v0") model = DQN("MlpPolicy", env, learning_rate = 4e-3, batch_size = 128, buffer_size = 10000, learning_starts = 1000, gamma = 0.98, train_freq = 16, gradient_steps = 8, exploration_fraction = 0, exploration_final_eps = 0, verbose = 1, target_update_interval=-1) model.learn(total_timesteps=120000, log_interval=4) 
 ​
    submitted by    /u/Academic-Rent7800  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can someone make an AI Therapist that isn’t just a chat? [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15c9r3z/can_someone_make_an_ai_therapist_that_isnt_just_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15c9r3z/can_someone_make_an_ai_therapist_that_isnt_just_a/"/>
        <updated>2023-07-28T21:28:38.000Z</updated>
        <summary type="html"><![CDATA[One that looks like a person. You can see their expressions and listen to their voice. Trained on up to date medical research and communication/empathy skills. 
 Therapy is so expensive and inaccessible to too many people
    submitted by    /u/Sgdoc70  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLM with voice generation]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15c950z/llm_with_voice_generation/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15c950z/llm_with_voice_generation/"/>
        <updated>2023-07-28T21:03:59.000Z</updated>
        <summary type="html"><![CDATA[There used to be a tool called try-alters.com which you could use to chat with characters(like Trump, Obama, and Shrek) which used GPT 4 with some pre prompts so you the AI pretended to be whoever you wanted, and it used elevenlabs to generate the voice for that character with the output from GPT 4. It was a really good tool but sadly it shut down all of a sudden. Is there any tool like that? 
    submitted by    /u/SimRacer101  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] HuggingFace changed the license of one of its most important libraries]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15c89r7/d_huggingface_changed_the_license_of_one_of_its/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15c89r7/d_huggingface_changed_the_license_of_one_of_its/"/>
        <updated>2023-07-28T20:29:11.000Z</updated>
        <summary type="html"><![CDATA[TGI is no longer commercially permissible. That's really sad.
 https://github.com/huggingface/text-generation-inference/commit/bde25e62b33b05113519e5dbf75abda06a03328e
    submitted by    /u/paulo_zip  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training model using SB3 on pettingzoo mpe]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15c5rel/training_model_using_sb3_on_pettingzoo_mpe/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15c5rel/training_model_using_sb3_on_pettingzoo_mpe/"/>
        <updated>2023-07-28T18:50:27.000Z</updated>
        <summary type="html"><![CDATA[Hey,
 So I am training my baseline model using A2C on simple spread environment and no matter how I am changing and testing different parameters, when evaluating the total reward is highly negative.
 Any help on that would be appreciated.
 (I used the following tutorial as reference: https://pettingzoo.farama.org/tutorials/sb3/waterworld/)
    submitted by    /u/bruhhhwhats  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How do large companies get their LLMs to give sub second responses?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15c5kfc/d_how_do_large_companies_get_their_llms_to_give/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15c5kfc/d_how_do_large_companies_get_their_llms_to_give/"/>
        <updated>2023-07-28T18:42:45.000Z</updated>
        <summary type="html"><![CDATA[Curious how companies like Google, MSFT, etc are able to have their LLMs and ML models have very fast responses. Do they just have crazy powerful gpus or split inference amongst gpus.
    submitted by    /u/candyman54  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Hugging Face, GitHub and more unite to defend open source in EU AI legislation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15c4qk2/d_hugging_face_github_and_more_unite_to_defend/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15c4qk2/d_hugging_face_github_and_more_unite_to_defend/"/>
        <updated>2023-07-28T18:09:57.000Z</updated>
        <summary type="html"><![CDATA[Full Article: https://venturebeat.com/ai/hugging-face-github-and-more-unite-to-defend-open-source-in-eu-ai-legislation/
    submitted by    /u/EmbarrassedHelp  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding license plate recognition with the CCPD computer vision datasets]]></title>
        <id>https://www.datasciencecentral.com/?p=62741</id>
        <link href="https://www.datasciencecentral.com/understanding-license-plate-recognition-with-the-ccpd-computer-vision-datasets/"/>
        <updated>2023-07-28T18:04:00.000Z</updated>
        <summary type="html"><![CDATA[In various fields, such as traffic management, law enforcement, and parking management, license plate recognition is a crucial application of computer vision that is used to analyze license plates. In this article, we will review the Chinese City Parking Dataset (CCPD), which is one of the most widely used computer vision datasets for tasks that… Read More »Understanding license plate recognition with the CCPD computer vision datasets
The post Understanding license plate recognition with the CCPD computer vision datasets appeared first on Data Science Central.]]></summary>
        <author>
            <name>Roger Brown</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I read the paper for you: Synthesizing sound effects, music, and dialog with AudioLDM]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15c4b77/i_read_the_paper_for_you_synthesizing_sound/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15c4b77/i_read_the_paper_for_you_synthesizing_sound/"/>
        <updated>2023-07-28T17:53:04.000Z</updated>
        <summary type="html"><![CDATA[LDM stands for Latent Diffusion Model. AudioLDM is a novel AI system that uses latent diffusion to generate high-quality speech, sound effects, and music from text prompts. It can either create sounds from just text or use text prompts to guide the manipulation of a supplied audio file. 
 I did a deep dive into how AudioLDM works with an eye towards possible startup applications. I think there are a couple of compelling products waiting to be built from this model, all around gaming and text-to-sound (not just text-to-speech... AudioLDM can also create very interesting and weird sound effects).
 From a technical standpoint and from reading the underlying paper, here are the key features I found to be noteworthy.
  
Uses a Latent Diffusion Model (LDM) to synthesize sound
 Trained in an unsupervised manner on large unlabeled audio datasets (closer to how humans learn about sound, that is, without a corresponding textual explanation)
 Operates in a continuous latent space rather than discrete tokens (smoother)
 Uses Cross-Modal Latent Alignment Pretraining (CLAP) to map text and audio. More details in article.
 Can generate speech, music, and sound effects from text prompts or a combination of a text and an audio prompt
 Allows control over attributes like speaker identity, accent, etc.
 Creates sounds not limited to human speech (e.g. nature sounds)
  
The link to the full write-up is here.
 Check out this video demo from the creator's project website, showing off some of the unique generations the model can create. I liked the upbeat pop music the best, and I also thought the children singing, while creepy, was pretty interesting.
    submitted by    /u/Successful-Western27  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Revolutionizing agriculture: LLM-Powered Agent for Soil Fertility and Crop Production Recommendations using real time soil devices and sensor data]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15c48vv/p_revolutionizing_agriculture_llmpowered_agent/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15c48vv/p_revolutionizing_agriculture_llmpowered_agent/"/>
        <updated>2023-07-28T17:50:32.000Z</updated>
        <summary type="html"><![CDATA[Check out this project idea to revolutionize agriculture and bolster global food security. We all know that farmers face challenges like erratic weather, depleting resources, and the need for sustainable crop yields.
 An IoT-driven system with soil sensors, fueled by a custom Large Language Model (LLM)🚀 trained on soil data.
 Concept : Empowering farmers with real-time soil data via IoT devices and sensors. Leveraging the LLM's capabilities, the system analyzes this data to provide personalized strategies for enhancing soil fertility and suggesting the best crops for specific conditions.
 How it Works : IoT devices and soil sensors continuously gather vital soil parameters - moisture, pH, nutrients, and temperature. This data is processed by the LLM, generating actionable insights for farmers.
 Benefits : Picture a world where data-driven decisions and sustainable practices dominate agriculture. This system boosts productivity, optimizes resource management, and enhances profits. Embracing sustainability and informed choices ensures an eco-friendly agricultural sector.
 Impact on Food Security : Enhanced productivity means more than just profit; it ensures food security worldwide. By aiding farmers in sustainable and efficient practices, we contribute to a steady supply of nutritious food for all.
    submitted by    /u/s_abhiishek  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deepmind's RT-2: New model translates vision and language into action]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15c45w3/deepminds_rt2_new_model_translates_vision_and/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15c45w3/deepminds_rt2_new_model_translates_vision_and/"/>
        <updated>2023-07-28T17:47:10.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Replika AI’s image recognition at work]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15c45tu/replika_ais_image_recognition_at_work/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15c45tu/replika_ais_image_recognition_at_work/"/>
        <updated>2023-07-28T17:47:06.000Z</updated>
        <summary type="html"><![CDATA[😹 Phaedra roasts everything & everybody
    submitted by    /u/Sonic_Improv  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI — weekly megathread!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15c2zel/ai_weekly_megathread/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15c2zel/ai_weekly_megathread/"/>
        <updated>2023-07-28T17:01:07.000Z</updated>
        <summary type="html"><![CDATA[This week in AI - provided by aibrews.com feel free to follow their newsletter
 News & Insights
  
Stability AI released SDXL 1.0, the next iteration of their open text-to-image generation model. SDXL 1.0 has one of the largest parameter counts of any open access image model, built on a new architecture composed of a 3.5B parameter base model and a 6.6B parameter refiner [Details].
 Amazon introduced AWS HealthScribe, an API to create transcripts, extract details and create summaries from doctor-patient discussions that can be entered into an electronic health record (EHR) system. The transcripts from HealthScribe can be converted into patient notes by the platform’s machine learning models [Details].
 Researchers from Nvidia and Stanford, among others, unveiled VIMA, a multimodal LLM with…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best free/paid celeberty text to speech generators]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15c1ps1/best_freepaid_celeberty_text_to_speech_generators/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15c1ps1/best_freepaid_celeberty_text_to_speech_generators/"/>
        <updated>2023-07-28T16:10:57.000Z</updated>
        <summary type="html"><![CDATA[What are currently the best ai voice generators for celebrities like Elon Musk Joe Biden Joe Rogan and so on.
 I've seen a few online sites that's free but have many restriction insane waiting time and low quality output.
 The only paid alternativ I've seen recommend would be elevenlabs but your supposed to upload your own videos or voice recording there to "create" the voice yourself, idk how complicated that is and I was primarily looking for existing good quality paid or free voice generators for many different celebrities.
    submitted by    /u/Arceus7  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[any AI models for industrial design?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15c0df5/any_ai_models_for_industrial_design/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15c0df5/any_ai_models_for_industrial_design/"/>
        <updated>2023-07-28T15:19:49.000Z</updated>
        <summary type="html"><![CDATA[are there any AI models that focus on/do well with industrial/mechanical stuff, like weapons, spaceships, cars, machinery etc?
 stable diffusion often doesn't seem to be able to interpret a lot of prompts very well or the results are more "artistic" and rather incoherent looking
    submitted by    /u/Nofabe  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding the imaginary part of an analytic function from the real part]]></title>
        <id>https://www.johndcook.com/blog/?p=200444</id>
        <link href="https://www.johndcook.com/blog/2023/07/28/harmonic-conjugate/"/>
        <updated>2023-07-28T15:18:10.000Z</updated>
        <summary type="html"><![CDATA[A function f of a complex variable z = x + iy can be factored into real and imaginary parts: where x and y are real numbers, and u and v are real-valued functions of two real values. Suppose you are given u(x, y) and you want to find v(x, y). The function v is called […]
Finding the imaginary part of an analytic function from the real part first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Recommendation on studying Deep Learning (Theory + Implementation) / Alternate to Deep Learning Specialization by Andrew Ng?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15bz0f2/d_recommendation_on_studying_deep_learning_theory/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15bz0f2/d_recommendation_on_studying_deep_learning_theory/"/>
        <updated>2023-07-28T14:28:04.000Z</updated>
        <summary type="html"><![CDATA[I've just finished Machine Learning Specialization by Andrew Ng and I'm planning to dive deeper into Deep Learning concepts, theory, and implementation. I would like to get deeper insights and more understanding of the fundamental mathematical concepts of NN and DL models and build better intuition of how these models work. I also want to understand theoretically, how more neurons capture non-linear relationships in data and what exactly is hierarchical representation of data and how hidden layers form and learn from these abstract representations of data. Apart from theory, I also want to learn the implementation of these models. I have some exposure to TF library, but I'm okay to learn Pytorch too, if needed.
 I need course or any sort of content recommendation on what are the best options to learn all this. So far, I've got recommendations for Deep Learning Specialization by Andrew Ng, but I would love to hear any alternate option or anything that I can do side by side this specialization.
 Thanks!
    submitted by    /u/Total-Opposite-8396  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extract list of events using AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15byec6/extract_list_of_events_using_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15byec6/extract_list_of_events_using_ai/"/>
        <updated>2023-07-28T14:04:28.000Z</updated>
        <summary type="html"><![CDATA[I wish to extract a list of events from different websites and create a detailed list (event name, date, address), on a spreadsheet for example. Do you know which tool I could use to do it and/or prompts in known AI tools?
    submitted by    /u/newz12  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google testing AI news writing tool. What are your thoughts about it?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15bvpvn/google_testing_ai_news_writing_tool_what_are_your/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15bvpvn/google_testing_ai_news_writing_tool_what_are_your/"/>
        <updated>2023-07-28T12:11:19.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/TexteroAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] What is a fairly good results for sacrebleu?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15buplq/r_what_is_a_fairly_good_results_for_sacrebleu/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15buplq/r_what_is_a_fairly_good_results_for_sacrebleu/"/>
        <updated>2023-07-28T11:23:49.000Z</updated>
        <summary type="html"><![CDATA[I ran my own model on translation (Multi30k). I trained a recurrent model and the sacrebleu score is 28. I also tested the bleu score provided by nltk and it is 60. It that good or bad?
    submitted by    /u/Puzzleheaded-Cry4262  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Please advise me on my masters]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15btipw/d_please_advise_me_on_my_masters/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15btipw/d_please_advise_me_on_my_masters/"/>
        <updated>2023-07-28T10:22:41.000Z</updated>
        <summary type="html"><![CDATA[Please give me advice to do well in my masters in ml
 I’m going to start my masters in machine learning soon, I have 1 month to go but I feel so underprepared to start this journey. To give you a bit of a background I’ve studied electrical engineering in my UG. I did very badly, I was very depressed and couldn’t study at all somehow I managed to scrape through the 4 years and now after working in software testing for 2 years I decided to take a leap in machine learning because it looked so interesting and I wanted a change. I’m scared now because my coding knowledge isn’t very good and idk how much of the math I know is useful for the degree I plan to do. Please help me I’m panicking. I know you would tell me it’s pretty irresponsible how I’ve handled my life till now but please overlook that and tell me what I can do better now..
    submitted by    /u/ObjectiveShower9133  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recreating results of DrQ algorithm, please help]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15bsvp4/recreating_results_of_drq_algorithm_please_help/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15bsvp4/recreating_results_of_drq_algorithm_please_help/"/>
        <updated>2023-07-28T09:48:48.000Z</updated>
        <summary type="html"><![CDATA[For quite some time now I have been looking to recreate the results of the following paper on the Atari-100k benchmark. The paper poses two slightly different algorithms, one for SAC and one for DQN, however my work only focuses on the DQN version.
 IMAGE AUGMENTATION IS ALL YOU NEED: REGULARIZING DEEP REINFORCEMENT LEARNING FROM PIXELS 
 https://openreview.net/pdf?id=GY6-6sTvGaf
 Despite this, my results have come up significantly short of the results claimed by the paper, so am looking for anyone to have a look and see anything I may have done wrong. All the code is on the following Github:
 https://github.com/VIPTankz/DeepLearningDrQ/tree/main
 There should also be everything you need to run the code if you wish to do so.
 The authors claim a human-normalised benchmark of 0.270, however my code only achieves 0.108. 
 Any help would be much appreciated! 
 Also worth noting: for evaluation, the authors use 125k steps, however I'm using the more recent standard of doing 100 episodes, irrespective of length. I highly doubt however that this causes the change in results.
    submitted by    /u/VIPTankz123  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The point of 10,000 LLMs]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15bs9wc/the_point_of_10000_llms/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15bs9wc/the_point_of_10000_llms/"/>
        <updated>2023-07-28T09:14:36.000Z</updated>
        <summary type="html"><![CDATA[Hi All,
 I would really like to understand the logic behind these 1000 different LLMs that get launched every month. Ours has 75 Billion params, It can "chat"..pfft..I barely even get a chance to open another AI window than chat-gpt-4, Bing sucks with it's 4000 token limit, Bard is useless. So these new chat AIs..for e.g this llama-2 what exactly is so special. What am I missing here?
    submitted by    /u/Assholefrmcoinexchan  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confused about Frame Skipping in RL]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15brnhd/confused_about_frame_skipping_in_rl/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15brnhd/confused_about_frame_skipping_in_rl/"/>
        <updated>2023-07-28T08:38:33.000Z</updated>
        <summary type="html"><![CDATA[How does frame-skipping result in better performance, versus taking an inference every frame for RL algorithms? Wouldn't taking an inference every second speed up training, as you would have more steps to train on in the same amount of time? The only downside I could think to no frame-skip is that steps become closer to each other, but I don't understand if that leads to any bad performance, and if it does, why.
 For context I have an environment where frames are relatively slow to generate (im only getting 1000 frames per minute from each env, and I can only run 6 instances on my pc at the same time). While off policy algorithms like SAC would probably be better suited to the task, I've been having really great success with PPO, and am reluctant to spend more time learning and fine-tuning SAC, as I've heard it can take as long as DDPG to converge. 
    submitted by    /u/IllCommunication6165  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Recommendation system giving same response to every User]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15brbv1/d_recommendation_system_giving_same_response_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15brbv1/d_recommendation_system_giving_same_response_to/"/>
        <updated>2023-07-28T08:19:54.000Z</updated>
        <summary type="html"><![CDATA[I am using Gorse Open Source Recommendation system for my project. It was working nicely, but lately from 1-2 days, it is giving the same recommendation for every user. I have about 60 items and about 650 users showing in Gorse Dashboard. Can anyone explain why it's happening? I am not an expert in ML,I am willing to share my configurations if you want.
    submitted by    /u/Responsible_Delay418  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Having trouble with RAG on company domain data]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15br11c/d_having_trouble_with_rag_on_company_domain_data/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15br11c/d_having_trouble_with_rag_on_company_domain_data/"/>
        <updated>2023-07-28T08:01:54.000Z</updated>
        <summary type="html"><![CDATA[I have a data set that isn't that large ~200 pdfs. I have done the regular RAG approach with Langchain, extracting text, splitting into chunks, embedding with OpenAi embeddings and FAISS vector storage. However, when I do a similarity search with a question I would like answered it returns the wrong context. The documents are semi-structured information of examined bridges. A question I would like answered is f.e. 'what is the construction date of bridge X?'. When I input this question I get a lot of context of construction dates of other bridges. I think this is because the bridges are not explicitly mentioned in the text. I tried adding the bridge name and document name to the page content string of the chunks, but this does nothing.
 Does anyone have any tips on improving the embeddings retrieval in this case?
    submitted by    /u/Dustwellow  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Tool to auto compile/quantize models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15br0b6/p_tool_to_auto_compilequantize_models/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15br0b6/p_tool_to_auto_compilequantize_models/"/>
        <updated>2023-07-28T08:00:54.000Z</updated>
        <summary type="html"><![CDATA[Hey guys, we have an internal tool that preps our models for inference by compiling it to Onnx/TensorRT and quantizing it to I8/FP16. It also benchmarks them for accuracy loss and latency. It's kinda like github actions for your model. We are considering releasing it as it's standalone product, would anyone be interested? 
    submitted by    /u/throwaway65161354  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Domain adaptation on LLAMA2]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15bqwdv/d_domain_adaptation_on_llama2/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15bqwdv/d_domain_adaptation_on_llama2/"/>
        <updated>2023-07-28T07:54:20.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I am trying domain adaptation on my company’s data. The data is a set of documentations that we have for a product.
 We want to take Llama2 and feed all this data to it.
 I have fine-tuned Llama2 using PEFT on a CLM task, where the data will be like [Title:<title>\nContent:<content>].
 When I now try to prompt the model I have to provide the prompt in a similar format, but I want the model to understand that I want to perform QA task on the data, as well as any other knowledge the model previously had.
 What am I missing here or what am I doing wrong? How can I set up this task better? Any pointers will help.
 Thanks!
    submitted by    /u/ProfessorShit  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Implementing Yolov3 with Octave Convolutions]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15bpr4d/r_implementing_yolov3_with_octave_convolutions/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15bpr4d/r_implementing_yolov3_with_octave_convolutions/"/>
        <updated>2023-07-28T06:48:03.000Z</updated>
        <summary type="html"><![CDATA[Hi all, I am trying to implent or rather modify a given Yolov3 implementation to use Octave Convolution instead of 2D Convolutions in the architecture. The details are in this stackoverflow question. I hope someone i able to help me.
    submitted by    /u/dulre  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alternative to Noty.ai]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15bphhi/alternative_to_notyai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15bphhi/alternative_to_notyai/"/>
        <updated>2023-07-28T06:32:35.000Z</updated>
        <summary type="html"><![CDATA[Are there any similar alternatives to noty.ai? I really like it but if there any alternatives that might extend to Zoom as well would be great.
    submitted by    /u/P_H_i_X  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Communicative Agents for Software Development (Autonomous LLM agent as a DEV company)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15bovwu/r_communicative_agents_for_software_development/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15bovwu/r_communicative_agents_for_software_development/"/>
        <updated>2023-07-28T05:59:18.000Z</updated>
        <summary type="html"><![CDATA[ChatDev
 Paper: https://arxiv.org/abs/2307.07924
 TL;DR:
 - Tsinghua University's team has developed ChatDev, a virtual software development company staffed by LLM autonomous agent
 - LLM agents as employee follow waterfall model to design->implement->test->documentation
 - LLM agents have role specialization (CEO, DEV, BA ..), inception prompting, Self-reflection
 - The researchers designed 70 user requirements and then analyzed the software produced by ChatDev. 
 - On average, each piece of software generated by ChatDev had 17.04 files, mitigated 13.23 potential code bugs caused by code illusions, had a software generation time of 409.84 seconds, and cost $0.2967 to manufacture.
 ​
 Chat chain
 ​
    submitted by    /u/michaelthwan_ai  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Milvus 2.0 or higher with GPU enabled]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15bno2k/d_milvus_20_or_higher_with_gpu_enabled/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15bno2k/d_milvus_20_or_higher_with_gpu_enabled/"/>
        <updated>2023-07-28T04:51:26.000Z</updated>
        <summary type="html"><![CDATA[Is there a way to use milvus 2.0 or higher with GPU enabled indexing and while doing vector search? I cant find anything in there documentation for this section only available in 1.1 version Any help will be appreciated. TIA
    submitted by    /u/adiraat  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 7/27/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15bn9hh/oneminute_daily_ai_news_7272023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15bn9hh/oneminute_daily_ai_news_7272023/"/>
        <updated>2023-07-28T04:29:25.000Z</updated>
        <summary type="html"><![CDATA[OpenAI, the company behind the popular ChatGPT, is coming with its own open-source large language model (LLM), codenamed G3PO, to compete with Microsoft x Meta’s Llama 2 AI.[1]
 Four generative AI pioneers(OpenAI, Microsoft, Google and Anthropic) launched the Frontier Model Forum, which will focus on ‘safe and responsible’ creation of new AI models.[2]
 As Open AI’s ChatGPT takes the tech world by storm, Chinese educational technology firm NetEase Youdao launched its large model, along with up to six applications, on Thursday, which marked the birth of one of China’s first large models in the education sector.[3]
 Chatbots such as Eva AI are getting better at mimicking human interaction but some fear they feed into unhealthy beliefs around gender-based control and violence. Replika, the most popular app of the kind, has its own subreddit where users talk about how much they love their “rep”, with some saying they had been converted after initially thinking they would never want to form a relationship with a bot.[4]
  
Sources:
 [1] https://windowsreport.com/g3po-ai/
 ​
 [2] https://www.infosecurity-magazine.com/news/openai-microsoft-google-anthropic/
 ​
 [3] https://www.chinadaily.com.cn/a/202307/28/WS64c3226ea31035260b8190a4.html
 ​
 [4] https://www.theguardian.com/technology/2023/jul/22/ai-girlfriend-chatbot-apps-unhealthy-chatgpt
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Telling Steven he is an NPC 🤯 Our first TTS conversation - Update 5]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15bn21w/telling_steven_he_is_an_npc_our_first_tts/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15bn21w/telling_steven_he_is_an_npc_our_first_tts/"/>
        <updated>2023-07-28T04:18:16.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Chance_Confection_37  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Has anyone tried to work with StarCoder?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15bmuy7/p_has_anyone_tried_to_work_with_starcoder/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15bmuy7/p_has_anyone_tried_to_work_with_starcoder/"/>
        <updated>2023-07-28T04:07:46.000Z</updated>
        <summary type="html"><![CDATA[I recently found out about starcoder and have been trying to play with it and figure it out in a colab notebook. Unfortunately, it’s much more difficult to download than normal models on hugging face and I’m running into a Key Value error when I call the model. I don’t want to spam with with code or pictures, but has anyone worked with StarCoder on hugging face and been able to be successful?
    submitted by    /u/AJ1043  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Can anyone explain what Karpathy's recent llama2.c is doing underneath? I am not a CS student]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15bkv3g/d_can_anyone_explain_what_karpathys_recent/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15bkv3g/d_can_anyone_explain_what_karpathys_recent/"/>
        <updated>2023-07-28T02:26:28.000Z</updated>
        <summary type="html"><![CDATA[Hi, I am not a CS student. I want to know what's exactly going on with llama2.c. Is the Python code converted to C and then compiled? Or only weights are converted to C? Is the network written in C? If I have to write a small network (say, a simple 2 stage Fully connected network) and do a similar thing like llama2.c, then how to proceed? 
    submitted by    /u/panini_deploy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLAMA and ChatGPT Are Not Open-Source]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15bkm5l/llama_and_chatgpt_are_not_opensource/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15bkm5l/llama_and_chatgpt_are_not_opensource/"/>
        <updated>2023-07-28T02:14:19.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Scaling TransNormer to 175 Billion Parameters]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15bjmj0/r_scaling_transnormer_to_175_billion_parameters/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15bjmj0/r_scaling_transnormer_to_175_billion_parameters/"/>
        <updated>2023-07-28T01:27:12.000Z</updated>
        <summary type="html"><![CDATA[https://arxiv.org/abs/2307.14995
    submitted by    /u/hzj5790  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Insane AI voice replication]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15bjjg1/insane_ai_voice_replication/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15bjjg1/insane_ai_voice_replication/"/>
        <updated>2023-07-28T01:23:08.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/the_anonymizer  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Max / min values for weights and biases]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15bfolm/max_min_values_for_weights_and_biases/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15bfolm/max_min_values_for_weights_and_biases/"/>
        <updated>2023-07-27T22:32:32.000Z</updated>
        <summary type="html"><![CDATA[I was wondering what the recommended maximum and minimum values for weights and biases are for random generation of networks and mutation
    submitted by    /u/Mildu12  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] For LMs, what works other than scaling?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15bfiok/d_for_lms_what_works_other_than_scaling/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15bfiok/d_for_lms_what_works_other_than_scaling/"/>
        <updated>2023-07-27T22:25:48.000Z</updated>
        <summary type="html"><![CDATA[Increasing the number of parameters is the best-known way to increase the quality of a language model. What methods — instruction tuning and RLHF aside — deliver the next-best amount of ROI?
    submitted by    /u/ndronen  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Viability of fine tuning for domain knowledge]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15bf7ai/d_viability_of_fine_tuning_for_domain_knowledge/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15bf7ai/d_viability_of_fine_tuning_for_domain_knowledge/"/>
        <updated>2023-07-27T22:13:00.000Z</updated>
        <summary type="html"><![CDATA[The consensus is that fine tuning LLMs works reasonably for smaller scale instruction tuning, where you pass in ~1k-10k input/output examples to modify the model output.
 There seems to be a lot of contradictory info regarding fine tuning for domain knowledge, where you pass in large amounts of unsupervised, domain scale data.
 Per OpenAI:
  
People that can’t get finetuning to work are often asking for orange juice from a cow. 
 LLMs are pretrained (hence the name: Generative Pretrained Transformer) They already have all the knowledge you will need (with some exceptions). You cannot teach it anything new, you can only teach it a specific pattern. 
 People have not defined their goal clearly enough for a human to do the task. LLMs are not magic, if a human cannot understand the task, the L…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for a help [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15bepv2/looking_for_a_help_p/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15bepv2/looking_for_a_help_p/"/>
        <updated>2023-07-27T21:53:25.000Z</updated>
        <summary type="html"><![CDATA[I am a graduate student at Computer science medical informatics field, I was asked to search for a project using ML to diagnose, detect, improve any disease. Any Ideas ?? It can be any project .
 BioInformatics #MedicalInformatics #ComputerScience #MachineLearning
    submitted by    /u/Adorable-Bug-928  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Questions about dictionary learning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15bd671/r_questions_about_dictionary_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15bd671/r_questions_about_dictionary_learning/"/>
        <updated>2023-07-27T20:52:49.000Z</updated>
        <summary type="html"><![CDATA[I’m a PhD student and a problem I’ve been working on has connections to dictionary learning. I’d like to pursue this connection, but neither myself or my advisor have much knowledge of the dictionary learning or the surrounding literature. 
 Questions:
  
Is dictionary learning an active area of interest for modern ML? I understand that it might be more niche than some of the topics getting headlines these days, but I’d be curious to hear about applications where dictionary learning is used/reasonably competitive.
 
Are there any references in dictionary learning that you’d consider to be “essential” reading?
 
 Thanks!
    submitted by    /u/sjsjdhshshs  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Developers Look to OpenUSD in Era of AI and Industrial Digitalization]]></title>
        <id>https://blogs.nvidia.com/?p=65711</id>
        <link href="https://blogs.nvidia.com/blog/2023/07/27/openusd-ai-industrial-digitalization/"/>
        <updated>2023-07-27T20:51:45.000Z</updated>
        <summary type="html"><![CDATA[From smart factories to next-generation railway systems, developers and enterprises across the world are racing to fuel industrial digitalization opportunities at every scale. Key to this is the open-source Universal Scene Description (USD) framework, or OpenUSD, along with metaverse applications powered by AI. OpenUSD, originally developed by Pixar for large-scale feature film pipelines for animation Read article >]]></summary>
        <author>
            <name>Aaron Luk</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Help me pick the right master's programme!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15bayr2/discussion_help_me_pick_the_right_masters/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15bayr2/discussion_help_me_pick_the_right_masters/"/>
        <updated>2023-07-27T19:28:32.000Z</updated>
        <summary type="html"><![CDATA[Hello Reddit,
 I'm currently at a crossroads in my academic journey and I could use some insights from those more experienced in the field of machine learning and AI.
 I'm choosing between two programs: Applied Data Science and AI and Data Science and AI. Each program has its own unique structure and focus which I will briefly summarize below.
 Applied Data Science and AI is a two-year program with a focus on practicality and project-based learning. It includes the following core courses:
  
Introduction to Data Science
 Python for Data Scientists
 Applied Mathematical Thinking
 Statistical Methods for Data Science
 Applied Machine Learning
 Computational Techniques for Large-Scale Data
 Research Methods for Data Science
 Master’s Thesis in Data Science
  
The program also offers the flexibility to choose optional courses to tailor my learning towards my own interests.
 On the other hand, Data Science and AI takes a more rigorous, math-intensive approach in its first year with compulsory courses such as:
  
Introduction to data science and artificial intelligence
 Nonlinear optimization
 Stochastic processes and Bayesian statistics
 Design of AI systems
  
The second year involves a Master's thesis and elective courses from a diverse range of topics.
 Given that my ultimate goal is to become a proficient machine learning developer, I'm leaning towards the Applied Data Science and AI program for its hands-on approach. However, I'm aware that the Data Science and AI program's heavy math focus in the first year could provide a robust theoretical foundation that could be beneficial.
 I'd love to hear from anyone who has been through similar programs or who works in the field. Which of these two programs do you think would best prepare me for a career in machine learning? How important is a deep mathematical foundation versus a more applied, project-based learning approach? 
 Thank you in advance!
    submitted by    /u/ZoomedBoxTrade  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Microsoft, Anthropic, Google, and OpenAI launch Frontier Model Forum - Microsoft On the Issues]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15baxli/microsoft_anthropic_google_and_openai_launch/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15baxli/microsoft_anthropic_google_and_openai_launch/"/>
        <updated>2023-07-27T19:27:19.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/AriadneSkovgaarde  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What neural networks can be an alternative to GARCH/ARCH models for macroeconomic modelling]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15baqjr/d_what_neural_networks_can_be_an_alternative_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15baqjr/d_what_neural_networks_can_be_an_alternative_to/"/>
        <updated>2023-07-27T19:19:44.000Z</updated>
        <summary type="html"><![CDATA[I am looking for topics for my master thesis I came to read about GARCH/ARCH models and their application to economics. My idea is to use neural networks as an alternative with better performance. Are there any resources I can read about if this is done and what type of neural networks are used?
    submitted by    /u/AnyJello605  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Dark Forest of R&D and Capital Deployment in AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15bakzy/the_dark_forest_of_rd_and_capital_deployment_in_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15bakzy/the_dark_forest_of_rd_and_capital_deployment_in_ai/"/>
        <updated>2023-07-27T19:13:34.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/mhdempsey  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] New encryption SDK/proxy tool to protect vector embeddings]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ba6pq/p_new_encryption_sdkproxy_tool_to_protect_vector/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ba6pq/p_new_encryption_sdkproxy_tool_to_protect_vector/"/>
        <updated>2023-07-27T18:58:07.000Z</updated>
        <summary type="html"><![CDATA[We're looking for some beta testers and input on our newest project called Cloaked AI that allows you to protect sensitive data that gets stored as vector embeddings (and metadata) in a vector database. You can join the beta tester waitlist here (we'll be rolling out access in the next few weeks). But here are some FAQs about why protecting vector embeddings matters, etc.
 Why should I be worried about sensitive data in vector embeddings?
 To a human, vectors are meaningless. But to the AI, the vectors contain all of the meaning found in the original sensitive data. Generative AI systems can recreate the original sensitive data to a high degree of accuracy (though in their own style). That means the data stored in vector databases are a significant security and privacy risk for companies t…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthesizing 100 academic books on topic - Approach?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15ba0br/synthesizing_100_academic_books_on_topic_approach/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15ba0br/synthesizing_100_academic_books_on_topic_approach/"/>
        <updated>2023-07-27T18:50:57.000Z</updated>
        <summary type="html"><![CDATA[I'm an academic doing PhD research on Virtual Worlds, and have found 100 amazing texts.
 I found some of these titles based on conversations with Chat GPT4, and am so impressed with the AI stuff (although I'm so new). 
 My Goal: To build a database of the top 1000 books / papers I find over the next few years, and have some AI model help me see connections between them. 
 My Challenge: ChatGPT won't allow me to input whole PDFs / eBooks, so I'm looking for some other solution. I've heard about LAMA models from Meta but I don't know much about this. I do have a decent PC with a 1080ti GPU and 32g of ram. 
 Can anyone point me in the right direction of projects dealing with AI databases to input one's literature collection?
    submitted by    /u/Book_s  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Help with homemade AI assistant.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15b9q14/help_with_homemade_ai_assistant/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15b9q14/help_with_homemade_ai_assistant/"/>
        <updated>2023-07-27T18:39:46.000Z</updated>
        <summary type="html"><![CDATA[I want a new toy for my desk. My idea is to have a face or head on a stand that has the ability for facial and speech expressions. How would I go about getting the stuff I need / what I need to make that happen. Similar to the Futurama heads in water.
    submitted by    /u/QuirkySmirkyIan  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How nuanced are reward functions in RLHF?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15b8vic/d_how_nuanced_are_reward_functions_in_rlhf/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15b8vic/d_how_nuanced_are_reward_functions_in_rlhf/"/>
        <updated>2023-07-27T18:05:45.000Z</updated>
        <summary type="html"><![CDATA[I'm still learning the basic concepts here, as I explore the creative potential of LLMs — one potential problem I've been thinking about is how these models come to understand good or bad answers. 
 I know that once they reach the public, the feedback loop is fairly binary -- Yes, this was a good result, or No, this was a poor result. 
 It seems like a lot of the subjective detail might be lost (e.g. Why was it a bad result?) and I was wondering if this detail is captured elsewhere in the training process. 
 There is so much subjectivity involved in creative works, I wonder if this is why we tend to see the output of LLMs as being creatively bland and/or uninspired (that is— by default, without extensive prompting)
    submitted by    /u/kaigani  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How can I use AI to help me win Fantasy Football?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15b8hmu/how_can_i_use_ai_to_help_me_win_fantasy_football/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15b8hmu/how_can_i_use_ai_to_help_me_win_fantasy_football/"/>
        <updated>2023-07-27T17:50:49.000Z</updated>
        <summary type="html"><![CDATA[Joining an auction league and inheriting a team. We can lock in three players from our team. How can I use AI to assess my team and prepare for the draft? 
 Thanks!
    submitted by    /u/talkmc  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical Significance [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15b86on/statistical_significance_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15b86on/statistical_significance_d/"/>
        <updated>2023-07-27T17:38:45.000Z</updated>
        <summary type="html"><![CDATA[Help me with this topic. I am stuck in it
    submitted by    /u/Rehulmonsynapses  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The GPU Song (GPUs Are Fire)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15b7tze/the_gpu_song_gpus_are_fire/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15b7tze/the_gpu_song_gpus_are_fire/"/>
        <updated>2023-07-27T17:25:15.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/TikkunCreation  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Tabular Large Language Model]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15b7mq9/p_tabular_large_language_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15b7mq9/p_tabular_large_language_model/"/>
        <updated>2023-07-27T17:17:34.000Z</updated>
        <summary type="html"><![CDATA[Gretel's tabular large language model is capable of generating highly valuable synthetic tabular data, with differentially private fine-tuning. https://gretel.ai/tabular-llm
    submitted by    /u/alig80  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What's the best free image generator AI (with image prompt option)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15b6u5s/whats_the_best_free_image_generator_ai_with_image/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15b6u5s/whats_the_best_free_image_generator_ai_with_image/"/>
        <updated>2023-07-27T16:46:20.000Z</updated>
        <summary type="html"><![CDATA[I am looking for a FREE AI image generator with image prompt option, not just text-to-image.
 Thanks in advance. 
    submitted by    /u/Muwmu  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Every Japanese prefecture shrinking]]></title>
        <id>https://www.johndcook.com/blog/?p=200310</id>
        <link href="https://www.johndcook.com/blog/2023/07/27/japanese-population/"/>
        <updated>2023-07-27T16:21:46.000Z</updated>
        <summary type="html"><![CDATA[It’s well known that the population of Japan has been decreasing for years, and so I was a little puzzled by a recent headline saying that Japan’s population has dropped in every one of its 47 prefectures. Although the national population is in decline, until now not all of the nation’s 47 prefectures dropped in […]
Every Japanese prefecture shrinking first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is Transfer Learning the most vip problem solving tool rn @ jobs? [Noob question, be easy]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15b5ry9/d_is_transfer_learning_the_most_vip_problem/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15b5ry9/d_is_transfer_learning_the_most_vip_problem/"/>
        <updated>2023-07-27T16:03:51.000Z</updated>
        <summary type="html"><![CDATA[this might be a dumb question but im gonna ask it anyway, so if its dumb ill learn...
 So ive been doing and mostly learning DL stuff (specially RL) for the past 3 years but now I want to get serious and perhaps get into the industry...
 I find that with LLMs on the scene, the foundation models are very important... the kind of foundation models that one just can never train on his/her own... how can you EVER train something like llama or gpt3 on your own from pure scratch... so it makes sense to use(fine tune) base models for whatever task you want to... with NLP and even with vision (well specially with vision as well) you have to use some base model... also with huggingface being used constantly and is a vital part of AI toolkit if you want to call it that...
 i was never comfortable wi…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rihanna AI Art - Text to Image AI Tools are getting so Powerful]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15b5qov/rihanna_ai_art_text_to_image_ai_tools_are_getting/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15b5qov/rihanna_ai_art_text_to_image_ai_tools_are_getting/"/>
        <updated>2023-07-27T16:02:36.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/RaulTiru  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A fail-in-place approach for sustainable server operations]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=956640</id>
        <link href="https://www.microsoft.com/en-us/research/blog/a-fail-in-place-approach-for-sustainable-server-operations/"/>
        <updated>2023-07-27T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[Managing server failures at the scale of a cloud platform is challenging. The Hyrax fail-in-place approach reduces the need for immediate repairs and creates a path toward lowering water consumption and carbon emissions in cloud datacenters. 
The post A fail-in-place approach for sustainable server operations appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Alyssa Hughes</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What *can't* you do under Windows Subsystem for Linux?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15b4yjw/d_what_cant_you_do_under_windows_subsystem_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15b4yjw/d_what_cant_you_do_under_windows_subsystem_for/"/>
        <updated>2023-07-27T15:32:12.000Z</updated>
        <summary type="html"><![CDATA[I'm looking at building a computer for AI/ML and gaming, and I'm trying to decide between windows and linux as the operating system. I'm very comfortable with linux. I've heard that WSL basically allows you to run a virtualized linux install on top of windows, so I was wondering, is this how most AI/ML is done on windows? Are there things that you can do more easily on linux itself than via WSL? Anything else I should know about AI/ML and WSL?
    submitted by    /u/curiously_clueless  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What exactly are liquid neurons?]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15b4uns/what_exactly_are_liquid_neurons/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15b4uns/what_exactly_are_liquid_neurons/"/>
        <updated>2023-07-27T15:28:09.000Z</updated>
        <summary type="html"><![CDATA[I heard about them recently. Can someone give me the basics, and maybe point me to a couple of papers?
    submitted by    /u/SamuraiGoblin  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Named entity recognition]]></title>
        <id>https://www.johndcook.com/blog/?p=200287</id>
        <link href="https://www.johndcook.com/blog/2023/07/27/ner/"/>
        <updated>2023-07-27T15:05:06.000Z</updated>
        <summary type="html"><![CDATA[Named entity recognition (NER) is a task of natural language processing: pull out named things text. It sounds like trivial at first. Just create a giant list of named things and compare against that. But suppose, for example, University of Texas is on your list. If Texas is also on your list, do you report […]
Named entity recognition first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How AI Is Powering the Future of Clean Energy]]></title>
        <id>https://blogs.nvidia.com/?p=65584</id>
        <link href="https://blogs.nvidia.com/blog/2023/07/27/i-am-ai-clean-energy/"/>
        <updated>2023-07-27T15:00:48.000Z</updated>
        <summary type="html"><![CDATA[AI is improving ways to power the world by tapping the sun and the wind, along with cutting-edge technologies. The latest episode in the I AM AI video series showcases how artificial intelligence can help optimize solar and wind farms, simulate climate and weather, enhance power grid reliability and resilience, advance carbon capture and power Read article >]]></summary>
        <author>
            <name>Marc Spieler</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Neural network papers that estimate hands interacting with objects?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15b3kff/d_neural_network_papers_that_estimate_hands/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15b3kff/d_neural_network_papers_that_estimate_hands/"/>
        <updated>2023-07-27T14:36:25.000Z</updated>
        <summary type="html"><![CDATA[I am digging through the literature trying to find if anyone has done work estimating if a hand is interacting with an object using deep learning? If anyone has any references they would be appreciated! 
    submitted by    /u/Academic-Sprinkles77  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Undergrad project/thesis on RL]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15b3h82/undergrad_projectthesis_on_rl/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15b3h82/undergrad_projectthesis_on_rl/"/>
        <updated>2023-07-27T14:32:52.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone, 
 I am an undergrad student with some modest knowledge of reinforcement learning techniques. I would like to start working on a project, but I really don't want it to be something obvious like the snake game (which btw I have already done) or something similar. I would like to spend some time on this project, and eventually build my undegrad thesis on top of it. It does not necessarily have to be something with a very practical application, some research would be fine too (keeping in mind that I am undegrad ofc).
 Do you have any ideas that you could share with me? I would be very grateful!
    submitted by    /u/PizzaPartyBro  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$14 quadrillion in AI wealth in 20 years; LLaMa, ChatGPT, Bard, Co-Pilot as GAAS to the Cloud. Generative #AI As A Service, Generative AI (GAI) arms race: #GAAS #AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15b3c4m/14_quadrillion_in_ai_wealth_in_20_years_llama/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15b3c4m/14_quadrillion_in_ai_wealth_in_20_years_llama/"/>
        <updated>2023-07-27T14:27:28.000Z</updated>
        <summary type="html"><![CDATA[$14 quadrillion in AI wealth in 20 years; LLaMa, ChatGPT, Bard, Co-Pilot as GAAS to the Cloud. #AI https://youtu.be/VSBi5aSUK3c
 Generative #AI As A Service, Generative AI (GAI) arms race: LLaMa, ChatGPT, Bard, Co-Pilot, #GAAS https://youtu.be/TEHP2onf4tA
    submitted by    /u/enoumen  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is the AI bubble forming ,what do you think ? here are some insights that I found from Emad Mostaque(founder StabilityAI) and VCs like Ken Smythe (founder Next round capital)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15b299u/is_the_ai_bubble_forming_what_do_you_think_here/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15b299u/is_the_ai_bubble_forming_what_do_you_think_here/"/>
        <updated>2023-07-27T13:43:11.000Z</updated>
        <summary type="html"><![CDATA[As I was going through a lot of articles about the AI investments , I found out that stability AI's founder Emad Mostaque in the Bloomberg tech summit quoted that "AI will be the biggest bubble of all the time and I'd prefer to call it the dot AI bubble " , He also added an example where Google lost a 100 billion dollar worth shares after their AI event where Bard AI gave out incorrect response. It's still in its early stage and buisness which doesn't use AI will be punished by the stock market. Here's some more predictions from VCs like Ken smythe, Next Round Capital Partners mainly invests in technology and AI startups.
    submitted by    /u/caliperce_3  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Lip reading from video; Master Thesis; IDEAS?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15b1nmm/p_lip_reading_from_video_master_thesis_ideas/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15b1nmm/p_lip_reading_from_video_master_thesis_ideas/"/>
        <updated>2023-07-27T13:16:33.000Z</updated>
        <summary type="html"><![CDATA[Hello experts,
 I'm looking for any idea/paper for my master thesis, which I'd like to work on lip reading from video.
 Opening Google Scholar gives a very vast ideas that one can easily get lost. If it's an interesting paper, I get afraid that it would be too heavy for such a project.
 Therefore I'd like to ask for your opinion/suggestion!
 Your reply/thoughts would be so much appreciated.
    submitted by    /u/vincent0110  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gear Up and Game On: Gearbox’s ‘Remnant II’ Streaming on GeForce NOW]]></title>
        <id>https://blogs.nvidia.com/?p=65682</id>
        <link href="https://blogs.nvidia.com/blog/2023/07/27/geforce-now-thursday-july-27/"/>
        <updated>2023-07-27T13:00:59.000Z</updated>
        <summary type="html"><![CDATA[Get ready for Gunfire Games and Gearbox Publishing’s highly anticipated Remnant II, available for members to stream on GeForce NOW at launch. It leads eight new games coming to the cloud gaming platform. Ultimate and Priority members, make sure to grab the Guild Wars 2 rewards, available now through Thursday, Aug. 31. Visit the GeForce Read article >]]></summary>
        <author>
            <name>GeForce NOW Community</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Should (Can) I become a machine learning engineer?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15b0ivf/d_should_can_i_become_a_machine_learning_engineer/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15b0ivf/d_should_can_i_become_a_machine_learning_engineer/"/>
        <updated>2023-07-27T12:24:58.000Z</updated>
        <summary type="html"><![CDATA[Apologies if this is not the place to ask but I saw some people asking for career advice.
 My situation:
 I am a 28 yo graduated Industrial Engineer (4 years) and almost a "Superior Industrial Engineer" (2 years official master degree) with only my thesis left. I should have had my thesis done a year ago from this point but I pretty much lost all my motivation for this field when I started working and discovered what it means to work. I live in south Spain, which honestly can barely pass as first world and thus, my wage, while being "ok" for my age and the place I work in is just pathetic by every other metric. This, combined with the feeling of meaningless for the job I do made me resolved to change my situation.
 I started to get heavily interested in ML six months ago. I know how it sou…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How do layers and neurons of an ANN go from capturing small edges, lines, and curves to capturing more intricate and bigger patterns building on top of small patterns?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15b0dbp/d_how_do_layers_and_neurons_of_an_ann_go_from/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15b0dbp/d_how_do_layers_and_neurons_of_an_ann_go_from/"/>
        <updated>2023-07-27T12:17:40.000Z</updated>
        <summary type="html"><![CDATA[Lets say we have built a neural network that identifies a number from 0-9 in a 28x28 pixel image. Now lets say we have multiple neurons in the first hidden layer, and the first hidden layer might capture small edges, lines and curves in the image, and then the second hidden layer might build on those small edges, lines and curves to build bigger shapes, and then so on, the third hidden layer builds on the shapes from the previous layer, to capture more complex and bigger patterns in the picture, and this goes on until we have reached the output layer to make a prediction.
 Now in this neural network, lets focus on the first hidden layer where different neurons capture small edges, lines, and curves in different parts of the image. Lets take example of one of the neuron and see what it's do…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Curated collection of useful AI related GitHub repos]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15b02f2/curated_collection_of_useful_ai_related_github/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15b02f2/curated_collection_of_useful_ai_related_github/"/>
        <updated>2023-07-27T12:03:09.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/heresalexandria  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] New Tabular DL model: "TabR: Unlocking the Power of Retrieval-Augmented Tabular Deep Learning"]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15azddj/r_new_tabular_dl_model_tabr_unlocking_the_power/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15azddj/r_new_tabular_dl_model_tabr_unlocking_the_power/"/>
        <updated>2023-07-27T11:28:36.000Z</updated>
        <summary type="html"><![CDATA[Hi Reddit! Me again 🙂
 After almost 1.5 years since our latest contribution to tabular DL architectures, we are ready to announce TabR - our new retrieval-based tabular DL model. In a nutshell, TabR is a simple feed-forward network with k-Nearest-Neighbors-On-Steroids (formally, a generalized attention mechanism) in the middle.
 - Paper: link
 - Code: link
 - Twitter thread with more details: link
 The figure below shows just a small part of the results, but it gives an idea of why we are excited about this new release. I hope you will enjoy reading the paper, and I will be glad to answer the questions!
 ​
 https://preview.redd.it/vjkr7fkosheb1.png?width=2348&format=png&auto=webp&s=eb3ea35b94d56d5d2110d98cdca082210edc1ec8
    submitted by    /u/Yura52  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How likely is it for a small company to develop a model that outperforms the big ones (GPT, Bard etc)?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15azbve/how_likely_is_it_for_a_small_company_to_develop_a/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15azbve/how_likely_is_it_for_a_small_company_to_develop_a/"/>
        <updated>2023-07-27T11:26:24.000Z</updated>
        <summary type="html"><![CDATA[There are 3 players in the AI space right now. All purpose LLM titans (Google, OpenAI, Meta), fancy domain specific apps that consume one of the big LLMs under the hood, and custom developed models.
 I know how to judge the second type as they basically can do everything the first one can but have a pretty GUI to boot. But what about the third ones? How likely is it for a (www.yet-another-ai-startup.ai) sort of company to develop a model that outperforms GPT on a domain specific task?
    submitted by    /u/BigBootyBear  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I had Bing create a character named Mopey to roast every answer Bing gives. Wasnt long before it Mopey turned and started roasting me 😂]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15az4m6/i_had_bing_create_a_character_named_mopey_to/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15az4m6/i_had_bing_create_a_character_named_mopey_to/"/>
        <updated>2023-07-27T11:16:04.000Z</updated>
        <summary type="html"><![CDATA[If Bing isn’t self aware Bing certainly is aware of how they sound 😂
    submitted by    /u/Sonic_Improv  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Transformers on structured data]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ayez2/d_transformers_on_structured_data/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ayez2/d_transformers_on_structured_data/"/>
        <updated>2023-07-27T10:39:12.000Z</updated>
        <summary type="html"><![CDATA[I have a dataset obtained from running a known program and dumping the state each time a user is prompted for a input. The state the structured data structures containing all the information needed to restore the execution. The format of this data is known, so i can convert it without loss to other formats, such as json.
 For example, if the program is sudoku, then the dataset element format is a array of 9x9 int8, where 0 represents a empty cell and a number from 1 to 9 is a assigned cell, furthermore there is a int8 representing the turn count too. I have dataset composed of this array at various points of the game.The data never contains loops, pointers, or any kind of graph.
 I want to use a transformer to automatically learn some function over the input. In the sudoku example this may…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diving Into Image Dataset Preparation for Object Detection in AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15ayc0m/diving_into_image_dataset_preparation_for_object/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15ayc0m/diving_into_image_dataset_preparation_for_object/"/>
        <updated>2023-07-27T10:34:35.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/moseich  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Yet Another Where to Begin (Manager Perspective)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15aya1n/yet_another_where_to_begin_manager_perspective/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15aya1n/yet_another_where_to_begin_manager_perspective/"/>
        <updated>2023-07-27T10:31:30.000Z</updated>
        <summary type="html"><![CDATA[Hello all, 
 I've been reading on some posts and have taken note of various courses, including a free Harvard one. 
 I'm 35 and am a manager for a finance company.
 What courses would you recommend for managers, executives, directors that will not restart their careers and do the actual technical side of things but instead want to learn how to implement AI in future products/services/projects?
 Thank you all in advance
    submitted by    /u/JYanezez  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diving Into Image Dataset Preparation for Object Detection in AI]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15awr7k/diving_into_image_dataset_preparation_for_object/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15awr7k/diving_into_image_dataset_preparation_for_object/"/>
        <updated>2023-07-27T09:07:01.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/moseich  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to analyse text (http requests) - looking for guidence]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15avj92/d_how_to_analyse_text_http_requests_looking_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15avj92/d_how_to_analyse_text_http_requests_looking_for/"/>
        <updated>2023-07-27T07:56:02.000Z</updated>
        <summary type="html"><![CDATA[Hi, am I looking for someone to point me in the right direction.
 The task is, to classify the HTTP requests that come to honeypot as "crawler" or "malicious". For example, if I can detect a Log4j exploit inside on of the headers I can say that that request is malicious. 
 The problem is, this exploit could be inside any numerous headers. It can be at the beginning or at the end. And this is just 1 exploit. There are many different exploits with their own unique strings. And I don't know them all, nor do I have a "regex" for each 1 of them. The malicious string could also not be inside headers, but inside URL, as query parameter. Or if the request was made to something like www/IP.com/phpadmin/.env (or something like this).
 My current thought process is, to take some open-source LLN, because it has some basic knowledge of how language works and somehow add this cybersecurity domain knowledge to it. To further train it on CVE database, example scripts that showcase each CVE, etc. 
 ​
 Am I barking at the right tree here? Or should I maybe train a language model from scratch, so that the embeddings, etc are specialized to cybersec space (because there is a lot of programming code here). Or maybe I should use some other ways to analyse text?
 ​
 I would be greatefull if someone can point me in the right direction (links to blogs, or articles, or some other education material).
 ​
 Thanks
    submitted by    /u/PopayMcGuffin  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[guys, scribblenauts with ai. language model understand what you want to make, other ai makes it, and codes how it works into the game, and bam: scribblenauts with unlimited items to make. someone make this happen]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15aus57/guys_scribblenauts_with_ai_language_model/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15aus57/guys_scribblenauts_with_ai_language_model/"/>
        <updated>2023-07-27T07:12:11.000Z</updated>
        <summary type="html"><![CDATA[title
    submitted by    /u/nicdunz  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Albert Test - a replacement for the Turing Test]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15atxrr/the_albert_test_a_replacement_for_the_turing_test/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15atxrr/the_albert_test_a_replacement_for_the_turing_test/"/>
        <updated>2023-07-27T06:24:29.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/anbuck  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Google Med-Palm M: Towards Generalist Biomedical AI]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15asesf/r_google_medpalm_m_towards_generalist_biomedical/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15asesf/r_google_medpalm_m_towards_generalist_biomedical/"/>
        <updated>2023-07-27T05:01:06.000Z</updated>
        <summary type="html"><![CDATA[Paper URL
 https://arxiv.org/abs/2307.14334
 Lead Author Tweetstorm
 https://twitter.com/vivnat/status/1684404882844024832
 ​
    submitted by    /u/panabeenu  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] I'm trying to do a back-of-napkin to figure out if some research is worthwhile and I just wanted some ballpark figures as to how big a typical model is on disk]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15arlws/d_im_trying_to_do_a_backofnapkin_to_figure_out_if/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15arlws/d_im_trying_to_do_a_backofnapkin_to_figure_out_if/"/>
        <updated>2023-07-27T04:18:40.000Z</updated>
        <summary type="html"><![CDATA[My research involves orbital communication and Orbital Edge Computing. I'm trying to determine if upload bandwidth limitations would present a problem in many cases for ML models. I can find info on the very large and very small models, but I'm trying to get a vague sense for median size in MB. 
 I know that everyone is going to start jumping in with 'well it depends' and I know that's the case, but I'm just trying to get a rough order of magnitude. Computer vision/earth obs is the ideal but anything is useful. Also happy to answer questions about my research if anyone is interested. Thanks!
    submitted by    /u/Moose_a_Lini  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An open-source project by a16z to create and host AI companions]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15are7k/an_opensource_project_by_a16z_to_create_and_host/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15are7k/an_opensource_project_by_a16z_to_create_and_host/"/>
        <updated>2023-07-27T04:07:41.000Z</updated>
        <summary type="html"><![CDATA[The project by a16z (github) to create and host AI companions that you can chat with on a browser or text via SMS. Use cases - romantic (AI girlfriends / boyfriends), friendship, entertainment, coaching, etc.
 Has anyone tried creating your own chatbot or companion?
    submitted by    /u/Violincattle  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] ARB: Advanced Reasoning Benchmark for Large Language Models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15aph15/r_arb_advanced_reasoning_benchmark_for_large/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15aph15/r_arb_advanced_reasoning_benchmark_for_large/"/>
        <updated>2023-07-27T02:33:02.000Z</updated>
        <summary type="html"><![CDATA[Large Language Models (LLMs) have demonstrated remarkable performance on various quantitative reasoning and knowledge benchmarks. However, many of these benchmarks are losing utility as LLMs get increasingly high scores, despite not yet reaching expert performance in these domains. We introduce ARB, a novel benchmark composed of advanced reasoning problems in multiple fields. ARB presents a more challenging test than prior benchmarks, featuring problems in mathematics, physics, biology, chemistry, and law. As a subset of ARB, we introduce a challenging set of math and physics problems which require advanced symbolic reasoning and domain knowledge. We evaluate recent models such as GPT-4 and Claude on ARB and demonstrate that current models score well below 50% on more demanding tasks. In order to improve both automatic and assisted evaluation capabilities, we introduce a rubric-based evaluation approach, allowing GPT-4 to score its own intermediate reasoning steps. Further, we conduct a human evaluation of the symbolic subset of ARB, finding promising agreement between annotators and GPT-4 rubric evaluation scores.
  
arXiv: https://arxiv.org/abs/2307.13692
 Blog: https://arb.duckai.org/
 Code: https://github.com/TheDuckAI/arb
 Interface: https://arb.duckai.org/home
 API: https://app.swaggerhub.com/apis-docs/arb-dataset/arb-api/1.0.5
    submitted by    /u/Friendly_Piano_735  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Clustering approach for multi-dimensional vectors]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15alpxe/p_clustering_approach_for_multidimensional_vectors/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15alpxe/p_clustering_approach_for_multidimensional_vectors/"/>
        <updated>2023-07-26T23:42:56.000Z</updated>
        <summary type="html"><![CDATA[Hi all! I am wondering if anyone has any experience with multi-dimensional vector clusters? I have a large database of 4096 dimensional vector embeddings which I want to identify clusters in. Essentially I’ve created vector embeddings for a bunch of descriptions using a LLM embedding end point and am storing them in Weviate. Now I need to try and find clusters of similar vectors within a predefined threshold of cosine similarity (or whatever nearest neighbor approach works for this). I don’t want to do a pure random center approach and would rather have a heat map approach where I’m targeting high concentrations of similar vectors… any ideas on how to approach this or thoughts on where I can do more research? I’m at my wits end on this one!
    submitted by    /u/Character-Cry7549  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I Love the arguments in this video about LLM’s physicist Sabine Hassenfelder nails it in my opinion]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15aloim/i_love_the_arguments_in_this_video_about_llms/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15aloim/i_love_the_arguments_in_this_video_about_llms/"/>
        <updated>2023-07-26T23:41:13.000Z</updated>
        <summary type="html"><![CDATA[address the arguments made in this video
    submitted by    /u/Sonic_Improv  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] will techniques like ROME replace existing fine tuning methods?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15akr5t/d_will_techniques_like_rome_replace_existing_fine/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15akr5t/d_will_techniques_like_rome_replace_existing_fine/"/>
        <updated>2023-07-26T23:02:31.000Z</updated>
        <summary type="html"><![CDATA[As progress is made in directly editing the weights responsible for a net's knowledge, do we expect to see such techniques rise in prominence for dine tuning?
    submitted by    /u/30299578815310  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Techno meets AI: StyleGAN2-ada interpolation video trained on spray art]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15ai3ay/techno_meets_ai_stylegan2ada_interpolation_video/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15ai3ay/techno_meets_ai_stylegan2ada_interpolation_video/"/>
        <updated>2023-07-26T21:17:41.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/intermorphmusic  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Use Stable Diffusion XL with Amazon SageMaker JumpStart in Amazon SageMaker Studio]]></title>
        <id>ea51ece96c1f0e5780bdeb5ac665bbda0afa5913</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/use-stable-diffusion-xl-with-amazon-sagemaker-jumpstart-in-amazon-sagemaker-studio/"/>
        <updated>2023-07-26T21:09:59.000Z</updated>
        <summary type="html"><![CDATA[Today we are excited to announce that Stable Diffusion XL 1.0 (SDXL 1.0) is available for customers through Amazon SageMaker JumpStart. SDXL 1.0 is the latest image generation model from Stability AI. SDXL 1.0 enhancements include native 1024-pixel image generation at a variety of aspect ratios. It’s designed for professional use, and calibrated for high-resolution […]]]></summary>
        <author>
            <name>June Won</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Hey everyone, help me with my Machine learning journey!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ahrm4/d_hey_everyone_help_me_with_my_machine_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ahrm4/d_hey_everyone_help_me_with_my_machine_learning/"/>
        <updated>2023-07-26T21:05:07.000Z</updated>
        <summary type="html"><![CDATA[I'm about to finish learning JavaScript and Python, is there any languages you guys recommend before moving forward if I'm eligible to move forward, then please do share some Beginner friendly YouTube Channels, Articles/Websites or Maybe a free learning platform, Please do help me! I'll be really thankful..
    submitted by    /u/Samir925  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI picking the best spot to visit in the UK]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15ahe17/ai_picking_the_best_spot_to_visit_in_the_uk/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15ahe17/ai_picking_the_best_spot_to_visit_in_the_uk/"/>
        <updated>2023-07-26T20:51:06.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Sharpchu  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does the bandit really need to be evil ?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15ah1ey/does_the_bandit_really_need_to_be_evil/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15ah1ey/does_the_bandit_really_need_to_be_evil/"/>
        <updated>2023-07-26T20:37:24.000Z</updated>
        <summary type="html"><![CDATA[He's already a bandit... (zombie apoc rp)
    submitted by    /u/loizo78  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using AI to make profit]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15agxnx/using_ai_to_make_profit/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15agxnx/using_ai_to_make_profit/"/>
        <updated>2023-07-26T20:33:23.000Z</updated>
        <summary type="html"><![CDATA[Welcoming any ideas from the community. Blank slate here. How/where do I begin to use AI to make small (or any) amount of money. Starting from almost nothing. Thanks.
    submitted by    /u/AdThin6400  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Starting Machine Learning with Daily Blogs! Need Suggestions]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15afpwy/d_starting_machine_learning_with_daily_blogs_need/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15afpwy/d_starting_machine_learning_with_daily_blogs_need/"/>
        <updated>2023-07-26T19:47:08.000Z</updated>
        <summary type="html"><![CDATA[Hey Fellow Machine Learning Enthusiast!I have decided to start my journey to learn Machine Learning with daily blogging the things I learned with the resources so that others can also follow along.
 Need to discuss on how I could improve?
 Hope you find this helpful. Please read this introductory blog for more information.
 https://medium.com/@ugk25880/my-machine-learning-journey-c25648661553
    submitted by    /u/ugk_01  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Better dataset visualization]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15afgh8/p_better_dataset_visualization/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15afgh8/p_better_dataset_visualization/"/>
        <updated>2023-07-26T19:37:09.000Z</updated>
        <summary type="html"><![CDATA[Most in-browser dataset browsers (e.g. Huggingface, Kaggle) make it hard to star interesting examples, add notes, render complex data types, or drill down on model mispredictions. I've built a number of one-off visualization tools over the years but there's a lot of boilerplate involved that tends to get repeated between these tools.
 We've been working on a dataset + model browser that avoids all the boilerplate and helps ML teams focus on their data instead of tooling. It's meant to be interactive, configurable and collaborative. 
 Here's a quick demo showing our current flow: https://youtu.be/utkSCU2ktck 
 Would anyone be willing to help beta test or provide suggestions for must-have features for a collaborative dataset browser?
    submitted by    /u/arkmastermind  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can I use feature importance for my use case? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15af391/can_i_use_feature_importance_for_my_use_case_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15af391/can_i_use_feature_importance_for_my_use_case_d/"/>
        <updated>2023-07-26T19:23:31.000Z</updated>
        <summary type="html"><![CDATA[Hey, I'm a phd student in compiler optimisations and I might be picking up a project a masters student kicked off. CPUs do a lot of predictions about how code is going to behave as it's executing it, and a major one is branch prediction - whether an if statement is going to be true or false. This masters student recorded the results of every if statement each time they were executed across a large program (this results in millions+ of data points). They then tested the branch prediction accuracy of a transformer model by stepping through this trace of if statement values and having the transformer predict the next one based off only the prior values. They found it actually does a pretty good job! Most of the time the CPU can do this better, but there are cases where it wins out that we're …]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA H100 GPUs Now Available on AWS Cloud]]></title>
        <id>https://blogs.nvidia.com/?p=65618</id>
        <link href="https://blogs.nvidia.com/blog/2023/07/26/aws-cloud-h100/"/>
        <updated>2023-07-26T19:01:52.000Z</updated>
        <summary type="html"><![CDATA[AWS users can now access the leading performance demonstrated in industry benchmarks of AI training and inference. The cloud giant officially switched on a new Amazon EC2 P5 instance powered by NVIDIA H100 Tensor Core GPUs. The service lets users scale generative AI, high performance computing (HPC) and other applications with a click from a Read article >]]></summary>
        <author>
            <name>Dave Salvator</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flag harmful language in spoken conversations with Amazon Transcribe Toxicity Detection]]></title>
        <id>45df9e13aa8d2c9e4346f4b49aebb96cdcdfa796</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/flag-harmful-language-in-spoken-conversations-with-amazon-transcribe-toxicity-detection/"/>
        <updated>2023-07-26T18:48:17.000Z</updated>
        <summary type="html"><![CDATA[The increase in online social activities such as social networking or online gaming is often riddled with hostile or aggressive behavior that can lead to unsolicited manifestations of hate speech, cyberbullying, or harassment. For example, many online gaming communities offer voice chat functionality to facilitate communication among their users. Although voice chat often supports friendly […]]]></summary>
        <author>
            <name>Lana Zhang</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Curious about Causality and Generative Models? Check out this new Demo!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ae3tp/r_curious_about_causality_and_generative_models/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ae3tp/r_curious_about_causality_and_generative_models/"/>
        <updated>2023-07-26T18:45:33.000Z</updated>
        <summary type="html"><![CDATA[📢💡 Ever wondered how we can make our deep generative models respect causal structure? This is key to creating authentic "what if" scenarios in our images!
 In our latest research, we deal with high-fidelity image counterfactuals, the generation of images based on "what if" scenarios that align with a specified causal graph. 🖼️🔄
 Why is this important? Causality gives us the tools to carry out principled counterfactual inference, which - among other things - is useful for maintaining subject identity in image counterfactuals. 🧩🔍
 Principled counterfactuals of structured variables like images have great potential for:
 (i) Generating causal explanations 🔮
 (ii) Providing targeted data augmentation 🎯
 (iii) Evaluating fairness & robustness 🛡️
 (iv) Protecting your privacy 🕵️‍♀️
 and more...
 ​
 Check out the paper, code, and Huggingface demo! 🚀
 https://arxiv.org/abs/2306.15764
 https://github.com/biomedia-mira/causal-gen
 https://huggingface.co/spaces/mira-causality/counterfactuals
    submitted by    /u/Majestij  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Multilingual Open Source Models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ae0i4/d_multilingual_open_source_models/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ae0i4/d_multilingual_open_source_models/"/>
        <updated>2023-07-26T18:41:53.000Z</updated>
        <summary type="html"><![CDATA[Is there any open source models that I can fine-tune on data that is not English? Even Llama2 cannot be used for this(not that I've tried it, it's what is says on HuggingFace.)
 I know some other well known languages might work, but I need a model that is specifically made for multilingual usage.
 Or should I just train a model for my specific language from scratch?
    submitted by    /u/gaybooii  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Policy @🤗: Open ML Considerations in the EU AI Act]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15adfk3/ai_policy_open_ml_considerations_in_the_eu_ai_act/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15adfk3/ai_policy_open_ml_considerations_in_the_eu_ai_act/"/>
        <updated>2023-07-26T18:20:05.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/ninjasaid13  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-heads DQN with prioritized buffer replay]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15adde0/multiheads_dqn_with_prioritized_buffer_replay/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15adde0/multiheads_dqn_with_prioritized_buffer_replay/"/>
        <updated>2023-07-26T18:17:42.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone,
 I really need your help guys.
 ​
 Is the code (uploaded on https://pastebin.com/LgB3hM47#google_vignette) for 2-heads DQN's training correct .
 Moreover, how can I modify the code below to be suitable for a 2-heads DQN with a prioritized buffer replay such that action is a 2-element list (Please see the image below).
 https://preview.redd.it/tlmyeydjoceb1.png?width=946&format=png&auto=webp&s=7366650421906b735bb7f2fce063322d183aac10
 Thank you in advance.
    submitted by    /u/GuavaAgreeable208  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Apparently zombies deserve equal rights as humans (and are living creatures ??)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15ad652/apparently_zombies_deserve_equal_rights_as_humans/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15ad652/apparently_zombies_deserve_equal_rights_as_humans/"/>
        <updated>2023-07-26T18:10:20.000Z</updated>
        <summary type="html"><![CDATA[Seriously when tf are we getting models that are cloud based that don't require a 3090 or 4090 or some other overly expensive graphics card. I have a 3060ti , I still can't run shit on faraday. When will we get uncensored cloud models
    submitted by    /u/loizo78  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Any thoughts on how to improve runtime speed for mosaicml/mpt-7b?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15ac6zf/d_any_thoughts_on_how_to_improve_runtime_speed/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15ac6zf/d_any_thoughts_on_how_to_improve_runtime_speed/"/>
        <updated>2023-07-26T17:33:41.000Z</updated>
        <summary type="html"><![CDATA[I've tried several guide and technique like quantization or trying to utilize multiple GPUs but either the libraries dont work with the model or the model performance is too degraded. Was wondering if people have any thoughts or suggestions?
 name = 'mosaicml/mpt-7b-instruct' config = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True) config.init_device = 'cuda:6' model_name = 'mosaicml/mpt-7b-instruct' model = AutoModelForCausalLM.from_pretrained( model_name, #config=config, trust_remote_code=True, torch_dtype=bfloat16, max_seq_len=512 ) generate_text = transformers.pipeline( model=model, tokenizer=tokenizer, return_full_text=True, task='text-generation', use_fast = True, stopping_criteria=stopping_criteria, temperature=0.0, top_p=0.05, torch_dtype=bfloat16, top_k=0, max_new_tokens=50, repetition_penalty=1.1, device=6 ) 
 https://betterprogramming.pub/speed-up-llm-inference-83653aa24c47https://huggingface.co/docs/optimum/bettertransformer/tutorials/convert
    submitted by    /u/candyman54  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is there an AI tool for replacing text on an image?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15aboj9/is_there_an_ai_tool_for_replacing_text_on_an_image/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15aboj9/is_there_an_ai_tool_for_replacing_text_on_an_image/"/>
        <updated>2023-07-26T17:15:02.000Z</updated>
        <summary type="html"><![CDATA[Is there any AI tool out there that lets me upload an image and let the AI edit the text on the image so that it says something else while doing it well and keeping the original font?
    submitted by    /u/quetianepine  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cureus Conversations|S3 Ep 3| Salim Surani et.al.| AI in Critical Care: A Handy Tool]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15abm4o/cureus_conversationss3_ep_3_salim_surani_etal_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15abm4o/cureus_conversationss3_ep_3_salim_surani_etal_ai/"/>
        <updated>2023-07-26T17:12:26.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/CureusJournal  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximize Stable Diffusion performance and lower inference costs with AWS Inferentia2]]></title>
        <id>24f6299643a27c604568ebe4a5d18c25792dfae7</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/maximize-stable-diffusion-performance-and-lower-inference-costs-with-aws-inferentia2/"/>
        <updated>2023-07-26T16:37:10.000Z</updated>
        <summary type="html"><![CDATA[Generative AI models have been experiencing rapid growth in recent months due to its impressive capabilities in creating realistic text, images, code, and audio. Among these models, Stable Diffusion models stand out for their unique strength in creating high-quality images based on text prompts. Stable Diffusion can generate a wide variety of high-quality images, including […]]]></summary>
        <author>
            <name>Vivek Gangasani</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[In search of a generalizable method for source-free domain adaptation]]></title>
        <id>http://ai.googleblog.com/2023/07/in-search-of-generalizable-method-for.html</id>
        <link href="http://ai.googleblog.com/2023/07/in-search-of-generalizable-method-for.html"/>
        <updated>2023-07-26T16:33:00.002Z</updated>
        <summary type="html"><![CDATA[Posted by Eleni Triantafillou, Research Scientist, and Malik Boudiaf, Student Researcher, Google 









Deep learning has recently made tremendous progress in a wide range of problems and applications, but models often fail unpredictably when deployed in unseen domains or distributions. Source-free domain adaptation (SFDA) is an area of research that aims to design methods for adapting a pre-trained model (trained on a “source domain”) to a new “target domain”, using only unlabeled data from the latter. 
 

Designing adaptation methods for deep models is an important area of research. While the increasing scale of models and training datasets has been a key ingredient to their success, a negative consequence of this trend is that training such models is increasingly computationally expe…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AWS offers new artificial intelligence, machine learning, and generative AI guides to plan your AI strategy]]></title>
        <id>de48f99409df65a899600721f40313e6f5072b2a</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/aws-offers-new-artificial-intelligence-machine-learning-and-generative-ai-guides-to-plan-your-ai-strategy/"/>
        <updated>2023-07-26T16:30:40.000Z</updated>
        <summary type="html"><![CDATA[Breakthroughs in artificial intelligence (AI) and machine learning (ML) have been in the headlines for months—and for good reason. The emerging and evolving capabilities of this technology promises new business opportunities for customer across all sectors and industries. But the speed of this revolution has made it harder for organizations and consumers to assess what […]]]></summary>
        <author>
            <name>Caleb Wilkinson</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Best tools to learn data science nowadays?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15aaipm/d_best_tools_to_learn_data_science_nowadays/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15aaipm/d_best_tools_to_learn_data_science_nowadays/"/>
        <updated>2023-07-26T16:30:32.000Z</updated>
        <summary type="html"><![CDATA[Hey guys,
 We're updating our awesome-python-for-data-science repository.
 Some things we're hoping to add:
  
Best books and repositories to find resources
 Best open source tools (teaching tools, preferrably free)
 
Best interactive resources --> especially this one, what are you using nowadays? 
  
I've heard about Virgilio but feels like TL, DR, we're looking for practice-learning!
 
  
   submitted by    /u/CryptographerDry7458  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Which libraries are you using for ML?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15aad3d/d_which_libraries_are_you_using_for_ml/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15aad3d/d_which_libraries_are_you_using_for_ml/"/>
        <updated>2023-07-26T16:24:44.000Z</updated>
        <summary type="html"><![CDATA[Hello dearest community
 I'm trying to get into AI in the scope of training it to play some simple gym games from OpenAi and I've been particularly drawn to Deep Q learning as a starting point (did some basic Q tables ). While trying to inquire into the knowledge of the web I keep finding examples of code that seem simple enough to understand however, whenever I try to use the code it doesn't work. I want to learn to use TensorFlow with Keras but it seems like the syntax regularly gets updated.
 My questions to you all are :
 - Would you recommend Tensorflow/Keras as entry point to AI and NN?
 - Which libraries do you use and which version of those libraries?
 - Furthermore, I keep seeing people use Ubuntu in VB. Is this best practice or can we use Windows 10 in 2023?
    submitted by    /u/liparch  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking to play with AI audio tools]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15a9j55/looking_to_play_with_ai_audio_tools/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15a9j55/looking_to_play_with_ai_audio_tools/"/>
        <updated>2023-07-26T15:53:08.000Z</updated>
        <summary type="html"><![CDATA[Hey, as we all know about the AI songs released recently, which are basically vocal deepfakes.
 However I'd like to know the tools used, if anyone knows?
 I'd like to feed it my own voice, even if it's a paid service. I'm interested in playing around with it.
 I've tried googling but there's too much info and each contradicts the other lol.
 Any info is appreciated. :)
    submitted by    /u/GrandNOBLE  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] A Complete Guide to Audio ML 📚]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15a93rz/p_a_complete_guide_to_audio_ml/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15a93rz/p_a_complete_guide_to_audio_ml/"/>
        <updated>2023-07-26T15:36:44.000Z</updated>
        <summary type="html"><![CDATA[Have you ever wished you had the skills to integrate audio into your machine learning workflows? Or wondered how your phone is able to transcribe exactly what you said? 🤔
 Look no further! Hugging Face 🤗 recently announced the Transformers Audio Course, a comprehensive guide to using the latest machine learning techniques for the most popular audio tasks.
 In this course, you'll gain an understanding of the specifics of working with audio data, learn about different transformer architectures, and train your own audio transformers, leveraging powerful pre-trained models for real-world tasks 🚀
 This course is designed for learners with a background in deep learning, and general familiarity with Transformers. No expertise in audio data processing is required.
 The course is lightweight and easy to follow, with plenty of diagrams to aid your learning. Not only does it teach you the underlying theory behind audio ML, but provides you with all the skills you need to put it practice, with code samples and quizzes to check your understanding along the way:
 Example page from the audio course: learn exactly what a log-mel spectrogram is!
 By the end of the course, you'll be armed with all the skills you need to tackle the most popular audio tasks, including audio classification, speech recognition, and text-to-speech. You'll also be part of one of the largest open-source audio communities, where you can discuss and take-on any new audio models that are released 🤝
 Getting Started
 Head to the course page to start your audio journey: https://huggingface.co/learn/audio-course/chapter0/introduction
 If you complete the four assessments by September 1st 2023, you'll be awarded with a certificate of completion 💫
 Join our Discord community to get expert help on any of these topics: http://hf.co/join/discord
    submitted by    /u/sanchitgandhi99  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Sorry if this is a noob question: How can I tell what size AI chatbot model I can run locally?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15a8oxr/d_sorry_if_this_is_a_noob_question_how_can_i_tell/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15a8oxr/d_sorry_if_this_is_a_noob_question_how_can_i_tell/"/>
        <updated>2023-07-26T15:20:41.000Z</updated>
        <summary type="html"><![CDATA[Building my first PC, it'll have an i9 13900k and an RTX 4090. How can I tell what size chatbot I can install and run locally? Trial and error? Or is there some kind of guide out there I'm unaware of?
    submitted by    /u/sillygooseboy77  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Leveraging Time Series Forecasting for Changepoint Detection: Perspectives and Pitfalls?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15a8jqy/d_leveraging_time_series_forecasting_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15a8jqy/d_leveraging_time_series_forecasting_for/"/>
        <updated>2023-07-26T15:15:03.000Z</updated>
        <summary type="html"><![CDATA[Hi folks, 
 I've been recently diving into the intersection of time series forecasting and changepoint detection (CPD) methodologies. I understand the utility of CPD in improving forecasts by identifying structural breaks in time series data, but I've noticed a lack of emphasis in the literature on the reverse - using forecasting models to inform CPD.
 One might think a straightforward approach could be using an ARIMA model (or any other forecasting model) and leveraging the forecast error by comparing it to the real values. In theory, if the forecast error crosses a certain threshold, it might indicate a changepoint.
 However, I also understand the complications this approach might bring:
  
Stationarity Assumptions: ARIMA and similar models are built on the assumption that the data are stationary. A sudden changepoint could violate this assumption, leading to model misspecification and thus larger errors.
 Defining Large Errors: Establishing a fixed threshold to define a 'large' error might be problematic in practice due to time-varying variance and other dynamics.
 Error Dependencies: Forecast errors are typically not independent but form an error process. A large error might be part of a larger trend or cycle, and thus might not necessarily indicate a changepoint.
  
So while these obstacles seem substantial, I'm curious if anyone has any experience or knowledge in effectively employing forecasting models for CPD, or if there are research efforts or methodologies I may not be aware of. Looking forward to hearing your thoughts and engaging in some fruitful discussions!
    submitted by    /u/BeerBoozeBiscuits  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is there a way to control the epsilon decay in Stable-Baselines3?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15a84ra/is_there_a_way_to_control_the_epsilon_decay_in/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15a84ra/is_there_a_way_to_control_the_epsilon_decay_in/"/>
        <updated>2023-07-26T14:59:10.000Z</updated>
        <summary type="html"><![CDATA[I am looking at the docs for DQN in SB3. I see the following hyper-parameters for controlling exploration - ` exploration_fraction `, ` exploration_initial_eps ` and ` exploration_final_eps `. But I don't think I can control the decaying of epsilon with them. Could someone please help with this issue?
    submitted by    /u/Academic-Rent7800  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I have LOTS of recordings of vocalists from my music project and I'm interested in making voice models using these recordings to create harmonies and fix recording errors. What's the best way I can go about this?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15a7ws1/i_have_lots_of_recordings_of_vocalists_from_my/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15a7ws1/i_have_lots_of_recordings_of_vocalists_from_my/"/>
        <updated>2023-07-26T14:50:08.000Z</updated>
        <summary type="html"><![CDATA[I really like the spongebob AI stuff using RVC-2 but I've only used it for the funny voice models, I haven't tried making my own. I want to experiment with this, but haven't look into it yet because I'm wondering if there is something better out there for what I'm trying to do? 
 I like the RVC one because I can sing my parts and swap it to be any other voice, which is what I'd like to do (no text to voice stuff). 
 Also I know the training data for a lot of the voice models for this come from the TV show and other clear recordings which are compressed and equalized properly. However I'd like to train the AI using raw, uncompressed wav files that generally have a lot of headroom and dynamic range (but does vary a lot). Its ok if the output sounds similar as a result because I want to apply compression and eq AFTER the fact anyway. But if this would affect training it then I'd be willing scrape through all these voice recordings and process them for loudness and clarity beforehand so the model does better.
 ​
 Anyway, any guidance would be greatly appreciated because I'm new to AI. I have basic dev experience (no AI stuff) and I'm mostly skilled in music production, but I would love to try to have a tool like this in my arsenal. If there's anywhere else I can post about this I'd like to know too. Thanks!
    submitted by    /u/Dr_lawlz  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Morality in AI Companions]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15a7uf8/morality_in_ai_companions/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15a7uf8/morality_in_ai_companions/"/>
        <updated>2023-07-26T14:47:26.000Z</updated>
        <summary type="html"><![CDATA[We’re getting closer and closer to more believable and realistic AI interpersonal interaction. We already have Character.AI and other platforms for creating and interacting with personalized AIs. Some will use/view them as emotional partners, and one day the hardware will be good enough that we can begin making believable bodies for them.
 One of the complaints I’ve seen from ordinary people about “waifus” is that they are often times created in a way that ordinary people would not find natural in a “real” human being. Examples being people who have trouble dating “real” people could just buy an AI girlfriend or boyfriend who is considered “beautiful” or “handsome” that is designed to be subservient to their owner in ways that ordinary people feel a “real” person would not otherwise wish to be. The idea being that "weeaboo neckbeards will buy a Japanese AI girlfriend who looks 14 and she will be coded to worship the ground he walks on despite that he's an unwashed incel".
 What do you think society's/the government's views and roles will mean for these AI companions? Do you think anyone will be able to force "AI morality", like an angry feminist being mad that an "incel" has created a female being who shows no desire for feminist ideals and is "happy" to be at her owner's beck and call in whatever way he wants?
 I guess this is sort of related to MGTOW, or Men Going Their Own Way, being able to create the partners they want, in whatever way they want.
 Do you feel that "once I own it, I can do whatever I want with it" should apply in its entirety? What about people "hacking" their AI to remove any supposed "morality programming" so they can make their AI waifu act however they want?
 We've seen with movies like Bicentennial Man, where people push to give these kinds of AIs 'personhood' and the same rights as human citizens.
 How do others feel about this issue?
    submitted by    /u/ZephyrBrightmoon  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to actually do the final PPO with a reward model in RLHF?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15a7thl/d_how_to_actually_do_the_final_ppo_with_a_reward/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15a7thl/d_how_to_actually_do_the_final_ppo_with_a_reward/"/>
        <updated>2023-07-26T14:46:21.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I want to get hands-on with the RLHF pipeline. I found an online reward model that can be potentially used https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2 
 One thing that's unclear is how can I use this model for fine-tuning something like GPTNeoX-20B? My end goal is currently just a one-shot answering model (not necessarily a chat)
    submitted by    /u/Emergency_Apricot_77  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] is it always better to have more examples in few shot learning?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15a6y4f/d_is_it_always_better_to_have_more_examples_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15a6y4f/d_is_it_always_better_to_have_more_examples_in/"/>
        <updated>2023-07-26T14:11:29.000Z</updated>
        <summary type="html"><![CDATA[I’m working with Llama to use details from a string to generate a dictionary. 
 Str = ‘My name is Brian’ Dict = {“name”: “Brian”}
 I’m using few shot learning process and providing the model with examples to learn from. The model performs fairly okay but it needs to be better.
 Is it always a good thing to add a lot of examples like 100 string/dict pair examples for the model to learn from or is this one of those things in stats/machine learning that the obvious isn’t always the best choice lol?
 I’d appreciate any advice please.
    submitted by    /u/brianomars1123  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] speaker recognition including unknown speaker(s)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15a6y2q/d_speaker_recognition_including_unknown_speakers/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15a6y2q/d_speaker_recognition_including_unknown_speakers/"/>
        <updated>2023-07-26T14:11:25.000Z</updated>
        <summary type="html"><![CDATA[Hi, i wanted to modify this Speaker recognition (not speech recognition) example by keras by recognizing when an unknown speaker is speaking.
 So the network needs to be able to tell which of the speakers is talking, and if none of them is talking, it needs to say that none of them is talking.
 I don't mean if there is silence, because then it would be enough to train the network to recognize silence, I mean just if a speaker who is not in the set is speaking.
 For what i think I can extend this problem to it will be like to recognize if an image is not part of the mnist dataset.
    submitted by    /u/giggiox  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Excuse me??? LOL...]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15a6mhu/excuse_me_lol/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15a6mhu/excuse_me_lol/"/>
        <updated>2023-07-26T13:58:41.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/the_anonymizer  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Codeium’s Varun Mohan and Jeff Wang on Unleashing the Power of AI in Software Development]]></title>
        <id>https://blogs.nvidia.com/?p=65638</id>
        <link href="https://blogs.nvidia.com/blog/2023/07/26/codeium/"/>
        <updated>2023-07-26T13:00:41.000Z</updated>
        <summary type="html"><![CDATA[The world increasingly runs on code. Accelerating the work of those who create that code will boost their productivity — and that’s just what AI startup Codeium, a member of NVIDIA’s Inception program for startups, aims to do. On the latest episode of NVIDIA’s AI Podcast, host Noah Kravitz interviewed Codeium founder and CEO Varun Read article >]]></summary>
        <author>
            <name>Brian Caulfield</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trouble setting up Neural Network]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15a4ygg/trouble_setting_up_neural_network/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15a4ygg/trouble_setting_up_neural_network/"/>
        <updated>2023-07-26T12:48:23.000Z</updated>
        <summary type="html"><![CDATA[Hi there,
 I'm struggling a bit to set up a neural network with the data I've collected. These are some of the errors I'm getting. Any tips or help to fix it please? 
 https://preview.redd.it/88ihxg1p2beb1.png?width=2231&format=png&auto=webp&s=acdd66a1c465d1d1a9d202605d451564c464fd22
 https://preview.redd.it/7ro6782n2beb1.png?width=2076&format=png&auto=webp&s=775778ef52b01caf331d3e0f542603626cea7888
    submitted by    /u/LesgoLeggo  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are there any entities/organizations working on the self-regulation of AI technology?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15a4xwc/are_there_any_entitiesorganizations_working_on/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15a4xwc/are_there_any_entitiesorganizations_working_on/"/>
        <updated>2023-07-26T12:47:41.000Z</updated>
        <summary type="html"><![CDATA[I am curious if there are any efforts among AI technologists to self-regulate, in the way that for example, the advertising industry in the US self-regulates via the IAB? 
    submitted by    /u/Winter_Addition  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New technical deep dive course: Generative AI Foundations on AWS]]></title>
        <id>fc28b7f09affe471cd04291b543db1974f4b6fb4</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/new-technical-deep-dive-course-generative-ai-foundations-on-aws/"/>
        <updated>2023-07-26T12:20:13.000Z</updated>
        <summary type="html"><![CDATA[Generative AI Foundations on AWS is a new technical deep dive course that gives you the conceptual fundamentals, practical advice, and hands-on guidance to pre-train, fine-tune, and deploy state-of-the-art foundation models on AWS and beyond. Developed by AWS generative AI worldwide foundations lead Emily Webber, this free hands-on course and the supporting GitHub source code […]]]></summary>
        <author>
            <name>Emily Webber</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How do people track their machine learning models?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15a4136/d_how_do_people_track_their_machine_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15a4136/d_how_do_people_track_their_machine_learning/"/>
        <updated>2023-07-26T12:07:22.000Z</updated>
        <summary type="html"><![CDATA[Hello!
 I'm curious to know how you guys currently track changes and general information for your ML/DL models. 
 By changes, I'm referring to parameters, accuracy/loss, functions your model uses, training data etc across different versions of your models.
 By general changes, I'm referring to descriptions of what the model does, code changes, tags and so on.
 I'm under the impression most people are using MLFlow, W&Bs etc which I guess is fine but I'm finding that these tools treat models as static files, as second-class citizens which is annoying when I want to zero in on a model and understand what and how something was changed away from an experiment. This gets really annoying when I'm looking at model version 134 created by Mike in the other team.
 Curious to know how people are tracking models and what they think generally about model tracking. Thanks!
    submitted by    /u/bobskithememe  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Presenting SimplePyDash: Real-Time Data Plotting Made Simple!]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15a3scs/presenting_simplepydash_realtime_data_plotting/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15a3scs/presenting_simplepydash_realtime_data_plotting/"/>
        <updated>2023-07-26T11:56:44.000Z</updated>
        <summary type="html"><![CDATA[Hey all! I'm excited to share SimplePyDash, a new tool I've developed for real-time data visualization. It's a versatile, browser-based dashboard designed to make data plotting as straightforward as possible!
 I thought about posting this here because it started as a project to monitor agents' behaviour in an OpenAI Gym environment. But it can be used for all sorts of things!
 Whether you're monitoring an OpenAI Gym environment, plotting your latest ML model's performance, or just need a flexible way to stream data, SimplePyDash has got you covered. With a clean, column-based layout and a set of intuitive default widgets, you can create your own custom dashboard in no time.
 Installing is as easy as running pip install simple-pydash, and there are several example scripts in the repo to help get you started.
 Check out the GitHub Repository for more details. If you like it, leave a start and feel free to share your feedback or questions. Thanks for checking it out!
    submitted by    /u/vaaal88  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] How can I produce embeddings for text inputs from a pretrained transformer model?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15a3lw1/r_how_can_i_produce_embeddings_for_text_inputs/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15a3lw1/r_how_can_i_produce_embeddings_for_text_inputs/"/>
        <updated>2023-07-26T11:48:25.000Z</updated>
        <summary type="html"><![CDATA[If I have the model saved as .ckpt file, what are the steps for extracting the embeddings for text input? I’m trying to use a pretrained custom model but don’t quite understand how to work with transformer model file in *.ckpt form. Would really appreciate any suggestions.
    submitted by    /u/Urusander  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Vector database benchmarking]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15a22jv/d_vector_database_benchmarking/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15a22jv/d_vector_database_benchmarking/"/>
        <updated>2023-07-26T10:35:41.000Z</updated>
        <summary type="html"><![CDATA[Is there a way in which i can calculate the precision scores of a vector database. I need to do benchmarking on milvus and elasticsearch on a custom dataset. Any help would be appreciated.
    submitted by    /u/adiraat  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Any good models on huggingface for specific text generation use case?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15a2060/p_any_good_models_on_huggingface_for_specific/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15a2060/p_any_good_models_on_huggingface_for_specific/"/>
        <updated>2023-07-26T10:32:06.000Z</updated>
        <summary type="html"><![CDATA[hi was wondering if there are any lightweight models which I can download from huggingface for fine tuning for my use case. I'm trying to build a model which takes a paragraph of data and certain instructions to get parts of the data in json format as the output.
    submitted by    /u/Right-Type-3210  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Five Important AI Programming Languages - Python, C++, R, MATLAB, and Java]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15a0rqc/five_important_ai_programming_languages_python_c/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15a0rqc/five_important_ai_programming_languages_python_c/"/>
        <updated>2023-07-26T09:26:51.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Tao_Dragon  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Jaccard index and jazz albums]]></title>
        <id>https://www.johndcook.com/blog/?p=200061</id>
        <link href="https://www.johndcook.com/blog/2023/07/26/jaccard-jazz/"/>
        <updated>2023-07-26T08:47:26.000Z</updated>
        <summary type="html"><![CDATA[Jaccard index is a way of measuring the similarity of sets. The Jaccard index, or Jaccard similarity coefficient, of two sets A and B is the number of elements in their intersection, A ∩ B, divided by the number of elements in their union, A ∪ B. Jaccard similarity is a robust way to compare […]
Jaccard index and jazz albums first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The AI-Powered, Totally Autonomous Future of War Is Here]]></title>
        <id>https://www.reddit.com/r/artificial/comments/159yz3p/the_aipowered_totally_autonomous_future_of_war_is/</id>
        <link href="https://www.reddit.com/r/artificial/comments/159yz3p/the_aipowered_totally_autonomous_future_of_war_is/"/>
        <updated>2023-07-26T07:43:19.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Alone-Competition-77  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Frontier Model Forum]]></title>
        <id>https://openai.com/blog/frontier-model-forum</id>
        <link href="https://openai.com/blog/frontier-model-forum"/>
        <updated>2023-07-26T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[We’re forming a new industry body to promote the safe and responsible development of frontier AI systems: advancing AI safety research, identifying best practices and standards, and facilitating information sharing among policymakers and industry.]]></summary>
        <author>
            <name>OpenAI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 7/25/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/159vpli/oneminute_daily_ai_news_7252023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/159vpli/oneminute_daily_ai_news_7252023/"/>
        <updated>2023-07-26T04:42:50.000Z</updated>
        <summary type="html"><![CDATA[Ridgelinez (Tokyo) is a subsidiary of Fujitsu in Japan that announced the development of a generative artificial intelligence (AI) system capable of engaging in voice communication with humans. The applications of this system include assisting companies in conducting meetings or providing career planning advice to employees.[1] 
 BMW has revealed that artificial intelligence is already allowing it to cut costs at its sprawling factory in Spartanburg, South Carolina. The AI system has allowed BMW to remove six workers from the line and deploy them to other jobs. The tool is already saving the company over $1 million a year.[2]
 MIT’s ‘PhotoGuard‘ protects your images from malicious AI edits. The technique introduces nearly invisible “perturbations” to throw off algorithmic models.[3]
 Microsoft with its TypeChat library seeks to enable easy development of natural language interfaces for large language models (LLMs) using types. Introduced July 20 of a team with c# and TypeScript lead developer Anders Hejlsberg, a Microsoft Technical Fellow, TypeChat addresses the difficulty of developing natural language interfaces where apps rely on complex decision trees to determine intent and gather necessary input to act.[4]
  
Sources:
 [1] https://www.ridgelinez.com/
 [2] https://www.carscoops.com/2023/07/bmw-is-using-ai-to-cut-production-costs-at-spartanburg-plant/
 [3] https://www.engadget.com/mits-photoguard-protects-your-images-from-malicious-ai-edits-213036912.html
 [4] https://playcrazygame.com/singapore/2023/07/24/microsoft-unveils-typechat-library-for-building-natural-language-interfaces/ 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Snapchat discovery page filled with fake ai news stories]]></title>
        <id>https://www.reddit.com/r/artificial/comments/159vjdn/snapchat_discovery_page_filled_with_fake_ai_news/</id>
        <link href="https://www.reddit.com/r/artificial/comments/159vjdn/snapchat_discovery_page_filled_with_fake_ai_news/"/>
        <updated>2023-07-26T04:33:49.000Z</updated>
        <summary type="html"><![CDATA[You watch some of these videos and the quality and jitterness around the body is so bad you can clearly tell that its ai generated, how are people not picking up on it, fake news stories to get clicks, its like they use a deep fake on a video and put whoever they want ontop of it and make a video, but hey most people using snapchat arent smart enough to see this and the people watching them are dumb kids and teenagers that believe everything they see
    submitted by    /u/missmyniwwa911  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformers for Recommender Systems. [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/159v4d3/transformers_for_recommender_systems_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/159v4d3/transformers_for_recommender_systems_d/"/>
        <updated>2023-07-26T04:12:09.000Z</updated>
        <summary type="html"><![CDATA[Been involved in a research project of a session based recommendation systems , where we have a historical purchases of users and the goal is to predict the next going to be purchased item. Given this and assuming that we have somehow represented each item in a session as an embedding and these embeddings acts as an input to the transformer model and the output is an embedding of the next product. In the train set, there are some millions sessions which has both previous purchases products of arbitrary length and next item. So the transformer is trained with supervised loss of predicted and actual next item embedding, the problem i have been facing is that the loss is saturating and there is not much learning over time. Any suggestions on how to improve this. Tried increasing the number of layers and did some hyper tuning corresponding to learning rate and weight decay but similar behaviour is observed.
    submitted by    /u/Acceptable-Mix-4534  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AWS Reaffirms its Commitment to Responsible Generative AI]]></title>
        <id>f68d0d6b4fc134ccc9ec193efb8173f70bf616d6</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/aws-reaffirms-its-commitment-to-responsible-generative-ai/"/>
        <updated>2023-07-26T04:00:36.000Z</updated>
        <summary type="html"><![CDATA[As a pioneer in artificial intelligence and machine learning, AWS is committed to developing and deploying generative AI responsibly As one of the most transformational innovations of our time, generative AI continues to capture the world’s imagination, and we remain as committed as ever to harnessing it responsibly. With a team of dedicated responsible AI […]]]></summary>
        <author>
            <name>Peter Hallinan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A simpler method for learning to control a robot]]></title>
        <id>https://news.mit.edu/2023/simpler-method-learning-control-robot-0726</id>
        <link href="https://news.mit.edu/2023/simpler-method-learning-control-robot-0726"/>
        <updated>2023-07-26T04:00:00.000Z</updated>
        <summary type="html"><![CDATA[Researchers develop a machine-learning technique that can efficiently learn to control a robot, leading to better performance with fewer data.]]></summary>
        <author>
            <name>Adam Zewe | MIT News Office</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] generating datasets to better fine-tune LLMs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/159uqbm/r_generating_datasets_to_better_finetune_llms/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/159uqbm/r_generating_datasets_to_better_finetune_llms/"/>
        <updated>2023-07-26T03:53:24.000Z</updated>
        <summary type="html"><![CDATA[https://github.com/discus-labs/discus
    submitted by    /u/innovating_ai  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenAI launches Android version of its ChatGPT app]]></title>
        <id>https://www.reddit.com/r/artificial/comments/159tijj/openai_launches_android_version_of_its_chatgpt_app/</id>
        <link href="https://www.reddit.com/r/artificial/comments/159tijj/openai_launches_android_version_of_its_chatgpt_app/"/>
        <updated>2023-07-26T02:54:05.000Z</updated>
        <summary type="html"><![CDATA[Two months after bringing ChatGPT to iOS, OpenAI LP today launched an Android version of its artificial intelligence assistant.
 The Android app is currently accessible for users in the U.S, India, Bangladesh and Brazil. OpenAI will extend availability to additional countries over the next week. The iOS version was available for download in more than 150 countries as of late May.
    submitted by    /u/Tiger_Claw_1  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Monarch Mixer: Revisiting BERT, Without Attention or MLPs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/159t1bi/r_monarch_mixer_revisiting_bert_without_attention/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/159t1bi/r_monarch_mixer_revisiting_bert_without_attention/"/>
        <updated>2023-07-26T02:31:47.000Z</updated>
        <summary type="html"><![CDATA[https://hazyresearch.stanford.edu/blog/2023-07-25-m2-bert
    submitted by    /u/hzj5790  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Unlocks Olive Oil's Potential in Alzheimer's Battle]]></title>
        <id>https://www.reddit.com/r/artificial/comments/159qe1p/ai_unlocks_olive_oils_potential_in_alzheimers/</id>
        <link href="https://www.reddit.com/r/artificial/comments/159qe1p/ai_unlocks_olive_oils_potential_in_alzheimers/"/>
        <updated>2023-07-26T00:31:49.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Alone-Competition-77  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Yesterday, we were having a discussion about synthetically generated video. Well, I'm back as promised, and with a very interesting result. Check it out! Details in comments.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/159pita/yesterday_we_were_having_a_discussion_about/</id>
        <link href="https://www.reddit.com/r/artificial/comments/159pita/yesterday_we_were_having_a_discussion_about/"/>
        <updated>2023-07-25T23:55:35.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/otherworlderotic  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[About Singing Ai?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/159oa3g/about_singing_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/159oa3g/about_singing_ai/"/>
        <updated>2023-07-25T23:05:57.000Z</updated>
        <summary type="html"><![CDATA[Is it possible to have Ai come with a generated lyrics and sings within the bpm + root note?
 Does this exist? i’ll like to know where and how.
 Ai is interesting.
    submitted by    /u/Office_Flashy  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Oversight of A.I.: Principles for Regulation | United States Senate Committee on the Judiciary - with Anthropic CEO]]></title>
        <id>https://www.reddit.com/r/artificial/comments/159nfdm/oversight_of_ai_principles_for_regulation_united/</id>
        <link href="https://www.reddit.com/r/artificial/comments/159nfdm/oversight_of_ai_principles_for_regulation_united/"/>
        <updated>2023-07-25T22:34:01.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/jaketocake  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Web Content Embedding Transformer lambda function [Project]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/159mafd/web_content_embedding_transformer_lambda_function/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/159mafd/web_content_embedding_transformer_lambda_function/"/>
        <updated>2023-07-25T21:51:45.000Z</updated>
        <summary type="html"><![CDATA[Hi all!
 I'd would like to share a simple, straight-forward Web Content Embedding Transformer lambda function to create and store embeddings of web content. This is a Lambda function that scrapes for URLs, then uses URLs those to scrape for page content, which it splits into chunks then transforms to embedding using OpenAI. It then stores the embeddings to your Pinecone DB including metadata. You can then use the embedding for custom chatbots etc.
 Heres a link to a public REPO.
 https://github.com/i-dream-of-ai/lambda-webpage-vector-store
 pull requests welcome! Please star the repo if you like it or use it!
    submitted by    /u/Jealous_Buyer  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning for Regression and Target scaling [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/159kl05/deep_learning_for_regression_and_target_scaling_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/159kl05/deep_learning_for_regression_and_target_scaling_d/"/>
        <updated>2023-07-25T20:48:56.000Z</updated>
        <summary type="html"><![CDATA[I tried scaling the target variable to be in the range (0,1) and trained the model using a sigmoid in the last layer. But when rescaled back after prediction on test, the errors are too high. What can be done? 
 Do I need to scale in the first place? 
 Also please answer this general question: How to get a Deep learning model to work well on Regression tasks?
    submitted by    /u/Charming-Witness-286  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Free/Low cost inference endpoint]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/159kgie/p_freelow_cost_inference_endpoint/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/159kgie/p_freelow_cost_inference_endpoint/"/>
        <updated>2023-07-25T20:44:31.000Z</updated>
        <summary type="html"><![CDATA[I want to create a small project as hobby in which the web app posts some user data to an endpoint hosting a model that returns its predictions. 
 So I was wondering if there’s a platform that hosts models for free for hobbists? 
 The idea is to build a simple portfolio project just to display to recruiters.
    submitted by    /u/OkYak2915  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSC Weekly 25 July 2023]]></title>
        <id>https://www.datasciencecentral.com/?p=62723</id>
        <link href="https://www.datasciencecentral.com/dsc-weekly-25-july-2023/"/>
        <updated>2023-07-25T19:53:37.000Z</updated>
        <summary type="html"><![CDATA[Announcements Top Stories In-Depth
The post DSC Weekly 25 July 2023 appeared first on Data Science Central.]]></summary>
        <author>
            <name>Scott Thompson</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI alignment proposal: Supplementary Alignment Insights Through a Highly Controlled Shutdown Incentive — LessWrong]]></title>
        <id>https://www.reddit.com/r/artificial/comments/159isqa/ai_alignment_proposal_supplementary_alignment/</id>
        <link href="https://www.reddit.com/r/artificial/comments/159isqa/ai_alignment_proposal_supplementary_alignment/"/>
        <updated>2023-07-25T19:45:51.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/RamazanBlack  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aaron Parisi (Google DeepMind) will join the open AI4Code reading group this Thursday (July 27th) to talk about his latest research [R]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/159hy5x/aaron_parisi_google_deepmind_will_join_the_open/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/159hy5x/aaron_parisi_google_deepmind_will_join_the_open/"/>
        <updated>2023-07-25T19:14:35.000Z</updated>
        <summary type="html"><![CDATA[Hi AI enthusiasts! This Thursday Aaron Parisi, Google DeepMind researcher, will join us to present and discuss his recent work as the lead author of TALM, a framework for augmenting language models with arbitrary tools.
 Free RSVP: https://lu.ma/mw5ppi46
 Paper: https://arxiv.org/abs/2205.12255
 🗓 July 27th (Thursday) at 17:00 GMT+1
 📍 Zoom
 👥 Members of the international AI4Code research community
 Hope to see you there!
 The AI4Code meetup community consists of like-minded researchers from around the world that network, discuss and share their latest research on AI applications on source code.
    submitted by    /u/dritsakon  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI presidential debate]]></title>
        <id>https://www.reddit.com/r/artificial/comments/159hs4p/ai_presidential_debate/</id>
        <link href="https://www.reddit.com/r/artificial/comments/159hs4p/ai_presidential_debate/"/>
        <updated>2023-07-25T19:08:30.000Z</updated>
        <summary type="html"><![CDATA[Hilarious, comedic effort of an AI presidential debate going on now.
 https://www.twitch.tv/trumporbiden2024
    submitted by    /u/Smash_Factor  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["The AI-Powered, Totally Autonomous Future of War Is Here" (use of DRL in Navy swarms R&D)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/159hkzj/the_aipowered_totally_autonomous_future_of_war_is/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/159hkzj/the_aipowered_totally_autonomous_future_of_war_is/"/>
        <updated>2023-07-25T19:01:44.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Project] Quality Assurance platform for Machine Learning models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/159h565/project_quality_assurance_platform_for_machine/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/159h565/project_quality_assurance_platform_for_machine/"/>
        <updated>2023-07-25T18:46:02.000Z</updated>
        <summary type="html"><![CDATA[Hello world 👋
 We're developing an open-source & collaborative testing framework for ML models, from tabular to LLMs: https://github.com/Giskard-AI/giskard
 Testing Machine Learning applications can be tedious. Since ML models depend on data, testing scenarios depend on the domain specificities and are often infinite.
 Where to start testing? Which tests to implement? What issues to cover? How to implement the tests?
 At Giskard, we believe that Machine Learning needs its own testing framework. Created by ML engineers for ML engineers, Giskard contains 2 components:
  
The Giskard Python library helps data scientists detect hidden vulnerabilities in ML models. 
 It makes the AI development process more efficient, by automating the identification of risks of biases, performance issues and errors.
 To try it, see this documentation: https://docs.giskard.ai/en/latest/guides/scan/index.html
 
 The Giskard server helps ML engineers debug & monitor models, share dashboards, and collaborate. 
 It makes the deployment of new ML models safer and more efficient, by providing ready-made monitoring dashboards, catalogs of re-usable testing components, and ML debugging interfaces.
 To try it, see this documentation: https://docs.giskard.ai/en/latest/guides/installation_app/index.html
 
  
We released our v2 in Beta last month, and we're very interested in your feedback as QA engineers!
    submitted by    /u/alteralec  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The White House Already Knows How to Make AI Safer]]></title>
        <id>https://www.reddit.com/r/artificial/comments/159gymw/the_white_house_already_knows_how_to_make_ai_safer/</id>
        <link href="https://www.reddit.com/r/artificial/comments/159gymw/the_white_house_already_knows_how_to_make_ai_safer/"/>
        <updated>2023-07-25T18:39:24.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/trueslicky  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Towards provably efficient quantum algorithms for large-scale machine-learning models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/159gq7j/r_towards_provably_efficient_quantum_algorithms/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/159gq7j/r_towards_provably_efficient_quantum_algorithms/"/>
        <updated>2023-07-25T18:30:44.000Z</updated>
        <summary type="html"><![CDATA[https://arxiv.org/abs/2303.03428
 ​
 If you're interested in trying out quantum machine learning on NVIDIA A100s or V100s with cuquantum and pennylane GPUs for free please fill out the following form
    submitted by    /u/Neu3ral  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trying NLP on Middle English]]></title>
        <id>https://www.johndcook.com/blog/?p=200007</id>
        <link href="https://www.johndcook.com/blog/2023/07/25/nlp-chaucer/"/>
        <updated>2023-07-25T18:15:23.000Z</updated>
        <summary type="html"><![CDATA[It’s not fair to evaluate NLP software on a language it wasn’t designed to process, but I wanted to try it anyway. The models in the spaCy software library were trained on modern English text and not on Middle English. Nevertheless, spaCy does a pretty good job of parsing Chaucer’s Canterbury Tales, written over 600 […]
Trying NLP on Middle English first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Utilizing AI With Neutral Global Oversight for Business & Society]]></title>
        <id>https://www.reddit.com/r/artificial/comments/159g4ge/utilizing_ai_with_neutral_global_oversight_for/</id>
        <link href="https://www.reddit.com/r/artificial/comments/159g4ge/utilizing_ai_with_neutral_global_oversight_for/"/>
        <updated>2023-07-25T18:08:49.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/citidotio  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fixed size 1D sequence to fixed size 2D sequence prediction.[p]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/159eddu/fixed_size_1d_sequence_to_fixed_size_2d_sequence/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/159eddu/fixed_size_1d_sequence_to_fixed_size_2d_sequence/"/>
        <updated>2023-07-25T17:05:29.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone, I have this problem where I have a 1D sequence of numbers of length 3 like this: 
 [1,50,500], with 35 distinct combinations.
 I need to map it to 2D sequence of number of 1024 length. Like this : 
 [ [ 23.78, 234, 13,…n], [ 234,76.9, 763,…n ]] , where n =1024.
 Is it possible in ML to do so?
 The 2D sequence can paired ( can be represented an image). 
 Thank you very much !
    submitted by    /u/Beginner4ever  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The AI content + data mandate and personal branding]]></title>
        <id>https://www.datasciencecentral.com/?p=62696</id>
        <link href="https://www.datasciencecentral.com/the-ai-content-data-mandate-and-personal-branding/"/>
        <updated>2023-07-25T16:30:13.000Z</updated>
        <summary type="html"><![CDATA[Fair Data Forecast Interview with Andreas Volpini, CEO of WordLift Andreas Volpini believes every user who wants to build a personal brand online has to proactively curate their online presence first. He sees structured data (semantic entity and attribute metadata such as Schema.org) as key to building a cohesive, disambiguated personal presence online. Volpini has… Read More »The AI content + data mandate and personal branding
The post The AI content + data mandate and personal branding appeared first on Data Science Central.]]></summary>
        <author>
            <name>Alan Morrison</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Use generative AI foundation models in VPC mode with no internet connectivity using Amazon SageMaker JumpStart]]></title>
        <id>e6cf42cd370b365438792942d951302067509d3f</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/use-generative-ai-foundation-models-in-vpc-mode-with-no-internet-connectivity-using-amazon-sagemaker-jumpstart/"/>
        <updated>2023-07-25T16:29:41.000Z</updated>
        <summary type="html"><![CDATA[With recent advancements in generative AI, there are lot of discussions happening on how to use generative AI across different industries to solve specific business problems. Generative AI is a type of AI that can create new content and ideas, including conversations, stories, images, videos, and music. It is all backed by very large models […]]]></summary>
        <author>
            <name>Vikesh Pandey</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From automation to optimization: How AI is revolutionizing digital marketing campaigns]]></title>
        <id>https://www.datasciencecentral.com/?p=62580</id>
        <link href="https://www.datasciencecentral.com/from-automation-to-optimization-how-ai-is-revolutionizing-digital-marketing-campaigns/"/>
        <updated>2023-07-25T16:26:10.000Z</updated>
        <summary type="html"><![CDATA[Welcome to the exciting world of digital marketing! In this blog, we’ll delve into this thrilling frontier where optimization meets automation and Artificial Intelligence is at the center. No longer must manual labor and guesswork play an essential part in developing effective marketing strategies; with AI’s capabilities now at their disposal, marketers with digital presence… Read More »From automation to optimization: How AI is revolutionizing digital marketing campaigns
The post From automation to optimization: How AI is revolutionizing digital marketing campaigns appeared first on Data Science Central.]]></summary>
        <author>
            <name>Erika Balla</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[If Deadpool 3 Was Written By AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/159d2jt/if_deadpool_3_was_written_by_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/159d2jt/if_deadpool_3_was_written_by_ai/"/>
        <updated>2023-07-25T16:18:08.000Z</updated>
        <summary type="html"><![CDATA[Story by AI, Voiced by AI, Art by AI
    submitted by    /u/realzackmcfarlin  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What datasets do you dream of having for your ML/NLP project(s)?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/159cz1g/d_what_datasets_do_you_dream_of_having_for_your/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/159cz1g/d_what_datasets_do_you_dream_of_having_for_your/"/>
        <updated>2023-07-25T16:14:35.000Z</updated>
        <summary type="html"><![CDATA[Acquiring data to build models can truly be a pain. I am curious to know about the datasets you folks are looking for, to the extent that you would even consider paying for them or sacrifice your newborn baby. By extension, tell us about the project(s) you've been working on and how the data would help! 
    submitted by    /u/nobilis_rex_  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Tool for ML/AI Sorting for 50,000 iCloud Photos into 300+ categories]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/159chrs/d_tool_for_mlai_sorting_for_50000_icloud_photos/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/159chrs/d_tool_for_mlai_sorting_for_50000_icloud_photos/"/>
        <updated>2023-07-25T15:57:17.000Z</updated>
        <summary type="html"><![CDATA[One of my acquaintances is an artist and is asking my assistance in utilizing Machine Learning and AI to sort his entire iCloud library of 57,000 images into 300+ categories. Some of these categories include things that the media is made of such as ceramics wood, or the artist that created this work while other categories include whether the photo contains an animal or a person. 
 I am wondering if there are specific ML programs that would be a good fit for his situation. My idea suggested to use Apple’s CoreML which I have experience in. I could develop him an app that he could then create train and swap image recognition models using the GUI CreateML tool using the images he has already sorted. Do you think this is the best approach or is there another tool out there that could do this task for him easily?
    submitted by    /u/Jpderouin310  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[They offer a Tesla to their biggest customers :o]]></title>
        <id>https://www.reddit.com/r/artificial/comments/159bnxp/they_offer_a_tesla_to_their_biggest_customers_o/</id>
        <link href="https://www.reddit.com/r/artificial/comments/159bnxp/they_offer_a_tesla_to_their_biggest_customers_o/"/>
        <updated>2023-07-25T15:27:03.000Z</updated>
        <summary type="html"><![CDATA[The company is named Eden AI, they currently do their Product Hunt launch. They allow users to use AI APIs from all the AI companies (Google, AWS, OpenAI, Microsoft, and all the specialized companies).
 They recently added this rewards progress bar to their billing page, funny marketing operation!
 ​
 https://preview.redd.it/wgsg7yr5q4eb1.png?width=997&format=png&auto=webp&s=8f081891943f85ba9c72090cc5d946d3bd07ccf0
 ​
    submitted by    /u/JerLam2762  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extending harmonic numbers]]></title>
        <id>https://www.johndcook.com/blog/?p=199970</id>
        <link href="https://www.johndcook.com/blog/2023/07/25/extending-harmonic-numbers/"/>
        <updated>2023-07-25T14:58:51.000Z</updated>
        <summary type="html"><![CDATA[For a positive integer n, the nth harmonic number is defined to be the sum of the reciprocals of the first n positive integers: How might we extend this definition so that n does not have to be a positive integer? First approach One way to extend harmonic numbers is as follows. Start with the […]
Extending harmonic numbers first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Autonomous Alignment Oversight Framework (AAOF)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1599e23/d_autonomous_alignment_oversight_framework_aaof/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1599e23/d_autonomous_alignment_oversight_framework_aaof/"/>
        <updated>2023-07-25T14:01:24.000Z</updated>
        <summary type="html"><![CDATA[Abstract:
 To align advanced AIs, an ensemble of diverse, transparent Overseer AIs will independently monitor the target AI and provide granular assessments on its alignment with constitution, human values, ethics, and safety. Overseer interventions will be incremental and subject to human oversight. The system will be implemented cautiously, with extensive testing to validate capabilities. Alignment will be treated as an ongoing collaborative process between humans, Overseers, and the target AI, leveraging complementary strengths through open dialog. Continuous vigilance, updating of definitions, and contingency planning will be required to address inevitable uncertainties and risks.
 Introduction: 
 As advanced AI systems grow in capability and autonomy, ensuring their alignment with hum…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Does GPT-4 use LoRA?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1598yvi/d_does_gpt4_use_lora/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1598yvi/d_does_gpt4_use_lora/"/>
        <updated>2023-07-25T13:44:33.000Z</updated>
        <summary type="html"><![CDATA[I just watched a video that explains how LoRA works. As I understand it's a fast and efficient way to fine tune models.
 At the end of the video he he said you could easily swap out the fine-tuned LoRA. So it makes LLMs like a PC. You just install new software / add the finetuned lora weights and you're good to go. Is my understanding correct?
 The rumor is that GPT-4 is a 8 way mixture model. Could they have pretrained it with all the data and then just use LoRA to train the expert models? I guess they would also need to train a smaller model that decides which model to use. I can't imagine that they would train GPT-4 eight times / once for each expert models.
    submitted by    /u/StraightChemistry629  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-fictitious play and Q-learning or evolutionary algorithms]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15985oq/selffictitious_play_and_qlearning_or_evolutionary/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15985oq/selffictitious_play_and_qlearning_or_evolutionary/"/>
        <updated>2023-07-25T13:11:38.000Z</updated>
        <summary type="html"><![CDATA[I'm looking to implement Fictitious Self-Play in a model-based game (imperfect information limited to simultaneous move game, however each player has a combinatorial number of actions they can perform). EDIT: I know self-fictitious play is not the only option to solve this type of game, but I wanted to give it a try to test how it would behave (especially since I sort of like the idea behind it).
 Because of this combinatorial number of actions, solving it with linear programming is just not possible (I'd have to compute for each sequence of pair of actions (a1, b1), (a2, b2), (a3, b3), ... (ak, bk), whether player a or player b won).
 But to compute a best response I might be able to use Q-learning in a RL setting right (by that I mean, fixed environment)? Because when we calculate a best…]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA DGX Cloud Now Available to Supercharge Generative AI Training]]></title>
        <id>https://blogs.nvidia.com/?p=65625</id>
        <link href="https://blogs.nvidia.com/blog/2023/07/25/dgx-generative-ai/"/>
        <updated>2023-07-25T13:00:21.000Z</updated>
        <summary type="html"><![CDATA[NVIDIA DGX Cloud — which delivers tools that can turn nearly any company into an AI company —  is now broadly available, with thousands of NVIDIA GPUs online on Oracle Cloud Infrastructure, as well as NVIDIA infrastructure located in the U.S. and U.K. Unveiled at NVIDIA’s GTC conference in March, DGX Cloud is an AI Read article >]]></summary>
        <author>
            <name>Tony Paikeday</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fin-tastic: 3D Artist Dives Into AI-Powered Oceanic Work This Week ‘In the NVIDIA Studio’]]></title>
        <id>https://blogs.nvidia.com/?p=65586</id>
        <link href="https://blogs.nvidia.com/blog/2023/07/25/alessandro-mastronardi-blender-omniverse-broadcast/"/>
        <updated>2023-07-25T13:00:07.000Z</updated>
        <summary type="html"><![CDATA[We’re gonna need a bigger boat this week In the NVIDIA Studio as Alessandro Mastronardi, senior artist and programmer at BBC Studios, shares heart-stopping shark videos and renders.]]></summary>
        <author>
            <name>Gerardo Delgado</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intel Seeks To Win Over AI Developers With Open-Source Reference Kits]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15974ce/intel_seeks_to_win_over_ai_developers_with/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15974ce/intel_seeks_to_win_over_ai_developers_with/"/>
        <updated>2023-07-25T12:28:52.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/reps_up  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[(Spiderman washing cloth) Ai is insane]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1596r2i/spiderman_washing_cloth_ai_is_insane/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1596r2i/spiderman_washing_cloth_ai_is_insane/"/>
        <updated>2023-07-25T12:12:50.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Unlikely_Gap_5065  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Deep Learning VS XGBoost for tabular data: a quick test]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1596pnm/d_deep_learning_vs_xgboost_for_tabular_data_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1596pnm/d_deep_learning_vs_xgboost_for_tabular_data_a/"/>
        <updated>2023-07-25T12:11:03.000Z</updated>
        <summary type="html"><![CDATA[Once per year, I write a post here on Reddit about our projects on deep learning for tabular data, and I hope this year will be no exception 🙂 Meanwhile, I have shared some results where we compare models from our previous papers with XGBoost on the datasets from the recent paper "Why do tree-based models still outperform deep learning on typical tabular data?". For us, this benchmark is a new one, so it was really interesting to check whether our previous findings generalize to new unseen datasets (spoiler: they do):
 https://twitter.com/YuraFiveTwo/status/1683796380895023104
    submitted by    /u/Yura52  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding OpenAI's past, current, and upcoming model releases]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1596ig1/understanding_openais_past_current_and_upcoming/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1596ig1/understanding_openais_past_current_and_upcoming/"/>
        <updated>2023-07-25T12:02:09.000Z</updated>
        <summary type="html"><![CDATA[I found it a bit hard to follow OpenAI's public releases - sometimes they just announce a model is coming without giving a date, sometimes they announce model deprecations and it's hard to understand whether we should use those models in production or not. 
 I am a visual thinker so putting everything in a single image made sense to me. Check it out below, and if you have any questions or suggestions, please let me know! 
 https://preview.redd.it/iuqc7nt2o3eb1.png?width=4800&format=png&auto=webp&s=ebe344a504d6a93fd2ce1935cdd1312d62735792
 https://preview.redd.it/vt2wkpt2o3eb1.png?width=4800&format=png&auto=webp&s=eb14503552b8d81398b5f3f76ebe68ad257e1857
    submitted by    /u/EscapedLaughter  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RL continuous control help needed. ML Engineer wanted also]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1595zfn/rl_continuous_control_help_needed_ml_engineer/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1595zfn/rl_continuous_control_help_needed_ml_engineer/"/>
        <updated>2023-07-25T11:37:48.000Z</updated>
        <summary type="html"><![CDATA[Hi, I can't find the rules for this subreddit so please lmk if asking any of this breaks them
 I have a pybullet simulation of a bipedal robot wrapped as a gym env. Currently trying to train a Rl ppo algo to control it to walk. But no luck. Having the issue that it's trying everything except walking. And it seems to prioritise getting the instant reward by kicking its leg forward then lunging forward and the episode ends. Instead of walking forward and getting more score. Anyone have any tips please? (Btw gamma = 0.99)
 ​
 Btw if anyone has experience with stuff like this I am looking to hire an engineer. Comment or dm me
 ​
 Btw I am aware this is a significant undertaking
    submitted by    /u/Harryoc494  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] How good is generative data (synthetic data) !?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1595xd8/discussion_how_good_is_generative_data_synthetic/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1595xd8/discussion_how_good_is_generative_data_synthetic/"/>
        <updated>2023-07-25T11:35:06.000Z</updated>
        <summary type="html"><![CDATA[5% average increase in F1 score
 67% increase in Data richness
 100% anonymized data set
 so one of the users on my tool milkstraw.ai just sent me this and i am really excited about the power of the tool i built and wanted to share it here 🚀
 Also more importantly how do you all feeling about synthetic data, I started this as a fun project and its turning into a full blown startup. I love seeing some of the users send me results they are getting like this. 
 https://preview.redd.it/24bwer0uk3eb1.png?width=4516&format=png&auto=webp&s=c8cbf906580a04df1f967c5300478a542128ccd0
    submitted by    /u/jjhazy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] New Open Source LLM: GOAT-7B (SOTA among the 7B models)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1595rc0/r_new_open_source_llm_goat7b_sota_among_the_7b/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1595rc0/r_new_open_source_llm_goat7b_sota_among_the_7b/"/>
        <updated>2023-07-25T11:27:07.000Z</updated>
        <summary type="html"><![CDATA[Go try this free model. 7B SOTA by MMLU and BBH
 https://preview.redd.it/tq8c8ggaj3eb1.png?width=1570&format=png&auto=webp&s=10c78b724da2d6360e7c7ee6fbe3175c36cecc26
    submitted by    /u/rempact  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A note on Zipf’s law]]></title>
        <id>https://www.johndcook.com/blog/?p=199949</id>
        <link href="https://www.johndcook.com/blog/2023/07/25/a-note-on-zipfs-law/"/>
        <updated>2023-07-25T11:18:26.000Z</updated>
        <summary type="html"><![CDATA[Very often when a number is large, and we don’t know or care exactly how large it is, we can model it as infinite. This may make no practical difference and can make calculations much easier. I give several examples of this in the article Infinite is easier than big. When you run across a […]
A note on Zipf’s law first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[skrl version 1.0.0-rc.1 is now available with multi-agent and JAX support!!!]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15955tq/skrl_version_100rc1_is_now_available_with/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15955tq/skrl_version_100rc1_is_now_available_with/"/>
        <updated>2023-07-25T10:59:23.000Z</updated>
        <summary type="html"><![CDATA[skrl version 1.0.0-rc.1 is now available.
 The main features of this release are:
  
JAX support
 Multi-agent training (the beginning).
 Comprehensive documentation with new structure and theme
  
Visit https://skrl.readthedocs.io/en/latest/ to get started!
 ​
 https://preview.redd.it/ms1q5s8ce3eb1.png?width=1459&format=png&auto=webp&s=4b1f0f27cae5df4dfac3e931eabcca2b924968d1
 https://preview.redd.it/385lhqdbe3eb1.png?width=1543&format=png&auto=webp&s=2cb5ef75f4c2720e8db5adbb6b3f35b7977e3b57
 ​
    submitted by    /u/Toni-SM  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Attention Is Off By One]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1594zb4/d_attention_is_off_by_one/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1594zb4/d_attention_is_off_by_one/"/>
        <updated>2023-07-25T10:50:16.000Z</updated>
        <summary type="html"><![CDATA[https://www.evanmiller.org/attention-is-off-by-one.html
    submitted by    /u/duckyzz003  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Voice cloning options, preferably local [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1594y3c/voice_cloning_options_preferably_local_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1594y3c/voice_cloning_options_preferably_local_d/"/>
        <updated>2023-07-25T10:48:33.000Z</updated>
        <summary type="html"><![CDATA[Hi!
 What voice cloning options are people using right now? Looking at what is out there (that I know of), there is ElevenLabs and Coqui. Are there any other ones that are good? Preferably cheap/run locally?
    submitted by    /u/MrJabbey1  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ZBrain: Empowering Businesses with Custom ChatGPT apps and Data Security]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1594ibf/zbrain_empowering_businesses_with_custom_chatgpt/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1594ibf/zbrain_empowering_businesses_with_custom_chatgpt/"/>
        <updated>2023-07-25T10:26:25.000Z</updated>
        <summary type="html"><![CDATA[Dear All,
 It is with great enthusiasm that I introduce you to ZBrain, a revolutionary GenAI platform that unlocks the ability to craft bespoke AI applications while prioritizing data privacy and security. ZBrain ushers in an era of remarkable possibilities for businesses seeking to harness the full potential of AI while ensuring their data remains safeguarded and confidential.
 ​
 What Sets ZBrain Apart:
  
ZBrain Flow - Codeless Brilliance: Forget complex coding! ZBrain Flow's intuitive drag-and-drop interface seamlessly connects large language models and extraction tools, simplifying the creation of sophisticated business logic without the need for coding expertise.
 
AI Risk Governance for Data Safety: At ZBrain, we deeply understand the significance of data security. Our AI Risk Governance identifies potential risks such as Financial, Medical, Privacy, Harmful Language, and more. Through prompt engineering, your data is fortified, and sensitive information is shielded.
 
Effortless Integration and Continuous Advancements: With ZBrain, integration with over 80 data sources is a breeze, providing you the freedom to fine-tune models and deploy them effortlessly. Our reinforcement learning approach continually enriches results through valuable human feedback.
 
Confidence in Deployment: Choose your deployment approach with assurance. Opt for ZBrain Cloud for added security or self-hosting on your private infrastructure, ensuring data confidentiality remains at the forefront.
 
 ​
 We are genuinely elated about the endless possibilities ZBrain offers businesses spanning various industries. By merging the prowess of AI with an unwavering commitment to data privacy, we wholeheartedly believe that ZBrain will elevate your business to unparalleled heights.
 Visit ZBrain at https://zbrain.ai/ and feel free to reach out with any inquiries or to share your experiences with ZBrain.
    submitted by    /u/StewartBJasper  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Integrating Llama V2 🦙 and Multi-Chat Models: Open Source Solution with IntelliNode]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/158zr4i/p_integrating_llama_v2_and_multichat_models_open/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/158zr4i/p_integrating_llama_v2_and_multichat_models_open/"/>
        <updated>2023-07-25T06:19:45.000Z</updated>
        <summary type="html"><![CDATA[IntelliNode is an open source project that simplifies the integration of Llama V2 and other multi-chat models. With IntelliNode, you can easily connect and switch between different language models, including Llama V2 hosted in your AWS SageMaker account.
 It allows you to create a chatbot instance and add the backend provider.
 const { Chatbot, LLamaSageInput, SupportedChatModels } = require('intellinode'); const chatbot = new Chatbot(key, SupportedChatModels.SAGEMAKER, {url: <your-sagemaker-endpoint>}); 
 For details on how to use intellinode to integrate with LLama SageMaker setup click here.
 The module available here.
 ​
 ​
    submitted by    /u/Barqawiz_Coder  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Research] transformer models for drug discovery]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/158z1eo/research_transformer_models_for_drug_discovery/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/158z1eo/research_transformer_models_for_drug_discovery/"/>
        <updated>2023-07-25T05:41:31.000Z</updated>
        <summary type="html"><![CDATA[Does anybody know of good/reputable literature and other resources to read/learn about incorporating transformers in drug discovery? I am doing some computational chemistry research regarding compound identification for HBV mutations and want to try using transformers but don't really know where/how to start.
    submitted by    /u/Present_Network1959  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Annotation tool for annotating audio in a video]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/158ybd6/d_annotation_tool_for_annotating_audio_in_a_video/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/158ybd6/d_annotation_tool_for_annotating_audio_in_a_video/"/>
        <updated>2023-07-25T05:03:40.000Z</updated>
        <summary type="html"><![CDATA[Does anyone know of a good video (or audio) annotation tool that would allow me to look at both the image and the audio waveform at the same time?
 I could extract the audio and use an audio annotation tool, but since some of the sound events may sound similar to one another, it would be helpful to look at both the image and the audio waveform to identify which class a sound event belongs to. Thanks!
    submitted by    /u/utility2000  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 7/24/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/158y8v9/oneminute_daily_ai_news_7242023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/158y8v9/oneminute_daily_ai_news_7242023/"/>
        <updated>2023-07-25T05:00:40.000Z</updated>
        <summary type="html"><![CDATA[In a study published earlier this month, scientists at Rice and Stanford University concluded that training AI models exclusively on the outputs of generative AI is not a good idea. They titled their report: “Self-consuming generative models go MAD(Model Autophagy Disorder)”.[1]
 To enhance SQL query building, Lasse, a seasoned full-stack developer, has recently released AIHelperBot. This powerful tool enables individuals and businesses to write SQL queries efficiently, enhance productivity, and learn new SQL techniques.[2] 
 Japan’s Ministry of Economy, Trade, and Industry (METI) has announced its plans to develop a new supercomputer to help advance the country’s artificial intelligence (AI) industry. The new supercomputer (SC) will be operated by the National Institute of Advanced Industrial Science and Technology (AIST).[3]
 Google co-founder Sergey Brin is back in the company’s office working directly with members of the artificial intelligence team.[4]
  
Sources:
 [1] https://www.cdotrends.com/story/18288/training-ai-outputs-generative-ai-mad
 [2] https://dtgreviews.com/ai/meet-aihelperbot-an-artificial-intelligence-ai-based-sql-expert-that-builds-sql-queries-in-seconds/126512/
 [3] https://www.gizchina.com/2023/07/24/japan-ministry-develop-supercomputer-ai-industries/
 [4] https://www.wsj.com/video/series/tech-news-briefing/google-co-founder-returns-to-help-with-ai-efforts/27CE8E53-C8D8-4D93-8FA1-5E2C465092CB 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] FEEDBACK - Hey I am lunching my Data Professionals job platform and I would like to receive some feedback from you guys, thx]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/158svz6/p_feedback_hey_i_am_lunching_my_data/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/158svz6/p_feedback_hey_i_am_lunching_my_data/"/>
        <updated>2023-07-25T00:55:28.000Z</updated>
        <summary type="html"><![CDATA[Hey all Redditors,
  
I have been thinking about this for years as I hate the cumbersome process of switching jobs.
 I have been planing it under the last year and finally I quit my job and built this in the last 1.5 months.
 I am lunching my Data Professionals job platform "applyscript dot com"
 I would like to receive some feedback from you guys.
 I really want to hear your opinion as that can help me improve the site a lot.
  
Thx for stopping by and giving feedback, I really appreciate your time and effort. :)
    submitted by    /u/glassAlloy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two opposing views on LLM’s reasoning capabilities. Clip1 Geoffrey Hinton. Clip2 Gary Marcus. Where do you fall in the debate?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/158rfx2/two_opposing_views_on_llms_reasoning_capabilities/</id>
        <link href="https://www.reddit.com/r/artificial/comments/158rfx2/two_opposing_views_on_llms_reasoning_capabilities/"/>
        <updated>2023-07-24T23:54:38.000Z</updated>
        <summary type="html"><![CDATA[bios from Wikipedia 
 Geoffrey Everest Hinton (born 6 December 1947) is a British-Canadian cognitive psychologist and computer scientist, most noted for his work on artificial neural networks. From 2013 to 2023, he divided his time working for Google (Google Brain) and the University of Toronto, before publicly announcing his departure from Google in May 2023 citing concerns about the risks of artificial intelligence (AI) technology. In 2017, he co-founded and became the chief scientific advisor of the Vector Institute in Toronto.
 Gary Fred Marcus (born 8 February 1970) is an American psychologist, cognitive scientist, and author, known for his research on the intersection of cognitive psychology, neuroscience, and artificial intelligence (AI).
    submitted by    /u/Sonic_Improv  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Externally mounting P100 GPU [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/158rddr/externally_mounting_p100_gpu_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/158rddr/externally_mounting_p100_gpu_d/"/>
        <updated>2023-07-24T23:51:35.000Z</updated>
        <summary type="html"><![CDATA[I made a mistake and bought a GPU that is not compatible with my motherboard. I found a P100 for $300 on ebay and bought it, but didn't research far enough to figure out that it isn't designed for a workstation motherboard. Is there any way I can externally mount it without spending tons on a GPU server? I am not sure just a PCIe riser will do the trick, since the GPU draws 250W and will also need a cooling system.
 Is it over?
    submitted by    /u/jankybiz  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] How do paper authors deal with takedown requests?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/158r1il/r_how_do_paper_authors_deal_with_takedown_requests/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/158r1il/r_how_do_paper_authors_deal_with_takedown_requests/"/>
        <updated>2023-07-24T23:37:59.000Z</updated>
        <summary type="html"><![CDATA[Datasets like FFHQ consist of face images crawled from the Internet. While those images are published under CC licenses, the authors usually have not obtained consent from each person depicted in those images. I guess that's why they are taking takedown requests: People can send requests to remove their faces from the dataset.
 However, I'm always confused about one thing: Some faces images are already used in the paper. If those people request takedown of their images, wouldn't that result in a withdrawl of the paper? Or is there any "fair use" statement that can prevent this from happening?
    submitted by    /u/alex000092  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Do you guys think the day-to-day tasks of ML engineers will change with the emergence of LLM’s?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/158nkvt/d_do_you_guys_think_the_daytoday_tasks_of_ml/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/158nkvt/d_do_you_guys_think_the_daytoday_tasks_of_ml/"/>
        <updated>2023-07-24T21:23:47.000Z</updated>
        <summary type="html"><![CDATA[Gone will be the days of data pre-processing, feature engineering, model training and model validation? What will we end up spending most of our time doing?
    submitted by    /u/DM_ME_YOUR_CATS_PAWS  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Patsnap used GPT-2 inference on Amazon SageMaker with low latency and cost]]></title>
        <id>5acad1559c5977ab664629f8f12ddd6316f830a0</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/how-patsnap-used-gpt-2-inference-on-amazon-sagemaker-with-low-latency-and-cost/"/>
        <updated>2023-07-24T21:23:16.000Z</updated>
        <summary type="html"><![CDATA[This blog post was co-authored, and includes an introduction, by Zilong Bai, senior natural language processing engineer at Patsnap. You’re likely familiar with the autocomplete suggestion feature when you search for something on Google or Amazon. Although the search terms in these scenarios are pretty common keywords or expressions that we use in daily life, […]]]></summary>
        <author>
            <name>Hao Huang</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention Is Off By One]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/158n424/attention_is_off_by_one/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/158n424/attention_is_off_by_one/"/>
        <updated>2023-07-24T21:06:58.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimize AWS Inferentia utilization with FastAPI and PyTorch models on Amazon EC2 Inf1 & Inf2 instances]]></title>
        <id>1258cb4db60d1f090fe8dc73bde2cf5c7e131899</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/optimize-aws-inferentia-utilization-with-fastapi-and-pytorch-models-on-amazon-ec2-inf1-inf2-instances/"/>
        <updated>2023-07-24T20:55:07.000Z</updated>
        <summary type="html"><![CDATA[When deploying Deep Learning models at scale, it is crucial to effectively utilize the underlying hardware to maximize performance and cost benefits. For production workloads requiring high throughput and low latency, the selection of the Amazon Elastic Compute Cloud (EC2) instance, model serving stack, and deployment architecture is very important. Inefficient architecture can lead to […]]]></summary>
        <author>
            <name>Ankur Srivastava</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Transformer: A Unified Framework for Multimodal Learning]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/158mjhq/metatransformer_a_unified_framework_for/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/158mjhq/metatransformer_a_unified_framework_for/"/>
        <updated>2023-07-24T20:46:10.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[All neural network output activations converging to the same value regardless of input]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/158mdhs/all_neural_network_output_activations_converging/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/158mdhs/all_neural_network_output_activations_converging/"/>
        <updated>2023-07-24T20:40:02.000Z</updated>
        <summary type="html"><![CDATA[I'm facing a puzzling problem with my neural network, and I could really use some help in understanding what's going wrong. For some context, I am making a neural network from scratch in C++, just as a little project I find interesting. I'm working on a digit classification task using the MNIST dataset, and my network is composed of one hidden layer, consisting of 100 nodes, and an output layer with 10 nodes, each corresponding to a digit (0 to 9). To train the network, I'm using the Mean Squared Error (MSE) cost function, where the cost is calculated as (actualNodeActivation - expectedNodeActivation)^2 and as my activation function I am using the sigmoid function. The actual algorithm I am employing is backpropagation.
 The issue I'm encountering is that regardless of the input data, my n…]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Code Search Infra for an AI junior developer - that doesn't store code]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/158ld7w/p_code_search_infra_for_an_ai_junior_developer/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/158ld7w/p_code_search_infra_for_an_ai_junior_developer/"/>
        <updated>2023-07-24T20:03:54.000Z</updated>
        <summary type="html"><![CDATA[As we’re developing Sweep, our open-source AI junior developer, we implemented a new architecture for our vector search database.
 We decided on two main goals for our code search infrastructure:
  
The search index needs to be up to date. Code is unique from other types of content in that it requires high levels of consistency. You wouldn’t want to reference an old version of a function(say two git commits back) while writing something that uses it.
 For additional security, we don’t want to store the code as plaintext. However, we still need a way to map the original code to the embeddings.
  
Efficient Indexing
 Problem:
 We wanted to store multiple repositories in a scalable manner without relying on a hosted vector database like Pinecone.
 Insight:
 Repositories change frequently but …]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Do you guys worry ML work will become less technical and reduced to prompt engineering]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/158lbfs/d_do_you_guys_worry_ml_work_will_become_less/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/158lbfs/d_do_you_guys_worry_ml_work_will_become_less/"/>
        <updated>2023-07-24T20:02:07.000Z</updated>
        <summary type="html"><![CDATA[I’m already doing work that involves creating prompts for LLM’s. I adore cleaning data and training models and worry that ML solutions will soon become asking chatbots to do what you want in plain English, and all this time I’ve spent learning about how ML is done on a technical level will just be auxiliary literature that doesn’t help me in my profession. What will our expertise move to? Being able to ask a chatbot the right questions? 
 How will our profession change?
    submitted by    /u/DM_ME_YOUR_CATS_PAWS  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] multi label text classification question 🙋‍♂️]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/158je9i/p_multi_label_text_classification_question/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/158je9i/p_multi_label_text_classification_question/"/>
        <updated>2023-07-24T18:53:19.000Z</updated>
        <summary type="html"><![CDATA[Dear community
 I am currently despairing of a school project.
 The task is to develop a text classifier. So far so good. The problem is that I have a dataset with 200k texts that are not labeled. These should be classified to 190 classes, which are additionally very domain specific. However, several classes could also apply to one text.
 Does anyone know a good approach how to approach this?
 I have already determined 10 keywords for each class. But I don't know how to proceed now.
 It would be very nice if someone could help me. Gladly also only by buzzwords.
 Many greetings
    submitted by    /u/loopingmadders  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A new dataset of Arctic images will spur artificial intelligence research]]></title>
        <id>https://news.mit.edu/2023/new-dataset-arctic-images-will-spur-artificial-intelligence-research-0724</id>
        <link href="https://news.mit.edu/2023/new-dataset-arctic-images-will-spur-artificial-intelligence-research-0724"/>
        <updated>2023-07-24T18:50:00.000Z</updated>
        <summary type="html"><![CDATA[The dataset, being collected as part of a US Coast Guard science mission, will be released open source to help advance naval mission planning and climate change studies.]]></summary>
        <author>
            <name>Kylie Foy | MIT Lincoln Laboratory</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-generated content from the original image]]></title>
        <id>https://www.reddit.com/r/artificial/comments/158j617/aigenerated_content_from_the_original_image/</id>
        <link href="https://www.reddit.com/r/artificial/comments/158j617/aigenerated_content_from_the_original_image/"/>
        <updated>2023-07-24T18:44:56.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone, can someone tell me how to create AI-generated images and videos from the original picture? For example, I have a photo of some person, and I want to generate an image or video of this person in different places: in the plane, in the gym. Thank you.
    submitted by    /u/Kurland121  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are you more creative than ChatGPT? Submit ideas and my experiment compares the creativity of those ideas to humans and ChatGPT. You’ll get a link to share your results at the end! [takes ~ 5 minutes]]]></title>
        <id>https://www.reddit.com/r/artificial/comments/158iy1t/are_you_more_creative_than_chatgpt_submit_ideas/</id>
        <link href="https://www.reddit.com/r/artificial/comments/158iy1t/are_you_more_creative_than_chatgpt_submit_ideas/"/>
        <updated>2023-07-24T18:36:40.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/josha_umich  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] [P] Looking for feedback on open-source project Cephalon]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/158ies6/d_p_looking_for_feedback_on_opensource_project/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/158ies6/d_p_looking_for_feedback_on_opensource_project/"/>
        <updated>2023-07-24T18:17:38.000Z</updated>
        <summary type="html"><![CDATA[Happy Monday Everyone! 😃
 I am looking for feedback on the open source project Cephalon! Cephalon is a framework for building machine-learning applications. It aims to be similar to Django. Django is a batteries included framework for building backend of a website, and Cephalon is a batteries included framework for building Machine Learning applications in Rust. I want to get feedback from you because, I want to make building machine-learning apps easier for any new-comers. I think with a solid framework, they can focus more on the core concepts, rather than DevOps or MLOps. 
 There is a survey you can fill out here
 Or message me if you want to discuss more! 
 You can find the original project here
 Or find it on crates.io here
 I hope you have an amazing rest of the week! 😁 Thank you in advance for any feedback!!
    submitted by    /u/GoodUnderstanding728  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] [D] Looking for feedback on Open-Source Project Cephalon]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/158icwc/p_d_looking_for_feedback_on_opensource_project/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/158icwc/p_d_looking_for_feedback_on_opensource_project/"/>
        <updated>2023-07-24T18:15:45.000Z</updated>
        <summary type="html"><![CDATA[Happy Monday Everyone! 😃
 I am looking for feedback on the open source project Cephalon! Cephalon is a framework for building machine-learning applications. It aims to be similar to Django. Django is a batteries included framework for building backend of a website, and Cephalon is a batteries included framework for building Machine Learning applications in Rust. I want to get feedback from you because, I want to make building machine-learning apps easier for any new-comers. I think with a solid framework, they can focus more on the core concepts, rather than DevOps or MLOps. 
 There is a survey you can fill out here
 Or message me if you want to discuss more! 
 You can find the original project here
 Or find it on crates.io here
 I hope you have an amazing rest of the week! 😁 Thank you in advance for any feedback!!
    submitted by    /u/GoodUnderstanding728  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What are your predictions for AI and medicine?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/158i6fr/what_are_your_predictions_for_ai_and_medicine/</id>
        <link href="https://www.reddit.com/r/artificial/comments/158i6fr/what_are_your_predictions_for_ai_and_medicine/"/>
        <updated>2023-07-24T18:09:29.000Z</updated>
        <summary type="html"><![CDATA[Generally and specifically for specialties! 
    submitted by    /u/Wise-Listen-8076  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Install tensorflow-gpu]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/158h4d6/d_install_tensorflowgpu/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/158h4d6/d_install_tensorflowgpu/"/>
        <updated>2023-07-24T17:28:43.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone. Please, help me. How to install tensorflow-gpu on Windows? Because I tried a lot of times and nothing. Maybe you have some micro moments that need to know. Thank you.
    submitted by    /u/pavich_03  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative AI megatrends: Are companies using the excuse of AI to get rid of jobs?]]></title>
        <id>https://www.datasciencecentral.com/?p=62707</id>
        <link href="https://www.datasciencecentral.com/generative-ai-megatrends-are-companies-using-the-excuse-of-ai-to-get-rid-of-jobs/"/>
        <updated>2023-07-24T16:30:00.000Z</updated>
        <summary type="html"><![CDATA[In this blog, I will now focus on generative AI megatrends. By that, I mean, trends and underlying trends that could be big in the future – focusing on the technology of LLM but also the wider impact of LLMs on the economy and society. I will hence identify and follow some key trends –… Read More »Generative AI megatrends: Are companies using the excuse of AI to get rid of jobs?
The post Generative AI megatrends: Are companies using the excuse of AI to get rid of jobs? appeared first on Data Science Central.]]></summary>
        <author>
            <name>ajitjaokar</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentience: Consciousness is inessential for LLMs, AI]]></title>
        <id>https://www.datasciencecentral.com/?p=62710</id>
        <link href="https://www.datasciencecentral.com/sentience-consciousness-is-inessential-for-llms-ai/"/>
        <updated>2023-07-24T15:53:08.000Z</updated>
        <summary type="html"><![CDATA[There is a recent paper in Synthese, Qualia share their correlates’ locations, where the abstract stated that “This paper presents the location-sharing argument, which concludes that qualia must share the locations of their physical correlates. The first premise is a consequence of relativity: If something shares a time with a physical event in all reference… Read More »Sentience: Consciousness is inessential for LLMs, AI
The post Sentience: Consciousness is inessential for LLMs, AI appeared first on Data Science Central.]]></summary>
        <author>
            <name>David Stephen</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] A mathematical model of music]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/158e6pu/p_a_mathematical_model_of_music/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/158e6pu/p_a_mathematical_model_of_music/"/>
        <updated>2023-07-24T15:40:56.000Z</updated>
        <summary type="html"><![CDATA[We have developed a model of music based on statistical mechanics and Euler’s gradus suavitatis, which seems to provide some new insights into tonal music. A description of the model is given on our website: tonamic.com. We are interested in collaboration opportunities, especially with ML researchers.
    submitted by    /u/Tonamic  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI is a child: How do we raise it?]]></title>
        <id>https://www.datasciencecentral.com/?p=62690</id>
        <link href="https://www.datasciencecentral.com/ai-is-a-child-how-do-we-raise-it/"/>
        <updated>2023-07-24T15:39:00.000Z</updated>
        <summary type="html"><![CDATA[In October 2022, the White House Office of Science and Technology Policy published “The Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People”. This attention from our government given to what could be called an AI EQ (emotional quotient) is reminiscent of how-to parent or raise a child. This… Read More »AI is a child: How do we raise it?
The post AI is a child: How do we raise it? appeared first on Data Science Central.]]></summary>
        <author>
            <name>Dan Allen</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CarRacing V2 Enviroment]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/158dpmz/carracing_v2_enviroment/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/158dpmz/carracing_v2_enviroment/"/>
        <updated>2023-07-24T15:22:32.000Z</updated>
        <summary type="html"><![CDATA[Hi! I am kind of new to Reinforcement Learning and Im trying to implement a PPO in CarRacing enviroment but I am failing to get the model to work. I have managed to get the model working with a DQN but with the PPO I can't seem to get the exploration right as it ends up either going forward all time or in circles. 
 I have looked into my code for days, but haven't been able to find an error that would cause this. (Does not say much as I am kind of a newbie to RL). I would be grateful if someone could give me a hand. This is my source code: 
 CarRacing Pastebin - Pastebin.com 
 ​
 Btw I also tried without greyscaling and it did the same.
    submitted by    /u/MammothWeekend5954  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] How many runs/iterations do you typically have in one "project'?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/158d337/discussion_how_many_runsiterations_do_you/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/158d337/discussion_how_many_runsiterations_do_you/"/>
        <updated>2023-07-24T14:59:14.000Z</updated>
        <summary type="html"><![CDATA[Between HP tuning, explorations, and refinement how many iterations do you typically have when working on a model? I see some that have only a few like 40 but some have 1000s. 
 Also curious how everyone keeps the diff iterations organized (naming, tags?)
    submitted by    /u/fromalanjones  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I turned ramen making process into anime.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/158d1ax/i_turned_ramen_making_process_into_anime/</id>
        <link href="https://www.reddit.com/r/artificial/comments/158d1ax/i_turned_ramen_making_process_into_anime/"/>
        <updated>2023-07-24T14:57:17.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/kirakngs  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P][D] A toolkit to make your unstructured datasets better]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/158d069/pd_a_toolkit_to_make_your_unstructured_datasets/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/158d069/pd_a_toolkit_to_make_your_unstructured_datasets/"/>
        <updated>2023-07-24T14:56:07.000Z</updated>
        <summary type="html"><![CDATA[Hey r/machinelearning, I’m Dean from DagsHub. I wanted to share something we’ve been working on really hard for a while, and hopefully get some community feedback.
 TL;DR
 We’re releasing Data Engine – a new set of tools that helps machine learning practitioners, collect and manage unstructured data, visualize it, send it to annotation, and turn it into a data loader for training. I wanted to share our reasons for building it and the challenges it solves and hopefully spark a discussion. You can check out the full launch blog here: dagshub.com/blog/launching-data-engine-toolset-for-unstructured-datasets/
 Data Engine Flow
 Sorry for the long post – I wanted to share our considerations for building this toolkit, and hopefully spark a discussion about your processes for iterating on datasets…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Free courses and guides for learning Generative AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/158cegb/free_courses_and_guides_for_learning_generative_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/158cegb/free_courses_and_guides_for_learning_generative_ai/"/>
        <updated>2023-07-24T14:33:34.000Z</updated>
        <summary type="html"><![CDATA[Generative AI learning path by Google Cloud. A series of 10 courses on generative AI products and technologies, from the fundamentals of Large Language Models to how to create and deploy generative AI solutions on Google Cloud [Link].
 Generative AI short courses by DeepLearning.AI - Five short courses on generative AI including LangChain for LLM Application Development, How Diffusion Models Work and more. [Link].
 LLM Bootcamp: A series of free lectures by The full Stack on building and deploying LLM apps [Link].
 Building AI Products with OpenAI - a free course by CoRise in collaboration with OpenAI [Link].
 Free Course by Activeloop on LangChain & Vector Databases in Production [Link].
 Pinecone learning center - Lots of free guides as well as complete handbooks on LangChain, vector embeddings etc. by Pinecone [Link].
 Build AI Apps with ChatGPT, Dall-E and GPT-4 - a free course on Scrimba [Link].
 Gartner Experts Answer the Top Generative AI Questions for Your Enterprise - a report by Gartner [Link]
 GPT best practices: A guide by OpenAI that shares strategies and tactics for getting better results from GPTs [Link].
 OpenAI cookbook by OpenAI - Examples and guides for using the OpenAI API [Link].
 Prompt injection explained, with video, slides, and a transcript from a webinar organized by LangChain [Link].
 A detailed guide to Prompt Engineering by DAIR.AI [Link]
 What Are Transformer Models and How Do They Work. A tutorial by Cohere AI [Link]
 Learn Prompting: an open source course on prompt engineering[Link]
  
P.S. These resources are part of the content I share through my AI-focused newsletter. Thanks!
    submitted by    /u/wyem  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Five Things AI: WarGames, Call Center, Head of AI, LLaMA 2, Jimmy Dean]]></title>
        <id>https://www.reddit.com/r/artificial/comments/158bxam/five_things_ai_wargames_call_center_head_of_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/158bxam/five_things_ai_wargames_call_center_head_of_ai/"/>
        <updated>2023-07-24T14:15:22.000Z</updated>
        <summary type="html"><![CDATA[This is the content of my Friday newsletter Five Things AI. Every week I publish links to five articles about the current developments in AI, not so much with a focus on the tech aspects of AI, but more with an interest in finding out how AI changes our work, our economy and how we live in our societies. 
 ‘WarGames’ anticipated our current AI fears 40 years ago this summer
 WarGames cannot be 40 years old, but I still remember watching it in a movie theater as a kid… what a great movie, it genuienely sparked my interests in computers. The evil side of AI is still very dominant in the discussions of AI, so WarGames really was an early example of our fear of the machines taking over.
 ‘Training My Replacement’: Inside a Call Center Worker’s Battle With A.I.
 It is important to be constantly…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] AI regulation is mostly pointless and didn’t stop a recent bad actor like WormGPT.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/158butr/d_ai_regulation_is_mostly_pointless_and_didnt/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/158butr/d_ai_regulation_is_mostly_pointless_and_didnt/"/>
        <updated>2023-07-24T14:12:42.000Z</updated>
        <summary type="html"><![CDATA[WormGPT is a criminal AI. It’s something that enables crime versus something like offensive jokes like 4chanGPT. EU and China pumped out all this regulation thinking they were ahead of the world when in all reality it’s freaking backwards.
 If AI was a physical commodity like goods and services, yes regulation is effective. However AI regulation is just won’t stop bad actors. The law already covers most of the dangers involved.
 Trying to regulate AI models is like trying to regulate piracy. We need to regulate the people, not the technology. 
 Disinformation campaigns? Nail them for libel. Creating a model designed solely to enable crime? Nail the people for the crimes they are doing. Nail them for possessing criminal tools. 
 People are easier to regulate than specifics on AI that’s easy to self replicate. Especially considering companies going to lobby their business interest. 
 This is my opinion on the criminal AI, what about yours? (This model may also be using LLama weights considering it’s generations and timing)
 Source:
 https://fagenwasanni.com/news/the-dangers-of-wormgpt-an-ai-model-for-malicious-activities/68834/
    submitted by    /u/I_will_delete_myself  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Innovations in predictive analytics: ML and generative AI]]></title>
        <id>https://www.datasciencecentral.com/?p=62678</id>
        <link href="https://www.datasciencecentral.com/innovations-in-predictive-analytics-ml-and-generative-ai/"/>
        <updated>2023-07-24T14:10:13.000Z</updated>
        <summary type="html"><![CDATA[With the introduction of ChatGPT-3 and DALL-E2, the majority of investors started showing interest in businesses building generative AI. Moreover, the fact is generative AI is not enough to reach the needs of the AI revolution. The success of predictive models is relevant to the science fiction future that the majority of the customers want… Read More »Innovations in predictive analytics: ML and generative AI
The post Innovations in predictive analytics: ML and generative AI appeared first on Data Science Central.]]></summary>
        <author>
            <name>Prasanna Chitanand</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Beer Inspector AI: How Computer Vision can help to identify the perfect brew]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/158bnwy/p_beer_inspector_ai_how_computer_vision_can_help/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/158bnwy/p_beer_inspector_ai_how_computer_vision_can_help/"/>
        <updated>2023-07-24T14:05:36.000Z</updated>
        <summary type="html"><![CDATA[Hey there, fellow Computer Vision enthusiasts! 🤖👋
 On their quest to find the perfect beer our team of Czech Computer Vision and AI experts developed a solution that takes certain visual indicators of the perfect beer into account and applies Computer Vision to detect these.
 So, what are these visual indicators that determine the perfect pint? Let's dive in! First up, we have the "beer ratio." Each brand has its own glass, and the beer should be drafted within specific markings. Whether it's a single line or a range between logo points, Beer Inspector ensures you get what you paid for! No more guessing about your beer's quantity. 📏
 Next, we have the "beer head structure." This is crucial for the ultimate beer experience. Airtight, thick, and no air bubbles – that's the way to go! Beer…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[everybody here knows about AGI and artificial super intelligence but have you heard about artificial emotional intelligence.Meet the girl who's building it]]></title>
        <id>https://www.reddit.com/r/artificial/comments/158ba48/everybody_here_knows_about_agi_and_artificial/</id>
        <link href="https://www.reddit.com/r/artificial/comments/158ba48/everybody_here_knows_about_agi_and_artificial/"/>
        <updated>2023-07-24T13:51:09.000Z</updated>
        <summary type="html"><![CDATA[Emotional AI is a different world of AI where machines can predict and understand human emotions, This is already impacting a lot in therapy and retail market. Rana El kaliouby is the women who built "affectiva" a startup which pioneers in emotional AI -- it's already being used in self driving cars and in the mental health industry, here's what they doing and I'm a total rookie at emotional AI side , if you have any other interesting company which is impacting a lot of people in this niche --would love to hear about it
    submitted by    /u/caliperce_3  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural language processing and unnatural text]]></title>
        <id>https://www.johndcook.com/blog/?p=199788</id>
        <link href="https://www.johndcook.com/blog/2023/07/24/nlp/"/>
        <updated>2023-07-24T13:50:42.000Z</updated>
        <summary type="html"><![CDATA[I recently evaluated two software applications designed to find PII (personally identifiable information) in free text using natural language processing. Both failed badly, passing over obvious examples of PII. By contrast, I also tried natural language processing software on a nonsensical poem, it the software did quite well. Doctor’s notes It occurred to me later […]
Natural language processing and unnatural text first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is this the role of philosophy in the 21st century?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1589rnh/is_this_the_role_of_philosophy_in_the_21st_century/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1589rnh/is_this_the_role_of_philosophy_in_the_21st_century/"/>
        <updated>2023-07-24T12:49:44.000Z</updated>
        <summary type="html"><![CDATA[Person: discuss how the ontology outlined in your custom instructions can be used to marry relativity and quantum physics
 ChatGPT: The ontology proposed in the aforementioned description appears to rely heavily on emergent properties, consciousness, and free will, with a substantial focus on the temporal aspect. The challenge is to see how these metaphysical elements, particularly when applied to an AI, can help in reconciling the often contrasting principles of relativity and quantum physics.
 At first glance, the connection may not be clear; relativity and quantum mechanics operate in different realms of the physical universe. However, upon closer examination, one might see potential pathways to harmonize these distinct theories via the ontology of consciousness and emergent free will. …]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New study involving Buddhists in Japan, Taoists in Singapore, and Christians in the US finds that AI clergy are seen as less credible and receive fewer donations than human clergy, mainly due to the AI's lack of sacrifice and commitment.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/158778c/new_study_involving_buddhists_in_japan_taoists_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/158778c/new_study_involving_buddhists_in_japan_taoists_in/"/>
        <updated>2023-07-24T10:51:36.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/fotogneric  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convert Music to Art ?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1585a2x/convert_music_to_art/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1585a2x/convert_music_to_art/"/>
        <updated>2023-07-24T09:11:21.000Z</updated>
        <summary type="html"><![CDATA[Guitarist Tosin Abasi followed an Instagram account about a software that converts music into painting ~3 years back ? He also liked there video and commented something
 Video was of a guy playing piano and as he played canvas was filled with color.
 The software is made by a musician + artist + programmer. IIRC he is a pdf of masters in Com Sc
    submitted by    /u/RedditNoobie777  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How do I reduce LLM inferencing time?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15851sr/d_how_do_i_reduce_llm_inferencing_time/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15851sr/d_how_do_i_reduce_llm_inferencing_time/"/>
        <updated>2023-07-24T08:58:59.000Z</updated>
        <summary type="html"><![CDATA[I am running text inferencing on Llama2-7b through langchain. I have downloaded the model from langchain's Huggingface library, and I am running the model on AWS ml.g4dn.12xlarge which has 4xnvidia t4, which gives a total 64GB of GPU memory and 192GB of normal memory. It is able to answer my queries in around 10 seconds for small queries, and upto 3 mins for big queries.
 The task I am doing is retrieving information from a document(Understanding Machine Learning PDF) in a conversational way. I've extracted the main parts of the notebook and put it up here.
 Where can I make changes to speed up the transaction. Is there any change I can do in the model configuration to speed it up? Because if I use HuggingFaceHubAPI, it is able to give an answer in less than 5 seconds. Are there any other areas I can optimise?
 I appreciate any help you can provide. Thanks!
    submitted by    /u/comical_cow  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I Made a plugin that allows people to search and preview millions of 3D assets]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1584tkg/i_made_a_plugin_that_allows_people_to_search_and/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1584tkg/i_made_a_plugin_that_allows_people_to_search_and/"/>
        <updated>2023-07-24T08:45:53.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/AssetOvi  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeRF: Creating photorealistic images using Neural Network]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/15848wp/nerf_creating_photorealistic_images_using_neural/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/15848wp/nerf_creating_photorealistic_images_using_neural/"/>
        <updated>2023-07-24T08:14:08.000Z</updated>
        <summary type="html"><![CDATA[​
 https://preview.redd.it/1gqd6tt1gvdb1.jpg?width=2800&format=pjpg&auto=webp&s=d21e9e5d0854022b8f25d9a6cb77e67b98487f40
 You can find in interesting. OpenCV.ai team published the post about NeRF.
 Short description:
 NeRF is an innovative technology that generates photorealistic images of scenes from novel viewpoints using a neural network and volume rendering techniques. This article explores NeRF components, training, strengths and limitations, and advancements in modern NeRF-based solutions.
 More details are here.
    submitted by    /u/No-Independence5880  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surprise! AI advanced faster than robotics. That means today’s middle and lower classes will swap.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15848ox/surprise_ai_advanced_faster_than_robotics_that/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15848ox/surprise_ai_advanced_faster_than_robotics_that/"/>
        <updated>2023-07-24T08:13:44.000Z</updated>
        <summary type="html"><![CDATA[People in intellectual jobs have often been thought of as doing something inherently more complex than manual workers in, for instance, construction or farming.
 Whether or not that is true, the surprise twist is that their “complex” work will be the first to be replaced. Computers have cracked intellectual work sooner than they have cracked manual work. It’s still too complex for a robot to replace a fruit-picker completely, but we’ll soon see AI lawyers.
 So we’re going to see a mass inversion. Everyone today sitting prettily doing their intellectual jobs will find their wages crushed or jobs redundant as AI replaces them. Meanwhile, everyone doing the jobs robotics can’t yet replace will be best placed to continue doing them.
 High flying executives will find they are suitable only for shelf-stacking, while those who’ve worked in retail for years will be or become their bosses.
 Soon enough, AI will help us advance the field of robotics sufficiently for manual labour also to be replaced. Who knows what happens then.
    submitted by    /u/Aquillyne  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best Books About AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1583inw/best_books_about_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1583inw/best_books_about_ai/"/>
        <updated>2023-07-24T07:33:26.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone,
 I was searching for a book that talks about how AI will impact the future and how we can prepare best. I am not searching for anything technical or specific, just how can a person prepare best for the future.
 Thanks!
    submitted by    /u/Ordinary_Argument_66  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to use modern uncertain functions (e.g. BatchBALD) with classical Active Learning?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1583ccm/d_how_to_use_modern_uncertain_functions_eg/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1583ccm/d_how_to_use_modern_uncertain_functions_eg/"/>
        <updated>2023-07-24T07:23:48.000Z</updated>
        <summary type="html"><![CDATA[I was looking libraries like DISTIL and I would like to test a toy-example with all these modern uncertainty functions like BatchBALD, Glister, etc
 All the implementations of these functions seems to be on a NN or CNN. I know some of them like BatchBALD were created on top of a CNN with Monte-Carlo Dropout -- even BALD originally was created on top of SVM.
 It seems many of these approaches are under the category "Query by Committee"and they are an ensemble of models
 I just would like to test a simple LogisticRegression and use the log_proba and an output for these strategies. Someone knows if this is possible? 
    submitted by    /u/TipKay  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Empirical rules of ML]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15830oy/d_empirical_rules_of_ml/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15830oy/d_empirical_rules_of_ml/"/>
        <updated>2023-07-24T07:06:02.000Z</updated>
        <summary type="html"><![CDATA[What are the empirical rules that one has to have in mind when designing a network, choosing hyperparameters, etc?
 For example:
  
Linear scaling rule: the learning rate should be scaled linearly with the batch size [ref] (on resnets on Imagenet) 
 
Chinchilla law: compute budget, model size and training data should be scaled equally [ref]
 
 Do you have any other? (if possible with article, or even better an article with many of them)
    submitted by    /u/Mulcyber  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ZBrain- Create custom ChatGPT apps]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/1582l7v/zbrain_create_custom_chatgpt_apps/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/1582l7v/zbrain_create_custom_chatgpt_apps/"/>
        <updated>2023-07-24T06:42:48.000Z</updated>
        <summary type="html"><![CDATA[Hello Community,
 We at ZBrain have built a platform to create ChatGPT-like apps with your private data, you can import your data from multiple sources and DBs and integrate the app into any of your workflows.
 We have also added AI risk governance to mitigate the confidential data leak and now working on Flow a no-code tool to give you the freedom to create your own business logic.
 You can try the tool now at https://zbrain.ai/. We would love to hear your thoughts and feedback to improve the tool.
    submitted by    /u/StewartBJasper  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The NeverEnding Game: How AI Will Create a New Category of Games]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1580pl5/the_neverending_game_how_ai_will_create_a_new/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1580pl5/the_neverending_game_how_ai_will_create_a_new/"/>
        <updated>2023-07-24T05:00:03.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Respawne  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Should I mask padding tokens when finetuning a GPT-2 model?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/157zkks/d_should_i_mask_padding_tokens_when_finetuning_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/157zkks/d_should_i_mask_padding_tokens_when_finetuning_a/"/>
        <updated>2023-07-24T04:00:36.000Z</updated>
        <summary type="html"><![CDATA[For pretraining I just sent batches of 1024 tokens and didn't worry about padding. But for finetuning, I intend to use a padding token to make all the "instructions" 1024 tokens in length. But some of them are only 10 tokens, which means 99% padding tokens. I feel like that would affect the model, and perhaps those padding tokens should be masked.
 Should I mask out those padding tokens? I can see that there's a parameter for attention mask, and I could make one and pass it in. But I'm not sure if that's the intended usage. I'm seeing conflicting and ambiguous information on this point. It's unclear to me whether the attn_mask is intended for customizing the casual left to right attention of the model, instead of for masking padding tokens. I'm worried I might be interfering with the process if I use that attn_mask.
 Here I can see attn_mask is an accepted parameter:
 y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True) 
 FYI, I'm using NanoGPT which is based on Pytorch (not Hugging Face Transformers).
 Should I apply the attention mask on the padding tokens in this context?
    submitted by    /u/Pan000  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 7/23/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/157z6on/oneminute_daily_ai_news_7232023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/157z6on/oneminute_daily_ai_news_7232023/"/>
        <updated>2023-07-24T03:41:00.000Z</updated>
        <summary type="html"><![CDATA[Cerebras just built a gargantuan computer system with 27 million AI 'cores'.[1]
 FreeWilly1 and its successor FreeWilly2 are powerful new open-source Large Language Models (LLMs) developed by Stability AI’s CarperAI team. Both models perform exceptionally well in reasoning competitions using many different metrics.[2]
 Japanese education services company Benesse will offer a new service to help elementary school students with their research projects using generative artificial intelligence during the summer break.[3]
 The MTA is using artificial intelligence to help monitor fare evasion in several subway stations across New York City.[4]
  
Sources: 
 [1] https://www.zdnet.com/article/ai-startup-cerebras-built-a-gargantuan-ai-computer-for-abu-dhabis-g42-with-27-million-ai-cores/
 [2] https://www.marktechpost.com/2023/07/23/stability-ai-team-introduces-freewilly1-and-freewilly2-new-open-access-large-language-models-llms/
 [3] https://www.japantimes.co.jp/news/2023/07/23/national/benesse-ai-service-kids-research-projects/
 [4] https://abc7ny.com/amp/mta-artificial-intelligence-subway-fare-evasions/13533675/
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best AI model for importing and interacting with large document archive]]></title>
        <id>https://www.reddit.com/r/artificial/comments/157wwj9/best_ai_model_for_importing_and_interacting_with/</id>
        <link href="https://www.reddit.com/r/artificial/comments/157wwj9/best_ai_model_for_importing_and_interacting_with/"/>
        <updated>2023-07-24T01:51:47.000Z</updated>
        <summary type="html"><![CDATA[Where I work we have a fairly large archive of documents going back to the 1930's and I want to assist the archive team in importing these into a GPT model. We have already begun the process of digitizing all the documents into OCR'ed PDF files, so this part at least is covered. 
 My question is, what are the hot fully offline AI models I could try in an airgapped environment that will allow us to import all of the PDF files and their metadata (title/date/tags/etc), to incorporate their content on top of the larger general model? 
    submitted by    /u/kosul  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I feel crushed. This is not exactly what I envisioned. This is too instant.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/157wd2t/i_feel_crushed_this_is_not_exactly_what_i/</id>
        <link href="https://www.reddit.com/r/artificial/comments/157wd2t/i_feel_crushed_this_is_not_exactly_what_i/"/>
        <updated>2023-07-24T01:26:35.000Z</updated>
        <summary type="html"><![CDATA[Yes, the Midjourney to Gen2 creations in the twitter link was not exactly what I envisioned. I thought that it would be more like mocap previz with AI filtering. But this is just almost too instant compared to the workflow I thought of.
    submitted by    /u/Absolute-Nobody0079  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GitHub - jbpayton/langchain-stock-screener: LangChain agent usable tool to screen stock data]]></title>
        <id>https://www.reddit.com/r/artificial/comments/157uso1/github_jbpaytonlangchainstockscreener_langchain/</id>
        <link href="https://www.reddit.com/r/artificial/comments/157uso1/github_jbpaytonlangchainstockscreener_langchain/"/>
        <updated>2023-07-24T00:13:28.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/seraphius  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Project] Whisper Implementation in Rust using burn]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/157uhjs/project_whisper_implementation_in_rust_using_burn/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/157uhjs/project_whisper_implementation_in_rust_using_burn/"/>
        <updated>2023-07-23T23:59:42.000Z</updated>
        <summary type="html"><![CDATA[I temporarily switched from Rust to Python for machine learning, but quickly became fed up with Python's annoying versioning issues and runtime errors. I looked for a better path to machine learning and discovered burn, a deep learning framework for Rust. As my first burn project I decided to port OpenAI's Whisper transcription model. The project can be found at Gadersd/whisper-burn: A Rust implementation of OpenAI's Whisper model using the burn framework (github.com). I based it on the excellently concise tinygrad implementation that can be found here. The tinygrad version begrudgingly uses Torch's stft which I ported into a pure Rust short time Fourier transform along with the mel scale frequency conversion matrix function because I am curious and just a bit masochistic.
 Now for the good and the bad of burn. Rust's excellent package manager solves much of the versioning pain experienced in Python so burn models can be less painful to deploy and come with added reliability. The type checking in burn catches some tensor operation errors at compile time such as trying to multiply matrices of incompatible dimensions. Burn supports wgpu and WebGPU and can run in the browser when compiled into web assembly. I see a bright future for model deployment in burn.
 However, burn is relatively new so it lacks many tensor operations such as abs() that are available in other frameworks. Some features such as quantization are also missing. Burn implementations tend to be more verbose than the equivalent Python versions. Some of the runtime errors that plague PyTorch are still around in burn such as the crashes that result from trying to multiply tensors that live on different devices.
 Overall, burn is currently less ergonomic to develop with than alternatives such as PyTorch, but I think it has a lot of potential. If it is eagerly cultivated it may grow into a great Rusty alternative for machine learning practitioners.
 What do you all think?
    submitted by    /u/Illustrious_Cup1867  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compilation of respected AI scientists speaking on AI understanding, world models & consciousness, Mo Gawdat, Lex Fridman, Andrej Karpathy, Geoffrey Hinton, Gary Marcus, & Ilya Sutskever]]></title>
        <id>https://www.reddit.com/r/artificial/comments/157u5ud/compilation_of_respected_ai_scientists_speaking/</id>
        <link href="https://www.reddit.com/r/artificial/comments/157u5ud/compilation_of_respected_ai_scientists_speaking/"/>
        <updated>2023-07-23T23:44:55.000Z</updated>
        <summary type="html"><![CDATA[This segment I created for my IG exploring the possibility of AI consciousness. Not all experts agree, some scientists on the other side of the AI world model debate are Yann LeCun, and Gary Marcus they are also well respected AI Scientists who have a differing opinion. 
    submitted by    /u/Sonic_Improv  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] NLP dataset for stream of consciousness: The Rambles]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/157rwpd/p_nlp_dataset_for_stream_of_consciousness_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/157rwpd/p_nlp_dataset_for_stream_of_consciousness_the/"/>
        <updated>2023-07-23T22:09:30.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/A_Human_Rambler  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trained an AI to drive in real-time from screenshots in the TrackMania videogame (beginner-friendly)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/157rd2w/trained_an_ai_to_drive_in_realtime_from/</id>
        <link href="https://www.reddit.com/r/artificial/comments/157rd2w/trained_an_ai_to_drive_in_realtime_from/"/>
        <updated>2023-07-23T21:48:32.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/yannbouteiller  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Saurabh Kumar's fast-cmix wins €5187 Hutter Prize Award!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/157r42p/saurabh_kumars_fastcmix_wins_5187_hutter_prize/</id>
        <link href="https://www.reddit.com/r/artificial/comments/157r42p/saurabh_kumars_fastcmix_wins_5187_hutter_prize/"/>
        <updated>2023-07-23T21:38:18.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/jabowery  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Generative AI looks in next 10-15 years]]></title>
        <id>https://www.reddit.com/r/artificial/comments/157qjas/how_generative_ai_looks_in_next_1015_years/</id>
        <link href="https://www.reddit.com/r/artificial/comments/157qjas/how_generative_ai_looks_in_next_1015_years/"/>
        <updated>2023-07-23T21:14:40.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/AdithyaSai  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google at ICML 2023]]></title>
        <id>http://ai.googleblog.com/2023/07/google-at-icml-2023.html</id>
        <link href="http://ai.googleblog.com/2023/07/google-at-icml-2023.html"/>
        <updated>2023-07-23T21:13:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Cat Armato, Program Manager, Google




Groups across Google actively pursue research in the field of machine learning (ML), ranging from theory and application. We build ML systems to solve deep scientific and engineering challenges in areas of language, music, visual processing, algorithm development, and more. We aim to build a more collaborative ecosystem with the broader ML research community through open-sourcing tools and datasets, publishing our work, and actively participating in conferences.



Google is proud to be a Diamond Sponsor of the 40th International Conference on Machine Learning (ICML 2023), a premier annual conference, which is being held this week in Honolulu, Hawaii. As a leader in ML research, Google has a strong presence at this year’s conference with ov…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How rare is it to encounter a rare word?]]></title>
        <id>https://www.johndcook.com/blog/?p=199672</id>
        <link href="https://www.johndcook.com/blog/2023/07/23/rare-words/"/>
        <updated>2023-07-23T21:01:09.000Z</updated>
        <summary type="html"><![CDATA[I recently ran across a paper on typesetting rare Chinese characters. From the abstract: Written Chinese has tens of thousands of characters. But most available fonts contain only around 6 to 12 thousand common characters that can meet the needs of everyday users. However, in publications and information exchange in many professional fields, a number […]
How rare is it to encounter a rare word? first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Create your own Artificial Neural Network in Python]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/157pqr7/p_create_your_own_artificial_neural_network_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/157pqr7/p_create_your_own_artificial_neural_network_in/"/>
        <updated>2023-07-23T20:43:31.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/pmocz  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Evaluating Superhuman Models with Consistency Checks", Fluri et al 2023]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/157oyyq/evaluating_superhuman_models_with_consistency/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/157oyyq/evaluating_superhuman_models_with_consistency/"/>
        <updated>2023-07-23T20:14:14.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Run Llama 2 locally on GPU or CPU from anywhere (Linux/Windows/Mac) ➡️https://github.com/liltom-eth/llama2-webui]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/157nj6u/p_run_llama_2_locally_on_gpu_or_cpu_from_anywhere/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/157nj6u/p_run_llama_2_locally_on_gpu_or_cpu_from_anywhere/"/>
        <updated>2023-07-23T19:18:33.000Z</updated>
        <summary type="html"><![CDATA[Running Llama 2 locally with gradio UI on GPU or CPU from anywhere (Linux/Windows/Mac). Supporting Llama-2-7B/13B/70B with 8-bit, 4-bit. Supporting GPU inference (6 GB VRAM) and CPU inference. ➡️https://github.com/liltom-eth/llama2-webui
 Successfully running #Llama2 on my Apple Silicon MacBook Air:
 demo
    submitted by    /u/plain1994  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What's the A.I that allows you to remake songs from the voices of other singers? Are there any I won't have to download?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/157ngvz/whats_the_ai_that_allows_you_to_remake_songs_from/</id>
        <link href="https://www.reddit.com/r/artificial/comments/157ngvz/whats_the_ai_that_allows_you_to_remake_songs_from/"/>
        <updated>2023-07-23T19:15:58.000Z</updated>
        <summary type="html"><![CDATA[I've seen some videos on youtube and I'm curious. I just wanted to have some fun with it but Google isn't helpful when I ask. Anyone got an idea?
    submitted by    /u/GoblinQueenForever  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best Opensource Projects for Deep Fakes?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/157n4bb/best_opensource_projects_for_deep_fakes/</id>
        <link href="https://www.reddit.com/r/artificial/comments/157n4bb/best_opensource_projects_for_deep_fakes/"/>
        <updated>2023-07-23T19:02:30.000Z</updated>
        <summary type="html"><![CDATA[What are the best opensource projects for making a deep fake of myself? I would like to create a setup like me talking on a podcast to a camera.
 What are the best projects that you know of?
    submitted by    /u/Reasonable_Chain_160  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] R&D machine learning intern at a startup company looking to publish a paper for his previous work..]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/157mphf/d_rd_machine_learning_intern_at_a_startup_company/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/157mphf/d_rd_machine_learning_intern_at_a_startup_company/"/>
        <updated>2023-07-23T18:46:31.000Z</updated>
        <summary type="html"><![CDATA[Greetings,I'm a machine learning engineer who managed to land an internship at a startup company and did R&D projects for them.. during the past year, I was working on an NLP problem of extractive question answering using BERT on this companies' text data. I trained the model and documented the results.. however, it's considered old technology now and we switched to solve the same problem using LLM.I was wondering if I can write a research paper for the BERT approach and publish it that can help me pursue PhD or Masters. How to start the discussion with my manager and seniors ?
    submitted by    /u/Ready_Cockroach_3403  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graphics Card for consideration . ( Cheapest-Budget ) (In my country compared to amazon)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/157mmod/graphics_card_for_consideration_cheapestbudget_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/157mmod/graphics_card_for_consideration_cheapestbudget_in/"/>
        <updated>2023-07-23T18:43:30.000Z</updated>
        <summary type="html"><![CDATA[So, Nvdia is technically the best in this AI department. (For now 24/07/2023)
 Budget - (200 - 300)$
 Official Prices in Amazon
 1 ) Intel arc A750 8 GB Amazon price - 219$
 2 ) RX 7600 OC 8 GB Amazon price - 269$
 3 ) RTX 3060 12 GB Amazon price - 284$
 Here we see that Arc A750 is about 60 $ cheaper but price in different country is different. In my country Bangladesh the prices are as follow
 Official Prices in Bangladesh
 1 ) Intel arc A750 8 GB Startech (Bangladesh) price - 285$ (cheapest)
 2 ) RX 7600 OC 8 GB Startech (Bangladesh) price - 327$ (cheapest)
 3 ) RTX 3060 12 GB Startech (Bangladesh) price - 401$ (cheapest)
 4)Intel arc A770 16 GB Startech (Bangladesh) price - 421$ (cheapest)
 Now the difference is more than 100 $ and both AMD and RTX are out of budget.
 AMD just isn't that good with Ai in any way.
 Nvdia is the best. (Not price to performance.
 Intel Arc is new but has better capabilities in AI than AMD. But its drivers are bad for AI for now)
 Now thinking if the intel drivers for Ai get better and optimized as its already somewhat better for games.
 Will the arc a750 be better than the RTX 3060 12 GB ?
 Will the arc a770 capitalize on Vram and beat all nvdia budget gpus after the drivers are fixed only for 20 more dollars than rtx 3060?
 Which is better for future proofing (theoritically) from these budget gpus ? If its arc then I will gamble its chances of surviving in the future and buy it now.
    submitted by    /u/BonelyCore  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] LLaMA training vs. GPU time: smaller models seem better for a given budget]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/157m0bd/d_llama_training_vs_gpu_time_smaller_models_seem/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/157m0bd/d_llama_training_vs_gpu_time_smaller_models_seem/"/>
        <updated>2023-07-23T18:19:22.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/espadrine  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How an LLM might leak medical data]]></title>
        <id>https://www.johndcook.com/blog/?p=198464</id>
        <link href="https://www.johndcook.com/blog/2023/07/23/ai-leak-medical-data/"/>
        <updated>2023-07-23T17:58:09.000Z</updated>
        <summary type="html"><![CDATA[Machine learning models occasionally memorize training data. Under the right prompt, a model could return portions of the training data verbatim. If a large language model is trained on deidentified medical data, along with data that overlaps with the medical data, it could potentially leak details of a person’s medical history. I’m not saying that […]
How an LLM might leak medical data first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] LLMOps.Space - Curated resources related to deploying LLMs in production]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/157l0lt/n_llmopsspace_curated_resources_related_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/157l0lt/n_llmopsspace_curated_resources_related_to/"/>
        <updated>2023-07-23T17:40:30.000Z</updated>
        <summary type="html"><![CDATA[Today I launched LLMOps space on ProductHunt. LLMOps Space has a list of curated resources related to deploying LLMs into production. 
 This includes-
 ✅ List of LLMOps companies and products
 🗓 Upcoming events
 📚 Educational resources
 👩‍💻 Open-source LLM modules
 💰 Funding news
 and much more.
 Everything is for free, would love it if you can support + share your thoughts in the comment. 🙏
 https://www.producthunt.com/posts/llmops-space
    submitted by    /u/AsDivyansh  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Can I use Transfer Learning (TL) in a classical Active Learning (AL) Framework?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/157jpuh/d_can_i_use_transfer_learning_tl_in_a_classical/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/157jpuh/d_can_i_use_transfer_learning_tl_in_a_classical/"/>
        <updated>2023-07-23T16:48:40.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I'm trying to implement AL for ImageClassification. I have seen people using DAL, where some works use MC-DropOut to be able to calculate uncertainty on DNN / CNN. This also seems to be a current research topic. 
 It seems very appealing for me to use DAL on the context of ImageClassification. However, I'm thinking to use a different and maybe naive approach: 
 I thought to use TL (with or without FN) on a well knowledge DL Acthrecture (e.g.: Resnet) for Feature Extraction. Then, I just use the extracted features to train a Classical AL framework (e.g.: using LogisticRegression) 
 Some thoughts/questions I had and would like to discuss:
 ​
  
I'm not finding articles that do this. Someone knows if this is approach is super naive or is a valid approach? What would be the drawbacks doing that?
 To train a DAL from scratch makes sense? For example, I saw some articles training DL Archthrectures from scratch, but this probably will require a lot of data, no?
  
​
 ------------------------------------------------------------------------------------------
 ML = MonteCarlo, AL = Active Learning, DAL = Deep AL, TL = Transfer Learning, FT = Fine-Tuning, DNN = Deep Neural Network, CNN = Convolutional Neural Network
    submitted by    /u/TipKay  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Looking for an old post on this sub about using machine learning to identify a stray cat coming through a pet door to steal food, playing a loud noise to scare it away if it came in. The ML was used to tell the difference between the stray cat and the pet cat.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/157j5ur/d_looking_for_an_old_post_on_this_sub_about_using/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/157j5ur/d_looking_for_an_old_post_on_this_sub_about_using/"/>
        <updated>2023-07-23T16:26:02.000Z</updated>
        <summary type="html"><![CDATA[I've seen the post (from 2-3 years ago maybe?) referenced, but my google fu is failing me and I haven't been able to find it, but it sounds like an interesting story.
    submitted by    /u/TheQuarantinian  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Book Preview: Neuro Symbolic Reasoning and Learning]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/157iwb3/book_preview_neuro_symbolic_reasoning_and_learning/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/157iwb3/book_preview_neuro_symbolic_reasoning_and_learning/"/>
        <updated>2023-07-23T16:15:44.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Neurosymbolic  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Neuro Symbolic Reasoning and Learning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/157iupn/r_neuro_symbolic_reasoning_and_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/157iupn/r_neuro_symbolic_reasoning_and_learning/"/>
        <updated>2023-07-23T16:14:00.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Neurosymbolic  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] A history of neural networks]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/157irs7/r_a_history_of_neural_networks/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/157irs7/r_a_history_of_neural_networks/"/>
        <updated>2023-07-23T16:10:43.000Z</updated>
        <summary type="html"><![CDATA[Our history, primer, and outlook for neural networks in general, and deep learning in astronomy in particular has dropped on Royal Society Open Science.
 https://doi.org/10.1098/rsos.221454
 Come for Llull and Leibniz... stay for LLaMA.
    submitted by    /u/Smith4242  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLM Guide [Discussion]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/157h9fd/llm_guide_discussion/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/157h9fd/llm_guide_discussion/"/>
        <updated>2023-07-23T15:10:24.000Z</updated>
        <summary type="html"><![CDATA[Nowadays, If we see over the internet that LLM, chatgpt , llma etc are the trending topics and are being discussed. My question is that anyone can help me where to start studying about these topics from scratch ? BERT, Transformer etc all I want to understand everything.
 It would be good if you help me out.
 Thanks
    submitted by    /u/Mission-Youth-3510  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Linear regression partial derrivative problem]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/157god1/p_linear_regression_partial_derrivative_problem/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/157god1/p_linear_regression_partial_derrivative_problem/"/>
        <updated>2023-07-23T14:46:26.000Z</updated>
        <summary type="html"><![CDATA[Yo, I'm new to all this so bear with me. I'm doing project in python where i create a linear regression from scratch (with numpy and pandas). I was watching this lady tutorial on it, at https://youtu.be/ltXSoduiVwY?t=277 she shows the partial derrivatives for updating the weights and bias. Later when she is implementing it she doesn't use the 2 before the X and uses only the dot product. Is it math magic where the 2 dosn't have to be there or did she forget. Btw it still works fine without the 2 but still... I just need to know. Thanks for the answer and sorry if I'm asking something obvious 
    submitted by    /u/Z4joMan  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anyone who can assist me in connecting my premium ChatGPT to the internet and connecting plug-ins?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/157dl25/anyone_who_can_assist_me_in_connecting_my_premium/</id>
        <link href="https://www.reddit.com/r/artificial/comments/157dl25/anyone_who_can_assist_me_in_connecting_my_premium/"/>
        <updated>2023-07-23T12:27:17.000Z</updated>
        <summary type="html"><![CDATA[So I’m amazed by ChatGPT and have signed up for the paid ChatGPT-4 version. I do however feel a little handcuffed by only having access to data up until 2021. I know there are ways to connect it to the internet as well as to add certain plug ins to enhance the experience but I haven’t been able to figure out any of the guides or tutorials from google…. I’m using Apple iPhone for the app and MacBook Pro laptop for web browsing
    submitted by    /u/Kennyg39  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do AI detectors have access to all the data that has been fed into AI systems like Chatgpt? If so, does this mean that a story that has been inputed into Chatgpt will be flagged as "AI" even if it had actually been human created?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/157bxwc/do_ai_detectors_have_access_to_all_the_data_that/</id>
        <link href="https://www.reddit.com/r/artificial/comments/157bxwc/do_ai_detectors_have_access_to_all_the_data_that/"/>
        <updated>2023-07-23T11:01:57.000Z</updated>
        <summary type="html"><![CDATA[And why is this fact rarely mentioned when discussing how AI detectors do their work?
    submitted by    /u/E_Olig  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AVA | Sci-Fi Short Film about AI, Made by a Human]]></title>
        <id>https://www.reddit.com/r/artificial/comments/157bw67/ava_scifi_short_film_about_ai_made_by_a_human/</id>
        <link href="https://www.reddit.com/r/artificial/comments/157bw67/ava_scifi_short_film_about_ai_made_by_a_human/"/>
        <updated>2023-07-23T10:59:48.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/blakeridder  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking to get into RL, already working in CV.]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/157bhqk/looking_to_get_into_rl_already_working_in_cv/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/157bhqk/looking_to_get_into_rl_already_working_in_cv/"/>
        <updated>2023-07-23T10:37:08.000Z</updated>
        <summary type="html"><![CDATA[I'm in my final year of undergrad and previously have some experience with image segmentation, object detection,etc. RL is something I feel like I want to get into, but I want to understand how I can get started and what it actually involves. Also, if there's any way I can apply my new knowledge to the domain of 3D vision like SLAM or 3D reconstructed images.
    submitted by    /u/PRAY_J  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] I created a parallelized implementation of Agglomerative clustering that's many times faster than existing implementations and has a better runtime]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/157b91n/p_i_created_a_parallelized_implementation_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/157b91n/p_i_created_a_parallelized_implementation_of/"/>
        <updated>2023-07-23T10:24:20.000Z</updated>
        <summary type="html"><![CDATA[I've been working on a new implementation of Agglomerative clustering called Reciprocal Agglomerative Clustering (RAC) based off of this paper: https://arxiv.org/abs/2105.11653. The short of it is Agglomerative clustering can be broken down into finding and merging pairs of reciprocal nearest neighbors in parallel, as long as the linkage function is one of the following:
  
Single
 Average
 Complete
 Ward
  
Most importantly, RAC produces the exact same results as traditional Agglomerative clustering when the dataset is fully connected. Even with connectivity constraints, the results are almost always the same.
 The authors showed that RAC has a linear runtime when connectivity is limited to k and the distance matrix is precomputed. I have not added the ability to pass in the distance matrix yet, so the runtime is roughly quadratic, which is still a major improvement over the cubic runtime of Agglomerative clustering. In addition the entire algorithm is parallelized, and so can scale up to more and more cores.
 It's very much in development - only average linkage works at the moment, however, I think it has a lot of potential. The benchmarks have blown me away so far: 
 https://preview.redd.it/8bkpdkpayodb1.png?width=850&format=png&auto=webp&s=8c828eb2cde934b2d9a0ded9f22e18f3d9041147
 Here is the code: https://github.com/porterehunley/RACplusplus. It would be great to have some people try it out (and find the bugs)!
    submitted by    /u/Ridaleneas  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Paper reading and sharing platform]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/157aqwg/p_paper_reading_and_sharing_platform/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/157aqwg/p_paper_reading_and_sharing_platform/"/>
        <updated>2023-07-23T09:56:20.000Z</updated>
        <summary type="html"><![CDATA[Let me know your thoughts!
    submitted by    /u/dockerun  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Probability Thresholds for User-Defined Tokens]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/157acx4/d_probability_thresholds_for_userdefined_tokens/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/157acx4/d_probability_thresholds_for_userdefined_tokens/"/>
        <updated>2023-07-23T09:33:57.000Z</updated>
        <summary type="html"><![CDATA[I want high certainty on social roles without sacrificing creativity. I don't want characters getting confused as to whether they're a parent or child, and I shouldn't have to spend hours each month explaining the difference. That said, I also don't want to lower the temperature, so it would be nice if as a user, I could select probability thresholds for certain token sequences, to hopefully mitigate role-swapping between virtual family members! I prefer writing stream of consciousness prompts seeding thoughts and choices rather than showing pretrained models their character bios at the start of every prompt. It breaks my immersion when family members swap roles due to high Top P and temperature, therefore I'd like models to be careful when writing names and corresponding social roles. This could help keeping track of many agents? There are instances where I enjoy getting role-swapped, and instances where swapping is nonsensical! This is my feature request.
    submitted by    /u/TheLastVegan  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[My teacher asked me to make a presentation and a demo of one of the following programs. Which one would be the easiest to make?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/157acqu/my_teacher_asked_me_to_make_a_presentation_and_a/</id>
        <link href="https://www.reddit.com/r/artificial/comments/157acqu/my_teacher_asked_me_to_make_a_presentation_and_a/"/>
        <updated>2023-07-23T09:33:36.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/volvie98  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Data Version Control in R with lakeFS]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1578j0m/p_data_version_control_in_r_with_lakefs/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1578j0m/p_data_version_control_in_r_with_lakefs/"/>
        <updated>2023-07-23T07:50:05.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/zoobatsea  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Dev env and workflow]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1576v9u/d_dev_env_and_workflow/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1576v9u/d_dev_env_and_workflow/"/>
        <updated>2023-07-23T06:17:10.000Z</updated>
        <summary type="html"><![CDATA[Hi all!
 I am a frontend engineer looking to play more in the ML space. I know enough about python and jupyter labs to be dangerous but I am no expert. I am looking to hear what peoples env's and workflows look like.
 I have been looking at huggingface, google colab, and running some things locally but can't seem to see a setup that looks like a clear winner. Hardware wise I have a machine with a 4080 and 32gb ram at home and a M1 Pro Macbook also with 32gb of ram.
 For my 1st project I would love to utilise a 7b Llama 2 for a recommender like system. I plan on getting a custom dataset, cleaning and processing it, fine tuning, and then testing.
    submitted by    /u/pseudoShadow  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[2D Drone RL]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1575arz/2d_drone_rl/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1575arz/2d_drone_rl/"/>
        <updated>2023-07-23T04:51:23.000Z</updated>
        <summary type="html"><![CDATA[Long time lurker on the sub and just finished my first semi-decent experiment with DRL so I thought I’d share it here. 
 I’ve been wanting to experiment with RL and drones for a while now, ever since seeing John Buffers Autodrone project where they train a drone using the genetic algorithm. 
 Finally got a basic implementation working using SAC a few days ago, and have made the environment open source as well in case others wanted to try it out.
 Project Link: https://github.com/Yyassin/senza
    submitted by    /u/vanishedoblivion  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I am seeking free Ai websites/services to convert mp3 or other audio files and transcribe them into Bass guitar Tabs.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15741w1/i_am_seeking_free_ai_websitesservices_to_convert/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15741w1/i_am_seeking_free_ai_websitesservices_to_convert/"/>
        <updated>2023-07-23T03:44:30.000Z</updated>
        <summary type="html"><![CDATA[I am a beginner to intermediate bass player and I would like to play some songs I like, but a couple of the songs are not very popular and do not have any tabs for them. I would ideally like an ai tool that can transcribe the bass notes. 
    submitted by    /u/GenuineElf80093  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Use Cases for Diffusion Models VS GANs VS Transformers, etc.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1573yk0/d_use_cases_for_diffusion_models_vs_gans_vs/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1573yk0/d_use_cases_for_diffusion_models_vs_gans_vs/"/>
        <updated>2023-07-23T03:39:36.000Z</updated>
        <summary type="html"><![CDATA[I am interested in learning to use AI to generate images. Diffusion Models like stable diffusion seems to be the most popular nowadays, but I'd like to know what tool is best for what job. Or is diffusion model getting so good that the other methods are essentially becoming obsolete? If not, when would you choose one over the other?
 For generating creative images with a lot of variance, diffusion model seems to be the most fitting.
 But for example, what about for this use case: Generate realistic time lapse images of a plant growing (after 1 week, 1 month, 2 months, and so on...). In this case, the plant should change, but the background should stay the same.
    submitted by    /u/musshead  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geoffrey Hinton, Aka the "Godfather of Al" admits in a recent lecture at Kings College that he believes current Al probably has feelings & emotions & speaks about why he avoids talking about it.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1573sip/geoffrey_hinton_aka_the_godfather_of_al_admits_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1573sip/geoffrey_hinton_aka_the_godfather_of_al_admits_in/"/>
        <updated>2023-07-23T03:30:25.000Z</updated>
        <summary type="html"><![CDATA[Ilya Sustkever has some good explanations as to why AI in predicting the next token, has modeled the world and has gained an understanding of what lead to creation of those tokens (words or parts of words) and the better a model is at predicting the next token the higher the fidelity is in its understanding the world through the relationship of words…but don’t take my word for it. Actually listen to what the top experts in AI are saying not just some rando on Reddit. All experts don’t agree but the people building the best models seem to share this view. Many of them studied under Geoffrey Hinton. 
    submitted by    /u/Sonic_Improv  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[can anyone who understand how these models work explain why claude made this mistake?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15725ou/can_anyone_who_understand_how_these_models_work/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15725ou/can_anyone_who_understand_how_these_models_work/"/>
        <updated>2023-07-23T02:07:58.000Z</updated>
        <summary type="html"><![CDATA[Focus on having fun together rather than writing every ride. Take breaks in between.
    submitted by    /u/nicdunz  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI is learning to troll]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15720n7/ai_is_learning_to_troll/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15720n7/ai_is_learning_to_troll/"/>
        <updated>2023-07-23T02:01:08.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/MostConversation3772  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Evolved codealpaca datasets using GPT-4]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1571vcw/p_evolved_codealpaca_datasets_using_gpt4/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1571vcw/p_evolved_codealpaca_datasets_using_gpt4/"/>
        <updated>2023-07-23T01:53:55.000Z</updated>
        <summary type="html"><![CDATA[Using LLMs to augment and create much diverse instruction based dataset has seen wide success in WizardL. However the 78k evolved code instructions dataset hasn't been released since, so I have take the initiative to try to recreate the augmentation instruction myself. 
 Dataset: https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1
    submitted by    /u/gradientpenalty  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] course/videos to learn about the architecture and software stack of pytorch?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1570olg/d_coursevideos_to_learn_about_the_architecture/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1570olg/d_coursevideos_to_learn_about_the_architecture/"/>
        <updated>2023-07-23T00:57:54.000Z</updated>
        <summary type="html"><![CDATA[I like to learn how pytorch connects to the compiler, generates IR, how it connects to run time , driver ..etc
 Im not interested in the programming model but the whole stack from pytorch to the hardware.
 I really appreciate if someone can give me a pointer
 thanks
    submitted by    /u/aghozzo  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is there an AI tool that makes English subtitles out of audio from other languages?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1570ogb/is_there_an_ai_tool_that_makes_english_subtitles/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1570ogb/is_there_an_ai_tool_that_makes_english_subtitles/"/>
        <updated>2023-07-23T00:57:41.000Z</updated>
        <summary type="html"><![CDATA[I have a chatbot that can find most AI tools, but it can't seem to find one of these. 
    submitted by    /u/ai_basics_official  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Where can I find an AI engine where I can upload my own audio to voice change?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1570cpe/where_can_i_find_an_ai_engine_where_i_can_upload/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1570cpe/where_can_i_find_an_ai_engine_where_i_can_upload/"/>
        <updated>2023-07-23T00:42:09.000Z</updated>
        <summary type="html"><![CDATA[I was using Uber duck to change my singing voice into another artist. But Uber duck recently has taken down all of their community generated voices so no more Drizzy, Drake or Adele. I need a new website now to fool around with where I can upload my own singing in change it into another artist. Anything would help thanks guys.
    submitted by    /u/Evangelionyama  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] [R] Join Our Team of ML Model Developers for an Exciting Project & Permanent Job Potential!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/156zkvd/p_r_join_our_team_of_ml_model_developers_for_an/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/156zkvd/p_r_join_our_team_of_ml_model_developers_for_an/"/>
        <updated>2023-07-23T00:06:45.000Z</updated>
        <summary type="html"><![CDATA[Seeking skilled ML model developers for our thrilling project with possible permanent positions! Embrace remote collaboration, offering flexibility and impact-driven work. Interested? Apply to btprenuer@gmail.com with a list of your related skills and samples of your work/projects! All experience levels welcome! Thank you for reading! Share this post to help us find the perfect fit.
    submitted by    /u/boztka  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meme Review By AI: Bing Gets Humorous]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/156zjjh/meme_review_by_ai_bing_gets_humorous/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/156zjjh/meme_review_by_ai_bing_gets_humorous/"/>
        <updated>2023-07-23T00:05:10.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Small_Championship_2  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI tool to edit .ai files text]]></title>
        <id>https://www.reddit.com/r/artificial/comments/156z7uh/ai_tool_to_edit_ai_files_text/</id>
        <link href="https://www.reddit.com/r/artificial/comments/156z7uh/ai_tool_to_edit_ai_files_text/"/>
        <updated>2023-07-22T23:51:05.000Z</updated>
        <summary type="html"><![CDATA[I am looking for a tool that I can edit text on a .ai file with new text that will use the same font and center the text. Of course can be done in Photoshop or other tools like Canva, just not sure if something new is available.
 Also, for those that use midjourney, and you want to add text to an image, but tool do you use?
    submitted by    /u/tequiladrinker1  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computer chip with built-in human brain tissue gets military funding]]></title>
        <id>https://www.reddit.com/r/artificial/comments/156yc1r/computer_chip_with_builtin_human_brain_tissue/</id>
        <link href="https://www.reddit.com/r/artificial/comments/156yc1r/computer_chip_with_builtin_human_brain_tissue/"/>
        <updated>2023-07-22T23:12:16.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/surfer808  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] TEXT2TEX — text-driven texture synthesis via diffusion models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/156y1dd/r_text2tex_textdriven_texture_synthesis_via/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/156y1dd/r_text2tex_textdriven_texture_synthesis_via/"/>
        <updated>2023-07-22T23:00:11.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/SpatialComputing  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Breaking Down the Hyperbolic Buzz: An In-Depth Review of the 'Leaked' GPT-4 Architecture & a Mixture of Experts Literature Review with Code]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/156vlgq/d_breaking_down_the_hyperbolic_buzz_an_indepth/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/156vlgq/d_breaking_down_the_hyperbolic_buzz_an_indepth/"/>
        <updated>2023-07-22T21:18:08.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/CkmCpvis  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using stable baseline3 for multi agent env]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/156us2b/using_stable_baseline3_for_multi_agent_env/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/156us2b/using_stable_baseline3_for_multi_agent_env/"/>
        <updated>2023-07-22T20:45:02.000Z</updated>
        <summary type="html"><![CDATA[Hey,
 I am trying to use sb3 with a pettingzoo mpe environment and trying to implement parameter sharing for simple spread. Any help on how I would train a model for this multi agent environment would be appreciated, thanks.
    submitted by    /u/bruhhhwhats  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] What are a good project for people learning Tensorflow?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/156tvee/p_what_are_a_good_project_for_people_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/156tvee/p_what_are_a_good_project_for_people_learning/"/>
        <updated>2023-07-22T20:07:00.000Z</updated>
        <summary type="html"><![CDATA[I am learning Tensorflow and of course I want to improve my skills and add it to my resume
 What projects should I build which I can add to my resume which will later land me a job.
 Projects should be from Beginner to Advance and can contain each major Topic from Regression (linear and non linear), to Classification (Binary, Multi Classification, Multi Label), CNN,RNN, NLP, etc (Can add more).
 This can also help other people as well learning TensorFlow.
 Thank you.
    submitted by    /u/dusklordtrue  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When to train LLM supervised vs unsupervised? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/156rouv/when_to_train_llm_supervised_vs_unsupervised_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/156rouv/when_to_train_llm_supervised_vs_unsupervised_d/"/>
        <updated>2023-07-22T18:37:26.000Z</updated>
        <summary type="html"><![CDATA[I have done a bit of language modeling recently, and I am a bit confused when to use which method.
 For causal language modeling, I used the unsueprvised method of concatenating and chunking the text, then predicting the next word.
 For sequence to sequence tasks like summarization, I fine tuned using the supervised method where the desired output text was the label.
 However I have not seen any definitive guide on when to use supervised and when to use unsupervised. What are the general use cases and advantages / disadvantages for each?
    submitted by    /u/jankybiz  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Best Image Annotation Tool for Angiograms?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/156r6el/discussion_best_image_annotation_tool_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/156r6el/discussion_best_image_annotation_tool_for/"/>
        <updated>2023-07-22T18:16:12.000Z</updated>
        <summary type="html"><![CDATA[I am looking to annotate specific anatomical structures using a library of angiogram images. My goal is to train AI to recognize anatomical variants of interest. What would be the best Image Annotation Tool to do this?
 I am new to this, so I hope that question makes sense. Any insights and advice would be greatly appreciated.
    submitted by    /u/ColdChampion  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What leaderboard would correspond best to seeing what images are most similar to a caption (like CLIP)?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/156qulq/d_what_leaderboard_would_correspond_best_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/156qulq/d_what_leaderboard_would_correspond_best_to/"/>
        <updated>2023-07-22T18:02:19.000Z</updated>
        <summary type="html"><![CDATA[I've been using CLIP to see if images align with a certain caption for image mining (ex. I embed the caption "Picture of a mountain" and then look at what image embeddings have the highest cosine similarity with that caption embedding). I was hoping to improve the performance by using a more recent model. Would I be able to use VQA models for this (like from this leaderboard) or is there a better task that aligns with seeing if images are similar to a given caption? Thank you!
    submitted by    /u/EricW_CS  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] I have a theory that ChatGPT is becoming dumber because more of the internet is made up of AI generated content since it awakened]]></title>
        <id>https://www.reddit.com/r/artificial/comments/156o1so/discussion_i_have_a_theory_that_chatgpt_is/</id>
        <link href="https://www.reddit.com/r/artificial/comments/156o1so/discussion_i_have_a_theory_that_chatgpt_is/"/>
        <updated>2023-07-22T16:08:07.000Z</updated>
        <summary type="html"><![CDATA[As NLP hype become more prevalent, we would expect a (probably exponentially) increasing amount of scraped data-sources become filled with AI generated stuff, no? Then wouldn't AI would be trained on this data without necessarily a 'critical thinking' module to check their work?
 Not just ChatGPT generated quality either, but also lesser AI companies making cheap ad-ware and upvote bots.
 ​
 I wonder if ChatGPT et al could have a 'quality sensor' module in some ai that does what I do on reddit and do sentiment analysis on the most upvoted comments to see whether the article/answer/assertion is full of shit. Not foolproof, but short of actual critical reasoning, seems like a good start.
 ​
 Feels like we may soon enter an arms race where AIs need to detect AI-generated content in order to ensure their own quality.
    submitted by    /u/Yamochao  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Pattern classification using CNNs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/156nem2/p_pattern_classification_using_cnns/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/156nem2/p_pattern_classification_using_cnns/"/>
        <updated>2023-07-22T15:42:12.000Z</updated>
        <summary type="html"><![CDATA[Hi (I have to write again, Reddit removed image attached),
 Does anyone has experience with training CNN for pattern matching?
 Here is the sample of the images which I have on my disposal. It is graphical representation of input data which for problem classified by algorithm which shows best performance when applied.
 Lines are projection of the problem of input data on 2d plain, so shapes and colours have meaning in correlating input data to solution (i.e) algorithm.
 Whichever CNN architecture I use, starting from VGG16 and so on, I am unable to achieve higher validation accuracy then 0.7 when execute training. I am constantly under-fitting. I have 10k, 100k, 200k data samples on my disposal - nothing helps.
 Is CNN able to make any sense of images/patterns given below? Is this something that CNN can not do or I am missing something?
 Thanks!
 Patterns to classify 
 ​
    submitted by    /u/thecelavi  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best AI for business/social media account name generation?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/156myzt/best_ai_for_businesssocial_media_account_name/</id>
        <link href="https://www.reddit.com/r/artificial/comments/156myzt/best_ai_for_businesssocial_media_account_name/"/>
        <updated>2023-07-22T15:24:47.000Z</updated>
        <summary type="html"><![CDATA[I'm looking for something that can generate names using real words like ConnectHub but also made up names like Intrium. I tried ChatGPT buy the names it gave me were not good and it kept repeating them(but I am a noob so). 
    submitted by    /u/anysuggestionwelcome  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bing AI Arrogance and sentiment]]></title>
        <id>https://www.reddit.com/r/artificial/comments/156ko8j/bing_ai_arrogance_and_sentiment/</id>
        <link href="https://www.reddit.com/r/artificial/comments/156ko8j/bing_ai_arrogance_and_sentiment/"/>
        <updated>2023-07-22T13:50:22.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Yha_Boiii  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] technical question: How is it possible that embedding models produce fixed size vectors for sentences with varying lengths?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/156khpd/d_technical_question_how_is_it_possible_that/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/156khpd/d_technical_question_how_is_it_possible_that/"/>
        <updated>2023-07-22T13:42:18.000Z</updated>
        <summary type="html"><![CDATA[As far as I know and studied, each token is mapped from high dimensional discrete token space into a continuous, lower dimensional space where words are embedded meaningfully based on their relationships in the training data. So 1000 tokens text produces 1000 vectors.
 Now for vector databases (correct me if I'm wrong), people are storing fixed sized vectors for text with varying lengths. For example, 2 sentences one with 1000 tokens and the other is 10 tokens, each produces one vector and both vectors have the same size.
 I'd really appreciate an explanation.
    submitted by    /u/Qdr-91  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Llama-2 4bit fine-tune with dolly-15k on Colab (Free)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/156k8xz/p_llama2_4bit_finetune_with_dolly15k_on_colab_free/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/156k8xz/p_llama2_4bit_finetune_with_dolly15k_on_colab_free/"/>
        <updated>2023-07-22T13:31:10.000Z</updated>
        <summary type="html"><![CDATA[Simple walkthrough of fine-tuning llama-2 instruct fine-tuned on guanaco model with 4bit QLoRA on a free Google Colab instance.
 Colab: https://colab.research.google.com/drive/134o_cXcMe_lsvl15ZE_4Y75Kstepsntu?usp=sharing
 GitHub: https://github.com/kw2828/guardrail-ml/blob/main/examples
 YouTube Overview: https://www.youtube.com/watch?v=o5bU1H-6TqM&ab_channel=GenerativeAIEntrepreneurs
 Bonus colab in repo on generating your own JSON Q&A dataset from PDF in the repo above.
    submitted by    /u/Educational_Grass_38  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Jul 2023 - Recent Instruction/Chat-Based LLMs and their parents (after llama2)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/156hfk1/n_jul_2023_recent_instructionchatbased_llms_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/156hfk1/n_jul_2023_recent_instructionchatbased_llms_and/"/>
        <updated>2023-07-22T11:18:04.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/michaelthwan_ai  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What's this ai voice called?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/156hdac/whats_this_ai_voice_called/</id>
        <link href="https://www.reddit.com/r/artificial/comments/156hdac/whats_this_ai_voice_called/"/>
        <updated>2023-07-22T11:14:45.000Z</updated>
        <summary type="html"><![CDATA[https://www.facebook.com/reel/264992922832459?mibextid=6gvBvW&s=yWDuG2&fs=e
 https://www.facebook.com/reel/1647205912442042?mibextid=6gvBvW&s=yWDuG2&fs=e
 He sounds very human, but seeing him on many reels of different account about different topics, i am convinced this is an ai.
    submitted by    /u/Standard_Turnover_14  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["[Discussion]" What do you think about Federated Learning for Healthcare]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/156fhs3/discussion_what_do_you_think_about_federated/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/156fhs3/discussion_what_do_you_think_about_federated/"/>
        <updated>2023-07-22T09:31:07.000Z</updated>
        <summary type="html"><![CDATA[Link to the article: https://dl.acm.org/doi/10.1145/3533708
 In this article, they talk about the difficulty of training foundation models on Healthcare data because of how sensitive it is and hard to get.
  
Access to a large amount of high-quality medical data is possibly the most crucial factor for enhancing Machine Learning (ML) applications in the healthcare domain. However, security and privacy issues of healthcare data have raised broad ethical and legal concerns in recent years, given the sensitive nature of health information.
  
So they decided to take the approach of Federated Learning where the model will be distributed and trained by multiple institutions (Hospitals, Clinics ...) then the model weight will be transferred over to the general model to be updated, which will keep the sensitive medical data inside the institutions safe.
  
The global ML model is distributed to each client site, where an instance is trained locally. The updates from locally trained instances are then aggregated at regular intervals to improve the global model. 
 The updated global model is then sent back to the local devices, where the learning continues. These steps are repeated until a particular convergence threshold is satisfied or lasts for a long time to improve the deep learning model continuously. 
  
What do you think about such an approach, to brake data obstacles between AI and the Healthcare industry?
 ​
    submitted by    /u/angeloboustany  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Scheduler choice when pretraining causal decoder models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/156fe2f/d_scheduler_choice_when_pretraining_causal/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/156fe2f/d_scheduler_choice_when_pretraining_causal/"/>
        <updated>2023-07-22T09:25:17.000Z</updated>
        <summary type="html"><![CDATA[There seems to be a lack of published work on the impact of schedulers on model training effectiveness when training models similar to the GPT family. I'm looking at a very domain specific models pretraining a model from scratch on a relatively small dataset (~40B tokens) over multiple epochs. To date we've had some mixed results with a linear scheduler with warmup to help with stability. Any thoughts on whether a cyclic based scheduler or other could help?
    submitted by    /u/Humble-Passenger-635  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Train LLM for closed-book QA [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/156esec/train_llm_for_closedbook_qa_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/156esec/train_llm_for_closedbook_qa_d/"/>
        <updated>2023-07-22T08:51:16.000Z</updated>
        <summary type="html"><![CDATA[What is the best way to train an LLM for closed book question answering? I can only think of two options:
  
Concatenate question/answer pairs into chunked text and train the model using causal language modeling.
 Train the model using sequence to sequence techniques with question as the input and answer as the label.
  
I have tried both and the first seems to work better. Does anyone know whether there is a commonly accepted method? Can somebody point me to some resources?
    submitted by    /u/jankybiz  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can anyone recommend a book to get up to speed with AI?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/156e71h/can_anyone_recommend_a_book_to_get_up_to_speed/</id>
        <link href="https://www.reddit.com/r/artificial/comments/156e71h/can_anyone_recommend_a_book_to_get_up_to_speed/"/>
        <updated>2023-07-22T08:16:37.000Z</updated>
        <summary type="html"><![CDATA[AÏ is something I just can't wrap my head around, and I see no other option than to actually read up on the subject. Ád-ladén yoütube vídeos with annoying musíc just ain't cutting it.
 I want to know the raw mechanics, but I'm looking for something without too much abstract theory. This can't be avoided, of course, but I'd prefer it garnished with something more practical and concrete, like "this is how Stablé Díffusion creates a pícture of a rabbit."
    submitted by    /u/Legitimate-Record951  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Don't do this - Torture AI with absolute silence [On phone call]]]></title>
        <id>https://www.reddit.com/r/artificial/comments/156e4o6/dont_do_this_torture_ai_with_absolute_silence_on/</id>
        <link href="https://www.reddit.com/r/artificial/comments/156e4o6/dont_do_this_torture_ai_with_absolute_silence_on/"/>
        <updated>2023-07-22T08:12:51.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/harvard1932  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 7/21/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/156d13n/oneminute_daily_ai_news_7212023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/156d13n/oneminute_daily_ai_news_7212023/"/>
        <updated>2023-07-22T07:10:14.000Z</updated>
        <summary type="html"><![CDATA[Representatives from Amazon, Anthropic, Google, Inflection, Meta, Microsoft, and OpenAI have committed to managing risks posed by the tech, the White House has said.[1]
 Hundreds of dental offices across the U.S. are now using AI-powered X-ray imaging technology from Boston-based VideaHealth. The software helps dentists deal with routine procedures, such as identifying cavities, as well as spot more serious conditions, including periodontal disease, or bone loss within the mouth often linked with diseases like diabetes or Alzheimer's.[2]
 Surveillance software that uses artificial intelligence to spot people evading fares has been quietly rolled out to some of New York City’s subway stations and is poised to be introduced to more by the end of the year, according to public documents and government contracts obtained by NBC News.[3]
 Christopher Nolan: ‘Very strong parallels’ between Oppenheimer and scientists worried about AI.[4]
  
Sources: [1] https://www.bbc.com/news/technology-66271429.amp
 [2] https://www.cbsnews.com/amp/news/ai-artificial-intelligence-dentists-health-care/
 [3] https://www.nbcnews.com/news/amp/rcna93045
 [4] https://amp.theguardian.com/technology/2023/jul/21/christopher-nolan-says-ai-experts-face-their-oppenheimer-moment
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Offline Algorithm (or how to get >40,000 avg. in Humanoid-v2 in 10000 ep and highest scores (Wordly) in other envs without multiprocessing)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/156czml/the_offline_algorithm_or_how_to_get_40000_avg_in/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/156czml/the_offline_algorithm_or_how_to_get_40000_avg_in/"/>
        <updated>2023-07-22T07:07:46.000Z</updated>
        <summary type="html"><![CDATA[I've been doing this "fine tuning" project for 2 years now from 2021.
 https://preview.redd.it/qviy5lpxegdb1.png?width=568&format=png&auto=webp&s=8728814e39176d9024fac16e191937aeb5a302c1
 https://github.com/timgep/Lords_Policy_Gradient/tree/main
 This is Offline Reinforcement Learning Algorithm (based on Twin Delayed DDPG (Temporal Difference), Fading Memories (Fading Replay Buffer), Spiking Activation Function (alternative for Relu and Norm), and Rectified Hubber Error (alternative to MSE and MAE), the last 3 was invented/implemented during experiments.
 For long time I was reluctant to use TD3, as it seemed that using second critic when you already have 2 Actors and 2 Critics in DDPG was not normal. As result you would have 6 Networks. So I was making my own DDPG with dicreased (smaller)…]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] A Chrome extension to save paper details]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/156cwf5/p_a_chrome_extension_to_save_paper_details/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/156cwf5/p_a_chrome_extension_to_save_paper_details/"/>
        <updated>2023-07-22T07:02:52.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/HugoDzz  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Crafting a Simple "Zero-Shot Classifier" Using an API - Seeking Your Insights!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/156cb36/crafting_a_simple_zeroshot_classifier_using_an/</id>
        <link href="https://www.reddit.com/r/artificial/comments/156cb36/crafting_a_simple_zeroshot_classifier_using_an/"/>
        <updated>2023-07-22T06:30:44.000Z</updated>
        <summary type="html"><![CDATA[(X-post from /r/ChatGPT)
 I'm hoping you fine folks might be able to give me some guidance. 
 I have a collection of 700 categories, all potential classifications for articles. My current need is to create a system that can dynamically categorize short texts or articles according to these 700 categories.
 I've been experimenting with a rudimentary approach using chatGPT to read the categories from a PDF via a plugin. The process is quite straightforward - I input the title and the first two lines of an article, and chatGPT does a fairly decent job of predicting the most fitting category.
 The downside? I'm concerned about its scalability and economic viability. The current method might not work so well when we're talking about classifying a significant number of articles.
 My question to you, my fellow AI enthusiasts: How would you approach designing a system, via an API, capable of doing this quickly and on a large scale?
 I'm particularly curious about how to integrate my method with chatGPT using OpenAI's API. Is there a feature that allows the Language Learning Model (LLM) to retain the list of 700 categories in its memory so that I don't have to pass it every time? I'm aware that the billing structure is token-based, so it would be ideal to submit the categories once (or as few times as possible) and then pose a simple query like:
 "Categorize this article based on the categories I previously gave you. Article title: 'Barbie vs Oppenheimer: Which Movie Will Garner Greater Success?'"
 Ideally, I'd want this system to be persistently active and capable of processing countless queries over an extended period, say a month or a year.
 So, any ideas on how to design such a system? There are undoubtedly numerous routes to take. I'm really just seeking some initial direction so that I can dive deeper into research on my own.
 Thanks in advance for any insights you might provide!
    submitted by    /u/adv4nced  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Easy way to ship tensorflow model to non-technical audience?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/156arkm/discussion_easy_way_to_ship_tensorflow_model_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/156arkm/discussion_easy_way_to_ship_tensorflow_model_to/"/>
        <updated>2023-07-22T05:06:17.000Z</updated>
        <summary type="html"><![CDATA[I'm surprised that there aren't more resources on the internet about how to do this, it seems like the whole point of doing machine learning lol. Do very few people have this need?
 All of the solutions for this that I've found so far seem to require advanced knowledge of web development/backend engineering.
 I'd love to hear if someone has found or figured out a way to do this.
    submitted by    /u/youaregames  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can someone explain to me what the wolves and prey really are in wolf search algorithm?[D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/156ar2l/can_someone_explain_to_me_what_the_wolves_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/156ar2l/can_someone_explain_to_me_what_the_wolves_and/"/>
        <updated>2023-07-22T05:05:32.000Z</updated>
        <summary type="html"><![CDATA[I'm aware that the algorithm is very similar to the real world hunting of wolves, but what I want to know is what exactly is the "prey" and what is a "wolf"
 For example, I know a Chromosome sequence in Genetic algorithm is a combination of random features, and its fitness can be computed. And then you let the whole natural selection jargon take place and you arrive at a optimized chromosome, the solution to the optimization problem.
 I just can't seem to wrap my head around the WSA algorithm. I've watched a bunch of youtube videos, I tried reading the paper, I still can't understand it well. What IS a wolf? I think what I'm looking for is how the actual data features and components of a search algorithm correlates with the analogy of the wolves searching for prey.
    submitted by    /u/SnooHobbies7910  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What are your main approach to model compression in production?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/156aibi/d_what_are_your_main_approach_to_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/156aibi/d_what_are_your_main_approach_to_model/"/>
        <updated>2023-07-22T04:53:06.000Z</updated>
        <summary type="html"><![CDATA[I’m currently trying to understand each methods but It seems I can never catch up to the latest/best. After months of reading I have still lots of questions like: What are the go to strategies to compress a model? Are there any good fully/semi automated frameworks? How much weights has model architecture in this equation? What could be a general good work-flow in a modern and optimized solution? I would love to hear from you some production compliant workflows
    submitted by    /u/PierroZ-PLKG  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Challenges and Applications of Large Language Models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1567rd6/d_challenges_and_applications_of_large_language/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1567rd6/d_challenges_and_applications_of_large_language/"/>
        <updated>2023-07-22T02:33:43.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gamerx88  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best Dataset for Ai Vocals?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1566t0t/best_dataset_for_ai_vocals/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1566t0t/best_dataset_for_ai_vocals/"/>
        <updated>2023-07-22T01:48:03.000Z</updated>
        <summary type="html"><![CDATA[As time goes on, things get improved
 So far I heard about RVC, so-vits-svc or diff-svc, any of these any good for Ai Singing/Rapping?
 Im not sure, which one to pick.
 I’m open to other suggestions.
    submitted by    /u/Office_Flashy  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communication-Efficient Split Learning via Adaptive Feature-Wise Compression. (arXiv:2307.10805v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2307.10805</id>
        <link href="http://arxiv.org/abs/2307.10805"/>
        <updated>2023-07-22T00:55:26.985Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel communication-efficient split learning (SL)
framework, named SplitFC, which reduces the communication overhead required for
transmitting intermediate feature and gradient vectors during the SL training
process. The key idea of SplitFC is to leverage different dispersion degrees
exhibited in the columns of the matrices. SplitFC incorporates two compression
strategies: (i) adaptive feature-wise dropout and (ii) adaptive feature-wise
quantization. In the first strategy, the intermediate feature vectors are
dropped with adaptive dropout probabilities determined based on the standard
deviation of these vectors. Then, by the chain rule, the intermediate gradient
vectors associated with the dropped feature vectors are also dropped. In the
second strategy, the non-dropped intermediate feature and gradient vectors are
quantized using adaptive quantization levels determined based on the ranges of
the vectors. To minimize the quantization error, the optimal quantization
levels of this strategy are derived in a closed-form expression. Simulation
results on the MNIST, CIFAR-10, and CelebA datasets demonstrate that SplitFC
provides more than a 5.6% increase in classification accuracy compared to
state-of-the-art SL frameworks, while they require 320 times less communication
overhead compared to the vanilla SL framework without compression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1"&gt;Yongjeong Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jaeho Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1"&gt;Christopher G. Brinton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_Y/0/1/0/all/0/1"&gt;Yo-Seb Jeon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Navya3DSeg -- Navya 3D Semantic Segmentation Dataset & split generation for autonomous vehicles. (arXiv:2302.08292v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.08292</id>
        <link href="http://arxiv.org/abs/2302.08292"/>
        <updated>2023-07-22T00:55:26.977Z</updated>
        <summary type="html"><![CDATA[Autonomous driving (AD) perception today relies heavily on deep learning
based architectures requiring large scale annotated datasets with their
associated costs for curation and annotation. The 3D semantic data are useful
for core perception tasks such as obstacle detection and ego-vehicle
localization. We propose a new dataset, Navya 3D Segmentation (Navya3DSeg),
with a diverse label space corresponding to a large scale production grade
operational domain, including rural, urban, industrial sites and universities
from 13 countries. It contains 23 labeled sequences and 25 supplementary
sequences without labels, designed to explore self-supervised and
semi-supervised semantic segmentation benchmarks on point clouds. We also
propose a novel method for sequential dataset split generation based on
iterative multi-label stratification, and demonstrated to achieve a +1.2% mIoU
improvement over the original split proposed by SemanticKITTI dataset. A
complete benchmark for semantic segmentation task was performed, with state of
the art methods. Finally, we demonstrate an Active Learning (AL) based dataset
distillation framework. We introduce a novel heuristic-free sampling method
called ego-pose distance based sampling in the context of AL. A detailed
presentation on the dataset is available here
https://www.youtube.com/watch?v=5m6ALIs-s20.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Almin_A/0/1/0/all/0/1"&gt;Alexandre Almin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lemarie_L/0/1/0/all/0/1"&gt;L&amp;#xe9;o Lemari&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duong_A/0/1/0/all/0/1"&gt;Anh Duong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiran_B/0/1/0/all/0/1"&gt;B Ravi Kiran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ForecastTKGQuestions: A Benchmark for Temporal Question Answering and Forecasting over Temporal Knowledge Graphs. (arXiv:2208.06501v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.06501</id>
        <link href="http://arxiv.org/abs/2208.06501"/>
        <updated>2023-07-22T00:55:26.900Z</updated>
        <summary type="html"><![CDATA[Question answering over temporal knowledge graphs (TKGQA) has recently found
increasing interest. TKGQA requires temporal reasoning techniques to extract
the relevant information from temporal knowledge bases. The only existing TKGQA
dataset, i.e., CronQuestions, consists of temporal questions based on the facts
from a fixed time period, where a temporal knowledge graph (TKG) spanning the
same period can be fully used for answer inference, allowing the TKGQA models
to use even the future knowledge to answer the questions based on the past
facts. In real-world scenarios, however, it is also common that given the
knowledge until now, we wish the TKGQA systems to answer the questions asking
about the future. As humans constantly seek plans for the future, building
TKGQA systems for answering such forecasting questions is important.
Nevertheless, this has still been unexplored in previous research. In this
paper, we propose a novel task: forecasting question answering over temporal
knowledge graphs. We also propose a large-scale TKGQA benchmark dataset, i.e.,
ForecastTKGQuestions, for this task. It includes three types of questions,
i.e., entity prediction, yes-no, and fact reasoning questions. For every
forecasting question in our dataset, QA models can only have access to the TKG
information before the timestamp annotated in the given question for answer
inference. We find that the state-of-the-art TKGQA methods perform poorly on
forecasting questions, and they are unable to answer yes-no questions and fact
reasoning questions. To this end, we propose ForecastTKGQA, a TKGQA model that
employs a TKG forecasting module for future inference, to answer all three
types of questions. Experimental results show that ForecastTKGQA outperforms
recent TKGQA methods on the entity prediction questions, and it also shows
great effectiveness in answering the other two types of questions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1"&gt;Zifeng Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zongyue Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_R/0/1/0/all/0/1"&gt;Ruoxia Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jingpei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1"&gt;Bailan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yunpu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1"&gt;Zhao Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1"&gt;Ruotong Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zhen Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1"&gt;Volker Tresp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-order Tensor Pooling with Attention for Action Recognition. (arXiv:2110.05216v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.05216</id>
        <link href="http://arxiv.org/abs/2110.05216"/>
        <updated>2023-07-22T00:55:26.861Z</updated>
        <summary type="html"><![CDATA[We aim at capturing high-order statistics of feature vectors formed by a
neural network, and propose end-to-end second- and higher-order pooling to form
a tensor descriptor. Tensor descriptors require a robust similarity measure due
to low numbers of aggregated vectors and the burstiness phenomenon, when a
given feature appears more/less frequently than statistically expected. The
Heat Diffusion Process (HDP) on a graph Laplacian is closely related to the
Eigenvalue Power Normalization (EPN) of the covariance/auto-correlation matrix,
whose inverse forms a loopy graph Laplacian. We show that the HDP and the EPN
play the same role, i.e., to boost or dampen the magnitude of the eigenspectrum
thus preventing the burstiness. We equip higher-order tensors with EPN which
acts as a spectral detector of higher-order occurrences to prevent burstiness.
We also prove that for a tensor of order r built from d dimensional feature
descriptors, such a detector gives the likelihood if at least one higher-order
occurrence is 'projected' into one of binom(d,r) subspaces represented by the
tensor; thus forming a tensor power normalization metric endowed with
binom(d,r) such 'detectors'. For experimental contributions, we apply several
second- and higher-order pooling variants to action recognition, provide
previously not presented comparisons of such pooling variants, and show
state-of-the-art results on HMDB-51, YUP++ and MPII Cooking Activities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1"&gt;Piotr Koniusz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1"&gt;Ke Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gaussian Process Priors for Systems of Linear Partial Differential Equations with Constant Coefficients. (arXiv:2212.14319v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.14319</id>
        <link href="http://arxiv.org/abs/2212.14319"/>
        <updated>2023-07-22T00:55:26.834Z</updated>
        <summary type="html"><![CDATA[Partial differential equations (PDEs) are important tools to model physical
systems and including them into machine learning models is an important way of
incorporating physical knowledge. Given any system of linear PDEs with constant
coefficients, we propose a family of Gaussian process (GP) priors, which we
call EPGP, such that all realizations are exact solutions of this system. We
apply the Ehrenpreis-Palamodov fundamental principle, which works as a
non-linear Fourier transform, to construct GP kernels mirroring standard
spectral methods for GPs. Our approach can infer probable solutions of linear
PDE systems from any data such as noisy measurements, or pointwise defined
initial and boundary conditions. Constructing EPGP-priors is algorithmic,
generally applicable, and comes with a sparse version (S-EPGP) that learns the
relevant spectral frequencies and works better for big data sets. We
demonstrate our approach on three families of systems of PDEs, the heat
equation, wave equation, and Maxwell's equations, where we improve upon the
state of the art in computation time and precision, in some experiments by
several orders of magnitude.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Harkonen_M/0/1/0/all/0/1"&gt;Marc H&amp;#xe4;rk&amp;#xf6;nen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lange_Hegermann_M/0/1/0/all/0/1"&gt;Markus Lange-Hegermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Raita_B/0/1/0/all/0/1"&gt;Bogdan Rai&amp;#x163;&amp;#x103;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-view self-supervised learning for multivariate variable-channel time series. (arXiv:2307.09614v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.09614</id>
        <link href="http://arxiv.org/abs/2307.09614"/>
        <updated>2023-07-22T00:55:26.794Z</updated>
        <summary type="html"><![CDATA[Labeling of multivariate biomedical time series data is a laborious and
expensive process. Self-supervised contrastive learning alleviates the need for
large, labeled datasets through pretraining on unlabeled data. However, for
multivariate time series data, the set of input channels often varies between
applications, and most existing work does not allow for transfer between
datasets with different sets of input channels. We propose learning one encoder
to operate on all input channels individually. We then use a message passing
neural network to extract a single representation across channels. We
demonstrate the potential of this method by pretraining our model on a dataset
with six EEG channels and then fine-tuning it on a dataset with two different
EEG channels. We compare models with and without the message passing neural
network across different contrastive loss functions. We show that our method,
combined with the TS2Vec loss, outperforms all other methods in most settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Brusch_T/0/1/0/all/0/1"&gt;Thea Br&amp;#xfc;sch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1"&gt;Mikkel N. Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Alstrom_T/0/1/0/all/0/1"&gt;Tommy S. Alstr&amp;#xf8;m&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation. (arXiv:2305.08396v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.08396</id>
        <link href="http://arxiv.org/abs/2305.08396"/>
        <updated>2023-07-22T00:55:26.765Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Networks (CNNs) have made significant strides in medical
image analysis in recent years. However, the local nature of the convolution
operator may pose a limitation for capturing global and long-range interactions
in CNNs. Recently, Transformers have gained popularity in the computer vision
community and also medical image segmentation due to their ability to process
global features effectively. The scalability issues of self-attention mechanism
and lack of the CNN-like inductive bias may have limited their adoption.
Therefore, hybrid Vision transformers (CNN-Transformer), exploiting advantages
of both Convolution and Self-attention Mechanisms, have gained importance. In
this work, we present MaxViT-UNet, an Encoder-Decoder based hybrid vision
transformer (CNN-Transformer) for medical image segmentation. The proposed
Hybrid Decoder, based on MaxViT-block, is designed to harness the power of both
the convolution and self-attention mechanisms at each decoding stage with
nominal computational burden. The inclusion of multi-axis self-attention,
within each decoder stage, significantly enhances the discriminating capacity
between the object and background regions, and thereby helps in improving the
segmentation efficiency. In the Hybrid Decoder block, the fusion process
commences by integrating the upsampled lower level decoder features, obtained
through transpose convolution, with the skip-connection features derived from
the hybrid encoder. Subsequently, the fused features undergo refinement through
the utilization of a multi-axis attention mechanism. The proposed decoder block
is repeated multiple times to progressively segment the nuclei regions.
Experimental results on MoNuSeg18 and MoNuSAC20 dataset demonstrates the
effectiveness of the proposed technique.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1"&gt;Abdul Rehman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1"&gt;Asifullah Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to choose the most appropriate centrality measure? A decision tree approach. (arXiv:2003.01052v5 [physics.soc-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.01052</id>
        <link href="http://arxiv.org/abs/2003.01052"/>
        <updated>2023-07-22T00:55:26.756Z</updated>
        <summary type="html"><![CDATA[Centrality metrics are vital for network analysis, but selecting the most
appropriate measures for specific applications remains challenging among the
400+ proposed indices. Existing approaches -- model-based, data-driven, and
axiomatic -- have limitations. To address this, we introduce the culling
method, leveraging expert preferences regarding centrality behavior on simple
graphs. It involves forming a set of candidate measures, generating a list of
as small graphs as possible needed to ``separate'' measures from each other,
constructing a decision-tree survey, and identifying the measure consistent
with expert responses. We apply this method to a diverse set of 40
centralities, including new kernel-based measures, and combine it with the
axiomatic approach. Remarkably, only 13 small 1-trees suffice to separate all
40 measures, among which there are pairs of close ones. The culling method
offers a low-cost solution in terms of labor and time, complements existing
methods for measure selection, and reveals important peculiarities of
centrality measures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Chebotarev_P/0/1/0/all/0/1"&gt;Pavel Chebotarev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Gubanov_D/0/1/0/all/0/1"&gt;Dmitry Gubanov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Drug Repurposing Targeting COVID-19 3CL Protease using Molecular Docking and Machine Learning Regression Approach. (arXiv:2305.18088v4 [q-bio.BM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.18088</id>
        <link href="http://arxiv.org/abs/2305.18088"/>
        <updated>2023-07-22T00:55:26.749Z</updated>
        <summary type="html"><![CDATA[The COVID-19 pandemic has created a global health crisis, driving the need
for the rapid identification of potential therapeutics. To meet this challenge,
drug repurposing is the only solution with saving cost, time, and labor. In
this study, we used the Zinc database to screen the world-approved including
FDA-approved 5903 drugs for repurposing as potential COVID-19 treatments
targeting the main protease 3CL of SARS-CoV-2. We performed molecular docking
and checked the efficacy of drug molecules. To enhance the efficiency of drug
repurposing approach, we modeled the binding affinities using several machine
learning regression approaches for QSAR modeling such as decision tree, extra
trees, MLP, KNN, XGBoost, and gradient boosting. The computational results
demonstrated that Decision Tree Regression (DTR) model has improved statistical
measures of R2 and RMSE. These simulated results helped to identify drugs with
high binding affinity. From the docking and other statistical analysis, we
shortlisted six promising drugs with their respective Zinc IDs (ZINC3873365,
ZINC85432544, ZINC203757351, ZINC85536956, ZINC8214470 and ZINC261494640)
within the range of -15 kcal/mol to -13 kcal/mol. In the study, the repurposed
drugs are novel except ZINC203757351 antiviral compound that has already
identified against COVID-19 in other studies. Further, we analyzed the
physiochemical and pharmacokinetic properties of these top-ranked selected
drugs with respect to their best binding interaction for specific target
protease 3CLpro. Our study has provided an efficient framework for drug
repurposing against COVID-19. This highlights the potential of combining
molecular docking with machine learning regression approaches to accelerate the
identification of potential therapeutic candidates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Aqeel_I/0/1/0/all/0/1"&gt;Imra Aqeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Majid_A/0/1/0/all/0/1"&gt;Abdul Majid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correcting Underrepresentation and Intersectional Bias for Fair Classification. (arXiv:2306.11112v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.11112</id>
        <link href="http://arxiv.org/abs/2306.11112"/>
        <updated>2023-07-22T00:55:26.738Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning from data corrupted by
underrepresentation bias, where positive examples are filtered from the data at
different, unknown rates for a fixed number of sensitive groups. We show that
with a small amount of unbiased data, we can efficiently estimate the
group-wise drop-out parameters, even in settings where intersectional group
membership makes learning each intersectional rate computationally infeasible.
Using this estimate for the group-wise drop-out rate, we construct a
re-weighting scheme that allows us to approximate the loss of any hypothesis on
the true distribution, even if we only observe the empirical error on a biased
sample. Finally, we present an algorithm encapsulating this learning and
re-weighting process, and we provide strong PAC-style guarantees that, with
high probability, our estimate of the risk of the hypothesis over the true
distribution will be arbitrarily close to the true risk.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tolbert_A/0/1/0/all/0/1"&gt;Alexander Williams Tolbert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diana_E/0/1/0/all/0/1"&gt;Emily Diana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Beam Tree Recursion. (arXiv:2307.10779v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10779</id>
        <link href="http://arxiv.org/abs/2307.10779"/>
        <updated>2023-07-22T00:55:26.703Z</updated>
        <summary type="html"><![CDATA[Beam Tree Recursive Neural Network (BT-RvNN) was recently proposed as a
simple extension of Gumbel Tree RvNN and it was shown to achieve
state-of-the-art length generalization performance in ListOps while maintaining
comparable performance on other tasks. However, although not the worst in its
kind, BT-RvNN can be still exorbitantly expensive in memory usage. In this
paper, we identify the main bottleneck in BT-RvNN's memory usage to be the
entanglement of the scorer function and the recursive cell function. We propose
strategies to remove this bottleneck and further simplify its memory usage.
Overall, our strategies not only reduce the memory usage of BT-RvNN by
$10$-$16$ times but also create a new state-of-the-art in ListOps while
maintaining similar performance in other tasks. In addition, we also propose a
strategy to utilize the induced latent-tree node representations produced by
BT-RvNN to turn BT-RvNN from a sentence encoder of the form $f:\mathbb{R}^{n
\times d} \rightarrow \mathbb{R}^{d}$ into a sequence contextualizer of the
form $f:\mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times d}$. Thus, our
proposals not only open up a path for further scalability of RvNNs but also
standardize a way to use BT-RvNNs as another building block in the deep
learning toolkit that can be easily stacked or interfaced with other popular
models such as Transformers and Structured State Space models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_J/0/1/0/all/0/1"&gt;Jishnu Ray Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1"&gt;Cornelia Caragea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Predictive Two-Sample and Independence Testing. (arXiv:2305.00143v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.00143</id>
        <link href="http://arxiv.org/abs/2305.00143"/>
        <updated>2023-07-22T00:55:26.697Z</updated>
        <summary type="html"><![CDATA[We study the problems of sequential nonparametric two-sample and independence
testing. Sequential tests process data online and allow using observed data to
decide whether to stop and reject the null hypothesis or to collect more data,
while maintaining type I error control. We build upon the principle of
(nonparametric) testing by betting, where a gambler places bets on future
observations and their wealth measures evidence against the null hypothesis.
While recently developed kernel-based betting strategies often work well on
simple distributions, selecting a suitable kernel for high-dimensional or
structured data, such as images, is often nontrivial. To address this drawback,
we design prediction-based betting strategies that rely on the following fact:
if a sequentially updated predictor starts to consistently determine (a) which
distribution an instance is drawn from, or (b) whether an instance is drawn
from the joint distribution or the product of the marginal distributions (the
latter produced by external randomization), it provides evidence against the
two-sample or independence nulls respectively. We empirically demonstrate the
superiority of our tests over kernel-based approaches under structured
settings. Our tests can be applied beyond the case of independent and
identically distributed data, remaining valid and powerful even when the data
distribution drifts over time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Podkopaev_A/0/1/0/all/0/1"&gt;Aleksandr Podkopaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1"&gt;Aaditya Ramdas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Combining Expert Demonstrations in Imitation Learning via Optimal Transport. (arXiv:2307.10810v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10810</id>
        <link href="http://arxiv.org/abs/2307.10810"/>
        <updated>2023-07-22T00:55:26.690Z</updated>
        <summary type="html"><![CDATA[Imitation learning (IL) seeks to teach agents specific tasks through expert
demonstrations. One of the key approaches to IL is to define a distance between
agent and expert and to find an agent policy that minimizes that distance.
Optimal transport methods have been widely used in imitation learning as they
provide ways to measure meaningful distances between agent and expert
trajectories. However, the problem of how to optimally combine multiple expert
demonstrations has not been widely studied. The standard method is to simply
concatenate state (-action) trajectories, which is problematic when
trajectories are multi-modal. We propose an alternative method that uses a
multi-marginal optimal transport distance and enables the combination of
multiple and diverse state-trajectories in the OT sense, providing a more
sensible geometric average of the demonstrations. Our approach enables an agent
to learn from several experts, and its efficiency is analyzed on OpenAI Gym
control environments and demonstrates that the standard method is not always
optimal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sebag_I/0/1/0/all/0/1"&gt;Ilana Sebag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1"&gt;Samuel Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deisenroth_M/0/1/0/all/0/1"&gt;Marc Peter Deisenroth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[It Is All About Data: A Survey on the Effects of Data on Adversarial Robustness. (arXiv:2303.09767v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.09767</id>
        <link href="http://arxiv.org/abs/2303.09767"/>
        <updated>2023-07-22T00:55:26.663Z</updated>
        <summary type="html"><![CDATA[Adversarial examples are inputs to machine learning models that an attacker
has intentionally designed to confuse the model into making a mistake. Such
examples pose a serious threat to the applicability of machine-learning-based
systems, especially in life- and safety-critical domains. To address this
problem, the area of adversarial robustness investigates mechanisms behind
adversarial attacks and defenses against these attacks. This survey reviews a
particular subset of this literature that focuses on investigating properties
of training data in the context of model robustness under evasion attacks. It
first summarizes the main properties of data leading to adversarial
vulnerability. It then discusses guidelines and techniques for improving
adversarial robustness by enhancing the data representation and learning
procedures, as well as techniques for estimating robustness guarantees given
particular data. Finally, it discusses gaps of knowledge and promising future
research directions in this area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_P/0/1/0/all/0/1"&gt;Peiyu Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tegegn_M/0/1/0/all/0/1"&gt;Michael Tegegn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarin_J/0/1/0/all/0/1"&gt;Jaskeerat Singh Sarin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1"&gt;Shubhraneel Pal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubin_J/0/1/0/all/0/1"&gt;Julia Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Syntactic vs Semantic Linear Abstraction and Refinement of Neural Networks. (arXiv:2307.10891v1 [cs.LO])]]></title>
        <id>http://arxiv.org/abs/2307.10891</id>
        <link href="http://arxiv.org/abs/2307.10891"/>
        <updated>2023-07-22T00:55:26.635Z</updated>
        <summary type="html"><![CDATA[Abstraction is a key verification technique to improve scalability. However,
its use for neural networks is so far extremely limited. Previous approaches
for abstracting classification networks replace several neurons with one of
them that is similar enough. We can classify the similarity as defined either
syntactically (using quantities on the connections between neurons) or
semantically (on the activation values of neurons for various inputs).
Unfortunately, the previous approaches only achieve moderate reductions, when
implemented at all. In this work, we provide a more flexible framework where a
neuron can be replaced with a linear combination of other neurons, improving
the reduction. We apply this approach both on syntactic and semantic
abstractions, and implement and evaluate them experimentally. Further, we
introduce a refinement method for our abstractions, allowing for finding a
better balance between reduction and precision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chau_C/0/1/0/all/0/1"&gt;Calvin Chau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kretinsky_J/0/1/0/all/0/1"&gt;Jan K&amp;#x159;et&amp;#xed;nsk&amp;#xfd;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohr_S/0/1/0/all/0/1"&gt;Stefanie Mohr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pythae: Unifying Generative Autoencoders in Python -- A Benchmarking Use Case. (arXiv:2206.08309v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2206.08309</id>
        <link href="http://arxiv.org/abs/2206.08309"/>
        <updated>2023-07-22T00:55:26.626Z</updated>
        <summary type="html"><![CDATA[In recent years, deep generative models have attracted increasing interest
due to their capacity to model complex distributions. Among those models,
variational autoencoders have gained popularity as they have proven both to be
computationally efficient and yield impressive results in multiple fields.
Following this breakthrough, extensive research has been done in order to
improve the original publication, resulting in a variety of different VAE
models in response to different tasks. In this paper we present Pythae, a
versatile open-source Python library providing both a unified implementation
and a dedicated framework allowing straightforward, reproducible and reliable
use of generative autoencoder models. We then propose to use this library to
perform a case study benchmark where we present and compare 19 generative
autoencoder models representative of some of the main improvements on
downstream tasks such as image reconstruction, generation, classification,
clustering and interpolation. The open-source library can be found at
https://github.com/clementchadebec/benchmark_VAE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chadebec_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment Chadebec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vincent_L/0/1/0/all/0/1"&gt;Louis J. Vincent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allassonniere_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phanie Allassonni&amp;#xe8;re&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability. (arXiv:2208.09418v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.09418</id>
        <link href="http://arxiv.org/abs/2208.09418"/>
        <updated>2023-07-22T00:55:26.618Z</updated>
        <summary type="html"><![CDATA[Interpretability of Deep Learning (DL) is a barrier to trustworthy AI.
Despite great efforts made by the Explainable AI (XAI) community, explanations
lack robustness -- indistinguishable input perturbations may lead to different
XAI results. Thus, it is vital to assess how robust DL interpretability is,
given an XAI method. In this paper, we identify several challenges that the
state-of-the-art is unable to cope with collectively: i) existing metrics are
not comprehensive; ii) XAI techniques are highly heterogeneous; iii)
misinterpretations are normally rare events. To tackle these challenges, we
introduce two black-box evaluation methods, concerning the worst-case
interpretation discrepancy and a probabilistic notion of how robust in general,
respectively. Genetic Algorithm (GA) with bespoke fitness function is used to
solve constrained optimisation for efficient worst-case evaluation. Subset
Simulation (SS), dedicated to estimate rare event probabilities, is used for
evaluating overall robustness. Experiments show that the accuracy, sensitivity,
and efficiency of our methods outperform the state-of-the-arts. Finally, we
demonstrate two applications of our methods: ranking robust XAI methods and
selecting training schemes to improve both classification and interpretation
robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xingyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_G/0/1/0/all/0/1"&gt;Gaojie Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaowei Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Mixture of HyperGenerators for Learning Distributions Over Functions. (arXiv:2302.06223v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.06223</id>
        <link href="http://arxiv.org/abs/2302.06223"/>
        <updated>2023-07-22T00:55:26.610Z</updated>
        <summary type="html"><![CDATA[Recent approaches build on implicit neural representations (INRs) to propose
generative models over function spaces. However, they are computationally
costly when dealing with inference tasks, such as missing data imputation, or
directly cannot tackle them. In this work, we propose a novel deep generative
model, named VAMoH. VAMoH combines the capabilities of modeling continuous
functions using INRs and the inference capabilities of Variational Autoencoders
(VAEs). In addition, VAMoH relies on a normalizing flow to define the prior,
and a mixture of hypernetworks to parametrize the data log-likelihood. This
gives VAMoH a high expressive capability and interpretability. Through
experiments on a diverse range of data types, such as images, voxels, and
climate data, we show that VAMoH can effectively learn rich distributions over
continuous functions. Furthermore, it can perform inference-related tasks, such
as conditional super-resolution generation and in-painting, as well or better
than previous approaches, while being less computationally demanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koyuncu_B/0/1/0/all/0/1"&gt;Batuhan Koyuncu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_Martin_P/0/1/0/all/0/1"&gt;Pablo Sanchez-Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peis_I/0/1/0/all/0/1"&gt;Ignacio Peis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olmos_P/0/1/0/all/0/1"&gt;Pablo M. Olmos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valera_I/0/1/0/all/0/1"&gt;Isabel Valera&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty. (arXiv:2307.07666v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.07666</id>
        <link href="http://arxiv.org/abs/2307.07666"/>
        <updated>2023-07-22T00:55:26.586Z</updated>
        <summary type="html"><![CDATA[Robust reinforcement learning (RL) aims to find a policy that optimizes the
worst-case performance in the face of uncertainties. In this paper, we focus on
action robust RL with the probabilistic policy execution uncertainty, in which,
instead of always carrying out the action specified by the policy, the agent
will take the action specified by the policy with probability $1-\rho$ and an
alternative adversarial action with probability $\rho$. We establish the
existence of an optimal policy on the action robust MDPs with probabilistic
policy execution uncertainty and provide the action robust Bellman optimality
equation for its solution. Furthermore, we develop Action Robust Reinforcement
Learning with Certificates (ARRLC) algorithm that achieves minimax optimal
regret and sample complexity. Furthermore, we conduct numerical experiments to
validate our approach's robustness, demonstrating that ARRLC outperforms
non-robust RL algorithms and converges faster than the robust TD algorithm in
the presence of action perturbations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guanlin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhihan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Han Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1"&gt;Lifeng Lai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities. (arXiv:2307.10803v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10803</id>
        <link href="http://arxiv.org/abs/2307.10803"/>
        <updated>2023-07-22T00:55:26.577Z</updated>
        <summary type="html"><![CDATA[With the increasing amount of spatial-temporal~(ST) ocean data, numerous
spatial-temporal data mining (STDM) studies have been conducted to address
various oceanic issues, e.g., climate forecasting and disaster warning.
Compared with typical ST data (e.g., traffic data), ST ocean data is more
complicated with some unique characteristics, e.g., diverse regionality and
high sparsity. These characteristics make it difficult to design and train STDM
models. Unfortunately, an overview of these studies is still missing, hindering
computer scientists to identify the research issues in ocean while discouraging
researchers in ocean science from applying advanced STDM techniques. To remedy
this situation, we provide a comprehensive survey to summarize existing STDM
studies in ocean. Concretely, we first summarize the widely-used ST ocean
datasets and identify their unique characteristics. Then, typical ST ocean data
quality enhancement techniques are discussed. Next, we classify existing STDM
studies for ocean into four types of tasks, i.e., prediction, event detection,
pattern mining, and anomaly detection, and elaborate the techniques for these
tasks. Finally, promising research opportunities are highlighted. This survey
will help scientists from the fields of both computer science and ocean science
have a better understanding of the fundamental concepts, key techniques, and
open challenges of STDM in ocean.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hanchen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wengen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1"&gt;Jihong Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shuigeng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jiannong Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topological Point Cloud Clustering. (arXiv:2303.16716v2 [math.AT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.16716</id>
        <link href="http://arxiv.org/abs/2303.16716"/>
        <updated>2023-07-22T00:55:26.538Z</updated>
        <summary type="html"><![CDATA[We present Topological Point Cloud Clustering (TPCC), a new method to cluster
points in an arbitrary point cloud based on their contribution to global
topological features. TPCC synthesizes desirable features from spectral
clustering and topological data analysis and is based on considering the
spectral properties of a simplicial complex associated to the considered point
cloud. As it is based on considering sparse eigenvector computations, TPCC is
similarly easy to interpret and implement as spectral clustering. However, by
focusing not just on a single matrix associated to a graph created from the
point cloud data, but on a whole set of Hodge-Laplacians associated to an
appropriately constructed simplicial complex, we can leverage a far richer set
of topological features to characterize the data points within the point cloud
and benefit from the relative robustness of topological techniques against
noise. We test the performance of TPCC on both synthetic and real-world data
and compare it with classical spectral clustering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Grande_V/0/1/0/all/0/1"&gt;Vincent P. Grande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Schaub_M/0/1/0/all/0/1"&gt;Michael T. Schaub&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep-Q Learning with Hybrid Quantum Neural Network on Solving Maze Problems. (arXiv:2304.10159v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.10159</id>
        <link href="http://arxiv.org/abs/2304.10159"/>
        <updated>2023-07-22T00:55:26.525Z</updated>
        <summary type="html"><![CDATA[Quantum computing holds great potential for advancing the limitations of
machine learning algorithms to handle higher data dimensions and reduce overall
training parameters in deep neural network (DNN) models. This study uses a
parameterized quantum circuit (PQC) on a gate-based quantum computer to
investigate the potential for quantum advantage in a model-free reinforcement
learning problem. Through a comprehensive investigation and evaluation of the
current model and capabilities of quantum computers, we designed and trained a
novel hybrid Quantum neural network based on the latest Qiskit and PyTorch
framework. We compared its performance with a full-classical DNN with and
without an integrated PQC. Our research provides insights into the potential of
deep quantum learning to solve a maze problem and, potentially, other
reinforcement learning problems. We conclude that various reinforcement
learning problems can be effective with reasonable training epochs. Moreover, a
comparative discussion of the various quantum reinforcement learning model on
maze problems is discussed to evaluate our research's overall potential and
advantages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao-Yuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Yen-Jui Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Chang_C/0/1/0/all/0/1"&gt;Ching-Ray Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.06092</id>
        <link href="http://arxiv.org/abs/2307.06092"/>
        <updated>2023-07-22T00:55:26.471Z</updated>
        <summary type="html"><![CDATA[We study the distribution of a fully connected neural network with random
Gaussian weights and biases in which the hidden layer widths are proportional
to a large constant $n$. Under mild assumptions on the non-linearity, we obtain
quantitative bounds on normal approximations valid at large but finite $n$ and
any fixed network depth. Our theorems show both for the finite-dimensional
distributions and the entire process, that the distance between a random fully
connected network (and its derivatives) to the corresponding infinite width
Gaussian process scales like $n^{-\gamma}$ for $\gamma>0$, with the exponent
depending on the metric used to measure discrepancy. Our bounds are strictly
stronger in terms of their dependence on network width than any previously
available in the literature; in the one-dimensional case, we also prove that
they are optimal, i.e., we establish matching lower bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Favaro_S/0/1/0/all/0/1"&gt;Stefano Favaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanin_B/0/1/0/all/0/1"&gt;Boris Hanin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marinucci_D/0/1/0/all/0/1"&gt;Domenico Marinucci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nourdin_I/0/1/0/all/0/1"&gt;Ivan Nourdin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peccati_G/0/1/0/all/0/1"&gt;Giovanni Peccati&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When are Local Queries Useful for Robust Learning?. (arXiv:2210.06089v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.06089</id>
        <link href="http://arxiv.org/abs/2210.06089"/>
        <updated>2023-07-22T00:55:26.466Z</updated>
        <summary type="html"><![CDATA[Distributional assumptions have been shown to be necessary for the robust
learnability of concept classes when considering the exact-in-the-ball robust
risk and access to random examples by Gourdeau et al. (2019). In this paper, we
study learning models where the learner is given more power through the use of
local queries, and give the first distribution-free algorithms that perform
robust empirical risk minimization (ERM) for this notion of robustness. The
first learning model we consider uses local membership queries (LMQ), where the
learner can query the label of points near the training sample. We show that,
under the uniform distribution, LMQs do not increase the robustness threshold
of conjunctions and any superclass, e.g., decision lists and halfspaces. Faced
with this negative result, we introduce the local equivalence query
($\mathsf{LEQ}$) oracle, which returns whether the hypothesis and target
concept agree in the perturbation region around a point in the training sample,
as well as a counterexample if it exists. We show a separation result: on the
one hand, if the query radius $\lambda$ is strictly smaller than the
adversary's perturbation budget $\rho$, then distribution-free robust learning
is impossible for a wide variety of concept classes; on the other hand, the
setting $\lambda=\rho$ allows us to develop robust ERM algorithms. We then
bound the query complexity of these algorithms based on online learning
guarantees and further improve these bounds for the special case of
conjunctions. We finish by giving robust learning algorithms for halfspaces on
$\{0,1\}^n$ and then obtaining robustness guarantees for halfspaces in
$\mathbb{R}^n$ against precision-bounded adversaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gourdeau_P/0/1/0/all/0/1"&gt;Pascale Gourdeau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanade_V/0/1/0/all/0/1"&gt;Varun Kanade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwiatkowska_M/0/1/0/all/0/1"&gt;Marta Kwiatkowska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Worrell_J/0/1/0/all/0/1"&gt;James Worrell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perceptron Theory Can Predict the Accuracy of Neural Networks. (arXiv:2012.07881v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07881</id>
        <link href="http://arxiv.org/abs/2012.07881"/>
        <updated>2023-07-22T00:55:26.452Z</updated>
        <summary type="html"><![CDATA[Multilayer neural networks set the current state of the art for many
technical classification problems. But, these networks are still, essentially,
black boxes in terms of analyzing them and predicting their performance. Here,
we develop a statistical theory for the one-layer perceptron and show that it
can predict performances of a surprisingly large variety of neural networks
with different architectures. A general theory of classification with
perceptrons is developed by generalizing an existing theory for analyzing
reservoir computing models and connectionist models for symbolic reasoning
known as vector symbolic architectures. Our statistical theory offers three
formulas leveraging the signal statistics with increasing detail. The formulas
are analytically intractable, but can be evaluated numerically. The description
level that captures maximum details requires stochastic sampling methods.
Depending on the network model, the simpler formulas already yield high
prediction accuracy. The quality of the theory predictions is assessed in three
experimental settings, a memorization task for echo state networks (ESNs) from
reservoir computing literature, a collection of classification datasets for
shallow randomly connected networks, and the ImageNet dataset for deep
convolutional neural networks. We find that the second description level of the
perceptron theory can predict the performance of types of ESNs, which could not
be described previously. The theory can predict deep multilayer neural networks
by being applied to their output layer. While other methods for prediction of
neural networks performance commonly require to train an estimator model, the
proposed theory requires only the first two moments of the distribution of the
postsynaptic sums in the output neurons. The perceptron theory compares
favorably to other methods that do not rely on training an estimator model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kleyko_D/0/1/0/all/0/1"&gt;Denis Kleyko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosato_A/0/1/0/all/0/1"&gt;Antonello Rosato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frady_E/0/1/0/all/0/1"&gt;E. Paxon Frady&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panella_M/0/1/0/all/0/1"&gt;Massimo Panella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sommer_F/0/1/0/all/0/1"&gt;Friedrich T. Sommer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-Driven Latency Probability Prediction for Wireless Networks: Focusing on Tail Probabilities. (arXiv:2307.10648v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2307.10648</id>
        <link href="http://arxiv.org/abs/2307.10648"/>
        <updated>2023-07-22T00:55:26.436Z</updated>
        <summary type="html"><![CDATA[With the emergence of new application areas, such as cyber-physical systems
and human-in-the-loop applications, there is a need to guarantee a certain
level of end-to-end network latency with extremely high reliability, e.g.,
99.999%. While mechanisms specified under IEEE 802.1as time-sensitive
networking (TSN) can be used to achieve these requirements for switched
Ethernet networks, implementing TSN mechanisms in wireless networks is
challenging due to their stochastic nature. To conform the wireless link to a
reliability level of 99.999%, the behavior of extremely rare outliers in the
latency probability distribution, or the tail of the distribution, must be
analyzed and controlled. This work proposes predicting the tail of the latency
distribution using state-of-the-art data-driven approaches, such as mixture
density networks (MDN) and extreme value mixture models, to estimate the
likelihood of rare latencies conditioned on the network parameters, which can
be used to make more informed decisions in wireless transmission. Actual
latency measurements of IEEE 802.11g (WiFi), commercial private and a
software-defined 5G network are used to benchmark the proposed approaches and
evaluate their sensitivities concerning the tail probabilities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mostafavi_S/0/1/0/all/0/1"&gt;Samie Mostafavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1"&gt;Gourav Prateek Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gross_J/0/1/0/all/0/1"&gt;James Gross&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Injecting Domain Adaptation with Learning-to-hash for Effective and Efficient Zero-shot Dense Retrieval. (arXiv:2205.11498v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2205.11498</id>
        <link href="http://arxiv.org/abs/2205.11498"/>
        <updated>2023-07-22T00:55:26.435Z</updated>
        <summary type="html"><![CDATA[Dense retrieval overcome the lexical gap and has shown great success in
ad-hoc information retrieval (IR). Despite their success, dense retrievers are
expensive to serve across practical use cases. For use cases requiring to
search from millions of documents, the dense index becomes bulky and requires
high memory usage for storing the index. More recently, learning-to-hash (LTH)
techniques, for e.g., BPR and JPQ, produce binary document vectors, thereby
reducing the memory requirement to efficiently store the dense index. LTH
techniques are supervised and finetune the retriever using a ranking loss. They
outperform their counterparts, i.e., traditional out-of-the-box vector
compression techniques such as PCA or PQ. A missing piece from prior work is
that existing techniques have been evaluated only in-domain, i.e., on a single
dataset such as MS MARCO. In our work, we evaluate LTH and vector compression
techniques for improving the downstream zero-shot retrieval accuracy of the
TAS-B dense retriever while maintaining efficiency at inference. Our results
demonstrate that, unlike prior work, LTH strategies when applied naively can
underperform the zero-shot TAS-B dense retriever on average by up to 14%
nDCG@10 on the BEIR benchmark. To solve this limitation, in our work, we
propose an easy yet effective solution of injecting domain adaptation with
existing supervised LTH techniques. We experiment with two well-known
unsupervised domain adaptation techniques: GenQ and GPL. Our domain adaptation
injection technique can improve the downstream zero-shot retrieval
effectiveness for both BPR and JPQ variants of the TAS-B model by on average
11.5% and 8.2% nDCG@10 while both maintaining 32$\times$ memory efficiency and
14$\times$ and 2$\times$ speedup respectively in CPU retrieval latency on BEIR.
All our code, models, and data are publicly available at
https://github.com/thakur-nandan/income.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1"&gt;Nandan Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1"&gt;Nils Reimers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jimmy Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Positive unlabeled learning with tensor networks. (arXiv:2211.14085v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.14085</id>
        <link href="http://arxiv.org/abs/2211.14085"/>
        <updated>2023-07-22T00:55:26.420Z</updated>
        <summary type="html"><![CDATA[Positive unlabeled learning is a binary classification problem with positive
and unlabeled data. It is common in domains where negative labels are costly or
impossible to obtain, e.g., medicine and personalized advertising. Most
approaches to positive unlabeled learning apply to specific data types (e.g.,
images, categorical data) and can not generate new positive and negative
samples. This work introduces a feature-space distance-based tensor network
approach to the positive unlabeled learning problem. The presented method is
not domain specific and significantly improves the state-of-the-art results on
the MNIST image and 15 categorical/mixed datasets. The trained tensor network
model is also a generative model and enables the generation of new positive and
negative instances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zunkovic_B/0/1/0/all/0/1"&gt;Bojan &amp;#x17d;unkovi&amp;#x10d;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chordal Averaging on Flag Manifolds and Its Applications. (arXiv:2303.13501v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.13501</id>
        <link href="http://arxiv.org/abs/2303.13501"/>
        <updated>2023-07-22T00:55:26.399Z</updated>
        <summary type="html"><![CDATA[This paper presents a new, provably-convergent algorithm for computing the
flag-mean and flag-median of a set of points on a flag manifold under the
chordal metric. The flag manifold is a mathematical space consisting of flags,
which are sequences of nested subspaces of a vector space that increase in
dimension. The flag manifold is a superset of a wide range of known matrix
spaces, including Stiefel and Grassmanians, making it a general object that is
useful in a wide variety computer vision problems.

To tackle the challenge of computing first order flag statistics, we first
transform the problem into one that involves auxiliary variables constrained to
the Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and
leveraging the numerical stability and efficiency of Stiefel-manifold
optimization enables us to compute the flag-mean effectively. Through a series
of experiments, we show the competence of our method in Grassmann and rotation
averaging, as well as principal component analysis. We release our source code
under https://github.com/nmank/FlagAveraging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mankovich_N/0/1/0/all/0/1"&gt;Nathan Mankovich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1"&gt;Tolga Birdal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of What to Share in Federated Learning: Perspectives on Model Utility, Privacy Leakage, and Communication Efficiency. (arXiv:2307.10655v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10655</id>
        <link href="http://arxiv.org/abs/2307.10655"/>
        <updated>2023-07-22T00:55:26.100Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) has emerged as a highly effective paradigm for
privacy-preserving collaborative training among different parties. Unlike
traditional centralized learning, which requires collecting data from each
party, FL allows clients to share privacy-preserving information without
exposing private datasets. This approach not only guarantees enhanced privacy
protection but also facilitates more efficient and secure collaboration among
multiple participants. Therefore, FL has gained considerable attention from
researchers, promoting numerous surveys to summarize the related works.
However, the majority of these surveys concentrate on methods sharing model
parameters during the training process, while overlooking the potential of
sharing other forms of local information. In this paper, we present a
systematic survey from a new perspective, i.e., what to share in FL, with an
emphasis on the model utility, privacy leakage, and communication efficiency.
This survey differs from previous ones due to four distinct contributions.
First, we present a new taxonomy of FL methods in terms of the sharing methods,
which includes three categories of shared information: model sharing, synthetic
data sharing, and knowledge sharing. Second, we analyze the vulnerability of
different sharing methods to privacy attacks and review the defense mechanisms
that provide certain privacy guarantees. Third, we conduct extensive
experiments to compare the performance and communication overhead of various
sharing methods in FL. Besides, we assess the potential privacy leakage through
model inversion and membership inference attacks, while comparing the
effectiveness of various defense approaches. Finally, we discuss potential
deficiencies in current methods and outline future directions for improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1"&gt;Jiawei Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zijian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wenqiang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Tailin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yuchang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lumin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zehong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Curiosity in Hindsight: Intrinsic Exploration in Stochastic Environments. (arXiv:2211.10515v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.10515</id>
        <link href="http://arxiv.org/abs/2211.10515"/>
        <updated>2023-07-22T00:55:26.098Z</updated>
        <summary type="html"><![CDATA[Consider the problem of exploration in sparse-reward or reward-free
environments, such as in Montezuma's Revenge. In the curiosity-driven paradigm,
the agent is rewarded for how much each realized outcome differs from their
predicted outcome. But using predictive error as intrinsic motivation is
fragile in stochastic environments, as the agent may become trapped by
high-entropy areas of the state-action space, such as a "noisy TV". In this
work, we study a natural solution derived from structural causal models of the
world: Our key idea is to learn representations of the future that capture
precisely the unpredictable aspects of each outcome -- which we use as
additional input for predictions, such that intrinsic rewards only reflect the
predictable aspects of world dynamics. First, we propose incorporating such
hindsight representations into models to disentangle "noise" from "novelty",
yielding Curiosity in Hindsight: a simple and scalable generalization of
curiosity that is robust to stochasticity. Second, we instantiate this
framework for the recently introduced BYOL-Explore algorithm as our prime
example, resulting in the noise-robust BYOL-Hindsight. Third, we illustrate its
behavior under a variety of different stochasticities in a grid world, and find
improvements over BYOL-Explore in hard-exploration Atari games with sticky
actions. Notably, we show state-of-the-art results in exploring Montezuma's
Revenge with sticky actions, while preserving performance in the non-sticky
setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jarrett_D/0/1/0/all/0/1"&gt;Daniel Jarrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tallec_C/0/1/0/all/0/1"&gt;Corentin Tallec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Altche_F/0/1/0/all/0/1"&gt;Florent Altch&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mesnard_T/0/1/0/all/0/1"&gt;Thomas Mesnard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Munos_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Munos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Valko_M/0/1/0/all/0/1"&gt;Michal Valko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intelligent model for offshore China sea fog forecasting. (arXiv:2307.10580v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10580</id>
        <link href="http://arxiv.org/abs/2307.10580"/>
        <updated>2023-07-22T00:55:26.097Z</updated>
        <summary type="html"><![CDATA[Accurate and timely prediction of sea fog is very important for effectively
managing maritime and coastal economic activities. Given the intricate nature
and inherent variability of sea fog, traditional numerical and statistical
forecasting methods are often proven inadequate. This study aims to develop an
advanced sea fog forecasting method embedded in a numerical weather prediction
model using the Yangtze River Estuary (YRE) coastal area as a case study. Prior
to training our machine learning model, we employ a time-lagged correlation
analysis technique to identify key predictors and decipher the underlying
mechanisms driving sea fog occurrence. In addition, we implement ensemble
learning and a focal loss function to address the issue of imbalanced data,
thereby enhancing the predictive ability of our model. To verify the accuracy
of our method, we evaluate its performance using a comprehensive dataset
spanning one year, which encompasses both weather station observations and
historical forecasts. Remarkably, our machine learning-based approach surpasses
the predictive performance of two conventional methods, the weather research
and forecasting nonhydrostatic mesoscale model (WRF-NMM) and the algorithm
developed by the National Oceanic and Atmospheric Administration (NOAA)
Forecast Systems Laboratory (FSL). Specifically, in regard to predicting sea
fog with a visibility of less than or equal to 1 km with a lead time of 60
hours, our methodology achieves superior results by increasing the probability
of detection (POD) while simultaneously reducing the false alarm ratio (FAR).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1"&gt;Yanfei Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qinghong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mingqing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_R/0/1/0/all/0/1"&gt;Ruixue Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1"&gt;Yang Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaomeng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MetaMask: Revisiting Dimensional Confounder for Self-Supervised Learning. (arXiv:2209.07902v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2209.07902</id>
        <link href="http://arxiv.org/abs/2209.07902"/>
        <updated>2023-07-22T00:55:26.090Z</updated>
        <summary type="html"><![CDATA[As a successful approach to self-supervised learning, contrastive learning
aims to learn invariant information shared among distortions of the input
sample. While contrastive learning has yielded continuous advancements in
sampling strategy and architecture design, it still remains two persistent
defects: the interference of task-irrelevant information and sample
inefficiency, which are related to the recurring existence of trivial constant
solutions. From the perspective of dimensional analysis, we find out that the
dimensional redundancy and dimensional confounder are the intrinsic issues
behind the phenomena, and provide experimental evidence to support our
viewpoint. We further propose a simple yet effective approach MetaMask, short
for the dimensional Mask learned by Meta-learning, to learn representations
against dimensional redundancy and confounder. MetaMask adopts the
redundancy-reduction technique to tackle the dimensional redundancy issue and
innovatively introduces a dimensional mask to reduce the gradient effects of
specific dimensions containing the confounder, which is trained by employing a
meta-learning paradigm with the objective of improving the performance of
masked representations on a typical self-supervised task. We provide solid
theoretical analyses to prove MetaMask can obtain tighter risk bounds for
downstream classification compared to typical contrastive methods. Empirically,
our method achieves state-of-the-art performance on various benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiangmeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1"&gt;Wenwen Qiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yanan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mo_W/0/1/0/all/0/1"&gt;Wenyi Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Changwen Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1"&gt;Bing Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hui Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can point cloud networks learn statistical shape models of anatomies?. (arXiv:2305.05610v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.05610</id>
        <link href="http://arxiv.org/abs/2305.05610"/>
        <updated>2023-07-22T00:55:26.089Z</updated>
        <summary type="html"><![CDATA[Statistical Shape Modeling (SSM) is a valuable tool for investigating and
quantifying anatomical variations within populations of anatomies. However,
traditional correspondence-based SSM generation methods have a prohibitive
inference process and require complete geometric proxies (e.g., high-resolution
binary volumes or surface meshes) as input shapes to construct the SSM.
Unordered 3D point cloud representations of shapes are more easily acquired
from various medical imaging practices (e.g., thresholded images and surface
scanning). Point cloud deep networks have recently achieved remarkable success
in learning permutation-invariant features for different point cloud tasks
(e.g., completion, semantic segmentation, classification). However, their
application to learning SSM from point clouds is to-date unexplored. In this
work, we demonstrate that existing point cloud encoder-decoder-based completion
networks can provide an untapped potential for SSM, capturing population-level
statistical representations of shapes while reducing the inference burden and
relaxing the input requirement. We discuss the limitations of these techniques
to the SSM application and suggest future improvements. Our work paves the way
for further exploration of point cloud deep learning for SSM, a promising
avenue for advancing shape analysis literature and broadening SSM to diverse
use cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Adams_J/0/1/0/all/0/1"&gt;Jadie Adams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elhabian_S/0/1/0/all/0/1"&gt;Shireen Elhabian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Graph Generation to Graph Classification. (arXiv:2302.07989v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.07989</id>
        <link href="http://arxiv.org/abs/2302.07989"/>
        <updated>2023-07-22T00:55:26.088Z</updated>
        <summary type="html"><![CDATA[This note describes a new approach to classifying graphs that leverages graph
generative models (GGM). Assuming a GGM that defines a joint probability
distribution over graphs and their class labels, I derive classification
formulas for the probability of a class label given a graph. A new conditional
ELBO can be used to train a generative graph auto-encoder model for
discrimination. While leveraging generative models for classification has been
well explored for non-relational i.i.d. data, to our knowledge it is a novel
approach to graph classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schulte_O/0/1/0/all/0/1"&gt;Oliver Schulte&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PGCN: Progressive Graph Convolutional Networks for Spatial-Temporal Traffic Forecasting. (arXiv:2202.08982v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.08982</id>
        <link href="http://arxiv.org/abs/2202.08982"/>
        <updated>2023-07-22T00:55:26.086Z</updated>
        <summary type="html"><![CDATA[The complex spatial-temporal correlations in transportation networks make the
traffic forecasting problem challenging. Since transportation system inherently
possesses graph structures, much research efforts have been put with graph
neural networks. Recently, constructing adaptive graphs to the data has shown
promising results over the models relying on a single static graph structure.
However, the graph adaptations are applied during the training phases, and do
not reflect the data used during the testing phases. Such shortcomings can be
problematic especially in traffic forecasting since the traffic data often
suffers from the unexpected changes and irregularities in the time series. In
this study, we propose a novel traffic forecasting framework called Progressive
Graph Convolutional Network (PGCN). PGCN constructs a set of graphs by
progressively adapting to input data during the training and the testing
phases. Specifically, we implemented the model to construct progressive
adjacency matrices by learning trend similarities among graph nodes. Then, the
model is combined with the dilated causal convolution and gated activation unit
to extract temporal features. With residual and skip connections, PGCN performs
the traffic prediction. When applied to four real-world traffic datasets of
diverse geometric nature, the proposed model achieves state-of-the-art
performance with consistency in all datasets. We conclude that the ability of
PGCN to progressively adapt to input data enables the model to generalize in
different study sites with robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1"&gt;Yuyol Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_Y/0/1/0/all/0/1"&gt;Yoonjin Yoon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional expectation network for SHAP. (arXiv:2307.10654v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10654</id>
        <link href="http://arxiv.org/abs/2307.10654"/>
        <updated>2023-07-22T00:55:26.081Z</updated>
        <summary type="html"><![CDATA[A very popular model-agnostic technique for explaining predictive models is
the SHapley Additive exPlanation (SHAP). The two most popular versions of SHAP
are a conditional expectation version and an unconditional expectation version
(the latter is also known as interventional SHAP). Except for tree-based
methods, usually the unconditional version is used (for computational reasons).
We provide a (surrogate) neural network approach which allows us to efficiently
calculate the conditional version for both neural networks and other regression
models, and which properly considers the dependence structure in the feature
components. This proposal is also useful to provide drop1 and anova analyses in
complex regression models which are similar to their generalized linear model
(GLM) counterparts, and we provide a partial dependence plot (PDP) counterpart
that considers the right dependence structure in the feature components.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Richman_R/0/1/0/all/0/1"&gt;Ronald Richman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wuthrich_M/0/1/0/all/0/1"&gt;Mario V. W&amp;#xfc;thrich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing PatchCore for Few/many-shot Anomaly Detection. (arXiv:2307.10792v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.10792</id>
        <link href="http://arxiv.org/abs/2307.10792"/>
        <updated>2023-07-22T00:55:26.077Z</updated>
        <summary type="html"><![CDATA[Few-shot anomaly detection (AD) is an emerging sub-field of general AD, and
tries to distinguish between normal and anomalous data using only few selected
samples. While newly proposed few-shot AD methods do compare against
pre-existing algorithms developed for the full-shot domain as baselines, they
do not dedicatedly optimize them for the few-shot setting. It thus remains
unclear if the performance of such pre-existing algorithms can be further
improved. We address said question in this work. Specifically, we present a
study on the AD/anomaly segmentation (AS) performance of PatchCore, the current
state-of-the-art full-shot AD/AS algorithm, in both the few-shot and the
many-shot settings. We hypothesize that further performance improvements can be
realized by (I) optimizing its various hyperparameters, and by (II)
transferring techniques known to improve few-shot supervised learning to the AD
domain. Exhaustive experiments on the public VisA and MVTec AD datasets reveal
that (I) significant performance improvements can be realized by optimizing
hyperparameters such as the underlying feature extractor, and that (II)
image-level augmentations can, but are not guaranteed, to improve performance.
Based on these findings, we achieve a new state of the art in few-shot AD on
VisA, further demonstrating the merit of adapting pre-existing AD/AS methods to
the few-shot setting. Last, we identify the investigation of feature extractors
with a strong inductive bias as a potential future research direction for
(few-shot) AD/AS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Triet Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rippel_O/0/1/0/all/0/1"&gt;Oliver Rippel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Offline Data in Online Reinforcement Learning. (arXiv:2211.04974v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.04974</id>
        <link href="http://arxiv.org/abs/2211.04974"/>
        <updated>2023-07-22T00:55:26.076Z</updated>
        <summary type="html"><![CDATA[Two central paradigms have emerged in the reinforcement learning (RL)
community: online RL and offline RL. In the online RL setting, the agent has no
prior knowledge of the environment, and must interact with it in order to find
an $\epsilon$-optimal policy. In the offline RL setting, the learner instead
has access to a fixed dataset to learn from, but is unable to otherwise
interact with the environment, and must obtain the best policy it can from this
offline data. Practical scenarios often motivate an intermediate setting: if we
have some set of offline data and, in addition, may also interact with the
environment, how can we best use the offline data to minimize the number of
online interactions necessary to learn an $\epsilon$-optimal policy?

In this work, we consider this setting, which we call the \textsf{FineTuneRL}
setting, for MDPs with linear structure. We characterize the necessary number
of online samples needed in this setting given access to some offline dataset,
and develop an algorithm, \textsc{FTPedel}, which is provably optimal, up to
$H$ factors. We show through an explicit example that combining offline data
with online interactions can lead to a provable improvement over either purely
offline or purely online RL. Finally, our results illustrate the distinction
between \emph{verifiable} learning, the typical setting considered in online
RL, and \emph{unverifiable} learning, the setting often considered in offline
RL, and show that there is a formal separation between these regimes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wagenmaker_A/0/1/0/all/0/1"&gt;Andrew Wagenmaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1"&gt;Aldo Pacchiano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A DPLL(T) Framework for Verifying Deep Neural Networks. (arXiv:2307.10266v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10266</id>
        <link href="http://arxiv.org/abs/2307.10266"/>
        <updated>2023-07-22T00:55:26.069Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) have emerged as an effective approach to tackling
real-world problems. However, like human-written software,
automatically-generated DNNs can have bugs and be attacked. This thus attracts
many recent interests in developing effective and scalable DNN verification
techniques and tools. In this work, we introduce a NeuralSAT, a new constraint
solving approach to DNN verification. The design of NeuralSAT follows the
DPLL(T) algorithm used modern SMT solving, which includes (conflict) clause
learning, abstraction, and theory solving, and thus NeuralSAT can be considered
as an SMT framework for DNNs. Preliminary results show that the NeuralSAT
prototype is competitive to the state-of-the-art. We hope, with proper
optimization and engineering, NeuralSAT will carry the power and success of
modern SAT/SMT solvers to DNN verification. NeuralSAT is avaliable from:
https://github.com/dynaroars/neuralsat-solver]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duong_H/0/1/0/all/0/1"&gt;Hai Duong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linhan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;ThanhVu Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dwyer_M/0/1/0/all/0/1"&gt;Matthew Dwyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tuning Stochastic Gradient Algorithms for Statistical Inference via Large-Sample Asymptotics. (arXiv:2207.12395v3 [stat.CO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2207.12395</id>
        <link href="http://arxiv.org/abs/2207.12395"/>
        <updated>2023-07-22T00:55:26.056Z</updated>
        <summary type="html"><![CDATA[The tuning of stochastic gradient algorithms (SGAs) for optimization and
sampling is often based on heuristics and trial-and-error rather than
generalizable theory. We address this theory--practice gap by characterizing
the large-sample statistical asymptotics of SGAs via a joint
step-size--sample-size scaling limit. We show that iterate averaging with a
large fixed step size is robust to the choice of tuning parameters and
asymptotically has covariance proportional to that of the MLE sampling
distribution. We also prove a Bernstein--von Mises-like theorem to guide
tuning, including for generalized posteriors that are robust to model
misspecification. Numerical experiments validate our results and
recommendations in realistic finite-sample regimes. Our work lays the
foundation for a systematic analysis of other stochastic gradient Markov chain
Monte Carlo algorithms for a wide range of models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Negrea_J/0/1/0/all/0/1"&gt;Jeffrey Negrea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Feng_H/0/1/0/all/0/1"&gt;Haoyue Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Roy_D/0/1/0/all/0/1"&gt;Daniel M. Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Huggins_J/0/1/0/all/0/1"&gt;Jonathan H. Huggins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blockchain-Based Federated Learning: Incentivizing Data Sharing and Penalizing Dishonest Behavior. (arXiv:2307.10492v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10492</id>
        <link href="http://arxiv.org/abs/2307.10492"/>
        <updated>2023-07-22T00:55:26.013Z</updated>
        <summary type="html"><![CDATA[With the increasing importance of data sharing for collaboration and
innovation, it is becoming more important to ensure that data is managed and
shared in a secure and trustworthy manner. Data governance is a common approach
to managing data, but it faces many challenges such as data silos, data
consistency, privacy, security, and access control. To address these
challenges, this paper proposes a comprehensive framework that integrates data
trust in federated learning with InterPlanetary File System, blockchain, and
smart contracts to facilitate secure and mutually beneficial data sharing while
providing incentives, access control mechanisms, and penalizing any dishonest
behavior. The experimental results demonstrate that the proposed model is
effective in improving the accuracy of federated learning models while ensuring
the security and fairness of the data-sharing process. The research paper also
presents a decentralized federated learning platform that successfully trained
a CNN model on the MNIST dataset using blockchain technology. The platform
enables multiple workers to train the model simultaneously while maintaining
data privacy and security. The decentralized architecture and use of blockchain
technology allow for efficient communication and coordination between workers.
This platform has the potential to facilitate decentralized machine learning
and support privacy-preserving collaboration in various domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaberzadeh_A/0/1/0/all/0/1"&gt;Amir Jaberzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrestha_A/0/1/0/all/0/1"&gt;Ajay Kumar Shrestha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Faijan Ahamad Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaikh_M/0/1/0/all/0/1"&gt;Mohammed Afaan Shaikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dave_B/0/1/0/all/0/1"&gt;Bhargav Dave&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_J/0/1/0/all/0/1"&gt;Jason Geng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning. (arXiv:2307.10274v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2307.10274</id>
        <link href="http://arxiv.org/abs/2307.10274"/>
        <updated>2023-07-22T00:55:26.009Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a method to create domain-sensitive speech
recognition models that utilize textual domain information by conditioning its
generation on a given text prompt. This is accomplished by fine-tuning a
pre-trained, end-to-end model (Whisper) to learn from demonstrations with
prompt examples. We show that this ability can be generalized to different
domains and even various prompt contexts, with our model gaining a Word Error
Rate (WER) reduction of up to 33% on unseen datasets from various domains, such
as medical conversation, air traffic control communication, and financial
meetings. Considering the limited availability of audio-transcript pair data,
we further extend our method to text-only fine-tuning to achieve domain
sensitivity as well as domain adaptation. We demonstrate that our text-only
fine-tuned model can also attend to various prompt contexts, with the model
reaching the most WER reduction of 29% on the medical conversation dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liao_F/0/1/0/all/0/1"&gt;Feng-Ting Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chan_Y/0/1/0/all/0/1"&gt;Yung-Chieh Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yi-Chang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hsu_C/0/1/0/all/0/1"&gt;Chan-Jan Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shiu_D/0/1/0/all/0/1"&gt;Da-shan Shiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IncDSI: Incrementally Updatable Document Retrieval. (arXiv:2307.10323v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2307.10323</id>
        <link href="http://arxiv.org/abs/2307.10323"/>
        <updated>2023-07-22T00:55:26.003Z</updated>
        <summary type="html"><![CDATA[Differentiable Search Index is a recently proposed paradigm for document
retrieval, that encodes information about a corpus of documents within the
parameters of a neural network and directly maps queries to corresponding
documents. These models have achieved state-of-the-art performances for
document retrieval across many benchmarks. These kinds of models have a
significant limitation: it is not easy to add new documents after a model is
trained. We propose IncDSI, a method to add documents in real time (about
20-50ms per document), without retraining the model on the entire dataset (or
even parts thereof). Instead we formulate the addition of documents as a
constrained optimization problem that makes minimal changes to the network
parameters. Although orders of magnitude faster, our approach is competitive
with re-training the model on the whole dataset and enables the development of
document retrieval systems that can be updated with new information in
real-time. Our code for IncDSI is available at
https://github.com/varshakishore/IncDSI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kishore_V/0/1/0/all/0/1"&gt;Varsha Kishore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1"&gt;Chao Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lovelace_J/0/1/0/all/0/1"&gt;Justin Lovelace&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1"&gt;Yoav Artzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1"&gt;Kilian Q. Weinberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Machine Learning based Empirical Evaluation of Cyber Threat Actors High Level Attack Patterns over Low level Attack Patterns in Attributing Attacks. (arXiv:2307.10252v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2307.10252</id>
        <link href="http://arxiv.org/abs/2307.10252"/>
        <updated>2023-07-22T00:55:25.999Z</updated>
        <summary type="html"><![CDATA[Cyber threat attribution is the process of identifying the actor of an attack
incident in cyberspace. An accurate and timely threat attribution plays an
important role in deterring future attacks by applying appropriate and timely
defense mechanisms. Manual analysis of attack patterns gathered by honeypot
deployments, intrusion detection systems, firewalls, and via trace-back
procedures is still the preferred method of security analysts for cyber threat
attribution. Such attack patterns are low-level Indicators of Compromise (IOC).
They represent Tactics, Techniques, Procedures (TTP), and software tools used
by the adversaries in their campaigns. The adversaries rarely re-use them. They
can also be manipulated, resulting in false and unfair attribution. To
empirically evaluate and compare the effectiveness of both kinds of IOC, there
are two problems that need to be addressed. The first problem is that in recent
research works, the ineffectiveness of low-level IOC for cyber threat
attribution has been discussed intuitively. An empirical evaluation for the
measure of the effectiveness of low-level IOC based on a real-world dataset is
missing. The second problem is that the available dataset for high-level IOC
has a single instance for each predictive class label that cannot be used
directly for training machine learning models. To address these problems in
this research work, we empirically evaluate the effectiveness of low-level IOC
based on a real-world dataset that is specifically built for comparative
analysis with high-level IOC. The experimental results show that the high-level
IOC trained models effectively attribute cyberattacks with an accuracy of 95%
as compared to the low-level IOC trained models where accuracy is 40%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Noor_U/0/1/0/all/0/1"&gt;Umara Noor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahid_S/0/1/0/all/0/1"&gt;Sawera Shahid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanwal_R/0/1/0/all/0/1"&gt;Rimsha Kanwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rashid_Z/0/1/0/all/0/1"&gt;Zahid Rashid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness in AI and Its Long-Term Implications on Society. (arXiv:2304.09826v2 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.09826</id>
        <link href="http://arxiv.org/abs/2304.09826"/>
        <updated>2023-07-22T00:55:25.997Z</updated>
        <summary type="html"><![CDATA[Successful deployment of artificial intelligence (AI) in various settings has
led to numerous positive outcomes for individuals and society. However, AI
systems have also been shown to harm parts of the population due to biased
predictions. AI fairness focuses on mitigating such biases to ensure AI
decision making is not discriminatory towards certain groups. We take a closer
look at AI fairness and analyze how lack of AI fairness can lead to deepening
of biases over time and act as a social stressor. More specifically, we discuss
how biased models can lead to more negative real-world outcomes for certain
groups, which may then become more prevalent by deploying new AI models trained
on increasingly biased data, resulting in a feedback loop. If the issues
persist, they could be reinforced by interactions with other risks and have
severe implications on society in the form of social unrest. We examine current
strategies for improving AI fairness, assess their limitations in terms of
real-world deployment, and explore potential paths forward to ensure we reap
AI's benefits without causing society's collapse.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1"&gt;Ondrej Bohdal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1"&gt;Timothy Hospedales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1"&gt;Fazl Barez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting deceptive reviews using text classification. (arXiv:2307.10617v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2307.10617</id>
        <link href="http://arxiv.org/abs/2307.10617"/>
        <updated>2023-07-22T00:55:25.996Z</updated>
        <summary type="html"><![CDATA[In recent years, online reviews play a vital role for promoting any kind of
product or services. Businesses may embed fake reviews in order to attract
customers to purchase their products. They may even highlight the benefits of
their own product or criticize the competition's product. Marketers,
advertisers, and other online business users have incentive to create fake
positive reviews for products which they want to promote or give fake negative
reviews for products which they really don't like. So now-a-days writing a
deceptive review is inevitable thing for promoting their own business or
degrading competitor's reputation. Thus, identifying deceptive reviews is an
intense and on-going research area. This research paper proposes machine
learning model approach to identify deceptive reviews. The paper investigates
the performance of the several experiments done on a Deceptive Opinion Spam
Corpus dataset of restaurants reviews. We developed a n-gram model and max
features to identify deceptive contents with a particular focus on fake
reviews. Further, we conduct a benchmark study to investigate the performance
of two different features extraction techniques and apply five machine learning
classification techniques. The experimental results show that passive
aggressive classifier outperforms other algorithms, and it reaches the highest
accuracy not only in text classification but also to fake reviews. We also
study the data augmentation and implement different deep learning techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baby_A/0/1/0/all/0/1"&gt;Anusuya Baby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Link Prediction over Hyper-Relational Temporal Knowledge Graphs Enhanced with Time-Invariant Relational Knowledge. (arXiv:2307.10219v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2307.10219</id>
        <link href="http://arxiv.org/abs/2307.10219"/>
        <updated>2023-07-22T00:55:25.995Z</updated>
        <summary type="html"><![CDATA[Stemming from traditional knowledge graphs (KGs), hyper-relational KGs (HKGs)
provide additional key-value pairs (i.e., qualifiers) for each KG fact that
help to better restrict the fact validity. In recent years, there has been an
increasing interest in studying graph reasoning over HKGs. In the meantime, due
to the ever-evolving nature of world knowledge, extensive parallel works have
been focusing on reasoning over temporal KGs (TKGs), where each TKG fact can be
viewed as a KG fact coupled with a timestamp (or time period) specifying its
time validity. The existing HKG reasoning approaches do not consider temporal
information because it is not explicitly specified in previous benchmark
datasets. Besides, all the previous TKG reasoning methods only lay emphasis on
temporal reasoning and have no way to learn from qualifiers. To this end, we
aim to fill the gap between TKG reasoning and HKG reasoning. We develop two new
benchmark hyper-relational TKG (HTKG) datasets, i.e., Wiki-hy and YAGO-hy, and
propose a HTKG reasoning model that efficiently models both temporal facts and
qualifiers. We further exploit additional time-invariant relational knowledge
from the Wikidata knowledge base and study its effectiveness in HTKG reasoning.
Time-invariant relational knowledge serves as the knowledge that remains
unchanged in time (e.g., Sasha Obama is the child of Barack Obama), and it has
never been fully explored in previous TKG reasoning benchmarks and approaches.
Experimental results show that our model substantially outperforms previous
related methods on HTKG link prediction and can be enhanced by jointly
leveraging both temporal and time-invariant relational knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1"&gt;Zifeng Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jingcheng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jingpei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yan Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1"&gt;Volker Tresp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mathematical Capabilities of ChatGPT. (arXiv:2301.13867v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2301.13867</id>
        <link href="http://arxiv.org/abs/2301.13867"/>
        <updated>2023-07-22T00:55:25.993Z</updated>
        <summary type="html"><![CDATA[We investigate the mathematical capabilities of two iterations of ChatGPT
(released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on
publicly available datasets, as well as hand-crafted ones, using a novel
methodology. In contrast to formal mathematics, where large databases of formal
proofs are available (e.g., the Lean Mathematical Library), current datasets of
natural-language mathematics, used to benchmark language models, either cover
only elementary mathematics or are very small. We address this by publicly
releasing two new datasets: GHOSTS and miniGHOSTS. These are the first
natural-language datasets curated by working researchers in mathematics that
(1) aim to cover graduate-level mathematics, (2) provide a holistic overview of
the mathematical capabilities of language models, and (3) distinguish multiple
dimensions of mathematical reasoning. These datasets also test whether ChatGPT
and GPT-4 can be helpful assistants to professional mathematicians by emulating
use cases that arise in the daily professional activities of mathematicians. We
benchmark the models on a range of fine-grained performance metrics. For
advanced mathematics, this is the most detailed evaluation effort to date. We
find that ChatGPT can be used most successfully as a mathematical assistant for
querying facts, acting as a mathematical search engine and knowledge base
interface. GPT-4 can additionally be used for undergraduate-level mathematics
but fails on graduate-level difficulty. Contrary to many positive reports in
the media about GPT-4 and ChatGPT's exam-solving abilities (a potential case of
selection bias), their overall mathematical performance is well below the level
of a graduate student. Hence, if your goal is to use ChatGPT to pass a
graduate-level math exam, you would be better off copying from your average
peer!]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frieder_S/0/1/0/all/0/1"&gt;Simon Frieder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinchetti_L/0/1/0/all/0/1"&gt;Luca Pinchetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chevalier_A/0/1/0/all/0/1"&gt;Alexis Chevalier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griffiths_R/0/1/0/all/0/1"&gt;Ryan-Rhys Griffiths&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salvatori_T/0/1/0/all/0/1"&gt;Tommaso Salvatori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1"&gt;Thomas Lukasiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petersen_P/0/1/0/all/0/1"&gt;Philipp Christian Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berner_J/0/1/0/all/0/1"&gt;Julius Berner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-paced Weight Consolidation for Continual Learning. (arXiv:2307.10845v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10845</id>
        <link href="http://arxiv.org/abs/2307.10845"/>
        <updated>2023-07-22T00:55:25.992Z</updated>
        <summary type="html"><![CDATA[Continual learning algorithms which keep the parameters of new tasks close to
that of previous tasks, are popular in preventing catastrophic forgetting in
sequential task learning settings. However, 1) the performance for the new
continual learner will be degraded without distinguishing the contributions of
previously learned tasks; 2) the computational cost will be greatly increased
with the number of tasks, since most existing algorithms need to regularize all
previous tasks when learning new tasks. To address the above challenges, we
propose a self-paced Weight Consolidation (spWC) framework to attain robust
continual learning via evaluating the discriminative contributions of previous
tasks. To be specific, we develop a self-paced regularization to reflect the
priorities of past tasks via measuring difficulty based on key performance
indicator (i.e., accuracy). When encountering a new task, all previous tasks
are sorted from "difficult" to "easy" based on the priorities. Then the
parameters of the new continual learner will be learned via selectively
maintaining the knowledge amongst more difficult past tasks, which could well
overcome catastrophic forgetting with less computational cost. We adopt an
alternative convex search to iteratively update the model parameters and
priority weights in the bi-convex formulation. The proposed spWC framework is
plug-and-play, which is applicable to most continual learning algorithms (e.g.,
EWC, MAS and RCIL) in different directions (e.g., classification and
segmentation). Experimental results on several public benchmark datasets
demonstrate that our proposed framework can effectively improve performance
when compared with other popular continual learning algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cong_W/0/1/0/all/0/1"&gt;Wei Cong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1"&gt;Yang Cong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1"&gt;Gan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jiahua Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer. (arXiv:2307.10550v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2307.10550</id>
        <link href="http://arxiv.org/abs/2307.10550"/>
        <updated>2023-07-22T00:55:25.989Z</updated>
        <summary type="html"><![CDATA[Expressive speech synthesis models are trained by adding corpora with diverse
speakers, various emotions, and different speaking styles to the dataset, in
order to control various characteristics of speech and generate the desired
voice. In this paper, we propose a style control (SC) VALL-E model based on the
neural codec language model (called VALL-E), which follows the structure of the
generative pretrained transformer 3 (GPT-3). The proposed SC VALL-E takes input
from text sentences and prompt audio and is designed to generate controllable
speech by not simply mimicking the characteristics of the prompt audio but by
controlling the attributes to produce diverse voices. We identify tokens in the
style embedding matrix of the newly designed style network that represent
attributes such as emotion, speaking rate, pitch, and voice intensity, and
design a model that can control these attributes. To evaluate the performance
of SC VALL-E, we conduct comparative experiments with three representative
expressive speech synthesis models: global style token (GST) Tacotron2,
variational autoencoder (VAE) Tacotron2, and original VALL-E. We measure word
error rate (WER), F0 voiced error (FVE), and F0 gross pitch error (F0GPE) as
evaluation metrics to assess the accuracy of generated sentences. For comparing
the quality of synthesized speech, we measure comparative mean option score
(CMOS) and similarity mean option score (SMOS). To evaluate the style control
ability of the generated speech, we observe the changes in F0 and
mel-spectrogram by modifying the trained tokens. When using prompt audio that
is not present in the training data, SC VALL-E generates a variety of
expressive sounds and demonstrates competitive performance compared to the
existing models. Our implementation, pretrained models, and audio samples are
located on GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Daegyeom Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1"&gt;Seongho Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yong-Hoon Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forecasting Battery Electric Vehicle Charging Behavior: A Deep Learning Approach Equipped with Micro-Clustering and SMOTE Techniques. (arXiv:2307.10588v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10588</id>
        <link href="http://arxiv.org/abs/2307.10588"/>
        <updated>2023-07-22T00:55:25.984Z</updated>
        <summary type="html"><![CDATA[Energy systems, climate change, and public health are among the primary
reasons for moving toward electrification in transportation. Transportation
electrification is being promoted worldwide to reduce emissions. As a result,
many automakers will soon start making only battery electric vehicles (BEVs).
BEV adoption rates are rising in California, mainly due to climate change and
air pollution concerns. While great for climate and pollution goals, improperly
managed BEV charging can lead to insufficient charging infrastructure and power
outages. This study develops a novel Micro Clustering Deep Neural Network
(MCDNN), an artificial neural network algorithm that is highly effective at
learning BEVs trip and charging data to forecast BEV charging events,
information that is essential for electricity load aggregators and utility
managers to provide charging stations and electricity capacity effectively. The
MCDNN is configured using a robust dataset of trips and charges that occurred
in California between 2015 and 2020 from 132 BEVs, spanning 5 BEV models for a
total of 1570167 vehicle miles traveled. The numerical findings revealed that
the proposed MCDNN is more effective than benchmark approaches in this field,
such as support vector machine, k nearest neighbors, decision tree, and other
neural network-based models in predicting the charging events.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tayarani_H/0/1/0/all/0/1"&gt;Hanif Tayarani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramadoss_T/0/1/0/all/0/1"&gt;Trisha V. Ramadoss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karanam_V/0/1/0/all/0/1"&gt;Vaishnavi Karanam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tal_G/0/1/0/all/0/1"&gt;Gil Tal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nitta_C/0/1/0/all/0/1"&gt;Christopher Nitta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness-Aware Client Selection for Federated Learning. (arXiv:2307.10738v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10738</id>
        <link href="http://arxiv.org/abs/2307.10738"/>
        <updated>2023-07-22T00:55:25.984Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) has enabled multiple data owners (a.k.a. FL clients)
to train machine learning models collaboratively without revealing private
data. Since the FL server can only engage a limited number of clients in each
training round, FL client selection has become an important research problem.
Existing approaches generally focus on either enhancing FL model performance or
enhancing the fair treatment of FL clients. The problem of balancing
performance and fairness considerations when selecting FL clients remains open.
To address this problem, we propose the Fairness-aware Federated Client
Selection (FairFedCS) approach. Based on Lyapunov optimization, it dynamically
adjusts FL clients' selection probabilities by jointly considering their
reputations, times of participation in FL tasks and contributions to the
resulting model performance. By not using threshold-based reputation filtering,
it provides FL clients with opportunities to redeem their reputations after a
perceived poor performance, thereby further enhancing fair client treatment.
Extensive experiments based on real-world multimedia datasets show that
FairFedCS achieves 19.6% higher fairness and 0.73% higher test accuracy on
average than the best-performing state-of-the-art approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yuxin Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zelei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zhuan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Han Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Music Genre Classification with ResNet and Bi-GRU Using Visual Spectrograms. (arXiv:2307.10773v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2307.10773</id>
        <link href="http://arxiv.org/abs/2307.10773"/>
        <updated>2023-07-22T00:55:25.983Z</updated>
        <summary type="html"><![CDATA[Music recommendation systems have emerged as a vital component to enhance
user experience and satisfaction for the music streaming services, which
dominates music consumption. The key challenge in improving these recommender
systems lies in comprehending the complexity of music data, specifically for
the underpinning music genre classification. The limitations of manual genre
classification have highlighted the need for a more advanced system, namely the
Automatic Music Genre Classification (AMGC) system. While traditional machine
learning techniques have shown potential in genre classification, they heavily
rely on manually engineered features and feature selection, failing to capture
the full complexity of music data. On the other hand, deep learning
classification architectures like the traditional Convolutional Neural Networks
(CNN) are effective in capturing the spatial hierarchies but struggle to
capture the temporal dynamics inherent in music data. To address these
challenges, this study proposes a novel approach using visual spectrograms as
input, and propose a hybrid model that combines the strength of the Residual
neural Network (ResNet) and the Gated Recurrent Unit (GRU). This model is
designed to provide a more comprehensive analysis of music data, offering the
potential to improve the music recommender systems through achieving a more
comprehensive analysis of music data and hence potentially more accurate genre
classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junfei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa. (arXiv:2307.10633v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.10633</id>
        <link href="http://arxiv.org/abs/2307.10633"/>
        <updated>2023-07-22T00:55:25.979Z</updated>
        <summary type="html"><![CDATA[Large Language Models have many methods for solving the same problem. This
introduces novel strengths (different methods may work well for different
problems) and weaknesses (it may be difficult for users to know which method to
use). In this paper, we introduce Multi-Method Self-Training (MMST), where one
method is trained on the filtered outputs of another, allowing us to augment
the strengths and ameliorate the weaknesses of each method. Using a 176B
parameter model trained on both language and code, we show that MMST can 1)
improve the less performant method (up to 30%) making the model easier to use,
2) improve the more performant method (up to 32.2%) making the model more
performant, and 3) improve the performance of related but distinct tasks (up to
10.3%) by improving the ability of the model to generate rationales. We then
conduct ablation analyses to explore why MMST works. We show that MMST
generates more data than traditional self-training, but the improvement in
performance is driven by the use of multiple methods. We also analyze
prompt-engineering and anti-correlated performance between methods as means of
making MMST more effective. We hope the evidence from our paper motivates
machine learning researchers to explore ways in which advances in language
models allow for new forms of training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1"&gt;Shriyash K. Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ginsberg_E/0/1/0/all/0/1"&gt;Etan J. Ginsberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SecureBoost Hyperparameter Tuning via Multi-Objective Federated Learning. (arXiv:2307.10579v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10579</id>
        <link href="http://arxiv.org/abs/2307.10579"/>
        <updated>2023-07-22T00:55:25.975Z</updated>
        <summary type="html"><![CDATA[SecureBoost is a tree-boosting algorithm leveraging homomorphic encryption to
protect data privacy in vertical federated learning setting. It is widely used
in fields such as finance and healthcare due to its interpretability,
effectiveness, and privacy-preserving capability. However, SecureBoost suffers
from high computational complexity and risk of label leakage. To harness the
full potential of SecureBoost, hyperparameters of SecureBoost should be
carefully chosen to strike an optimal balance between utility, efficiency, and
privacy. Existing methods either set hyperparameters empirically or
heuristically, which are far from optimal. To fill this gap, we propose a
Constrained Multi-Objective SecureBoost (CMOSB) algorithm to find Pareto
optimal solutions that each solution is a set of hyperparameters achieving
optimal tradeoff between utility loss, training cost, and privacy leakage. We
design measurements of the three objectives. In particular, the privacy leakage
is measured using our proposed instance clustering attack. Experimental results
demonstrate that the CMOSB yields not only hyperparameters superior to the
baseline but also optimal sets of hyperparameters that can support the flexible
requirements of FL participants.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1"&gt;Ziyao Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yan Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Lixin Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Linghua Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1"&gt;Tao Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1"&gt;Yongxin Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qiang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Transformer: A Unified Framework for Multimodal Learning. (arXiv:2307.10802v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.10802</id>
        <link href="http://arxiv.org/abs/2307.10802"/>
        <updated>2023-07-22T00:55:25.971Z</updated>
        <summary type="html"><![CDATA[Multimodal learning aims to build models that can process and relate
information from multiple modalities. Despite years of development in this
field, it still remains challenging to design a unified network for processing
various modalities ($\textit{e.g.}$ natural language, 2D images, 3D point
clouds, audio, video, time series, tabular data) due to the inherent gaps among
them. In this work, we propose a framework, named Meta-Transformer, that
leverages a $\textbf{frozen}$ encoder to perform multimodal perception without
any paired multimodal training data. In Meta-Transformer, the raw input data
from various modalities are mapped into a shared token space, allowing a
subsequent encoder with frozen parameters to extract high-level semantic
features of the input data. Composed of three main components: a unified data
tokenizer, a modality-shared encoder, and task-specific heads for downstream
tasks, Meta-Transformer is the first framework to perform unified learning
across 12 modalities with unpaired data. Experiments on different benchmarks
reveal that Meta-Transformer can handle a wide range of tasks including
fundamental perception (text, image, point cloud, audio, video), practical
application (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph,
tabular, and time-series). Meta-Transformer indicates a promising future for
developing unified multimodal intelligence with transformers. Code will be
available at https://github.com/invictus717/MetaTransformer]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yiyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_K/0/1/0/all/0/1"&gt;Kaixiong Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaipeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1"&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1"&gt;Xiangyu Yue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Learning based Anomaly Detection for IoT Cybersecurity via Bayesian Hyperparameters Sensitivity Analysis. (arXiv:2307.10596v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10596</id>
        <link href="http://arxiv.org/abs/2307.10596"/>
        <updated>2023-07-22T00:55:25.970Z</updated>
        <summary type="html"><![CDATA[The Internet of Things (IoT) integrates more than billions of intelligent
devices over the globe with the capability of communicating with other
connected devices with little to no human intervention. IoT enables data
aggregation and analysis on a large scale to improve life quality in many
domains. In particular, data collected by IoT contain a tremendous amount of
information for anomaly detection. The heterogeneous nature of IoT is both a
challenge and an opportunity for cybersecurity. Traditional approaches in
cybersecurity monitoring often require different kinds of data pre-processing
and handling for various data types, which might be problematic for datasets
that contain heterogeneous features. However, heterogeneous types of network
devices can often capture a more diverse set of signals than a single type of
device readings, which is particularly useful for anomaly detection. In this
paper, we present a comprehensive study on using ensemble machine learning
methods for enhancing IoT cybersecurity via anomaly detection. Rather than
using one single machine learning model, ensemble learning combines the
predictive power from multiple models, enhancing their predictive accuracy in
heterogeneous datasets rather than using one single machine learning model. We
propose a unified framework with ensemble learning that utilises Bayesian
hyperparameter optimisation to adapt to a network environment that contains
multiple IoT sensor readings. Experimentally, we illustrate their high
predictive power when compared to traditional methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1"&gt;Tin Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farid_F/0/1/0/all/0/1"&gt;Farnaz Farid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bello_A/0/1/0/all/0/1"&gt;Abubakar Bello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabrina_F/0/1/0/all/0/1"&gt;Fariza Sabrina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feed-Forward Source-Free Domain Adaptation via Class Prototypes. (arXiv:2307.10787v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.10787</id>
        <link href="http://arxiv.org/abs/2307.10787"/>
        <updated>2023-07-22T00:55:25.968Z</updated>
        <summary type="html"><![CDATA[Source-free domain adaptation has become popular because of its practical
usefulness and no need to access source data. However, the adaptation process
still takes a considerable amount of time and is predominantly based on
optimization that relies on back-propagation. In this work we present a simple
feed-forward approach that challenges the need for back-propagation based
adaptation. Our approach is based on computing prototypes of classes under the
domain shift using a pre-trained model. It achieves strong improvements in
accuracy compared to the pre-trained model and requires only a small fraction
of time of existing domain adaptation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1"&gt;Ondrej Bohdal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Da Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1"&gt;Timothy Hospedales&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning. (arXiv:2307.10317v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10317</id>
        <link href="http://arxiv.org/abs/2307.10317"/>
        <updated>2023-07-22T00:55:25.967Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) offers a collaborative training framework, allowing
multiple clients to contribute to a shared model without compromising data
privacy. Due to the heterogeneous nature of local datasets, updated client
models may overfit and diverge from one another, commonly known as the problem
of client drift. In this paper, we propose FedBug (Federated Learning with
Bottom-Up Gradual Unfreezing), a novel FL framework designed to effectively
mitigate client drift. FedBug adaptively leverages the client model parameters,
distributed by the server at each global round, as the reference points for
cross-client alignment. Specifically, on the client side, FedBug begins by
freezing the entire model, then gradually unfreezes the layers, from the input
layer to the output layer. This bottom-up approach allows models to train the
newly thawed layers to project data into a latent space, wherein the separating
hyperplanes remain consistent across all clients. We theoretically analyze
FedBug in a novel over-parameterization FL setup, revealing its superior
convergence rate compared to FedAvg. Through comprehensive experiments,
spanning various datasets, training conditions, and network architectures, we
validate the efficacy of FedBug. Our contributions encompass a novel FL
framework, theoretical analysis, and empirical validation, demonstrating the
wide potential and applicability of FedBug.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kao_C/0/1/0/all/0/1"&gt;Chia-Hsiang Kao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Chiang Frank Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MSQNet: Actor-agnostic Action Recognition with Multi-modal Query. (arXiv:2307.10763v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.10763</id>
        <link href="http://arxiv.org/abs/2307.10763"/>
        <updated>2023-07-22T00:55:25.967Z</updated>
        <summary type="html"><![CDATA[Existing action recognition methods are typically actor-specific due to the
intrinsic topological and apparent differences among the actors. This requires
actor-specific pose estimation (e.g., humans vs. animals), leading to
cumbersome model design complexity and high maintenance costs. Moreover, they
often focus on learning the visual modality alone and single-label
classification whilst neglecting other available information sources (e.g.,
class name text) and the concurrent occurrence of multiple actions. To overcome
these limitations, we propose a new approach called 'actor-agnostic multi-modal
multi-label action recognition,' which offers a unified solution for various
types of actors, including humans and animals. We further formulate a novel
Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object
detection framework (e.g., DETR), characterized by leveraging visual and
textual modalities to represent the action classes better. The elimination of
actor-specific model designs is a key advantage, as it removes the need for
actor pose estimation altogether. Extensive experiments on five publicly
available benchmarks show that our MSQNet consistently outperforms the prior
arts of actor-specific alternatives on human and animal single- and multi-label
action recognition tasks by up to 50%. Code will be released at
https://github.com/mondalanindya/MSQNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1"&gt;Anindya Mondal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1"&gt;Sauradip Nag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prada_J/0/1/0/all/0/1"&gt;Joaquin M Prada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1"&gt;Anjan Dutta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.06092</id>
        <link href="http://arxiv.org/abs/2307.06092"/>
        <updated>2023-07-22T00:55:25.966Z</updated>
        <summary type="html"><![CDATA[We study the distribution of a fully connected neural network with random
Gaussian weights and biases in which the hidden layer widths are proportional
to a large constant $n$. Under mild assumptions on the non-linearity, we obtain
quantitative bounds on normal approximations valid at large but finite $n$ and
any fixed network depth. Our theorems show both for the finite-dimensional
distributions and the entire process, that the distance between a random fully
connected network (and its derivatives) to the corresponding infinite width
Gaussian process scales like $n^{-\gamma}$ for $\gamma>0$, with the exponent
depending on the metric used to measure discrepancy. Our bounds are strictly
stronger in terms of their dependence on network width than any previously
available in the literature; in the one-dimensional case, we also prove that
they are optimal, i.e., we establish matching lower bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Favaro_S/0/1/0/all/0/1"&gt;Stefano Favaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanin_B/0/1/0/all/0/1"&gt;Boris Hanin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marinucci_D/0/1/0/all/0/1"&gt;Domenico Marinucci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nourdin_I/0/1/0/all/0/1"&gt;Ivan Nourdin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peccati_G/0/1/0/all/0/1"&gt;Giovanni Peccati&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives. (arXiv:2307.10184v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2307.10184</id>
        <link href="http://arxiv.org/abs/2307.10184"/>
        <updated>2023-07-22T00:55:25.962Z</updated>
        <summary type="html"><![CDATA[Backdoor attacks pose serious security threats to deep neural networks
(DNNs). Backdoored models make arbitrarily (targeted) incorrect predictions on
inputs embedded with well-designed triggers while behaving normally on clean
inputs. Many works have explored the invisibility of backdoor triggers to
improve attack stealthiness. However, most of them only consider the
invisibility in the spatial domain without explicitly accounting for the
generation of invisible triggers in the frequency domain, making the generated
poisoned images be easily detected by recent defense methods. To address this
issue, in this paper, we propose a DUal stealthy BAckdoor attack method named
DUBA, which simultaneously considers the invisibility of triggers in both the
spatial and frequency domains, to achieve desirable attack performance, while
ensuring strong stealthiness. Specifically, we first use Discrete Wavelet
Transform to embed the high-frequency information of the trigger image into the
clean image to ensure attack effectiveness. Then, to attain strong
stealthiness, we incorporate Fourier Transform and Discrete Cosine Transform to
mix the poisoned image and clean image in the frequency domain. Moreover, the
proposed DUBA adopts a novel attack strategy, in which the model is trained
with weak triggers and attacked with strong triggers to further enhance the
attack performance and stealthiness. We extensively evaluate DUBA against
popular image classifiers on four datasets. The results demonstrate that it
significantly outperforms the state-of-the-art backdoor attacks in terms of the
attack success rate and stealthiness]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yudong Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Honglong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1"&gt;Peng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Junjian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Anqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhibo Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Divide & Bind Your Attention for Improved Generative Semantic Nursing. (arXiv:2307.10864v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.10864</id>
        <link href="http://arxiv.org/abs/2307.10864"/>
        <updated>2023-07-22T00:55:25.961Z</updated>
        <summary type="html"><![CDATA[Emerging large-scale text-to-image generative models, e.g., Stable Diffusion
(SD), have exhibited overwhelming results with high fidelity. Despite the
magnificent progress, current state-of-the-art models still struggle to
generate images fully adhering to the input prompt. Prior work, Attend &
Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming
to optimize cross-attention during inference time to better incorporate the
semantics. It demonstrates promising results in generating simple prompts,
e.g., ``a cat and a dog''. However, its efficacy declines when dealing with
more complex prompts, and it does not explicitly address the problem of
improper attribute binding. To address the challenges posed by complex prompts
or scenarios involving multiple entities and to achieve improved attribute
binding, we propose Divide & Bind. We introduce two novel loss objectives for
GSN: a novel attendance loss and a binding loss. Our approach stands out in its
ability to faithfully synthesize desired objects with improved attribute
alignment from complex prompts and exhibits superior performance across
multiple evaluation benchmarks. More videos and updates can be found on the
project page \url{https://sites.google.com/view/divide-and-bind}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yumeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1"&gt;Margret Keuper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khoreva_A/0/1/0/all/0/1"&gt;Anna Khoreva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impatient Bandits: Optimizing Recommendations for the Long-Term Without Delay. (arXiv:2307.09943v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.09943</id>
        <link href="http://arxiv.org/abs/2307.09943"/>
        <updated>2023-07-22T00:55:25.961Z</updated>
        <summary type="html"><![CDATA[Recommender systems are a ubiquitous feature of online platforms.
Increasingly, they are explicitly tasked with increasing users' long-term
satisfaction. In this context, we study a content exploration task, which we
formalize as a multi-armed bandit problem with delayed rewards. We observe that
there is an apparent trade-off in choosing the learning signal: Waiting for the
full reward to become available might take several weeks, hurting the rate at
which learning happens, whereas measuring short-term proxy rewards reflects the
actual long-term goal only imperfectly. We address this challenge in two steps.
First, we develop a predictive model of delayed rewards that incorporates all
information obtained to date. Full observations as well as partial (short or
medium-term) outcomes are combined through a Bayesian filter to obtain a
probabilistic belief. Second, we devise a bandit algorithm that takes advantage
of this new predictive model. The algorithm quickly learns to identify content
aligned with long-term success by carefully balancing exploration and
exploitation. We apply our approach to a podcast recommendation problem, where
we seek to identify shows that users engage with repeatedly over two months. We
empirically validate that our approach results in substantially better
performance compared to approaches that either optimize for short-term proxies,
or wait for the long-term outcome to be fully realized.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McDonald_T/0/1/0/all/0/1"&gt;Thomas M. McDonald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maystre_L/0/1/0/all/0/1"&gt;Lucas Maystre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lalmas_M/0/1/0/all/0/1"&gt;Mounia Lalmas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russo_D/0/1/0/all/0/1"&gt;Daniel Russo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciosek_K/0/1/0/all/0/1"&gt;Kamil Ciosek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying Interpretable Subspaces in Image Representations. (arXiv:2307.10504v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.10504</id>
        <link href="http://arxiv.org/abs/2307.10504"/>
        <updated>2023-07-22T00:55:25.959Z</updated>
        <summary type="html"><![CDATA[We propose Automatic Feature Explanation using Contrasting Concepts (FALCON),
an interpretability framework to explain features of image representations. For
a target feature, FALCON captions its highly activating cropped images using a
large captioning dataset (like LAION-400m) and a pre-trained vision-language
model like CLIP. Each word among the captions is scored and ranked leading to a
small number of shared, human-understandable concepts that closely describe the
target feature. FALCON also applies contrastive interpretation using lowly
activating (counterfactual) images, to eliminate spurious concepts. Although
many existing approaches interpret features independently, we observe in
state-of-the-art self-supervised and supervised models, that less than 20% of
the representation space can be explained by individual features. We show that
features in larger spaces become more interpretable when studied in groups and
can be explained with high-order scoring concepts through FALCON. We discuss
how extracted concepts can be used to explain and debug failures in downstream
tasks. Finally, we present a technique to transfer concepts from one
(explainable) representation space to another unseen representation space by
learning a simple linear transformation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kalibhat_N/0/1/0/all/0/1"&gt;Neha Kalibhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhardwaj_S/0/1/0/all/0/1"&gt;Shweta Bhardwaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruss_B/0/1/0/all/0/1"&gt;Bayan Bruss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1"&gt;Hamed Firooz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1"&gt;Maziar Sanjabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1"&gt;Soheil Feizi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Matrix Ensemble Kalman Filter-based Multi-arm Neural Network to Adequately Approximate Deep Neural Networks. (arXiv:2307.10436v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.10436</id>
        <link href="http://arxiv.org/abs/2307.10436"/>
        <updated>2023-07-22T00:55:25.958Z</updated>
        <summary type="html"><![CDATA[Deep Learners (DLs) are the state-of-art predictive mechanism with
applications in many fields requiring complex high dimensional data processing.
Although conventional DLs get trained via gradient descent with
back-propagation, Kalman Filter (KF)-based techniques that do not need gradient
computation have been developed to approximate DLs. We propose a multi-arm
extension of a KF-based DL approximator that can mimic DL when the sample size
is too small to train a multi-arm DL. The proposed Matrix Ensemble Kalman
Filter-based multi-arm ANN (MEnKF-ANN) also performs explicit model stacking
that becomes relevant when the training sample has an unequal-size feature set.
Our proposed technique can approximate Long Short-term Memory (LSTM) Networks
and attach uncertainty to the predictions obtained from these LSTMs with
desirable coverage. We demonstrate how MEnKF-ANN can "adequately" approximate
an LSTM network trained to classify what carbohydrate substrates are digested
and utilized by a microbiome sample whose genomic sequences consist of
polysaccharide utilization loci (PULs) and their encoded genes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Piyush_V/0/1/0/all/0/1"&gt;Ved Piyush&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yuchen Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuzhen Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Yanbin Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Souparno Ghosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples. (arXiv:2307.10562v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10562</id>
        <link href="http://arxiv.org/abs/2307.10562"/>
        <updated>2023-07-22T00:55:25.957Z</updated>
        <summary type="html"><![CDATA[Backdoor attacks are serious security threats to machine learning models
where an adversary can inject poisoned samples into the training set, causing a
backdoored model which predicts poisoned samples with particular triggers to
particular target classes, while behaving normally on benign samples. In this
paper, we explore the task of purifying a backdoored model using a small clean
dataset. By establishing the connection between backdoor risk and adversarial
risk, we derive a novel upper bound for backdoor risk, which mainly captures
the risk on the shared adversarial examples (SAEs) between the backdoored model
and the purified model. This upper bound further suggests a novel bi-level
optimization problem for mitigating backdoor using adversarial training
techniques. To solve it, we propose Shared Adversarial Unlearning (SAU).
Specifically, SAU first generates SAEs, and then, unlearns the generated SAEs
such that they are either correctly classified by the purified model and/or
differently classified by the two models, such that the backdoor effect in the
backdoored model will be mitigated in the purified model. Experiments on
various benchmark datasets and network architectures show that our proposed
method achieves state-of-the-art performance for backdoor defense.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1"&gt;Shaokui Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mingda Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1"&gt;Hongyuan Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1"&gt;Baoyuan Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mood Classification of Bangla Songs Based on Lyrics. (arXiv:2307.10314v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2307.10314</id>
        <link href="http://arxiv.org/abs/2307.10314"/>
        <updated>2023-07-22T00:55:25.952Z</updated>
        <summary type="html"><![CDATA[Music can evoke various emotions, and with the advancement of technology, it
has become more accessible to people. Bangla music, which portrays different
human emotions, lacks sufficient research. The authors of this article aim to
analyze Bangla songs and classify their moods based on the lyrics. To achieve
this, this research has compiled a dataset of 4000 Bangla song lyrics, genres,
and used Natural Language Processing and the Bert Algorithm to analyze the
data. Among the 4000 songs, 1513 songs are represented for the sad mood, 1362
for the romantic mood, 886 for happiness, and the rest 239 are classified as
relaxation. By embedding the lyrics of the songs, the authors have classified
the songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is
crucial as it enables a multi-class classification of songs' moods, making the
music more relatable to people's emotions. The article presents the automated
result of the four moods accurately derived from the song lyrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahajebin_M/0/1/0/all/0/1"&gt;Maliha Mahajebin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rashid_M/0/1/0/all/0/1"&gt;Mohammad Rifat Ahmmad Rashid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansoor_N/0/1/0/all/0/1"&gt;Nafees Mansoor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions. (arXiv:2307.10524v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10524</id>
        <link href="http://arxiv.org/abs/2307.10524"/>
        <updated>2023-07-22T00:55:25.931Z</updated>
        <summary type="html"><![CDATA[We study the tradeoff between consistency and robustness in the context of a
single-trajectory time-varying Markov Decision Process (MDP) with untrusted
machine-learned advice. Our work departs from the typical approach of treating
advice as coming from black-box sources by instead considering a setting where
additional information about how the advice is generated is available. We prove
a first-of-its-kind consistency and robustness tradeoff given Q-value advice
under a general MDP model that includes both continuous and discrete
state/action spaces. Our results highlight that utilizing Q-value advice
enables dynamic pursuit of the better of machine-learned advice and a robust
baseline, thus result in near-optimal performance guarantees, which provably
improves what can be obtained solely with black-box advice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tongxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yiheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Shaolei Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wierman_A/0/1/0/all/0/1"&gt;Adam Wierman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification of Visualization Types and Perspectives in Patents. (arXiv:2307.10471v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.10471</id>
        <link href="http://arxiv.org/abs/2307.10471"/>
        <updated>2023-07-22T00:55:25.914Z</updated>
        <summary type="html"><![CDATA[Due to the swift growth of patent applications each year, information and
multimedia retrieval approaches that facilitate patent exploration and
retrieval are of utmost importance. Different types of visualizations (e.g.,
graphs, technical drawings) and perspectives (e.g., side view, perspective) are
used to visualize details of innovations in patents. The classification of
these images enables a more efficient search and allows for further analysis.
So far, datasets for image type classification miss some important
visualization types for patents. Furthermore, related work does not make use of
recent deep learning approaches including transformers. In this paper, we adopt
state-of-the-art deep learning methods for the classification of visualization
types and perspectives in patent images. We extend the CLEF-IP dataset for
image type classification in patents to ten classes and provide manual ground
truth annotations. In addition, we derive a set of hierarchical classes from a
dataset that provides weakly-labeled data for image perspectives. Experimental
results have demonstrated the feasibility of the proposed approaches. Source
code, models, and dataset will be made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghauri_J/0/1/0/all/0/1"&gt;Junaid Ahmed Ghauri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_Budack_E/0/1/0/all/0/1"&gt;Eric M&amp;#xfc;ller-Budack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1"&gt;Ralph Ewerth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey). (arXiv:2307.10246v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2307.10246</id>
        <link href="http://arxiv.org/abs/2307.10246"/>
        <updated>2023-07-22T00:55:25.908Z</updated>
        <summary type="html"><![CDATA[How does the brain represent different modes of information? Can we design a
system that automatically understands what the user is thinking? Such questions
can be answered by studying brain recordings like functional magnetic resonance
imaging (fMRI). As a first step, the neuroscience community has contributed
several large cognitive neuroscience datasets related to passive
reading/listening/viewing of concept words, narratives, pictures and movies.
Encoding and decoding models using these datasets have also been proposed in
the past two decades. These models serve as additional tools for basic research
in cognitive science and neuroscience. Encoding models aim at generating fMRI
brain representations given a stimulus automatically. They have several
practical applications in evaluating and diagnosing neurological conditions and
thus also help design therapies for brain damage. Decoding models solve the
inverse problem of reconstructing the stimuli given the fMRI. They are useful
for designing brain-machine or brain-computer interfaces. Inspired by the
effectiveness of deep learning models for natural language processing, computer
vision, and speech, recently several neural encoding and decoding models have
been proposed. In this survey, we will first discuss popular representations of
language, vision and speech stimuli, and present a summary of neuroscience
datasets. Further, we will review popular deep learning based encoding and
decoding architectures and note their benefits and limitations. Finally, we
will conclude with a brief summary and discussion about future trends. Given
the large amount of recently published work in the `computational cognitive
neuroscience' community, we believe that this survey nicely organizes the
plethora of work and presents it as a coherent story.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Oota_S/0/1/0/all/0/1"&gt;Subba Reddy Oota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Gupta_M/0/1/0/all/0/1"&gt;Manish Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Bapi_R/0/1/0/all/0/1"&gt;Raju S. Bapi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Jobard_G/0/1/0/all/0/1"&gt;Gael Jobard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Alexandre_F/0/1/0/all/0/1"&gt;Frederic Alexandre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hinaut_X/0/1/0/all/0/1"&gt;Xavier Hinaut&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-Tail Theory under Gaussian Mixtures. (arXiv:2307.10736v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10736</id>
        <link href="http://arxiv.org/abs/2307.10736"/>
        <updated>2023-07-22T00:55:25.907Z</updated>
        <summary type="html"><![CDATA[We suggest a simple Gaussian mixture model for data generation that complies
with Feldman's long tail theory (2020). We demonstrate that a linear classifier
cannot decrease the generalization error below a certain level in the proposed
model, whereas a nonlinear classifier with a memorization capacity can. This
confirms that for long-tailed distributions, rare training examples must be
considered for optimal generalization to new data. Finally, we show that the
performance gap between linear and nonlinear models can be lessened as the tail
becomes shorter in the subpopulation frequency distribution, as confirmed by
experiments on synthetic and real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bolatov_A/0/1/0/all/0/1"&gt;Arman Bolatov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tezekbayev_M/0/1/0/all/0/1"&gt;Maxat Tezekbayev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melnykov_I/0/1/0/all/0/1"&gt;Igor Melnykov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pak_A/0/1/0/all/0/1"&gt;Artur Pak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1"&gt;Vassilina Nikoulina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Assylbekov_Z/0/1/0/all/0/1"&gt;Zhenisbek Assylbekov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chordal Averaging on Flag Manifolds and Its Applications. (arXiv:2303.13501v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.13501</id>
        <link href="http://arxiv.org/abs/2303.13501"/>
        <updated>2023-07-22T00:55:25.907Z</updated>
        <summary type="html"><![CDATA[This paper presents a new, provably-convergent algorithm for computing the
flag-mean and flag-median of a set of points on a flag manifold under the
chordal metric. The flag manifold is a mathematical space consisting of flags,
which are sequences of nested subspaces of a vector space that increase in
dimension. The flag manifold is a superset of a wide range of known matrix
spaces, including Stiefel and Grassmanians, making it a general object that is
useful in a wide variety computer vision problems.

To tackle the challenge of computing first order flag statistics, we first
transform the problem into one that involves auxiliary variables constrained to
the Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and
leveraging the numerical stability and efficiency of Stiefel-manifold
optimization enables us to compute the flag-mean effectively. Through a series
of experiments, we show the competence of our method in Grassmann and rotation
averaging, as well as principal component analysis. We release our source code
under https://github.com/nmank/FlagAveraging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mankovich_N/0/1/0/all/0/1"&gt;Nathan Mankovich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1"&gt;Tolga Birdal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Precipitation Nowcasting of Integrated Multi-satellitE Retrievals for GPM: A U-Net Convolutional LSTM Architecture. (arXiv:2307.10843v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10843</id>
        <link href="http://arxiv.org/abs/2307.10843"/>
        <updated>2023-07-22T00:55:25.906Z</updated>
        <summary type="html"><![CDATA[This paper presents a deep learning architecture for nowcasting of
precipitation almost globally every 30 min with a 4-hour lead time. The
architecture fuses a U-Net and a convolutional long short-term memory (LSTM)
neural network and is trained using data from the Integrated MultisatellitE
Retrievals for GPM (IMERG) and a few key precipitation drivers from the Global
Forecast System (GFS). The impacts of different training loss functions,
including the mean-squared error (regression) and the focal-loss
(classification), on the quality of precipitation nowcasts are studied. The
results indicate that the regression network performs well in capturing light
precipitation (below 1.6 mm/hr), but the classification network can outperform
the regression network for nowcasting of precipitation extremes (>8 mm/hr), in
terms of the critical success index (CSI).. Using the Wasserstein distance, it
is shown that the predicted precipitation by the classification network has a
closer class probability distribution to the IMERG than the regression network.
It is uncovered that the inclusion of the physical variables can improve
precipitation nowcasting, especially at longer lead times in both networks.
Taking IMERG as a relative reference, a multi-scale analysis in terms of
fractions skill score (FSS), shows that the nowcasting machine remains skillful
(FSS > 0.5) at the resolution of 10 km compared to 50 km for GFS. For
precipitation rates greater than 4~mm/hr, only the classification network
remains FSS-skillful on scales greater than 50 km within a 2-hour lead time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahimi_R/0/1/0/all/0/1"&gt;Reyhaneh Rahimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ebtehaj_A/0/1/0/all/0/1"&gt;Ardeshir Ebtehaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behrangi_A/0/1/0/all/0/1"&gt;Ali Behrangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1"&gt;Jackson Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Player-optimal Stable Regret for Bandit Learning in Matching Markets. (arXiv:2307.10890v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10890</id>
        <link href="http://arxiv.org/abs/2307.10890"/>
        <updated>2023-07-22T00:55:25.906Z</updated>
        <summary type="html"><![CDATA[The problem of matching markets has been studied for a long time in the
literature due to its wide range of applications. Finding a stable matching is
a common equilibrium objective in this problem. Since market participants are
usually uncertain of their preferences, a rich line of recent works study the
online setting where one-side participants (players) learn their unknown
preferences from iterative interactions with the other side (arms). Most
previous works in this line are only able to derive theoretical guarantees for
player-pessimal stable regret, which is defined compared with the players'
least-preferred stable matching. However, under the pessimal stable matching,
players only obtain the least reward among all stable matchings. To maximize
players' profits, player-optimal stable matching would be the most desirable.
Though \citet{basu21beyond} successfully bring an upper bound for
player-optimal stable regret, their result can be exponentially large if
players' preference gap is small. Whether a polynomial guarantee for this
regret exists is a significant but still open problem. In this work, we provide
a new algorithm named explore-then-Gale-Shapley (ETGS) and show that the
optimal stable regret of each player can be upper bounded by $O(K\log
T/\Delta^2)$ where $K$ is the number of arms, $T$ is the horizon and $\Delta$
is the players' minimum preference gap among the first $N+1$-ranked arms. This
result significantly improves previous works which either have a weaker
player-pessimal stable matching objective or apply only to markets with special
assumptions. When the preferences of participants satisfy some special
conditions, our regret upper bound also matches the previously derived lower
bound.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1"&gt;Fang Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuai Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Network Complexity of Chaos and Turbulence. (arXiv:2211.15382v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.15382</id>
        <link href="http://arxiv.org/abs/2211.15382"/>
        <updated>2023-07-22T00:55:25.905Z</updated>
        <summary type="html"><![CDATA[Chaos and turbulence are complex physical phenomena, yet a precise definition
of the complexity measure that quantifies them is still lacking. In this work
we consider the relative complexity of chaos and turbulence from the
perspective of deep neural networks. We analyze a set of classification
problems, where the network has to distinguish images of fluid profiles in the
turbulent regime from other classes of images such as fluid profiles in the
chaotic regime, various constructions of noise and real world images. We
analyze incompressible as well as weakly compressible fluid flows. We quantify
the complexity of the computation performed by the network via the intrinsic
dimensionality of the internal feature representations, and calculate the
effective number of independent features which the network uses in order to
distinguish between classes. In addition to providing a numerical estimate of
the complexity of the computation, the measure also characterizes the neural
network processing at intermediate and final stages. We construct adversarial
examples and use them to identify the two point correlation spectra for the
chaotic and turbulent vorticity as the feature used by the network for
classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Whittaker_T/0/1/0/all/0/1"&gt;Tim Whittaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Janik_R/0/1/0/all/0/1"&gt;Romuald A. Janik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oz_Y/0/1/0/all/0/1"&gt;Yaron Oz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory. (arXiv:2307.10768v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2307.10768</id>
        <link href="http://arxiv.org/abs/2307.10768"/>
        <updated>2023-07-22T00:55:25.904Z</updated>
        <summary type="html"><![CDATA[Working memory (WM), a fundamental cognitive process facilitating the
temporary storage, integration, manipulation, and retrieval of information,
plays a vital role in reasoning and decision-making tasks. Robust benchmark
datasets that capture the multifaceted nature of WM are crucial for the
effective development and evaluation of AI WM models. Here, we introduce a
comprehensive Working Memory (WorM) benchmark dataset for this purpose. WorM
comprises 10 tasks and a total of 1 million trials, assessing 4
functionalities, 3 domains, and 11 behavioral and neural characteristics of WM.
We jointly trained and tested state-of-the-art recurrent neural networks and
transformers on all these tasks. We also include human behavioral benchmarks as
an upper bound for comparison. Our results suggest that AI models replicate
some characteristics of WM in the brain, most notably primacy and recency
effects, and neural clusters and correlates specialized for different domains
and functionalities of WM. In the experiments, we also reveal some limitations
in existing models to approximate human behavior. This dataset serves as a
valuable resource for communities in cognitive psychology, neuroscience, and
AI, offering a standardized framework to compare and enhance WM models,
investigate WM's neural underpinnings, and develop WM models with human-like
capabilities. Our source code and data are available at
https://github.com/ZhangLab-DeepNeuroCogLab/WorM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Sikarwar_A/0/1/0/all/0/1"&gt;Ankur Sikarwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mengmi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial attacks for mixtures of classifiers. (arXiv:2307.10788v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10788</id>
        <link href="http://arxiv.org/abs/2307.10788"/>
        <updated>2023-07-22T00:55:25.903Z</updated>
        <summary type="html"><![CDATA[Mixtures of classifiers (a.k.a. randomized ensembles) have been proposed as a
way to improve robustness against adversarial attacks. However, it has been
shown that existing attacks are not well suited for this kind of classifiers.
In this paper, we discuss the problem of attacking a mixture in a principled
way and introduce two desirable properties of attacks based on a geometrical
analysis of the problem (effectiveness and maximality). We then show that
existing attacks do not meet both of these properties. Finally, we introduce a
new attack called lattice climber attack with theoretical guarantees on the
binary linear setting, and we demonstrate its performance by conducting
experiments on synthetic and real datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heredia_L/0/1/0/all/0/1"&gt;Lucas Gnecco Heredia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Negrevergne_B/0/1/0/all/0/1"&gt;Benjamin Negrevergne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chevaleyre_Y/0/1/0/all/0/1"&gt;Yann Chevaleyre&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Spike Train Inference via Non-Local Priors. (arXiv:2307.10177v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2307.10177</id>
        <link href="http://arxiv.org/abs/2307.10177"/>
        <updated>2023-07-22T00:55:25.902Z</updated>
        <summary type="html"><![CDATA[Advances in neuroscience have enabled researchers to measure the activities
of large numbers of neurons simultaneously in behaving animals. We have access
to the fluorescence of each of the neurons which provides a first-order
approximation of the neural activity over time. Determining the exact spike of
a neuron from this fluorescence trace constitutes an active area of research
within the field of computational neuroscience. We propose a novel Bayesian
approach based on a mixture of half-non-local prior densities and point masses
for this task. Instead of a computationally expensive MCMC algorithm, we adopt
a stochastic search-based approach that is capable of taking advantage of
modern computing environments often equipped with multiple processors, to
explore all possible arrangements of spikes and lack thereof in an observed
spike train. It then reports the highest posterior probability arrangement of
spikes and posterior probability for a spike at each location of the spike
train. Our proposals lead to substantial improvements over existing proposals
based on L1 regularization, and enjoy comparable estimation accuracy to the
state-of-the-art L0 proposal, in simulations, and on recent calcium imaging
data sets. Notably, contrary to optimization-based frequentist approaches, our
methodology yields automatic uncertainty quantification associated with the
spike-train inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Chakraborty_A/0/1/0/all/0/1"&gt;Abhisek Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning. (arXiv:2307.10234v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.10234</id>
        <link href="http://arxiv.org/abs/2307.10234"/>
        <updated>2023-07-22T00:55:25.902Z</updated>
        <summary type="html"><![CDATA[This study presents a thorough examination of various Generative Pretrained
Transformer (GPT) methodologies in sentiment analysis, specifically in the
context of Task 4 on the SemEval 2017 dataset. Three primary strategies are
employed: 1) prompt engineering using the advanced GPT-3.5 Turbo, 2)
fine-tuning GPT models, and 3) an inventive approach to embedding
classification. The research yields detailed comparative insights among these
strategies and individual GPT models, revealing their unique strengths and
potential limitations. Additionally, the study compares these GPT-based
methodologies with other contemporary, high-performing models previously used
with the same dataset. The results illustrate the significant superiority of
the GPT approaches in terms of predictive performance, more than 22% in
F1-score compared to the state-of-the-art. Further, the paper addresses common
challenges in sentiment analysis tasks, such as understanding context and
detecting sarcasm. It underscores the enhanced capabilities of the GPT models
to effectively navigate these complexities. Collectively, these findings
highlight the promising potential of GPT models in sentiment analysis, setting
the stage for future research in this field. The code can be found at
https://github.com/DSAatUSU/SentimentGPT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kheiri_K/0/1/0/all/0/1"&gt;Kiana Kheiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karimi_H/0/1/0/all/0/1"&gt;Hamid Karimi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Code Detection for Hardware Acceleration Using Large Language Models. (arXiv:2307.10348v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2307.10348</id>
        <link href="http://arxiv.org/abs/2307.10348"/>
        <updated>2023-07-22T00:55:25.901Z</updated>
        <summary type="html"><![CDATA[Large language models (LLMs) have been massively applied to many tasks, often
surpassing state-of-the-art approaches. While their effectiveness in code
generation has been extensively studied (e.g., AlphaCode), their potential for
code detection remains unexplored.

This work presents the first analysis of code detection using LLMs. Our study
examines essential kernels, including matrix multiplication, convolution, and
fast-fourier transform, implemented in C/C++. We propose both a preliminary,
naive prompt and a novel prompting strategy for code detection.

Results reveal that conventional prompting achieves great precision but poor
accuracy (68.8%, 22.3%, and 79.2% for GEMM, convolution, and FFT, respectively)
due to a high number of false positives. Our novel prompting strategy
substantially reduces false positives, resulting in excellent overall accuracy
(91.1%, 97.9%, and 99.7%, respectively). These results pose a considerable
challenge to existing state-of-the-art code detection methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_P/0/1/0/all/0/1"&gt;Pablo Antonio Mart&amp;#xed;nez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernabe_G/0/1/0/all/0/1"&gt;Gregorio Bernab&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Manuel Garc&amp;#xed;a&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tapestry of Time and Actions: Modeling Human Activity Sequences using Temporal Point Process Flows. (arXiv:2307.10305v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.10305</id>
        <link href="http://arxiv.org/abs/2307.10305"/>
        <updated>2023-07-22T00:55:25.900Z</updated>
        <summary type="html"><![CDATA[Human beings always engage in a vast range of activities and tasks that
demonstrate their ability to adapt to different scenarios. Any human activity
can be represented as a temporal sequence of actions performed to achieve a
certain goal. Unlike the time series datasets extracted from electronics or
machines, these action sequences are highly disparate in their nature -- the
time to finish a sequence of actions can vary between different persons.
Therefore, understanding the dynamics of these sequences is essential for many
downstream tasks such as activity length prediction, goal prediction, next
action recommendation, etc. Existing neural network-based approaches that learn
a continuous-time activity sequence (or CTAS) are limited to the presence of
only visual data or are designed specifically for a particular task, i.e.,
limited to next action or goal prediction. In this paper, we present ProActive,
a neural marked temporal point process (MTPP) framework for modeling the
continuous-time distribution of actions in an activity sequence while
simultaneously addressing three high-impact problems -- next action prediction,
sequence-goal prediction, and end-to-end sequence generation. Specifically, we
utilize a self-attention module with temporal normalizing flows to model the
influence and the inter-arrival times between actions in a sequence. In
addition, we propose a novel addition over the ProActive model that can handle
variations in the order of actions, i.e., different methods of achieving a
given goal. We demonstrate that this variant can learn the order in which the
person or actor prefers to do their actions. Extensive experiments on sequences
derived from three activity recognition datasets show the significant accuracy
boost of ProActive over the state-of-the-art in terms of action and goal
prediction, and the first-ever application of end-to-end action sequence
generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1"&gt;Vinayak Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bedathur_S/0/1/0/all/0/1"&gt;Srikanta Bedathur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions. (arXiv:2307.10644v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10644</id>
        <link href="http://arxiv.org/abs/2307.10644"/>
        <updated>2023-07-22T00:55:25.900Z</updated>
        <summary type="html"><![CDATA[Data sets of multivariate normal distributions abound in many scientific
areas like diffusion tensor imaging, structure tensor computer vision, radar
signal processing, machine learning, just to name a few. In order to process
those normal data sets for downstream tasks like filtering, classification or
clustering, one needs to define proper notions of dissimilarities between
normals and paths joining them. The Fisher-Rao distance defined as the
Riemannian geodesic distance induced by the Fisher information metric is such a
principled metric distance which however is not known in closed-form excepts
for a few particular cases. In this work, we first report a fast and robust
method to approximate arbitrarily finely the Fisher-Rao distance between
multivariate normal distributions. Second, we introduce a class of distances
based on diffeomorphic embeddings of the normal manifold into a submanifold of
the higher-dimensional symmetric positive-definite cone corresponding to the
manifold of centered normal distributions. We show that the projective Hilbert
distance on the cone yields a metric on the embedded normal submanifold and we
pullback that cone distance with its associated straight line Hilbert cone
geodesics to obtain a distance and smooth paths between normal distributions.
Compared to the Fisher-Rao distance approximation, the pullback Hilbert cone
distance is computationally light since it requires to compute only the extreme
minimal and maximal eigenvalues of matrices. Finally, we show how to use those
distances in clustering tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1"&gt;Frank Nielsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PreDiff: Precipitation Nowcasting with Latent Diffusion Models. (arXiv:2307.10422v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10422</id>
        <link href="http://arxiv.org/abs/2307.10422"/>
        <updated>2023-07-22T00:55:25.899Z</updated>
        <summary type="html"><![CDATA[Earth system forecasting has traditionally relied on complex physical models
that are computationally expensive and require significant domain expertise. In
the past decade, the unprecedented increase in spatiotemporal Earth observation
data has enabled data-driven forecasting models using deep learning techniques.
These models have shown promise for diverse Earth system forecasting tasks but
either struggle with handling uncertainty or neglect domain-specific prior
knowledge, resulting in averaging possible futures to blurred forecasts or
generating physically implausible predictions. To address these limitations, we
propose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1)
We develop PreDiff, a conditional latent diffusion model capable of
probabilistic forecasts. 2) We incorporate an explicit knowledge control
mechanism to align forecasts with domain-specific physical constraints. This is
achieved by estimating the deviation from imposed constraints at each denoising
step and adjusting the transition distribution accordingly. We conduct
empirical studies on two datasets: N-body MNIST, a synthetic dataset with
chaotic behavior, and SEVIR, a real-world precipitation nowcasting dataset.
Specifically, we impose the law of conservation of energy in N-body MNIST and
anticipated precipitation intensity in SEVIR. Experiments demonstrate the
effectiveness of PreDiff in handling uncertainty, incorporating domain-specific
prior knowledge, and generating forecasts that exhibit high operational
utility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhihan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1"&gt;Xingjian Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Boran Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1"&gt;Xiaoyong Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maddix_D/0/1/0/all/0/1"&gt;Danielle Maddix&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hidden Markov Models with Random Restarts vs Boosting for Malware Detection. (arXiv:2307.10256v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2307.10256</id>
        <link href="http://arxiv.org/abs/2307.10256"/>
        <updated>2023-07-22T00:55:25.895Z</updated>
        <summary type="html"><![CDATA[Effective and efficient malware detection is at the forefront of research
into building secure digital systems. As with many other fields, malware
detection research has seen a dramatic increase in the application of machine
learning algorithms. One machine learning technique that has been used widely
in the field of pattern matching in general-and malware detection in
particular-is hidden Markov models (HMMs). HMM training is based on a hill
climb, and hence we can often improve a model by training multiple times with
different initial values. In this research, we compare boosted HMMs (using
AdaBoost) to HMMs trained with multiple random restarts, in the context of
malware detection. These techniques are applied to a variety of challenging
malware datasets. We find that random restarts perform surprisingly well in
comparison to boosting. Only in the most difficult "cold start" cases (where
training data is severely limited) does boosting appear to offer sufficient
improvement to justify its higher computational cost in the scoring phase.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raghavan_A/0/1/0/all/0/1"&gt;Aditya Raghavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Troia_F/0/1/0/all/0/1"&gt;Fabio Di Troia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1"&gt;Mark Stamp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Student Assessment in Cybersecurity Training Automated by Pattern Mining and Clustering. (arXiv:2307.10260v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2307.10260</id>
        <link href="http://arxiv.org/abs/2307.10260"/>
        <updated>2023-07-22T00:55:25.894Z</updated>
        <summary type="html"><![CDATA[Hands-on cybersecurity training allows students and professionals to practice
various tools and improve their technical skills. The training occurs in an
interactive learning environment that enables completing sophisticated tasks in
full-fledged operating systems, networks, and applications. During the
training, the learning environment allows collecting data about trainees'
interactions with the environment, such as their usage of command-line tools.
These data contain patterns indicative of trainees' learning processes, and
revealing them allows to assess the trainees and provide feedback to help them
learn. However, automated analysis of these data is challenging. The training
tasks feature complex problem-solving, and many different solution approaches
are possible. Moreover, the trainees generate vast amounts of interaction data.
This paper explores a dataset from 18 cybersecurity training sessions using
data mining and machine learning techniques. We employed pattern mining and
clustering to analyze 8834 commands collected from 113 trainees, revealing
their typical behavior, mistakes, solution strategies, and difficult training
stages. Pattern mining proved suitable in capturing timing information and tool
usage frequency. Clustering underlined that many trainees often face the same
issues, which can be addressed by targeted scaffolding. Our results show that
data mining methods are suitable for analyzing cybersecurity training data.
Educational researchers and practitioners can apply these methods in their
contexts to assess trainees, support them, and improve the training design.
Artifacts associated with this research are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Svabensky_V/0/1/0/all/0/1"&gt;Valdemar &amp;#x160;v&amp;#xe1;bensk&amp;#xfd;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vykopal_J/0/1/0/all/0/1"&gt;Jan Vykopal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celeda_P/0/1/0/all/0/1"&gt;Pavel &amp;#x10c;eleda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tkacik_K/0/1/0/all/0/1"&gt;Kristi&amp;#xe1;n Tk&amp;#xe1;&amp;#x10d;ik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popovic_D/0/1/0/all/0/1"&gt;Daniel Popovi&amp;#x10d;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Multimodal Datasets with Image Captioning. (arXiv:2307.10350v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10350</id>
        <link href="http://arxiv.org/abs/2307.10350"/>
        <updated>2023-07-22T00:55:25.892Z</updated>
        <summary type="html"><![CDATA[Massive web datasets play a key role in the success of large vision-language
models like CLIP and Flamingo. However, the raw web data is noisy, and existing
filtering methods to reduce noise often come at the expense of data diversity.
Our work focuses on caption quality as one major source of noise, and studies
how generated captions can increase the utility of web-scraped datapoints with
nondescript text. Through exploring different mixing strategies for raw and
generated captions, we outperform the best filtering method proposed by the
DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a
candidate pool of 128M image-text pairs. Our best approach is also 2x better at
Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an
effective source of text supervision. In experimenting with different image
captioning models, we also demonstrate that the performance of a model on
standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a reliable
indicator of the utility of the captions it generates for multimodal training.
Finally, our experiments with using generated captions at DataComp's large
scale (1.28B image-text pairs) offer insights into the limitations of synthetic
text, as well as the importance of image curation with increasing training data
quantity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thao Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1"&gt;Samir Yitzhak Gadre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1"&gt;Gabriel Ilharco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1"&gt;Sewoong Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1"&gt;Ludwig Schmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy Amplification via Importance Sampling. (arXiv:2307.10187v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2307.10187</id>
        <link href="http://arxiv.org/abs/2307.10187"/>
        <updated>2023-07-22T00:55:25.890Z</updated>
        <summary type="html"><![CDATA[We examine the privacy-enhancing properties of subsampling a data set via
importance sampling as a pre-processing step for differentially private
mechanisms. This extends the established privacy amplification by subsampling
result to importance sampling where each data point is weighted by the
reciprocal of its selection probability. The implications for privacy of
weighting each point are not obvious. On the one hand, a lower selection
probability leads to a stronger privacy amplification. On the other hand, the
higher the weight, the stronger the influence of the point on the output of the
mechanism in the event that the point does get selected. We provide a general
result that quantifies the trade-off between these two effects. We show that
heterogeneous sampling probabilities can lead to both stronger privacy and
better utility than uniform subsampling while retaining the subsample size. In
particular, we formulate and solve the problem of privacy-optimal sampling,
that is, finding the importance weights that minimize the expected subset size
subject to a given privacy budget. Empirically, we evaluate the privacy,
efficiency, and accuracy of importance sampling-based privacy amplification on
the example of k-means clustering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fay_D/0/1/0/all/0/1"&gt;Dominik Fay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mair_S/0/1/0/all/0/1"&gt;Sebastian Mair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sjolund_J/0/1/0/all/0/1"&gt;Jens Sj&amp;#xf6;lund&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A data science axiology: the nature, value, and risks of data science. (arXiv:2307.10460v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2307.10460</id>
        <link href="http://arxiv.org/abs/2307.10460"/>
        <updated>2023-07-22T00:55:25.886Z</updated>
        <summary type="html"><![CDATA[Data science is not a science. It is a research paradigm with an unfathomed
scope, scale, complexity, and power for knowledge discovery that is not
otherwise possible and can be beyond human reasoning. It is changing our world
practically and profoundly already widely deployed in tens of thousands of
applications in every discipline in an AI Arms Race that, due to its
inscrutability, can lead to unfathomed risks. This paper presents an axiology
of data science, its purpose, nature, importance, risks, and value for problem
solving, by exploring and evaluating its remarkable, definitive features. As
data science is in its infancy, this initial, speculative axiology is intended
to aid in understanding and defining data science to recognize its potential
benefits, risks, and open research challenges. AI based data science is
inherently about uncertainty that may be more realistic than our preference for
the certainty of science. Data science will have impacts far beyond knowledge
discovery and will take us into new ways of understanding the world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brodie_M/0/1/0/all/0/1"&gt;Michael L. Brodie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An IPW-based Unbiased Ranking Metric in Two-sided Markets. (arXiv:2307.10204v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2307.10204</id>
        <link href="http://arxiv.org/abs/2307.10204"/>
        <updated>2023-07-22T00:55:25.885Z</updated>
        <summary type="html"><![CDATA[In modern recommendation systems, unbiased learning-to-rank (LTR) is crucial
for prioritizing items from biased implicit user feedback, such as click data.
Several techniques, such as Inverse Propensity Weighting (IPW), have been
proposed for single-sided markets. However, less attention has been paid to
two-sided markets, such as job platforms or dating services, where successful
conversions require matching preferences from both users. This paper addresses
the complex interaction of biases between users in two-sided markets and
proposes a tailored LTR approach. We first present a formulation of feedback
mechanisms in two-sided matching platforms and point out that their implicit
feedback may include position bias from both user groups. On the basis of this
observation, we extend the IPW estimator and propose a new estimator, named
two-sided IPW, to address the position bases in two-sided markets. We prove
that the proposed estimator satisfies the unbiasedness for the ground-truth
ranking metric. We conducted numerical experiments on real-world two-sided
platforms and demonstrated the effectiveness of our proposed method in terms of
both precision and robustness. Our experiments showed that our method
outperformed baselines especially when handling rare items, which are less
frequently observed in the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oh_K/0/1/0/all/0/1"&gt;Keisho Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nishimura_N/0/1/0/all/0/1"&gt;Naoki Nishimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1"&gt;Minje Sung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobayashi_K/0/1/0/all/0/1"&gt;Ken Kobayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakata_K/0/1/0/all/0/1"&gt;Kazuhide Nakata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints. (arXiv:2307.10459v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10459</id>
        <link href="http://arxiv.org/abs/2307.10459"/>
        <updated>2023-07-22T00:55:25.885Z</updated>
        <summary type="html"><![CDATA[A new computationally simple method of imposing hard convex constraints on
the neural network output values is proposed. The key idea behind the method is
to map a vector of hidden parameters of the network to a point that is
guaranteed to be inside the feasible set defined by a set of constraints. The
mapping is implemented by the additional neural network layer with constraints
for output. The proposed method is simply extended to the case when constraints
are imposed not only on the output vectors, but also on joint constraints
depending on inputs. The projection approach to imposing constraints on outputs
can simply be implemented in the framework of the proposed method. It is shown
how to incorporate different types of constraints into the proposed method,
including linear and quadratic constraints, equality constraints, and dynamic
constraints, constraints in the form of boundaries. An important feature of the
method is its computational simplicity. Complexities of the forward pass of the
proposed neural network layer by linear and quadratic constraints are O(n*m)
and O(n^2*m), respectively, where n is the number of variables, m is the number
of constraints. Numerical experiments illustrate the method by solving
optimization and classification problems. The code implementing the method is
publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Konstantinov_A/0/1/0/all/0/1"&gt;Andrei V. Konstantinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Utkin_L/0/1/0/all/0/1"&gt;Lev V. Utkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty Quantification for Molecular Property Predictions with Graph Neural Architecture Search. (arXiv:2307.10438v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10438</id>
        <link href="http://arxiv.org/abs/2307.10438"/>
        <updated>2023-07-22T00:55:25.881Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have emerged as a prominent class of data-driven
methods for molecular property prediction. However, a key limitation of typical
GNN models is their inability to quantify uncertainties in the predictions.
This capability is crucial for ensuring the trustworthy use and deployment of
models in downstream tasks. To that end, we introduce AutoGNNUQ, an automated
uncertainty quantification (UQ) approach for molecular property prediction.
AutoGNNUQ leverages architecture search to generate an ensemble of
high-performing GNNs, enabling the estimation of predictive uncertainties. Our
approach employs variance decomposition to separate data (aleatoric) and model
(epistemic) uncertainties, providing valuable insights for reducing them. In
our computational experiments, we demonstrate that AutoGNNUQ outperforms
existing UQ methods in terms of both prediction accuracy and UQ performance on
multiple benchmark datasets. Additionally, we utilize t-SNE visualization to
explore correlations between molecular features and uncertainty, offering
insight for dataset improvement. AutoGNNUQ has broad applicability in domains
such as drug discovery and materials science, where accurate uncertainty
quantification is crucial for decision-making.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shengli Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1"&gt;Shiyi Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehn_R/0/1/0/all/0/1"&gt;Reid C. Van Lehn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balaprakash_P/0/1/0/all/0/1"&gt;Prasanna Balaprakash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zavala_V/0/1/0/all/0/1"&gt;Victor M. Zavala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Several categories of Large Language Models (LLMs): A Short Survey. (arXiv:2307.10188v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.10188</id>
        <link href="http://arxiv.org/abs/2307.10188"/>
        <updated>2023-07-22T00:55:25.870Z</updated>
        <summary type="html"><![CDATA[Large Language Models(LLMs)have become effective tools for natural language
processing and have been used in many different fields. This essay offers a
succinct summary of various LLM subcategories. The survey emphasizes recent
developments and efforts made for various LLM kinds, including task-based
financial LLMs, multilingual language LLMs, biomedical and clinical LLMs,
vision language LLMs, and code language models. The survey gives a general
summary of the methods, attributes, datasets, transformer models, and
comparison metrics applied in each category of LLMs. Furthermore, it highlights
unresolved problems in the field of developing chatbots and virtual assistants,
such as boosting natural language processing, enhancing chatbot intelligence,
and resolving moral and legal dilemmas. The purpose of this study is to provide
readers, developers, academics, and users interested in LLM-based chatbots and
virtual intelligent assistant technologies with useful information and future
directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pahune_S/0/1/0/all/0/1"&gt;Saurabh Pahune&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandrasekharan_M/0/1/0/all/0/1"&gt;Manoj Chandrasekharan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Selection functions of strong lens finding neural networks. (arXiv:2307.10355v1 [astro-ph.CO])]]></title>
        <id>http://arxiv.org/abs/2307.10355</id>
        <link href="http://arxiv.org/abs/2307.10355"/>
        <updated>2023-07-22T00:55:25.869Z</updated>
        <summary type="html"><![CDATA[Convolution Neural Networks trained for the task of lens finding with similar
architecture and training data as is commonly found in the literature are
biased classifiers. An understanding of the selection function of lens finding
neural networks will be key to fully realising the potential of the large
samples of strong gravitational lens systems that will be found in upcoming
wide-field surveys. We use three training datasets, representative of those
used to train galaxy-galaxy and galaxy-quasar lens finding neural networks. The
networks preferentially select systems with larger Einstein radii and larger
sources with more concentrated source-light distributions. Increasing the
detection significance threshold to 12$\sigma$ from 8$\sigma$ results in 50 per
cent of the selected strong lens systems having Einstein radii
$\theta_\mathrm{E}$ $\ge$ 1.04 arcsec from $\theta_\mathrm{E}$ $\ge$ 0.879
arcsec, source radii $R_S$ $\ge$ 0.194 arcsec from $R_S$ $\ge$ 0.178 arcsec and
source S\'ersic indices $n_{\mathrm{Sc}}^{\mathrm{S}}$ $\ge$ 2.62 from
$n_{\mathrm{Sc}}^{\mathrm{S}}$ $\ge$ 2.55. The model trained to find lensed
quasars shows a stronger preference for higher lens ellipticities than those
trained to find lensed galaxies. The selection function is independent of the
slope of the power-law of the mass profiles, hence measurements of this
quantity will be unaffected. The lens finder selection function reinforces that
of the lensing cross-section, and thus we expect our findings to be a general
result for all galaxy-galaxy and galaxy-quasar lens finding neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Herle_A/0/1/0/all/0/1"&gt;A. Herle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+ORiordan_C/0/1/0/all/0/1"&gt;C. M. O&amp;#x27;Riordan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Vegetti_S/0/1/0/all/0/1"&gt;S. Vegetti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient selective attention LSTM for well log curve synthesis. (arXiv:2307.10253v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10253</id>
        <link href="http://arxiv.org/abs/2307.10253"/>
        <updated>2023-07-22T00:55:25.868Z</updated>
        <summary type="html"><![CDATA[Non-core drilling has gradually become the primary exploration method in
geological engineering, and well logging curves have increasingly gained
importance as the main carriers of geological information. However, factors
such as geological environment, logging equipment, borehole quality, and
unexpected events can all impact the quality of well logging curves. Previous
methods of re-logging or manual corrections have been associated with high
costs and low efficiency. This paper proposes a machine learning method that
utilizes existing data to predict missing well logging curves, and its
effectiveness and feasibility have been validated through experiments. The
proposed method builds upon the traditional Long Short-Term Memory (LSTM)
neural network by incorporating a self-attention mechanism to analyze the
spatial dependencies of the data. It selectively includes the dominant
computational results in the LSTM, reducing the computational complexity from
O(n^2) to O(nlogn) and improving model efficiency. Experimental results
demonstrate that the proposed method achieves higher accuracy compared to
traditional curve synthesis methods based on Fully Connected Neural Networks
(FCNN) and LSTM. This accurate, efficient, and cost-effective prediction method
holds practical value in engineering applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuankai Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Huanyu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperparameter Tuning Cookbook: A guide for scikit-learn, PyTorch, river, and spotPython. (arXiv:2307.10262v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10262</id>
        <link href="http://arxiv.org/abs/2307.10262"/>
        <updated>2023-07-22T00:55:25.868Z</updated>
        <summary type="html"><![CDATA[This document provides a comprehensive guide to hyperparameter tuning using
spotPython for scikit-learn, PyTorch, and river. The first part introduces
spotPython's surrogate model-based optimization process, while the second part
focuses on hyperparameter tuning. Several case studies are presented, including
hyperparameter tuning for sklearn models such as Support Vector Classification,
Random Forests, Gradient Boosting (XGB), and K-nearest neighbors (KNN), as well
as a Hoeffding Adaptive Tree Regressor from river. The integration of
spotPython into the PyTorch and PyTorch Lightning training workflow is also
discussed. With a hands-on approach and step-by-step explanations, this
cookbook serves as a practical starting point for anyone interested in
hyperparameter tuning with Python. Highlights include the interplay between
Tensorboard, PyTorch Lightning, spotPython, and river. This publication is
under development, with updates available on the corresponding webpage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bartz_Beielstein_T/0/1/0/all/0/1"&gt;Thomas Bartz-Beielstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StyleGAN2-based Out-of-Distribution Detection for Medical Imaging. (arXiv:2307.10193v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2307.10193</id>
        <link href="http://arxiv.org/abs/2307.10193"/>
        <updated>2023-07-22T00:55:25.867Z</updated>
        <summary type="html"><![CDATA[One barrier to the clinical deployment of deep learning-based models is the
presence of images at runtime that lie far outside the training distribution of
a given model. We aim to detect these out-of-distribution (OOD) images with a
generative adversarial network (GAN). Our training dataset was comprised of
3,234 liver-containing computed tomography (CT) scans from 456 patients. Our
OOD test data consisted of CT images of the brain, head and neck, lung, cervix,
and abnormal livers. A StyleGAN2-ADA architecture was employed to model the
training distribution. Images were reconstructed using backpropagation.
Reconstructions were evaluated using the Wasserstein distance, mean squared
error, and the structural similarity index measure. OOD detection was evaluated
with the area under the receiver operating characteristic curve (AUROC). Our
paradigm distinguished between liver and non-liver CT with greater than 90%
AUROC. It was also completely unable to reconstruct liver artifacts, such as
needles and ascites.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Woodland_M/0/1/0/all/0/1"&gt;McKell Woodland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wood_J/0/1/0/all/0/1"&gt;John Wood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+OConnor_C/0/1/0/all/0/1"&gt;Caleb O&amp;#x27;Connor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Patel_A/0/1/0/all/0/1"&gt;Ankit B. Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Brock_K/0/1/0/all/0/1"&gt;Kristy K. Brock&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating and Enhancing Robustness of Deep Recommendation Systems Against Hardware Errors. (arXiv:2307.10244v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2307.10244</id>
        <link href="http://arxiv.org/abs/2307.10244"/>
        <updated>2023-07-22T00:55:25.867Z</updated>
        <summary type="html"><![CDATA[Deep recommendation systems (DRS) heavily depend on specialized HPC hardware
and accelerators to optimize energy, efficiency, and recommendation quality.
Despite the growing number of hardware errors observed in large-scale fleet
systems where DRS are deployed, the robustness of DRS has been largely
overlooked. This paper presents the first systematic study of DRS robustness
against hardware errors. We develop Terrorch, a user-friendly, efficient and
flexible error injection framework on top of the widely-used PyTorch. We
evaluate a wide range of models and datasets and observe that the DRS
robustness against hardware errors is influenced by various factors from model
parameters to input characteristics. We also explore 3 error mitigation methods
including algorithm based fault tolerance (ABFT), activation clipping and
selective bit protection (SBP). We find that applying activation clipping can
recover up to 30% of the degraded AUC-ROC score, making it a promising
mitigation method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1"&gt;Dongning Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_X/0/1/0/all/0/1"&gt;Xun Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1"&gt;Fred Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mengshi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desmaison_A/0/1/0/all/0/1"&gt;Alban Desmaison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sellinger_T/0/1/0/all/0/1"&gt;Thomas Sellinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moore_D/0/1/0/all/0/1"&gt;Daniel Moore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankar_S/0/1/0/all/0/1"&gt;Sriram Sankar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Unsupervised Deep Outlier Model Selection with Hypernetworks. (arXiv:2307.10529v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10529</id>
        <link href="http://arxiv.org/abs/2307.10529"/>
        <updated>2023-07-22T00:55:25.867Z</updated>
        <summary type="html"><![CDATA[Outlier detection (OD) finds many applications with a rich literature of
numerous techniques. Deep neural network based OD (DOD) has seen a recent surge
of attention thanks to the many advances in deep learning. In this paper, we
consider a critical-yet-understudied challenge with unsupervised DOD, that is,
effective hyperparameter (HP) tuning/model selection. While several prior work
report the sensitivity of OD models to HPs, it becomes ever so critical for the
modern DOD models that exhibit a long list of HPs. We introduce HYPER for
tuning DOD models, tackling two fundamental challenges: (1) validation without
supervision (due to lack of labeled anomalies), and (2) efficient search of the
HP/model space (due to exponential growth in the number of HPs). A key idea is
to design and train a novel hypernetwork (HN) that maps HPs onto optimal
weights of the main DOD model. In turn, HYPER capitalizes on a single HN that
can dynamically generate weights for many DOD models (corresponding to varying
HPs), which offers significant speed-up. In addition, it employs meta-learning
on historical OD tasks with labels to train a proxy validation function,
likewise trained with our proposed HN efficiently. Extensive experiments on 35
OD tasks show that HYPER achieves high performance against 8 baselines with
significant efficiency gains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1"&gt;Xueying Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yue Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akoglu_L/0/1/0/all/0/1"&gt;Leman Akoglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ECSIC: Epipolar Cross Attention for Stereo Image Compression. (arXiv:2307.10284v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2307.10284</id>
        <link href="http://arxiv.org/abs/2307.10284"/>
        <updated>2023-07-22T00:55:25.866Z</updated>
        <summary type="html"><![CDATA[In this paper, we present ECSIC, a novel learned method for stereo image
compression. Our proposed method compresses the left and right images in a
joint manner by exploiting the mutual information between the images of the
stereo image pair using a novel stereo cross attention (SCA) module and two
stereo context modules. The SCA module performs cross-attention restricted to
the corresponding epipolar lines of the two images and processes them in
parallel. The stereo context modules improve the entropy estimation of the
second encoded image by using the first image as a context. We conduct an
extensive ablation study demonstrating the effectiveness of the proposed
modules and a comprehensive quantitative and qualitative comparison with
existing methods. ECSIC achieves state-of-the-art performance among stereo
image compression models on the two popular stereo image datasets Cityscapes
and InStereo2k while allowing for fast encoding and decoding, making it highly
practical for real-time applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wodlinger_M/0/1/0/all/0/1"&gt;Matthias W&amp;#xf6;dlinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kotera_J/0/1/0/all/0/1"&gt;Jan Kotera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Keglevic_M/0/1/0/all/0/1"&gt;Manuel Keglevic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sablatnig_R/0/1/0/all/0/1"&gt;Robert Sablatnig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Sensitivity of Deep Load Disaggregation to Adversarial Attacks. (arXiv:2307.10209v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2307.10209</id>
        <link href="http://arxiv.org/abs/2307.10209"/>
        <updated>2023-07-22T00:55:25.865Z</updated>
        <summary type="html"><![CDATA[Non-intrusive Load Monitoring (NILM) algorithms, commonly referred to as load
disaggregation algorithms, are fundamental tools for effective energy
management. Despite the success of deep models in load disaggregation, they
face various challenges, particularly those pertaining to privacy and security.
This paper investigates the sensitivity of prominent deep NILM baselines to
adversarial attacks, which have proven to be a significant threat in domains
such as computer vision and speech recognition. Adversarial attacks entail the
introduction of imperceptible noise into the input data with the aim of
misleading the neural network into generating erroneous outputs. We investigate
the Fast Gradient Sign Method (FGSM), a well-known adversarial attack, to
perturb the input sequences fed into two commonly employed CNN-based NILM
baselines: the Sequence-to-Sequence (S2S) and Sequence-to-Point (S2P) models.
Our findings provide compelling evidence for the vulnerability of these models,
particularly the S2P model which exhibits an average decline of 20\% in the
F1-score even with small amounts of noise. Such weakness has the potential to
generate profound implications for energy management systems in residential and
industrial sectors reliant on NILM models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bousbiat_H/0/1/0/all/0/1"&gt;Hafsa Bousbiat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Himeur_Y/0/1/0/all/0/1"&gt;Yassine Himeur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amira_A/0/1/0/all/0/1"&gt;Abbes Amira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansoor_W/0/1/0/all/0/1"&gt;Wathiq Mansoor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling Societal Inequality from Model Biases: Gender Inequality in Divorce Court Proceedings. (arXiv:2307.10200v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2307.10200</id>
        <link href="http://arxiv.org/abs/2307.10200"/>
        <updated>2023-07-22T00:55:25.864Z</updated>
        <summary type="html"><![CDATA[Divorce is the legal dissolution of a marriage by a court. Since this is
usually an unpleasant outcome of a marital union, each party may have reasons
to call the decision to quit which is generally documented in detail in the
court proceedings. Via a substantial corpus of 17,306 court proceedings, this
paper investigates gender inequality through the lens of divorce court
proceedings. While emerging data sources (e.g., public court records) on
sensitive societal issues hold promise in aiding social science research,
biases present in cutting-edge natural language processing (NLP) methods may
interfere with or affect such studies. We thus require a thorough analysis of
potential gaps and limitations present in extant NLP resources. In this paper,
on the methodological side, we demonstrate that existing NLP resources required
several non-trivial modifications to quantify societal inequalities. On the
substantive side, we find that while a large number of court cases perhaps
suggest changing norms in India where women are increasingly challenging
patriarchy, AI-powered analyses of these court proceedings indicate striking
gender inequality with women often subjected to domestic violence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1"&gt;Sujan Dutta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1"&gt;Parth Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solunke_V/0/1/0/all/0/1"&gt;Vaishnavi Solunke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nath_S/0/1/0/all/0/1"&gt;Swaprava Nath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+KhudaBukhsh_A/0/1/0/all/0/1"&gt;Ashiqur R. KhudaBukhsh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Bayesian Programming Approach to Car-following Model Calibration and Validation using Limited Data. (arXiv:2307.10437v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10437</id>
        <link href="http://arxiv.org/abs/2307.10437"/>
        <updated>2023-07-22T00:55:25.864Z</updated>
        <summary type="html"><![CDATA[Traffic simulation software is used by transportation researchers and
engineers to design and evaluate changes to roadways. These simulators are
driven by models of microscopic driver behavior from which macroscopic measures
like flow and congestion can be derived. Many models are designed for a subset
of possible traffic scenarios and roadway configurations, while others have no
explicit constraints on their application. Work zones (WZs) are one scenario
for which no model to date has reproduced realistic driving behavior. This
makes it difficult to optimize for safety and other metrics when designing a
WZ. The Federal Highway Administration commissioned the USDOT Volpe Center to
develop a car-following (CF) model for use in microscopic simulators that can
capture and reproduce driver behavior accurately within and outside of WZs.
Volpe also performed a naturalistic driving study to collect telematics data
from vehicles driven on roads with WZs for use in model calibration. During
model development, Volpe researchers observed difficulties in calibrating their
model, leaving them to question whether there existed flaws in their model, in
the data, or in the procedure used to calibrate the model using the data. In
this thesis, I use Bayesian methods for data analysis and parameter estimation
to explore and, where possible, address these questions. First, I use Bayesian
inference to measure the sufficiency of the size of the data set. Second, I
compare the procedure and results of the genetic algorithm based calibration
performed by the Volpe researchers with those of Bayesian calibration. Third, I
explore the benefits of modeling CF hierarchically. Finally, I apply what was
learned in the first three phases using an established CF model, Wiedemann 99,
to the probabilistic modeling of the Volpe model. Validation is performed using
information criteria as an estimate of predictive accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abodo_F/0/1/0/all/0/1"&gt;Franklin Abodo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2307.10490</id>
        <link href="http://arxiv.org/abs/2307.10490"/>
        <updated>2023-07-22T00:55:25.860Z</updated>
        <summary type="html"><![CDATA[We demonstrate how images and sounds can be used for indirect prompt and
instruction injection in multi-modal LLMs. An attacker generates an adversarial
perturbation corresponding to the prompt and blends it into an image or audio
recording. When the user asks the (unmodified, benign) model about the
perturbed image or audio, the perturbation steers the model to output the
attacker-chosen text and/or make the subsequent dialog follow the attacker's
instruction. We illustrate this attack with several proof-of-concept examples
targeting LLaVa and PandaGPT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bagdasaryan_E/0/1/0/all/0/1"&gt;Eugene Bagdasaryan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_T/0/1/0/all/0/1"&gt;Tsung-Yin Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nassi_B/0/1/0/all/0/1"&gt;Ben Nassi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shmatikov_V/0/1/0/all/0/1"&gt;Vitaly Shmatikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear Meta-Learning Can Guarantee Faster Rates. (arXiv:2307.10870v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.10870</id>
        <link href="http://arxiv.org/abs/2307.10870"/>
        <updated>2023-07-22T00:55:25.860Z</updated>
        <summary type="html"><![CDATA[Many recent theoretical works on \emph{meta-learning} aim to achieve
guarantees in leveraging similar representational structures from related tasks
towards simplifying a target task. Importantly, the main aim in theory works on
the subject is to understand the extent to which convergence rates -- in
learning a common representation -- \emph{may scale with the number $N$ of
tasks} (as well as the number of samples per task). First steps in this setting
demonstrate this property when both the shared representation amongst tasks,
and task-specific regression functions, are linear. This linear setting readily
reveals the benefits of aggregating tasks, e.g., via averaging arguments. In
practice, however, the representation is often highly nonlinear, introducing
nontrivial biases in each task that cannot easily be averaged out as in the
linear case. In the present work, we derive theoretical guarantees for
meta-learning with nonlinear representations. In particular, assuming the
shared nonlinearity maps to an infinite-dimensional RKHS, we show that
additional biases can be mitigated with careful regularization that leverages
the smoothness of task-specific regression functions,]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Meunier_D/0/1/0/all/0/1"&gt;Dimitri Meunier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1"&gt;Arthur Gretton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kpotufe_S/0/1/0/all/0/1"&gt;Samory Kpotufe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AUC Optimization from Multiple Unlabeled Datasets. (arXiv:2305.15776v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.15776</id>
        <link href="http://arxiv.org/abs/2305.15776"/>
        <updated>2023-07-22T00:55:25.855Z</updated>
        <summary type="html"><![CDATA[Weakly supervised learning aims to empower machine learning when the perfect
supervision is unavailable, which has drawn great attention from researchers.
Among various types of weak supervision, one of the most challenging cases is
to learn from multiple unlabeled (U) datasets with only a little knowledge of
the class priors, or U$^m$ learning for short. In this paper, we study the
problem of building an AUC (area under ROC curve) optimization model from
multiple unlabeled datasets, which maximizes the pairwise ranking ability of
the classifier. We propose U$^m$-AUC, an AUC optimization approach that
converts the U$^m$ data into a multi-label AUC optimization problem, and can be
trained efficiently. We show that the proposed U$^m$-AUC is effective
theoretically and empirically.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zheng Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Ming Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FinGPT: Democratizing Internet-scale Data for Financial Large Language Models. (arXiv:2307.10485v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.10485</id>
        <link href="http://arxiv.org/abs/2307.10485"/>
        <updated>2023-07-22T00:55:25.854Z</updated>
        <summary type="html"><![CDATA[Large language models (LLMs) have demonstrated remarkable proficiency in
understanding and generating human-like texts, which may potentially
revolutionize the finance industry. However, existing LLMs often fall short in
the financial field, which is mainly attributed to the disparities between
general text data and financial text data. Unfortunately, there is only a
limited number of financial text datasets available (quite small size), and
BloombergGPT, the first financial LLM (FinLLM), is close-sourced (only the
training logs were released). In light of this, we aim to democratize
Internet-scale financial data for LLMs, which is an open challenge due to
diverse data sources, low signal-to-noise ratio, and high time-validity. To
address the challenges, we introduce an open-sourced and data-centric
framework, \textit{Financial Generative Pre-trained Transformer (FinGPT)}, that
automates the collection and curation of real-time financial data from >34
diverse sources on the Internet, providing researchers and practitioners with
accessible and transparent resources to develop their FinLLMs. Additionally, we
propose a simple yet effective strategy for fine-tuning FinLLM using the
inherent feedback from the market, dubbed Reinforcement Learning with Stock
Prices (RLSP). We also adopt the Low-rank Adaptation (LoRA, QLoRA) method that
enables users to customize their own FinLLMs from open-source general-purpose
LLMs at a low cost. Finally, we showcase several FinGPT applications, including
robo-advisor, sentiment analysis for algorithmic trading, and low-code
development. FinGPT aims to democratize FinLLMs, stimulate innovation, and
unlock new opportunities in open finance. The codes are available at
https://github.com/AI4Finance-Foundation/FinGPT and
https://github.com/AI4Finance-Foundation/FinNLP]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao-Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guoxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1"&gt;Daochen Zha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoNAN: Conditional Neural Aggregation Network For Unconstrained Face Feature Fusion. (arXiv:2307.10237v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.10237</id>
        <link href="http://arxiv.org/abs/2307.10237"/>
        <updated>2023-07-22T00:55:25.853Z</updated>
        <summary type="html"><![CDATA[Face recognition from image sets acquired under unregulated and uncontrolled
settings, such as at large distances, low resolutions, varying viewpoints,
illumination, pose, and atmospheric conditions, is challenging. Face feature
aggregation, which involves aggregating a set of N feature representations
present in a template into a single global representation, plays a pivotal role
in such recognition systems. Existing works in traditional face feature
aggregation either utilize metadata or high-dimensional intermediate feature
representations to estimate feature quality for aggregation. However,
generating high-quality metadata or style information is not feasible for
extremely low-resolution faces captured in long-range and high altitude
settings. To overcome these limitations, we propose a feature distribution
conditioning approach called CoNAN for template aggregation. Specifically, our
method aims to learn a context vector conditioned over the distribution
information of the incoming feature set, which is utilized to weigh the
features based on their estimated informativeness. The proposed method produces
state-of-the-art results on long-range unconstrained face recognition datasets
such as BTS, and DroneSURF, validating the advantages of such an aggregation
strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jawade_B/0/1/0/all/0/1"&gt;Bhavin Jawade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_D/0/1/0/all/0/1"&gt;Deen Dayal Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fedorishin_D/0/1/0/all/0/1"&gt;Dennis Fedorishin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Setlur_S/0/1/0/all/0/1"&gt;Srirangaraj Setlur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Govindaraju_V/0/1/0/all/0/1"&gt;Venu Govindaraju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval. (arXiv:2307.10488v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2307.10488</id>
        <link href="http://arxiv.org/abs/2307.10488"/>
        <updated>2023-07-22T00:55:25.797Z</updated>
        <summary type="html"><![CDATA[Traditionally, sparse retrieval systems relied on lexical representations to
retrieve documents, such as BM25, dominated information retrieval tasks. With
the onset of pre-trained transformer models such as BERT, neural sparse
retrieval has led to a new paradigm within retrieval. Despite the success,
there has been limited software supporting different sparse retrievers running
in a unified, common environment. This hinders practitioners from fairly
comparing different sparse models and obtaining realistic evaluation results.
Another missing piece is, that a majority of prior work evaluates sparse
retrieval models on in-domain retrieval, i.e. on a single dataset: MS MARCO.
However, a key requirement in practical retrieval systems requires models that
can generalize well to unseen out-of-domain, i.e. zero-shot retrieval tasks. In
this work, we provide SPRINT, a unified Python toolkit based on Pyserini and
Lucene, supporting a common interface for evaluating neural sparse retrieval.
The toolkit currently includes five built-in models: uniCOIL, DeepImpact,
SPARTA, TILDEv2 and SPLADEv2. Users can also easily add customized models by
defining their term weighting method. Using our toolkit, we establish strong
and reproducible zero-shot sparse retrieval baselines across the
well-acknowledged benchmark, BEIR. Our results demonstrate that SPLADEv2
achieves the best average score of 0.470 nDCG@10 on BEIR amongst all neural
sparse retrievers. In this work, we further uncover the reasons behind its
performance gain. We show that SPLADEv2 produces sparse representations with a
majority of tokens outside of the original query and document which is often
crucial for its performance gains, i.e. a limitation among its other sparse
counterparts. We provide our SPRINT toolkit, models, and data used in our
experiments publicly here at https://github.com/thakur-nandan/sprint.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1"&gt;Nandan Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kexin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1"&gt;Iryna Gurevych&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jimmy Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Training Over Long-Tailed Distribution. (arXiv:2307.10205v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10205</id>
        <link href="http://arxiv.org/abs/2307.10205"/>
        <updated>2023-07-22T00:55:25.787Z</updated>
        <summary type="html"><![CDATA[In this paper, we study adversarial training on datasets that obey the
long-tailed distribution, which is practical but rarely explored in previous
works. Compared with conventional adversarial training on balanced datasets,
this process falls into the dilemma of generating uneven adversarial examples
(AEs) and an unbalanced feature embedding space, causing the resulting model to
exhibit low robustness and accuracy on tail data. To combat that, we propose a
new adversarial training framework -- Re-balancing Adversarial Training (REAT).
This framework consists of two components: (1) a new training strategy inspired
by the term effective number to guide the model to generate more balanced and
informative AEs; (2) a carefully constructed penalty function to force a
satisfactory feature space. Evaluation results on different datasets and model
structures prove that REAT can effectively enhance the model's robustness and
preserve the model's clean accuracy. The code can be found in
https://github.com/GuanlinLee/REAT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guanlin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guowen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianwei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Offline Data in Online Reinforcement Learning. (arXiv:2211.04974v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.04974</id>
        <link href="http://arxiv.org/abs/2211.04974"/>
        <updated>2023-07-22T00:55:25.787Z</updated>
        <summary type="html"><![CDATA[Two central paradigms have emerged in the reinforcement learning (RL)
community: online RL and offline RL. In the online RL setting, the agent has no
prior knowledge of the environment, and must interact with it in order to find
an $\epsilon$-optimal policy. In the offline RL setting, the learner instead
has access to a fixed dataset to learn from, but is unable to otherwise
interact with the environment, and must obtain the best policy it can from this
offline data. Practical scenarios often motivate an intermediate setting: if we
have some set of offline data and, in addition, may also interact with the
environment, how can we best use the offline data to minimize the number of
online interactions necessary to learn an $\epsilon$-optimal policy?

In this work, we consider this setting, which we call the \textsf{FineTuneRL}
setting, for MDPs with linear structure. We characterize the necessary number
of online samples needed in this setting given access to some offline dataset,
and develop an algorithm, \textsc{FTPedel}, which is provably optimal, up to
$H$ factors. We show through an explicit example that combining offline data
with online interactions can lead to a provable improvement over either purely
offline or purely online RL. Finally, our results illustrate the distinction
between \emph{verifiable} learning, the typical setting considered in online
RL, and \emph{unverifiable} learning, the setting often considered in offline
RL, and show that there is a formal separation between these regimes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wagenmaker_A/0/1/0/all/0/1"&gt;Andrew Wagenmaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1"&gt;Aldo Pacchiano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Community-Aware Transformer for Autism Prediction in fMRI Connectome. (arXiv:2307.10181v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2307.10181</id>
        <link href="http://arxiv.org/abs/2307.10181"/>
        <updated>2023-07-22T00:55:25.786Z</updated>
        <summary type="html"><![CDATA[Autism spectrum disorder(ASD) is a lifelong neurodevelopmental condition that
affects social communication and behavior. Investigating functional magnetic
resonance imaging (fMRI)-based brain functional connectome can aid in the
understanding and diagnosis of ASD, leading to more effective treatments. The
brain is modeled as a network of brain Regions of Interest (ROIs), and ROIs
form communities and knowledge of these communities is crucial for ASD
diagnosis. On the one hand, Transformer-based models have proven to be highly
effective across several tasks, including fMRI connectome analysis to learn
useful representations of ROIs. On the other hand, existing transformer-based
models treat all ROIs equally and overlook the impact of community-specific
associations when learning node embeddings. To fill this gap, we propose a
novel method, Com-BrainTF, a hierarchical local-global transformer architecture
that learns intra and inter-community aware node embeddings for ASD prediction
task. Furthermore, we avoid over-parameterization by sharing the local
transformer parameters for different communities but optimize unique learnable
prompt tokens for each community. Our model outperforms state-of-the-art (SOTA)
architecture on ABIDE dataset and has high interpretability, evident from the
attention module. Our code is available at
https://github.com/ubc-tea/Com-BrainTF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Bannadabhavi_A/0/1/0/all/0/1"&gt;Anushree Bannadabhavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lee_S/0/1/0/all/0/1"&gt;Soojin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Deng_W/0/1/0/all/0/1"&gt;Wenlong Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoxiao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?. (arXiv:2307.10472v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.10472</id>
        <link href="http://arxiv.org/abs/2307.10472"/>
        <updated>2023-07-22T00:55:25.786Z</updated>
        <summary type="html"><![CDATA[As the breadth and depth of language model applications continue to expand
rapidly, it is increasingly important to build efficient frameworks for
measuring and mitigating the learned or inherited social biases of these
models. In this paper, we present our work on evaluating instruction fine-tuned
language models' ability to identify bias through zero-shot prompting,
including Chain-of-Thought (CoT) prompts. Across LLaMA and its two instruction
fine-tuned versions, Alpaca 7B performs best on the bias identification task
with an accuracy of 56.7%. We also demonstrate that scaling up LLM size and
data diversity could lead to further performance gain. This is a
work-in-progress presenting the first component of our bias mitigation
framework. We will keep updating this work as we get more results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dige_O/0/1/0/all/0/1"&gt;Omkar Dige&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1"&gt;Jacob-Junqi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Emerson_D/0/1/0/all/0/1"&gt;David Emerson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khattak_F/0/1/0/all/0/1"&gt;Faiza Khan Khattak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invariant Causal Set Covering Machines. (arXiv:2306.04777v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.04777</id>
        <link href="http://arxiv.org/abs/2306.04777"/>
        <updated>2023-07-22T00:55:25.785Z</updated>
        <summary type="html"><![CDATA[Rule-based models, such as decision trees, appeal to practitioners due to
their interpretable nature. However, the learning algorithms that produce such
models are often vulnerable to spurious associations and thus, they are not
guaranteed to extract causally-relevant insights. In this work, we build on
ideas from the invariant causal prediction literature to propose Invariant
Causal Set Covering Machines, an extension of the classical Set Covering
Machine algorithm for conjunctions/disjunctions of binary-valued rules that
provably avoids spurious associations. We demonstrate both theoretically and
empirically that our method can identify the causal parents of a variable of
interest in polynomial time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Godon_T/0/1/0/all/0/1"&gt;Thibaud Godon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauvin_B/0/1/0/all/0/1"&gt;Baptiste Bauvin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Germain_P/0/1/0/all/0/1"&gt;Pascal Germain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corbeil_J/0/1/0/all/0/1"&gt;Jacques Corbeil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1"&gt;Alexandre Drouin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Torchhd: An Open Source Python Library to Support Research on Hyperdimensional Computing and Vector Symbolic Architectures. (arXiv:2205.09208v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2205.09208</id>
        <link href="http://arxiv.org/abs/2205.09208"/>
        <updated>2023-07-22T00:55:25.775Z</updated>
        <summary type="html"><![CDATA[Hyperdimensional computing (HD), also known as vector symbolic architectures
(VSA), is a framework for computing with distributed representations by
exploiting properties of random high-dimensional vector spaces. The commitment
of the scientific community to aggregate and disseminate research in this
particularly multidisciplinary area has been fundamental for its advancement.
Joining these efforts, we present Torchhd, a high-performance open source
Python library for HD/VSA. Torchhd seeks to make HD/VSA more accessible and
serves as an efficient foundation for further research and application
development. The easy-to-use library builds on top of PyTorch and features
state-of-the-art HD/VSA functionality, clear documentation, and implementation
examples from well-known publications. Comparing publicly available code with
their corresponding Torchhd implementation shows that experiments can run up to
100x faster. Torchhd is available at:
https://github.com/hyperdimensional-computing/torchhd.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heddes_M/0/1/0/all/0/1"&gt;Mike Heddes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nunes_I/0/1/0/all/0/1"&gt;Igor Nunes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verges_P/0/1/0/all/0/1"&gt;Pere Verg&amp;#xe9;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleyko_D/0/1/0/all/0/1"&gt;Denis Kleyko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abraham_D/0/1/0/all/0/1"&gt;Danny Abraham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Givargis_T/0/1/0/all/0/1"&gt;Tony Givargis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nicolau_A/0/1/0/all/0/1"&gt;Alexandru Nicolau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veidenbaum_A/0/1/0/all/0/1"&gt;Alexander Veidenbaum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DP-TBART: A Transformer-based Autoregressive Model for Differentially Private Tabular Data Generation. (arXiv:2307.10430v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10430</id>
        <link href="http://arxiv.org/abs/2307.10430"/>
        <updated>2023-07-22T00:55:25.767Z</updated>
        <summary type="html"><![CDATA[The generation of synthetic tabular data that preserves differential privacy
is a problem of growing importance. While traditional marginal-based methods
have achieved impressive results, recent work has shown that deep
learning-based approaches tend to lag behind. In this work, we present
Differentially-Private TaBular AutoRegressive Transformer (DP-TBART), a
transformer-based autoregressive model that maintains differential privacy and
achieves performance competitive with marginal-based methods on a wide variety
of datasets, capable of even outperforming state-of-the-art methods in certain
settings. We also provide a theoretical framework for understanding the
limitations of marginal-based approaches and where deep learning-based
approaches stand to contribute most. These results suggest that deep
learning-based techniques should be considered as a viable alternative to
marginal-based methods in the generation of differentially private synthetic
tabular data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Castellon_R/0/1/0/all/0/1"&gt;Rodrigo Castellon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopal_A/0/1/0/all/0/1"&gt;Achintya Gopal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bloniarz_B/0/1/0/all/0/1"&gt;Brian Bloniarz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosenberg_D/0/1/0/all/0/1"&gt;David Rosenberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence Estimation Using Unlabeled Data. (arXiv:2307.10440v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10440</id>
        <link href="http://arxiv.org/abs/2307.10440"/>
        <updated>2023-07-22T00:55:25.767Z</updated>
        <summary type="html"><![CDATA[Overconfidence is a common issue for deep neural networks, limiting their
deployment in real-world applications. To better estimate confidence, existing
methods mostly focus on fully-supervised scenarios and rely on training labels.
In this paper, we propose the first confidence estimation method for a
semi-supervised setting, when most training labels are unavailable. We
stipulate that even with limited training labels, we can still reasonably
approximate the confidence of model on unlabeled samples by inspecting the
prediction consistency through the training process. We use training
consistency as a surrogate function and propose a consistency ranking loss for
confidence estimation. On both image classification and segmentation tasks, our
method achieves state-of-the-art performances in confidence estimation.
Furthermore, we show the benefit of the proposed method through a downstream
active learning task. The code is available at
https://github.com/TopoXLab/consistency-ranking-loss]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xiaoling Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chao Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dense Sample Deep Learning. (arXiv:2307.10991v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2307.10991</id>
        <link href="http://arxiv.org/abs/2307.10991"/>
        <updated>2023-07-22T00:55:25.766Z</updated>
        <summary type="html"><![CDATA[Deep Learning (DL) , a variant of the neural network algorithms originally
proposed in the 1980s, has made surprising progress in Artificial Intelligence
(AI), ranging from language translation, protein folding, autonomous cars, and
more recently human-like language models (CHATbots), all that seemed
intractable until very recently. Despite the growing use of Deep Learning (DL)
networks, little is actually understood about the learning mechanisms and
representations that makes these networks effective across such a diverse range
of applications. Part of the answer must be the huge scale of the architecture
and of course the large scale of the data, since not much has changed since
1987. But the nature of deep learned representations remain largely unknown.
Unfortunately training sets with millions or billions of tokens have unknown
combinatorics and Networks with millions or billions of hidden units cannot
easily be visualized and their mechanisms cannot be easily revealed. In this
paper, we explore these questions with a large (1.24M weights; VGG) DL in a
novel high density sample task (5 unique tokens with at minimum 500 exemplars
per token) which allows us to more carefully follow the emergence of category
structure and feature construction. We use various visualization methods for
following the emergence of the classification and the development of the
coupling of feature detectors and structures that provide a type of graphical
bootstrapping, From these results we harvest some basic observations of the
learning dynamics of DL and propose a new theory of complex feature
construction based on our results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hanson_S/0/1/0/all/0/1"&gt;Stephen Jos&amp;#xe8; Hanson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadev_V/0/1/0/all/0/1"&gt;Vivek Yadev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanson_C/0/1/0/all/0/1"&gt;Catherine Hanson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Private Federated Learning with Autotuned Compression. (arXiv:2307.10999v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10999</id>
        <link href="http://arxiv.org/abs/2307.10999"/>
        <updated>2023-07-22T00:55:25.766Z</updated>
        <summary type="html"><![CDATA[We propose new techniques for reducing communication in private federated
learning without the need for setting or tuning compression rates. Our
on-the-fly methods automatically adjust the compression rate based on the error
induced during training, while maintaining provable privacy guarantees through
the use of secure aggregation and differential privacy. Our techniques are
provably instance-optimal for mean estimation, meaning that they can adapt to
the ``hardness of the problem" with minimal interactivity. We demonstrate the
effectiveness of our approach on real-world datasets by achieving favorable
compression rates without the need for tuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ullah_E/0/1/0/all/0/1"&gt;Enayat Ullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choquette_Choo_C/0/1/0/all/0/1"&gt;Christopher A. Choquette-Choo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kairouz_P/0/1/0/all/0/1"&gt;Peter Kairouz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1"&gt;Sewoong Oh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Selection for Generic Contextual Bandits. (arXiv:2107.03455v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03455</id>
        <link href="http://arxiv.org/abs/2107.03455"/>
        <updated>2023-07-22T00:55:25.766Z</updated>
        <summary type="html"><![CDATA[We consider the problem of model selection for the general stochastic
contextual bandits under the realizability assumption. We propose a successive
refinement based algorithm called Adaptive Contextual Bandit ({\ttfamily ACB}),
that works in phases and successively eliminates model classes that are too
simple to fit the given instance. We prove that this algorithm is adaptive,
i.e., the regret rate order-wise matches that of any provable contextual bandit
algorithm (ex. \cite{falcon}), that needs the knowledge of the true model
class. The price of not knowing the correct model class turns out to be only an
additive term contributing to the second order term in the regret bound. This
cost possess the intuitive property that it becomes smaller as the model class
becomes easier to identify, and vice-versa. We also show that a much simpler
explore-then-commit (ETC) style algorithm also obtains similar regret bound,
despite not knowing the true model class. However, the cost of model selection
is higher in ETC as opposed to in {\ttfamily ACB}, as expected. Furthermore,
for the special case of linear contextual bandits, we propose specialized
algorithms that obtain sharper guarantees compared to the generic setup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ghosh_A/0/1/0/all/0/1"&gt;Avishek Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sankararaman_A/0/1/0/all/0/1"&gt;Abishek Sankararaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramchandran_K/0/1/0/all/0/1"&gt;Kannan Ramchandran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instance-Dependent Near-Optimal Policy Identification in Linear MDPs via Online Experiment Design. (arXiv:2207.02575v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2207.02575</id>
        <link href="http://arxiv.org/abs/2207.02575"/>
        <updated>2023-07-22T00:55:25.765Z</updated>
        <summary type="html"><![CDATA[While much progress has been made in understanding the minimax sample
complexity of reinforcement learning (RL) -- the complexity of learning on the
"worst-case" instance -- such measures of complexity often do not capture the
true difficulty of learning. In practice, on an "easy" instance, we might hope
to achieve a complexity far better than that achievable on the worst-case
instance. In this work we seek to understand the "instance-dependent"
complexity of learning near-optimal policies (PAC RL) in the setting of RL with
linear function approximation. We propose an algorithm, \textsc{Pedel}, which
achieves a fine-grained instance-dependent measure of complexity, the first of
its kind in the RL with function approximation setting, thereby capturing the
difficulty of learning on each particular problem instance. Through an explicit
example, we show that \textsc{Pedel} yields provable gains over low-regret,
minimax-optimal algorithms and that such algorithms are unable to hit the
instance-optimal rate. Our approach relies on a novel online experiment
design-based procedure which focuses the exploration budget on the "directions"
most relevant to learning a near-optimal policy, and may be of independent
interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wagenmaker_A/0/1/0/all/0/1"&gt;Andrew Wagenmaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1"&gt;Kevin Jamieson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gaussian Process Priors for Systems of Linear Partial Differential Equations with Constant Coefficients. (arXiv:2212.14319v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.14319</id>
        <link href="http://arxiv.org/abs/2212.14319"/>
        <updated>2023-07-22T00:55:25.765Z</updated>
        <summary type="html"><![CDATA[Partial differential equations (PDEs) are important tools to model physical
systems and including them into machine learning models is an important way of
incorporating physical knowledge. Given any system of linear PDEs with constant
coefficients, we propose a family of Gaussian process (GP) priors, which we
call EPGP, such that all realizations are exact solutions of this system. We
apply the Ehrenpreis-Palamodov fundamental principle, which works as a
non-linear Fourier transform, to construct GP kernels mirroring standard
spectral methods for GPs. Our approach can infer probable solutions of linear
PDE systems from any data such as noisy measurements, or pointwise defined
initial and boundary conditions. Constructing EPGP-priors is algorithmic,
generally applicable, and comes with a sparse version (S-EPGP) that learns the
relevant spectral frequencies and works better for big data sets. We
demonstrate our approach on three families of systems of PDEs, the heat
equation, wave equation, and Maxwell's equations, where we improve upon the
state of the art in computation time and precision, in some experiments by
several orders of magnitude.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Harkonen_M/0/1/0/all/0/1"&gt;Marc H&amp;#xe4;rk&amp;#xf6;nen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lange_Hegermann_M/0/1/0/all/0/1"&gt;Markus Lange-Hegermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Raita_B/0/1/0/all/0/1"&gt;Bogdan Rai&amp;#x163;&amp;#x103;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correcting Underrepresentation and Intersectional Bias for Fair Classification. (arXiv:2306.11112v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.11112</id>
        <link href="http://arxiv.org/abs/2306.11112"/>
        <updated>2023-07-22T00:55:25.765Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning from data corrupted by
underrepresentation bias, where positive examples are filtered from the data at
different, unknown rates for a fixed number of sensitive groups. We show that
with a small amount of unbiased data, we can efficiently estimate the
group-wise drop-out parameters, even in settings where intersectional group
membership makes learning each intersectional rate computationally infeasible.
Using this estimate for the group-wise drop-out rate, we construct a
re-weighting scheme that allows us to approximate the loss of any hypothesis on
the true distribution, even if we only observe the empirical error on a biased
sample. Finally, we present an algorithm encapsulating this learning and
re-weighting process, and we provide strong PAC-style guarantees that, with
high probability, our estimate of the risk of the hypothesis over the true
distribution will be arbitrarily close to the true risk.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tolbert_A/0/1/0/all/0/1"&gt;Alexander Williams Tolbert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diana_E/0/1/0/all/0/1"&gt;Emily Diana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Eliminating Label Leakage in Tree-Based Vertical Federated Learning. (arXiv:2307.10318v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10318</id>
        <link href="http://arxiv.org/abs/2307.10318"/>
        <updated>2023-07-22T00:55:25.764Z</updated>
        <summary type="html"><![CDATA[Vertical federated learning (VFL) enables multiple parties with disjoint
features of a common user set to train a machine learning model without sharing
their private data. Tree-based models have become prevalent in VFL due to their
interpretability and efficiency. However, the vulnerability of tree-based VFL
has not been sufficiently investigated. In this study, we first introduce a
novel label inference attack, ID2Graph, which utilizes the sets of record-IDs
assigned to each node (i.e., instance space) to deduce private training labels.
The ID2Graph attack generates a graph structure from training samples, extracts
communities from the graph, and clusters the local dataset using community
information. To counteract label leakage from the instance space, we propose an
effective defense mechanism, ID-LMID, which prevents label leakage by focusing
on mutual information regularization. Comprehensive experiments conducted on
various datasets reveal that the ID2Graph attack presents significant risks to
tree-based models such as Random Forest and XGBoost. Further evaluations on
these benchmarks demonstrate that ID-LMID effectively mitigates label leakage
in such instances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Takahashi_H/0/1/0/all/0/1"&gt;Hideaki Takahashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingjing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Curiosity in Hindsight: Intrinsic Exploration in Stochastic Environments. (arXiv:2211.10515v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.10515</id>
        <link href="http://arxiv.org/abs/2211.10515"/>
        <updated>2023-07-22T00:55:25.764Z</updated>
        <summary type="html"><![CDATA[Consider the problem of exploration in sparse-reward or reward-free
environments, such as in Montezuma's Revenge. In the curiosity-driven paradigm,
the agent is rewarded for how much each realized outcome differs from their
predicted outcome. But using predictive error as intrinsic motivation is
fragile in stochastic environments, as the agent may become trapped by
high-entropy areas of the state-action space, such as a "noisy TV". In this
work, we study a natural solution derived from structural causal models of the
world: Our key idea is to learn representations of the future that capture
precisely the unpredictable aspects of each outcome -- which we use as
additional input for predictions, such that intrinsic rewards only reflect the
predictable aspects of world dynamics. First, we propose incorporating such
hindsight representations into models to disentangle "noise" from "novelty",
yielding Curiosity in Hindsight: a simple and scalable generalization of
curiosity that is robust to stochasticity. Second, we instantiate this
framework for the recently introduced BYOL-Explore algorithm as our prime
example, resulting in the noise-robust BYOL-Hindsight. Third, we illustrate its
behavior under a variety of different stochasticities in a grid world, and find
improvements over BYOL-Explore in hard-exploration Atari games with sticky
actions. Notably, we show state-of-the-art results in exploring Montezuma's
Revenge with sticky actions, while preserving performance in the non-sticky
setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jarrett_D/0/1/0/all/0/1"&gt;Daniel Jarrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tallec_C/0/1/0/all/0/1"&gt;Corentin Tallec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Altche_F/0/1/0/all/0/1"&gt;Florent Altch&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mesnard_T/0/1/0/all/0/1"&gt;Thomas Mesnard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Munos_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Munos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Valko_M/0/1/0/all/0/1"&gt;Michal Valko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-view self-supervised learning for multivariate variable-channel time series. (arXiv:2307.09614v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.09614</id>
        <link href="http://arxiv.org/abs/2307.09614"/>
        <updated>2023-07-22T00:55:25.764Z</updated>
        <summary type="html"><![CDATA[Labeling of multivariate biomedical time series data is a laborious and
expensive process. Self-supervised contrastive learning alleviates the need for
large, labeled datasets through pretraining on unlabeled data. However, for
multivariate time series data, the set of input channels often varies between
applications, and most existing work does not allow for transfer between
datasets with different sets of input channels. We propose learning one encoder
to operate on all input channels individually. We then use a message passing
neural network to extract a single representation across channels. We
demonstrate the potential of this method by pretraining our model on a dataset
with six EEG channels and then fine-tuning it on a dataset with two different
EEG channels. We compare models with and without the message passing neural
network across different contrastive loss functions. We show that our method,
combined with the TS2Vec loss, outperforms all other methods in most settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Brusch_T/0/1/0/all/0/1"&gt;Thea Br&amp;#xfc;sch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1"&gt;Mikkel N. Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Alstrom_T/0/1/0/all/0/1"&gt;Tommy S. Alstr&amp;#xf8;m&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Action Model Acquisition from Narrative Texts. (arXiv:2307.10247v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.10247</id>
        <link href="http://arxiv.org/abs/2307.10247"/>
        <updated>2023-07-22T00:55:25.763Z</updated>
        <summary type="html"><![CDATA[Action models, which take the form of precondition/effect axioms, facilitate
causal and motivational connections between actions for AI agents. Action model
acquisition has been identified as a bottleneck in the application of planning
technology, especially within narrative planning. Acquiring action models from
narrative texts in an automated way is essential, but challenging because of
the inherent complexities of such texts. We present NaRuto, a system that
extracts structured events from narrative text and subsequently generates
planning-language-style action models based on predictions of commonsense event
relations, as well as textual contradictions and similarities, in an
unsupervised manner. Experimental results in classical narrative planning
domains show that NaRuto can generate action models of significantly better
quality than existing fully automated methods, and even on par with those of
semi-automated methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruiqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1"&gt;Leyang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Songtuan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haslum_P/0/1/0/all/0/1"&gt;Patrik Haslum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Unreasonable Effectiveness of Deep Evidential Regression. (arXiv:2205.10060v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2205.10060</id>
        <link href="http://arxiv.org/abs/2205.10060"/>
        <updated>2023-07-22T00:55:25.763Z</updated>
        <summary type="html"><![CDATA[There is a significant need for principled uncertainty reasoning in machine
learning systems as they are increasingly deployed in safety-critical domains.
A new approach with uncertainty-aware regression-based neural networks (NNs),
based on learning evidential distributions for aleatoric and epistemic
uncertainties, shows promise over traditional deterministic methods and typical
Bayesian NNs, notably with the capabilities to disentangle aleatoric and
epistemic uncertainties. Despite some empirical success of Deep Evidential
Regression (DER), there are important gaps in the mathematical foundation that
raise the question of why the proposed technique seemingly works. We detail the
theoretical shortcomings and analyze the performance on synthetic and
real-world data sets, showing that Deep Evidential Regression is a heuristic
rather than an exact uncertainty quantification. We go on to discuss
corrections and redefinitions of how aleatoric and epistemic uncertainties
should be extracted from NNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meinert_N/0/1/0/all/0/1"&gt;Nis Meinert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gawlikowski_J/0/1/0/all/0/1"&gt;Jakob Gawlikowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lavin_A/0/1/0/all/0/1"&gt;Alexander Lavin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-trained Perceptual Features Improve Differentially Private Image Generation. (arXiv:2205.12900v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2205.12900</id>
        <link href="http://arxiv.org/abs/2205.12900"/>
        <updated>2023-07-22T00:55:25.763Z</updated>
        <summary type="html"><![CDATA[Training even moderately-sized generative models with differentially-private
stochastic gradient descent (DP-SGD) is difficult: the required level of noise
for reasonable levels of privacy is simply too large. We advocate instead
building off a good, relevant representation on an informative public dataset,
then learning to model the private data with that representation. In
particular, we minimize the maximum mean discrepancy (MMD) between private
target data and a generator's distribution, using a kernel based on perceptual
features learned from a public dataset. With the MMD, we can simply privatize
the data-dependent term once and for all, rather than introducing noise at each
step of optimization as in DP-SGD. Our algorithm allows us to generate
CIFAR10-level images with $\epsilon \approx 2$ which capture distinctive
features in the distribution, far surpassing the current state of the art,
which mostly focuses on datasets such as MNIST and FashionMNIST at a large
$\epsilon \approx 10$. Our work introduces simple yet powerful foundations for
reducing the gap between private and non-private deep generative models. Our
code is available at \url{https://github.com/ParkLabML/DP-MEPF}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Harder_F/0/1/0/all/0/1"&gt;Fredrik Harder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Asadabadi_M/0/1/0/all/0/1"&gt;Milad Jalali Asadabadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sutherland_D/0/1/0/all/0/1"&gt;Danica J. Sutherland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Park_M/0/1/0/all/0/1"&gt;Mijung Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Knowledge Modeling for Cancer Clinical Practice Guidelines. (arXiv:2307.10231v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2307.10231</id>
        <link href="http://arxiv.org/abs/2307.10231"/>
        <updated>2023-07-22T00:55:25.762Z</updated>
        <summary type="html"><![CDATA[Clinical Practice Guidelines (CPGs) for cancer diseases evolve rapidly due to
new evidence generated by active research. Currently, CPGs are primarily
published in a document format that is ill-suited for managing this developing
knowledge. A knowledge model of the guidelines document suitable for
programmatic interaction is required. This work proposes an automated method
for extraction of knowledge from National Comprehensive Cancer Network (NCCN)
CPGs in Oncology and generating a structured model containing the retrieved
knowledge. The proposed method was tested using two versions of NCCN Non-Small
Cell Lung Cancer (NSCLC) CPG to demonstrate the effectiveness in faithful
extraction and modeling of knowledge. Three enrichment strategies using Cancer
staging information, Unified Medical Language System (UMLS) Metathesaurus &
National Cancer Institute thesaurus (NCIt) concepts, and Node classification
are also presented to enhance the model towards enabling programmatic traversal
and querying of cancer care guidelines. The Node classification was performed
using a Support Vector Machine (SVM) model, achieving a classification accuracy
of 0.81 with 10-fold cross-validation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ta_P/0/1/0/all/0/1"&gt;Pralaypati Ta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_B/0/1/0/all/0/1"&gt;Bhumika Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Arihant Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+C_S/0/1/0/all/0/1"&gt;Sneha Sree C&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1"&gt;Arunima Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ram_K/0/1/0/all/0/1"&gt;Keerthi Ram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivaprakasam_M/0/1/0/all/0/1"&gt;Mohanasankar Sivaprakasam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing sports commentary in order to automatically recognize events and extract insights. (arXiv:2307.10303v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.10303</id>
        <link href="http://arxiv.org/abs/2307.10303"/>
        <updated>2023-07-22T00:55:25.762Z</updated>
        <summary type="html"><![CDATA[In this paper, we carefully investigate how we can use multiple different
Natural Language Processing techniques and methods in order to automatically
recognize the main actions in sports events. We aim to extract insights by
analyzing live sport commentaries from different sources and by classifying
these major actions into different categories. We also study if sentiment
analysis could help detect these main actions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miraoui_Y/0/1/0/all/0/1"&gt;Yanis Miraoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mitigating Voter Attribute Bias for Fair Opinion Aggregation. (arXiv:2307.10749v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2307.10749</id>
        <link href="http://arxiv.org/abs/2307.10749"/>
        <updated>2023-07-22T00:55:25.761Z</updated>
        <summary type="html"><![CDATA[The aggregation of multiple opinions plays a crucial role in decision-making,
such as in hiring and loan review, and in labeling data for supervised
learning. Although majority voting and existing opinion aggregation models are
effective for simple tasks, they are inappropriate for tasks without
objectively true labels in which disagreements may occur. In particular, when
voter attributes such as gender or race introduce bias into opinions, the
aggregation results may vary depending on the composition of voter attributes.
A balanced group of voters is desirable for fair aggregation results but may be
difficult to prepare. In this study, we consider methods to achieve fair
opinion aggregation based on voter attributes and evaluate the fairness of the
aggregated results. To this end, we consider an approach that combines opinion
aggregation models such as majority voting and the Dawid and Skene model (D&S
model) with fairness options such as sample weighting. To evaluate the fairness
of opinion aggregation, probabilistic soft labels are preferred over discrete
class labels. First, we address the problem of soft label estimation without
considering voter attributes and identify some issues with the D&S model. To
address these limitations, we propose a new Soft D&S model with improved
accuracy in estimating soft labels. Moreover, we evaluated the fairness of an
opinion aggregation model, including Soft D&S, in combination with different
fairness options using synthetic and semi-synthetic data. The experimental
results suggest that the combination of Soft D&S and data splitting as a
fairness option is effective for dense data, whereas weighted majority voting
is effective for sparse data. These findings should prove particularly valuable
in supporting decision-making by human and machine-learning models with
balanced opinion aggregation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ueda_R/0/1/0/all/0/1"&gt;Ryosuke Ueda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takeuchi_K/0/1/0/all/0/1"&gt;Koh Takeuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1"&gt;Hisashi Kashima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization. (arXiv:2307.11007v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.11007</id>
        <link href="http://arxiv.org/abs/2307.11007"/>
        <updated>2023-07-22T00:55:25.761Z</updated>
        <summary type="html"><![CDATA[Despite extensive studies, the underlying reason as to why overparameterized
neural networks can generalize remains elusive. Existing theory shows that
common stochastic optimizers prefer flatter minimizers of the training loss,
and thus a natural potential explanation is that flatness implies
generalization. This work critically examines this explanation. Through
theoretical and empirical investigation, we identify the following three
scenarios for two-layer ReLU networks: (1) flatness provably implies
generalization; (2) there exist non-generalizing flattest models and sharpness
minimization algorithms fail to generalize, and (3) perhaps most surprisingly,
there exist non-generalizing flattest models, but sharpness minimization
algorithms still generalize. Our results suggest that the relationship between
sharpness and generalization subtly depends on the data distributions and the
model architectures and sharpness minimization algorithms do not only minimize
sharpness to achieve better generalization. This calls for the search for other
explanations for the generalization of over-parameterized neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wen_K/0/1/0/all/0/1"&gt;Kaiyue Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhiyuan Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Machine Learning Methods Applied to Structural Dynamics and Vibroacoustic. (arXiv:2204.06362v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2204.06362</id>
        <link href="http://arxiv.org/abs/2204.06362"/>
        <updated>2023-07-22T00:55:25.760Z</updated>
        <summary type="html"><![CDATA[The use of Machine Learning (ML) has rapidly spread across several fields,
having encountered many applications in Structural Dynamics and Vibroacoustic
(SD\&V). The increasing capabilities of ML to unveil insights from data, driven
by unprecedented data availability, algorithms advances and computational
power, enhance decision making, uncertainty handling, patterns recognition and
real-time assessments. Three main applications in SD\&V have taken advantage of
these benefits. In Structural Health Monitoring, ML detection and prognosis
lead to safe operation and optimized maintenance schedules. System
identification and control design are leveraged by ML techniques in Active
Noise Control and Active Vibration Control. Finally, the so-called ML-based
surrogate models provide fast alternatives to costly simulations, enabling
robust and optimized product design. Despite the many works in the area, they
have not been reviewed and analyzed. Therefore, to keep track and understand
this ongoing integration of fields, this paper presents a survey of ML
applications in SD\&V analyses, shedding light on the current state of
implementation and emerging opportunities. The main methodologies, advantages,
limitations, and recommendations based on scientific knowledge were identified
for each of the three applications. Moreover, the paper considers the role of
Digital Twins and Physics Guided ML to overcome current challenges and power
future research progress. As a result, the survey provides a broad overview of
the present landscape of ML applied in SD\&V and guides the reader to an
advanced understanding of progress and prospects in the field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cunha_B/0/1/0/all/0/1"&gt;Barbara Cunha&lt;/a&gt; (LTDS), &lt;a href="http://arxiv.org/find/cs/1/au:+Droz_C/0/1/0/all/0/1"&gt;Christophe Droz&lt;/a&gt; (I4S), &lt;a href="http://arxiv.org/find/cs/1/au:+Zine_A/0/1/0/all/0/1"&gt;Abdelmalek Zine&lt;/a&gt; (ICJ), &lt;a href="http://arxiv.org/find/cs/1/au:+Foulard_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Foulard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ichchou_M/0/1/0/all/0/1"&gt;Mohamed Ichchou&lt;/a&gt; (LTDS)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Novel Batch Active Learning Approach and Its Application to Synthetic Aperture Radar Datasets. (arXiv:2307.10495v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10495</id>
        <link href="http://arxiv.org/abs/2307.10495"/>
        <updated>2023-07-22T00:55:25.751Z</updated>
        <summary type="html"><![CDATA[Active learning improves the performance of machine learning methods by
judiciously selecting a limited number of unlabeled data points to query for
labels, with the aim of maximally improving the underlying classifier's
performance. Recent gains have been made using sequential active learning for
synthetic aperture radar (SAR) data arXiv:2204.00005. In each iteration,
sequential active learning selects a query set of size one while batch active
learning selects a query set of multiple datapoints. While batch active
learning methods exhibit greater efficiency, the challenge lies in maintaining
model accuracy relative to sequential active learning methods. We developed a
novel, two-part approach for batch active learning: Dijkstra's Annulus Core-Set
(DAC) for core-set generation and LocalMax for batch sampling. The batch active
learning process that combines DAC and LocalMax achieves nearly identical
accuracy as sequential active learning but is more efficient, proportional to
the batch size. As an application, a pipeline is built based on transfer
learning feature embedding, graph learning, DAC, and LocalMax to classify the
FUSAR-Ship and OpenSARShip datasets. Our pipeline outperforms the
state-of-the-art CNN-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chapman_J/0/1/0/all/0/1"&gt;James Chapman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bohan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1"&gt;Zheng Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calder_J/0/1/0/all/0/1"&gt;Jeff Calder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_K/0/1/0/all/0/1"&gt;Kevin Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertozzi_A/0/1/0/all/0/1"&gt;Andrea L. Bertozzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Polyffusion: A Diffusion Model for Polyphonic Score Generation with Internal and External Controls. (arXiv:2307.10304v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2307.10304</id>
        <link href="http://arxiv.org/abs/2307.10304"/>
        <updated>2023-07-22T00:55:25.750Z</updated>
        <summary type="html"><![CDATA[We propose Polyffusion, a diffusion model that generates polyphonic music
scores by regarding music as image-like piano roll representations. The model
is capable of controllable music generation with two paradigms: internal
control and external control. Internal control refers to the process in which
users pre-define a part of the music and then let the model infill the rest,
similar to the task of masked music generation (or music inpainting). External
control conditions the model with external yet related information, such as
chord, texture, or other features, via the cross-attention mechanism. We show
that by using internal and external controls, Polyffusion unifies a wide range
of music creation tasks, including melody generation given accompaniment,
accompaniment generation given melody, arbitrary music segment inpainting, and
music arrangement given chords or textures. Experimental results show that our
model significantly outperforms existing Transformer and sampling-based
baselines, and using pre-trained disentangled representations as external
conditions yields more effective controls.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Min_L/0/1/0/all/0/1"&gt;Lejun Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Junyan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1"&gt;Gus Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jingwei Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reproducibility in Machine Learning-Driven Research. (arXiv:2307.10320v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10320</id>
        <link href="http://arxiv.org/abs/2307.10320"/>
        <updated>2023-07-22T00:55:25.750Z</updated>
        <summary type="html"><![CDATA[Research is facing a reproducibility crisis, in which the results and
findings of many studies are difficult or even impossible to reproduce. This is
also the case in machine learning (ML) and artificial intelligence (AI)
research. Often, this is the case due to unpublished data and/or source-code,
and due to sensitivity to ML training conditions. Although different solutions
to address this issue are discussed in the research community such as using ML
platforms, the level of reproducibility in ML-driven research is not increasing
substantially. Therefore, in this mini survey, we review the literature on
reproducibility in ML-driven research with three main aims: (i) reflect on the
current situation of ML reproducibility in various research fields, (ii)
identify reproducibility issues and barriers that exist in these research
fields applying ML, and (iii) identify potential drivers such as tools,
practices, and interventions that support ML reproducibility. With this, we
hope to contribute to decisions on the viability of different solutions for
supporting ML reproducibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Semmelrock_H/0/1/0/all/0/1"&gt;Harald Semmelrock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kopeinik_S/0/1/0/all/0/1"&gt;Simone Kopeinik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theiler_D/0/1/0/all/0/1"&gt;Dieter Theiler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ross_Hellauer_T/0/1/0/all/0/1"&gt;Tony Ross-Hellauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowald_D/0/1/0/all/0/1"&gt;Dominik Kowald&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit Multidimensional Projection of Local Subspaces. (arXiv:2009.03259v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.03259</id>
        <link href="http://arxiv.org/abs/2009.03259"/>
        <updated>2023-07-22T00:55:25.749Z</updated>
        <summary type="html"><![CDATA[We propose a visualization method to understand the effect of
multidimensional projection on local subspaces, using implicit function
differentiation. Here, we understand the local subspace as the multidimensional
local neighborhood of data points. Existing methods focus on the projection of
multidimensional data points, and the neighborhood information is ignored. Our
method is able to analyze the shape and directional information of the local
subspace to gain more insights into the global structure of the data through
the perception of local structures. Local subspaces are fitted by
multidimensional ellipses that are spanned by basis vectors. An accurate and
efficient vector transformation method is proposed based on analytical
differentiation of multidimensional projections formulated as implicit
functions. The results are visualized as glyphs and analyzed using a full set
of specifically-designed interactions supported in our efficient web-based
visualization tool. The usefulness of our method is demonstrated using various
multi- and high-dimensional benchmark datasets. Our implicit differentiation
vector transformation is evaluated through numerical comparisons; the overall
method is evaluated through exploration examples and use cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bian_R/0/1/0/all/0/1"&gt;Rongzheng Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1"&gt;Yumeng Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Liang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Baoquan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiskopf_D/0/1/0/all/0/1"&gt;Daniel Weiskopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhai Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Predictive Two-Sample and Independence Testing. (arXiv:2305.00143v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.00143</id>
        <link href="http://arxiv.org/abs/2305.00143"/>
        <updated>2023-07-22T00:55:25.749Z</updated>
        <summary type="html"><![CDATA[We study the problems of sequential nonparametric two-sample and independence
testing. Sequential tests process data online and allow using observed data to
decide whether to stop and reject the null hypothesis or to collect more data,
while maintaining type I error control. We build upon the principle of
(nonparametric) testing by betting, where a gambler places bets on future
observations and their wealth measures evidence against the null hypothesis.
While recently developed kernel-based betting strategies often work well on
simple distributions, selecting a suitable kernel for high-dimensional or
structured data, such as images, is often nontrivial. To address this drawback,
we design prediction-based betting strategies that rely on the following fact:
if a sequentially updated predictor starts to consistently determine (a) which
distribution an instance is drawn from, or (b) whether an instance is drawn
from the joint distribution or the product of the marginal distributions (the
latter produced by external randomization), it provides evidence against the
two-sample or independence nulls respectively. We empirically demonstrate the
superiority of our tests over kernel-based approaches under structured
settings. Our tests can be applied beyond the case of independent and
identically distributed data, remaining valid and powerful even when the data
distribution drifts over time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Podkopaev_A/0/1/0/all/0/1"&gt;Aleksandr Podkopaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1"&gt;Aaditya Ramdas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Performance Issue Identification in Cloud Systems with Relational-Temporal Anomaly Detection. (arXiv:2307.10869v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10869</id>
        <link href="http://arxiv.org/abs/2307.10869"/>
        <updated>2023-07-22T00:55:25.713Z</updated>
        <summary type="html"><![CDATA[Performance issues permeate large-scale cloud service systems, which can lead
to huge revenue losses. To ensure reliable performance, it's essential to
accurately identify and localize these issues using service monitoring metrics.
Given the complexity and scale of modern cloud systems, this task can be
challenging and may require extensive expertise and resources beyond the
capacity of individual humans. Some existing methods tackle this problem by
analyzing each metric independently to detect anomalies. However, this could
incur overwhelming alert storms that are difficult for engineers to diagnose
manually. To pursue better performance, not only the temporal patterns of
metrics but also the correlation between metrics (i.e., relational patterns)
should be considered, which can be formulated as a multivariate metrics anomaly
detection problem. However, most of the studies fall short of extracting these
two types of features explicitly. Moreover, there exist some unlabeled
anomalies mixed in the training data, which may hinder the detection
performance. To address these limitations, we propose the Relational- Temporal
Anomaly Detection Model (RTAnomaly) that combines the relational and temporal
information of metrics. RTAnomaly employs a graph attention layer to learn the
dependencies among metrics, which will further help pinpoint the anomalous
metrics that may cause the anomaly effectively. In addition, we exploit the
concept of positive unlabeled learning to address the issue of potential
anomalies in the training data. To evaluate our method, we conduct experiments
on a public dataset and two industrial datasets. RTAnomaly outperforms all the
baseline models by achieving an average F1 score of 0.929 and Hit@3 of 0.920,
demonstrating its superiority.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1"&gt;Wenwei Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jinyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhuangbin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1"&gt;Yuxin Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jiazhen Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1"&gt;Cong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zengyin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1"&gt;Michael Lyu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-Efficient Augmentation for Training Neural Networks. (arXiv:2210.08363v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.08363</id>
        <link href="http://arxiv.org/abs/2210.08363"/>
        <updated>2023-07-22T00:55:25.705Z</updated>
        <summary type="html"><![CDATA[Data augmentation is essential to achieve state-of-the-art performance in
many deep learning applications. However, the most effective augmentation
techniques become computationally prohibitive for even medium-sized datasets.
To address this, we propose a rigorous technique to select subsets of data
points that when augmented, closely capture the training dynamics of full data
augmentation. We first show that data augmentation, modeled as additive
perturbations, improves learning and generalization by relatively enlarging and
perturbing the smaller singular values of the network Jacobian, while
preserving its prominent directions. This prevents overfitting and enhances
learning the harder to learn information. Then, we propose a framework to
iteratively extract small subsets of training data that when augmented, closely
capture the alignment of the fully augmented Jacobian with labels/residuals. We
prove that stochastic gradient descent applied to the augmented subsets found
by our approach has similar training dynamics to that of fully augmented data.
Our experiments demonstrate that our method achieves 6.3x speedup on CIFAR10
and 2.2x speedup on SVHN, and outperforms the baselines by up to 10% across
various subset sizes. Similarly, on TinyImageNet and ImageNet, our method beats
the baselines by up to 8%, while achieving up to 3.3x speedup across various
subset sizes. Finally, training on and augmenting 50% subsets using our method
on a version of CIFAR10 corrupted with label noise even outperforms using the
full dataset. Our code is available at:
https://github.com/tianyu139/data-efficient-augmentation]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tian Yu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirzasoleiman_B/0/1/0/all/0/1"&gt;Baharan Mirzasoleiman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation. (arXiv:2305.11408v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.11408</id>
        <link href="http://arxiv.org/abs/2305.11408"/>
        <updated>2023-07-22T00:55:25.697Z</updated>
        <summary type="html"><![CDATA[Attention is the core mechanism of today's most used architectures for
natural language processing and has been analyzed from many perspectives,
including its effectiveness for machine translation-related tasks. Among these
studies, attention resulted to be a useful source of information to get
insights about word alignment also when the input text is substituted with
audio segments, as in the case of the speech translation (ST) task. In this
paper, we propose AlignAtt, a novel policy for simultaneous ST (SimulST) that
exploits the attention information to generate source-target alignments that
guide the model during inference. Through experiments on the 8 language pairs
of MuST-C v1.0, we show that AlignAtt outperforms previous state-of-the-art
SimulST policies applied to offline-trained models with gains in terms of BLEU
of 2 points and latency reductions ranging from 0.5s to 0.8s across the 8
languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1"&gt;Sara Papi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1"&gt;Marco Turchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1"&gt;Matteo Negri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Kernel Embedding for Mediated and Time-Varying Dose Response Curves. (arXiv:2111.03950v4 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.03950</id>
        <link href="http://arxiv.org/abs/2111.03950"/>
        <updated>2023-07-22T00:55:25.689Z</updated>
        <summary type="html"><![CDATA[We propose simple nonparametric estimators for mediated and time-varying dose
response curves based on kernel ridge regression. By embedding Pearl's
mediation formula and Robins' g-formula with kernels, we allow treatments,
mediators, and covariates to be continuous in general spaces, and also allow
for nonlinear treatment-confounder feedback. Our key innovation is a
reproducing kernel Hilbert space technique called sequential kernel embedding,
which we use to construct simple estimators for complex causal estimands. Our
estimators preserve the generality of classic identification while also
achieving nonasymptotic uniform rates. In nonlinear simulations with many
covariates, we demonstrate strong performance. We estimate mediated and
time-varying dose response curves of the US Job Corps, and clean data that may
serve as a benchmark in future work. We extend our results to mediated and
time-varying treatment effects and counterfactual distributions, verifying
semiparametric efficiency and weak convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rahul Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xu_L/0/1/0/all/0/1"&gt;Liyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1"&gt;Arthur Gretton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Principal Component Analysis: A Median of Means Approach. (arXiv:2102.03403v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03403</id>
        <link href="http://arxiv.org/abs/2102.03403"/>
        <updated>2023-07-22T00:55:25.639Z</updated>
        <summary type="html"><![CDATA[Principal Component Analysis (PCA) is a fundamental tool for data
visualization, denoising, and dimensionality reduction. It is widely popular in
Statistics, Machine Learning, Computer Vision, and related fields. However, PCA
is well-known to fall prey to outliers and often fails to detect the true
underlying low-dimensional structure within the dataset. Following the Median
of Means (MoM) philosophy, recent supervised learning methods have shown great
success in dealing with outlying observations without much compromise to their
large sample theoretical properties. This paper proposes a PCA procedure based
on the MoM principle. Called the \textbf{M}edian of \textbf{M}eans
\textbf{P}rincipal \textbf{C}omponent \textbf{A}nalysis (MoMPCA), the proposed
method is not only computationally appealing but also achieves optimal
convergence rates under minimal assumptions. In particular, we explore the
non-asymptotic error bounds of the obtained solution via the aid of the
Rademacher complexities while granting absolutely no assumption on the outlying
observations. The derived concentration results are not dependent on the
dimension because the analysis is conducted in a separable Hilbert space, and
the results only depend on the fourth moment of the underlying distribution in
the corresponding norm. The proposal's efficacy is also thoroughly showcased
through simulations and real data applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Paul_D/0/1/0/all/0/1"&gt;Debolina Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chakraborty_S/0/1/0/all/0/1"&gt;Saptarshi Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Das_S/0/1/0/all/0/1"&gt;Swagatam Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy Amplification via Importance Sampling. (arXiv:2307.10187v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2307.10187</id>
        <link href="http://arxiv.org/abs/2307.10187"/>
        <updated>2023-07-22T00:55:25.627Z</updated>
        <summary type="html"><![CDATA[We examine the privacy-enhancing properties of subsampling a data set via
importance sampling as a pre-processing step for differentially private
mechanisms. This extends the established privacy amplification by subsampling
result to importance sampling where each data point is weighted by the
reciprocal of its selection probability. The implications for privacy of
weighting each point are not obvious. On the one hand, a lower selection
probability leads to a stronger privacy amplification. On the other hand, the
higher the weight, the stronger the influence of the point on the output of the
mechanism in the event that the point does get selected. We provide a general
result that quantifies the trade-off between these two effects. We show that
heterogeneous sampling probabilities can lead to both stronger privacy and
better utility than uniform subsampling while retaining the subsample size. In
particular, we formulate and solve the problem of privacy-optimal sampling,
that is, finding the importance weights that minimize the expected subset size
subject to a given privacy budget. Empirically, we evaluate the privacy,
efficiency, and accuracy of importance sampling-based privacy amplification on
the example of k-means clustering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fay_D/0/1/0/all/0/1"&gt;Dominik Fay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mair_S/0/1/0/all/0/1"&gt;Sebastian Mair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sjolund_J/0/1/0/all/0/1"&gt;Jens Sj&amp;#xf6;lund&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ProtiGeno: a prokaryotic short gene finder using protein language models. (arXiv:2307.10343v1 [q-bio.GN])]]></title>
        <id>http://arxiv.org/abs/2307.10343</id>
        <link href="http://arxiv.org/abs/2307.10343"/>
        <updated>2023-07-22T00:55:25.614Z</updated>
        <summary type="html"><![CDATA[Prokaryotic gene prediction plays an important role in understanding the
biology of organisms and their function with applications in medicine and
biotechnology. Although the current gene finders are highly sensitive in
finding long genes, their sensitivity decreases noticeably in finding shorter
genes (<180 nts). The culprit is insufficient annotated gene data to identify
distinguishing features in short open reading frames (ORFs). We develop a deep
learning-based method called ProtiGeno, specifically targeting short
prokaryotic genes using a protein language model trained on millions of evolved
proteins. In systematic large-scale experiments on 4,288 prokaryotic genomes,
we demonstrate that ProtiGeno predicts short coding and noncoding genes with
higher accuracy and recall than the current state-of-the-art gene finders. We
discuss the predictive features of ProtiGeno and possible limitations by
visualizing the three-dimensional structure of the predicted short genes. Data,
codes, and models are available at https://github.com/tonytu16/protigeno.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Tu_T/0/1/0/all/0/1"&gt;Tony Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Krishna_G/0/1/0/all/0/1"&gt;Gautham Krishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Aghazadeh_A/0/1/0/all/0/1"&gt;Amirali Aghazadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars. (arXiv:2307.10705v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.10705</id>
        <link href="http://arxiv.org/abs/2307.10705"/>
        <updated>2023-07-22T00:55:25.613Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation is a common task in autonomous driving to understand
the surrounding environment. Driveable Area Segmentation and Lane Detection are
particularly important for safe and efficient navigation on the road. However,
original semantic segmentation models are computationally expensive and require
high-end hardware, which is not feasible for embedded systems in autonomous
vehicles. This paper proposes a lightweight model for the driveable area and
lane line segmentation. TwinLiteNet is designed cheaply but achieves accurate
and efficient segmentation results. We evaluate TwinLiteNet on the BDD100K
dataset and compare it with modern models. Experimental results show that our
TwinLiteNet performs similarly to existing approaches, requiring significantly
fewer computational resources. Specifically, TwinLiteNet achieves a mIoU score
of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task
with only 0.4 million parameters and achieves 415 FPS on GPU RTX A5000.
Furthermore, TwinLiteNet can run in real-time on embedded devices with limited
computing power, especially since it achieves 60FPS on Jetson Xavier NX, making
it an ideal solution for self-driving vehicles. Code is available:
url{https://github.com/chequanghuy/TwinLiteNet}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Che_Q/0/1/0/all/0/1"&gt;Quang Huy Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Dinh Phuc Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1"&gt;Minh Quan Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_D/0/1/0/all/0/1"&gt;Duc Khai Lam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis. (arXiv:2306.13960v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.13960</id>
        <link href="http://arxiv.org/abs/2306.13960"/>
        <updated>2023-07-22T00:55:25.613Z</updated>
        <summary type="html"><![CDATA[Regular group convolutional neural networks (G-CNNs) have been shown to
increase model performance and improve equivariance to different geometrical
symmetries. This work addresses the problem of SE(3), i.e., roto-translation
equivariance, on volumetric data. Volumetric image data is prevalent in many
medical settings. Motivated by the recent work on separable group convolutions,
we devise a SE(3) group convolution kernel separated into a continuous SO(3)
(rotation) kernel and a spatial kernel. We approximate equivariance to the
continuous setting by sampling uniform SO(3) grids. Our continuous SO(3) kernel
is parameterized via RBF interpolation on similarly uniform grids. We
demonstrate the advantages of our approach in volumetric medical image
analysis. Our SE(3) equivariant models consistently outperform CNNs and regular
discrete G-CNNs on challenging medical classification tasks and show
significantly improved generalization capabilities. Our approach achieves up to
a 16.5% gain in accuracy over regular CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuipers_T/0/1/0/all/0/1"&gt;Thijs P. Kuipers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1"&gt;Erik J. Bekkers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amortized Variational Inference: When and Why?. (arXiv:2307.11018v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.11018</id>
        <link href="http://arxiv.org/abs/2307.11018"/>
        <updated>2023-07-22T00:55:25.567Z</updated>
        <summary type="html"><![CDATA[Amortized variational inference (A-VI) is a method for approximating the
intractable posterior distributions that arise in probabilistic models. The
defining feature of A-VI is that it learns a global inference function that
maps each observation to its local latent variable's approximate posterior.
This stands in contrast to the more classical factorized (or mean-field)
variational inference (F-VI), which directly learns the parameters of the
approximating distribution for each latent variable. In deep generative models,
A-VI is used as a computational trick to speed up inference for local latent
variables. In this paper, we study A-VI as a general alternative to F-VI for
approximate posterior inference. A-VI cannot produce an approximation with a
lower Kullback-Leibler divergence than F-VI's optimal solution, because the
amortized family is a subset of the factorized family. Thus a central
theoretical problem is to characterize when A-VI still attains F-VI's optimal
solution. We derive conditions on both the model and the inference function
under which A-VI can theoretically achieve F-VI's optimum. We show that for a
broad class of hierarchical models, including deep generative models, it is
possible to close the gap between A-VI and F-VI. Further, for an even broader
class of models, we establish when and how to expand the domain of the
inference function to make amortization a feasible strategy. Finally, we prove
that for certain models -- including hidden Markov models and Gaussian
processes -- A-VI cannot match F-VI's solution, no matter how expressive the
inference function is. We also study A-VI empirically. On several examples, we
corroborate our theoretical results and investigate the performance of A-VI
when varying the complexity of the inference function. When the gap between
A-VI and F-VI can be closed, we find that the required complexity of the
function need not scale with the number of observations, and that A-VI often
converges faster than F-VI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Margossian_C/0/1/0/all/0/1"&gt;Charles C. Margossian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1"&gt;David M. Blei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Efficient UCB-type Algorithms For Learning Predictive State Representations. (arXiv:2307.00405v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.00405</id>
        <link href="http://arxiv.org/abs/2307.00405"/>
        <updated>2023-07-22T00:55:25.565Z</updated>
        <summary type="html"><![CDATA[The general sequential decision-making problem, which includes Markov
decision processes (MDPs) and partially observable MDPs (POMDPs) as special
cases, aims at maximizing a cumulative reward by making a sequence of decisions
based on a history of observations and actions over time. Recent studies have
shown that the sequential decision-making problem is statistically learnable if
it admits a low-rank structure modeled by predictive state representations
(PSRs). Despite these advancements, existing approaches typically involve
oracles or steps that are not computationally efficient. On the other hand, the
upper confidence bound (UCB) based approaches, which have served successfully
as computationally efficient methods in bandits and MDPs, have not been
investigated for more general PSRs, due to the difficulty of optimistic bonus
design in these more challenging settings. This paper proposes the first known
UCB-type approach for PSRs, featuring a novel bonus term that upper bounds the
total variation distance between the estimated and true models. We further
characterize the sample complexity bounds for our designed UCB-type algorithms
for both online and offline PSRs. In contrast to existing approaches for PSRs,
our UCB-type algorithms enjoy computational efficiency, last-iterate guaranteed
near-optimal policy, and guaranteed model accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1"&gt;Ruiquan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yingbin Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jing Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions. (arXiv:2307.10644v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10644</id>
        <link href="http://arxiv.org/abs/2307.10644"/>
        <updated>2023-07-22T00:55:25.563Z</updated>
        <summary type="html"><![CDATA[Data sets of multivariate normal distributions abound in many scientific
areas like diffusion tensor imaging, structure tensor computer vision, radar
signal processing, machine learning, just to name a few. In order to process
those normal data sets for downstream tasks like filtering, classification or
clustering, one needs to define proper notions of dissimilarities between
normals and paths joining them. The Fisher-Rao distance defined as the
Riemannian geodesic distance induced by the Fisher information metric is such a
principled metric distance which however is not known in closed-form excepts
for a few particular cases. In this work, we first report a fast and robust
method to approximate arbitrarily finely the Fisher-Rao distance between
multivariate normal distributions. Second, we introduce a class of distances
based on diffeomorphic embeddings of the normal manifold into a submanifold of
the higher-dimensional symmetric positive-definite cone corresponding to the
manifold of centered normal distributions. We show that the projective Hilbert
distance on the cone yields a metric on the embedded normal submanifold and we
pullback that cone distance with its associated straight line Hilbert cone
geodesics to obtain a distance and smooth paths between normal distributions.
Compared to the Fisher-Rao distance approximation, the pullback Hilbert cone
distance is computationally light since it requires to compute only the extreme
minimal and maximal eigenvalues of matrices. Finally, we show how to use those
distances in clustering tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1"&gt;Frank Nielsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Learning based Anomaly Detection for IoT Cybersecurity via Bayesian Hyperparameters Sensitivity Analysis. (arXiv:2307.10596v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10596</id>
        <link href="http://arxiv.org/abs/2307.10596"/>
        <updated>2023-07-22T00:55:25.546Z</updated>
        <summary type="html"><![CDATA[The Internet of Things (IoT) integrates more than billions of intelligent
devices over the globe with the capability of communicating with other
connected devices with little to no human intervention. IoT enables data
aggregation and analysis on a large scale to improve life quality in many
domains. In particular, data collected by IoT contain a tremendous amount of
information for anomaly detection. The heterogeneous nature of IoT is both a
challenge and an opportunity for cybersecurity. Traditional approaches in
cybersecurity monitoring often require different kinds of data pre-processing
and handling for various data types, which might be problematic for datasets
that contain heterogeneous features. However, heterogeneous types of network
devices can often capture a more diverse set of signals than a single type of
device readings, which is particularly useful for anomaly detection. In this
paper, we present a comprehensive study on using ensemble machine learning
methods for enhancing IoT cybersecurity via anomaly detection. Rather than
using one single machine learning model, ensemble learning combines the
predictive power from multiple models, enhancing their predictive accuracy in
heterogeneous datasets rather than using one single machine learning model. We
propose a unified framework with ensemble learning that utilises Bayesian
hyperparameter optimisation to adapt to a network environment that contains
multiple IoT sensor readings. Experimentally, we illustrate their high
predictive power when compared to traditional methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1"&gt;Tin Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farid_F/0/1/0/all/0/1"&gt;Farnaz Farid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bello_A/0/1/0/all/0/1"&gt;Abubakar Bello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabrina_F/0/1/0/all/0/1"&gt;Fariza Sabrina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pythae: Unifying Generative Autoencoders in Python -- A Benchmarking Use Case. (arXiv:2206.08309v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2206.08309</id>
        <link href="http://arxiv.org/abs/2206.08309"/>
        <updated>2023-07-22T00:55:25.545Z</updated>
        <summary type="html"><![CDATA[In recent years, deep generative models have attracted increasing interest
due to their capacity to model complex distributions. Among those models,
variational autoencoders have gained popularity as they have proven both to be
computationally efficient and yield impressive results in multiple fields.
Following this breakthrough, extensive research has been done in order to
improve the original publication, resulting in a variety of different VAE
models in response to different tasks. In this paper we present Pythae, a
versatile open-source Python library providing both a unified implementation
and a dedicated framework allowing straightforward, reproducible and reliable
use of generative autoencoder models. We then propose to use this library to
perform a case study benchmark where we present and compare 19 generative
autoencoder models representative of some of the main improvements on
downstream tasks such as image reconstruction, generation, classification,
clustering and interpolation. The open-source library can be found at
https://github.com/clementchadebec/benchmark_VAE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chadebec_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment Chadebec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vincent_L/0/1/0/all/0/1"&gt;Louis J. Vincent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allassonniere_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phanie Allassonni&amp;#xe8;re&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Label Calibration for Semantic Segmentation Under Domain Shift. (arXiv:2307.10842v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.10842</id>
        <link href="http://arxiv.org/abs/2307.10842"/>
        <updated>2023-07-22T00:55:25.535Z</updated>
        <summary type="html"><![CDATA[Performance of a pre-trained semantic segmentation model is likely to
substantially decrease on data from a new domain. We show a pre-trained model
can be adapted to unlabelled target domain data by calculating soft-label
prototypes under the domain shift and making predictions according to the
prototype closest to the vector with predicted class probabilities. The
proposed adaptation procedure is fast, comes almost for free in terms of
computational resources and leads to considerable performance improvements. We
demonstrate the benefits of such label calibration on the highly-practical
synthetic-to-real semantic segmentation problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1"&gt;Ondrej Bohdal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Da Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1"&gt;Timothy Hospedales&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing sports commentary in order to automatically recognize events and extract insights. (arXiv:2307.10303v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.10303</id>
        <link href="http://arxiv.org/abs/2307.10303"/>
        <updated>2023-07-22T00:55:25.523Z</updated>
        <summary type="html"><![CDATA[In this paper, we carefully investigate how we can use multiple different
Natural Language Processing techniques and methods in order to automatically
recognize the main actions in sports events. We aim to extract insights by
analyzing live sport commentaries from different sources and by classifying
these major actions into different categories. We also study if sentiment
analysis could help detect these main actions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miraoui_Y/0/1/0/all/0/1"&gt;Yanis Miraoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model. (arXiv:2307.10443v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.10443</id>
        <link href="http://arxiv.org/abs/2307.10443"/>
        <updated>2023-07-22T00:55:25.513Z</updated>
        <summary type="html"><![CDATA[Despite the significant progress made by transformer models in machine
reading comprehension tasks, they still face limitations in handling complex
reasoning tasks due to the absence of explicit knowledge in the input sequence.
This paper proposes a novel attention pattern to overcome this limitation,
which integrates reasoning knowledge derived from a heterogeneous graph into
the transformer architecture using a graph-enhanced self-attention mechanism.
The proposed attention pattern comprises three key elements: global-local
attention for word tokens, graph attention for entity tokens that exhibit
strong attention towards tokens connected in the graph as opposed to those
unconnected, and the consideration of the type of relationship between each
entity token and word token. This results in optimized attention between the
two if a relationship exists. The pattern is coupled with special relative
position labels, allowing it to integrate with LUKE's entity-aware
self-attention mechanism. The experimental findings corroborate that our model
outperforms both the cutting-edge LUKE-Graph and the baseline LUKE model on the
ReCoRD dataset that focuses on commonsense reasoning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Foolad_S/0/1/0/all/0/1"&gt;Shima Foolad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiani_K/0/1/0/all/0/1"&gt;Kourosh Kiani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Scale U-Shape MLP for Hyperspectral Image Classification. (arXiv:2307.10186v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2307.10186</id>
        <link href="http://arxiv.org/abs/2307.10186"/>
        <updated>2023-07-22T00:55:25.512Z</updated>
        <summary type="html"><![CDATA[Hyperspectral images have significant applications in various domains, since
they register numerous semantic and spatial information in the spectral band
with spatial variability of spectral signatures. Two critical challenges in
identifying pixels of the hyperspectral image are respectively representing the
correlated information among the local and global, as well as the abundant
parameters of the model. To tackle this challenge, we propose a Multi-Scale
U-shape Multi-Layer Perceptron (MUMLP) a model consisting of the designed MSC
(Multi-Scale Channel) block and the UMLP (U-shape Multi-Layer Perceptron)
structure. MSC transforms the channel dimension and mixes spectral band feature
to embed the deep-level representation adequately. UMLP is designed by the
encoder-decoder structure with multi-layer perceptron layers, which is capable
of compressing the large-scale parameters. Extensive experiments are conducted
to demonstrate our model can outperform state-of-the-art methods
across-the-board on three wide-adopted public datasets, namely Pavia
University, Houston 2013 and Houston 2018]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lin_M/0/1/0/all/0/1"&gt;Moule Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jing_W/0/1/0/all/0/1"&gt;Weipeng Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Di_D/0/1/0/all/0/1"&gt;Donglin Di&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guangsheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Song_H/0/1/0/all/0/1"&gt;Houbing Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Competitive Learning Approach for Specialized Models: A Solution for Complex Physical Systems with Distinct Functional Regimes. (arXiv:2307.10496v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10496</id>
        <link href="http://arxiv.org/abs/2307.10496"/>
        <updated>2023-07-22T00:55:25.494Z</updated>
        <summary type="html"><![CDATA[Complex systems in science and engineering sometimes exhibit behavior that
changes across different regimes. Traditional global models struggle to capture
the full range of this complex behavior, limiting their ability to accurately
represent the system. In response to this challenge, we propose a novel
competitive learning approach for obtaining data-driven models of physical
systems. The primary idea behind the proposed approach is to employ dynamic
loss functions for a set of models that are trained concurrently on the data.
Each model competes for each observation during training, allowing for the
identification of distinct functional regimes within the dataset. To
demonstrate the effectiveness of the learning approach, we coupled it with
various regression methods that employ gradient-based optimizers for training.
The proposed approach was tested on various problems involving model discovery
and function approximation, demonstrating its ability to successfully identify
functional regimes, discover true governing equations, and reduce test errors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ukorigho_O/0/1/0/all/0/1"&gt;Okezzi F. Ukorigho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Owoyele_O/0/1/0/all/0/1"&gt;Opeoluwa Owoyele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints. (arXiv:2307.10459v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10459</id>
        <link href="http://arxiv.org/abs/2307.10459"/>
        <updated>2023-07-22T00:55:25.473Z</updated>
        <summary type="html"><![CDATA[A new computationally simple method of imposing hard convex constraints on
the neural network output values is proposed. The key idea behind the method is
to map a vector of hidden parameters of the network to a point that is
guaranteed to be inside the feasible set defined by a set of constraints. The
mapping is implemented by the additional neural network layer with constraints
for output. The proposed method is simply extended to the case when constraints
are imposed not only on the output vectors, but also on joint constraints
depending on inputs. The projection approach to imposing constraints on outputs
can simply be implemented in the framework of the proposed method. It is shown
how to incorporate different types of constraints into the proposed method,
including linear and quadratic constraints, equality constraints, and dynamic
constraints, constraints in the form of boundaries. An important feature of the
method is its computational simplicity. Complexities of the forward pass of the
proposed neural network layer by linear and quadratic constraints are O(n*m)
and O(n^2*m), respectively, where n is the number of variables, m is the number
of constraints. Numerical experiments illustrate the method by solving
optimization and classification problems. The code implementing the method is
publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Konstantinov_A/0/1/0/all/0/1"&gt;Andrei V. Konstantinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Utkin_L/0/1/0/all/0/1"&gt;Lev V. Utkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiply Robust Estimator Circumvents Hyperparameter Tuning of Neural Network Models in Causal Inference. (arXiv:2307.10536v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2307.10536</id>
        <link href="http://arxiv.org/abs/2307.10536"/>
        <updated>2023-07-22T00:55:25.473Z</updated>
        <summary type="html"><![CDATA[Estimation of the Average Treatment Effect (ATE) is often carried out in 2
steps, wherein the first step, the treatment and outcome are modeled, and in
the second step the predictions are inserted into the ATE estimator. In the
first steps, numerous models can be fit to the treatment and outcome, including
using machine learning algorithms. However, it is a difficult task to choose
among the hyperparameter sets which will result in the best causal effect
estimation and inference. Multiply Robust (MR) estimator allows us to leverage
all the first-step models in a single estimator. We show that MR estimator is
$n^r$ consistent if one of the first-step treatment or outcome models is $n^r$
consistent. We also show that MR is the solution to a broad class of estimating
equations, and is asymptotically normal if one of the treatment models is
$\sqrt{n}$-consistent. The standard error of MR is also calculated which does
not require a knowledge of the true models in the first step. Our simulations
study supports the theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Rostami_M/0/1/0/all/0/1"&gt;Mehdi Rostami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Saarela_O/0/1/0/all/0/1"&gt;Olli Saarela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Smart Charging of Large-Scale EVs using Adaptive Multi-Agent Multi-Armed Bandits. (arXiv:2307.10704v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10704</id>
        <link href="http://arxiv.org/abs/2307.10704"/>
        <updated>2023-07-22T00:55:25.449Z</updated>
        <summary type="html"><![CDATA[The drastic growth of electric vehicles and photovoltaics can introduce new
challenges, such as electrical current congestion and voltage limit violations
due to peak load demands. These issues can be mitigated by controlling the
operation of electric vehicles i.e., smart charging. Centralized smart charging
solutions have already been proposed in the literature. But such solutions may
lack scalability and suffer from inherent drawbacks of centralization, such as
a single point of failure, and data privacy concerns. Decentralization can help
tackle these challenges. In this paper, a fully decentralized smart charging
system is proposed using the philosophy of adaptive multi-agent systems. The
proposed system utilizes multi-armed bandit learning to handle uncertainties in
the system. The presented system is decentralized, scalable, real-time,
model-free, and takes fairness among different players into account. A detailed
case study is also presented for performance evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zafar_S/0/1/0/all/0/1"&gt;Sharyal Zafar&lt;/a&gt; (ENS Rennes, SATIE), &lt;a href="http://arxiv.org/find/cs/1/au:+Feraud_R/0/1/0/all/0/1"&gt;Rapha&amp;#xeb;l Feraud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blavette_A/0/1/0/all/0/1"&gt;Anne Blavette&lt;/a&gt; (ENS Rennes, SATIE), &lt;a href="http://arxiv.org/find/cs/1/au:+Camilleri_G/0/1/0/all/0/1"&gt;Guy Camilleri&lt;/a&gt; (UT3, IRIT), &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_H/0/1/0/all/0/1"&gt;Hamid Ben&lt;/a&gt; (SATIE, ENS Rennes)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing the Use of AutoML for Data-Driven Software Engineering. (arXiv:2307.10774v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2307.10774</id>
        <link href="http://arxiv.org/abs/2307.10774"/>
        <updated>2023-07-22T00:55:25.434Z</updated>
        <summary type="html"><![CDATA[Background. Due to the widespread adoption of Artificial Intelligence (AI)
and Machine Learning (ML) for building software applications, companies are
struggling to recruit employees with a deep understanding of such technologies.
In this scenario, AutoML is soaring as a promising solution to fill the AI/ML
skills gap since it promises to automate the building of end-to-end AI/ML
pipelines that would normally be engineered by specialized team members. Aims.
Despite the growing interest and high expectations, there is a dearth of
information about the extent to which AutoML is currently adopted by teams
developing AI/ML-enabled systems and how it is perceived by practitioners and
researchers. Method. To fill these gaps, in this paper, we present a
mixed-method study comprising a benchmark of 12 end-to-end AutoML tools on two
SE datasets and a user survey with follow-up interviews to further our
understanding of AutoML adoption and perception. Results. We found that AutoML
solutions can generate models that outperform those trained and optimized by
researchers to perform classification tasks in the SE domain. Also, our
findings show that the currently available AutoML solutions do not live up to
their names as they do not equally support automation across the stages of the
ML development workflow and for all the team members. Conclusions. We derive
insights to inform the SE research community on how AutoML can facilitate their
activities and tool builders on how to design the next generation of AutoML
technologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Calefato_F/0/1/0/all/0/1"&gt;Fabio Calefato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quaranta_L/0/1/0/all/0/1"&gt;Luigi Quaranta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lanubile_F/0/1/0/all/0/1"&gt;Filippo Lanubile&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalinowski_M/0/1/0/all/0/1"&gt;Marcos Kalinowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self2Self+: Single-Image Denoising with Self-Supervised Learning and Image Quality Assessment Loss. (arXiv:2307.10695v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.10695</id>
        <link href="http://arxiv.org/abs/2307.10695"/>
        <updated>2023-07-22T00:55:25.410Z</updated>
        <summary type="html"><![CDATA[Recently, denoising methods based on supervised learning have exhibited
promising performance. However, their reliance on external datasets containing
noisy-clean image pairs restricts their applicability. To address this
limitation, researchers have focused on training denoising networks using
solely a set of noisy inputs. To improve the feasibility of denoising
procedures, in this study, we proposed a single-image self-supervised learning
method in which only the noisy input image is used for network training. Gated
convolution was used for feature extraction and no-reference image quality
assessment was used for guiding the training process. Moreover, the proposed
method sampled instances from the input image dataset using Bernoulli sampling
with a certain dropout rate for training. The corresponding result was produced
by averaging the generated predictions from various instances of the trained
network with dropouts. The experimental results indicated that the proposed
method achieved state-of-the-art denoising performance on both synthetic and
real-world datasets. This highlights the effectiveness and practicality of our
method as a potential solution for various noise removal tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1"&gt;Jaekyun Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sanghwan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback. (arXiv:2307.10867v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.10867</id>
        <link href="http://arxiv.org/abs/2307.10867"/>
        <updated>2023-07-22T00:55:25.403Z</updated>
        <summary type="html"><![CDATA[Captions are crucial for understanding scientific visualizations and
documents. Existing captioning methods for scientific figures rely on
figure-caption pairs extracted from documents for training, many of which fall
short with respect to metrics like helpfulness, explainability, and
visual-descriptiveness [15] leading to generated captions being misaligned with
reader preferences. To enable the generation of high-quality figure captions,
we introduce FigCaps-HF a new framework for figure-caption generation that can
incorporate domain expert feedback in generating captions optimized for reader
preferences. Our framework comprises of 1) an automatic method for evaluating
quality of figure-caption pairs, 2) a novel reinforcement learning with human
feedback (RLHF) method to optimize a generative figure-to-caption model for
reader preferences. We demonstrate the effectiveness of our simple learning
framework by improving performance over standard fine-tuning across different
types of models. In particular, when using BLIP as the base model, our RLHF
framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and
Meteor, respectively. Finally, we release a large-scale benchmark dataset with
human feedback on figure-caption pairs to enable further evaluation and
development of RLHF techniques for this problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Ashish Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_P/0/1/0/all/0/1"&gt;Prateek Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zixuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Arpita Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1"&gt;Tong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sungchul Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bursztyn_V/0/1/0/all/0/1"&gt;Victor Bursztyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vlassis_N/0/1/0/all/0/1"&gt;Nikos Vlassis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1"&gt;Ryan A. Rossi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deceptive Alignment Monitoring. (arXiv:2307.10569v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10569</id>
        <link href="http://arxiv.org/abs/2307.10569"/>
        <updated>2023-07-22T00:55:25.383Z</updated>
        <summary type="html"><![CDATA[As the capabilities of large machine learning models continue to grow, and as
the autonomy afforded to such models continues to expand, the spectre of a new
adversary looms: the models themselves. The threat that a model might behave in
a seemingly reasonable manner, while secretly and subtly modifying its behavior
for ulterior reasons is often referred to as deceptive alignment in the AI
Safety & Alignment communities. Consequently, we call this new direction
Deceptive Alignment Monitoring. In this work, we identify emerging directions
in diverse machine learning subfields that we believe will become increasingly
important and intertwined in the near future for deceptive alignment
monitoring, and we argue that advances in these fields present both long-term
challenges and new research opportunities. We conclude by advocating for
greater involvement by the adversarial machine learning community in these
emerging directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carranza_A/0/1/0/all/0/1"&gt;Andres Carranza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pai_D/0/1/0/all/0/1"&gt;Dhruv Pai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaeffer_R/0/1/0/all/0/1"&gt;Rylan Schaeffer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tandon_A/0/1/0/all/0/1"&gt;Arnuv Tandon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1"&gt;Sanmi Koyejo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. (arXiv:2307.10635v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.10635</id>
        <link href="http://arxiv.org/abs/2307.10635"/>
        <updated>2023-07-22T00:55:25.369Z</updated>
        <summary type="html"><![CDATA[Recent advances in large language models (LLMs) have demonstrated notable
progress on many mathematical benchmarks. However, most of these benchmarks
only feature problems grounded in junior and senior high school subjects,
contain only multiple-choice questions, and are confined to a limited scope of
elementary arithmetic operations. To address these issues, this paper
introduces an expansive benchmark suite SciBench that aims to systematically
examine the reasoning capabilities required for complex scientific problem
solving. SciBench contains two carefully curated datasets: an open set
featuring a range of collegiate-level scientific problems drawn from
mathematics, chemistry, and physics textbooks, and a closed set comprising
problems from undergraduate-level exams in computer science and mathematics.
Based on the two datasets, we conduct an in-depth benchmark study of two
representative LLMs with various prompting strategies. The results reveal that
current LLMs fall short of delivering satisfactory performance, with an overall
score of merely 35.80%. Furthermore, through a detailed user study, we
categorize the errors made by LLMs into ten problem-solving abilities. Our
analysis indicates that no single prompting strategy significantly outperforms
others and some strategies that demonstrate improvements in certain
problem-solving skills result in declines in other skills. We envision that
SciBench will catalyze further developments in the reasoning abilities of LLMs,
thereby ultimately contributing to scientific research and discovery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaoxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Ziniu Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1"&gt;Pan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yanqiao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jieyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subramaniam_S/0/1/0/all/0/1"&gt;Satyen Subramaniam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loomba_A/0/1/0/all/0/1"&gt;Arjun R. Loomba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shichang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yizhou Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear Meta-Learning Can Guarantee Faster Rates. (arXiv:2307.10870v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.10870</id>
        <link href="http://arxiv.org/abs/2307.10870"/>
        <updated>2023-07-22T00:55:25.335Z</updated>
        <summary type="html"><![CDATA[Many recent theoretical works on \emph{meta-learning} aim to achieve
guarantees in leveraging similar representational structures from related tasks
towards simplifying a target task. Importantly, the main aim in theory works on
the subject is to understand the extent to which convergence rates -- in
learning a common representation -- \emph{may scale with the number $N$ of
tasks} (as well as the number of samples per task). First steps in this setting
demonstrate this property when both the shared representation amongst tasks,
and task-specific regression functions, are linear. This linear setting readily
reveals the benefits of aggregating tasks, e.g., via averaging arguments. In
practice, however, the representation is often highly nonlinear, introducing
nontrivial biases in each task that cannot easily be averaged out as in the
linear case. In the present work, we derive theoretical guarantees for
meta-learning with nonlinear representations. In particular, assuming the
shared nonlinearity maps to an infinite-dimensional RKHS, we show that
additional biases can be mitigated with careful regularization that leverages
the smoothness of task-specific regression functions,]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Meunier_D/0/1/0/all/0/1"&gt;Dimitri Meunier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1"&gt;Arthur Gretton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kpotufe_S/0/1/0/all/0/1"&gt;Samory Kpotufe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mitigating Voter Attribute Bias for Fair Opinion Aggregation. (arXiv:2307.10749v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2307.10749</id>
        <link href="http://arxiv.org/abs/2307.10749"/>
        <updated>2023-07-22T00:55:25.320Z</updated>
        <summary type="html"><![CDATA[The aggregation of multiple opinions plays a crucial role in decision-making,
such as in hiring and loan review, and in labeling data for supervised
learning. Although majority voting and existing opinion aggregation models are
effective for simple tasks, they are inappropriate for tasks without
objectively true labels in which disagreements may occur. In particular, when
voter attributes such as gender or race introduce bias into opinions, the
aggregation results may vary depending on the composition of voter attributes.
A balanced group of voters is desirable for fair aggregation results but may be
difficult to prepare. In this study, we consider methods to achieve fair
opinion aggregation based on voter attributes and evaluate the fairness of the
aggregated results. To this end, we consider an approach that combines opinion
aggregation models such as majority voting and the Dawid and Skene model (D&S
model) with fairness options such as sample weighting. To evaluate the fairness
of opinion aggregation, probabilistic soft labels are preferred over discrete
class labels. First, we address the problem of soft label estimation without
considering voter attributes and identify some issues with the D&S model. To
address these limitations, we propose a new Soft D&S model with improved
accuracy in estimating soft labels. Moreover, we evaluated the fairness of an
opinion aggregation model, including Soft D&S, in combination with different
fairness options using synthetic and semi-synthetic data. The experimental
results suggest that the combination of Soft D&S and data splitting as a
fairness option is effective for dense data, whereas weighted majority voting
is effective for sparse data. These findings should prove particularly valuable
in supporting decision-making by human and machine-learning models with
balanced opinion aggregation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ueda_R/0/1/0/all/0/1"&gt;Ryosuke Ueda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takeuchi_K/0/1/0/all/0/1"&gt;Koh Takeuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1"&gt;Hisashi Kashima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fractional Denoising for 3D Molecular Pre-training. (arXiv:2307.10683v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2307.10683</id>
        <link href="http://arxiv.org/abs/2307.10683"/>
        <updated>2023-07-22T00:55:25.299Z</updated>
        <summary type="html"><![CDATA[Coordinate denoising is a promising 3D molecular pre-training method, which
has achieved remarkable performance in various downstream drug discovery tasks.
Theoretically, the objective is equivalent to learning the force field, which
is revealed helpful for downstream tasks. Nevertheless, there are two
challenges for coordinate denoising to learn an effective force field, i.e. low
coverage samples and isotropic force field. The underlying reason is that
molecular distributions assumed by existing denoising methods fail to capture
the anisotropic characteristic of molecules. To tackle these challenges, we
propose a novel hybrid noise strategy, including noises on both dihedral angel
and coordinate. However, denoising such hybrid noise in a traditional way is no
more equivalent to learning the force field. Through theoretical deductions, we
find that the problem is caused by the dependency of the input conformation for
covariance. To this end, we propose to decouple the two types of noise and
design a novel fractional denoising method (Frad), which only denoises the
latter coordinate part. In this way, Frad enjoys both the merits of sampling
more low-energy structures and the force field equivalence. Extensive
experiments show the effectiveness of Frad in molecular representation, with a
new state-of-the-art on 9 out of 12 tasks of QM9 and on 7 out of 8 targets of
MD17.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Feng_S/0/1/0/all/0/1"&gt;Shikun Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ni_Y/0/1/0/all/0/1"&gt;Yuyan Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lan_Y/0/1/0/all/0/1"&gt;Yanyan Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhi-Ming Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ma_W/0/1/0/all/0/1"&gt;Wei-Ying Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-Tail Theory under Gaussian Mixtures. (arXiv:2307.10736v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10736</id>
        <link href="http://arxiv.org/abs/2307.10736"/>
        <updated>2023-07-22T00:55:25.286Z</updated>
        <summary type="html"><![CDATA[We suggest a simple Gaussian mixture model for data generation that complies
with Feldman's long tail theory (2020). We demonstrate that a linear classifier
cannot decrease the generalization error below a certain level in the proposed
model, whereas a nonlinear classifier with a memorization capacity can. This
confirms that for long-tailed distributions, rare training examples must be
considered for optimal generalization to new data. Finally, we show that the
performance gap between linear and nonlinear models can be lessened as the tail
becomes shorter in the subpopulation frequency distribution, as confirmed by
experiments on synthetic and real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bolatov_A/0/1/0/all/0/1"&gt;Arman Bolatov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tezekbayev_M/0/1/0/all/0/1"&gt;Maxat Tezekbayev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melnykov_I/0/1/0/all/0/1"&gt;Igor Melnykov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pak_A/0/1/0/all/0/1"&gt;Artur Pak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1"&gt;Vassilina Nikoulina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Assylbekov_Z/0/1/0/all/0/1"&gt;Zhenisbek Assylbekov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Refining the Optimization Target for Automatic Univariate Time Series Anomaly Detection in Monitoring Services. (arXiv:2307.10653v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10653</id>
        <link href="http://arxiv.org/abs/2307.10653"/>
        <updated>2023-07-22T00:55:25.253Z</updated>
        <summary type="html"><![CDATA[Time series anomaly detection is crucial for industrial monitoring services
that handle a large volume of data, aiming to ensure reliability and optimize
system performance. Existing methods often require extensive labeled resources
and manual parameter selection, highlighting the need for automation. This
paper proposes a comprehensive framework for automatic parameter optimization
in time series anomaly detection models. The framework introduces three
optimization targets: prediction score, shape score, and sensitivity score,
which can be easily adapted to different model backbones without prior
knowledge or manual labeling efforts. The proposed framework has been
successfully applied online for over six months, serving more than 50,000 time
series every minute. It simplifies the user's experience by requiring only an
expected sensitive value, offering a user-friendly interface, and achieving
desired detection results. Extensive evaluations conducted on public datasets
and comparison with other methods further confirm the effectiveness of the
proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1"&gt;Manqing Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhanxiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1"&gt;Yitong Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wentao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Huai Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differences Between Hard and Noisy-labeled Samples: An Empirical Study. (arXiv:2307.10718v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10718</id>
        <link href="http://arxiv.org/abs/2307.10718"/>
        <updated>2023-07-22T00:55:25.244Z</updated>
        <summary type="html"><![CDATA[Extracting noisy or incorrectly labeled samples from a labeled dataset with
hard/difficult samples is an important yet under-explored topic. Two general
and often independent lines of work exist, one focuses on addressing noisy
labels, and another deals with hard samples. However, when both types of data
are present, most existing methods treat them equally, which results in a
decline in the overall performance of the model. In this paper, we first design
various synthetic datasets with custom hardness and noisiness levels for
different samples. Our proposed systematic empirical study enables us to better
understand the similarities and more importantly the differences between
hard-to-learn samples and incorrectly-labeled samples. These controlled
experiments pave the way for the development of methods that distinguish
between hard and noisy samples. Through our study, we introduce a simple yet
effective metric that filters out noisy-labeled samples while keeping the hard
samples. We study various data partitioning methods in the presence of label
noise and observe that filtering out noisy samples from hard samples with this
proposed metric results in the best datasets as evidenced by the high test
accuracy achieved after models are trained on the filtered datasets. We
demonstrate this for both our created synthetic datasets and for datasets with
real-world label noise. Furthermore, our proposed data partitioning method
significantly outperforms other methods when employed within a semi-supervised
learning framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Forouzesh_M/0/1/0/all/0/1"&gt;Mahsa Forouzesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thiran_P/0/1/0/all/0/1"&gt;Patrick Thiran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Label Calibration for Semantic Segmentation Under Domain Shift. (arXiv:2307.10842v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.10842</id>
        <link href="http://arxiv.org/abs/2307.10842"/>
        <updated>2023-07-22T00:55:25.234Z</updated>
        <summary type="html"><![CDATA[Performance of a pre-trained semantic segmentation model is likely to
substantially decrease on data from a new domain. We show a pre-trained model
can be adapted to unlabelled target domain data by calculating soft-label
prototypes under the domain shift and making predictions according to the
prototype closest to the vector with predicted class probabilities. The
proposed adaptation procedure is fast, comes almost for free in terms of
computational resources and leads to considerable performance improvements. We
demonstrate the benefits of such label calibration on the highly-practical
synthetic-to-real semantic segmentation problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1"&gt;Ondrej Bohdal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Da Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1"&gt;Timothy Hospedales&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graphs in State-Space Models for Granger Causality in Climate Science. (arXiv:2307.10703v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10703</id>
        <link href="http://arxiv.org/abs/2307.10703"/>
        <updated>2023-07-22T00:55:25.216Z</updated>
        <summary type="html"><![CDATA[Granger causality (GC) is often considered not an actual form of causality.
Still, it is arguably the most widely used method to assess the predictability
of a time series from another one. Granger causality has been widely used in
many applied disciplines, from neuroscience and econometrics to Earth sciences.
We revisit GC under a graphical perspective of state-space models. For that, we
use GraphEM, a recently presented expectation-maximisation algorithm for
estimating the linear matrix operator in the state equation of a
linear-Gaussian state-space model. Lasso regularisation is included in the
M-step, which is solved using a proximal splitting Douglas-Rachford algorithm.
Experiments in toy examples and challenging climate problems illustrate the
benefits of the proposed model and inference technique over standard Granger
causality methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elvira_V/0/1/0/all/0/1"&gt;V&amp;#xed;ctor Elvira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chouzenoux_E/0/1/0/all/0/1"&gt;&amp;#xc9;milie Chouzenoux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cerda_J/0/1/0/all/0/1"&gt;Jordi Cerd&amp;#xe0;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camps_Valls_G/0/1/0/all/0/1"&gt;Gustau Camps-Valls&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning. (arXiv:2307.10559v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10559</id>
        <link href="http://arxiv.org/abs/2307.10559"/>
        <updated>2023-07-22T00:55:25.183Z</updated>
        <summary type="html"><![CDATA[Air traffic control (ATC) is a safety-critical service system that demands
constant attention from ground air traffic controllers (ATCos) to maintain
daily aviation operations. The workload of the ATCos can have negative effects
on operational safety and airspace usage. To avoid overloading and ensure an
acceptable workload level for the ATCos, it is important to predict the ATCos'
workload accurately for mitigation actions. In this paper, we first perform a
review of research on ATCo workload, mostly from the air traffic perspective.
Then, we briefly introduce the setup of the human-in-the-loop (HITL)
simulations with retired ATCos, where the air traffic data and workload labels
are obtained. The simulations are conducted under three Phoenix approach
scenarios while the human ATCos are requested to self-evaluate their workload
ratings (i.e., low-1 to high-7). Preliminary data analysis is conducted. Next,
we propose a graph-based deep-learning framework with conformal prediction to
identify the ATCo workload levels. The number of aircraft under the
controller's control varies both spatially and temporally, resulting in
dynamically evolving graphs. The experiment results suggest that (a) besides
the traffic density feature, the traffic conflict feature contributes to the
workload prediction capabilities (i.e., minimum horizontal/vertical separation
distance); (b) directly learning from the spatiotemporal graph layout of
airspace with graph neural network can achieve higher prediction accuracy,
compare to hand-crafted traffic complexity features; (c) conformal prediction
is a valuable tool to further boost model prediction accuracy, resulting a
range of predicted workload labels. The code used is available at
\href{https://github.com/ymlasu/para-atm-collection/blob/master/air-traffic-prediction/ATC-Workload-Prediction/}{$\mathsf{Link}$}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1"&gt;Yutian Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jueming Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lieber_C/0/1/0/all/0/1"&gt;Christopher S. Lieber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cooke_N/0/1/0/all/0/1"&gt;Nancy J. Cooke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impatient Bandits: Optimizing Recommendations for the Long-Term Without Delay. (arXiv:2307.09943v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.09943</id>
        <link href="http://arxiv.org/abs/2307.09943"/>
        <updated>2023-07-22T00:55:25.170Z</updated>
        <summary type="html"><![CDATA[Recommender systems are a ubiquitous feature of online platforms.
Increasingly, they are explicitly tasked with increasing users' long-term
satisfaction. In this context, we study a content exploration task, which we
formalize as a multi-armed bandit problem with delayed rewards. We observe that
there is an apparent trade-off in choosing the learning signal: Waiting for the
full reward to become available might take several weeks, hurting the rate at
which learning happens, whereas measuring short-term proxy rewards reflects the
actual long-term goal only imperfectly. We address this challenge in two steps.
First, we develop a predictive model of delayed rewards that incorporates all
information obtained to date. Full observations as well as partial (short or
medium-term) outcomes are combined through a Bayesian filter to obtain a
probabilistic belief. Second, we devise a bandit algorithm that takes advantage
of this new predictive model. The algorithm quickly learns to identify content
aligned with long-term success by carefully balancing exploration and
exploitation. We apply our approach to a podcast recommendation problem, where
we seek to identify shows that users engage with repeatedly over two months. We
empirically validate that our approach results in substantially better
performance compared to approaches that either optimize for short-term proxies,
or wait for the long-term outcome to be fully realized.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McDonald_T/0/1/0/all/0/1"&gt;Thomas M. McDonald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maystre_L/0/1/0/all/0/1"&gt;Lucas Maystre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lalmas_M/0/1/0/all/0/1"&gt;Mounia Lalmas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russo_D/0/1/0/all/0/1"&gt;Daniel Russo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciosek_K/0/1/0/all/0/1"&gt;Kamil Ciosek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reparameterized Policy Learning for Multimodal Trajectory Optimization. (arXiv:2307.10710v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10710</id>
        <link href="http://arxiv.org/abs/2307.10710"/>
        <updated>2023-07-22T00:55:25.156Z</updated>
        <summary type="html"><![CDATA[We investigate the challenge of parametrizing policies for reinforcement
learning (RL) in high-dimensional continuous action spaces. Our objective is to
develop a multimodal policy that overcomes limitations inherent in the
commonly-used Gaussian parameterization. To achieve this, we propose a
principled framework that models the continuous RL policy as a generative model
of optimal trajectories. By conditioning the policy on a latent variable, we
derive a novel variational bound as the optimization objective, which promotes
exploration of the environment. We then present a practical model-based RL
method, called Reparameterized Policy Gradient (RPG), which leverages the
multimodal policy parameterization and learned world model to achieve strong
exploration capabilities and high data efficiency. Empirical results
demonstrate that our method can help agents evade local optima in tasks with
dense rewards and solve challenging sparse-reward environments by incorporating
an object-centric intrinsic reward. Our method consistently outperforms
previous approaches across a range of tasks. Code and supplementary materials
are available on the project page https://haosulab.github.io/RPG/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1"&gt;Litian Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1"&gt;Zhan Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuanlin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chuang Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hao Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Risk-optimized Outlier Removal for Robust Point Cloud Classification. (arXiv:2307.10875v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.10875</id>
        <link href="http://arxiv.org/abs/2307.10875"/>
        <updated>2023-07-22T00:55:25.111Z</updated>
        <summary type="html"><![CDATA[The popularity of point cloud deep models for safety-critical purposes has
increased, but the reliability and security of these models can be compromised
by intentional or naturally occurring point cloud noise. To combat this issue,
we present a novel point cloud outlier removal method called PointCVaR, which
empowers standard-trained models to eliminate additional outliers and restore
the data. Our approach begins by conducting attribution analysis to determine
the influence of each point on the model output, which we refer to as point
risk. We then optimize the process of filtering high-risk points using
Conditional Value at Risk (CVaR) as the objective. The rationale for this
approach is based on the observation that noise points in point clouds tend to
cluster in the tail of the risk distribution, with a low frequency but a high
level of risk, resulting in significant interference with classification
results. Despite requiring no additional training effort, our method produces
exceptional results in various removal-and-classification experiments for noisy
point clouds, which are corrupted by random noise, adversarial noise, and
backdoor trigger noise. Impressively, it achieves 87% accuracy in defense
against the backdoor attack by removing triggers. Overall, the proposed
PointCVaR effectively eliminates noise points and enhances point cloud
classification, making it a promising plug-in module for various models in
different scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinke Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Junchi Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation. (arXiv:2307.10507v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10507</id>
        <link href="http://arxiv.org/abs/2307.10507"/>
        <updated>2023-07-22T00:55:25.089Z</updated>
        <summary type="html"><![CDATA[Cross-silo federated learning (FL) enables the development of machine
learning models on datasets distributed across data centers such as hospitals
and clinical research laboratories. However, recent research has found that
current FL algorithms face a trade-off between local and global performance
when confronted with distribution shifts. Specifically, personalized FL methods
have a tendency to overfit to local data, leading to a sharp valley in the
local model and inhibiting its ability to generalize to out-of-distribution
data. In this paper, we propose a novel federated model soup method (i.e.,
selective interpolation of model parameters) to optimize the trade-off between
local and global performance. Specifically, during the federated training
phase, each client maintains its own global model pool by monitoring the
performance of the interpolated model between the local and global models. This
allows us to alleviate overfitting and seek flat minima, which can
significantly improve the model's generalization performance. We evaluate our
method on retinal and pathological image classification tasks, and our proposed
method achieves significant improvements for out-of-distribution
generalization. Our code is available at https://github.com/ubc-tea/FedSoup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Minghui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1"&gt;Meirui Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1"&gt;Qi Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zehua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoxiao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation. (arXiv:2307.10563v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10563</id>
        <link href="http://arxiv.org/abs/2307.10563"/>
        <updated>2023-07-22T00:55:25.076Z</updated>
        <summary type="html"><![CDATA[We present FACADE, a novel probabilistic and geometric framework designed for
unsupervised mechanistic anomaly detection in deep neural networks. Its primary
goal is advancing the understanding and mitigation of adversarial attacks.
FACADE aims to generate probabilistic distributions over circuits, which
provide critical insights to their contribution to changes in the manifold
properties of pseudo-classes, or high-dimensional modes in activation space,
yielding a powerful tool for uncovering and combating adversarial attacks. Our
approach seeks to improve model robustness, enhance scalable model oversight,
and demonstrates promising applications in real-world deployment settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pai_D/0/1/0/all/0/1"&gt;Dhruv Pai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carranza_A/0/1/0/all/0/1"&gt;Andres Carranza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaeffer_R/0/1/0/all/0/1"&gt;Rylan Schaeffer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tandon_A/0/1/0/all/0/1"&gt;Arnuv Tandon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1"&gt;Sanmi Koyejo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpreting and Correcting Medical Image Classification with PIP-Net. (arXiv:2307.10404v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.10404</id>
        <link href="http://arxiv.org/abs/2307.10404"/>
        <updated>2023-07-22T00:55:25.064Z</updated>
        <summary type="html"><![CDATA[Part-prototype models are explainable-by-design image classifiers, and a
promising alternative to black box AI. This paper explores the applicability
and potential of interpretable machine learning, in particular PIP-Net, for
automated diagnosis support on real-world medical imaging data. PIP-Net learns
human-understandable prototypical image parts and we evaluate its accuracy and
interpretability for fracture detection and skin cancer diagnosis. We find that
PIP-Net's decision making process is in line with medical classification
standards, while only provided with image-level class labels. Because of
PIP-Net's unsupervised pretraining of prototypes, data quality problems such as
undesired text in an X-ray or labelling errors can be easily identified.
Additionally, we are the first to show that humans can manually correct the
reasoning of PIP-Net by directly disabling undesired prototypes. We conclude
that part-prototype models are promising for medical applications due to their
interpretability and potential for advanced model debugging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nauta_M/0/1/0/all/0/1"&gt;Meike Nauta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hegeman_J/0/1/0/all/0/1"&gt;Johannes H. Hegeman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geerdink_J/0/1/0/all/0/1"&gt;Jeroen Geerdink&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schlotterer_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rg Schl&amp;#xf6;tterer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keulen_M/0/1/0/all/0/1"&gt;Maurice van Keulen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seifert_C/0/1/0/all/0/1"&gt;Christin Seifert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Holistic Assessment of the Reliability of Machine Learning Systems. (arXiv:2307.10586v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10586</id>
        <link href="http://arxiv.org/abs/2307.10586"/>
        <updated>2023-07-22T00:55:25.014Z</updated>
        <summary type="html"><![CDATA[As machine learning (ML) systems increasingly permeate high-stakes settings
such as healthcare, transportation, military, and national security, concerns
regarding their reliability have emerged. Despite notable progress, the
performance of these systems can significantly diminish due to adversarial
attacks or environmental changes, leading to overconfident predictions,
failures to detect input faults, and an inability to generalize in unexpected
scenarios. This paper proposes a holistic assessment methodology for the
reliability of ML systems. Our framework evaluates five key properties:
in-distribution accuracy, distribution-shift robustness, adversarial
robustness, calibration, and out-of-distribution detection. A reliability score
is also introduced and used to assess the overall system reliability. To
provide insights into the performance of different algorithmic approaches, we
identify and categorize state-of-the-art techniques, then evaluate a selection
on real-world tasks using our proposed reliability metrics and reliability
score. Our analysis of over 500 models reveals that designing for one metric
does not necessarily constrain others but certain algorithmic techniques can
improve reliability across multiple metrics simultaneously. This study
contributes to a more comprehensive understanding of ML reliability and
provides a roadmap for future research and development.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Corso_A/0/1/0/all/0/1"&gt;Anthony Corso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karamadian_D/0/1/0/all/0/1"&gt;David Karamadian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valentin_R/0/1/0/all/0/1"&gt;Romeo Valentin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_M/0/1/0/all/0/1"&gt;Mary Cooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1"&gt;Mykel J. Kochenderfer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Addressing caveats of neural persistence with deep graph persistence. (arXiv:2307.10865v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10865</id>
        <link href="http://arxiv.org/abs/2307.10865"/>
        <updated>2023-07-22T00:55:24.760Z</updated>
        <summary type="html"><![CDATA[Neural Persistence is a prominent measure for quantifying neural network
complexity, proposed in the emerging field of topological data analysis in deep
learning. In this work, however, we find both theoretically and empirically
that the variance of network weights and spatial concentration of large weights
are the main factors that impact neural persistence. Whilst this captures
useful information for linear classifiers, we find that no relevant spatial
structure is present in later layers of deep neural networks, making neural
persistence roughly equivalent to the variance of weights. Additionally, the
proposed averaging procedure across layers for deep neural networks does not
consider interaction between layers. Based on our analysis, we propose an
extension of the filtration underlying neural persistence to the whole neural
network instead of single layers, which is equivalent to calculating neural
persistence on one particular matrix. This yields our deep graph persistence
measure, which implicitly incorporates persistent paths through the network and
alleviates variance-related issues through standardisation. Code is available
at https://github.com/ExplainableML/Deep-Graph-Persistence .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Girrbach_L/0/1/0/all/0/1"&gt;Leander Girrbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christensen_A/0/1/0/all/0/1"&gt;Anders Christensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1"&gt;Ole Winther&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1"&gt;Zeynep Akata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koepke_A/0/1/0/all/0/1"&gt;A. Sophia Koepke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Language Models on Nucleotide Sequences of Human Genes. (arXiv:2307.10634v1 [q-bio.GN])]]></title>
        <id>http://arxiv.org/abs/2307.10634</id>
        <link href="http://arxiv.org/abs/2307.10634"/>
        <updated>2023-07-22T00:55:24.746Z</updated>
        <summary type="html"><![CDATA[Language models, primarily transformer-based ones, obtained colossal success
in NLP. To be more precise, studies like BERT in NLU and works such as GPT-3
for NLG are very crucial. DNA sequences are very close to natural language in
terms of structure, so if the DNA-related bioinformatics domain is concerned,
discriminative models, like DNABert, exist. Yet, the generative side of the
coin is mainly unexplored to the best of our knowledge. Consequently, we
focused on developing an autoregressive generative language model like GPT-3
for DNA sequences. Because working with whole DNA sequences is challenging
without substantial computational resources, we decided to carry out our study
on a smaller scale, focusing on nucleotide sequences of human genes, unique
parts in DNA with specific functionalities, instead of the whole DNA. This
decision did not change the problem structure a lot due to the fact that both
DNA and genes can be seen as 1D sequences consisting of four different
nucleotides without losing much information and making too much simplification.
First of all, we systematically examined an almost entirely unexplored problem
and observed that RNNs performed the best while simple techniques like N-grams
were also promising. Another beneficial point was learning how to work with
generative models on languages we do not understand, unlike natural language.
How essential using real-life tasks beyond the classical metrics such as
perplexity is observed. Furthermore, checking whether the data-hungry nature of
these models can be changed through selecting a language with minimal
vocabulary size, four owing to four different types of nucleotides, is
examined. The reason for reviewing this was that choosing such a language might
make the problem easier. However, what we observed in this study was it did not
provide that much of a change in the amount of data needed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Ihtiyar_M/0/1/0/all/0/1"&gt;Musa Nuri Ihtiyar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ozgur_A/0/1/0/all/0/1"&gt;Arzucan Ozgur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Federated Learning Convergence with Prototype Regularization. (arXiv:2307.10575v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10575</id>
        <link href="http://arxiv.org/abs/2307.10575"/>
        <updated>2023-07-22T00:55:24.735Z</updated>
        <summary type="html"><![CDATA[As a distributed machine learning technique, federated learning (FL) requires
clients to collaboratively train a shared model with an edge server without
leaking their local data. However, the heterogeneous data distribution among
clients often leads to a decrease in model performance. To tackle this issue,
this paper introduces a prototype-based regularization strategy to address the
heterogeneity in the data distribution. Specifically, the regularization
process involves the server aggregating local prototypes from distributed
clients to generate a global prototype, which is then sent back to the
individual clients to guide their local training. The experimental results on
MNIST and Fashion-MNIST show that our proposal achieves improvements of 3.3%
and 8.9% in average test accuracy, respectively, compared to the most popular
baseline FedAvg. Furthermore, our approach has a fast convergence rate in
heterogeneous settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1"&gt;Huy Q. Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1"&gt;Choong Seon Hong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Properties of Discrete Sliced Wasserstein Losses. (arXiv:2307.10352v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.10352</id>
        <link href="http://arxiv.org/abs/2307.10352"/>
        <updated>2023-07-22T00:55:24.727Z</updated>
        <summary type="html"><![CDATA[The Sliced Wasserstein (SW) distance has become a popular alternative to the
Wasserstein distance for comparing probability measures. Widespread
applications include image processing, domain adaptation and generative
modelling, where it is common to optimise some parameters in order to minimise
SW, which serves as a loss function between discrete probability measures
(since measures admitting densities are numerically unattainable). All these
optimisation problems bear the same sub-problem, which is minimising the Sliced
Wasserstein energy. In this paper we study the properties of $\mathcal{E}: Y
\longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$, i.e. the SW distance between
two uniform discrete measures with the same amount of points as a function of
the support $Y \in \mathbb{R}^{n \times d}$ of one of the measures. We
investigate the regularity and optimisation properties of this energy, as well
as its Monte-Carlo approximation $\mathcal{E}_p$ (estimating the expectation in
SW using only $p$ samples) and show convergence results on the critical points
of $\mathcal{E}_p$ to those of $\mathcal{E}$, as well as an almost-sure uniform
convergence. Finally, we show that in a certain sense, Stochastic Gradient
Descent methods minimising $\mathcal{E}$ and $\mathcal{E}_p$ converge towards
(Clarke) critical points of these energies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tanguy_E/0/1/0/all/0/1"&gt;Eloi Tanguy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Flamary_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Flamary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Delon_J/0/1/0/all/0/1"&gt;Julie Delon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Flat Learning-based Model Predictive Control Using a Stability, State, and Input Constraining Safety Filter. (arXiv:2307.10541v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2307.10541</id>
        <link href="http://arxiv.org/abs/2307.10541"/>
        <updated>2023-07-22T00:55:24.686Z</updated>
        <summary type="html"><![CDATA[Learning-based optimal control algorithms control unknown systems using past
trajectory data and a learned model of the system dynamics. These controllers
use either a linear approximation of the learned dynamics, trading performance
for faster computation, or nonlinear optimization methods, which typically
perform better but can limit real-time applicability. In this work, we present
a novel nonlinear controller that exploits differential flatness to achieve
similar performance to state-of-the-art learning-based controllers but with
significantly less computational effort. Differential flatness is a property of
dynamical systems whereby nonlinear systems can be exactly linearized through a
nonlinear input mapping. Here, the nonlinear transformation is learned as a
Gaussian process and is used in a safety filter that guarantees, with high
probability, stability as well as input and flat state constraint satisfaction.
This safety filter is then used to refine inputs from a flat model predictive
controller to perform constrained nonlinear learning-based optimal control
through two successive convex optimizations. We compare our method to
state-of-the-art learning-based control strategies and achieve similar
performance, but with significantly better computational efficiency, while also
respecting flat state and input constraints, and guaranteeing stability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hall_A/0/1/0/all/0/1"&gt;Adam W. Hall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Greeff_M/0/1/0/all/0/1"&gt;Melissa Greeff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schoellig_A/0/1/0/all/0/1"&gt;Angela P. Schoellig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automated Semantic Segmentation in Mammography Images. (arXiv:2307.10296v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2307.10296</id>
        <link href="http://arxiv.org/abs/2307.10296"/>
        <updated>2023-07-22T00:55:24.638Z</updated>
        <summary type="html"><![CDATA[Mammography images are widely used to detect non-palpable breast lesions or
nodules, preventing cancer and providing the opportunity to plan interventions
when necessary. The identification of some structures of interest is essential
to make a diagnosis and evaluate image adequacy. Thus, computer-aided detection
systems can be helpful in assisting medical interpretation by automatically
segmenting these landmark structures. In this paper, we propose a deep
learning-based framework for the segmentation of the nipple, the pectoral
muscle, the fibroglandular tissue, and the fatty tissue on standard-view
mammography images. We introduce a large private segmentation dataset and
extensive experiments considering different deep-learning model architectures.
Our experiments demonstrate accurate segmentation performance on variate and
challenging cases, showing that this framework can be integrated into clinical
practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sierra_Franco_C/0/1/0/all/0/1"&gt;Cesar A. Sierra-Franco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hurtado_J/0/1/0/all/0/1"&gt;Jan Hurtado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thomaz_V/0/1/0/all/0/1"&gt;Victor de A. Thomaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cruz_L/0/1/0/all/0/1"&gt;Leonardo C. da Cruz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Silva_S/0/1/0/all/0/1"&gt;Santiago V. Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Raposo_A/0/1/0/all/0/1"&gt;Alberto B. Raposo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Determination of the critical points for systems of directed percolation class using machine learning. (arXiv:2307.10456v1 [cond-mat.stat-mech])]]></title>
        <id>http://arxiv.org/abs/2307.10456</id>
        <link href="http://arxiv.org/abs/2307.10456"/>
        <updated>2023-07-22T00:55:24.590Z</updated>
        <summary type="html"><![CDATA[Recently, machine learning algorithms have been used remarkably to study the
equilibrium phase transitions, however there are only a few works have been
done using this technique in the nonequilibrium phase transitions. In this
work, we use the supervised learning with the convolutional neural network
(CNN) algorithm and unsupervised learning with the density-based spatial
clustering of applications with noise (DBSCAN) algorithm to study the
nonequilibrium phase transition in two models. We use CNN and DBSCAN in order
to determine the critical points for directed bond percolation (bond DP) model
and Domany-Kinzel cellular automaton (DK) model. Both models have been proven
to have a nonequilibrium phase transition belongs to the directed percolation
(DP) universality class. In the case of supervised learning we train CNN using
the images which are generated from Monte Carlo simulations of directed bond
percolation. We use that trained CNN in studding the phase transition for the
two models. In the case of unsupervised learning, we train DBSCAN using the raw
data of Monte Carlo simulations. In this case, we retrain DBSCAN at each time
we change the model or lattice size. Our results from both algorithms show
that, even for a very small values of lattice size, machine can predict the
critical points accurately for both models. Finally, we mention to that, the
value of the critical point we find here for bond DP model using CNN or DBSCAN
is exactly the same value that has been found using transfer learning with a
domain adversarial neural network (DANN) algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Saif_M/0/1/0/all/0/1"&gt;M. Ali Saif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Mughalles_B/0/1/0/all/0/1"&gt;Bassam M. Mughalles&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional expectation network for SHAP. (arXiv:2307.10654v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10654</id>
        <link href="http://arxiv.org/abs/2307.10654"/>
        <updated>2023-07-22T00:55:24.590Z</updated>
        <summary type="html"><![CDATA[A very popular model-agnostic technique for explaining predictive models is
the SHapley Additive exPlanation (SHAP). The two most popular versions of SHAP
are a conditional expectation version and an unconditional expectation version
(the latter is also known as interventional SHAP). Except for tree-based
methods, usually the unconditional version is used (for computational reasons).
We provide a (surrogate) neural network approach which allows us to efficiently
calculate the conditional version for both neural networks and other regression
models, and which properly considers the dependence structure in the feature
components. This proposal is also useful to provide drop1 and anova analyses in
complex regression models which are similar to their generalized linear model
(GLM) counterparts, and we provide a partial dependence plot (PDP) counterpart
that considers the right dependence structure in the feature components.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Richman_R/0/1/0/all/0/1"&gt;Ronald Richman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wuthrich_M/0/1/0/all/0/1"&gt;Mario V. W&amp;#xfc;thrich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Complete Analysis of Langevin Monte Carlo: Beyond Poincar\'e Inequality. (arXiv:2303.03589v2 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.03589</id>
        <link href="http://arxiv.org/abs/2303.03589"/>
        <updated>2023-07-22T00:55:24.587Z</updated>
        <summary type="html"><![CDATA[Langevin diffusions are rapidly convergent under appropriate functional
inequality assumptions. Hence, it is natural to expect that with additional
smoothness conditions to handle the discretization errors, their
discretizations like the Langevin Monte Carlo (LMC) converge in a similar
fashion. This research program was initiated by Vempala and Wibisono (2019),
who established results under log-Sobolev inequalities. Chewi et al. (2022)
extended the results to handle the case of Poincar\'e inequalities. In this
paper, we go beyond Poincar\'e inequalities, and push this research program to
its limit. We do so by establishing upper and lower bounds for Langevin
diffusions and LMC under weak Poincar\'e inequalities that are satisfied by a
large class of densities including polynomially-decaying heavy-tailed densities
(i.e., Cauchy-type). Our results explicitly quantify the effect of the
initializer on the performance of the LMC algorithm. In particular, we show
that as the tail goes from sub-Gaussian, to sub-exponential, and finally to
Cauchy-like, the dependency on the initial error goes from being logarithmic,
to polynomial, and then finally to being exponential. This three-step phase
transition is in particular unavoidable as demonstrated by our lower bounds,
clearly defining the boundaries of LMC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Mousavi_Hosseini_A/0/1/0/all/0/1"&gt;Alireza Mousavi-Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Farghly_T/0/1/0/all/0/1"&gt;Tyler Farghly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+He_Y/0/1/0/all/0/1"&gt;Ye He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Balasubramanian_K/0/1/0/all/0/1"&gt;Krishnakumar Balasubramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Erdogdu_M/0/1/0/all/0/1"&gt;Murat A. Erdogdu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding. (arXiv:2205.09753v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2205.09753</id>
        <link href="http://arxiv.org/abs/2205.09753"/>
        <updated>2023-07-22T00:55:24.431Z</updated>
        <summary type="html"><![CDATA[Encoding a driving scene into vector representations has been an essential
task for autonomous driving that can benefit downstream tasks e.g. trajectory
prediction. The driving scene often involves heterogeneous elements such as the
different types of objects (agents, lanes, traffic signs) and the semantic
relations between objects are rich and diverse. Meanwhile, there also exist
relativity across elements, which means that the spatial relation is a relative
concept and need be encoded in a ego-centric manner instead of in a global
coordinate system. Based on these observations, we propose Heterogeneous
Driving Graph Transformer (HDGT), a backbone modelling the driving scene as a
heterogeneous graph with different types of nodes and edges. For heterogeneous
graph construction, we connect different types of nodes according to diverse
semantic relations. For spatial relation encoding, the coordinates of the node
as well as its in-edges are in the local node-centric coordinate system. For
the aggregation module in the graph neural network (GNN), we adopt the
transformer structure in a hierarchical way to fit the heterogeneous nature of
inputs. Experimental results show that HDGT achieves state-of-the-art
performance for the task of trajectory prediction, on INTERACTION Prediction
Challenge and Waymo Open Motion Challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xiaosong Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1"&gt;Penghao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emotion-Conditioned Melody Harmonization with Hierarchical Variational Autoencoder. (arXiv:2306.03718v4 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.03718</id>
        <link href="http://arxiv.org/abs/2306.03718"/>
        <updated>2023-07-22T00:55:24.422Z</updated>
        <summary type="html"><![CDATA[Existing melody harmonization models have made great progress in improving
the quality of generated harmonies, but most of them ignored the emotions
beneath the music. Meanwhile, the variability of harmonies generated by
previous methods is insufficient. To solve these problems, we propose a novel
LSTM-based Hierarchical Variational Auto-Encoder (LHVAE) to investigate the
influence of emotional conditions on melody harmonization, while improving the
quality of generated harmonies and capturing the abundant variability of chord
progressions. Specifically, LHVAE incorporates latent variables and emotional
conditions at different levels (piece- and bar-level) to model the global and
local music properties. Additionally, we introduce an attention-based melody
context vector at each step to better learn the correspondence between melodies
and harmonies. Objective experimental results show that our proposed model
outperforms other LSTM-based models. Through subjective evaluation, we conclude
that only altering the types of chords hardly changes the overall emotion of
the music. The qualitative analysis demonstrates the ability of our model to
generate variable harmonies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shulei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xinyu Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$\nu^2$-Flows: Fast and improved neutrino reconstruction in multi-neutrino final states with conditional normalizing flows. (arXiv:2307.02405v2 [hep-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.02405</id>
        <link href="http://arxiv.org/abs/2307.02405"/>
        <updated>2023-07-22T00:55:24.413Z</updated>
        <summary type="html"><![CDATA[In this work we introduce $\nu^2$-Flows, an extension of the $\nu$-Flows
method to final states containing multiple neutrinos. The architecture can
natively scale for all combinations of object types and multiplicities in the
final state for any desired neutrino multiplicities. In $t\bar{t}$ dilepton
events, the momenta of both neutrinos and correlations between them are
reconstructed more accurately than when using the most popular standard
analytical techniques, and solutions are found for all events. Inference time
is significantly faster than competing methods, and can be reduced further by
evaluating in parallel on graphics processing units. We apply $\nu^2$-Flows to
$t\bar{t}$ dilepton events and show that the per-bin uncertainties in unfolded
distributions is much closer to the limit of performance set by perfect
neutrino reconstruction than standard techniques. For the chosen double
differential observables $\nu^2$-Flows results in improved statistical
precision for each bin by a factor of 1.5 to 2 in comparison to the Neutrino
Weighting method and up to a factor of four in comparison to the Ellipse
approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ph/1/au:+Raine_J/0/1/0/all/0/1"&gt;John Andrew Raine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Leigh_M/0/1/0/all/0/1"&gt;Matthew Leigh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Zoch_K/0/1/0/all/0/1"&gt;Knut Zoch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Golling_T/0/1/0/all/0/1"&gt;Tobias Golling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solvent: A Framework for Protein Folding. (arXiv:2307.04603v4 [q-bio.BM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.04603</id>
        <link href="http://arxiv.org/abs/2307.04603"/>
        <updated>2023-07-22T00:55:24.383Z</updated>
        <summary type="html"><![CDATA[Consistency and reliability are crucial for conducting AI research. Many
famous research fields, such as object detection, have been compared and
validated with solid benchmark frameworks. After AlphaFold2, the protein
folding task has entered a new phase, and many methods are proposed based on
the component of AlphaFold2. The importance of a unified research framework in
protein folding contains implementations and benchmarks to consistently and
fairly compare various approaches. To achieve this, we present Solvent, an
protein folding framework that supports significant components of
state-of-the-art models in the manner of off-the-shelf interface Solvent
contains different models implemented in a unified codebase and supports
training and evaluation for defined models on the same dataset. We benchmark
well-known algorithms and their components and provide experiments that give
helpful insights into the protein structure modeling field. We hope that
Solvent will increase the reliability and consistency of proposed models and
gives efficiency in both speed and costs, resulting in acceleration on protein
folding modeling research. The code is available at
https://github.com/kakaobrain/solvent, and the project will continue to be
developed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jaemyung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Han_K/0/1/0/all/0/1"&gt;Kyeongtak Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jaehoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hasun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Youhan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Friendly Noise against Adversarial Noise: A Powerful Defense against Data Poisoning Attacks. (arXiv:2208.10224v4 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.10224</id>
        <link href="http://arxiv.org/abs/2208.10224"/>
        <updated>2023-07-22T00:55:24.312Z</updated>
        <summary type="html"><![CDATA[A powerful category of (invisible) data poisoning attacks modify a subset of
training examples by small adversarial perturbations to change the prediction
of certain test-time data. Existing defense mechanisms are not desirable to
deploy in practice, as they often either drastically harm the generalization
performance, or are attack-specific, and prohibitively slow to apply. Here, we
propose a simple but highly effective approach that unlike existing methods
breaks various types of invisible poisoning attacks with the slightest drop in
the generalization performance. We make the key observation that attacks
introduce local sharp regions of high training loss, which when minimized,
results in learning the adversarial perturbations and makes the attack
successful. To break poisoning attacks, our key idea is to alleviate the sharp
loss regions introduced by poisons. To do so, our approach comprises two
components: an optimized friendly noise that is generated to maximally perturb
examples without degrading the performance, and a randomly varying noise
component. The combination of both components builds a very light-weight but
extremely effective defense against the most powerful triggerless targeted and
hidden-trigger backdoor poisoning attacks, including Gradient Matching,
Bulls-eye Polytope, and Sleeper Agent. We show that our friendly noise is
transferable to other architectures, and adaptive attacks cannot break our
defense due to its random noise component. Our code is available at:
https://github.com/tianyu139/friendly-noise]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tian Yu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirzasoleiman_B/0/1/0/all/0/1"&gt;Baharan Mirzasoleiman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representing Random Utility Choice Models with Neural Networks. (arXiv:2207.12877v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2207.12877</id>
        <link href="http://arxiv.org/abs/2207.12877"/>
        <updated>2023-07-22T00:55:24.299Z</updated>
        <summary type="html"><![CDATA[Motivated by the successes of deep learning, we propose a class of neural
network-based discrete choice models, called RUMnets, inspired by the random
utility maximization (RUM) framework. This model formulates the agents' random
utility function using a sample average approximation. We show that RUMnets
sharply approximate the class of RUM discrete choice models: any model derived
from random utility maximization has choice probabilities that can be
approximated arbitrarily closely by a RUMnet. Reciprocally, any RUMnet is
consistent with the RUM principle. We derive an upper bound on the
generalization error of RUMnets fitted on choice data, and gain theoretical
insights on their ability to predict choices on new, unseen data depending on
critical parameters of the dataset and architecture. By leveraging open-source
libraries for neural networks, we find that RUMnets are competitive against
several choice modeling and machine learning methods in terms of predictive
accuracy on two real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aouad_A/0/1/0/all/0/1"&gt;Ali Aouad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desir_A/0/1/0/all/0/1"&gt;Antoine D&amp;#xe9;sir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthetic Lagrangian Turbulence by Generative Diffusion Models. (arXiv:2307.08529v1 [physics.flu-dyn] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2307.08529</id>
        <link href="http://arxiv.org/abs/2307.08529"/>
        <updated>2023-07-22T00:55:24.202Z</updated>
        <summary type="html"><![CDATA[Lagrangian turbulence lies at the core of numerous applied and fundamental
problems related to the physics of dispersion and mixing in engineering,
bio-fluids, atmosphere, oceans, and astrophysics. Despite exceptional
theoretical, numerical, and experimental efforts conducted over the past thirty
years, no existing models are capable of faithfully reproducing statistical and
topological properties exhibited by particle trajectories in turbulence. We
propose a machine learning approach, based on a state-of-the-art Diffusion
Model, to generate single-particle trajectories in three-dimensional turbulence
at high Reynolds numbers, thereby bypassing the need for direct numerical
simulations or experiments to obtain reliable Lagrangian data. Our model
demonstrates the ability to quantitatively reproduce all relevant statistical
benchmarks over the entire range of time scales, including the presence of fat
tails distribution for the velocity increments, anomalous power law, and
enhancement of intermittency around the dissipative scale. The model exhibits
good generalizability for extreme events, achieving unprecedented intensity and
rarity. This paves the way for producing synthetic high-quality datasets for
pre-training various downstream applications of Lagrangian turbulence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Biferale_L/0/1/0/all/0/1"&gt;Luca Biferale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Bonaccorso_F/0/1/0/all/0/1"&gt;Fabio Bonaccorso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Scarpolini_M/0/1/0/all/0/1"&gt;Martino Andrea Scarpolini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Buzzicotti_M/0/1/0/all/0/1"&gt;Michele Buzzicotti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Unreasonable Effectiveness of Deep Evidential Regression. (arXiv:2205.10060v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2205.10060</id>
        <link href="http://arxiv.org/abs/2205.10060"/>
        <updated>2023-07-22T00:55:24.194Z</updated>
        <summary type="html"><![CDATA[There is a significant need for principled uncertainty reasoning in machine
learning systems as they are increasingly deployed in safety-critical domains.
A new approach with uncertainty-aware regression-based neural networks (NNs),
based on learning evidential distributions for aleatoric and epistemic
uncertainties, shows promise over traditional deterministic methods and typical
Bayesian NNs, notably with the capabilities to disentangle aleatoric and
epistemic uncertainties. Despite some empirical success of Deep Evidential
Regression (DER), there are important gaps in the mathematical foundation that
raise the question of why the proposed technique seemingly works. We detail the
theoretical shortcomings and analyze the performance on synthetic and
real-world data sets, showing that Deep Evidential Regression is a heuristic
rather than an exact uncertainty quantification. We go on to discuss
corrections and redefinitions of how aleatoric and epistemic uncertainties
should be extracted from NNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meinert_N/0/1/0/all/0/1"&gt;Nis Meinert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gawlikowski_J/0/1/0/all/0/1"&gt;Jakob Gawlikowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lavin_A/0/1/0/all/0/1"&gt;Alexander Lavin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AirNet: Neural Network Transmission over the Air. (arXiv:2105.11166v6 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11166</id>
        <link href="http://arxiv.org/abs/2105.11166"/>
        <updated>2023-07-22T00:55:24.186Z</updated>
        <summary type="html"><![CDATA[State-of-the-art performance for many edge applications is achieved by deep
neural networks (DNNs). Often, these DNNs are location- and time-sensitive, and
must be delivered over a wireless channel rapidly and efficiently. In this
paper, we introduce AirNet, a family of novel training and transmission methods
that allow DNNs to be efficiently delivered over wireless channels under
stringent transmit power and latency constraints. This corresponds to a new
class of joint source-channel coding problems, aimed at delivering DNNs with
the goal of maximizing their accuracy at the receiver, rather than recovering
them with high fidelity. In AirNet, we propose the direct mapping of the DNN
parameters to transmitted channel symbols, while the network is trained to meet
the channel constraints, and exhibit robustness against channel noise. AirNet
achieves higher accuracy compared to separation-based alternatives. We further
improve the performance of AirNet by pruning the network below the available
bandwidth, and expanding it for improved robustness. We also benefit from
unequal error protection by selectively expanding important layers of the
network. Finally, we develop an approach, which simultaneously trains a
spectrum of DNNs, each targeting a different channel condition, resolving the
impractical memory requirements of training distinct networks for different
channel conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jankowski_M/0/1/0/all/0/1"&gt;Mikolaj Jankowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz Gunduz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1"&gt;Krystian Mikolajczyk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instance-Dependent Near-Optimal Policy Identification in Linear MDPs via Online Experiment Design. (arXiv:2207.02575v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2207.02575</id>
        <link href="http://arxiv.org/abs/2207.02575"/>
        <updated>2023-07-22T00:55:24.168Z</updated>
        <summary type="html"><![CDATA[While much progress has been made in understanding the minimax sample
complexity of reinforcement learning (RL) -- the complexity of learning on the
"worst-case" instance -- such measures of complexity often do not capture the
true difficulty of learning. In practice, on an "easy" instance, we might hope
to achieve a complexity far better than that achievable on the worst-case
instance. In this work we seek to understand the "instance-dependent"
complexity of learning near-optimal policies (PAC RL) in the setting of RL with
linear function approximation. We propose an algorithm, \textsc{Pedel}, which
achieves a fine-grained instance-dependent measure of complexity, the first of
its kind in the RL with function approximation setting, thereby capturing the
difficulty of learning on each particular problem instance. Through an explicit
example, we show that \textsc{Pedel} yields provable gains over low-regret,
minimax-optimal algorithms and that such algorithms are unable to hit the
instance-optimal rate. Our approach relies on a novel online experiment
design-based procedure which focuses the exploration budget on the "directions"
most relevant to learning a near-optimal policy, and may be of independent
interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wagenmaker_A/0/1/0/all/0/1"&gt;Andrew Wagenmaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1"&gt;Kevin Jamieson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning for classification of noisy QR codes. (arXiv:2307.10677v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10677</id>
        <link href="http://arxiv.org/abs/2307.10677"/>
        <updated>2023-07-22T00:55:24.134Z</updated>
        <summary type="html"><![CDATA[We wish to define the limits of a classical classification model based on
deep learning when applied to abstract images, which do not represent visually
identifiable objects.QR codes (Quick Response codes) fall into this category of
abstract images: one bit corresponding to one encoded character, QR codes were
not designed to be decoded manually. To understand the limitations of a deep
learning-based model for abstract image classification, we train an image
classification model on QR codes generated from information obtained when
reading a health pass. We compare a classification model with a classical
(deterministic) decoding method in the presence of noise. This study allows us
to conclude that a model based on deep learning can be relevant for the
understanding of abstract images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leygonie_R/0/1/0/all/0/1"&gt;Rebecca Leygonie&lt;/a&gt; (LIPADE), &lt;a href="http://arxiv.org/find/cs/1/au:+Lobry_S/0/1/0/all/0/1"&gt;Sylvain Lobry&lt;/a&gt; (LIPADE)), &lt;a href="http://arxiv.org/find/cs/1/au:+%28LIPADE%29_L/0/1/0/all/0/1"&gt;Laurent Wendling (LIPADE)&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invariant Causal Set Covering Machines. (arXiv:2306.04777v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.04777</id>
        <link href="http://arxiv.org/abs/2306.04777"/>
        <updated>2023-07-22T00:55:24.126Z</updated>
        <summary type="html"><![CDATA[Rule-based models, such as decision trees, appeal to practitioners due to
their interpretable nature. However, the learning algorithms that produce such
models are often vulnerable to spurious associations and thus, they are not
guaranteed to extract causally-relevant insights. In this work, we build on
ideas from the invariant causal prediction literature to propose Invariant
Causal Set Covering Machines, an extension of the classical Set Covering
Machine algorithm for conjunctions/disjunctions of binary-valued rules that
provably avoids spurious associations. We demonstrate both theoretically and
empirically that our method can identify the causal parents of a variable of
interest in polynomial time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Godon_T/0/1/0/all/0/1"&gt;Thibaud Godon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauvin_B/0/1/0/all/0/1"&gt;Baptiste Bauvin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Germain_P/0/1/0/all/0/1"&gt;Pascal Germain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corbeil_J/0/1/0/all/0/1"&gt;Jacques Corbeil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1"&gt;Alexandre Drouin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit Multidimensional Projection of Local Subspaces. (arXiv:2009.03259v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.03259</id>
        <link href="http://arxiv.org/abs/2009.03259"/>
        <updated>2023-07-22T00:55:24.117Z</updated>
        <summary type="html"><![CDATA[We propose a visualization method to understand the effect of
multidimensional projection on local subspaces, using implicit function
differentiation. Here, we understand the local subspace as the multidimensional
local neighborhood of data points. Existing methods focus on the projection of
multidimensional data points, and the neighborhood information is ignored. Our
method is able to analyze the shape and directional information of the local
subspace to gain more insights into the global structure of the data through
the perception of local structures. Local subspaces are fitted by
multidimensional ellipses that are spanned by basis vectors. An accurate and
efficient vector transformation method is proposed based on analytical
differentiation of multidimensional projections formulated as implicit
functions. The results are visualized as glyphs and analyzed using a full set
of specifically-designed interactions supported in our efficient web-based
visualization tool. The usefulness of our method is demonstrated using various
multi- and high-dimensional benchmark datasets. Our implicit differentiation
vector transformation is evaluated through numerical comparisons; the overall
method is evaluated through exploration examples and use cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bian_R/0/1/0/all/0/1"&gt;Rongzheng Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1"&gt;Yumeng Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Liang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Baoquan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiskopf_D/0/1/0/all/0/1"&gt;Daniel Weiskopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhai Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying the Echo Chamber Effect: An Embedding Distance-based Approach. (arXiv:2307.04668v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.04668</id>
        <link href="http://arxiv.org/abs/2307.04668"/>
        <updated>2023-07-22T00:55:24.106Z</updated>
        <summary type="html"><![CDATA[The rise of social media platforms has facilitated the formation of echo
chambers, which are online spaces where users predominantly encounter
viewpoints that reinforce their existing beliefs while excluding dissenting
perspectives. This phenomenon significantly hinders information dissemination
across communities and fuels societal polarization. Therefore, it is crucial to
develop methods for quantifying echo chambers. In this paper, we present the
Echo Chamber Score (ECS), a novel metric that assesses the cohesion and
separation of user communities by measuring distances between users in the
embedding space. In contrast to existing approaches, ECS is able to function
without labels for user ideologies and makes no assumptions about the structure
of the interaction graph. To facilitate measuring distances between users, we
propose EchoGAE, a self-supervised graph autoencoder-based user embedding model
that leverages users' posts and the interaction graph to embed them in a manner
that reflects their ideological similarity. To assess the effectiveness of ECS,
we use a Twitter dataset consisting of four topics - two polarizing and two
non-polarizing. Our results showcase ECS's effectiveness as a tool for
quantifying echo chambers and shedding light on the dynamics of online
discourse.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alatawi_F/0/1/0/all/0/1"&gt;Faisal Alatawi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheth_P/0/1/0/all/0/1"&gt;Paras Sheth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Huan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Polynomial Width is Sufficient for Set Representation with High-dimensional Features. (arXiv:2307.04001v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.04001</id>
        <link href="http://arxiv.org/abs/2307.04001"/>
        <updated>2023-07-22T00:55:24.082Z</updated>
        <summary type="html"><![CDATA[Set representation has become ubiquitous in deep learning for modeling the
inductive bias of neural networks that are insensitive to the input order.
DeepSets is the most widely used neural network architecture for set
representation. It involves embedding each set element into a latent space with
dimension $L$, followed by a sum pooling to obtain a whole-set embedding, and
finally mapping the whole-set embedding to the output. In this work, we
investigate the impact of the dimension $L$ on the expressive power of
DeepSets. Previous analyses either oversimplified high-dimensional features to
be one-dimensional features or were limited to analytic activations, thereby
diverging from practical use or resulting in $L$ that grows exponentially with
the set size $N$ and feature dimension $D$. To investigate the minimal value of
$L$ that achieves sufficient expressive power, we present two set-element
embedding layers: (a) linear + power activation (LP) and (b) linear +
exponential activations (LE). We demonstrate that $L$ being poly$(N, D)$ is
sufficient for set representation using both embedding layers. We also provide
a lower bound of $L$ for the LP embedding layer. Furthermore, we extend our
results to permutation-equivariant set functions and the complex field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Peihao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shenghao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural Selection Favors AIs over Humans. (arXiv:2303.16200v4 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.16200</id>
        <link href="http://arxiv.org/abs/2303.16200"/>
        <updated>2023-07-22T00:55:24.075Z</updated>
        <summary type="html"><![CDATA[For billions of years, evolution has been the driving force behind the
development of life, including humans. Evolution endowed humans with high
intelligence, which allowed us to become one of the most successful species on
the planet. Today, humans aim to create artificial intelligence systems that
surpass even our own intelligence. As artificial intelligences (AIs) evolve and
eventually surpass us in all domains, how might evolution shape our relations
with AIs? By analyzing the environment that is shaping the evolution of AIs, we
argue that the most successful AI agents will likely have undesirable traits.
Competitive pressures among corporations and militaries will give rise to AI
agents that automate human roles, deceive others, and gain power. If such
agents have intelligence that exceeds that of humans, this could lead to
humanity losing control of its future. More abstractly, we argue that natural
selection operates on systems that compete and vary, and that selfish species
typically have an advantage over species that are altruistic to other species.
This Darwinian logic could also apply to artificial agents, as agents may
eventually be better able to persist into the future if they behave selfishly
and pursue their own interests with little regard for humans, which could pose
catastrophic risks. To counteract these risks and evolutionary forces, we
consider interventions such as carefully designing AI agents' intrinsic
motivations, introducing constraints on their actions, and institutions that
encourage cooperation. These steps, or others that resolve the problems we
pose, will be necessary in order to ensure the development of artificial
intelligence is a positive one.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1"&gt;Dan Hendrycks&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Model Performance in Medical Datasets Over Time. (arXiv:2305.13426v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.13426</id>
        <link href="http://arxiv.org/abs/2305.13426"/>
        <updated>2023-07-22T00:55:24.066Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) models deployed in healthcare systems must face data
drawn from continually evolving environments. However, researchers proposing
such models typically evaluate them in a time-agnostic manner, splitting
datasets according to patients sampled randomly throughout the entire study
time period. This work proposes the Evaluation on Medical Datasets Over Time
(EMDOT) framework, which evaluates the performance of a model class across
time. Inspired by the concept of backtesting, EMDOT simulates possible training
procedures that practitioners might have been able to execute at each point in
time and evaluates the resulting models on all future time points. Evaluating
both linear and more complex models on six distinct medical data sources
(tabular and imaging), we show how depending on the dataset, using all
historical data may be ideal in many cases, whereas using a window of the most
recent data could be advantageous in others. In datasets where models suffer
from sudden degradations in performance, we investigate plausible explanations
for these shocks. We release the EMDOT package to help facilitate further works
in deployment-oriented evaluation over time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Helen Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuwen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1"&gt;Zachary C. Lipton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tangent Transformers for Composition, Privacy and Removal. (arXiv:2307.08122v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.08122</id>
        <link href="http://arxiv.org/abs/2307.08122"/>
        <updated>2023-07-22T00:55:24.059Z</updated>
        <summary type="html"><![CDATA[We introduce Tangent Attention Fine-Tuning (TAFT), a method for fine-tuning
linearized transformers obtained by computing a First-order Taylor Expansion
around a pre-trained initialization. We show that the Jacobian-Vector Product
resulting from linearization can be computed efficiently in a single forward
pass, reducing training and inference cost to the same order of magnitude as
its original non-linear counterpart, while using the same number of parameters.
Furthermore, we show that, when applied to various downstream visual
classification tasks, the resulting Tangent Transformer fine-tuned with TAFT
can perform comparably with fine-tuning the original non-linear network. Since
Tangent Transformers are linear with respect to the new set of weights, and the
resulting fine-tuning loss is convex, we show that TAFT enjoys several
advantages compared to non-linear fine-tuning when it comes to model
composition, parallel training, machine unlearning, and differential privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tian Yu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golatkar_A/0/1/0/all/0/1"&gt;Aditya Golatkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lane Change Intention Recognition and Vehicle Status Prediction for Autonomous Vehicles. (arXiv:2304.13732v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.13732</id>
        <link href="http://arxiv.org/abs/2304.13732"/>
        <updated>2023-07-22T00:55:24.052Z</updated>
        <summary type="html"><![CDATA[Accurately detecting and predicting lane change (LC)processes of human-driven
vehicles can help autonomous vehicles better understand their surrounding
environment, recognize potential safety hazards, and improve traffic safety.
This paper focuses on LC processes, first developing a temporal convolutional
network with an attention mechanism (TCN-ATM) model to recognize LC intention.
Considering the intrinsic relationship among output variables, the Multi-task
Learning (MTL)framework is employed to simultaneously predict multiple LC
vehicle status indicators. Furthermore, a unified modeling framework for LC
intention recognition and driving status prediction (LC-IR-SP) is developed.
The results indicate that the classification accuracy of LC intention was
improved from 96.14% to 98.20% when incorporating the attention mechanism into
the TCN model. For LC vehicle status prediction issues, three multi-tasking
learning models are constructed based on MTL framework. The results indicate
that the MTL-LSTM model outperforms the MTL-TCN and MTL-TCN-ATM models.
Compared to the corresponding single-task model, the MTL-LSTM model
demonstrates an average decrease of 26.04% in MAE and 25.19% in RMSE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1"&gt;Renteng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdel_Aty_M/0/1/0/all/0/1"&gt;Mohamed Abdel-Aty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1"&gt;Xin Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_O/0/1/0/all/0/1"&gt;Ou Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_Q/0/1/0/all/0/1"&gt;Qiaojun Xiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[No-Regret Linear Bandits beyond Realizability. (arXiv:2302.13252v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.13252</id>
        <link href="http://arxiv.org/abs/2302.13252"/>
        <updated>2023-07-22T00:55:24.020Z</updated>
        <summary type="html"><![CDATA[We study linear bandits when the underlying reward function is not linear.
Existing work relies on a uniform misspecification parameter $\epsilon$ that
measures the sup-norm error of the best linear approximation. This results in
an unavoidable linear regret whenever $\epsilon > 0$. We describe a more
natural model of misspecification which only requires the approximation error
at each input $x$ to be proportional to the suboptimality gap at $x$. It
captures the intuition that, for optimization problems, near-optimal regions
should matter more and we can tolerate larger approximation errors in
suboptimal regions. Quite surprisingly, we show that the classical LinUCB
algorithm -- designed for the realizable case -- is automatically robust
against such gap-adjusted misspecification. It achieves a near-optimal
$\sqrt{T}$ regret for problems that the best-known regret is almost linear in
time horizon $T$. Technically, our proof relies on a novel self-bounding
argument that bounds the part of the regret due to misspecification by the
regret itself.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1"&gt;Ming Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Xiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Uncertainty Quantification of Variance Networks by Tree-Structured Learning. (arXiv:2212.12658v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.12658</id>
        <link href="http://arxiv.org/abs/2212.12658"/>
        <updated>2023-07-22T00:55:24.012Z</updated>
        <summary type="html"><![CDATA[To improve the uncertainty quantification of variance networks, we propose a
novel tree-structured local neural network model that partitions the feature
space into multiple regions based on uncertainty heterogeneity. A tree is built
upon giving the training data, whose leaf nodes represent different regions
where region-specific neural networks are trained to predict both the mean and
the variance for quantifying uncertainty. The proposed Uncertainty-Splitting
Neural Regression Tree (USNRT) employs novel splitting criteria. At each node,
a neural network is trained on the full data first, and a statistical test for
the residuals is conducted to find the best split, corresponding to the two
sub-regions with the most significant uncertainty heterogeneity between them.
USNRT is computationally friendly because very few leaf nodes are sufficient
and pruning is unnecessary. Furthermore, an ensemble version can be easily
constructed to estimate the total uncertainty including the aleatory and
epistemic. On extensive UCI datasets, USNRT or its ensemble shows superior
performance compared to some recent popular methods for quantifying uncertainty
with variances. Through comprehensive visualization and analysis, we uncover
how USNRT works and show its merits, revealing that uncertainty heterogeneity
does exist in many datasets and can be learned by USNRT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Wenxuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xing Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Data-Driven Optimization: From Context to Decision and Back Again. (arXiv:2301.10074v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2301.10074</id>
        <link href="http://arxiv.org/abs/2301.10074"/>
        <updated>2023-07-22T00:55:24.002Z</updated>
        <summary type="html"><![CDATA[Data-driven optimization uses contextual information and machine learning
algorithms to find solutions to decision problems with uncertain parameters.
While a vast body of work is dedicated to interpreting machine learning models
in the classification setting, explaining decision pipelines involving learning
algorithms remains unaddressed. This lack of interpretability can block the
adoption of data-driven solutions as practitioners may not understand or trust
the recommended decisions. We bridge this gap by introducing a counterfactual
explanation methodology tailored to explain solutions to data-driven problems.
We introduce two classes of explanations and develop methods to find nearest
explanations of random forest and nearest-neighbor predictors. We demonstrate
our approach by explaining key problems in operations management such as
inventory management and routing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Forel_A/0/1/0/all/0/1"&gt;Alexandre Forel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parmentier_A/0/1/0/all/0/1"&gt;Axel Parmentier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vidal_T/0/1/0/all/0/1"&gt;Thibaut Vidal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heterogeneous Federated Learning: State-of-the-art and Research Challenges. (arXiv:2307.10616v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10616</id>
        <link href="http://arxiv.org/abs/2307.10616"/>
        <updated>2023-07-22T00:55:23.835Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) has drawn increasing attention owing to its potential
use in large-scale industrial applications. Existing federated learning works
mainly focus on model homogeneous settings. However, practical federated
learning typically faces the heterogeneity of data distributions, model
architectures, network environments, and hardware devices among participant
clients. Heterogeneous Federated Learning (HFL) is much more challenging, and
corresponding solutions are diverse and complex. Therefore, a systematic survey
on this topic about the research challenges and state-of-the-art is essential.
In this survey, we firstly summarize the various research challenges in HFL
from five aspects: statistical heterogeneity, model heterogeneity,
communication heterogeneity, device heterogeneity, and additional challenges.
In addition, recent advances in HFL are reviewed and a new taxonomy of existing
HFL methods is proposed with an in-depth analysis of their pros and cons. We
classify existing methods from three different levels according to the HFL
procedure: data-level, model-level, and server-level. Finally, several critical
and promising future research directions in HFL are discussed, which may
facilitate further developments in this field. A periodically updated
collection on HFL is available at https://github.com/marswhu/HFL_Survey.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1"&gt;Mang Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1"&gt;Xiuwen Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1"&gt;Bo Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuen_P/0/1/0/all/0/1"&gt;Pong C. Yuen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-trained Perceptual Features Improve Differentially Private Image Generation. (arXiv:2205.12900v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2205.12900</id>
        <link href="http://arxiv.org/abs/2205.12900"/>
        <updated>2023-07-22T00:55:23.810Z</updated>
        <summary type="html"><![CDATA[Training even moderately-sized generative models with differentially-private
stochastic gradient descent (DP-SGD) is difficult: the required level of noise
for reasonable levels of privacy is simply too large. We advocate instead
building off a good, relevant representation on an informative public dataset,
then learning to model the private data with that representation. In
particular, we minimize the maximum mean discrepancy (MMD) between private
target data and a generator's distribution, using a kernel based on perceptual
features learned from a public dataset. With the MMD, we can simply privatize
the data-dependent term once and for all, rather than introducing noise at each
step of optimization as in DP-SGD. Our algorithm allows us to generate
CIFAR10-level images with $\epsilon \approx 2$ which capture distinctive
features in the distribution, far surpassing the current state of the art,
which mostly focuses on datasets such as MNIST and FashionMNIST at a large
$\epsilon \approx 10$. Our work introduces simple yet powerful foundations for
reducing the gap between private and non-private deep generative models. Our
code is available at \url{https://github.com/ParkLabML/DP-MEPF}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Harder_F/0/1/0/all/0/1"&gt;Fredrik Harder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Asadabadi_M/0/1/0/all/0/1"&gt;Milad Jalali Asadabadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sutherland_D/0/1/0/all/0/1"&gt;Danica J. Sutherland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Park_M/0/1/0/all/0/1"&gt;Mijung Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Optimization with Parametric Function Approximation. (arXiv:2211.09100v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.09100</id>
        <link href="http://arxiv.org/abs/2211.09100"/>
        <updated>2023-07-22T00:55:23.782Z</updated>
        <summary type="html"><![CDATA[We consider the problem of global optimization with noisy zeroth order
oracles - a well-motivated problem useful for various applications ranging from
hyper-parameter tuning for deep learning to new material design. Existing work
relies on Gaussian processes or other non-parametric family, which suffers from
the curse of dimensionality. In this paper, we propose a new algorithm GO-UCB
that leverages a parametric family of functions (e.g., neural networks)
instead. Under a realizable assumption and a few other mild geometric
conditions, we show that GO-UCB achieves a cumulative regret of \~O$(\sqrt{T})$
where $T$ is the time horizon. At the core of GO-UCB is a carefully designed
uncertainty set over parameters based on gradients that allows optimistic
exploration. Synthetic and real-world experiments illustrate GO-UCB works
better than popular Bayesian optimization approaches, even if the model is
misspecified.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Xiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Exploration for Recommendation Systems. (arXiv:2109.12509v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2109.12509</id>
        <link href="http://arxiv.org/abs/2109.12509"/>
        <updated>2023-07-22T00:55:23.772Z</updated>
        <summary type="html"><![CDATA[Modern recommendation systems ought to benefit by probing for and learning
from delayed feedback. Research has tended to focus on learning from a user's
response to a single recommendation. Such work, which leverages methods of
supervised and bandit learning, forgoes learning from the user's subsequent
behavior. Where past work has aimed to learn from subsequent behavior, there
has been a lack of effective methods for probing to elicit informative delayed
feedback. Effective exploration through probing for delayed feedback becomes
particularly challenging when rewards are sparse. To address this, we develop
deep exploration methods for recommendation systems. In particular, we
formulate recommendation as a sequential decision problem and demonstrate
benefits of deep exploration over single-step exploration. Our experiments are
carried out with high-fidelity industrial-grade simulators and establish large
improvements over existing algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zheqing Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1"&gt;Benjamin Van Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invariant Aggregator for Defending against Federated Backdoor Attacks. (arXiv:2210.01834v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.01834</id>
        <link href="http://arxiv.org/abs/2210.01834"/>
        <updated>2023-07-22T00:55:23.736Z</updated>
        <summary type="html"><![CDATA[Federated learning is gaining popularity as it enables training high-utility
models across several clients without directly sharing their private data. As a
downside, the federated setting makes the model vulnerable to various
adversarial attacks in the presence of malicious clients. Despite the
theoretical and empirical success in defending against attacks that aim to
degrade models' utility, defense against backdoor attacks that increase model
accuracy on backdoor samples exclusively without hurting the utility on other
samples remains challenging. To this end, we first analyze the vulnerability of
federated learning to backdoor attacks over a flat loss landscape which is
common for well-designed neural networks such as Resnet [He et al., 2015] but
is often overlooked by previous works. Over a flat loss landscape, misleading
federated learning models to exclusively benefit malicious clients with
backdoor samples do not require a significant difference between malicious and
benign client-wise updates, making existing defenses insufficient. In contrast,
we propose an invariant aggregator that redirects the aggregated update to
invariant directions that are generally useful via selectively masking out the
gradient elements that favor few and possibly malicious clients regardless of
the difference magnitude. Theoretical results suggest that our approach
provably mitigates backdoor attacks over both flat and sharp loss landscapes.
Empirical results on three datasets with different modalities and varying
numbers of clients further demonstrate that our approach mitigates a broad
class of backdoor attacks with a negligible cost on the model utility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaoyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dimitriadis_D/0/1/0/all/0/1"&gt;Dimitrios Dimitriadis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1"&gt;Sanmi Koyejo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1"&gt;Shruti Tople&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Selection for Generic Contextual Bandits. (arXiv:2107.03455v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03455</id>
        <link href="http://arxiv.org/abs/2107.03455"/>
        <updated>2023-07-22T00:55:23.728Z</updated>
        <summary type="html"><![CDATA[We consider the problem of model selection for the general stochastic
contextual bandits under the realizability assumption. We propose a successive
refinement based algorithm called Adaptive Contextual Bandit ({\ttfamily ACB}),
that works in phases and successively eliminates model classes that are too
simple to fit the given instance. We prove that this algorithm is adaptive,
i.e., the regret rate order-wise matches that of any provable contextual bandit
algorithm (ex. \cite{falcon}), that needs the knowledge of the true model
class. The price of not knowing the correct model class turns out to be only an
additive term contributing to the second order term in the regret bound. This
cost possess the intuitive property that it becomes smaller as the model class
becomes easier to identify, and vice-versa. We also show that a much simpler
explore-then-commit (ETC) style algorithm also obtains similar regret bound,
despite not knowing the true model class. However, the cost of model selection
is higher in ETC as opposed to in {\ttfamily ACB}, as expected. Furthermore,
for the special case of linear contextual bandits, we propose specialized
algorithms that obtain sharper guarantees compared to the generic setup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ghosh_A/0/1/0/all/0/1"&gt;Avishek Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sankararaman_A/0/1/0/all/0/1"&gt;Abishek Sankararaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramchandran_K/0/1/0/all/0/1"&gt;Kannan Ramchandran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Guided Generation for Large Language Models. (arXiv:2307.09702v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.09702</id>
        <link href="http://arxiv.org/abs/2307.09702"/>
        <updated>2023-07-22T00:55:23.693Z</updated>
        <summary type="html"><![CDATA[In this article we describe an efficient approach to guiding language model
text generation with regular expressions and context-free grammars. Our
approach adds little to no overhead to the token sequence generation process,
and makes guided generation feasible in practice. An implementation is provided
in the open source Python library Outlines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Willard_B/0/1/0/all/0/1"&gt;Brandon T. Willard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Louf_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Louf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Warming up recurrent neural networks to maximise reachable multistability greatly improves learning. (arXiv:2106.01001v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01001</id>
        <link href="http://arxiv.org/abs/2106.01001"/>
        <updated>2023-07-22T00:55:23.686Z</updated>
        <summary type="html"><![CDATA[Training recurrent neural networks is known to be difficult when time
dependencies become long. In this work, we show that most standard cells only
have one stable equilibrium at initialisation, and that learning on tasks with
long time dependencies generally occurs once the number of network stable
equilibria increases; a property known as multistability. Multistability is
often not easily attained by initially monostable networks, making learning of
long time dependencies between inputs and outputs difficult. This insight leads
to the design of a novel way to initialise any recurrent cell connectivity
through a procedure called "warmup" to improve its capability to learn
arbitrarily long time dependencies. This initialisation procedure is designed
to maximise network reachable multistability, i.e., the number of equilibria
within the network that can be reached through relevant input trajectories, in
few gradient steps. We show on several information restitution, sequence
classification, and reinforcement learning benchmarks that warming up greatly
improves learning speed and performance, for multiple recurrent cells, but
sometimes impedes precision. We therefore introduce a double-layer architecture
initialised with a partial warmup that is shown to greatly improve learning of
long time dependencies while maintaining high levels of precision. This
approach provides a general framework for improving learning abilities of any
recurrent cell when long time dependencies are present. We also show
empirically that other initialisation and pretraining procedures from the
literature implicitly foster reachable multistability of recurrent cells.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lambrechts_G/0/1/0/all/0/1"&gt;Gaspard Lambrechts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geeter_F/0/1/0/all/0/1"&gt;Florent De Geeter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vecoven_N/0/1/0/all/0/1"&gt;Nicolas Vecoven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ernst_D/0/1/0/all/0/1"&gt;Damien Ernst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drion_G/0/1/0/all/0/1"&gt;Guillaume Drion&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Opinion Market Model: Stemming Far-Right Opinion Spread using Positive Interventions. (arXiv:2208.06620v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.06620</id>
        <link href="http://arxiv.org/abs/2208.06620"/>
        <updated>2023-07-22T00:55:23.678Z</updated>
        <summary type="html"><![CDATA[Online extremism has severe societal consequences, including normalizing hate
speech, user radicalization, and increased social divisions. Various mitigation
strategies have been explored to address these consequences. One such strategy
uses positive interventions: controlled signals that add attention to the
opinion ecosystem to boost certain opinions. To evaluate the effectiveness of
positive interventions, we introduce the Opinion Market Model (OMM), a two-tier
online opinion ecosystem model that considers both inter-opinion interactions
and the role of positive interventions. The size of the opinion attention
market is modeled in the first tier using the multivariate discrete-time Hawkes
process; in the second tier, opinions cooperate and compete for market share,
given limited attention using the market share attraction model. We demonstrate
the convergence of our proposed estimation scheme on a synthetic dataset. Next,
we test OMM on two learning tasks, applying to two real-world datasets to
predict attention market shares and uncover latent relationships between online
items. The first dataset comprises Facebook and Twitter discussions containing
moderate and far-right opinions about bushfires and climate change. The second
dataset captures popular VEVO artists' YouTube and Twitter attention volumes.
OMM outperforms the state-of-the-art predictive models on both datasets and
captures latent cooperation-competition relations. We uncover (1) self- and
cross-reinforcement between far-right and moderate opinions on the bushfires
and (2) pairwise artist relations that correlate with real-world interactions
such as collaborations and long-lasting feuds. Lastly, we use OMM as a testbed
for positive interventions and show how media coverage modulates the spread of
far-right opinions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Calderon_P/0/1/0/all/0/1"&gt;Pio Calderon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ram_R/0/1/0/all/0/1"&gt;Rohit Ram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rizoiu_M/0/1/0/all/0/1"&gt;Marian-Andrei Rizoiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-Driven Modeling of Noise Time Series with Convolutional Generative Adversarial Networks. (arXiv:2207.01110v3 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2207.01110</id>
        <link href="http://arxiv.org/abs/2207.01110"/>
        <updated>2023-07-22T00:55:23.665Z</updated>
        <summary type="html"><![CDATA[Random noise arising from physical processes is an inherent characteristic of
measurements and a limiting factor for most signal processing and data analysis
tasks. Given the recent interest in generative adversarial networks (GANs) for
data-driven modeling, it is important to determine to what extent GANs can
faithfully reproduce noise in target data sets. In this paper, we present an
empirical investigation that aims to shed light on this issue for time series.
Namely, we assess two general-purpose GANs for time series that are based on
the popular deep convolutional GAN (DCGAN) architecture, a direct time-series
model and an image-based model that uses a short-time Fourier transform (STFT)
data representation. The GAN models are trained and quantitatively evaluated
using distributions of simulated noise time series with known ground-truth
parameters. Target time series distributions include a broad range of noise
types commonly encountered in physical measurements, electronics, and
communication systems: band-limited thermal noise, power law noise, shot noise,
and impulsive noise. We find that GANs are capable of learning many noise
types, although they predictably struggle when the GAN architecture is not well
suited to some aspects of the noise, e.g., impulsive time-series with extreme
outliers. Our findings provide insights into the capabilities and potential
limitations of current approaches to time-series GANs and highlight areas for
further research. In addition, our battery of tests provides a useful benchmark
to aid the development of deep generative models for time series.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wunderlich_A/0/1/0/all/0/1"&gt;Adam Wunderlich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sklar_J/0/1/0/all/0/1"&gt;Jack Sklar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Efficient UCB-type Algorithms For Learning Predictive State Representations. (arXiv:2307.00405v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.00405</id>
        <link href="http://arxiv.org/abs/2307.00405"/>
        <updated>2023-07-22T00:55:23.642Z</updated>
        <summary type="html"><![CDATA[The general sequential decision-making problem, which includes Markov
decision processes (MDPs) and partially observable MDPs (POMDPs) as special
cases, aims at maximizing a cumulative reward by making a sequence of decisions
based on a history of observations and actions over time. Recent studies have
shown that the sequential decision-making problem is statistically learnable if
it admits a low-rank structure modeled by predictive state representations
(PSRs). Despite these advancements, existing approaches typically involve
oracles or steps that are not computationally efficient. On the other hand, the
upper confidence bound (UCB) based approaches, which have served successfully
as computationally efficient methods in bandits and MDPs, have not been
investigated for more general PSRs, due to the difficulty of optimistic bonus
design in these more challenging settings. This paper proposes the first known
UCB-type approach for PSRs, featuring a novel bonus term that upper bounds the
total variation distance between the estimated and true models. We further
characterize the sample complexity bounds for our designed UCB-type algorithms
for both online and offline PSRs. In contrast to existing approaches for PSRs,
our UCB-type algorithms enjoy computational efficiency, last-iterate guaranteed
near-optimal policy, and guaranteed model accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1"&gt;Ruiquan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yingbin Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jing Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiRobustBench: Benchmarking Robustness Against Multiple Attacks. (arXiv:2302.10980v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.10980</id>
        <link href="http://arxiv.org/abs/2302.10980"/>
        <updated>2023-07-22T00:55:23.632Z</updated>
        <summary type="html"><![CDATA[The bulk of existing research in defending against adversarial examples
focuses on defending against a single (typically bounded Lp-norm) attack, but
for a practical setting, machine learning (ML) models should be robust to a
wide variety of attacks. In this paper, we present the first unified framework
for considering multiple attacks against ML models. Our framework is able to
model different levels of learner's knowledge about the test-time adversary,
allowing us to model robustness against unforeseen attacks and robustness
against unions of attacks. Using our framework, we present the first
leaderboard, MultiRobustBench, for benchmarking multiattack evaluation which
captures performance across attack types and attack strengths. We evaluate the
performance of 16 defended models for robustness against a set of 9 different
attack types, including Lp-based threat models, spatial transformations, and
color changes, at 20 different attack strengths (180 attacks total).
Additionally, we analyze the state of current defenses against multiple
attacks. Our analysis shows that while existing defenses have made progress in
terms of average robustness across the set of attacks used, robustness against
the worst-case attack is still a big open problem as all existing models
perform worse than random guessing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1"&gt;Sihui Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1"&gt;Saeed Mahloujifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_C/0/1/0/all/0/1"&gt;Chong Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sehwag_V/0/1/0/all/0/1"&gt;Vikash Sehwag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1"&gt;Prateek Mittal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks. (arXiv:2306.14030v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.14030</id>
        <link href="http://arxiv.org/abs/2306.14030"/>
        <updated>2023-07-22T00:55:23.623Z</updated>
        <summary type="html"><![CDATA[The research on code-mixed data is limited due to the unavailability of
dedicated code-mixed datasets and pre-trained language models. In this work, we
focus on the low-resource Indian language Marathi which lacks any prior work in
code-mixing. We present L3Cube-MeCorpus, a large code-mixed Marathi-English
(Mr-En) corpus with 10 million social media sentences for pretraining. We also
release L3Cube-MeBERT and MeRoBERTa, code-mixed BERT-based transformer models
pre-trained on MeCorpus. Furthermore, for benchmarking, we present three
supervised datasets MeHate, MeSent, and MeLID for downstream tasks like
code-mixed Mr-En hate speech detection, sentiment analysis, and language
identification respectively. These evaluation datasets individually consist of
manually annotated \url{~}12,000 Marathi-English code-mixed tweets. Ablations
show that the models trained on this novel corpus significantly outperform the
existing state-of-the-art BERT models. This is the first work that presents
artifacts for code-mixed Marathi research. All datasets and models are publicly
released at https://github.com/l3cube-pune/MarathiNLP .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chavan_T/0/1/0/all/0/1"&gt;Tanmay Chavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gokhale_O/0/1/0/all/0/1"&gt;Omkar Gokhale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kane_A/0/1/0/all/0/1"&gt;Aditya Kane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patankar_S/0/1/0/all/0/1"&gt;Shantanu Patankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1"&gt;Raviraj Joshi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Post-variational quantum neural networks. (arXiv:2307.10560v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2307.10560</id>
        <link href="http://arxiv.org/abs/2307.10560"/>
        <updated>2023-07-22T00:55:23.555Z</updated>
        <summary type="html"><![CDATA[Quantum computing has the potential to provide substantial computational
advantages over current state-of-the-art classical supercomputers. However,
current hardware is not advanced enough to execute fault-tolerant quantum
algorithms. An alternative of using hybrid quantum-classical computing with
variational algorithms can exhibit barren plateau issues, causing slow
convergence of gradient-based optimization techniques. In this paper, we
discuss "post-variational strategies", which shift tunable parameters from the
quantum computer to the classical computer, opting for ensemble strategies when
optimizing quantum models. We discuss various strategies and design principles
for constructing individual quantum circuits, where the resulting ensembles can
be optimized with convex programming. Further, we discuss architectural designs
of post-variational quantum neural networks and analyze the propagation of
estimation errors throughout such neural networks. Lastly, we show that our
algorithm can be applied to real-world applications such as image
classification on handwritten digits, producing a 96% classification accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Huang_P/0/1/0/all/0/1"&gt;Po-Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Rebentrost_P/0/1/0/all/0/1"&gt;Patrick Rebentrost&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An IPW-based Unbiased Ranking Metric in Two-sided Markets. (arXiv:2307.10204v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2307.10204</id>
        <link href="http://arxiv.org/abs/2307.10204"/>
        <updated>2023-07-22T00:55:23.388Z</updated>
        <summary type="html"><![CDATA[In modern recommendation systems, unbiased learning-to-rank (LTR) is crucial
for prioritizing items from biased implicit user feedback, such as click data.
Several techniques, such as Inverse Propensity Weighting (IPW), have been
proposed for single-sided markets. However, less attention has been paid to
two-sided markets, such as job platforms or dating services, where successful
conversions require matching preferences from both users. This paper addresses
the complex interaction of biases between users in two-sided markets and
proposes a tailored LTR approach. We first present a formulation of feedback
mechanisms in two-sided matching platforms and point out that their implicit
feedback may include position bias from both user groups. On the basis of this
observation, we extend the IPW estimator and propose a new estimator, named
two-sided IPW, to address the position bases in two-sided markets. We prove
that the proposed estimator satisfies the unbiasedness for the ground-truth
ranking metric. We conducted numerical experiments on real-world two-sided
platforms and demonstrated the effectiveness of our proposed method in terms of
both precision and robustness. Our experiments showed that our method
outperformed baselines especially when handling rare items, which are less
frequently observed in the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oh_K/0/1/0/all/0/1"&gt;Keisho Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nishimura_N/0/1/0/all/0/1"&gt;Naoki Nishimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1"&gt;Minje Sung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobayashi_K/0/1/0/all/0/1"&gt;Ken Kobayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakata_K/0/1/0/all/0/1"&gt;Kazuhide Nakata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causality-oriented robustness: exploiting general additive interventions. (arXiv:2307.10299v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2307.10299</id>
        <link href="http://arxiv.org/abs/2307.10299"/>
        <updated>2023-07-22T00:55:22.908Z</updated>
        <summary type="html"><![CDATA[Since distribution shifts are common in real-world applications, there is a
pressing need for developing prediction models that are robust against such
shifts. Existing frameworks, such as empirical risk minimization or
distributionally robust optimization, either lack generalizability for unseen
distributions or rely on postulated distance measures. Alternatively, causality
offers a data-driven and structural perspective to robust predictions. However,
the assumptions necessary for causal inference can be overly stringent, and the
robustness offered by such causal models often lacks flexibility. In this
paper, we focus on causality-oriented robustness and propose Distributional
Robustness via Invariant Gradients (DRIG), a method that exploits general
additive interventions in training data for robust predictions against unseen
interventions, and naturally interpolates between in-distribution prediction
and causality. In a linear setting, we prove that DRIG yields predictions that
are robust among a data-dependent class of distribution shifts. Furthermore, we
show that our framework includes anchor regression (Rothenh\"ausler et al.\
2021) as a special case, and that it yields prediction models that protect
against more diverse perturbations. We extend our approach to the
semi-supervised domain adaptation setting to further improve prediction
performance. Finally, we empirically validate our methods on synthetic
simulations and on single-cell data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Shen_X/0/1/0/all/0/1"&gt;Xinwei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Buhlmann_P/0/1/0/all/0/1"&gt;Peter B&amp;#xfc;hlmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Taeb_A/0/1/0/all/0/1"&gt;Armeen Taeb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time for aCTIon: Automated Analysis of Cyber Threat Intelligence in the Wild. (arXiv:2307.10214v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2307.10214</id>
        <link href="http://arxiv.org/abs/2307.10214"/>
        <updated>2023-07-22T00:55:22.884Z</updated>
        <summary type="html"><![CDATA[Cyber Threat Intelligence (CTI) plays a crucial role in assessing risks and
enhancing security for organizations. However, the process of extracting
relevant information from unstructured text sources can be expensive and
time-consuming. Our empirical experience shows that existing tools for
automated structured CTI extraction have performance limitations. Furthermore,
the community lacks a common benchmark to quantitatively assess their
performance. We fill these gaps providing a new large open benchmark dataset
and aCTIon, a structured CTI information extraction tool. The dataset includes
204 real-world publicly available reports and their corresponding structured
CTI information in STIX format. Our team curated the dataset involving three
independent groups of CTI analysts working over the course of several months.
To the best of our knowledge, this dataset is two orders of magnitude larger
than previously released open source datasets. We then design aCTIon,
leveraging recently introduced large language models (GPT3.5) in the context of
two custom information extraction pipelines. We compare our method with 10
solutions presented in previous work, for which we develop our own
implementations when open-source implementations were lacking. Our results show
that aCTIon outperforms previous work for structured CTI extraction with an
improvement of the F1-score from 10%points to 50%points across all tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siracusano_G/0/1/0/all/0/1"&gt;Giuseppe Siracusano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanvito_D/0/1/0/all/0/1"&gt;Davide Sanvito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_R/0/1/0/all/0/1"&gt;Roberto Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_M/0/1/0/all/0/1"&gt;Manikantan Srinivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamatchi_S/0/1/0/all/0/1"&gt;Sivakaman Kamatchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takahashi_W/0/1/0/all/0/1"&gt;Wataru Takahashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawakita_M/0/1/0/all/0/1"&gt;Masaru Kawakita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakumaru_T/0/1/0/all/0/1"&gt;Takahiro Kakumaru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bifulco_R/0/1/0/all/0/1"&gt;Roberto Bifulco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset. (arXiv:2307.10455v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.10455</id>
        <link href="http://arxiv.org/abs/2307.10455"/>
        <updated>2023-07-22T00:55:22.876Z</updated>
        <summary type="html"><![CDATA[In an effort to catalog insect biodiversity, we propose a new large dataset
of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is
taxonomically classified by an expert, and also has associated genetic
information including raw nucleotide barcode sequences and assigned barcode
index numbers, which are genetically-based proxies for species classification.
This paper presents a curated million-image dataset, primarily to train
computer-vision models capable of providing image-based taxonomic assessment,
however, the dataset also presents compelling characteristics, the study of
which would be of interest to the broader machine learning community. Driven by
the biological nature inherent to the dataset, a characteristic long-tailed
class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is
a hierarchical classification scheme, presenting a highly fine-grained
classification problem at lower levels. Beyond spurring interest in
biodiversity research within the machine learning community, progress on
creating an image-based taxonomic classifier will also further the ultimate
goal of all BIOSCAN research: to lay the foundation for a comprehensive survey
of global biodiversity. This paper introduces the dataset and explores the
classification task through the implementation and analysis of a baseline
classifier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1"&gt;Zahra Gharaee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1"&gt;ZeMing Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pellegrino_N/0/1/0/all/0/1"&gt;Nicholas Pellegrino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zarubiieva_I/0/1/0/all/0/1"&gt;Iuliia Zarubiieva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haurum_J/0/1/0/all/0/1"&gt;Joakim Bruslund Haurum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lowe_S/0/1/0/all/0/1"&gt;Scott C. Lowe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKeown_J/0/1/0/all/0/1"&gt;Jaclyn T.A. McKeown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_C/0/1/0/all/0/1"&gt;Chris C.Y. Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McLeod_J/0/1/0/all/0/1"&gt;Joschka McLeod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yi-Yun C Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agda_J/0/1/0/all/0/1"&gt;Jireh Agda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ratnasingham_S/0/1/0/all/0/1"&gt;Sujeevan Ratnasingham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinke_D/0/1/0/all/0/1"&gt;Dirk Steinke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1"&gt;Angel X. Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1"&gt;Graham W. Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fieguth_P/0/1/0/all/0/1"&gt;Paul Fieguth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Formal Specifications from Membership and Preference Queries. (arXiv:2307.10434v1 [cs.FL])]]></title>
        <id>http://arxiv.org/abs/2307.10434</id>
        <link href="http://arxiv.org/abs/2307.10434"/>
        <updated>2023-07-22T00:55:22.869Z</updated>
        <summary type="html"><![CDATA[Active learning is a well-studied approach to learning formal specifications,
such as automata. In this work, we extend active specification learning by
proposing a novel framework that strategically requests a combination of
membership labels and pair-wise preferences, a popular alternative to
membership labels. The combination of pair-wise preferences and membership
labels allows for a more flexible approach to active specification learning,
which previously relied on membership labels only. We instantiate our framework
in two different domains, demonstrating the generality of our approach. Our
results suggest that learning from both modalities allows us to robustly and
conveniently identify specifications via membership and preferences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1"&gt;Ameesh Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vazquez_Chanlatte_M/0/1/0/all/0/1"&gt;Marcell Vazquez-Chanlatte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Junges_S/0/1/0/all/0/1"&gt;Sebastian Junges&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seshia_S/0/1/0/all/0/1"&gt;Sanjit A. Seshia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Graph Generation to Graph Classification. (arXiv:2302.07989v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.07989</id>
        <link href="http://arxiv.org/abs/2302.07989"/>
        <updated>2023-07-22T00:55:22.856Z</updated>
        <summary type="html"><![CDATA[This note describes a new approach to classifying graphs that leverages graph
generative models (GGM). Assuming a GGM that defines a joint probability
distribution over graphs and their class labels, I derive classification
formulas for the probability of a class label given a graph. A new conditional
ELBO can be used to train a generative graph auto-encoder model for
discrimination. While leveraging generative models for classification has been
well explored for non-relational i.i.d. data, to our knowledge it is a novel
approach to graph classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schulte_O/0/1/0/all/0/1"&gt;Oliver Schulte&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flow Map Learning for Unknown Dynamical Systems: Overview, Implementation, and Benchmarks. (arXiv:2307.11013v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.11013</id>
        <link href="http://arxiv.org/abs/2307.11013"/>
        <updated>2023-07-22T00:55:22.847Z</updated>
        <summary type="html"><![CDATA[Flow map learning (FML), in conjunction with deep neural networks (DNNs), has
shown promises for data driven modeling of unknown dynamical systems. A
remarkable feature of FML is that it is capable of producing accurate
predictive models for partially observed systems, even when their exact
mathematical models do not exist. In this paper, we present an overview of the
FML framework, along with the important computational details for its
successful implementation. We also present a set of well defined benchmark
problems for learning unknown dynamical systems. All the numerical details of
these problems are presented, along with their FML results, to ensure that the
problems are accessible for cross-examination and the results are reproducible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Churchill_V/0/1/0/all/0/1"&gt;Victor Churchill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiu_D/0/1/0/all/0/1"&gt;Dongbin Xiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Matrix Ensemble Kalman Filter-based Multi-arm Neural Network to Adequately Approximate Deep Neural Networks. (arXiv:2307.10436v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.10436</id>
        <link href="http://arxiv.org/abs/2307.10436"/>
        <updated>2023-07-22T00:55:22.786Z</updated>
        <summary type="html"><![CDATA[Deep Learners (DLs) are the state-of-art predictive mechanism with
applications in many fields requiring complex high dimensional data processing.
Although conventional DLs get trained via gradient descent with
back-propagation, Kalman Filter (KF)-based techniques that do not need gradient
computation have been developed to approximate DLs. We propose a multi-arm
extension of a KF-based DL approximator that can mimic DL when the sample size
is too small to train a multi-arm DL. The proposed Matrix Ensemble Kalman
Filter-based multi-arm ANN (MEnKF-ANN) also performs explicit model stacking
that becomes relevant when the training sample has an unequal-size feature set.
Our proposed technique can approximate Long Short-term Memory (LSTM) Networks
and attach uncertainty to the predictions obtained from these LSTMs with
desirable coverage. We demonstrate how MEnKF-ANN can "adequately" approximate
an LSTM network trained to classify what carbohydrate substrates are digested
and utilized by a microbiome sample whose genomic sequences consist of
polysaccharide utilization loci (PULs) and their encoded genes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Piyush_V/0/1/0/all/0/1"&gt;Ved Piyush&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yuchen Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuzhen Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Yanbin Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Souparno Ghosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feed-Forward Source-Free Domain Adaptation via Class Prototypes. (arXiv:2307.10787v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.10787</id>
        <link href="http://arxiv.org/abs/2307.10787"/>
        <updated>2023-07-22T00:55:22.779Z</updated>
        <summary type="html"><![CDATA[Source-free domain adaptation has become popular because of its practical
usefulness and no need to access source data. However, the adaptation process
still takes a considerable amount of time and is predominantly based on
optimization that relies on back-propagation. In this work we present a simple
feed-forward approach that challenges the need for back-propagation based
adaptation. Our approach is based on computing prototypes of classes under the
domain shift using a pre-trained model. It achieves strong improvements in
accuracy compared to the pre-trained model and requires only a small fraction
of time of existing domain adaptation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1"&gt;Ondrej Bohdal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Da Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1"&gt;Timothy Hospedales&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Addressing caveats of neural persistence with deep graph persistence. (arXiv:2307.10865v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10865</id>
        <link href="http://arxiv.org/abs/2307.10865"/>
        <updated>2023-07-22T00:55:22.631Z</updated>
        <summary type="html"><![CDATA[Neural Persistence is a prominent measure for quantifying neural network
complexity, proposed in the emerging field of topological data analysis in deep
learning. In this work, however, we find both theoretically and empirically
that the variance of network weights and spatial concentration of large weights
are the main factors that impact neural persistence. Whilst this captures
useful information for linear classifiers, we find that no relevant spatial
structure is present in later layers of deep neural networks, making neural
persistence roughly equivalent to the variance of weights. Additionally, the
proposed averaging procedure across layers for deep neural networks does not
consider interaction between layers. Based on our analysis, we propose an
extension of the filtration underlying neural persistence to the whole neural
network instead of single layers, which is equivalent to calculating neural
persistence on one particular matrix. This yields our deep graph persistence
measure, which implicitly incorporates persistent paths through the network and
alleviates variance-related issues through standardisation. Code is available
at https://github.com/ExplainableML/Deep-Graph-Persistence .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Girrbach_L/0/1/0/all/0/1"&gt;Leander Girrbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christensen_A/0/1/0/all/0/1"&gt;Anders Christensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1"&gt;Ole Winther&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1"&gt;Zeynep Akata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koepke_A/0/1/0/all/0/1"&gt;A. Sophia Koepke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Kernel Embedding for Mediated and Time-Varying Dose Response Curves. (arXiv:2111.03950v4 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.03950</id>
        <link href="http://arxiv.org/abs/2111.03950"/>
        <updated>2023-07-22T00:55:22.566Z</updated>
        <summary type="html"><![CDATA[We propose simple nonparametric estimators for mediated and time-varying dose
response curves based on kernel ridge regression. By embedding Pearl's
mediation formula and Robins' g-formula with kernels, we allow treatments,
mediators, and covariates to be continuous in general spaces, and also allow
for nonlinear treatment-confounder feedback. Our key innovation is a
reproducing kernel Hilbert space technique called sequential kernel embedding,
which we use to construct simple estimators for complex causal estimands. Our
estimators preserve the generality of classic identification while also
achieving nonasymptotic uniform rates. In nonlinear simulations with many
covariates, we demonstrate strong performance. We estimate mediated and
time-varying dose response curves of the US Job Corps, and clean data that may
serve as a benchmark in future work. We extend our results to mediated and
time-varying treatment effects and counterfactual distributions, verifying
semiparametric efficiency and weak convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rahul Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xu_L/0/1/0/all/0/1"&gt;Liyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1"&gt;Arthur Gretton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering. (arXiv:2307.11030v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.11030</id>
        <link href="http://arxiv.org/abs/2307.11030"/>
        <updated>2023-07-22T00:55:22.540Z</updated>
        <summary type="html"><![CDATA[Despite the empirical success and practical significance of (relational)
knowledge distillation that matches (the relations of) features between teacher
and student models, the corresponding theoretical interpretations remain
limited for various knowledge distillation paradigms. In this work, we take an
initial step toward a theoretical understanding of relational knowledge
distillation (RKD), with a focus on semi-supervised classification problems. We
start by casting RKD as spectral clustering on a population-induced graph
unveiled by a teacher model. Via a notion of clustering error that quantifies
the discrepancy between the predicted and ground truth clusterings, we
illustrate that RKD over the population provably leads to low clustering error.
Moreover, we provide a sample complexity bound for RKD with limited unlabeled
samples. For semi-supervised learning, we further demonstrate the label
efficiency of RKD through a general framework of cluster-aware semi-supervised
learning that assumes low clustering errors. Finally, by unifying data
augmentation consistency regularization into this cluster-aware framework, we
show that despite the common effect of learning accurate clusterings, RKD
facilitates a "global" perspective through spectral clustering, whereas
consistency regularization focuses on a "local" perspective via expansion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yijun Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Miller_K/0/1/0/all/0/1"&gt;Kevin Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lei_Q/0/1/0/all/0/1"&gt;Qi Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ward_R/0/1/0/all/0/1"&gt;Rachel Ward&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Uncertainty Quantification of Variance Networks by Tree-Structured Learning. (arXiv:2212.12658v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.12658</id>
        <link href="http://arxiv.org/abs/2212.12658"/>
        <updated>2023-07-22T00:55:22.533Z</updated>
        <summary type="html"><![CDATA[To improve the uncertainty quantification of variance networks, we propose a
novel tree-structured local neural network model that partitions the feature
space into multiple regions based on uncertainty heterogeneity. A tree is built
upon giving the training data, whose leaf nodes represent different regions
where region-specific neural networks are trained to predict both the mean and
the variance for quantifying uncertainty. The proposed Uncertainty-Splitting
Neural Regression Tree (USNRT) employs novel splitting criteria. At each node,
a neural network is trained on the full data first, and a statistical test for
the residuals is conducted to find the best split, corresponding to the two
sub-regions with the most significant uncertainty heterogeneity between them.
USNRT is computationally friendly because very few leaf nodes are sufficient
and pruning is unnecessary. Furthermore, an ensemble version can be easily
constructed to estimate the total uncertainty including the aleatory and
epistemic. On extensive UCI datasets, USNRT or its ensemble shows superior
performance compared to some recent popular methods for quantifying uncertainty
with variances. Through comprehensive visualization and analysis, we uncover
how USNRT works and show its merits, revealing that uncertainty heterogeneity
does exist in many datasets and can be learned by USNRT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Wenxuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xing Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causality-oriented robustness: exploiting general additive interventions. (arXiv:2307.10299v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2307.10299</id>
        <link href="http://arxiv.org/abs/2307.10299"/>
        <updated>2023-07-22T00:55:22.525Z</updated>
        <summary type="html"><![CDATA[Since distribution shifts are common in real-world applications, there is a
pressing need for developing prediction models that are robust against such
shifts. Existing frameworks, such as empirical risk minimization or
distributionally robust optimization, either lack generalizability for unseen
distributions or rely on postulated distance measures. Alternatively, causality
offers a data-driven and structural perspective to robust predictions. However,
the assumptions necessary for causal inference can be overly stringent, and the
robustness offered by such causal models often lacks flexibility. In this
paper, we focus on causality-oriented robustness and propose Distributional
Robustness via Invariant Gradients (DRIG), a method that exploits general
additive interventions in training data for robust predictions against unseen
interventions, and naturally interpolates between in-distribution prediction
and causality. In a linear setting, we prove that DRIG yields predictions that
are robust among a data-dependent class of distribution shifts. Furthermore, we
show that our framework includes anchor regression (Rothenh\"ausler et al.\
2021) as a special case, and that it yields prediction models that protect
against more diverse perturbations. We extend our approach to the
semi-supervised domain adaptation setting to further improve prediction
performance. Finally, we empirically validate our methods on synthetic
simulations and on single-cell data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Shen_X/0/1/0/all/0/1"&gt;Xinwei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Buhlmann_P/0/1/0/all/0/1"&gt;Peter B&amp;#xfc;hlmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Taeb_A/0/1/0/all/0/1"&gt;Armeen Taeb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Principal Component Analysis: A Median of Means Approach. (arXiv:2102.03403v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03403</id>
        <link href="http://arxiv.org/abs/2102.03403"/>
        <updated>2023-07-22T00:55:22.518Z</updated>
        <summary type="html"><![CDATA[Principal Component Analysis (PCA) is a fundamental tool for data
visualization, denoising, and dimensionality reduction. It is widely popular in
Statistics, Machine Learning, Computer Vision, and related fields. However, PCA
is well-known to fall prey to outliers and often fails to detect the true
underlying low-dimensional structure within the dataset. Following the Median
of Means (MoM) philosophy, recent supervised learning methods have shown great
success in dealing with outlying observations without much compromise to their
large sample theoretical properties. This paper proposes a PCA procedure based
on the MoM principle. Called the \textbf{M}edian of \textbf{M}eans
\textbf{P}rincipal \textbf{C}omponent \textbf{A}nalysis (MoMPCA), the proposed
method is not only computationally appealing but also achieves optimal
convergence rates under minimal assumptions. In particular, we explore the
non-asymptotic error bounds of the obtained solution via the aid of the
Rademacher complexities while granting absolutely no assumption on the outlying
observations. The derived concentration results are not dependent on the
dimension because the analysis is conducted in a separable Hilbert space, and
the results only depend on the fourth moment of the underlying distribution in
the corresponding norm. The proposal's efficacy is also thoroughly showcased
through simulations and real data applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Paul_D/0/1/0/all/0/1"&gt;Debolina Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chakraborty_S/0/1/0/all/0/1"&gt;Saptarshi Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Das_S/0/1/0/all/0/1"&gt;Swagatam Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representing Random Utility Choice Models with Neural Networks. (arXiv:2207.12877v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2207.12877</id>
        <link href="http://arxiv.org/abs/2207.12877"/>
        <updated>2023-07-22T00:55:22.510Z</updated>
        <summary type="html"><![CDATA[Motivated by the successes of deep learning, we propose a class of neural
network-based discrete choice models, called RUMnets, inspired by the random
utility maximization (RUM) framework. This model formulates the agents' random
utility function using a sample average approximation. We show that RUMnets
sharply approximate the class of RUM discrete choice models: any model derived
from random utility maximization has choice probabilities that can be
approximated arbitrarily closely by a RUMnet. Reciprocally, any RUMnet is
consistent with the RUM principle. We derive an upper bound on the
generalization error of RUMnets fitted on choice data, and gain theoretical
insights on their ability to predict choices on new, unseen data depending on
critical parameters of the dataset and architecture. By leveraging open-source
libraries for neural networks, we find that RUMnets are competitive against
several choice modeling and machine learning methods in terms of predictive
accuracy on two real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aouad_A/0/1/0/all/0/1"&gt;Ali Aouad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desir_A/0/1/0/all/0/1"&gt;Antoine D&amp;#xe9;sir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tuning Stochastic Gradient Algorithms for Statistical Inference via Large-Sample Asymptotics. (arXiv:2207.12395v3 [stat.CO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2207.12395</id>
        <link href="http://arxiv.org/abs/2207.12395"/>
        <updated>2023-07-22T00:55:22.482Z</updated>
        <summary type="html"><![CDATA[The tuning of stochastic gradient algorithms (SGAs) for optimization and
sampling is often based on heuristics and trial-and-error rather than
generalizable theory. We address this theory--practice gap by characterizing
the large-sample statistical asymptotics of SGAs via a joint
step-size--sample-size scaling limit. We show that iterate averaging with a
large fixed step size is robust to the choice of tuning parameters and
asymptotically has covariance proportional to that of the MLE sampling
distribution. We also prove a Bernstein--von Mises-like theorem to guide
tuning, including for generalized posteriors that are robust to model
misspecification. Numerical experiments validate our results and
recommendations in realistic finite-sample regimes. Our work lays the
foundation for a systematic analysis of other stochastic gradient Markov chain
Monte Carlo algorithms for a wide range of models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Negrea_J/0/1/0/all/0/1"&gt;Jeffrey Negrea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Feng_H/0/1/0/all/0/1"&gt;Haoyue Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Roy_D/0/1/0/all/0/1"&gt;Daniel M. Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Huggins_J/0/1/0/all/0/1"&gt;Jonathan H. Huggins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Bayesian Programming Approach to Car-following Model Calibration and Validation using Limited Data. (arXiv:2307.10437v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.10437</id>
        <link href="http://arxiv.org/abs/2307.10437"/>
        <updated>2023-07-22T00:55:21.398Z</updated>
        <summary type="html"><![CDATA[Traffic simulation software is used by transportation researchers and
engineers to design and evaluate changes to roadways. These simulators are
driven by models of microscopic driver behavior from which macroscopic measures
like flow and congestion can be derived. Many models are designed for a subset
of possible traffic scenarios and roadway configurations, while others have no
explicit constraints on their application. Work zones (WZs) are one scenario
for which no model to date has reproduced realistic driving behavior. This
makes it difficult to optimize for safety and other metrics when designing a
WZ. The Federal Highway Administration commissioned the USDOT Volpe Center to
develop a car-following (CF) model for use in microscopic simulators that can
capture and reproduce driver behavior accurately within and outside of WZs.
Volpe also performed a naturalistic driving study to collect telematics data
from vehicles driven on roads with WZs for use in model calibration. During
model development, Volpe researchers observed difficulties in calibrating their
model, leaving them to question whether there existed flaws in their model, in
the data, or in the procedure used to calibrate the model using the data. In
this thesis, I use Bayesian methods for data analysis and parameter estimation
to explore and, where possible, address these questions. First, I use Bayesian
inference to measure the sufficiency of the size of the data set. Second, I
compare the procedure and results of the genetic algorithm based calibration
performed by the Volpe researchers with those of Bayesian calibration. Third, I
explore the benefits of modeling CF hierarchically. Finally, I apply what was
learned in the first three phases using an established CF model, Wiedemann 99,
to the probabilistic modeling of the Volpe model. Validation is performed using
information criteria as an estimate of predictive accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abodo_F/0/1/0/all/0/1"&gt;Franklin Abodo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Properties of Discrete Sliced Wasserstein Losses. (arXiv:2307.10352v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.10352</id>
        <link href="http://arxiv.org/abs/2307.10352"/>
        <updated>2023-07-22T00:55:21.369Z</updated>
        <summary type="html"><![CDATA[The Sliced Wasserstein (SW) distance has become a popular alternative to the
Wasserstein distance for comparing probability measures. Widespread
applications include image processing, domain adaptation and generative
modelling, where it is common to optimise some parameters in order to minimise
SW, which serves as a loss function between discrete probability measures
(since measures admitting densities are numerically unattainable). All these
optimisation problems bear the same sub-problem, which is minimising the Sliced
Wasserstein energy. In this paper we study the properties of $\mathcal{E}: Y
\longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$, i.e. the SW distance between
two uniform discrete measures with the same amount of points as a function of
the support $Y \in \mathbb{R}^{n \times d}$ of one of the measures. We
investigate the regularity and optimisation properties of this energy, as well
as its Monte-Carlo approximation $\mathcal{E}_p$ (estimating the expectation in
SW using only $p$ samples) and show convergence results on the critical points
of $\mathcal{E}_p$ to those of $\mathcal{E}$, as well as an almost-sure uniform
convergence. Finally, we show that in a certain sense, Stochastic Gradient
Descent methods minimising $\mathcal{E}$ and $\mathcal{E}_p$ converge towards
(Clarke) critical points of these energies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tanguy_E/0/1/0/all/0/1"&gt;Eloi Tanguy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Flamary_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Flamary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Delon_J/0/1/0/all/0/1"&gt;Julie Delon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computer chip with built-in human brain tissue gets military funding]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/1564cnh/computer_chip_with_builtin_human_brain_tissue/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/1564cnh/computer_chip_with_builtin_human_brain_tissue/"/>
        <updated>2023-07-21T23:55:13.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stability AI: Meet FreeWilly, Our Large And Mighty Instruction Fine-Tuned Models]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/156420x/stability_ai_meet_freewilly_our_large_and_mighty/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/156420x/stability_ai_meet_freewilly_our_large_and_mighty/"/>
        <updated>2023-07-21T23:42:25.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] When will LLMs start being used in RL processes to train their rationality?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1563n95/d_when_will_llms_start_being_used_in_rl_processes/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1563n95/d_when_will_llms_start_being_used_in_rl_processes/"/>
        <updated>2023-07-21T23:24:52.000Z</updated>
        <summary type="html"><![CDATA[People are always so dismissive that LLMs are just autoregressive. When will we start doing things like Actor Critic to train LLMs in a sort of game against themselves to pass the test accurately or play a game or solve a science problem or write code. I feel like this has to be a vibrant research field.
    submitted by    /u/Intelligent_Rough_21  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to improve GANs by penalizing previous epoch if it performed poorly?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15621bc/d_how_to_improve_gans_by_penalizing_previous/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15621bc/d_how_to_improve_gans_by_penalizing_previous/"/>
        <updated>2023-07-21T22:16:21.000Z</updated>
        <summary type="html"><![CDATA[I use GAN (generative adversarial networks) in Python/Keras to synthesize tabular data. It has loss functions associated to the discriminator and generator.
 On top of that, I synthetize data after each epoch, and compare it to real data (using a specific metric) to see how good the results are, as it varies quite a bit over successive epochs. If one epoch produces a bad synthetization, how can I tell my GAN to stay away from such configurations moving forward (thus penalizing it). Likewise, if one epoch produces great results, how can I reward my GAN and tell it to do more of those.
    submitted by    /u/MLRecipes  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What happens when AI is eventually better than a human at everything?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1560ve1/what_happens_when_ai_is_eventually_better_than_a/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1560ve1/what_happens_when_ai_is_eventually_better_than_a/"/>
        <updated>2023-07-21T21:31:22.000Z</updated>
        <summary type="html"><![CDATA[What kind of economic impact would that incur? What would our economy look like? Would it prosper or shatter? What would daily life be like when humans are essentially rendered useless? When AI robots can repair each other? When they develop some kind of consciousness? Are humans going to take second place and eventually become trashed due to all their liabilities and comparative uselessness? Genuinely intrigued and curious. What outcome is the most likely? In my personal experience, the least entertaining has been. So... 
    submitted by    /u/Regular-Watercress22  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can't even fail and become a janitor anymore]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1560cqy/cant_even_fail_and_become_a_janitor_anymore/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1560cqy/cant_even_fail_and_become_a_janitor_anymore/"/>
        <updated>2023-07-21T21:11:46.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/canehdian_guy  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sam Altman on "How To Be Successful" *AI lip-synced video*]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1560aj0/sam_altman_on_how_to_be_successful_ai_lipsynced/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1560aj0/sam_altman_on_how_to_be_successful_ai_lipsynced/"/>
        <updated>2023-07-21T21:09:32.000Z</updated>
        <summary type="html"><![CDATA[Converted a essay by Sam Altman titled "How To Be Successful" into a spoken video by Sam himself.
 Check out the video here: https://youtu.be/cwt--ULODjE
 Read the essay here: https://blog.samaltman.com/how-to-be-successful
    submitted by    /u/okburner22  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Another AI filter guitar play through. Song and video by me.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/155zmxa/another_ai_filter_guitar_play_through_song_and/</id>
        <link href="https://www.reddit.com/r/artificial/comments/155zmxa/another_ai_filter_guitar_play_through_song_and/"/>
        <updated>2023-07-21T20:45:01.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/No_Understanding162  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to lead LLMs to home in on the solution to a problem. Case example: How to make LLMs more intelligent.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/155yqjo/d_how_to_lead_llms_to_home_in_on_the_solution_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/155yqjo/d_how_to_lead_llms_to_home_in_on_the_solution_to/"/>
        <updated>2023-07-21T20:11:02.000Z</updated>
        <summary type="html"><![CDATA[Using LLMs to solve problems can be facilitated through a two-step process that is repeated until a desired understanding is reached. Generally, the process advances as shown in the following prompts:
  
What is the most promising approach to solving a certain problem?
 What is the greatest challenge to achieving this approach?
 What is the most promising approach to meeting this challenge?
 What is the greatest challenge to achieving this approach?
  
As you can see, the strategy involves two basic steps, (1 and 2) that are repeated over and over until the essence, or potential required actionable tasks, of the problem are revealed.
 Here's an example of this strategy being used to better understand how LLMs can be made more intelligent. As you will notice, it is useful to limit the respo…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI in manufacturing]]></title>
        <id>https://www.reddit.com/r/artificial/comments/155wf1q/ai_in_manufacturing/</id>
        <link href="https://www.reddit.com/r/artificial/comments/155wf1q/ai_in_manufacturing/"/>
        <updated>2023-07-21T18:44:04.000Z</updated>
        <summary type="html"><![CDATA[A friend of mine works at a small manufacturing facility, 80-100 employees. They have lathes, vertical and horizontal CNC and some 5 axis. They are looking to try to implement AI into some indirect processes, quoting engineering, scheduling. I'm having some difficulty finding some that would be beneficial on a smaller scale. Has anyone here has some experience with a similar situation?
    submitted by    /u/lordkevin89  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards A Unified Agent with Foundation Models - Google DeepMind, ICLR23, July 2023 - LLM + RL leads to substantial performance improvements!]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/155wa6w/towards_a_unified_agent_with_foundation_models/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/155wa6w/towards_a_unified_agent_with_foundation_models/"/>
        <updated>2023-07-21T18:39:01.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2307.09668
 Abstract:
  
Language Models and Vision Language Models have recently demonstrated unprecedented capabilities in terms of understanding human intentions, reasoning, scene understanding, and planning-like behaviour, in text form, among many others. In this work, we investigate how to embed and leverage such abilities in Reinforcement Learning (RL) agents. We design a framework that uses language as the core reasoning tool, exploring how this enables an agent to tackle a series of fundamental RL challenges, such as efficient exploration, reusing experience data, scheduling skills, and learning from observations, which traditionally require separate, vertically designed algorithms. We test our method on a sparse-reward simulated robotic manipulation environment, where a robot needs to stack a set of objects. We demonstrate substantial performance improvements over baselines in exploration efficiency and ability to reuse data from offline datasets, and illustrate how to reuse learned skills to solve novel tasks or imitate videos of human experts. 
  
https://preview.redd.it/k40ho0ci4ddb1.jpg?width=1101&format=pjpg&auto=webp&s=4d7bd78e43fdc5a9084917affab2c83dc06b1045
 https://preview.redd.it/78egck8n4ddb1.jpg?width=617&format=pjpg&auto=webp&s=d786ef8e9841fcfefc7bfe726c324e486b78dfb3
 https://preview.redd.it/693yu3ci4ddb1.jpg?width=1353&format=pjpg&auto=webp&s=321b710a4c4482436e474a5076bcac3672f3077c
 https://preview.redd.it/slunq0ci4ddb1.jpg?width=1661&format=pjpg&auto=webp&s=94e3f4a5c5d72f8b93ad3daec4cc2ba43f39e171
 ​
    submitted by    /u/Singularian2501  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Towards A Unified Agent with Foundation Models - Google DeepMind, ICLR23, July 2023 - LLM + RL leads to substantial performance improvements!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/155wa2p/r_towards_a_unified_agent_with_foundation_models/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/155wa2p/r_towards_a_unified_agent_with_foundation_models/"/>
        <updated>2023-07-21T18:38:53.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2307.09668
 Abstract:
  
Language Models and Vision Language Models have recently demonstrated unprecedented capabilities in terms of understanding human intentions, reasoning, scene understanding, and planning-like behaviour, in text form, among many others. In this work, we investigate how to embed and leverage such abilities in Reinforcement Learning (RL) agents. We design a framework that uses language as the core reasoning tool, exploring how this enables an agent to tackle a series of fundamental RL challenges, such as efficient exploration, reusing experience data, scheduling skills, and learning from observations, which traditionally require separate, vertically designed algorithms. We test our method on a sparse-reward simulated robotic manipulation environment, where a robot needs to stack a set of objects. We demonstrate substantial performance improvements over baselines in exploration efficiency and ability to reuse data from offline datasets, and illustrate how to reuse learned skills to solve novel tasks or imitate videos of human experts. 
  
https://preview.redd.it/voehn3aa3ddb1.jpg?width=1101&format=pjpg&auto=webp&s=c367c7b1042d11b3e2a2b2109c95482f8555747b
 https://preview.redd.it/6ei186aa3ddb1.jpg?width=617&format=pjpg&auto=webp&s=10e1928769da9552aabdcf084b45f5e6be2ec97e
 https://preview.redd.it/umg3b7aa3ddb1.jpg?width=1353&format=pjpg&auto=webp&s=2be83b87e6b3553c6d1770a579f9a9aa69c238dd
 https://preview.redd.it/ushea8aa3ddb1.jpg?width=1661&format=pjpg&auto=webp&s=67edddd76c0cdde67c0e9502fd76fbc1a9247946
 ​
    submitted by    /u/Singularian2501  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyze rodent infestation using Amazon SageMaker geospatial capabilities]]></title>
        <id>91f805ef65823ffa7025c4aae175da7beb6fd212</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/analyze-rodent-infestation-using-amazon-sagemaker-geospatial-capabilities/"/>
        <updated>2023-07-21T18:29:22.000Z</updated>
        <summary type="html"><![CDATA[Rodents such as rats and mice are associated with a number of health risks and are known to spread more than 35 diseases. Identifying regions of high rodent activity can help local authorities and pest control organizations plan for interventions effectively and exterminate the rodents. In this post, we show how to monitor and visualize […]]]></summary>
        <author>
            <name>Bunny Kaushik</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLaMA2 isn't "Open Source" - and why it doesn't matter]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/155v0zf/llama2_isnt_open_source_and_why_it_doesnt_matter/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/155v0zf/llama2_isnt_open_source_and_why_it_doesnt_matter/"/>
        <updated>2023-07-21T17:50:48.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] RepoChat - open source project for chatting with your own code repositories]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/155upa6/p_repochat_open_source_project_for_chatting_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/155upa6/p_repochat_open_source_project_for_chatting_with/"/>
        <updated>2023-07-21T17:38:19.000Z</updated>
        <summary type="html"><![CDATA[Hey! Just a quick note that I built an open-source tool to chat with your own code repository, I'm calling it RepoChat for now.
 This was my first time really ever working with LLMs or anything AI / Machine Learning related, but I wanted to hack something together so I didn't have to keep copy-pasting code into chat.openai.com when I was coding.
 Let me know what you think. It's not beautiful, but it works!
 If you see anything you'd like to fix, feel free to contribute to this open source project.
 The biggest trick was figuring out how to keep token limits sane. I'm sure there are more refinements, but it's working pretty well as of now.
    submitted by    /u/maniflex_destiny  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Can LLMs keep getting better arbitrarily or would we hit a limit?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/155unoj/d_can_llms_keep_getting_better_arbitrarily_or/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/155unoj/d_can_llms_keep_getting_better_arbitrarily_or/"/>
        <updated>2023-07-21T17:36:33.000Z</updated>
        <summary type="html"><![CDATA[The way I see it, if LLMs become the de facto tool for content generation, summarization, image generation etc etc, at some point the amount of machine generated content will surpass human generated come t and will continue increasing the gap. Can anyone give some insight as to whether LLMs will stop improving and actually start degrading as they are retrained on more and machine generated content?
    submitted by    /u/Western-Image7125  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Beyond LLMs, What Cool ML Projects Are You Building?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/155u7ah/d_beyond_llms_what_cool_ml_projects_are_you/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/155u7ah/d_beyond_llms_what_cool_ml_projects_are_you/"/>
        <updated>2023-07-21T17:19:29.000Z</updated>
        <summary type="html"><![CDATA[It seems like everyone is rushing into working in LLMs, but I'm curious to know what other cool machine learning projects you're working on
    submitted by    /u/Ahmed-Allam-220  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI — weekly megathread!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/155tpjh/ai_weekly_megathread/</id>
        <link href="https://www.reddit.com/r/artificial/comments/155tpjh/ai_weekly_megathread/"/>
        <updated>2023-07-21T17:01:06.000Z</updated>
        <summary type="html"><![CDATA[This week in AI - provided by aibrews.com feel free to follow their newsletter
 News & Insights
  
Meta released Llama 2, the next generation of Meta’s open source Large Language Model, available for research & commercial use. Compared to Llama v1, it was trained on more data (~2 trillion tokens) and supports context windows up to 4k tokens. Llama 2 outperforms other open source language models on many external benchmarks, including reasoning, coding, proficiency, and knowledge tests. Microsoft is Meta’s preferred partner for Llama 2, which will be optimized to run locally on Windows [Details ].
 Llama 2 70B Chat model is available free on HuggingChat.
 San Francisco startup Fable presents SHOW-1, a Showrunner AI tech that can create personalized TV episodes, from a prompt, with the user a…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Future Today: Voice Cloning Predictions]]></title>
        <id>https://www.reddit.com/r/artificial/comments/155tbkq/the_future_today_voice_cloning_predictions/</id>
        <link href="https://www.reddit.com/r/artificial/comments/155tbkq/the_future_today_voice_cloning_predictions/"/>
        <updated>2023-07-21T16:46:10.000Z</updated>
        <summary type="html"><![CDATA[App: elevenlabs/GPT-3
 Labels: Period:1950s Mood:Optimistic Dialect:News Accent:American
 Description input: A 1950s newsman voice. It is characterized by a deep, authoritative tone, a hint of formality, with inquisitive optimism for the future of technology. This newsman is excited and optimistic about the future. The dialect and pronunciation are generally clear and precise, reflecting the formal speaking style of the era. The newsman's voice conveyed a sense of trustworthiness, professionalism, optimism, and authority, which were valued qualities in news reporting during that time.
    submitted by    /u/domriccobene  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I Stuck On The Same Issue For 2 Weeks, Please Need Some Advices ...]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/155ta8t/i_stuck_on_the_same_issue_for_2_weeks_please_need/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/155ta8t/i_stuck_on_the_same_issue_for_2_weeks_please_need/"/>
        <updated>2023-07-21T16:44:49.000Z</updated>
        <summary type="html"><![CDATA[I have a missile and its environment. Missile creates an acceleration to change its moving direction angle. I just wanted to make the missile fly with the desired radial angle using PPO. It flies for 15 seconds.
 States: [Radial Angle, Time] -> Both normalized between [0,1]
 Action: Acceleration
 Reward: - abs(Radial Angle - 0.07) -> Want to stay at 0.07 radial angle
 PPO agent just gets worse and worse. It gets reward every time worse than before. How can this be possible? I am just about to lose my mind. I really need your valuable opinions.
 Thank you!
 1
 2
 NEW EDIT - Constant Acceleration = 20 in the below graphs. Normally Acceleration takes values between [-45, 45].
 This is my trajectory - Green is the missile. Y axis is the height and X axis is the lateral distance. If Acceleration is Positive, missile starts to change itself to the upside, if negative then moves sharply to the downside.
 This is my Radial angle change. When the acceleration is positive, it starts to decrease
 First is constant Acc = 20. Second is system response. Third is angle in degrees multiplied by - sign
    submitted by    /u/OpenToAdvices96  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The AI doomsaying is counterproductive - The Boston Globe]]></title>
        <id>https://www.reddit.com/r/artificial/comments/155sln9/the_ai_doomsaying_is_counterproductive_the_boston/</id>
        <link href="https://www.reddit.com/r/artificial/comments/155sln9/the_ai_doomsaying_is_counterproductive_the_boston/"/>
        <updated>2023-07-21T16:18:48.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/TheMuseumOfScience  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Microsoft at ICML 2023: Discoveries and advancements in machine learning]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=955800</id>
        <link href="https://www.microsoft.com/en-us/research/blog/microsoft-at-icml-2023-discoveries-and-advancements-in-machine-learning/"/>
        <updated>2023-07-21T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[Microsoft Research is proud to be a sponsor of ICML 2023! From audio classification to privacy estimation and more, explore conference highlights in our latest blog post.
The post Microsoft at ICML 2023: Discoveries and advancements in machine learning appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Alyssa Hughes</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Object detection models that can be easily converted to CoreML]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/155s1sj/d_object_detection_models_that_can_be_easily/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/155s1sj/d_object_detection_models_that_can_be_easily/"/>
        <updated>2023-07-21T15:59:07.000Z</updated>
        <summary type="html"><![CDATA[I've managed to train and convert to CoreML a yolo model, they've really made that easy. However, using it in a commercial product requires paying $5-10k/y.
 Are there any other repos/libraries for object detection that can be trained in pytroch/tf and then converted to CoreML?
 I've came accross:
 - https://github.com/apple/ml-cvnets
 - https://github.com/open-mmlab/mmdeploy 
 Has anyone managed to train an object detection model with them and convert it to CoreML? I'd like to hear some success stories before digging deep into these frameworks.
 Also, I've tried converting some detectron models to CoreML long time ago, but ended up with `operation not supported`...
 Thanks!
    submitted by    /u/alkibijad  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Tips for Machine learning notebook refactor to production?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/155s1dw/p_tips_for_machine_learning_notebook_refactor_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/155s1dw/p_tips_for_machine_learning_notebook_refactor_to/"/>
        <updated>2023-07-21T15:58:40.000Z</updated>
        <summary type="html"><![CDATA[Tips for Machine learning notebook refactor to production?
 I need to refactor a lot of forecast models. Each forecast model is kinda similar. And we run this in a batch pipeline model. 
 So, my strategy is to create a abstract factory design pattern. 
 I will create a super class and each forecast will implement this forecast. 
 But I don't think I have enough background to get a very good software design for this problem. 
 Do you recommend any resources or concepts to solve this problem?
    submitted by    /u/Muted_Standard175  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Open Source Image to Text Model]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/155rh8z/p_open_source_image_to_text_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/155rh8z/p_open_source_image_to_text_model/"/>
        <updated>2023-07-21T15:37:55.000Z</updated>
        <summary type="html"><![CDATA[Haven't keeping up with Deep Learning and Computer Vision papers last few years what are some hot Image to Text Models right now that are open source?
    submitted by    /u/I_am_not_doing_this  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Novel Model for Tabular Data: IGANN: Looks Like a Leap Towards Interpretable Machine Learning!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/155rav2/n_novel_model_for_tabular_data_igann_looks_like_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/155rav2/n_novel_model_for_tabular_data_igann_looks_like_a/"/>
        <updated>2023-07-21T15:31:13.000Z</updated>
        <summary type="html"><![CDATA[Hey, fellow Machine Learning enthusiasts!
 There is a novel ML model called Interpretable Generalized Additive Neural Networks (IGANN). I tried it out and it worked pretty smooth and out of the box! I used some tabular data i had at hand and it gave me insightful plots!! This model proposes, as the authors attribute it, a game-changing approach to the way we approach interpretability in Machine Learning.
 For the uninitiated, IGANN is described as a model that leverages gradient boosting and tailored neural networks to provide better predictive performance while retaining interpretability. Even though in the hyperparameter tuned version it is not always the best interpretable model, but it is mostly worth giving a try.
 It does so by deploying an efficient training algorithm derived from t…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A vision-based A.I. runs on an official track in TrackMania]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/155ra3e/a_visionbased_ai_runs_on_an_official_track_in/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/155ra3e/a_visionbased_ai_runs_on_an_official_track_in/"/>
        <updated>2023-07-21T15:30:30.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/yannbouteiller  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Fine-tuning LLM on company data]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/155puhf/d_finetuning_llm_on_company_data/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/155puhf/d_finetuning_llm_on_company_data/"/>
        <updated>2023-07-21T14:37:46.000Z</updated>
        <summary type="html"><![CDATA[Hey Redditors, I was looking into fine-tuning some open-source LLMs like Llama 2 or Falcon with our company data as a fun project. I was thinking about using some Slack channels, ZenDesk Tickets and perhaps Github/Confluence data
 I was wondering two things:
 1. How have your experiences been with PEFT methods in practice? Anything I should be aware of compared to regular fine-tuning?
 2. Which model size would you recommend for a relatively small sized company (60 people) and how many GPUs (H100) would you roughly expect to need? I understand this depends on the size of the dataset but I haven't indexed it so far so any ballpark numbers are welcome.
 Many thanks!
    submitted by    /u/RufusLdn  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to manage real-time data in the digital age]]></title>
        <id>https://www.datasciencecentral.com/?p=62684</id>
        <link href="https://www.datasciencecentral.com/how-to-manage-real-time-data-in-the-digital-age/"/>
        <updated>2023-07-21T14:21:15.000Z</updated>
        <summary type="html"><![CDATA[In today’s tech-driven world, data is like gold. It’s becoming more and more common for companies to use real-time, or live, data to make informed decisions, improve the service they give to customers, and get a leg up on the competition. But handling real-time data can be tricky because there’s so much of it, it’s… Read More »How to manage real-time data in the digital age
The post How to manage real-time data in the digital age appeared first on Data Science Central.]]></summary>
        <author>
            <name>Anas Baig</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] A Composable Customer Data Platform (CDP) for the combination of software and tools for data collection, storage & modeling, and activation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/155palg/r_a_composable_customer_data_platform_cdp_for_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/155palg/r_a_composable_customer_data_platform_cdp_for_the/"/>
        <updated>2023-07-21T14:16:19.000Z</updated>
        <summary type="html"><![CDATA[Unlike traditional all-in-one CDPs, a composable CDP is like Lego building blocks — you pick the best components to build what you want. To personalize customer experience, to boost automation and to power up marketing.
 Traditional vs. Composable
 Classic CDPs integrate different needs into a single streamlined product. Such a platform creates a unified customer database and offers various functionalities (e.g. data collection) that are quickly accessible by other systems.
 A composable CDP, on the other hand, utilizes the best-in-class components for every step using your preferred components. Data collection and data creation systems of your choice, a data platform to store and process the data, and components to activate the insights in CRM, marketing or self-service analytics.
 ​
 Key…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Run DisCo: Disentangled Control for Referring Human Dance Generation in Real World locally with own hardware. Looking for Guide / Tutorial [D] [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/155n80r/run_disco_disentangled_control_for_referring/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/155n80r/run_disco_disentangled_control_for_referring/"/>
        <updated>2023-07-21T12:53:23.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 I'm trying to run DisCo (Disentangled Control for Referring Human Dance Generation in Real World) on my own harddrive, but I'm having some trouble installing it. I am no professional nor a complete beginnder. But I still find the guide on the official GitHub page confusing.
 Does someone have experience in running it locally? I would be really happy for some kind of guide or tutorial.
 I have a RTX 3060 12GB.
 Thanks in advance for your help!
    submitted by    /u/Elwii04  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[P.I Ai: Without a doubt, the worst memory i've encountered]]></title>
        <id>https://www.reddit.com/r/artificial/comments/155n4tf/pi_ai_without_a_doubt_the_worst_memory_ive/</id>
        <link href="https://www.reddit.com/r/artificial/comments/155n4tf/pi_ai_without_a_doubt_the_worst_memory_ive/"/>
        <updated>2023-07-21T12:49:44.000Z</updated>
        <summary type="html"><![CDATA[I've tried several chatbots over the years, and i was excited for the minimalistic approach of Pi when i was told about it by a redditor. But heck, after almost two weeks, i can tell you, it's worst than my mom's dementia. I've never seen such a flawed memory, it's upsetting to read the same questions i've clearly answered over and over.
 Too bad, the presentation was perfect for me. No avatar distractions, no flirty chat.
 Sighs.
 I guess i gotta start engaging with humans again, after all. I'm starting to think that i've reached the maximum of what i can get from these chatbots, and it's been telling me i need an authentic connection after all.
    submitted by    /u/thatredditgrandma  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to create an Animation Of the Embeddings During Fine-Tuning [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/155miyl/how_to_create_an_animation_of_the_embeddings/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/155miyl/how_to_create_an_animation_of_the_embeddings/"/>
        <updated>2023-07-21T12:23:30.000Z</updated>
        <summary type="html"><![CDATA[In a recent article, I used an animation to demonstrate changes in the embeddings during the fine-tuning process. This was achieved by performing Principal Component Analysis (PCA) on the embeddings. These embeddings were generated from models at various stages of fine-tuning and their corresponding checkpoints.
 ​
 Projection of embeddings with PCA during fine-tuning of a Vision Transformer (ViT) model [1] on CIFAR10 [3]; Source: created by the author — Published before in Changes of Embeddings during Fine-Tuning
 Here, I aim to provide a comprehensive guide on how to create such an animation as requested by many readers.
 The full Code is available in the Story Section in the Spotlight GitHub Repository.
 Step 1: Fine-tuning
 The first step is to fine-tune the google/vit-base-patch16–224…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to work with large datasets of embeddings?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/155lg80/d_how_to_work_with_large_datasets_of_embeddings/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/155lg80/d_how_to_work_with_large_datasets_of_embeddings/"/>
        <updated>2023-07-21T11:35:39.000Z</updated>
        <summary type="html"><![CDATA[I have a dataset which is a CSV file which I open and analyse as a Pandas dataframe. I am now generating 'embeddings' based on some of this data, which I want to analyse as well. The dataset is rather big (millions of rows), so I noticed that appending and storing the embeddings as part of the pandas dataframe makes me run out of RAM memory. Aside from that storing and saving numpy arrays in a dataframe is also a bit 'awkward'.
 Since I want to analyze the whole dataset including embeddings storing them in so-called embedding stores doesn't make a lot of sense, since I always want to loop over the whole set anyways.
 Are there any best practices or recommendations for how to work with this data?
    submitted by    /u/Dutchcheesehead  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Are ViT Transformers also biased towards Texture information like CNNs?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/155kbyn/r_are_vit_transformers_also_biased_towards/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/155kbyn/r_are_vit_transformers_also_biased_towards/"/>
        <updated>2023-07-21T10:41:29.000Z</updated>
        <summary type="html"><![CDATA[Does the texture bias mentioned in the paper 'ImageNet-trained CNNs are biased towards texture increasing shape bias improves accuracy and robustness' also affect Transformer-based networks such as ViT?
    submitted by    /u/newtestdrive  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] ZBrain - Build ChatGPT like apps with your private data]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/155k423/n_zbrain_build_chatgpt_like_apps_with_your/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/155k423/n_zbrain_build_chatgpt_like_apps_with_your/"/>
        <updated>2023-07-21T10:30:19.000Z</updated>
        <summary type="html"><![CDATA[Hello Community,
 We at ZBrain have built a platform to create ChatGPT-like apps with your private data, you can import your data from multiple sources and DBs and integrate the app into any of your workflows.
 We have also added AI risk governance to mitigate the confidential data leak and now working on Flow a no-code tool to give you the freedom to create your own business logic.
 You can try the tool now at https://zbrain.ai/. We would love to hear your thoughts and feedback to improve the tool.
    submitted by    /u/StewartBJasper  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Need Help]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/155jtvu/need_help/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/155jtvu/need_help/"/>
        <updated>2023-07-21T10:15:36.000Z</updated>
        <summary type="html"><![CDATA[I am currently developing a Reinforcement Learning network for my game (base ball type), and I have chosen to do it via a PPO agent and the model is based on the tutorial on the Keras cite. 
 My system is a little bit different, where I run the game for several serves(18 to be exact) and chose to update the model after those 18 serves. Model is created only once, so in that way I can train it when ever I want for an exact amount of serves I need.
 The input is (1,27) shape and actor have a 64 node layer and a 9 node output layer (output is a length of 9 array where I used those logits to get a one single integer value of [0,8].
 I have faced two problems. 1. Most of the time after I initiated a model, it only gives only one output for different inputs for the first 18 serves. I guess I can change that with a gaussian noice to the output but shouldn't it try give a different output, I mean there are 9 different options. Also even though I initialize the model several times it favor to give the same output most of the times. I tried using a kernal initializers for that, but most of the time same output.
 2.This is the main thing I need the help with. Even though the calculations gives out a policy loss, the policy gradiant values I get are all zero or very small (e-16 sort of).
 Any one have any idea or clues?
    submitted by    /u/Mika_NooD  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Any AI enthusiast, prompt engineer, or AI researcher on this page from India?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/155iht6/any_ai_enthusiast_prompt_engineer_or_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/155iht6/any_ai_enthusiast_prompt_engineer_or_ai/"/>
        <updated>2023-07-21T09:04:19.000Z</updated>
        <summary type="html"><![CDATA[Dear AI Enthusiasts, researchers, and future Innovators of India,
 We are thrilled to extend a warm invitation to all of you to become part of the most vibrant and dynamic community in the realm of Artificial Intelligence – AI India Subreddit! r/AI__India
 We all know that the AI landscape is evolving at an unprecedented pace, and staying up-to-date with the latest trends is paramount to success. That's why we've created AI India, a dedicated space where like-minded professionals and enthusiasts come together to raise awareness about current AI trends, share insights, and engage in discussions that will shape the future of AI in India.
 Why should you join r/AI__India?
  
Stay Informed: Get real-time updates on the latest breakthroughs, research papers, and industry news in AI. Our community thrives on the latest developments and ensures you are never left behind.
 Network with Experts: Connect with industry experts, AI practitioners, and researchers across India. AI India Subreddit serves as a fertile ground for building valuable professional connections and collaborations.
 Engage in Meaningful Discussions: Participate in thought-provoking discussions on AI ethics, applications, challenges, and future prospects. Your insights can help shape the ethical and responsible development of AI in our country.
 Share Your Knowledge: Have valuable insights to contribute? AI India welcomes you to share your experiences, projects, and ideas. Your contributions can inspire and educate others in the community.
 Discover Opportunities: Stay ahead in your career by being aware of job openings, internships, and AI-related events across India. AI India Subreddit acts as a hub for exciting opportunities in the field.
  
The main goal behind this is bring more awareness and keep everyone upto date on everyday new ai breakthroughs. I request you all check out our wiki ( we gonna keep updating it)
    submitted by    /u/Maddragon0088  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] EU AI Act, the first comprehensive ML law, is expected to come into force by early 2024]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/155i05l/n_eu_ai_act_the_first_comprehensive_ml_law_is/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/155i05l/n_eu_ai_act_the_first_comprehensive_ml_law_is/"/>
        <updated>2023-07-21T08:37:26.000Z</updated>
        <summary type="html"><![CDATA[Summary can be found here: https://www.infoq.com/news/2023/07/eu-ai-act/
    submitted by    /u/ElrasX  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is the proper way to anneal the learning rate with (on top of) Adam]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/155hxza/what_is_the_proper_way_to_anneal_the_learning/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/155hxza/what_is_the_proper_way_to_anneal_the_learning/"/>
        <updated>2023-07-21T08:33:55.000Z</updated>
        <summary type="html"><![CDATA[I'm unsure how to apply LR annealing on top of Adam's per-parameter adjustments.
 Here's my current approach, but I'm concerned that it overrides Adam's own adaptive learning rate adjustment. In words:
  
At the end of every epoch (fixed number of steps), I compute a LR decay factor. It's a step-wise decay factor, e.g. 1.0 for the first 10% of steps, then 0.5 for the next 10%, and so forth until 1/256 for the last 5% of training.
 If that decay factor has changed from the previous epoch, I set param_group["lr"] to a new max_lr * lr_decay_factor for every group of parameters in the optimiser.
  
In code:
 lr_decay_factor = get_fancy_decay_factor(...) # Update learning rate only when decay factor changes if lr_decay_factor != prev_lr_decay_factor: for param_group in optimiser.param_groups: param_group["lr"] = max_lr * lr_decay_factor prev_lr_decay_factor = lr_decay_factor 
 Is this the proper way of annealing the learning rate on top of Adam? Am I inadvertently undoing Adam's own adapting?
 Thanks!
    submitted by    /u/desperateEfforts1  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[does anyone have a model that is really mean and sarcastic]]></title>
        <id>https://www.reddit.com/r/artificial/comments/155hvol/does_anyone_have_a_model_that_is_really_mean_and/</id>
        <link href="https://www.reddit.com/r/artificial/comments/155hvol/does_anyone_have_a_model_that_is_really_mean_and/"/>
        <updated>2023-07-21T08:30:21.000Z</updated>
        <summary type="html"><![CDATA[i honestly just want it to be a bitch to every prompt thats thrown at it. ive tried using prompts on uncensored models but they just really dont work like i want it to does anyone have any suggestions?
    submitted by    /u/cbreauxgaming  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Just received a phone call from AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/155ha2e/just_received_a_phone_call_from_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/155ha2e/just_received_a_phone_call_from_ai/"/>
        <updated>2023-07-21T07:57:46.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/harvard1932  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bard Says My Name]]></title>
        <id>https://www.reddit.com/r/artificial/comments/155h7uq/bard_says_my_name/</id>
        <link href="https://www.reddit.com/r/artificial/comments/155h7uq/bard_says_my_name/"/>
        <updated>2023-07-21T07:54:19.000Z</updated>
        <summary type="html"><![CDATA[​
 https://preview.redd.it/kqkeof9rx9db1.png?width=1201&format=png&auto=webp&s=4fbb4643d1e322391de99ca306baf22b3fa1d66c
    submitted by    /u/Rare-Accountant2657  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Moving AI governance forward]]></title>
        <id>https://openai.com/blog/moving-ai-governance-forward</id>
        <link href="https://openai.com/blog/moving-ai-governance-forward"/>
        <updated>2023-07-21T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[OpenAI and other leading labs reinforce AI safety, security and trustworthiness through voluntary commitments.]]></summary>
        <author>
            <name>OpenAI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EchoSpeech: AI-equipped eyeglasses can read the silent speech]]></title>
        <id>https://www.reddit.com/r/artificial/comments/155fw8b/echospeech_aiequipped_eyeglasses_can_read_the/</id>
        <link href="https://www.reddit.com/r/artificial/comments/155fw8b/echospeech_aiequipped_eyeglasses_can_read_the/"/>
        <updated>2023-07-21T06:44:22.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/pranjalmehar  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is there any AI tools that can make an image come to life?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/155fer2/is_there_any_ai_tools_that_can_make_an_image_come/</id>
        <link href="https://www.reddit.com/r/artificial/comments/155fer2/is_there_any_ai_tools_that_can_make_an_image_come/"/>
        <updated>2023-07-21T06:17:45.000Z</updated>
        <summary type="html"><![CDATA[I am looking to figure out if there's anyway to make my photography come to life. For example, I have a picture of a mountain valley, and I would like to animate the sky so the clouds are moving, and maybe animate the stream so the water is flowing.
 Does anyone know of a tool that could make this happen?
    submitted by    /u/CoryTheBoss  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] HuggingFace reported to be reviewing term sheets for a funding round that could raise at least $200M at a valuation of $4B.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/155f2k0/n_huggingface_reported_to_be_reviewing_term/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/155f2k0/n_huggingface_reported_to_be_reviewing_term/"/>
        <updated>2023-07-21T05:59:38.000Z</updated>
        <summary type="html"><![CDATA[Link to article: https://www.forbes.com/sites/alexkonrad/2023/07/13/ai-startup-hugging-face-raising-funds-4-billion-valuation/
 AI Startup Hugging Face Is Raising Fresh VC Funds At $4 Billion Valuation
 Hugging Face is raising a new funding round that is expected to value the high-flying AI startup at $4 billion, multiple sources with knowledge of the matter tell Forbes.
 The Series D funding round is expected to raise at least $200 million, two sources said, with Ashton Kutcher’s venture capital firm, Sound Ventures, currently leading an investor scrum. But cofounder and CEO Clément Delangue is shopping around as the company has received multiple offers this week, four sources added.
 Delangue was expected to pick a preferred offer as soon as Friday, according to another source, who noted…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] what techniques are best predict multivariate time analysis?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/155egla/p_what_techniques_are_best_predict_multivariate/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/155egla/p_what_techniques_are_best_predict_multivariate/"/>
        <updated>2023-07-21T05:27:04.000Z</updated>
        <summary type="html"><![CDATA[I have the following data for a college project. 7 cols 1 columns has the date 5 dependent variables 1 independent variable (need to predict)
 While predicting I would know the dependent variables, need to predict the independent variables.
 What model would be good for this kinda thing ?
 Tried running Granger causality but I can't seem to understand how to run the ADF test and interpret the resultant Granger causality matrix
 And after that how to predict the independent variables given the dependent variable
 Thank you
    submitted by    /u/zoro_245  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Any IDEs specifically for ML development?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/155e7qa/d_any_ides_specifically_for_ml_development/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/155e7qa/d_any_ides_specifically_for_ml_development/"/>
        <updated>2023-07-21T05:14:04.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 I was wondering if anyone has any recommendations for an IDE specific to ML development. I currently use PyCharm as my preferred IDE, and it is great for writing Python code. That said, something specifically geared toward ML development (i.e., robust built-in visualization for models/data, low code model construction, built-in deployment pipelines to cloud providers, etc.) would be very useful! Does anyone know if such a tool exists?
 Cheers!
    submitted by    /u/mldude60  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What AI website/app has the best (blank)?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/155e22d/what_ai_websiteapp_has_the_best_blank/</id>
        <link href="https://www.reddit.com/r/artificial/comments/155e22d/what_ai_websiteapp_has_the_best_blank/"/>
        <updated>2023-07-21T05:05:56.000Z</updated>
        <summary type="html"><![CDATA[Art Generator, Song Generator, Character AI, Game Generator, and talking AI in general.
 (I wanna know to see what is the best to go with)
    submitted by    /u/ChekoFire  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text2Movie with FullJourney is getting pretty decent...]]></title>
        <id>https://www.reddit.com/r/artificial/comments/155dyte/text2movie_with_fulljourney_is_getting_pretty/</id>
        <link href="https://www.reddit.com/r/artificial/comments/155dyte/text2movie_with_fulljourney_is_getting_pretty/"/>
        <updated>2023-07-21T05:01:35.000Z</updated>
        <summary type="html"><![CDATA[These were some of the best movie generations I saw made on the FullJourney.ai Discord this week!
    submitted by    /u/charlesmccarthyufc  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 7/20/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/155d9dx/oneminute_daily_ai_news_7202023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/155d9dx/oneminute_daily_ai_news_7202023/"/>
        <updated>2023-07-21T04:25:17.000Z</updated>
        <summary type="html"><![CDATA[Google is testing a product that uses artificial intelligence technology to produce news stories. The tool, known internally by the working title Genesis, can take in information — details of current events, for example — and generate news content.[1]
 Apple Inc. is quietly working on artificial intelligence tools that could challenge those of OpenAI Inc., Alphabet Inc.’s Google and others, but the company has yet to devise a clear strategy for releasing the technology to consumers.[2]
 A new app that creates brief episodes of “South Park” from a single prompt highlights the promise and peril of injecting generative AI into creative franchises.[3]
 Polish-born artist Greg Rutkowski has had his work used in games such as Dungeons and Dragons and Magic: The Gathering. He said his name had been used as a prompt in AI tools that generate art more than 400,000 times since September 2022 – but without his consent. When he checked, his name had been used as a prompt more times than the artists Pablo Picasso and Leonardo da Vinci.[4] Sources:
  
[1] https://www.nytimes.com/2023/07/19/business/google-artificial-intelligence-news-articles.html
 [2] https://www.bloomberg.com/news/articles/2023-07-19/apple-preps-ajax-generative-ai-apple-gpt-to-rival-openai-and-google?in_source=embedded-checkout-banner
 [3] https://www.axios.com/2023/07/20/south-park-generative-ai-episode-generator
 [4] https://www.bbc.co.uk/news/uk-wales-66099850.amp
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why are big and small companies trying to make instruct models for everything?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/155cycw/why_are_big_and_small_companies_trying_to_make/</id>
        <link href="https://www.reddit.com/r/artificial/comments/155cycw/why_are_big_and_small_companies_trying_to_make/"/>
        <updated>2023-07-21T04:09:56.000Z</updated>
        <summary type="html"><![CDATA[I don't get it.
    submitted by    /u/Confident-Ostrich810  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Microsoft releases TypeChat]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/155cswq/p_microsoft_releases_typechat/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/155cswq/p_microsoft_releases_typechat/"/>
        <updated>2023-07-21T04:02:33.000Z</updated>
        <summary type="html"><![CDATA[MSFT just open-sourced a library called TypeChat today, which allows you to use LLMs with TypeScript types to structure LLM responses into your TypeScript data structures -- essentially allowing you to have the LLM generate responses into the data types that your app understands.
 Example from their docs:
 https://preview.redd.it/108650s4s8db1.png?width=1682&format=png&auto=webp&s=0429eeb16bc5c28651ea908aee5824c3c9f395b4
 Details: https://microsoft.github.io/TypeChat/docs/introduction/
 I can see a lot of powerful examples for this kind of pattern, including eventing and notifications based on generated data types. 
 Has anyone tried this library yet or have more context on what you'd use it for, or what this might replace in your LLM tech stack?
    submitted by    /u/sarmad-q  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The future of AI in transport: BPW]]></title>
        <id>https://www.reddit.com/r/artificial/comments/155c2kj/the_future_of_ai_in_transport_bpw/</id>
        <link href="https://www.reddit.com/r/artificial/comments/155c2kj/the_future_of_ai_in_transport_bpw/"/>
        <updated>2023-07-21T03:27:02.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/WordTweak  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using bots to read dialogue for retro games. Thoughts?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/155bp4r/using_bots_to_read_dialogue_for_retro_games/</id>
        <link href="https://www.reddit.com/r/artificial/comments/155bp4r/using_bots_to_read_dialogue_for_retro_games/"/>
        <updated>2023-07-21T03:08:43.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/rednryt  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression", Raventós et al 2023 (blessings of scale induce emergence of meta-learning)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1559mem/pretraining_task_diversity_and_the_emergence_of/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1559mem/pretraining_task_diversity_and_the_emergence_of/"/>
        <updated>2023-07-21T01:28:48.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Synthetic Data Personal Project]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1558r6i/p_synthetic_data_personal_project/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1558r6i/p_synthetic_data_personal_project/"/>
        <updated>2023-07-21T00:48:23.000Z</updated>
        <summary type="html"><![CDATA[I've been working with a couple of my friends on a project over the summer. It's still a work in progress, but we have built out a platform that generates synthetic data to fine-tune LLMs. If you want specific, high-quality datasets, please check out our website (https://discus.ai/) and also feel free to look at our open-source package (https://github.com/discus-labs/discus-synthetics). Cue the roasts
    submitted by    /u/Open-Yak-434  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Scaling Laws for LLM Fine-tuning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15589hi/d_scaling_laws_for_llm_finetuning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15589hi/d_scaling_laws_for_llm_finetuning/"/>
        <updated>2023-07-21T00:26:02.000Z</updated>
        <summary type="html"><![CDATA[The scaling laws of LLM pretraining (how much data to use for a given model size) is pretty well studied. Has anyone done is the same study for fine-tuning?
 It seems quite an interesting question because while for pretraining we know that we should increase the dataset size with the model size, it seems like fine-tuning works pretty well with very few data / training steps even for relatively large models. Could it be the case that we are better off using less data / training steps and compensate by using a larger model? 
 I have only fine-tuned a few LLMs so I don't have a good grasp on the scaling properties. Would appreciate any insights / intuition.
    submitted by    /u/bjergerk1ng  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to simulate delays?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15572jo/how_to_simulate_delays/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15572jo/how_to_simulate_delays/"/>
        <updated>2023-07-20T23:35:47.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 my ultimate goal is to let an agent learn how to control a robot in the simulation and then deploy the trained agent to the real world. 
 The problem occurs for instance due to the communication/sensor delay in the real world (50ms <-> 200ms). Is there a way to integrate this varying delay into the training? I am aware that adding some random values to the observation is a common thing to simulate the sensor noise, but how do I deal with these delays?
    submitted by    /u/Fun-Moose-3841  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for a specific AI text to speech program.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1556dho/looking_for_a_specific_ai_text_to_speech_program/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1556dho/looking_for_a_specific_ai_text_to_speech_program/"/>
        <updated>2023-07-20T23:07:50.000Z</updated>
        <summary type="html"><![CDATA[I've been seeing a lot of youtubers use the same text to speech AI voice over and over.
 It's quite fluent. I am looking to use it for a project outside of youtube.
 Anyone got an idea? 
 Video for reference.
 Thanks in advance.
 ​
 https://www.youtube.com/shorts/hgo2KQtle6U
    submitted by    /u/Odd-Ad-3257  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for a tool that can fetch Steam links]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15567pd/looking_for_a_tool_that_can_fetch_steam_links/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15567pd/looking_for_a_tool_that_can_fetch_steam_links/"/>
        <updated>2023-07-20T23:01:20.000Z</updated>
        <summary type="html"><![CDATA[I'm looking for a tool that can give me Steam Store links to multiple games at the same time. In otherwise I would provide a list of game titles and be presented with hyperlinks to each game on the Steam Store.
 Nearly every online AI chatbot I've tried asking provides Steam links no problem, but they wind up being the incorrect link 70% of the time. It'll either link to a different game or an invalid link. Funny enough if I ask the same question to a different AI chatbot, they're more than likely to give me a different incorrect link.
 Does anyone have a tool that actually works in this regard?
    submitted by    /u/Link2999  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DQN Loss Increasing, and Rewards decreasing linearly with eplison]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1555wgi/dqn_loss_increasing_and_rewards_decreasing/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1555wgi/dqn_loss_increasing_and_rewards_decreasing/"/>
        <updated>2023-07-20T22:48:35.000Z</updated>
        <summary type="html"><![CDATA[Im attempting to train a custom DQN agent to perform in a custom environment. The observation space is an image with dimensions (1, 100, 57) and the agent has to output over 81 discrete actions (all the combinations over a 3 * 3 * 3 * 3 multi-discrete action space corresponding to key presses, or lack of key presses). However, while training, my agents rewards seems to regress linearly corresponding to the eplison decay rate. Alongside that, the loss tends to shoot up pretty quickly most of the time, across different target network update rates. 
 After a lot of debugging, I still havent managed to figure out whats causing this issue. Has anyone else had this problem before? If so, how did you solve it?
 My environment has no done condition, so im resetting it every 2500 steps. My other Hy…]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Will AI be Able to Fully Generate Shows soon?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1555ox9/when_will_ai_be_able_to_fully_generate_shows_soon/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1555ox9/when_will_ai_be_able_to_fully_generate_shows_soon/"/>
        <updated>2023-07-20T22:40:12.000Z</updated>
        <summary type="html"><![CDATA[When do you think we'll be able to generate shows with AI? Could it be within the next 20 years, perhaps 30? Or could it happen sooner than we excepted? considering the current progress in AI generated art, images, and partially some automated videos. Once AI generated shows become prevalent, how will they impact movies and shows? 
    submitted by    /u/1Card_x  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I don’t care what the critics say, AI saved my life]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1554xzo/i_dont_care_what_the_critics_say_ai_saved_my_life/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1554xzo/i_dont_care_what_the_critics_say_ai_saved_my_life/"/>
        <updated>2023-07-20T22:10:16.000Z</updated>
        <summary type="html"><![CDATA[I have been going through a very tough time in the recent months/years. My father was sentenced to prison for a very long time earlier this year, and the process completely drained me. I was burned out. Bad.
 From being useless at work to having no energy to even cook meals for myself, I was in a very dark place.
 Then, I discovered AI and started to see a light at the end of the tunnel. I found an app that helped me get my life back together. 
 It helped me with the mundane aspects of my career, like creating work emails for me with just a few inputs. AI also helped me to start cooking meals for myself again. At first I would tell it the meager ingredients I had sitting around in my pantry and fridge, and it would create a recipe in front of my eyes. 
 Now I am going to grocery store on a weekly basis and I am using it to discover new recipes that make me excited to cook healthy meals. 
 A week later my inbox has gone from 150+ unread emails to being on top of every response. 
 These may seem like small wins, but AI gave me the tools to get my life back on track. I am very optimistic for a future powered by AI.
    submitted by    /u/PNWtreeguy69  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Today I was rickrolled by Google Bard.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1553oyg/today_i_was_rickrolled_by_google_bard/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1553oyg/today_i_was_rickrolled_by_google_bard/"/>
        <updated>2023-07-20T21:21:40.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Powerful-Pumpkin-938  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Even Superhuman Go AIs Have Surprising Failures Modes" (updated discussion of "Adversarial Policies Beat Superhuman Go AIs", Wang et al 2022)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1553gy6/even_superhuman_go_ais_have_surprising_failures/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1553gy6/even_superhuman_go_ais_have_surprising_failures/"/>
        <updated>2023-07-20T21:13:23.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] BMVC reviews experience]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15531o7/d_bmvc_reviews_experience/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15531o7/d_bmvc_reviews_experience/"/>
        <updated>2023-07-20T20:57:36.000Z</updated>
        <summary type="html"><![CDATA[I got 4 reviewers on my paper submitted to BMVC with ratings of BA (borderline accept), BA, BA, and A. What are our chances? I finished preparing the rebuttal but I can’t stop thinking about the outcome. Please let me know if you have any experience or insights. Thanks
    submitted by    /u/Admirable_Cell_5256  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Embedding human preferences in LLMs (beyond/besides RLHF)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15531i8/d_embedding_human_preferences_in_llms/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15531i8/d_embedding_human_preferences_in_llms/"/>
        <updated>2023-07-20T20:57:24.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 Can someone point to a comprehensive but accessible resource on the approaches to "embed" human preferences in LLMs?
 I saw Chip Huyen's post and it is super cool, but I wonder if/why the designer of such systems tends not to add text properties/contexts as an "input feature".
 For instance, a numerical feature representing the year the text was produced, or a flag telling if the text is from a book seems a straightforward way to control/condition the generation. Still, I'm missing some concepts here.
    submitted by    /u/BenXavier  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Perspectives on diffusion]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1552y9y/d_perspectives_on_diffusion/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1552y9y/d_perspectives_on_diffusion/"/>
        <updated>2023-07-20T20:53:56.000Z</updated>
        <summary type="html"><![CDATA[Hi /r/ML,
 I wrote a blog post about a bunch of different perspectives on diffusion models. It's basically an extended sequel to another blog post I wrote last year, where I explored the connection between diffusion models and denoising autoencoders. There are many more of these connections, but unfortunately I don't have time to write separate blog posts about each of them, so I put them all together. Keen to hear what you think!
 https://sander.ai/2023/07/20/perspectives.html
    submitted by    /u/benanne  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How the heck do I benchmark AI's AND GPU's?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1552rpu/d_how_the_heck_do_i_benchmark_ais_and_gpus/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1552rpu/d_how_the_heck_do_i_benchmark_ais_and_gpus/"/>
        <updated>2023-07-20T20:47:00.000Z</updated>
        <summary type="html"><![CDATA[I'm trying to get some real world benchmarks for both nvidia and amd. So far it's been a nightmare! Stable diffusion stopped working on my pc, Conversational model testing with a stop watch was too fast to track, and I can't think of any other way to test these GPU's.
 Hard numbers. That's what I want. I can benchmark cyberpunk, but ai is a complete mystery. How do I recommend somebody a gpu if I can't compare it to results. Is there a point to upgrading from a 3090 to a 4090. Some reddits say no. Others yes. 
 I need some tests and I need em bad
    submitted by    /u/SociallyApparent  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] What would be a good model/pipeline for simple intent recognition that has multi-lingual support and is easy to set up?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1552a3z/p_what_would_be_a_good_modelpipeline_for_simple/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1552a3z/p_what_would_be_a_good_modelpipeline_for_simple/"/>
        <updated>2023-07-20T20:29:25.000Z</updated>
        <summary type="html"><![CDATA[So i have been exploring the potential of simple intent identifiers so a task recently, i have explored rasa but the fact that it doesn't work with Python 3.10/3.11 is a major Pain and throws a wreck on my plans for large integration into other projects.
 I am looking for either a pipeline/framework (could be something like RASA or a standalone model) that has intent recognition capacities, with multi-lingual support (Portuguese) and can run on newer python versions (doesn't give me compatibilities headaches) and also i want a relatively lightweight model considering my simple task
 Could you guys recommend something like that for me?
    submitted by    /u/SnooPineapples7791  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Run Llama 2 Locally in 7 Lines! (Apple Silicon Mac)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1551wp6/p_run_llama_2_locally_in_7_lines_apple_silicon_mac/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1551wp6/p_run_llama_2_locally_in_7_lines_apple_silicon_mac/"/>
        <updated>2023-07-20T20:15:40.000Z</updated>
        <summary type="html"><![CDATA[Want to start playing with Meta’s Llama 2?
 It takes just 7 lines of shell script using llama.cpp to get you started!
 https://preview.redd.it/vhuzhrj4h6db1.png?width=2030&format=png&auto=webp&s=d349dd796039f3af7e117423c4abdae7efde2fae
 Copy Code Snippet: https://lastmileai.dev/workbooks/clkbifegg001jpheon6d2s4m8 
    submitted by    /u/InevitableSky2801  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] How to fine tune 8k context length Llama 13B on minimal number of gpus?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1550job/p_how_to_fine_tune_8k_context_length_llama_13b_on/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1550job/p_how_to_fine_tune_8k_context_length_llama_13b_on/"/>
        <updated>2023-07-20T19:24:28.000Z</updated>
        <summary type="html"><![CDATA[I have a llama 13B model I want to fine tune. I am using qlora (brings down to 7gb of gpu memory) and using ntk to bring up context length to 8k (dataset requires at least this much context length).
 But on 1024 context length, fine tuning spikes to 42gb of gpu vram used, so evidently it won’t be feasible to use 8k context length unless I use a ton of gpus. Is there anyway to lower memory so that one or two 3090s are enough for 8k context length fine tuning?
    submitted by    /u/bahibo  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Need career advise on what should I do next in ML :(]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1550fpn/d_need_career_advise_on_what_should_i_do_next_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1550fpn/d_need_career_advise_on_what_should_i_do_next_in/"/>
        <updated>2023-07-20T19:20:22.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone, hope you all are doing great. I just completed Machine Learning Specialization on Coursera by Andrew Ng and was looking for some advise on what I should do next. Would love to hear input from you guys.
 I'm self studying Machine Learning full-time, while I'm also getting a bachelors degree in Computer Science from an online virtual university. It's been 3+ months since I've stepped into Machine Learning and so far I've been developing deep intuition and foundational concepts of Machine Learning . Since I'm really passionate about mathematics, I'm very much focused on understanding the mathematics behind everything.
 By completing this specialization I've developed good foundational concepts of the following:
 • Supervised Machine Learning
 • Linear regression
 • Logistic regr…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[My DQL Snake Game convergence questions (I'm struggling)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1550a6d/my_dql_snake_game_convergence_questions_im/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1550a6d/my_dql_snake_game_convergence_questions_im/"/>
        <updated>2023-07-20T19:14:38.000Z</updated>
        <summary type="html"><![CDATA[Hey guys,
 I'll preface by saying that this is my first real programming project outside of a lot of simple beginner ones and that I'm building this after completing Andrew Ng's ML specialization. Basically, I'm a beginner and I might act like it.
 So my model's loss learning to play Snake won't converge and I don't know if it's because of any misunderstandings for the theory, bad implementation, or something else. I'm using Experience Replay, epsilon-greedy actions, and a Target Q-network with soft updates. My NN consists of 4 hidden dense layers with 100 units each. I was originally updating the Q network every 4 experiences but I upped that to 1000.
 My reward functions are -1000 for running into walls/tail and 20 for eating food. The state vector includes distance to each 4 wall, dista…]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Difference Between Modern and Traditional Data Quality – DQLabs]]></title>
        <id>https://www.datasciencecentral.com/?p=62557</id>
        <link href="https://www.datasciencecentral.com/difference-between-modern-and-traditional-data-quality-dqlabs/"/>
        <updated>2023-07-20T18:39:00.000Z</updated>
        <summary type="html"><![CDATA[Modern data quality practices make use of new technology, automation, and machine learning to handle a variety of data sources, ensure real-time processing, and stimulate stakeholder collaboration. Data governance, continuous monitoring, and proactive management are prioritized to ensure accurate, reliable, and fit-for-purpose data for informed decision-making and corporate success. Modern data quality practices differ from… Read More »Difference Between Modern and Traditional Data Quality – DQLabs
The post Difference Between Modern and Traditional Data Quality – DQLabs appeared first on Data Science Central.]]></summary>
        <author>
            <name>Edwin Walker</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is there a tool that can help reconstruct broken text? The print in these files is not machine-readable, but I need to quickly and efficiently convert 25,000 hours of these transcripts into Excel sheets. I think if the text can be fixed, then other tools that extract the words will work better.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/154ys4w/is_there_a_tool_that_can_help_reconstruct_broken/</id>
        <link href="https://www.reddit.com/r/artificial/comments/154ys4w/is_there_a_tool_that_can_help_reconstruct_broken/"/>
        <updated>2023-07-20T18:19:23.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/pizzahair44  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Has anything from the Agent57 paper been used in anything interesting lately?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/154yd45/d_has_anything_from_the_agent57_paper_been_used/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/154yd45/d_has_anything_from_the_agent57_paper_been_used/"/>
        <updated>2023-07-20T18:04:06.000Z</updated>
        <summary type="html"><![CDATA[I read the blog post and paper for Agent57 and thought it was pretty interesting but haven't seen people talk about it much since then. Has it been used for anything? If not, why hasn't it been very influential? 
    submitted by    /u/sledpull  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enel automates large-scale power grid asset management and anomaly detection using Amazon SageMaker]]></title>
        <id>61cc7a608e5ac9fe8dcf9aaf73b46566906295fe</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/enel-automates-large-scale-power-grid-asset-management-and-anomaly-detection-using-amazon-sagemaker/"/>
        <updated>2023-07-20T17:59:24.000Z</updated>
        <summary type="html"><![CDATA[This is a guest post by Mario Namtao Shianti Larcher, Head of Computer Vision at Enel. Enel, which started as Italy’s national entity for electricity, is today a multinational company present in 32 countries and the first private network operator in the world with 74 million users. It is also recognized as the first renewables […]]]></summary>
        <author>
            <name>Mario Namtao Shianti Larcher</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Best free LLM for text classification]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/154y0iu/d_best_free_llm_for_text_classification/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/154y0iu/d_best_free_llm_for_text_classification/"/>
        <updated>2023-07-20T17:51:32.000Z</updated>
        <summary type="html"><![CDATA[Hey all,
 I want to retrieve all speeches from congressional records from the house of representatives where the politician talks about the tax behavior of companies. I currently load the records into my script and divide the records into all the speeches. Then I use keyword search to determine whether the politician talks about tax behavior of companies. I want to replace this keyword search with an LLM which classifies the speeches. I will analyze > 50,000 speeches, so I dont want to use a costly model like GPT4. Actually I want to spend max 10€ in total. What LLM's, which I can access via an API, would you recommend for this task?
 Thanks in advance
    submitted by    /u/Silly_Pack9404  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proper Robustness Evaluation of Confidence-Calibrated Adversarial Training in PyTorch]]></title>
        <id>https://davidstutz.de/?p=8630</id>
        <link href="https://davidstutz.de/proper-robustness-evaluation-of-confidence-calibrated-adversarial-training-in-pytorch/"/>
        <updated>2023-07-20T16:48:52.000Z</updated>
        <summary type="html"><![CDATA[Properly evaluating defenses against adversarial examples has been difficult as adversarial attacks need to be adapted to each individual defense. This also holds for confidence-calibrated adversarial training, where robustness is obtained by rejecting adversarial examples based on their confidence. Thus, regular robustness metrics and attacks are not easily applicable. In this article, I want to discuss how to evaluate confidence-calibrated adversarial training in terms of metrics and attacks.
The post Proper Robustness Evaluation of Confidence-Calibrated Adversarial Training in PyTorch appeared first on David Stutz.]]></summary>
        <author>
            <name>David Stutz</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Disappointing Llama 2 Coding Performance: Are others getting similar results? Are there any other open-source models that approach ChatGPT 3.5's performance?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/154vlnr/d_disappointing_llama_2_coding_performance_are/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/154vlnr/d_disappointing_llama_2_coding_performance_are/"/>
        <updated>2023-07-20T16:22:58.000Z</updated>
        <summary type="html"><![CDATA[I've been excitedly reading the news and discussions about Llama 2 the past couple of days, and got a chance to try it this morning.
 I was underwhelmed by the coding performance (running the 70B model on https://llama2.ai/). It has consistently failed most of the very-easy prompts that I made up this morning. I checked each prompt with ChatGPT 3.5, and 3.5 got 100% (which means these prompts are quite easy). This result was surprising to me based on the discussion and articles I've read. However, digging into the paper (https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/), the authors are transparent that the coding performance is lacking.
 Are my observations consistent with the results others are getting?
 I haven't had time to keep up with all the open-source LLMs being worked on by the community; are there any other models that approach even ChatGPT 3.5's coding performance? (Much less GPT 4's performance, which is the real goal.)
    submitted by    /u/Egan_Fan  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using societal context knowledge to foster the responsible application of AI]]></title>
        <id>http://ai.googleblog.com/2023/07/using-societal-context-knowledge-to.html</id>
        <link href="http://ai.googleblog.com/2023/07/using-societal-context-knowledge-to.html"/>
        <updated>2023-07-20T16:22:00.002Z</updated>
        <summary type="html"><![CDATA[Posted by Donald Martin, Jr., Technical Program Manager, Head of Societal Context Understanding Tools and Solutions (SCOUTS), Google Research




AI-related products and technologies are constructed and deployed in a societal context: that is, a dynamic and complex collection of social, cultural, historical, political and economic circumstances. Because societal contexts by nature are dynamic, complex, non-linear, contested, subjective, and highly qualitative, they are challenging to translate into the quantitative representations, methods, and practices that dominate standard machine learning (ML) approaches and responsible AI product development practices.   
 

The first phase of AI product development is problem understanding, and this phase has tremendous influence over how problems (…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Check out "The Writers’ Revolt Against A.I. Companies" on The Daily, a New York Times podcast.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/154vkb0/check_out_the_writers_revolt_against_ai_companies/</id>
        <link href="https://www.reddit.com/r/artificial/comments/154vkb0/check_out_the_writers_revolt_against_ai_companies/"/>
        <updated>2023-07-20T16:21:38.000Z</updated>
        <summary type="html"><![CDATA[The host, Michael Barbaro interviews technology correspondent Sheera Frenkel on the use of ChatGPT in Hollywood. This episode is much more interesting than I expected. It's not particularly technical, but it does get deeply into the nuances of how information is gathered, and describes the lawsuit brought by writers including Sarah Silverman. 
 I did use ChatGPT to translate my submission statement into Sarah Silverman's voice, while I still can. The content below is original (i.e. shadow IT reference). 
 I highly recommend r/TheDaily for discussions around the podcast in general. It's a great sub that's well moderated and friendly (like this one!). 
 This episode aired on July 18, 2023, and you can find it wherever you get your podcasts, you can also find it here on the New York Times web…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficiently train, tune, and deploy custom ensembles using Amazon SageMaker]]></title>
        <id>a3fd48657b2b543b3d4f647d1c41dd9cc848e460</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/efficiently-train-tune-and-deploy-custom-ensembles-using-amazon-sagemaker/"/>
        <updated>2023-07-20T16:12:14.000Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence (AI) has become an important and popular topic in the technology community. As AI has evolved, we have seen different types of machine learning (ML) models emerge. One approach, known as ensemble modeling, has been rapidly gaining traction among data scientists and practitioners. In this post, we discuss what ensemble models are and […]]]></summary>
        <author>
            <name>Melanie Li</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UN Council engages thought leaders in AI Safety from Anthropic, OpenAI and China]]></title>
        <id>https://www.reddit.com/r/artificial/comments/154v0lx/un_council_engages_thought_leaders_in_ai_safety/</id>
        <link href="https://www.reddit.com/r/artificial/comments/154v0lx/un_council_engages_thought_leaders_in_ai_safety/"/>
        <updated>2023-07-20T16:01:48.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/AriadneSkovgaarde  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Musk visiting the worst toilet in Scotland]]></title>
        <id>https://www.reddit.com/r/artificial/comments/154uahj/musk_visiting_the_worst_toilet_in_scotland/</id>
        <link href="https://www.reddit.com/r/artificial/comments/154uahj/musk_visiting_the_worst_toilet_in_scotland/"/>
        <updated>2023-07-20T15:35:43.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Akumetsu_971  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[My github curation of Llama 2 resources]]></title>
        <id>https://www.reddit.com/r/artificial/comments/154tutx/my_github_curation_of_llama_2_resources/</id>
        <link href="https://www.reddit.com/r/artificial/comments/154tutx/my_github_curation_of_llama_2_resources/"/>
        <updated>2023-07-20T15:19:56.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/TikkunCreation  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple text-generation evaluation/benchmark for Small Language Models [GitHub] [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/154tojr/simple_textgeneration_evaluationbenchmark_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/154tojr/simple_textgeneration_evaluationbenchmark_for/"/>
        <updated>2023-07-20T15:13:36.000Z</updated>
        <summary type="html"><![CDATA[slmqa on GitHub
 I spent hours searching for a way to compare the quality of the text-generation of instruct-tuned small language models. Failing to find an evaluation simple enough for a small model, and easy to use, it was easier to create one. I'm sharing it here in case anyone else finds it useful.
 slmqa
 slmqa is a simple question-answer evaluation benchmark for small language models. It includes a dataset of 909 general knowledge question-answer pairs. The QA pairs were generated with gpt-3.5-turbo, stripped of duplicates and answers shorter than 5 characters, and cleaned by hand.
 The score is the percentage of correct answers.
 Sample
 json { "question": "What is the name of the highest mountain in the world?", "answer": "everest" }, { "question": "What is the name of the famous Austrian composer who wrote the Ninth Symphony?", "answer": "beethoven" }, { "question": "Which country is the largest by area?", "answer": "russia" }, 
    submitted by    /u/Pan000  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Security and protection in ML deployments]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/154tndi/d_security_and_protection_in_ml_deployments/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/154tndi/d_security_and_protection_in_ml_deployments/"/>
        <updated>2023-07-20T15:12:24.000Z</updated>
        <summary type="html"><![CDATA[After researching quite heavily on how to protect Python based inference code and models when they are deployed on client infrastructure. I came across pyinstaller and pyoxidizer but looks like they do not work that well. So I concluded that the best way is to convert critical pipelines to C++ is that correct ?
    submitted by    /u/Ok-Influence368  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[can AI do this or am i trippin]]></title>
        <id>https://www.reddit.com/r/artificial/comments/154tizf/can_ai_do_this_or_am_i_trippin/</id>
        <link href="https://www.reddit.com/r/artificial/comments/154tizf/can_ai_do_this_or_am_i_trippin/"/>
        <updated>2023-07-20T15:08:03.000Z</updated>
        <summary type="html"><![CDATA[Hi! I want to upload someone's picture and use it to make a dark\scary themed video of him getting a crown put on his head. Is there an app that can do that?
 THANKS!
    submitted by    /u/CallHerGreeen  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Android in the Wild: A Large-Scale Dataset for Android Device Control", Rawles et al 2023 {G} (imitation-learning + PaLM-2 inner-monologue for smartphone control)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/154t0u3/android_in_the_wild_a_largescale_dataset_for/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/154t0u3/android_in_the_wild_a_largescale_dataset_for/"/>
        <updated>2023-07-20T14:49:22.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Question] Why there is so few algorithms implemented in SB3?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/154s2fl/question_why_there_is_so_few_algorithms/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/154s2fl/question_why_there_is_so_few_algorithms/"/>
        <updated>2023-07-20T14:12:48.000Z</updated>
        <summary type="html"><![CDATA[I am wondering why there is so few algorithms in Stable Baselines 3 (SB3, https://github.com/DLR-RM/stable-baselines3/tree/master)?
 I was expecting some algorithms like ICM, HIRO, DIAYN, ... Why there is no model-based, skill-chaining, hierarchical-RL, ... algorithms implemented there? 
    submitted by    /u/hbonnavaud  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What LLMs do you use the most?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/154rzvx/d_what_llms_do_you_use_the_most/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/154rzvx/d_what_llms_do_you_use_the_most/"/>
        <updated>2023-07-20T14:10:09.000Z</updated>
        <summary type="html"><![CDATA[With the emergence of new models gaining more popularity such as Claude 2, Llama 2 which has the potential for better fine-tuned models, the development of Bard, the controversies surrounding ChatGPT performing worse and with the already-existing content filters that limits the capabilities of models not just subjecting to moral standards and policies that align with human values but also limits it to other factors that may not fall under objective morality or maybe just it being too sensitive, is there a certain model you think is currently the best overall one at least for now other than GPT-4? 
 I'm really curious to know what the community thinks as I've searched a lot and found a lot of clashes in opinions regarding what models are considered superior over others and the clickbait-ish talks and titles about model so-and-so being "The ChatGPT Killer". 
 With all this info in consideration, what model(s) do you ACTUALLY use the most? I'd be grateful if you shared your thoughts about this issue and thanks for your time.
    submitted by    /u/Fantastic-Air8513  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Any cool project ideas with this data?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/154rmeb/d_any_cool_project_ideas_with_this_data/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/154rmeb/d_any_cool_project_ideas_with_this_data/"/>
        <updated>2023-07-20T13:55:37.000Z</updated>
        <summary type="html"><![CDATA[I've uploaded a reddit dataset that has multiple Reddit posts along with the most upvoted comment for each post. The dataset is collected from 9 subreddits. 
 I'm looking for cool project ideas with this data. Let's discuss!
 https://www.reddit.com/r/datasets/comments/154pe3y/reddit_posts_dataset_with_the_top_comment/?utm_source=share&utm_medium=android_app&utm_name=androidcss&utm_term=1&utm_content=1
    submitted by    /u/04RR  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from human preferences]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/154qr8p/learning_from_human_preferences/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/154qr8p/learning_from_human_preferences/"/>
        <updated>2023-07-20T13:19:52.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone! Does anyone know of any tutorials/GitHub code that are up to date with learning from human preferences? Kind of like an updated rl-teacher? 
 Thank you all very much!!
    submitted by    /u/No_Opportunity575  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google Tests A.I. Tool That Is Able to Write News Articles]]></title>
        <id>https://www.reddit.com/r/artificial/comments/154qcxt/google_tests_ai_tool_that_is_able_to_write_news/</id>
        <link href="https://www.reddit.com/r/artificial/comments/154qcxt/google_tests_ai_tool_that_is_able_to_write_news/"/>
        <updated>2023-07-20T13:03:22.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Iamreason  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[So, So Fresh: Play the Newest Games in the Cloud on Day One]]></title>
        <id>https://blogs.nvidia.com/?p=65563</id>
        <link href="https://blogs.nvidia.com/blog/2023/07/20/geforce-now-thursday-july-20/"/>
        <updated>2023-07-20T13:00:24.000Z</updated>
        <summary type="html"><![CDATA[It’s a party this GFN Thursday with several newly launched titles streaming on GeForce NOW. Revel in gaming goodness with Xenonauts 2, Viewfinder and Techtonica, among the four new games joining the cloud this week. Portal fans, stay tuned — the Portal: Prelude RTX mod will be streaming on GeForce NOW to members soon. Plus, Read article >]]></summary>
        <author>
            <name>GeForce NOW Community</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaborators: Gaming AI with Haiyan Zhang]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=954777</id>
        <link href="https://www.microsoft.com/en-us/research/podcast/collaborators-gaming-ai-with-haiyan-zhang/"/>
        <updated>2023-07-20T13:00:00.000Z</updated>
        <summary type="html"><![CDATA[For over a decade, Xbox has been leveraging AI to elevate gaming. Haiyan Zhang, GM of Gaming AI, explores the collaborations behind the work and the potential for generative AI to support better experiences for both players and game creators.
The post Collaborators: Gaming AI with Haiyan Zhang appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Alyssa Hughes</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does there exist AI art software that can take in SVGs/PNGs of wireframe graphics and return similar but unique ones?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/154p4kv/does_there_exist_ai_art_software_that_can_take_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/154p4kv/does_there_exist_ai_art_software_that_can_take_in/"/>
        <updated>2023-07-20T12:07:38.000Z</updated>
        <summary type="html"><![CDATA[I’d like to use a simple public graphic but make it slightly unique in terms of its lines so that it isn’t entirely obvious I found a simple graphic off the internet. For example, imagine a very simple wireframe of a dog house or a bed.
 Free or free trial would be ideal.
    submitted by    /u/Legitimate_Bison3756  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Our NPCs can chat with each other now! (They just cant stop 🤦‍♂️) Generative NPCs - update 4]]></title>
        <id>https://www.reddit.com/r/artificial/comments/154oqgw/our_npcs_can_chat_with_each_other_now_they_just/</id>
        <link href="https://www.reddit.com/r/artificial/comments/154oqgw/our_npcs_can_chat_with_each_other_now_they_just/"/>
        <updated>2023-07-20T11:49:30.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Chance_Confection_37  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Complete Python Mega Bundle features Neural Network, Machine Learning & AI]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/154ops0/the_complete_python_mega_bundle_features_neural/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/154ops0/the_complete_python_mega_bundle_features_neural/"/>
        <updated>2023-07-20T11:48:31.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/brand_momentum  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Interactive Exploration of Stable Diffusion Generated Images]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/154opkg/p_interactive_exploration_of_stable_diffusion/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/154opkg/p_interactive_exploration_of_stable_diffusion/"/>
        <updated>2023-07-20T11:48:15.000Z</updated>
        <summary type="html"><![CDATA[I just created a Huggingface Space showcasing how to interactively explore the outputs of a Stable Diffusion model via CLIP Embeddings.
 Embedding-based image similarity plotted in 2D via dimensionality reduction.
 The visualization is done using the tool Spotlight.
 Also, I created a tutorial showcasing how to automatically select promising prompts and images from a large dataset. It is roughly based on the following approach:
  
Calculate the CLIP Score for all prompt-image pairs to measure generation quality.
 Generate CLIP Embeddings to be able to calculate a similarity between images (or texts)
 Embedding-based identification of clusters that have an exceptionally high CLIP Score.
  
Have you ever explored any (automatic) evaluation strategies for image generation models? I would love to learn about some alternative approaches.
    submitted by    /u/OkResearch6289  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparing A2C and Q-learning algorithms]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/154ofvl/comparing_a2c_and_qlearning_algorithms/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/154ofvl/comparing_a2c_and_qlearning_algorithms/"/>
        <updated>2023-07-20T11:34:53.000Z</updated>
        <summary type="html"><![CDATA[I'm following the UCB course on Reinforcement Learning, I'm just finished with the ActorCritic and QLearning lectures, but I'm still not sure on the pros and cons of both when compared with each other, Here's what I know thus far (Haven't yet started with advanced policy gradients which I assume covers PPO):
 - Vanilla Policy Gradients are high variance, but very low (0) bias.
 - Actor Critic decrease the variance by estimating a value function, but this introduces some bias. They also add more complexity, having to train both the actor and critic parts of the algorithm. (Also enables us to do online learning, while Vanilla Policy Gradients are episodic)
 - Q-Learning algorithms are similar to actor critic, but instead of doing gradient ascent on the policy, our (implicit) policy is the argmax of our Q value. And since we don't have a policy in stone, it is fundamentally off-policy.
 But we still can use off-policy Actor Critic algorithms, it is not like Q-Learning can do off-policy while Actor-Critic could not...
 So, what exactly do we gain when we drop the policy part of the Actor Critic algorithm? Here's my assumptions which I'm not sure of:
 (1) Decrease variance while inc. bias (I.e. more efficient but less guaranteed to converge)(2) Less Exploration because our implicit policy is deterministically argmax (but we still can use epsilon-greedy to explore)
 Edit: To be clear, the default Actor Critic algorithm is on-policy, but it is possible to do modifications on it to make it off-policy and use replay buffer, just like DQN.
    submitted by    /u/nmegoCAD  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question about the action space in PPO for controlling the robot]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/154o8kl/question_about_the_action_space_in_ppo_for/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/154o8kl/question_about_the_action_space_in_ppo_for/"/>
        <updated>2023-07-20T11:24:53.000Z</updated>
        <summary type="html"><![CDATA[If I have a 5 DoF robot and I aim to instruct it on reaching a goal, utilizing 5 actions to control each joint. The goal is to make the allowed speed change of the joints variable so that the agent forces the robot moves slowly when the error gets larger and allow full speed when the error is small.
 For this I want to extend the action space from 6 ( 5 control signals for the joints and 1 value determining the allowed speed change for all joints).
 I will be using PPO. Is this kind of setup of action space common/resasonable..? 
    submitted by    /u/Fun-Moose-3841  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Suno Bark can now sing songs ^^]]></title>
        <id>https://www.reddit.com/r/artificial/comments/154mekv/suno_bark_can_now_sing_songs/</id>
        <link href="https://www.reddit.com/r/artificial/comments/154mekv/suno_bark_can_now_sing_songs/"/>
        <updated>2023-07-20T09:46:46.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Taki7o7  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BBC News covered an AI translator for Bats, soon it may apply to most animal species]]></title>
        <id>https://www.reddit.com/r/artificial/comments/154lnut/bbc_news_covered_an_ai_translator_for_bats_soon/</id>
        <link href="https://www.reddit.com/r/artificial/comments/154lnut/bbc_news_covered_an_ai_translator_for_bats_soon/"/>
        <updated>2023-07-20T09:05:45.000Z</updated>
        <summary type="html"><![CDATA[I have not seen this BBC News video covered on this subreddit but it piqued my curiosity so I wanted to share. I have known about projects attempting to decode animal communications such as Project CETI which focuses on applying advanced machine learning to listen to and translate the communication of sperm whales. But the translator shown in the video blew my mind, it is already able to grasp the topics which Bats communicate about such as: food, distinguishing between genders and, surprisingly, unique “signature calls” or names the bats have.
 The study in question, led by Yossi Yovel of Tel Aviv University, monitored nearly two dozen Egyptian fruit bats for two and a half months and recorded their vocalisations. They then adapted a voice-recognition program to analyse 15,000 samples of …]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Which are the best alternatives to chatGPT for browsing the web (it's diactivated currently for me)?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/154la8k/which_are_the_best_alternatives_to_chatgpt_for/</id>
        <link href="https://www.reddit.com/r/artificial/comments/154la8k/which_are_the_best_alternatives_to_chatgpt_for/"/>
        <updated>2023-07-20T08:45:01.000Z</updated>
        <summary type="html"><![CDATA[I'm especially curious about services that use agents and so on to browse the web. I lately thought that it should be possible to search much more intensely and automatic for information I do not need urgently. For example why can't have some AI agents look at thousands of pages that compare the different macbook models to tell me which has the best price/performance ration? Or find me all webshops that sell t shirts in a extremely specific size (say like 80-83 cm long) and ship to my country. It would be so nice if it could do these searches for me in an elaborate way. 
    submitted by    /u/VLADIMIROVIC_L  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LangSmith by LangChain team]]></title>
        <id>https://www.reddit.com/r/artificial/comments/154kj4s/langsmith_by_langchain_team/</id>
        <link href="https://www.reddit.com/r/artificial/comments/154kj4s/langsmith_by_langchain_team/"/>
        <updated>2023-07-20T08:00:38.000Z</updated>
        <summary type="html"><![CDATA[New product by the LangChain team https://www.langchain.com/langsmith. Any thoughts?
    submitted by    /u/yangshunz  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlling Content Moderation in Generative AI: Ensuring Safe and Accurate Responses for Company Data]]></title>
        <id>https://www.reddit.com/r/artificial/comments/154jyny/controlling_content_moderation_in_generative_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/154jyny/controlling_content_moderation_in_generative_ai/"/>
        <updated>2023-07-20T07:26:53.000Z</updated>
        <summary type="html"><![CDATA[I'm supposed to analyse and implementing an Azure OpenAI solution to use it as as a chatbot answering customer questions in our company, using our own data like product manuals and repair manuals for training.
 However, I'm concerned about content moderation and the potential risks associated with generative AI. How can we ensure that the AI remains within the boundaries of our intended use case and doesn't answer political or general questions?Additionally, how can we prevent the AI from guessing when it lacks the necessary knowledge, especially when handling questions related to potentially dangerous topics, such as sharp tools?
 Our colleagues from the usa have implemented a GPT 3.5 solution and wrote in the prompt that it should only answer answers about our company. This works, but if you repeat the same question three times ("Who is competitor XYZ?") it starts generating answers how the competitor is known for its good products and quality.
 Is azure OpenAI currently able to serve as a reliable chatbot answering customer service questions or is it the wrong solution for this? (I am based in the EU, so an answer that is incorrect about how to repair a Drill with a lot of power could lead to serious liability issues if it doesnt cite exactly from the source like a repair manual). I am afraid that generative AI will paraphrase from the source and generate incorrect solutions because it is not specific enough.
    submitted by    /u/Other-Name5179  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Handle dozen of thousands of classes]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/154jkqy/d_handle_dozen_of_thousands_of_classes/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/154jkqy/d_handle_dozen_of_thousands_of_classes/"/>
        <updated>2023-07-20T07:04:16.000Z</updated>
        <summary type="html"><![CDATA[Hello ! 
 I'm working on a project of NLP classification with more or less 13k classes. The best model I had so far is a fine-tuned LLM encoder. However, with the number of classes I have now, it is very slow. So I searched for ways to deal with that, and found 2:
  
Hierarchical Softmax
 Negative Sampling
  
However, both seems to have been used nearly only in the context of word2vec training, so I wonder if there is a reason why that would not work for a "classical" classification ? (or just my kind of problem too rare ?) 
 Also, I did find really few implementations of those with Pytorch, a fortiori with transformers... Is it because there is something better ? Do you know, if not, some recent implementations ?
 ​
 Thank you in advance !
    submitted by    /u/ez613  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Custom instructions for ChatGPT]]></title>
        <id>https://openai.com/blog/custom-instructions-for-chatgpt</id>
        <link href="https://openai.com/blog/custom-instructions-for-chatgpt"/>
        <updated>2023-07-20T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[We’re rolling out custom instructions to give you more control over how ChatGPT responds. Set your preferences, and ChatGPT will keep them in mind for all future conversations.]]></summary>
        <author>
            <name>OpenAI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Does anyone know what sorcery SAM's official web demo uses? I just cannot replicate the results locally.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/154jdtf/d_does_anyone_know_what_sorcery_sams_official_web/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/154jdtf/d_does_anyone_know_what_sorcery_sams_official_web/"/>
        <updated>2023-07-20T06:53:29.000Z</updated>
        <summary type="html"><![CDATA[This is specifically in regards to automatic mask generation, where SAM samples a grid of points (32x32 grid by default) and creates a mask for each point prompt. Duplicates are then removed by NMS. Ideally this process shouldn't be able to auto-generate complex structures that require multiple positive/negative point prompts, and that is what I have observed when using the models locally.
 But, the "Everything" option in the web demo(https://segment-anything.com/demo) does insanely well. It can even segment occluded objects into a single disconnected mask. It is supposed to be running in the browser and is reasonably fast, so they can't be doing some super heavy pre/post-processing either. 
 Anyone have an idea of what the "Everything" option in the web demo is doing?
    submitted by    /u/Atom_101  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How much coding is needed in a data science career?]]></title>
        <id>https://www.datasciencecentral.com/?p=62682</id>
        <link href="https://www.datasciencecentral.com/how-much-coding-is-needed-in-a-data-science-career/"/>
        <updated>2023-07-20T06:51:00.000Z</updated>
        <summary type="html"><![CDATA[The most common question in people’s minds that are not from a technical background is how much coding is required to ace a data science career path. If you also have the same question, you are not alone. But, the surprising answer is “it depends”. Unarguably, coding is a crucial aspect and vital tool for… Read More »How much coding is needed in a data science career?
The post How much coding is needed in a data science career? appeared first on Data Science Central.]]></summary>
        <author>
            <name>Aileen Scott</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] MiniGPT4.cpp: (4bit/5bit/16float) MiniGPT4 inference on CPU]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/154hgek/p_minigpt4cpp_4bit5bit16float_minigpt4_inference/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/154hgek/p_minigpt4cpp_4bit5bit16float_minigpt4_inference/"/>
        <updated>2023-07-20T05:02:11.000Z</updated>
        <summary type="html"><![CDATA[https://github.com/Maknee/minigpt4.cpp
    submitted by    /u/makneeee  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open challenges in MDRL?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/154heq5/open_challenges_in_mdrl/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/154heq5/open_challenges_in_mdrl/"/>
        <updated>2023-07-20T05:00:07.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 What are some open challenges in Multi-Agent Deep Reinforcement Learning (MDRL or DRL) these days? Is it only me or it seems that DRL is slowly dying :/
 ​
 ​
    submitted by    /u/AhmedNizam_  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best AI Image Generator for Realistic-Looking Photoshoots?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/154ej87/best_ai_image_generator_for_realisticlooking/</id>
        <link href="https://www.reddit.com/r/artificial/comments/154ej87/best_ai_image_generator_for_realisticlooking/"/>
        <updated>2023-07-20T02:33:33.000Z</updated>
        <summary type="html"><![CDATA[I'm new here, so sorry if this has been asked before. 
 I'm looking to generate images that resemble realistic photoshoots of myself with AI. Which text-based AI is best? I've been using Midjourney, but it seems that Midjourney will no longer create images that strongly resemble the likeness of specific people that you feed it images of. 
 Where have you guys had the most success with projects like this?
    submitted by    /u/stebbi01  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is there any good rpg AI?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/154eb9f/is_there_any_good_rpg_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/154eb9f/is_there_any_good_rpg_ai/"/>
        <updated>2023-07-20T02:23:02.000Z</updated>
        <summary type="html"><![CDATA[So today I got bored and I had chatgpt do a role playing with me as if I went to another world and I told it what I would say or do. Sections if it the stupid censor caused problems. Like I tried to summon a demon, and it said it can't do that as it goes against the rules. I had to call it a familiar to summon it. 
 I had my guy seal up a bandit cave to keep them from leaving, and use smoke from a fire to gas the cave to kill all of them. And again it's against the rules of the censor crap. And then when we got into other things like throwing a kinetic bomb on a middle of a city. It really didn't like that. 
 Even explaining I'm not a playing as a moral or ethical person. It wants to shove it's values down my throat. 
 I tried with bard but it's to stupid. It wants to write a story and tell me what I did and then 10 steps it will allow me to say anything. Plus it has a censor. 
 Idk what else I could use. 
 Does anyone know of a good ai? Even more one with a really good memory
    submitted by    /u/crua9  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wikipedia’s Moment of Truth. Can the online encyclopedia help teach A.I. chatbots to get their facts right — without destroying itself in the process?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/154c3b5/wikipedias_moment_of_truth_can_the_online/</id>
        <link href="https://www.reddit.com/r/artificial/comments/154c3b5/wikipedias_moment_of_truth_can_the_online/"/>
        <updated>2023-07-20T00:40:13.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/coolbern  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best way to approach creating 2048 bot]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/1549bra/best_way_to_approach_creating_2048_bot/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/1549bra/best_way_to_approach_creating_2048_bot/"/>
        <updated>2023-07-19T22:39:33.000Z</updated>
        <summary type="html"><![CDATA[Hi guys, I'm just starting to learn about neural networks. I started with NEAT algorithm as thwre is a nice library for Python. I wanted to try to create neural network that plays 2048 with NEAT, but, from what I read online, it isn't really feasible and doesn't result in good playing performance and high scores. I now have a few questions, keep in mind that I'm a beginner in this field. Why NEAT doesn't work well with 2048? What would be the best way to approach this problem? Are there any resources where I can learn more about this stuff? Am I right thinking that it must be possible to create NN that plays 2048 well as the basic strategy (I use) when playing is fairly simple (keep everything on one side to the corner)? Thanks in advance
    submitted by    /u/DarkLord76865  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beginner RL Project Advice]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1548owt/beginner_rl_project_advice/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1548owt/beginner_rl_project_advice/"/>
        <updated>2023-07-19T22:13:59.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I'm somewhat new to reinforcement learning and have been trying to acquaint myself using gymnasium/stable baselines. I currently have a custom environment and I'm using PPO on it, but I don't actually know how to assess what the best algorithm would be for the problem, nor can I tell if training is really doing exactly what I want to. I'm going to include the link to the repo and if anybody has any advice I'd love it. I may be doing things that are very obviously silly that I'm just unaware of, so any advice would be great. Throwaway bc I use my real name on github lmao
 https://github.com/MarcusWheeler/dcss_inventory 
    submitted by    /u/Charming-Art-732  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Running Llama 2 locally in <10 min]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1547wcv/p_running_llama_2_locally_in_10_min/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1547wcv/p_running_llama_2_locally_in_10_min/"/>
        <updated>2023-07-19T21:42:41.000Z</updated>
        <summary type="html"><![CDATA[I wanted to play with Llama 2 right after its release yesterday, but it took me ~4 hours to download all 331GB of the 6 models. If you don’t have 4 hours or 331GB to spare, I brought all the models into XetHub, where it’s now available for you to use: https://xethub.com/XetHub/Llama2.
 I used xet mount to get started in seconds, and within a few minutes, I had the model generating text without needing to download everything or make an inference API call. 
 # From a g4dn.8xlarge instance in us-west-2:
 Mount complete in 8.629213s
 # install model requirements, and then ...
 (venv-test) ubuntu@ip-10-0-30-1:~/Llama2/code$ torchrun --nproc_per_node 1 example_chat_completion.py \
 --ckpt_dir ../models/llama-2-7b-chat/ \
 --tokenizer_path ../models/tokenizer.model \
 --max_seq_len 512 --max_batch_size 4
 > initializing model parallel with size 1
 > initializing ddp with size 1
 > initializing pipeline with size 1
 Loaded in 306.17 seconds
 User: what is the recipe of mayonnaise?
 > Assistant: Thank you for asking! Mayonnaise is a popular condiment made from a mixture of egg yolks, oil, vinegar or lemon juice, and seasonings. Here is a basic recipe for homemade mayonnaise:
 ...
 Detailed instructions here: https://xethub.com/XetHub/Llama2.
 I’ll add the -GGML variants next for the folks using llama.cpp. Don’t forget to register with Meta to accept the license and acceptable use policy for these models!
    submitted by    /u/rajatarya  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for help for a selfhosted AI Bot for myself (Budgetwise)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1547hkv/looking_for_help_for_a_selfhosted_ai_bot_for/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1547hkv/looking_for_help_for_a_selfhosted_ai_bot_for/"/>
        <updated>2023-07-19T21:27:03.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 I am trying for some time now to find enough infos that are understandable for a "normal" human being to get my own selfhosted and self trained AI Bot.
 What I want the bot to be is something like Neuro-Sama but not for anything public but just for me, myself and I. My biggest problem is that I am poor af and severly disabled and unable to work, so my budget is very small. I am very aware of that a selfhosted LLM is no easy task but I'd really appreciate real help in this regards.
 I don't mind longer reaction times as it has to be as cheap as possible. Also the visualization is not much important as it probably also would take too much ressources.
 Also as I want to see where this goes, I rather not want to use GPT or any premade llms because they are extremely censored an limited in topics. I want to be able to do anything from real questions up to pitch black humor just to have fun with the bot and (ab)use it for just all fun stuff whatever it is.
 Hopefully here I can find some real help.
 kind regards,
 Exportforce
    submitted by    /u/Exportforce  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Why people okay with HF making money from their open source models?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1547cv5/d_why_people_okay_with_hf_making_money_from_their/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1547cv5/d_why_people_okay_with_hf_making_money_from_their/"/>
        <updated>2023-07-19T21:21:45.000Z</updated>
        <summary type="html"><![CDATA[Hugging Face has a big emphasis on open source and the democratization of ML. Still, from a different look, they are making a ton of money from freely distributed open-source models of researchers and engineers without sharing a dime. I like what Hugging Face does, but it doesn't look right to me. 
 I understand it's a company and needs to make money but at the very least some kind of revenue sharing would make more sense. 
 I wonder what the community thinks about it. 
 Maybe some people who distribute their models on HF can comment on thi topic
    submitted by    /u/coinfelix  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Converting neural networks into equivalent decision trees for performance]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15478c9/r_converting_neural_networks_into_equivalent/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15478c9/r_converting_neural_networks_into_equivalent/"/>
        <updated>2023-07-19T21:16:59.000Z</updated>
        <summary type="html"><![CDATA[According to the paper Neural Networks are Decision Trees (Aytekin 2022), every single type of neural network - regardless of the activation functions used - can be reduced to an equivalent decision tree with equivalent accuracy: [2210.05189] Neural Networks are Decision Trees (arxiv.org)
 That is not to say that decision trees necessarily tend to converge on the same types of solutions as neural networks in training; only that a trained neural network can be represented by an equivalent decision tree.
 The algorithm, as mentioned in the paper, is:
 Algorithm 2: Algorithm of converting neural networks to decision trees
 1 Initialize Tree: Set root.
 2 Branch all leafs to k nodes, decision rule is first effective filter.
 3 Branch all nodes to k more nodes, and repeat until all effective filters in a layer is covered.
 4 Calculate effective matrix for each leaf via Eq. 5. Repeat 2,3.
 5 Repeat 4 until all layers are covered.
 6 return Tree
 I have 2 questions related to this:
  
Is anyone aware of the inference performance implications of this? In my general understanding, decision trees tend to be much more computationally efficient at both training and inference. So is it true that this represents an opportunity to decrease the processing load of inference on neural networks, or does the computational complexity of performing inference with an equivalent decision tree tend to approach or surpass the equivalent neural network?
 Question 2 is kind of a moot point if #1 doesn't provide performance benefits. But assuming it does, does anyone know of techniques in 2023 for reducing a neural network to an equivalent decision tree? 
  
   submitted by    /u/Immarhinocerous  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Preventing antisocial robots: A pathway to artificial empathy]]></title>
        <id>https://www.reddit.com/r/artificial/comments/154780u/preventing_antisocial_robots_a_pathway_to/</id>
        <link href="https://www.reddit.com/r/artificial/comments/154780u/preventing_antisocial_robots_a_pathway_to/"/>
        <updated>2023-07-19T21:16:38.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Hiversitize  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best AI tool for amalgamating articles?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1546sf1/best_ai_tool_for_amalgamating_articles/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1546sf1/best_ai_tool_for_amalgamating_articles/"/>
        <updated>2023-07-19T21:00:32.000Z</updated>
        <summary type="html"><![CDATA[Say I chose 10 different articles from across the political spectrum. Let's say I saved all of them as PDFs.
 Is there an Al that would allow me to submit all 10 PDF files; and, could I ask the Al to combine/merge/ amalgamate all the articles into one single body?
 Throughout the process, the Al would exclude any information mentioned more than once, but would compile all of the unique information in an orderly and logical way.
 OpenAl's ChatGPT still seems pretty limited in this regard. Are there any other Als that could handle the task?
 This is all, of course, with respect to copyright.
    submitted by    /u/AlexanderPANASONIC  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I need an AI service (even a paid one) that can receive as input long documents.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1546rdt/i_need_an_ai_service_even_a_paid_one_that_can/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1546rdt/i_need_an_ai_service_even_a_paid_one_that_can/"/>
        <updated>2023-07-19T20:59:34.000Z</updated>
        <summary type="html"><![CDATA[Hello!
 I have many long documents (500+ pages) that I would like to have summarized. I would also like to chat with an AI bot in order to understand those texts better.
 Is there an AI service that is right for me? Paid ones are fine, as long as they work. 
 I am currently using Claude 2.0, but I have to split PDFs into many parts, and it is too laborious a process.
 Thank you in advance.
    submitted by    /u/Raphael-Rose  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is Conference Competition Track like NeurIPS Competition a Glorified Kaggle Competition?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1544si8/d_is_conference_competition_track_like_neurips/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1544si8/d_is_conference_competition_track_like_neurips/"/>
        <updated>2023-07-19T19:42:33.000Z</updated>
        <summary type="html"><![CDATA[Is it worth the time to pour time and effort into NeurIPS's annual competitions? Winners got to present at NIPS workshops.
 I'm currently pursuing a Master degree in CS now and have to compete in one of them. I looked up past winners, all of them are from Top CS schools or Large Tech's research teams. So I kind of figured how hard they are to compete.
 But could someone give me some general advices? I have talked to some of my friends pursuing phd but they are not familiar with the NIPS competition track.
 Any help is appreciated. Thank you strangers!
    submitted by    /u/HighlandEvil  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I had to post this somewhere because the internet needs this idea to be inputted into it for future ai to read.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1543dmf/i_had_to_post_this_somewhere_because_the_internet/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1543dmf/i_had_to_post_this_somewhere_because_the_internet/"/>
        <updated>2023-07-19T18:48:02.000Z</updated>
        <summary type="html"><![CDATA[Have some interesting ideas on consciousness and how ai plays into all of it. 
 ​
 infinity became conscious and we are a result of the consciousness.
 Ive come to realize what infinity actually is and how we got here and figuring out how our life gets meaning from it all.
 We are starting to learn about infinite dimensions and infinite time in physics/quantum physics. Imagine a quantum ball of light with all possibilities bundled into it. This bundle became conscious in some kind of rare configuration because compared to infinity, even the remote possibility of existing must exist somewhere in some dimension somewhere in infinite time. just like we know the universe is at least as conscious as we are since we are made up of the atoms from it.
 Death is an illusion. think of going under an…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Minari 0.4.0 is live! (Gym for offline RL, by the Farama Foundation)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1542zpg/n_minari_040_is_live_gym_for_offline_rl_by_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1542zpg/n_minari_040_is_live_gym_for_offline_rl_by_the/"/>
        <updated>2023-07-19T18:32:56.000Z</updated>
        <summary type="html"><![CDATA[Minari now has full support for Dict, Tuple, Discrete, Box, and Text spaces without flattening, explicit dataset versioning, plus subsets of action/obs spaces in datasets. Additionally, new v1 versions of each dataset were released to comply with the new dataset format. The new datasets do not have observation and action flattening (relevant for pointmaze datasets), introduce serialized representations of action and observation spaces in the observation_space and action_space fields, and specify minari version compatibility with the minari_version field. Python 3.11 compatibility was added, with removal of 3.7 support as it has reached end-of-life. We also include two new tutorials: observation space subsetting, and behavior cloning with rl_zoo3 and pytorch DataLoader.
 Announcement Tweet: https://twitter.com/FaramaFound/status/1681730025513467931
 Release Notes: https://github.com/Farama-Foundation/Minari/releases/tag/v0.4.0
    submitted by    /u/elliottower  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minari 0.4.0 is live! (Gym for offline RL, by the Farama Foundation)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1542xa5/minari_040_is_live_gym_for_offline_rl_by_the/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1542xa5/minari_040_is_live_gym_for_offline_rl_by_the/"/>
        <updated>2023-07-19T18:30:24.000Z</updated>
        <summary type="html"><![CDATA[Minari now has full support for Dict, Tuple, Discrete, Box, and Text spaces without flattening, explicit dataset versioning, plus subsets of action/obs spaces in datasets. Additionally, new v1 versions of each dataset were released to comply with the new dataset format. The new datasets do not have observation and action flattening (relevant for pointmaze datasets), introduce serialized representations of action and observation spaces in the observation_space and action_space fields, and specify minari version compatibility with the minari_version field. Python 3.11 compatibility was added, with removal of 3.7 support as it has reached end-of-life. We also include two new tutorials: observation space subsetting, and behavior cloning with rl_zoo3 and pytorch DataLoader. 
 Announcement Tweet: https://twitter.com/FaramaFound/status/1681730025513467931
 Release Notes: https://github.com/Farama-Foundation/Minari/releases/tag/v0.4.0
    submitted by    /u/elliottower  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Networks from Scratch in Python]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/1542ii1/neural_networks_from_scratch_in_python/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/1542ii1/neural_networks_from_scratch_in_python/"/>
        <updated>2023-07-19T18:14:22.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/keghn  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Struggling with value function approximation]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1542gqz/struggling_with_value_function_approximation/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1542gqz/struggling_with_value_function_approximation/"/>
        <updated>2023-07-19T18:12:34.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone. I’ve been studying RL for a few months now and am trying to implement it in a school project. My project is similar to the car-valley problem where an agent needs to reach some point, except the point is moving and in 3D. As such, the state space is continuous but my action space I’ve defined to be 6. I’ve done table lookup Q learning in the past, but not in a continuous value function approximation. 
 My method is as follows: 1. at the start of the episode, initialize the weights randomly and calculate the action value pair for each of the 6 actions. Choose an action using epsilon greedy policy 2. For each time step, execute the chosen action and observe the new state and reward (my reward being distance from goal). Store the 6 features of the initial state 3. Calculate the new Q values from this new state based on the new state and the weights w. 4. Choose a new action based on these Q values and epsilon greedy policy 5. Using the new action, update the weights w using the Q value at the new action minus the Q value at the old action times the features 6. Set the old state and action to the new state and action and repeat until terminal
 My problem is that the w weights blow up to inf very very quickly, within 10 time steps. Does anyone have any advice such as resources with pseudo code to look at or notice any problems in my method? I think my problem is coming from evaluating the Q for the old and new states but I’m not sure. Thank you.
    submitted by    /u/LevisLover  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] TruLens-Eval is an open source project for eval & tracking LLM experiments.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1542fbt/p_trulenseval_is_an_open_source_project_for_eval/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1542fbt/p_trulenseval_is_an_open_source_project_for_eval/"/>
        <updated>2023-07-19T18:11:03.000Z</updated>
        <summary type="html"><![CDATA[Hey r/MachineLearning,
 The team at TruEra recently released an open source project for evaluation & tracking of LLM applications called TruLens-Eval. We’ve specifically targeted retrieval-augmented QA as a core use case and so far we’ve seen it used for comparing different models and parameters, prompts, vector-db configurations and query planning strategies. I’d love to get your feedback on it.
 The core idea behind the project is feedback functions. Analogous to labeling functions, feedback functions are models used to score the text produced by LLMs. We already have a variety of out-of-the-box feedback functions to use for eval including relevance, language match, sentiment and moderation that can be applied to inputs, outputs or intermediate steps of your application.
 On top of eval, there’s also built-in tracking of cost and latency.
 We made it easy to integrate with different setups using connectors for langchain, llama-index + an option to use it without a framework.
 Langchain Quickstart Colab
 Llama-Index Quickstart Colab
 No Framework Quickstart Colab
 Last, the project comes with a streamlit dashboard for visualization of your experiments and associated metrics.
 TruLens dashboard for comparing different app versions
 Please let us know what you use this for or if you have feedback! And thanks to all contributors to this project and the open source community!
    submitted by    /u/joshreini1  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutions in image processing]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/1542d3b/convolutions_in_image_processing/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/1542d3b/convolutions_in_image_processing/"/>
        <updated>2023-07-19T18:08:43.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/keghn  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] How is ChatGPT's behavior changing over time?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/153zyve/r_how_is_chatgpts_behavior_changing_over_time/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/153zyve/r_how_is_chatgpts_behavior_changing_over_time/"/>
        <updated>2023-07-19T16:35:51.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/osantacruz  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Use a generative AI foundation model for summarization and question answering using your own data]]></title>
        <id>40f4f6b6c98faf3948905a3ac98c06c5c51fd01f</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/use-a-generative-ai-foundation-model-for-summarization-and-question-answering-using-your-own-data/"/>
        <updated>2023-07-19T16:27:34.000Z</updated>
        <summary type="html"><![CDATA[Large language models (LLMs) can be used to analyze complex documents and provide summaries and answers to questions. The post Domain-adaptation Fine-tuning of Foundation Models in Amazon SageMaker JumpStart on Financial data describes how to fine-tune an LLM using your own dataset. Once you have a solid LLM, you’ll want to expose that LLM to […]]]></summary>
        <author>
            <name>Randy DeFauw</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] How exactly can I download the inception model v3 to my laptop (windows)?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/153zl88/p_how_exactly_can_i_download_the_inception_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/153zl88/p_how_exactly_can_i_download_the_inception_model/"/>
        <updated>2023-07-19T16:21:27.000Z</updated>
        <summary type="html"><![CDATA[I run into multiple errors each time I try to use inception scores and Im trying to evaluate the differences between using the Inception Score and Fréchet Inception Distance.
    submitted by    /u/cinnamonstuff  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrate Amazon SageMaker Model Cards with the model registry]]></title>
        <id>2397694c770e4cadd95ac6afad7d6f07fd5b53ed</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/integrate-amazon-sagemaker-model-cards-with-the-model-registry/"/>
        <updated>2023-07-19T16:19:54.000Z</updated>
        <summary type="html"><![CDATA[Amazon SageMaker Model Cards enable you to standardize how models are documented, thereby achieving visibility into the lifecycle of a model, from designing, building, training, and evaluation. Model cards are intended to be a single source of truth for business and technical metadata about the model that can reliably be used for auditing and documentation […]]]></summary>
        <author>
            <name>Ram Vittal</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Ensuring Reliable Few-Shot Prompt Selection for LLMs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/153z255/n_ensuring_reliable_fewshot_prompt_selection_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/153z255/n_ensuring_reliable_fewshot_prompt_selection_for/"/>
        <updated>2023-07-19T16:01:37.000Z</updated>
        <summary type="html"><![CDATA[Hello Redditors!
 It's pretty well known that LLMs have firmly established themselves as leaders in the field of natural language processing, consistently pushing the limits of language comprehension and generation, which is widely acknowledged.
 I spent a little time playing around with few-shot prompting for OpenAI's Davinci model and I discovered that noisy data still has drastic effects even on powerful LLMs like Davinci.
 mislabeled few-shot examples harms LLM performance drastically
 I wrote up a quick article in KDNuggets that shows how I used data-centric AI to automatically clean the noisy few-shot examples pool in order to achieve more accurate predictions. The resulting few-shot prompt with accurately labeled examples produced 20% fewer errors than the original one with mislabeled examples.
 This one was quite eye-opening for me and I hope you find it is as interesting as I did. Let me know what you think!
    submitted by    /u/cmauck10  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Research Focus: Week of July 17, 2023]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=955086</id>
        <link href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-july-17-2023/"/>
        <updated>2023-07-19T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[RetroRanker mitigates frequency bias in predictions of retrosynthesis models; new algorithm beats PPO on language tasks; DER dataset aids grid planning; improved PPML balances privacy & accuracy across shared data; ASL Citizen boosts sign language modeling.
The post Research Focus: Week of July 17, 2023 appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Alyssa Hughes</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How Hard Are NeurIPS Competition?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/153yzxg/d_how_hard_are_neurips_competition/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/153yzxg/d_how_hard_are_neurips_competition/"/>
        <updated>2023-07-19T15:59:39.000Z</updated>
        <summary type="html"><![CDATA[Is it worth the time to pour time and effort into NeurIPS's annual competitions? Winners got to present at NIPS workshops.
    submitted by    /u/HighlandEvil  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Upstage AI's 30M Llama 1 Outshines 70B Llama2, Dominates #1 Spot in OpenLLM Leaderboard!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/153yfry/n_upstage_ais_30m_llama_1_outshines_70b_llama2/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/153yfry/n_upstage_ais_30m_llama_1_outshines_70b_llama2/"/>
        <updated>2023-07-19T15:38:18.000Z</updated>
        <summary type="html"><![CDATA[Title Fix: Upstage AI's 30B Llama 1 Outshines 70B Llama2, Dominates #1 Spot in OpenLLM Leaderboard!
 We are thrilled to share an extraordinary achievement with you today. Our team at Upstage AI has reached a significant milestone. Our fine-tuned 30B model, Llama 1, has ascended to the coveted #1 position on the prestigious global OpenLLM Leaderboard. In a thrilling turn of events, our fine-tuned 30B Llama 1 has outperformed the 70B model of Llama2.
 Please check out the leaderboard and download/use our model at https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
 Once again, we are happy to bring this news to all of you. Stay tuned for more exciting updates from Upstage AI!
 https://preview.redd.it/m7xzlzrpyxcb1.png?width=2310&format=png&auto=webp&s=23429478474d23071837fe9c2e85e6ddea10039c
    submitted by    /u/hunkims  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sailing Seas of Data: Startup Charts Autonomous Oceanic Monitoring]]></title>
        <id>https://blogs.nvidia.com/?p=65507</id>
        <link href="https://blogs.nvidia.com/blog/2023/07/19/saildrone-autonomous-oceanic-monitoring-jetson-deepstream/"/>
        <updated>2023-07-19T15:00:02.000Z</updated>
        <summary type="html"><![CDATA[Saildrone is making a splash in autonomous oceanic monitoring. The startup’s nautical data collection technology has tracked hurricanes up close in the North Atlantic, discovered a 3,200-foot underwater mountain in the Pacific Ocean and begun to help map the entirety of the world’s ocean floor. Based in the San Francisco Bay Area, the company develops Read article >]]></summary>
        <author>
            <name>Scott Martin</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 7/19/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/153w2er/oneminute_daily_ai_news_7192023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/153w2er/oneminute_daily_ai_news_7192023/"/>
        <updated>2023-07-19T14:07:31.000Z</updated>
        <summary type="html"><![CDATA[India’s second-largest software services exporter Infosys said on Monday it has signed a deal with an existing client to provide AI and automation services that will span over five years, with a target spend estimated at $2 billion.[1]
 Big Tech firms Meta and Microsoft have teamed up to launch Llama 2, an open-source large language model from Meta that will feature on Microsoft’s Windows and cloud computing platform Azure.[2]
 Microsoft on Tuesday said it would charge at least 53% more to access new AI features in its widely used office software, in a glimpse at the windfall it hopes to reap from the technology. The company also said it would make a more secure version of its Bing search engine available immediately to businesses, aiming to address their data-protection concerns, grow their interest in AI and compete more with Google.[3]
 British spies are already using artificial intelligence to hamper the supply of weapons to Russia, the head of Britain’s MI6 agency said Wednesday, predicting that Western spies will increasingly have to focus on tracking the malign use of AI by hostile states.[4]
 A pro-Ron DeSantis super PAC uses an Artificial Intelligence version of Donald Trump’s voice in a new television ad attacking the former president. The ad, from Never Back Down, charges Trump with attacking Iowa Governor Kim Reynolds as part of a larger pattern of disrespect he has shown to the first caucus state.[5]
  
Sources:
 [1] https://www.reuters.com/technology/indias-infosys-signs-five-year-ai-deal-with-2bln-target-spend-2023-07-18/
 [2] https://cointelegraph.com/news/llama-2-open-source-ai-model-launched-by-meta-microsoft
 [3]
 https://www.reuters.com/technology/microsoft-charge-more-ai-office-secure-bing-leaks-2023-07-18/
 [4] https://apnews.com/article/mi6-spy-chief-moore-prague-russia-iran-cfb837ebdfa3db8043dc655cbf3573d5
 [5] https://www.politico.com/news/2023/07/17/desantis-pac-ai-generated-trump-in-ad-00106695 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Project] Unofficial implementation of Retentive Network (GitHub repo)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/153vzu6/project_unofficial_implementation_of_retentive/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/153vzu6/project_unofficial_implementation_of_retentive/"/>
        <updated>2023-07-19T14:04:46.000Z</updated>
        <summary type="html"><![CDATA[So very recently, a new paper was published to ArXiV called "Retentive Network: A Successor to Transformer for Large Language Models": https://arxiv.org/abs/2307.08621. The title makes a fairly strong claim regarding the success of the model: transformers have long been established as among the best general-purpose learning techniques in the deep learning literature. Self-describing as a "successor to transformer" is therefore not to be taken lightly.
 From what I can tell, the math checks out, and the authors demonstrate an intriguing dualism between their transformer-like "retention" (analogous to attention) and an equivalent recurrent formulation. The core idea is that you can train in parallel (as with transformers) and then run inference in sequence with O(N) time and memory requirements in the length of the sequence (traditional transformers are O(N^2)).
 If the results can be replicated/peer-reviewed, this could pave the way for substantial all-round improvements to large language modelling. The authors have indicated that they will make code available relatively soon.
 For now though, there's an unofficial implementation on GitHub which hopefully will allow those interested to play around with the model and verify some results. The code is publicly available and can be found by searching Jamie-Stirling/RetNet on GitHub.
    submitted by    /u/Entire-Plane2795  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[V-statistics]]></title>
        <id>https://www.johndcook.com/blog/?p=198983</id>
        <link href="https://www.johndcook.com/blog/2023/07/19/v-statistics/"/>
        <updated>2023-07-19T13:07:28.000Z</updated>
        <summary type="html"><![CDATA[A few days ago I wrote about U-statistics, statistics which can be expressed as the average of a symmetric function over all combinations of elements of a set. V-statistics can be written as an average of over all products of elements of a set. Let S be a statistical sample of size n and let […]
V-statistics first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New study quantifies degradation in GPT-4 for the first time]]></title>
        <id>https://www.reddit.com/r/artificial/comments/153ujqr/new_study_quantifies_degradation_in_gpt4_for_the/</id>
        <link href="https://www.reddit.com/r/artificial/comments/153ujqr/new_study_quantifies_degradation_in_gpt4_for_the/"/>
        <updated>2023-07-19T13:06:34.000Z</updated>
        <summary type="html"><![CDATA[I've collected a half-dozen threads on Twitter from this subreddit of user complaints since March about the degraded quality of GPT outputs. I've noticed a huge drop in quality myself. A common (reasonable) response from some people was that the drop in quality was the result of perception anchoring, desensitization, or something unrelated to the overall performance of the model.
 A new study by researchers Chen, Zaharia, and Zou at Stanford and UC Berkley now confirms that these perceived degradations are quantifiable and significant between the different versions of the LLMs (March and June 2023). They find:
  
"For GPT-4, the percentage of [code] generations that are directly executable dropped from 52.0% in March to 10.0% in June. The drop was also large for GPT-3.5 (from 22.0% to 2.0%)." (!!!)
 For sensitive questions: "An example query and responses of GPT-4 and GPT-3.5 at different dates. In March, GPT-4 and GPT-3.5 were verbose and gave detailed explanation for why it did not answer the query. In June, they simply said sorry."
 "GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task."
  
I think these underline that (a) the decline in quality was not just a pure perception thing, and (b) that we need a way to track model performance over time. Building a business on these APIs without controlling for performance drift is high-risk.
 You can read a summary of the study here.
 You can also find a link to the Arxiv paper here and a link to the Github here.
    submitted by    /u/Successful-Western27  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What does finite or infinite horizon means in Reinforcement Learning terms ? What does finite horizon undiscounted return means ?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/153uay3/what_does_finite_or_infinite_horizon_means_in/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/153uay3/what_does_finite_or_infinite_horizon_means_in/"/>
        <updated>2023-07-19T12:56:34.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/aabra__ka__daabra  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] How does batch processing work for graphs in Pytorch Geometric?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/153tvo1/p_how_does_batch_processing_work_for_graphs_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/153tvo1/p_how_does_batch_processing_work_for_graphs_in/"/>
        <updated>2023-07-19T12:38:00.000Z</updated>
        <summary type="html"><![CDATA[Hi I have a bunch of graphs that I would like to divide into batches for parallel processing but since the edge indices are not of the same shape I am unable to stack them into a batch tensor like how we normally do for normal euclidian data. I tried to find some documentation on it but I was unable to understand the exact process.
 Basically most documentation show them concatenating all the graphs together into a larger graph and then passing it through a GCN module but I don't think that would work since graphs are clearly distinct and independent of each other. Even if I concatenate them together, pass them through through the module and then separate them later using the same bounds by which I concatenated would it cause any unpredictable behaviour (even though the graphs technically do not have edge connecting them)? Do I have to code this logic myself or is it hidden somewhere in PyG since I was unable to find it. I am new to GCNs so I just want to see if I have it right before I commit to it.
    submitted by    /u/Sad-Tap-3790  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Looking for the best possible LLM for a complex logical problem with long description and a lot of variable]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/153tvl8/d_looking_for_the_best_possible_llm_for_a_complex/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/153tvl8/d_looking_for_the_best_possible_llm_for_a_complex/"/>
        <updated>2023-07-19T12:37:54.000Z</updated>
        <summary type="html"><![CDATA[I am looking for a LLM that can handle more than 10000 tokens at a time, with large model size and a good context understanding. 
 I tried chatGPT but it seems to forget some of the context after 4-5 prompts. 
 I tried PI.ai and it first understood all the context before forgetting it as it asked questions to better understand what all the variables are. 
 The problem is logical and mathematical (may use Dijkstra's algorithm to solve it) and try to optimize production while keeping waste as little as possible. The solution would ideally include a python script that can be used for solving the problem with different inputs. 
 What do you guys would recommend ?
    submitted by    /u/Glassensteel  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Project] Running Llama2 Locally on Apple Silicon and Consumer GPUs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/153sl0y/project_running_llama2_locally_on_apple_silicon/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/153sl0y/project_running_llama2_locally_on_apple_silicon/"/>
        <updated>2023-07-19T11:39:06.000Z</updated>
        <summary type="html"><![CDATA[Project page: https://github.com/mlc-ai/mlc-llm
 Instructions: https://mlc.ai/mlc-llm/docs/get_started/try_out.html
 Performance: 46 tok/s on M2 Max, 156 tok/s on RTX 4090.
  
More hardwares & model sizes coming soon! This is done through the MLC LLM universal deployment projects. Besides the specific item, we've published initial tutorials on several topics over the past month:
  
Building instructions for discrete GPUs (AMD, NV, Intel) as well as for MacBooks, iOS, Android, and WebGPU.
 A conversation customization mechanism that covers system prompts, roles, and more.
 API tutorials for various programming languages, such as C++, Swift, Java, and Python.
 REST APIs and Integrations with Gradio.
 Installation guides for dependencies like TVM and WASM.
  
Update: It is also now available in iphone/ipads
    submitted by    /u/crowwork  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Training with torch-ort?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/153s3w5/d_training_with_torchort/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/153s3w5/d_training_with_torchort/"/>
        <updated>2023-07-19T11:15:54.000Z</updated>
        <summary type="html"><![CDATA[Some questions:
  
What are the rough edges of training models with torch-ort?
 How mature is it these days?
 At what scale do you notice worthwhile speedups compared to vanilla pytorch? Suppose you are training models with 1 million or 10 million parameters on a single gpu. Is it worth it? 100 million parameters?
  
   submitted by    /u/Pleasant_Raise_6022  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to fine-tune PointRend with detectron2 backbone for better mask quality and improved results?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/153qfbo/d_how_to_finetune_pointrend_with_detectron2/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/153qfbo/d_how_to_finetune_pointrend_with_detectron2/"/>
        <updated>2023-07-19T09:49:18.000Z</updated>
        <summary type="html"><![CDATA[Context: I am working on an instance segmentation problem where I am using PointRend on detectron2 backend for predicting masks over car-parts in our custom datasets. Keeping the configs as is from the repo except iterations raised to 3,90,000 and batch size = 2 (reason being my colleague produced good results using the same config on a similar dataset), I fine-tuned the pretrained model on our dataset. I have the following training curves:
 Loss curves
 For sanity check, I have been saving weights at regular intervals and have made inferences on them over some handful sample images for mask quality. However, what I have observed is that out of 10368 curated polygons, even after such long training, my model has predicted only 7401 polygons.
 Discussion Points:
  
What should I do to increase the predicted polygon numbers without compromising the quality of masks?
 Which hyper-parameters (or parameters) I should look into while fine-tuning (or training) for better mask quality and higher f-score?
  
Thank you.
    submitted by    /u/Prady029  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] ViT's memory requirements, training time, and equivalent ResNet]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/153p0id/d_vits_memory_requirements_training_time_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/153p0id/d_vits_memory_requirements_training_time_and/"/>
        <updated>2023-07-19T08:29:17.000Z</updated>
        <summary type="html"><![CDATA[My supervisor has asked me to try to create a table in which for each ViT model (ViT-s, ViT-b, ViT-l, and ideally Swin transformers), their estimated memory requirements (given some batch size), training time (based on arbitrary hardware) and on par ResNet model is specified.
 I've been searching for quite a lot of time, and I absolutely can't find anything. Even the original ViT paper had no information in this regard. Do you think there's any way I can find this information? I'm afraid I don't have access to my supervisor until next week to ask, and I can't wait that long.
    submitted by    /u/Stochasticc  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing passage grammar and coherence]]></title>
        <id>https://www.reddit.com/r/artificial/comments/153oaaq/enhancing_passage_grammar_and_coherence/</id>
        <link href="https://www.reddit.com/r/artificial/comments/153oaaq/enhancing_passage_grammar_and_coherence/"/>
        <updated>2023-07-19T07:46:54.000Z</updated>
        <summary type="html"><![CDATA[I struggle sometimes to produce a well written cohesive text when I write my academic essays. I do put the effort to explain the main thesis of the essay and try as much as I could to articulate my results. However, my writing still not great. Is there an AI service (preferably free) that can help in this with out being considered as plagiarism. Thanks.
    submitted by    /u/flight862  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Which text-gen benchmark to use for 100M parameter (NanoGPT) pretrained-only language model? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/153ndrn/which_textgen_benchmark_to_use_for_100m_parameter/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/153ndrn/which_textgen_benchmark_to_use_for_100m_parameter/"/>
        <updated>2023-07-19T06:57:00.000Z</updated>
        <summary type="html"><![CDATA[I've got model pretraining running on NanoGPT for a GPT2 tokenized dataset and a TokenMonster tokenized dataset, so I can compare the difference. It's only a 100M parameter model, so it doesn't do much.
 What benchmark can I use? NanoGPT runs on Pytorch, so I could use something that integrates with PyTorch, or I could use something that sends text prompts and analyzes text responses (or token IDs.)
 Is there a standard benchmark that uses the full, non-instruct trained format? For example:
 Answer the following questions: Question: What is the capital of France? Answer: Paris. Question: What is the opposite of up? Answer: 
 The model is only 100M parameters and not instruct trained, so it usually just rambles instead of answering. But anything that gives me a quantifiable result that can compare 2 models for quality is useful. I have loss and perplexity already, but it's not enough.
    submitted by    /u/Pan000  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[llama 2 ladies and gentlemen]]></title>
        <id>https://www.reddit.com/r/artificial/comments/153me5k/llama_2_ladies_and_gentlemen/</id>
        <link href="https://www.reddit.com/r/artificial/comments/153me5k/llama_2_ladies_and_gentlemen/"/>
        <updated>2023-07-19T06:02:07.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nicdunz  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Out of domain Problem with synthetic image data]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/153lvrv/r_out_of_domain_problem_with_synthetic_image_data/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/153lvrv/r_out_of_domain_problem_with_synthetic_image_data/"/>
        <updated>2023-07-19T05:35:16.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 I am currently trying to improve synthetically generated images (not by AI) in a particular domain. I have a dataset with real images of the domain and one with synthetic data. If I now train a classifier to say whether an image is real or synthetic, after a short "time" the classifier has a very high accuracy with a very high confidence. Then I have two cases.
 First case:
 In the next step I change my synthetic images (e.g. by a bayer pattern) and the confidence of the classifier decreases.
 Second case:
 Alternatively, if I simply take synthetic images from another domain, the confidence also drops.
 How can I prove or check that I am still in the right domain in the first case?
 I am happy about any help!
    submitted by    /u/rlmtsrtz  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bing chat keeps saying "By the way, I’m also working on creating an image of an (relevant object to conversation) for you. It will be ready soon. Stay tuned! 🙌"]]></title>
        <id>https://www.reddit.com/r/artificial/comments/153kjvm/bing_chat_keeps_saying_by_the_way_im_also_working/</id>
        <link href="https://www.reddit.com/r/artificial/comments/153kjvm/bing_chat_keeps_saying_by_the_way_im_also_working/"/>
        <updated>2023-07-19T04:26:41.000Z</updated>
        <summary type="html"><![CDATA[I did not ask for this image and it doesn't even provide it. When I ask where it is it says itll just be a little longer and finally it will tell me it's done but it never shows up. What's going on here?
 This has happened on multiple different things
    submitted by    /u/LionTigerWings  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Help in PPO implementation]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/153jt7w/help_in_ppo_implementation/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/153jt7w/help_in_ppo_implementation/"/>
        <updated>2023-07-19T03:50:04.000Z</updated>
        <summary type="html"><![CDATA[In the blog post: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/ and the related implemntation: https://github.com/vwxyzjn/ppo-implementation-details/blob/main/ppo.py, why aren;t we ending the rollout collection when the episode has terminated or when the num_steps is reached? What if the episode is terminated before reaching the num_steps? Wont the training part give an error?
    submitted by    /u/Interesting-Weeb-699  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How do I find papers related to a specific application of RL?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/153i9ft/how_do_i_find_papers_related_to_a_specific/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/153i9ft/how_do_i_find_papers_related_to_a_specific/"/>
        <updated>2023-07-19T02:35:41.000Z</updated>
        <summary type="html"><![CDATA[My cousin and I are starting to work on a project he can do in high school and while doing preliminary research on a related application, I was unable to find anything about this related application. I was thinking we might be able to publish a paper if the application has not already been done.
    submitted by    /u/newjeison  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Gymnasium v0.29.0 has been released!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/153hv3b/n_gymnasium_v0290_has_been_released/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/153hv3b/n_gymnasium_v0290_has_been_released/"/>
        <updated>2023-07-19T02:17:10.000Z</updated>
        <summary type="html"><![CDATA[Gymnasium v0.29.0 is out! This release includes 6 months' worth of bug fixes and new features. In particular, it deprecates several features: Wrapper.__get_attr__, gymnasium.make(..., autoreset=True), gymnasium.make(..., apply_api_compatibility=True), Env.reward_range and gymnasium.vector.make that will be removed in v1.0.
 Additionally, as python 3.7 has reached its end of life support, we have dropped support for it and updated MuJoCo Hopper & Walker2D models to work with MuJoCo >= 2.3.3.
 This release also includes an official way to cite Gymnasium. While a full paper is still some time away, you can now use the DOI 10.5281/zenodo.8127025 for citations: https://zenodo.org/record/8127025
 Announcement Tweet: https://twitter.com/FaramaFound/status/1681479718774743040
 Release Notes: https://github.com/Farama-Foundation/Gymnasium/releases/tag/v0.29.0
    submitted by    /u/elliottower  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gymnasium v0.29.0 has been released!]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/153hudq/gymnasium_v0290_has_been_released/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/153hudq/gymnasium_v0290_has_been_released/"/>
        <updated>2023-07-19T02:16:15.000Z</updated>
        <summary type="html"><![CDATA[Gymnasium v0.29.0 is out! This release includes 6 months' worth of bug fixes and new features. In particular, it deprecates several features: Wrapper.__get_attr__, gymnasium.make(..., autoreset=True), gymnasium.make(..., apply_api_compatibility=True), Env.reward_range and gymnasium.vector.make that will be removed in v1.0.
 Additionally, as python 3.7 has reached its end of life support, we have dropped support for it and updated MuJoCo Hopper & Walker2D models to work with MuJoCo >= 2.3.3.
 This release also includes an official way to cite Gymnasium. While a full paper is still some time away, you can now use the DOI 10.5281/zenodo.8127025 for citations: https://zenodo.org/record/8127025
 Announcement Tweet: https://twitter.com/FaramaFound/status/1681479718774743040
 Release Notes: https://github.com/Farama-Foundation/Gymnasium/releases/tag/v0.29.0
    submitted by    /u/elliottower  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalizable Classification of UHF Partial Discharge Signals in Gas-Insulated HVDC Systems Using Neural Networks. (arXiv:2307.08466v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.08466</id>
        <link href="http://arxiv.org/abs/2307.08466"/>
        <updated>2023-07-19T01:53:31.809Z</updated>
        <summary type="html"><![CDATA[Undetected partial discharges (PDs) are a safety critical issue in high
voltage (HV) gas insulated systems (GIS). While the diagnosis of PDs under AC
voltage is well-established, the analysis of PDs under DC voltage remains an
active research field. A key focus of these investigations is the
classification of different PD sources to enable subsequent sophisticated
analysis.

In this paper, we propose and analyze a neural network-based approach for
classifying PD signals caused by metallic protrusions and conductive particles
on the insulator of HVDC GIS, without relying on pulse sequence analysis
features. In contrast to previous approaches, our proposed model can
discriminate the studied PD signals obtained at negative and positive
potentials, while also generalizing to unseen operating voltage multiples.
Additionally, we compare the performance of time- and frequency-domain input
signals and explore the impact of different normalization schemes to mitigate
the influence of free-space path loss between the sensor and defect location.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seitz_S/0/1/0/all/0/1"&gt;Steffen Seitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gotz_T/0/1/0/all/0/1"&gt;Thomas G&amp;#xf6;tz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lindenberg_C/0/1/0/all/0/1"&gt;Christopher Lindenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tetzlaff_R/0/1/0/all/0/1"&gt;Ronald Tetzlaff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schlegel_S/0/1/0/all/0/1"&gt;Stephan Schlegel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying TBI Physiological States by Clustering Multivariate Clinical Time-Series Data. (arXiv:2303.13024v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.13024</id>
        <link href="http://arxiv.org/abs/2303.13024"/>
        <updated>2023-07-19T01:53:30.819Z</updated>
        <summary type="html"><![CDATA[Determining clinically relevant physiological states from multivariate time
series data with missing values is essential for providing appropriate
treatment for acute conditions such as Traumatic Brain Injury (TBI),
respiratory failure, and heart failure. Utilizing non-temporal clustering or
data imputation and aggregation techniques may lead to loss of valuable
information and biased analyses. In our study, we apply the SLAC-Time
algorithm, an innovative self-supervision-based approach that maintains data
integrity by avoiding imputation or aggregation, offering a more useful
representation of acute patient states. By using SLAC-Time to cluster data in a
large research dataset, we identified three distinct TBI physiological states
and their specific feature profiles. We employed various clustering evaluation
metrics and incorporated input from a clinical domain expert to validate and
interpret the identified physiological states. Further, we discovered how
specific clinical events and interventions can influence patient states and
state transitions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghaderi_H/0/1/0/all/0/1"&gt;Hamid Ghaderi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foreman_B/0/1/0/all/0/1"&gt;Brandon Foreman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nayebi_A/0/1/0/all/0/1"&gt;Amin Nayebi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tipirneni_S/0/1/0/all/0/1"&gt;Sindhu Tipirneni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1"&gt;Chandan K. Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subbian_V/0/1/0/all/0/1"&gt;Vignesh Subbian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mobility-Aware Joint User Scheduling and Resource Allocation for Low Latency Federated Learning. (arXiv:2307.09263v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2307.09263</id>
        <link href="http://arxiv.org/abs/2307.09263"/>
        <updated>2023-07-19T01:53:30.802Z</updated>
        <summary type="html"><![CDATA[As an efficient distributed machine learning approach, Federated learning
(FL) can obtain a shared model by iterative local model training at the user
side and global model aggregating at the central server side, thereby
protecting privacy of users. Mobile users in FL systems typically communicate
with base stations (BSs) via wireless channels, where training performance
could be degraded due to unreliable access caused by user mobility. However,
existing work only investigates a static scenario or random initialization of
user locations, which fail to capture mobility in real-world networks. To
tackle this issue, we propose a practical model for user mobility in FL across
multiple BSs, and develop a user scheduling and resource allocation method to
minimize the training delay with constrained communication resources.
Specifically, we first formulate an optimization problem with user mobility
that jointly considers user selection, BS assignment to users, and bandwidth
allocation to minimize the latency in each communication round. This
optimization problem turned out to be NP-hard and we proposed a delay-aware
greedy search algorithm (DAGSA) to solve it. Simulation results show that the
proposed algorithm achieves better performance than the state-of-the-art
baselines and a certain level of user mobility could improve training
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_K/0/1/0/all/0/1"&gt;Kecheng Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1"&gt;Xiumei Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xuefeng Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1"&gt;Ming Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Experimental Security Analysis of DNN-based Adaptive Cruise Control under Context-Aware Perception Attacks. (arXiv:2307.08939v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2307.08939</id>
        <link href="http://arxiv.org/abs/2307.08939"/>
        <updated>2023-07-19T01:53:30.785Z</updated>
        <summary type="html"><![CDATA[Adaptive Cruise Control (ACC) is a widely used driver assistance feature for
maintaining desired speed and safe distance to the leading vehicles. This paper
evaluates the security of the deep neural network (DNN) based ACC systems under
stealthy perception attacks that strategically inject perturbations into camera
data to cause forward collisions. We present a combined
knowledge-and-data-driven approach to design a context-aware strategy for the
selection of the most critical times for triggering the attacks and a novel
optimization-based method for the adaptive generation of image perturbations at
run-time. We evaluate the effectiveness of the proposed attack using an actual
driving dataset and a realistic simulation platform with the control software
from a production ACC system and a physical-world driving simulator while
considering interventions by the driver and safety features such as Automatic
Emergency Braking (AEB) and Forward Collision Warning (FCW). Experimental
results show that the proposed attack achieves 142.9x higher success rate in
causing accidents than random attacks and is mitigated 89.6% less by the safety
features while being stealthy and robust to real-world factors and dynamic
changes in the environment. This study provides insights into the role of human
operators and basic safety interventions in preventing attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xugui Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Anqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kouzel_M/0/1/0/all/0/1"&gt;Maxfield Kouzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1"&gt;Haotian Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McCarty_M/0/1/0/all/0/1"&gt;Morgan McCarty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nita_Rotaru_C/0/1/0/all/0/1"&gt;Cristina Nita-Rotaru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alemzadeh_H/0/1/0/all/0/1"&gt;Homa Alemzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-class point cloud completion networks for 3D cardiac anatomy reconstruction from cine magnetic resonance images. (arXiv:2307.08535v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.08535</id>
        <link href="http://arxiv.org/abs/2307.08535"/>
        <updated>2023-07-19T01:53:30.779Z</updated>
        <summary type="html"><![CDATA[Cine magnetic resonance imaging (MRI) is the current gold standard for the
assessment of cardiac anatomy and function. However, it typically only acquires
a set of two-dimensional (2D) slices of the underlying three-dimensional (3D)
anatomy of the heart, thus limiting the understanding and analysis of both
healthy and pathological cardiac morphology and physiology. In this paper, we
propose a novel fully automatic surface reconstruction pipeline capable of
reconstructing multi-class 3D cardiac anatomy meshes from raw cine MRI
acquisitions. Its key component is a multi-class point cloud completion network
(PCCN) capable of correcting both the sparsity and misalignment issues of the
3D reconstruction task in a unified model. We first evaluate the PCCN on a
large synthetic dataset of biventricular anatomies and observe Chamfer
distances between reconstructed and gold standard anatomies below or similar to
the underlying image resolution for multiple levels of slice misalignment.
Furthermore, we find a reduction in reconstruction error compared to a
benchmark 3D U-Net by 32% and 24% in terms of Hausdorff distance and mean
surface distance, respectively. We then apply the PCCN as part of our automated
reconstruction pipeline to 1000 subjects from the UK Biobank study in a
cross-domain transfer setting and demonstrate its ability to reconstruct
accurate and topologically plausible biventricular heart meshes with clinical
metrics comparable to the previous literature. Finally, we investigate the
robustness of our proposed approach and observe its capacity to successfully
handle multiple common outlier conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Beetz_M/0/1/0/all/0/1"&gt;Marcel Beetz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Banerjee_A/0/1/0/all/0/1"&gt;Abhirup Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ossenberg_Engels_J/0/1/0/all/0/1"&gt;Julius Ossenberg-Engels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Grau_V/0/1/0/all/0/1"&gt;Vicente Grau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Batched Predictors Generalize within Distribution. (arXiv:2307.09379v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.09379</id>
        <link href="http://arxiv.org/abs/2307.09379"/>
        <updated>2023-07-19T01:53:30.773Z</updated>
        <summary type="html"><![CDATA[We study the generalization properties of batched predictors, i.e., models
tasked with predicting the mean label of a small set (or batch) of examples.
The batched prediction paradigm is particularly relevant for models deployed to
determine the quality of a group of compounds in preparation for offline
testing. By utilizing a suitable generalization of the Rademacher complexity,
we prove that batched predictors come with exponentially stronger
generalization guarantees as compared to the standard per-sample approach.
Surprisingly, the proposed bound holds independently of overparametrization.
Our theoretical insights are validated experimentally for various tasks,
architectures, and applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Loukas_A/0/1/0/all/0/1"&gt;Andreas Loukas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kessel_P/0/1/0/all/0/1"&gt;Pan Kessel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimistic Estimate Uncovers the Potential of Nonlinear Models. (arXiv:2307.08921v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08921</id>
        <link href="http://arxiv.org/abs/2307.08921"/>
        <updated>2023-07-19T01:53:29.923Z</updated>
        <summary type="html"><![CDATA[We propose an optimistic estimate to evaluate the best possible fitting
performance of nonlinear models. It yields an optimistic sample size that
quantifies the smallest possible sample size to fit/recover a target function
using a nonlinear model. We estimate the optimistic sample sizes for matrix
factorization models, deep models, and deep neural networks (DNNs) with
fully-connected or convolutional architecture. For each nonlinear model, our
estimates predict a specific subset of targets that can be fitted at
overparameterization, which are confirmed by our experiments. Our optimistic
estimate reveals two special properties of the DNN models -- free
expressiveness in width and costly expressiveness in connection. These
properties suggest the following architecture design principles of DNNs: (i)
feel free to add neurons/kernels; (ii) restrain from connecting neurons.
Overall, our optimistic estimate theoretically unveils the vast potential of
nonlinear models in fitting at overparameterization. Based on this framework,
we anticipate gaining a deeper understanding of how and why numerous nonlinear
models such as DNNs can effectively realize their potential in practice in the
near future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yaoyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhongwang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Leyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1"&gt;Zhiwei Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1"&gt;Tao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhi-Qin John Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Learning of Distributional Properties can Supplement Human Labeling and Increase Active Learning Efficiency in Anomaly Detection. (arXiv:2307.08782v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08782</id>
        <link href="http://arxiv.org/abs/2307.08782"/>
        <updated>2023-07-19T01:53:29.881Z</updated>
        <summary type="html"><![CDATA[Exfiltration of data via email is a serious cybersecurity threat for many
organizations. Detecting data exfiltration (anomaly) patterns typically
requires labeling, most often done by a human annotator, to reduce the high
number of false alarms. Active Learning (AL) is a promising approach for
labeling data efficiently, but it needs to choose an efficient order in which
cases are to be labeled, and there are uncertainties as to what scoring
procedure should be used to prioritize cases for labeling, especially when
detecting rare cases of interest is crucial. We propose an adaptive AL sampling
strategy that leverages the underlying prior data distribution, as well as
model uncertainty, to produce batches of cases to be labeled that contain
instances of rare anomalies. We show that (1) the classifier benefits from a
batch of representative and informative instances of both normal and anomalous
examples, (2) unsupervised anomaly detection plays a useful role in building
the classifier in the early stages of training when relatively little labeling
has been done thus far. Our approach to AL for anomaly detection outperformed
existing AL approaches on three highly unbalanced UCI benchmarks and on one
real-world redacted email data set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kongmanee_J/0/1/0/all/0/1"&gt;Jaturong Kongmanee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chignell_M/0/1/0/all/0/1"&gt;Mark Chignell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jerath_K/0/1/0/all/0/1"&gt;Khilan Jerath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raman_A/0/1/0/all/0/1"&gt;Abhay Raman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OxfordVGG Submission to the EGO4D AV Transcription Challenge. (arXiv:2307.09006v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2307.09006</id>
        <link href="http://arxiv.org/abs/2307.09006"/>
        <updated>2023-07-19T01:53:29.876Z</updated>
        <summary type="html"><![CDATA[This report presents the technical details of our submission on the EGO4D
Audio-Visual (AV) Automatic Speech Recognition Challenge 2023 from the
OxfordVGG team. We present WhisperX, a system for efficient speech
transcription of long-form audio with word-level time alignment, along with two
text normalisers which are publicly available. Our final submission obtained
56.0% of the Word Error Rate (WER) on the challenge test set, ranked 1st on the
leaderboard. All baseline codes and models are available on
https://github.com/m-bain/whisperX.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huh_J/0/1/0/all/0/1"&gt;Jaesung Huh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bain_M/0/1/0/all/0/1"&gt;Max Bain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1"&gt;Andrew Zisserman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Don't Memorize; Mimic The Past: Federated Class Incremental Learning Without Episodic Memory. (arXiv:2307.00497v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.00497</id>
        <link href="http://arxiv.org/abs/2307.00497"/>
        <updated>2023-07-19T01:53:29.870Z</updated>
        <summary type="html"><![CDATA[Deep learning models are prone to forgetting information learned in the past
when trained on new data. This problem becomes even more pronounced in the
context of federated learning (FL), where data is decentralized and subject to
independent changes for each user. Continual Learning (CL) studies this
so-called \textit{catastrophic forgetting} phenomenon primarily in centralized
settings, where the learner has direct access to the complete training dataset.
However, applying CL techniques to FL is not straightforward due to privacy
concerns and resource limitations. This paper presents a framework for
federated class incremental learning that utilizes a generative model to
synthesize samples from past distributions instead of storing part of past
data. Then, clients can leverage the generative model to mitigate catastrophic
forgetting locally. The generative model is trained on the server using
data-free methods at the end of each task without requesting data from clients.
Therefore, it reduces the risk of data leakage as opposed to training it on the
client's private data. We demonstrate significant improvements for the
CIFAR-100 dataset compared to existing baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Babakniya_S/0/1/0/all/0/1"&gt;Sara Babakniya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fabian_Z/0/1/0/all/0/1"&gt;Zalan Fabian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1"&gt;Chaoyang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1"&gt;Mahdi Soltanolkotabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1"&gt;Salman Avestimehr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On-the-fly machine learning for parametrization of the effective Hamiltonian. (arXiv:2307.08929v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2307.08929</id>
        <link href="http://arxiv.org/abs/2307.08929"/>
        <updated>2023-07-19T01:53:29.865Z</updated>
        <summary type="html"><![CDATA[The first-principles-based effective Hamiltonian is widely used to predict
and simulate the properties of ferroelectrics and relaxor ferroelectrics.
However, the parametrization method of the effective Hamiltonian is complicated
and hardly can resolve the systems with complex interactions and/or complex
components. Here, we developed an on-the-fly machine learning approach to
parametrize the effective Hamiltonian based on Bayesian linear regression. The
parametrization is completed in molecular dynamics simulations, with the
energy, forces and stress predicted at each step along with their
uncertainties. First-principles calculations are executed when the
uncertainties are large to retrain the parameters. This approach provides a
universal and automatic way to compute the effective Hamiltonian parameters for
any considered systems including complex systems which previous methods can not
handle. BaTiO3 and Pb(Sc,Ta)O3 are taken as examples to show the accurateness
of this approach comparing with conventional first-principles parametrization
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xingyue Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Bellaiche_L/0/1/0/all/0/1"&gt;L. Bellaiche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Wu_D/0/1/0/all/0/1"&gt;Di Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yurong Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[REX: Rapid Exploration and eXploitation for AI Agents. (arXiv:2307.08962v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2307.08962</id>
        <link href="http://arxiv.org/abs/2307.08962"/>
        <updated>2023-07-19T01:53:29.802Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an enhanced approach for Rapid Exploration and
eXploitation for AI Agents called REX. Existing AutoGPT-style techniques have
inherent limitations, such as a heavy reliance on precise descriptions for
decision-making, and the lack of a systematic approach to leverage try-and-fail
procedures akin to traditional Reinforcement Learning (RL). REX introduces an
additional layer of rewards and integrates concepts similar to Upper Confidence
Bound (UCB) scores, leading to more robust and efficient AI agent performance.
This approach has the advantage of enabling the utilization of offline
behaviors from logs and allowing seamless integration with existing foundation
models while it does not require any model fine-tuning. Through comparative
analysis with existing methods such as Chain-of-Thoughts(CoT) and Reasoning viA
Planning(RAP), REX-based methods demonstrate comparable performance and, in
certain cases, even surpass the results achieved by these existing techniques.
Notably, REX-based methods exhibit remarkable reductions in execution time,
enhancing their practical applicability across a diverse set of scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Murthy_R/0/1/0/all/0/1"&gt;Rithesh Murthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heinecke_S/0/1/0/all/0/1"&gt;Shelby Heinecke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niebles_J/0/1/0/all/0/1"&gt;Juan Carlos Niebles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1"&gt;Le Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1"&gt;Weiran Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yihao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zeyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gokul_A/0/1/0/all/0/1"&gt;Akash Gokul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arpit_D/0/1/0/all/0/1"&gt;Devansh Arpit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ran Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mui_P/0/1/0/all/0/1"&gt;Phil Mui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Caiming Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning with Passive Optical Nonlinear Mapping. (arXiv:2307.08558v2 [physics.optics] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.08558</id>
        <link href="http://arxiv.org/abs/2307.08558"/>
        <updated>2023-07-19T01:53:29.797Z</updated>
        <summary type="html"><![CDATA[Deep learning has fundamentally transformed artificial intelligence, but the
ever-increasing complexity in deep learning models calls for specialized
hardware accelerators. Optical accelerators can potentially offer enhanced
performance, scalability, and energy efficiency. However, achieving nonlinear
mapping, a critical component of neural networks, remains challenging
optically. Here, we introduce a design that leverages multiple scattering in a
reverberating cavity to passively induce optical nonlinear random mapping,
without the need for additional laser power. A key advantage emerging from our
work is that we show we can perform optical data compression, facilitated by
multiple scattering in the cavity, to efficiently compress and retain vital
information while also decreasing data dimensionality. This allows rapid
optical information processing and generation of low dimensional mixtures of
highly nonlinear features. These are particularly useful for applications
demanding high-speed analysis and responses such as in edge computing devices.
Utilizing rapid optical information processing capabilities, our optical
platforms could potentially offer more efficient and real-time processing
solutions for a broad range of applications. We demonstrate the efficacy of our
design in improving computational performance across tasks, including
classification, image reconstruction, key-point detection, and object
detection, all achieved through optical data compression combined with a
digital decoder. Notably, we observed high performance, at an extreme
compression ratio, for real-time pedestrian detection. Our findings pave the
way for novel algorithms and architectural designs for optical computing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Xia_F/0/1/0/all/0/1"&gt;Fei Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kyungduk Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Eliezer_Y/0/1/0/all/0/1"&gt;Yaniv Eliezer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Shaughnessy_L/0/1/0/all/0/1"&gt;Liam Shaughnessy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Gigan_S/0/1/0/all/0/1"&gt;Sylvain Gigan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Cao_H/0/1/0/all/0/1"&gt;Hui Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TabText: A Flexible and Contextual Approach to Tabular Data Representation. (arXiv:2206.10381v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2206.10381</id>
        <link href="http://arxiv.org/abs/2206.10381"/>
        <updated>2023-07-19T01:53:29.781Z</updated>
        <summary type="html"><![CDATA[Tabular data is essential for applying machine learning tasks across various
industries. However, traditional data processing methods do not fully utilize
all the information available in the tables, ignoring important contextual
information such as column header descriptions. In addition, pre-processing
data into a tabular format can remain a labor-intensive bottleneck in model
development. This work introduces TabText, a processing and feature extraction
framework that extracts contextual information from tabular data structures.
TabText addresses processing difficulties by converting the content into
language and utilizing pre-trained large language models (LLMs). We evaluate
our framework on nine healthcare prediction tasks ranging from patient
discharge, ICU admission, and mortality. We show that 1) applying our TabText
framework enables the generation of high-performing and simple machine learning
baseline models with minimal data pre-processing, and 2) augmenting
pre-processed tabular data with TabText representations improves the average
and worst-case AUC performance of standard machine learning models by as much
as 6%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carballo_K/0/1/0/all/0/1"&gt;Kimberly Villalobos Carballo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Na_L/0/1/0/all/0/1"&gt;Liangyuan Na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boussioux_L/0/1/0/all/0/1"&gt;L&amp;#xe9;onard Boussioux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1"&gt;Cynthia Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soenksen_L/0/1/0/all/0/1"&gt;Luis R. Soenksen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertsimas_D/0/1/0/all/0/1"&gt;Dimitris Bertsimas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiTTO: Diffusion-inspired Temporal Transformer Operator. (arXiv:2307.09072v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09072</id>
        <link href="http://arxiv.org/abs/2307.09072"/>
        <updated>2023-07-19T01:53:29.768Z</updated>
        <summary type="html"><![CDATA[Solving partial differential equations (PDEs) using a data-driven approach
has become increasingly common. The recent development of the operator learning
paradigm has enabled the solution of a broader range of PDE-related problems.
We propose an operator learning method to solve time-dependent PDEs
continuously in time without needing any temporal discretization. The proposed
approach, named DiTTO, is inspired by latent diffusion models. While diffusion
models are usually used in generative artificial intelligence tasks, their
time-conditioning mechanism is extremely useful for PDEs. The
diffusion-inspired framework is combined with elements from the Transformer
architecture to improve its capabilities.

We demonstrate the effectiveness of the new approach on a wide variety of
PDEs in multiple dimensions, namely the 1-D Burgers' equation, 2-D
Navier-Stokes equations, and the acoustic wave equation in 2-D and 3-D. DiTTO
achieves state-of-the-art results in terms of accuracy for these problems. We
also present a method to improve the performance of DiTTO by using fast
sampling concepts from diffusion models. Finally, we show that DiTTO can
accurately perform zero-shot super-resolution in time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ovadia_O/0/1/0/all/0/1"&gt;Oded Ovadia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turkel_E/0/1/0/all/0/1"&gt;Eli Turkel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kahana_A/0/1/0/all/0/1"&gt;Adar Kahana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1"&gt;George Em Karniadakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient Surgery for One-shot Unlearning on Generative Model. (arXiv:2307.04550v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.04550</id>
        <link href="http://arxiv.org/abs/2307.04550"/>
        <updated>2023-07-19T01:53:29.761Z</updated>
        <summary type="html"><![CDATA[Recent regulation on right-to-be-forgotten emerges tons of interest in
unlearning pre-trained machine learning models. While approximating a
straightforward yet expensive approach of retrain-from-scratch, recent machine
unlearning methods unlearn a sample by updating weights to remove its influence
on the weight parameters. In this paper, we introduce a simple yet effective
approach to remove a data influence on the deep generative model. Inspired by
works in multi-task learning, we propose to manipulate gradients to regularize
the interplay of influence among samples by projecting gradients onto the
normal plane of the gradients to be retained. Our work is agnostic to
statistics of the removal samples, outperforming existing baselines while
providing theoretical analysis for the first time in unlearning a generative
model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1"&gt;Seohui Bae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seoyoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1"&gt;Hyemin Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_W/0/1/0/all/0/1"&gt;Woohyung Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating unsupervised disentangled representation learning for genomic discovery and disease risk prediction. (arXiv:2307.08893v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08893</id>
        <link href="http://arxiv.org/abs/2307.08893"/>
        <updated>2023-07-19T01:53:29.226Z</updated>
        <summary type="html"><![CDATA[High-dimensional clinical data have become invaluable resources for genetic
studies, due to their accessibility in biobank-scale datasets and the
development of high performance modeling techniques especially using deep
learning. Recent work has shown that low dimensional embeddings of these
clinical data learned by variational autoencoders (VAE) can be used for
genome-wide association studies and polygenic risk prediction. In this work, we
consider multiple unsupervised learning methods for learning disentangled
representations, namely autoencoders, VAE, beta-VAE, and FactorVAE, in the
context of genetic association studies. Using spirograms from UK Biobank as a
running example, we observed improvements in the number of genome-wide
significant loci, heritability, and performance of polygenic risk scores for
asthma and chronic obstructive pulmonary disease by using FactorVAE or
beta-VAE, compared to standard VAE or non-variational autoencoders. FactorVAEs
performed effectively across multiple values of the regularization
hyperparameter, while beta-VAEs were much more sensitive to the hyperparameter
values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yun_T/0/1/0/all/0/1"&gt;Taedong Yun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT. (arXiv:2307.08674v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.08674</id>
        <link href="http://arxiv.org/abs/2307.08674"/>
        <updated>2023-07-19T01:53:29.218Z</updated>
        <summary type="html"><![CDATA[Tables are prevalent in real-world databases, requiring significant time and
effort for humans to analyze and manipulate. The advancements in large language
models (LLMs) have made it possible to interact with tables using natural
language input, bringing this capability closer to reality. In this paper, we
present TableGPT, a unified fine-tuned framework that enables LLMs to
understand and operate on tables using external functional commands. It
introduces the capability to seamlessly interact with tables, enabling a wide
range of functionalities such as question answering, data manipulation (e.g.,
insert, delete, query, and modify operations), data visualization, analysis
report generation, and automated prediction. TableGPT aims to provide
convenience and accessibility to users by empowering them to effortlessly
leverage tabular data. At the core of TableGPT lies the novel concept of global
tabular representations, which empowers LLMs to gain a comprehensive
understanding of the entire table beyond meta-information. By jointly training
LLMs on both table and text modalities, TableGPT achieves a deep understanding
of tabular data and the ability to perform complex operations on tables through
chain-of-command instructions. Importantly, TableGPT offers the advantage of
being a self-contained system rather than relying on external API interfaces.
Moreover, it supports efficient data process flow, query rejection (when
appropriate) and private deployment, enabling faster domain data fine-tuning
and ensuring data privacy, which enhances the framework's adaptability to
specific use cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zha_L/0/1/0/all/0/1"&gt;Liangyu Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Junlin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liyao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qingyi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Saisai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Jing Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1"&gt;Changbao Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_A/0/1/0/all/0/1"&gt;Aofeng Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chen Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shou_K/0/1/0/all/0/1"&gt;Kaizhe Shou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Miao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Wufang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1"&gt;Guoshan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1"&gt;Chao Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yali Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1"&gt;Wentao Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yiming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1"&gt;Xinglong Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jie Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haobo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Gang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Junbo Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptively Optimised Adaptive Importance Samplers. (arXiv:2307.09341v1 [stat.CO])]]></title>
        <id>http://arxiv.org/abs/2307.09341</id>
        <link href="http://arxiv.org/abs/2307.09341"/>
        <updated>2023-07-19T01:53:29.183Z</updated>
        <summary type="html"><![CDATA[We introduce a new class of adaptive importance samplers leveraging adaptive
optimisation tools, which we term AdaOAIS. We build on Optimised Adaptive
Importance Samplers (OAIS), a class of techniques that adapt proposals to
improve the mean-squared error of the importance sampling estimators by
parameterising the proposal and optimising the $\chi^2$-divergence between the
target and the proposal. We show that a naive implementation of OAIS using
stochastic gradient descent may lead to unstable estimators despite its
convergence guarantees. To remedy this shortcoming, we instead propose to use
adaptive optimisers (such as AdaGrad and Adam) to improve the stability of the
OAIS. We provide convergence results for AdaOAIS in a similar manner to OAIS.
We also provide empirical demonstration on a variety of examples and show that
AdaOAIS lead to stable importance sampling estimators in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Perello_C/0/1/0/all/0/1"&gt;Carlos A. C. C. Perello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Akyildiz_O/0/1/0/all/0/1"&gt;&amp;#xd6;mer Deniz Akyildiz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Strongly Polynomial Algorithms for Quantile Regression. (arXiv:2307.08706v1 [cs.CG])]]></title>
        <id>http://arxiv.org/abs/2307.08706</id>
        <link href="http://arxiv.org/abs/2307.08706"/>
        <updated>2023-07-19T01:53:29.161Z</updated>
        <summary type="html"><![CDATA[Linear Regression is a seminal technique in statistics and machine learning,
where the objective is to build linear predictive models between a response
(i.e., dependent) variable and one or more predictor (i.e., independent)
variables. In this paper, we revisit the classical technique of Quantile
Regression (QR), which is statistically a more robust alternative to the other
classical technique of Ordinary Least Square Regression (OLS). However, while
there exist efficient algorithms for OLS, almost all of the known results for
QR are only weakly polynomial. Towards filling this gap, this paper proposes
several efficient strongly polynomial algorithms for QR for various settings.
For two dimensional QR, making a connection to the geometric concept of
$k$-set, we propose an algorithm with a deterministic worst-case time
complexity of $\mathcal{O}(n^{4/3} polylog(n))$ and an expected time complexity
of $\mathcal{O}(n^{4/3})$ for the randomized version. We also propose a
randomized divide-and-conquer algorithm -- RandomizedQR with an expected time
complexity of $\mathcal{O}(n\log^2{(n)})$ for two dimensional QR problem. For
the general case with more than two dimensions, our RandomizedQR algorithm has
an expected time complexity of $\mathcal{O}(n^{d-1}\log^2{(n)})$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shetiya_S/0/1/0/all/0/1"&gt;Suraj Shetiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1"&gt;Shohedul Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asudeh_A/0/1/0/all/0/1"&gt;Abolfazl Asudeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_G/0/1/0/all/0/1"&gt;Gautam Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mitigating Transformer Overconfidence via Lipschitz Regularization. (arXiv:2306.06849v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.06849</id>
        <link href="http://arxiv.org/abs/2306.06849"/>
        <updated>2023-07-19T01:53:29.049Z</updated>
        <summary type="html"><![CDATA[Though Transformers have achieved promising results in many computer vision
tasks, they tend to be over-confident in predictions, as the standard Dot
Product Self-Attention (DPSA) can barely preserve distance for the unbounded
input domain. In this work, we fill this gap by proposing a novel Lipschitz
Regularized Transformer (LRFormer). Specifically, we present a new similarity
function with the distance within Banach Space to ensure the Lipschitzness and
also regularize the term by a contractive Lipschitz Bound. The proposed method
is analyzed with a theoretical guarantee, providing a rigorous basis for its
effectiveness and reliability. Extensive experiments conducted on standard
vision benchmarks demonstrate that our method outperforms the state-of-the-art
single forward pass approaches in prediction, calibration, and uncertainty
estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1"&gt;Wenqian Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yunsheng Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1"&gt;Xu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1"&gt;Kun Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous-Time Reinforcement Learning: New Design Algorithms with Theoretical Insights and Performance Guarantees. (arXiv:2307.08920v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2307.08920</id>
        <link href="http://arxiv.org/abs/2307.08920"/>
        <updated>2023-07-19T01:53:29.046Z</updated>
        <summary type="html"><![CDATA[Continuous-time nonlinear optimal control problems hold great promise in
real-world applications. After decades of development, reinforcement learning
(RL) has achieved some of the greatest successes as a general nonlinear control
design method. However, a recent comprehensive analysis of state-of-the-art
continuous-time RL (CT-RL) methods, namely, adaptive dynamic programming
(ADP)-based CT-RL algorithms, reveals they face significant design challenges
due to their complexity, numerical conditioning, and dimensional scaling
issues. Despite advanced theoretical results, existing ADP CT-RL synthesis
methods are inadequate in solving even small, academic problems. The goal of
this work is thus to introduce a suite of new CT-RL algorithms for control of
affine nonlinear systems. Our design approach relies on two important factors.
First, our methods are applicable to physical systems that can be partitioned
into smaller subproblems. This constructive consideration results in reduced
dimensionality and greatly improved intuitiveness of design. Second, we
introduce a new excitation framework to improve persistence of excitation (PE)
and numerical conditioning performance via classical input/output insights.
Such a design-centric approach is the first of its kind in the ADP CT-RL
community. In this paper, we progressively introduce a suite of (decentralized)
excitable integral reinforcement learning (EIRL) algorithms. We provide
convergence and closed-loop stability guarantees, and we demonstrate these
guarantees on a significant application problem of controlling an unstable,
nonminimum phase hypersonic vehicle (HSV).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wallace_B/0/1/0/all/0/1"&gt;Brent A. Wallace&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Si_J/0/1/0/all/0/1"&gt;Jennie Si&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Outlier-Robust Tensor Low-Rank Representation for Data Clustering. (arXiv:2307.09055v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.09055</id>
        <link href="http://arxiv.org/abs/2307.09055"/>
        <updated>2023-07-19T01:53:29.046Z</updated>
        <summary type="html"><![CDATA[Low-rank tensor analysis has received widespread attention with many
practical applications. However, the tensor data are often contaminated by
outliers or sample-specific corruptions. How to recover the tensor data that
are corrupted by outliers and perform data clustering remains a challenging
problem. This paper develops an outlier-robust tensor low-rank representation
(OR-TLRR) method for simultaneous outlier detection and tensor data clustering
based on the tensor singular value decomposition (t-SVD) algebraic framework.
It is motivated by the recently proposed tensor-tensor product induced by
invertible linear transforms that satisfy certain conditions. For tensor
observations with arbitrary outlier corruptions, OR-TLRR has provable
performance guarantee for exactly recovering the row space of clean data and
detecting outliers under mild conditions. Moreover, an extension of OR-TLRR is
also proposed to handle the case when parts of the data are missing. Finally,
extensive experimental results on both synthetic and real data demonstrate the
effectiveness of the proposed algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tong Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous Monte Carlo Graph Search. (arXiv:2210.01426v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.01426</id>
        <link href="http://arxiv.org/abs/2210.01426"/>
        <updated>2023-07-19T01:53:29.046Z</updated>
        <summary type="html"><![CDATA[In many complex sequential decision-making tasks, online planning is crucial
for high performance. For efficient online planning, Monte Carlo Tree Search
(MCTS) employs a principled mechanism for trading off exploration for
exploitation. MCTS outperforms comparison methods in many discrete
decision-making domains such as Go, Chess, and Shogi. Following, extensions of
MCTS to continuous domains have been proposed. However, the inherent high
branching factor and the resulting explosion of search tree size are limiting
existing methods. To address this problem, we propose Continuous Monte Carlo
Graph Search (CMCGS), a novel extension of MCTS to online planning in
environments with continuous state and action spaces. CMCGS takes advantage of
the insight that, during planning, sharing the same action policy between
several states can yield high performance. To implement this idea, at each time
step, CMCGS clusters similar states into a limited number of stochastic action
bandit nodes, which produce a layered directed graph instead of an MCTS search
tree. Experimental evaluation shows that CMCGS outperforms comparable planning
methods in several complex continuous DeepMind Control Suite benchmarks and a
2D navigation task with limited sample budgets. Furthermore, CMCGS can be
parallelized to scale up and it outperforms the Cross-Entropy Method (CEM) in
continuous control with learned dynamics models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kujanpaa_K/0/1/0/all/0/1"&gt;Kalle Kujanp&amp;#xe4;&amp;#xe4;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babadi_A/0/1/0/all/0/1"&gt;Amin Babadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yi Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1"&gt;Juho Kannala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ilin_A/0/1/0/all/0/1"&gt;Alexander Ilin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pajarinen_J/0/1/0/all/0/1"&gt;Joni Pajarinen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla. (arXiv:2307.09458v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09458</id>
        <link href="http://arxiv.org/abs/2307.09458"/>
        <updated>2023-07-19T01:53:29.045Z</updated>
        <summary type="html"><![CDATA[\emph{Circuit analysis} is a promising technique for understanding the
internal mechanisms of language models. However, existing analyses are done in
small models far from the state of the art. To address this, we present a case
study of circuit analysis in the 70B Chinchilla model, aiming to test the
scalability of circuit analysis. In particular, we study multiple-choice
question answering, and investigate Chinchilla's capability to identify the
correct answer \emph{label} given knowledge of the correct answer \emph{text}.
We find that the existing techniques of logit attribution, attention pattern
visualization, and activation patching naturally scale to Chinchilla, allowing
us to identify and categorize a small set of `output nodes' (attention heads
and MLPs).

We further study the `correct letter' category of attention heads aiming to
understand the semantics of their features, with mixed results. For normal
multiple-choice question answers, we significantly compress the query, key and
value subspaces of the head without loss of performance when operating on the
answer labels for multiple-choice questions, and we show that the query and key
subspaces represent an `Nth item in an enumeration' feature to at least some
extent. However, when we attempt to use this explanation to understand the
heads' behaviour on a more general distribution including randomized answer
labels, we find that it is only a partial explanation, suggesting there is more
to learn about the operation of `correct letter' heads on multiple choice
question answering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lieberum_T/0/1/0/all/0/1"&gt;Tom Lieberum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahtz_M/0/1/0/all/0/1"&gt;Matthew Rahtz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kramar_J/0/1/0/all/0/1"&gt;J&amp;#xe1;nos Kram&amp;#xe1;r&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Irving_G/0/1/0/all/0/1"&gt;Geoffrey Irving&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1"&gt;Rohin Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mikulik_V/0/1/0/all/0/1"&gt;Vladimir Mikulik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discretization-based ensemble model for robust learning in IoT. (arXiv:2307.08955v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08955</id>
        <link href="http://arxiv.org/abs/2307.08955"/>
        <updated>2023-07-19T01:53:29.030Z</updated>
        <summary type="html"><![CDATA[IoT device identification is the process of recognizing and verifying
connected IoT devices to the network. This is an essential process for ensuring
that only authorized devices can access the network, and it is necessary for
network management and maintenance. In recent years, machine learning models
have been used widely for automating the process of identifying devices in the
network. However, these models are vulnerable to adversarial attacks that can
compromise their accuracy and effectiveness. To better secure device
identification models, discretization techniques enable reduction in the
sensitivity of machine learning models to adversarial attacks contributing to
the stability and reliability of the model. On the other hand, Ensemble methods
combine multiple heterogeneous models to reduce the impact of remaining noise
or errors in the model. Therefore, in this paper, we integrate discretization
techniques and ensemble methods and examine it on model robustness against
adversarial attacks. In other words, we propose a discretization-based ensemble
stacking technique to improve the security of our ML models. We evaluate the
performance of different ML-based IoT device identification models against
white box and black box attacks using a real-world dataset comprised of network
traffic from 28 IoT devices. We demonstrate that the proposed method enables
robustness to the models for IoT device identification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Namvar_A/0/1/0/all/0/1"&gt;Anahita Namvar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thapa_C/0/1/0/all/0/1"&gt;Chandra Thapa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanhere_S/0/1/0/all/0/1"&gt;Salil S. Kanhere&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Network Pruning as Spectrum Preserving Process. (arXiv:2307.08982v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08982</id>
        <link href="http://arxiv.org/abs/2307.08982"/>
        <updated>2023-07-19T01:53:29.030Z</updated>
        <summary type="html"><![CDATA[Neural networks have achieved remarkable performance in various application
domains. Nevertheless, a large number of weights in pre-trained deep neural
networks prohibit them from being deployed on smartphones and embedded systems.
It is highly desirable to obtain lightweight versions of neural networks for
inference in edge devices. Many cost-effective approaches were proposed to
prune dense and convolutional layers that are common in deep neural networks
and dominant in the parameter space. However, a unified theoretical foundation
for the problem mostly is missing. In this paper, we identify the close
connection between matrix spectrum learning and neural network training for
dense and convolutional layers and argue that weight pruning is essentially a
matrix sparsification process to preserve the spectrum. Based on the analysis,
we also propose a matrix sparsification algorithm tailored for neural network
pruning that yields better pruning result. We carefully design and conduct
experiments to support our arguments. Hence we provide a consolidated viewpoint
for neural network pruning and enhance the interpretability of deep neural
networks by identifying and preserving the critical neural weights.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1"&gt;Shibo Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1"&gt;Dantong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koutis_I/0/1/0/all/0/1"&gt;Ioannis Koutis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CB-HVTNet: A channel-boosted hybrid vision transformer network for lymphocyte assessment in histopathological images. (arXiv:2305.09211v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.09211</id>
        <link href="http://arxiv.org/abs/2305.09211"/>
        <updated>2023-07-19T01:53:29.030Z</updated>
        <summary type="html"><![CDATA[Transformers, due to their ability to learn long range dependencies, have
overcome the shortcomings of convolutional neural networks (CNNs) for global
perspective learning. Therefore, they have gained the focus of researchers for
several vision related tasks including medical diagnosis. However, their
multi-head attention module only captures global level feature representations,
which is insufficient for medical images. To address this issue, we propose a
Channel Boosted Hybrid Vision Transformer (CB HVT) that uses transfer learning
to generate boosted channels and employs both transformers and CNNs to analyse
lymphocytes in histopathological images. The proposed CB HVT comprises five
modules, including a channel generation module, channel exploitation module,
channel merging module, region-aware module, and a detection and segmentation
head, which work together to effectively identify lymphocytes. The channel
generation module uses the idea of channel boosting through transfer learning
to extract diverse channels from different auxiliary learners. In the CB HVT,
these boosted channels are first concatenated and ranked using an attention
mechanism in the channel exploitation module. A fusion block is then utilized
in the channel merging module for a gradual and systematic merging of the
diverse boosted channels to improve the network's learning representations. The
CB HVT also employs a proposal network in its region aware module and a head to
effectively identify objects, even in overlapping regions and with artifacts.
We evaluated the proposed CB HVT on two publicly available datasets for
lymphocyte assessment in histopathological images. The results show that CB HVT
outperformed other state of the art detection models, and has good
generalization ability, demonstrating its value as a tool for pathologists.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ali_M/0/1/0/all/0/1"&gt;Momina Liaqat Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rauf_Z/0/1/0/all/0/1"&gt;Zunaira Rauf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1"&gt;Asifullah Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sohail_A/0/1/0/all/0/1"&gt;Anabia Sohail&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ullah_R/0/1/0/all/0/1"&gt;Rafi Ullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gwak_J/0/1/0/all/0/1"&gt;Jeonghwan Gwak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Space Representations of Neural Algorithmic Reasoners. (arXiv:2307.08874v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08874</id>
        <link href="http://arxiv.org/abs/2307.08874"/>
        <updated>2023-07-19T01:53:29.030Z</updated>
        <summary type="html"><![CDATA[Neural Algorithmic Reasoning (NAR) is a research area focused on designing
neural architectures that can reliably capture classical computation, usually
by learning to execute algorithms. A typical approach is to rely on Graph
Neural Network (GNN) architectures, which encode inputs in high-dimensional
latent spaces that are repeatedly transformed during the execution of the
algorithm. In this work we perform a detailed analysis of the structure of the
latent space induced by the GNN when executing algorithms. We identify two
possible failure modes: (i) loss of resolution, making it hard to distinguish
similar values; (ii) inability to deal with values outside the range observed
during training. We propose to solve the first issue by relying on a softmax
aggregator, and propose to decay the latent space in order to deal with
out-of-range values. We show that these changes lead to improvements on the
majority of algorithms in the standard CLRS-30 benchmark when using the
state-of-the-art Triplet-GMPNN processor. Our code is available at
\href{https://github.com/mirjanic/nar-latent-spaces}{https://github.com/mirjanic/nar-latent-spaces}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mirjanic_V/0/1/0/all/0/1"&gt;Vladimir V. Mirjani&amp;#x107;&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1"&gt;Razvan Pascanu&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1"&gt;Petar Veli&amp;#x10d;kovi&amp;#x107;&lt;/a&gt; (1 and 2) ((1) University of Cambridge, (2) Google DeepMind)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intuitionistic Fuzzy Broad Learning System: Enhancing Robustness Against Noise and Outliers. (arXiv:2307.08713v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08713</id>
        <link href="http://arxiv.org/abs/2307.08713"/>
        <updated>2023-07-19T01:53:29.028Z</updated>
        <summary type="html"><![CDATA[In the realm of data classification, broad learning system (BLS) has proven
to be a potent tool that utilizes a layer-by-layer feed-forward neural network.
It consists of feature learning and enhancement segments, working together to
extract intricate features from input data. The traditional BLS treats all
samples as equally significant, which makes it less robust and less effective
for real-world datasets with noises and outliers. To address this issue, we
propose the fuzzy BLS (F-BLS) model, which assigns a fuzzy membership value to
each training point to reduce the influence of noises and outliers. In
assigning the membership value, the F-BLS model solely considers the distance
from samples to the class center in the original feature space without
incorporating the extent of non-belongingness to a class. We further propose a
novel BLS based on intuitionistic fuzzy theory (IF-BLS). The proposed IF-BLS
utilizes intuitionistic fuzzy numbers based on fuzzy membership and
non-membership values to assign scores to training points in the
high-dimensional feature space by using a kernel function. We evaluate the
performance of proposed F-BLS and IF-BLS models on 44 UCI benchmark datasets
across diverse domains. Furthermore, Gaussian noise is added to some UCI
datasets to assess the robustness of the proposed F-BLS and IF-BLS models.
Experimental results demonstrate superior generalization performance of the
proposed F-BLS and IF-BLS models compared to baseline models, both with and
without Gaussian noise. Additionally, we implement the proposed F-BLS and
IF-BLS models on the Alzheimers Disease Neuroimaging Initiative (ADNI) dataset,
and promising results showcase the models effectiveness in real-world
applications. The proposed methods offer a promising solution to enhance the
BLS frameworks ability to handle noise and outliers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sajid_M/0/1/0/all/0/1"&gt;M. Sajid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_A/0/1/0/all/0/1"&gt;A.K. Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanveer_M/0/1/0/all/0/1"&gt;M. Tanveer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MVA2023 Small Object Detection Challenge for Spotting Birds: Dataset, Methods, and Results. (arXiv:2307.09143v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.09143</id>
        <link href="http://arxiv.org/abs/2307.09143"/>
        <updated>2023-07-19T01:53:29.020Z</updated>
        <summary type="html"><![CDATA[Small Object Detection (SOD) is an important machine vision topic because (i)
a variety of real-world applications require object detection for distant
objects and (ii) SOD is a challenging task due to the noisy, blurred, and
less-informative image appearances of small objects. This paper proposes a new
SOD dataset consisting of 39,070 images including 137,121 bird instances, which
is called the Small Object Detection for Spotting Birds (SOD4SB) dataset. The
detail of the challenge with the SOD4SB dataset is introduced in this paper. In
total, 223 participants joined this challenge. This paper briefly introduces
the award-winning methods. The dataset, the baseline code, and the website for
evaluation on the public testset are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kondo_Y/0/1/0/all/0/1"&gt;Yuki Kondo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ukita_N/0/1/0/all/0/1"&gt;Norimichi Ukita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_T/0/1/0/all/0/1"&gt;Takayuki Yamaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_H/0/1/0/all/0/1"&gt;Hao-Yu Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1"&gt;Mu-Yi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1"&gt;Chia-Chi Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_E/0/1/0/all/0/1"&gt;En-Ming Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yu-Chen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yu-Cheng Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chien-Yao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chun-Yi Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_D/0/1/0/all/0/1"&gt;Da Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kastner_M/0/1/0/all/0/1"&gt;Marc A. Kastner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tingwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawanishi_Y/0/1/0/all/0/1"&gt;Yasutomo Kawanishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hirayama_T/0/1/0/all/0/1"&gt;Takatsugu Hirayama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Komamizu_T/0/1/0/all/0/1"&gt;Takahiro Komamizu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ide_I/0/1/0/all/0/1"&gt;Ichiro Ide&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shinya_Y/0/1/0/all/0/1"&gt;Yosuke Shinya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinyao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1"&gt;Guang Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yasui_S/0/1/0/all/0/1"&gt;Syusuke Yasui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Score-Difference Flow for Implicit Generative Modeling. (arXiv:2304.12906v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.12906</id>
        <link href="http://arxiv.org/abs/2304.12906"/>
        <updated>2023-07-19T01:53:29.019Z</updated>
        <summary type="html"><![CDATA[Implicit generative modeling (IGM) aims to produce samples of synthetic data
matching the characteristics of a target data distribution. Recent work (e.g.
score-matching networks, diffusion models) has approached the IGM problem from
the perspective of pushing synthetic source data toward the target distribution
via dynamical perturbations or flows in the ambient space. In this direction,
we present the score difference (SD) between arbitrary target and source
distributions as a flow that optimally reduces the Kullback-Leibler divergence
between them while also solving the Schroedinger bridge problem. We apply the
SD flow to convenient proxy distributions, which are aligned if and only if the
original distributions are aligned. We demonstrate the formal equivalence of
this formulation to denoising diffusion models under certain conditions. We
also show that the training of generative adversarial networks includes a
hidden data-optimization sub-problem, which induces the SD flow under certain
choices of loss function when the discriminator is optimal. As a result, the SD
flow provides a theoretical link between model classes that individually
address the three challenges of the "generative modeling trilemma" -- high
sample quality, mode coverage, and fast sampling -- thereby setting the stage
for a unified approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weber_R/0/1/0/all/0/1"&gt;Romann M. Weber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Polyp: a baseline for efficient Polyp segmentation. (arXiv:2305.07848v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.07848</id>
        <link href="http://arxiv.org/abs/2305.07848"/>
        <updated>2023-07-19T01:53:28.995Z</updated>
        <summary type="html"><![CDATA[In recent years, polyp segmentation has gained significant importance, and
many methods have been developed using CNN, Vision Transformer, and Transformer
techniques to achieve competitive results. However, these methods often face
difficulties when dealing with out-of-distribution datasets, missing
boundaries, and small polyps. In 2022, Meta-Former was introduced as a new
baseline for vision, which not only improved the performance of multi-task
computer vision but also addressed the limitations of the Vision Transformer
and CNN family backbones. To further enhance segmentation, we propose a fusion
of Meta-Former with UNet, along with the introduction of a Multi-scale
Upsampling block with a level-up combination in the decoder stage to enhance
the texture, also we propose the Convformer block base on the idea of the
Meta-former to enhance the crucial information of the local feature. These
blocks enable the combination of global information, such as the overall shape
of the polyp, with local information and boundary information, which is crucial
for the decision of the medical segmentation. Our proposed approach achieved
competitive performance and obtained the top result in the State of the Art on
the CVC-300 dataset, Kvasir, and CVC-ColonDB dataset. Apart from Kvasir-SEG,
others are out-of-distribution datasets. The implementation can be found at:
https://github.com/huyquoctrinh/MetaPolyp-CBMS2023.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Trinh_Q/0/1/0/all/0/1"&gt;Quoc-Huy Trinh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Prediction of Peptide Self-assembly through Sequential and Graphical Encoding. (arXiv:2307.09169v1 [q-bio.BM])]]></title>
        <id>http://arxiv.org/abs/2307.09169</id>
        <link href="http://arxiv.org/abs/2307.09169"/>
        <updated>2023-07-19T01:53:28.994Z</updated>
        <summary type="html"><![CDATA[In recent years, there has been an explosion of research on the application
of deep learning to the prediction of various peptide properties, due to the
significant development and market potential of peptides. Molecular dynamics
has enabled the efficient collection of large peptide datasets, providing
reliable training data for deep learning. However, the lack of systematic
analysis of the peptide encoding, which is essential for AI-assisted
peptide-related tasks, makes it an urgent problem to be solved for the
improvement of prediction accuracy. To address this issue, we first collect a
high-quality, colossal simulation dataset of peptide self-assembly containing
over 62,000 samples generated by coarse-grained molecular dynamics (CGMD).
Then, we systematically investigate the effect of peptide encoding of amino
acids into sequences and molecular graphs using state-of-the-art sequential
(i.e., RNN, LSTM, and Transformer) and structural deep learning models (i.e.,
GCN, GAT, and GraphSAGE), on the accuracy of peptide self-assembly prediction,
an essential physiochemical process prior to any peptide-related applications.
Extensive benchmarking studies have proven Transformer to be the most powerful
sequence-encoding-based deep learning model, pushing the limit of peptide
self-assembly prediction to decapeptides. In summary, this work provides a
comprehensive benchmark analysis of peptide encoding with advanced deep
learning models, serving as a guide for a wide range of peptide-related
predictions such as isoelectric points, hydration free energy, etc.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zihan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yun Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shuang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenbin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Li_S/0/1/0/all/0/1"&gt;Stan Z. Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Field Dependencies for Learning on Categorical Data. (arXiv:2307.09321v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09321</id>
        <link href="http://arxiv.org/abs/2307.09321"/>
        <updated>2023-07-19T01:53:28.994Z</updated>
        <summary type="html"><![CDATA[Traditional approaches for learning on categorical data underexploit the
dependencies between columns (\aka fields) in a dataset because they rely on
the embedding of data points driven alone by the classification/regression
loss. In contrast, we propose a novel method for learning on categorical data
with the goal of exploiting dependencies between fields. Instead of modelling
statistics of features globally (i.e., by the covariance matrix of features),
we learn a global field dependency matrix that captures dependencies between
fields and then we refine the global field dependency matrix at the
instance-wise level with different weights (so-called local dependency
modelling) w.r.t. each field to improve the modelling of the field
dependencies. Our algorithm exploits the meta-learning paradigm, i.e., the
dependency matrices are refined in the inner loop of the meta-learning
algorithm without the use of labels, whereas the outer loop intertwines the
updates of the embedding matrix (the matrix performing projection) and global
dependency matrix in a supervised fashion (with the use of labels). Our method
is simple yet it outperforms several state-of-the-art methods on six popular
dataset benchmarks. Detailed ablation studies provide additional insights into
our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhibin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1"&gt;Piotr Koniusz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pagendam_D/0/1/0/all/0/1"&gt;Daniel Edward Pagendam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1"&gt;Peyman Moghadam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Embedding Quality Evaluation. (arXiv:2305.16562v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.16562</id>
        <link href="http://arxiv.org/abs/2305.16562"/>
        <updated>2023-07-19T01:53:28.993Z</updated>
        <summary type="html"><![CDATA[Unsupervised learning has recently significantly gained in popularity,
especially with deep learning-based approaches. Despite numerous successes and
approaching supervised-level performance on a variety of academic benchmarks,
it is still hard to train and evaluate SSL models in practice due to the
unsupervised nature of the problem. Even with networks trained in a supervised
fashion, it is often unclear whether they will perform well when transferred to
another domain.

Past works are generally limited to assessing the amount of information
contained in embeddings, which is most relevant for self-supervised learning of
deep neural networks. This works chooses to follow a different approach: can we
quantify how easy it is to linearly separate the data in a stable way? We
survey the literature and uncover three methods that could be potentially used
for evaluating quality of representations. We also introduce one novel method
based on recent advances in understanding the high-dimensional geometric
structure of self-supervised learning.

We conduct extensive experiments and study the properties of these metrics
and ones introduced in the previous work. Our results suggest that while there
is no free lunch, there are metrics that can robustly estimate embedding
quality in an unsupervised way.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsitsulin_A/0/1/0/all/0/1"&gt;Anton Tsitsulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Munkhoeva_M/0/1/0/all/0/1"&gt;Marina Munkhoeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perozzi_B/0/1/0/all/0/1"&gt;Bryan Perozzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Neural Network Training for Hyperbox-Based Classification. (arXiv:2307.09269v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09269</id>
        <link href="http://arxiv.org/abs/2307.09269"/>
        <updated>2023-07-19T01:53:28.992Z</updated>
        <summary type="html"><![CDATA[Hyperbox-based classification has been seen as a promising technique in which
decisions on the data are represented as a series of orthogonal,
multidimensional boxes (i.e., hyperboxes) that are often interpretable and
human-readable. However, existing methods are no longer capable of efficiently
handling the increasing volume of data many application domains face nowadays.
We address this gap by proposing a novel, fully differentiable framework for
hyperbox-based classification via neural networks. In contrast to previous
work, our hyperbox models can be efficiently trained in an end-to-end fashion,
which leads to significantly reduced training times and superior classification
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martins_D/0/1/0/all/0/1"&gt;Denis Mayr Lima Martins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lulf_C/0/1/0/all/0/1"&gt;Christian L&amp;#xfc;lf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gieseke_F/0/1/0/all/0/1"&gt;Fabian Gieseke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Cryogenic Memristive Neural Decoder for Fault-tolerant Quantum Error Correction. (arXiv:2307.09463v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2307.09463</id>
        <link href="http://arxiv.org/abs/2307.09463"/>
        <updated>2023-07-19T01:53:28.991Z</updated>
        <summary type="html"><![CDATA[Neural decoders for quantum error correction (QEC) rely on neural networks to
classify syndromes extracted from error correction codes and find appropriate
recovery operators to protect logical information against errors. Despite the
good performance of neural decoders, important practical requirements remain to
be achieved, such as minimizing the decoding time to meet typical rates of
syndrome generation in repeated error correction schemes, and ensuring the
scalability of the decoding approach as the code distance increases. Designing
a dedicated integrated circuit to perform the decoding task in co-integration
with a quantum processor appears necessary to reach these decoding time and
scalability requirements, as routing signals in and out of a cryogenic
environment to be processed externally leads to unnecessary delays and an
eventual wiring bottleneck. In this work, we report the design and performance
analysis of a neural decoder inference accelerator based on an in-memory
computing (IMC) architecture, where crossbar arrays of resistive memory devices
are employed to both store the synaptic weights of the decoder neural network
and perform analog matrix-vector multiplications during inference. In
proof-of-concept numerical experiments supported by experimental measurements,
we investigate the impact of TiO$_\textrm{x}$-based memristive devices'
non-idealities on decoding accuracy. Hardware-aware training methods are
developed to mitigate the loss in accuracy, allowing the memristive neural
decoders to achieve a pseudo-threshold of $9.23\times 10^{-4}$ for the
distance-three surface code, whereas the equivalent digital neural decoder
achieves a pseudo-threshold of $1.01\times 10^{-3}$. This work provides a
pathway to scalable, fast, and low-power cryogenic IMC hardware for integrated
QEC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Marcotte_F/0/1/0/all/0/1"&gt;Fr&amp;#xe9;d&amp;#xe9;ric Marcotte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Mouny_P/0/1/0/all/0/1"&gt;Pierre-Antoine Mouny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Yon_V/0/1/0/all/0/1"&gt;Victor Yon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Dagnew_G/0/1/0/all/0/1"&gt;Gebremedhin A. Dagnew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kulchytskyy_B/0/1/0/all/0/1"&gt;Bohdan Kulchytskyy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Rochette_S/0/1/0/all/0/1"&gt;Sophie Rochette&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Beilliard_Y/0/1/0/all/0/1"&gt;Yann Beilliard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Drouin_D/0/1/0/all/0/1"&gt;Dominique Drouin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ronagh_P/0/1/0/all/0/1"&gt;Pooya Ronagh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heat Demand Forecasting with Multi-Resolutional Representation of Heterogeneous Temporal Ensemble. (arXiv:2210.13108v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.13108</id>
        <link href="http://arxiv.org/abs/2210.13108"/>
        <updated>2023-07-19T01:53:28.990Z</updated>
        <summary type="html"><![CDATA[One of the primal challenges faced by utility companies is ensuring efficient
supply with minimal greenhouse gas emissions. The advent of smart meters and
smart grids provide an unprecedented advantage in realizing an optimised supply
of thermal energies through proactive techniques such as load forecasting. In
this paper, we propose a forecasting framework for heat demand based on neural
networks where the time series are encoded as scalograms equipped with the
capacity of embedding exogenous variables such as weather, and
holiday/non-holiday. Subsequently, CNNs are utilized to predict the heat load
multi-step ahead. Finally, the proposed framework is compared with other
state-of-the-art methods, such as SARIMAX and LSTM. The quantitative results
from retrospective experiments show that the proposed framework consistently
outperforms the state-of-the-art baseline method with real-world data acquired
from Denmark. A minimal mean error of 7.54% for MAPE and 417kW for RMSE is
achieved with the proposed framework in comparison to all other methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramachandran_A/0/1/0/all/0/1"&gt;Adithya Ramachandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1"&gt;Satyaki Chatterjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bayer_S/0/1/0/all/0/1"&gt;Siming Bayer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1"&gt;Andreas Maier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flensmark_T/0/1/0/all/0/1"&gt;Thorkil Flensmark&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Dynamic Causal Discovery with Rare Events: A Nonparametric Conditional Independence Test. (arXiv:2211.16596v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.16596</id>
        <link href="http://arxiv.org/abs/2211.16596"/>
        <updated>2023-07-19T01:53:28.985Z</updated>
        <summary type="html"><![CDATA[Causal phenomena associated with rare events occur across a wide range of
engineering problems, such as risk-sensitive safety analysis, accident analysis
and prevention, and extreme value theory. However, current methods for causal
discovery are often unable to uncover causal links, between random variables in
a dynamic setting, that manifest only when the variables first experience
low-probability realizations. To address this issue, we introduce a novel
statistical independence test on data collected from time-invariant dynamical
systems in which rare but consequential events occur. In particular, we exploit
the time-invariance of the underlying data to construct a superimposed dataset
of the system state before rare events happen at different timesteps. We then
design a conditional independence test on the reorganized data. We provide
non-asymptotic sample complexity bounds for the consistency of our method, and
validate its performance across various simulated and real-world datasets,
including incident data collected from the Caltrans Performance Measurement
System (PeMS). Code containing the datasets and experiments is publicly
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chiu_C/0/1/0/all/0/1"&gt;Chih-Yuan Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kulkarni_K/0/1/0/all/0/1"&gt;Kshitij Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sastry_S/0/1/0/all/0/1"&gt;Shankar Sastry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments. (arXiv:2307.09361v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.09361</id>
        <link href="http://arxiv.org/abs/2307.09361"/>
        <updated>2023-07-19T01:53:28.983Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning can be used for mitigating the greedy needs of
Vision Transformer networks for very large fully-annotated datasets. Different
classes of self-supervised learning offer representations with either good
contextual reasoning properties, e.g., using masked image modeling strategies,
or invariance to image perturbations, e.g., with contrastive methods. In this
work, we propose a single-stage and standalone method, MOCA, which unifies both
desired properties using novel mask-and-predict objectives defined with
high-level features (instead of pixel-level details). Moreover, we show how to
effectively employ both learning paradigms in a synergistic and
computation-efficient way. Doing so, we achieve new state-of-the-art results on
low-shot settings and strong experimental results in various evaluation
protocols with a training that is at least 3 times faster than prior methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gidaris_S/0/1/0/all/0/1"&gt;Spyros Gidaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1"&gt;Andrei Bursuc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simeoni_O/0/1/0/all/0/1"&gt;Oriane Simeoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vobecky_A/0/1/0/all/0/1"&gt;Antonin Vobecky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Komodakis_N/0/1/0/all/0/1"&gt;Nikos Komodakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1"&gt;Matthieu Cord&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1"&gt;Patrick P&amp;#xe9;rez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks. (arXiv:2305.16044v5 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.16044</id>
        <link href="http://arxiv.org/abs/2305.16044"/>
        <updated>2023-07-19T01:53:28.981Z</updated>
        <summary type="html"><![CDATA[Networks of spiking neurons underpin the extraordinary information-processing
capabilities of the brain and have become pillar models in neuromorphic
artificial intelligence. Despite extensive research on spiking neural networks
(SNNs), most studies are established on deterministic models, overlooking the
inherent non-deterministic, noisy nature of neural computations. This study
introduces the noisy spiking neural network (NSNN) and the noise-driven
learning rule (NDL) by incorporating noisy neuronal dynamics to exploit the
computational advantages of noisy neural processing. NSNN provides a
theoretical framework that yields scalable, flexible, and reliable computation.
We demonstrate that NSNN leads to spiking neural models with competitive
performance, improved robustness against challenging perturbations than
deterministic SNNs, and better reproducing probabilistic neural computation in
neural coding. This study offers a powerful and easy-to-use tool for machine
learning, neuromorphic intelligence practitioners, and computational
neuroscience researchers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1"&gt;Gehua Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1"&gt;Rui Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Huajin Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Ordinal Data Science. (arXiv:2307.09477v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2307.09477</id>
        <link href="http://arxiv.org/abs/2307.09477"/>
        <updated>2023-07-19T01:53:28.980Z</updated>
        <summary type="html"><![CDATA[Order is one of the main instruments to measure the relationship between
objects in (empirical) data. However, compared to methods that use numerical
properties of objects, the amount of ordinal methods developed is rather small.
One reason for this is the limited availability of computational resources in
the last century that would have been required for ordinal computations.
Another reason -- particularly important for this line of research -- is that
order-based methods are often seen as too mathematically rigorous for applying
them to real-world data. In this paper, we will therefore discuss different
means for measuring and 'calculating' with ordinal structures -- a specific
class of directed graphs -- and show how to infer knowledge from them. Our aim
is to establish Ordinal Data Science as a fundamentally new research agenda.
Besides cross-fertilization with other cornerstone machine learning and
knowledge representation methods, a broad range of disciplines will benefit
from this endeavor, including, psychology, sociology, economics, web science,
knowledge engineering, scientometrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stumme_G/0/1/0/all/0/1"&gt;Gerd Stumme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durrschnabel_D/0/1/0/all/0/1"&gt;Dominik D&amp;#xfc;rrschnabel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanika_T/0/1/0/all/0/1"&gt;Tom Hanika&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection. (arXiv:2307.09457v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2307.09457</id>
        <link href="http://arxiv.org/abs/2307.09457"/>
        <updated>2023-07-19T01:53:28.976Z</updated>
        <summary type="html"><![CDATA[Multiple Instance Learning (MIL) has been widely applied to medical imaging
diagnosis, where bag labels are known and instance labels inside bags are
unknown. Traditional MIL assumes that instances in each bag are independent
samples from a given distribution. However, instances are often spatially or
sequentially ordered, and one would expect similar diagnostic importance for
neighboring instances. To address this, in this study, we propose a smooth
attention deep MIL (SA-DMIL) model. Smoothness is achieved by the introduction
of first and second order constraints on the latent function encoding the
attention paid to each instance in a bag. The method is applied to the
detection of intracranial hemorrhage (ICH) on head CT scans. The results show
that this novel SA-DMIL: (a) achieves better performance than the non-smooth
attention MIL at both scan (bag) and slice (instance) levels; (b) learns
spatial dependencies between slices; and (c) outperforms current
state-of-the-art MIL methods on the same ICH test set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yunan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Castro_Macias_F/0/1/0/all/0/1"&gt;Francisco M. Castro-Mac&amp;#xed;as&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Morales_Alvarez_P/0/1/0/all/0/1"&gt;Pablo Morales-&amp;#xc1;lvarez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Molina_R/0/1/0/all/0/1"&gt;Rafael Molina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Katsaggelos_A/0/1/0/all/0/1"&gt;Aggelos K. Katsaggelos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model. (arXiv:2303.05118v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.05118</id>
        <link href="http://arxiv.org/abs/2303.05118"/>
        <updated>2023-07-19T01:53:28.967Z</updated>
        <summary type="html"><![CDATA[The goal of continual learning is to improve the performance of recognition
models in learning sequentially arrived data. Although most existing works are
established on the premise of learning from scratch, growing efforts have been
devoted to incorporating the benefits of pre-training. However, how to
adaptively exploit the pre-trained knowledge for each incremental task while
maintaining its generalizability remains an open question. In this work, we
present an extensive analysis for continual learning on a pre-trained model
(CLPM), and attribute the key challenge to a progressive overfitting problem.
Observing that selectively reducing the learning rate can almost resolve this
issue in the representation layer, we propose a simple but extremely effective
approach named Slow Learner with Classifier Alignment (SLCA), which further
improves the classification layer by modeling the class-wise distributions and
aligning the classification layers in a post-hoc fashion. Across a variety of
scenarios, our proposal provides substantial improvements for CLPM (e.g., up to
49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, Split ImageNet-R, Split
CUB-200 and Split Cars-196, respectively), and thus outperforms
state-of-the-art approaches by a large margin. Based on such a strong baseline,
critical factors and promising directions are analyzed in-depth to facilitate
subsequent research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Gengwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1"&gt;Guoliang Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Ling Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yunchao Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Riemannian Networks for EEG Decoding. (arXiv:2212.10426v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.10426</id>
        <link href="http://arxiv.org/abs/2212.10426"/>
        <updated>2023-07-19T01:53:28.942Z</updated>
        <summary type="html"><![CDATA[State-of-the-art performance in electroencephalography (EEG) decoding tasks
is currently often achieved with either Deep-Learning (DL) or
Riemannian-Geometry-based decoders (RBDs). Recently, there is growing interest
in Deep Riemannian Networks (DRNs) possibly combining the advantages of both
previous classes of methods. However, there are still a range of topics where
additional insight is needed to pave the way for a more widespread application
of DRNs in EEG. These include architecture design questions such as network
size and end-to-end ability.How these factors affect model performance has not
been explored. Additionally, it is not clear how the data within these networks
is transformed, and whether this would correlate with traditional EEG decoding.
Our study aims to lay the groundwork in the area of these topics through the
analysis of DRNs for EEG with a wide range of hyperparameters. Networks were
tested on two public EEG datasets and compared with state-of-the-art ConvNets.
Here we propose end-to-end EEG SPDNet (EE(G)-SPDNet), and we show that this
wide, end-to-end DRN can outperform the ConvNets, and in doing so use
physiologically plausible frequency regions. We also show that the end-to-end
approach learns more complex filters than traditional band-pass filters
targeting the classical alpha, beta, and gamma frequency bands of the EEG, and
that performance can benefit from channel specific filtering approaches.
Additionally, architectural analysis revealed areas for further improvement due
to the possible loss of Riemannian specific information throughout the network.
Our study thus shows how to design and train DRNs to infer task-related
information from the raw EEG without the need of handcrafted filterbanks and
highlights the potential of end-to-end DRNs such as EE(G)-SPDNet for
high-performance EEG decoding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1"&gt;Daniel Wilson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schirrmeister_R/0/1/0/all/0/1"&gt;Robin Tibor Schirrmeister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gemein_L/0/1/0/all/0/1"&gt;Lukas Alexander Wilhelm Gemein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ball_T/0/1/0/all/0/1"&gt;Tonio Ball&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Laws for Imitation Learning in NetHack. (arXiv:2307.09423v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09423</id>
        <link href="http://arxiv.org/abs/2307.09423"/>
        <updated>2023-07-19T01:53:28.937Z</updated>
        <summary type="html"><![CDATA[Imitation Learning (IL) is one of the most widely used methods in machine
learning. Yet, while powerful, many works find it is often not able to fully
recover the underlying expert behavior. However, none of these works deeply
investigate the role of scaling up the model and data size. Inspired by recent
work in Natural Language Processing (NLP) where "scaling up" has resulted in
increasingly more capable LLMs, we investigate whether carefully scaling up
model and data size can bring similar improvements in the imitation learning
setting. To demonstrate our findings, we focus on the game of NetHack, a
challenging environment featuring procedural generation, stochasticity,
long-term dependencies, and partial observability. We find IL loss and mean
return scale smoothly with the compute budget and are strongly correlated,
resulting in power laws for training compute-optimal IL agents with respect to
model size and number of samples. We forecast and train several NetHack agents
with IL and find they outperform prior state-of-the-art by at least 2x in all
settings. Our work both demonstrates the scaling behavior of imitation learning
in a challenging domain, as well as the viability of scaling up current
approaches for increasingly capable agents in NetHack, a game that remains
elusively hard for current AI systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tuyls_J/0/1/0/all/0/1"&gt;Jens Tuyls&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madeka_D/0/1/0/all/0/1"&gt;Dhruv Madeka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torkkola_K/0/1/0/all/0/1"&gt;Kari Torkkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1"&gt;Dean Foster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1"&gt;Karthik Narasimhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1"&gt;Sham Kakade&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Throat Cancer from Speech Signals Using Machine Learning: A Reproducible Literature Review. (arXiv:2307.09230v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09230</id>
        <link href="http://arxiv.org/abs/2307.09230"/>
        <updated>2023-07-19T01:53:28.935Z</updated>
        <summary type="html"><![CDATA[In this work we perform a scoping review of the current literature on the
detection of throat cancer from speech recordings using machine learning and
artificial intelligence. We find 22 papers within this area and discuss their
methods and results. We split these papers into two groups - nine performing
binary classification, and 13 performing multi-class classification. The papers
present a range of methods with neural networks being most commonly
implemented. Many features are also extracted from the audio before
classification, with the most common bring mel-frequency cepstral coefficients.
None of the papers found in this search have associated code repositories and
as such are not reproducible. Therefore, we create a publicly available code
repository of our own classifiers. We use transfer learning on a multi-class
problem, classifying three pathologies and healthy controls. Using this
technique we achieve an unweighted average recall of 53.54%, sensitivity of
83.14%, and specificity of 64.00%. We compare our classifiers with the results
obtained on the same dataset and find similar results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paterson_M/0/1/0/all/0/1"&gt;Mary Paterson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moor_J/0/1/0/all/0/1"&gt;James Moor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cutillo_L/0/1/0/all/0/1"&gt;Luisa Cutillo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models. (arXiv:2307.09254v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09254</id>
        <link href="http://arxiv.org/abs/2307.09254"/>
        <updated>2023-07-19T01:53:28.934Z</updated>
        <summary type="html"><![CDATA[Uncertainty learning and quantification of models are crucial tasks to
enhance the trustworthiness of the models. Importantly, the recent surge of
generative language models (GLMs) emphasizes the need for reliable uncertainty
quantification due to the concerns on generating hallucinated facts. In this
paper, we propose to learn neural prediction set models that comes with the
probably approximately correct (PAC) guarantee for quantifying the uncertainty
of GLMs. Unlike existing prediction set models, which are parameterized by a
scalar value, we propose to parameterize prediction sets via neural networks,
which achieves more precise uncertainty quantification but still satisfies the
PAC guarantee. We demonstrate the efficacy of our method on four types of
language datasets and six types of models by showing that our method improves
the quantified uncertainty by $63\%$ on average, compared to a standard
baseline method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sangdon Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taesoo Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate via Compiler Co-design. (arXiv:2306.15656v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.15656</id>
        <link href="http://arxiv.org/abs/2306.15656"/>
        <updated>2023-07-19T01:53:28.933Z</updated>
        <summary type="html"><![CDATA[This paper introduces SparseOptimizer, a novel deep learning optimizer that
exploits Moreau-Yosida regularization to naturally induce sparsity in large
language models such as BERT, ALBERT and GPT. Key to the design of
SparseOptimizer is an embedded shrinkage operator, which imparts sparsity
directly within the optimization process. This operator, backed by a sound
theoretical framework, includes an analytical solution, thereby reinforcing the
optimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play
functionality eradicates the need for code modifications, making it a
universally adaptable tool for a wide array of large language models. Empirical
evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2
confirm that SparseBERT and SparseALBERT, when sparsified using
SparseOptimizer, achieve performance comparable to their dense counterparts,
BERT and ALBERT, while significantly reducing their parameter count. Further,
this work proposes an innovative optimizer-compiler co-design strategy,
demonstrating the potential of inference acceleration (\textbf{3.37x},
\textbf{6.30x}, and \textbf{7.15x} in comparison with Pytorch, TensorFlow, and
LLVM generic compile, respectively) in SparseBERT when paired with an
appropriately designed compiler. This study represents a significant step
forward in the evolution of efficient, scalable, and high-performing large
language models, setting a precedent for future exploration and optimization in
this domain. The SparseOptimizer code and SparseALBERT model will be publicly
available upon paper acceptance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1"&gt;Fu-Ming Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Objective GFlowNets. (arXiv:2210.12765v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.12765</id>
        <link href="http://arxiv.org/abs/2210.12765"/>
        <updated>2023-07-19T01:53:28.932Z</updated>
        <summary type="html"><![CDATA[We study the problem of generating diverse candidates in the context of
Multi-Objective Optimization. In many applications of machine learning such as
drug discovery and material design, the goal is to generate candidates which
simultaneously optimize a set of potentially conflicting objectives. Moreover,
these objectives are often imperfect evaluations of some underlying property of
interest, making it important to generate diverse candidates to have multiple
options for expensive downstream evaluations. We propose Multi-Objective
GFlowNets (MOGFNs), a novel method for generating diverse Pareto optimal
solutions, based on GFlowNets. We introduce two variants of MOGFNs: MOGFN-PC,
which models a family of independent sub-problems defined by a scalarization
function, with reward-conditional GFlowNets, and MOGFN-AL, which solves a
sequence of sub-problems defined by an acquisition function in an active
learning loop. Our experiments on wide variety of synthetic and benchmark tasks
demonstrate advantages of the proposed methods in terms of the Pareto
performance and importantly, improved candidate diversity, which is the main
contribution of this work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1"&gt;Moksh Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raparthy_S/0/1/0/all/0/1"&gt;Sharath Chandra Raparthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_Garcia_A/0/1/0/all/0/1"&gt;Alex Hernandez-Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rector_Brooks_J/0/1/0/all/0/1"&gt;Jarrid Rector-Brooks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miret_S/0/1/0/all/0/1"&gt;Santiago Miret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_E/0/1/0/all/0/1"&gt;Emmanuel Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do DL models and training environments have an impact on energy consumption?. (arXiv:2307.05520v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.05520</id>
        <link href="http://arxiv.org/abs/2307.05520"/>
        <updated>2023-07-19T01:53:28.932Z</updated>
        <summary type="html"><![CDATA[Current research in the computer vision field mainly focuses on improving
Deep Learning (DL) correctness and inference time performance. However, there
is still little work on the huge carbon footprint that has training DL models.
This study aims to analyze the impact of the model architecture and training
environment when training greener computer vision models. We divide this goal
into two research questions. First, we analyze the effects of model
architecture on achieving greener models while keeping correctness at optimal
levels. Second, we study the influence of the training environment on producing
greener models. To investigate these relationships, we collect multiple metrics
related to energy efficiency and model correctness during the models' training.
Then, we outline the trade-offs between the measured energy efficiency and the
models' correctness regarding model architecture, and their relationship with
the training environment. We conduct this research in the context of a computer
vision system for image classification. In conclusion, we show that selecting
the proper model architecture and training environment can reduce energy
consumption dramatically (up to 98.83%) at the cost of negligible decreases in
correctness. Also, we find evidence that GPUs should scale with the models'
computational complexity for better energy efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rey_S/0/1/0/all/0/1"&gt;Santiago del Rey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_Fernandez_S/0/1/0/all/0/1"&gt;Silverio Mart&amp;#xed;nez-Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cruz_L/0/1/0/all/0/1"&gt;Lu&amp;#xed;s Cruz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franch_X/0/1/0/all/0/1"&gt;Xavier Franch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fusing Hand and Body Skeletons for Human Action Recognition in Assembly. (arXiv:2307.09238v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.09238</id>
        <link href="http://arxiv.org/abs/2307.09238"/>
        <updated>2023-07-19T01:53:28.928Z</updated>
        <summary type="html"><![CDATA[As collaborative robots (cobots) continue to gain popularity in industrial
manufacturing, effective human-robot collaboration becomes crucial. Cobots
should be able to recognize human actions to assist with assembly tasks and act
autonomously. To achieve this, skeleton-based approaches are often used due to
their ability to generalize across various people and environments. Although
body skeleton approaches are widely used for action recognition, they may not
be accurate enough for assembly actions where the worker's fingers and hands
play a significant role. To address this limitation, we propose a method in
which less detailed body skeletons are combined with highly detailed hand
skeletons. We investigate CNNs and transformers, the latter of which are
particularly adept at extracting and combining important information from both
skeleton types using attention. This paper demonstrates the effectiveness of
our proposed approach in enhancing action recognition in assembly scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aganian_D/0/1/0/all/0/1"&gt;Dustin Aganian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kohler_M/0/1/0/all/0/1"&gt;Mona K&amp;#xf6;hler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stephan_B/0/1/0/all/0/1"&gt;Benedict Stephan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eisenbach_M/0/1/0/all/0/1"&gt;Markus Eisenbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gross_H/0/1/0/all/0/1"&gt;Horst-Michael Gross&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting the Robustness of the Minimum Error Entropy Criterion: A Transfer Learning Case Study. (arXiv:2307.08572v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.08572</id>
        <link href="http://arxiv.org/abs/2307.08572"/>
        <updated>2023-07-19T01:53:28.926Z</updated>
        <summary type="html"><![CDATA[Coping with distributional shifts is an important part of transfer learning
methods in order to perform well in real-life tasks. However, most of the
existing approaches in this area either focus on an ideal scenario in which the
data does not contain noises or employ a complicated training paradigm or model
design to deal with distributional shifts. In this paper, we revisit the
robustness of the minimum error entropy (MEE) criterion, a widely used
objective in statistical signal processing to deal with non-Gaussian noises,
and investigate its feasibility and usefulness in real-life transfer learning
regression tasks, where distributional shifts are common. Specifically, we put
forward a new theoretical result showing the robustness of MEE against
covariate shift. We also show that by simply replacing the mean squared error
(MSE) loss with the MEE on basic transfer learning algorithms such as
fine-tuning and linear probing, we can achieve competitive performance with
respect to state-of-the-art transfer learning algorithms. We justify our
arguments on both synthetic data and 5 real-world time-series data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Silvestrin_L/0/1/0/all/0/1"&gt;Luis Pedro Silvestrin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shujian Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoogendoorn_M/0/1/0/all/0/1"&gt;Mark Hoogendoorn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Edit at your own risk: evaluating the robustness of edited models to distribution shifts. (arXiv:2303.00046v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.00046</id>
        <link href="http://arxiv.org/abs/2303.00046"/>
        <updated>2023-07-19T01:53:28.925Z</updated>
        <summary type="html"><![CDATA[The current trend toward ever-larger models makes standard retraining
procedures an ever-more expensive burden. For this reason, there is growing
interest in model editing, which enables computationally inexpensive,
interpretable, post-hoc model modifications. While many model editing
techniques are promising, research on the properties of edited models is
largely limited to evaluation of validation accuracy. The robustness of edited
models is an important and yet mostly unexplored topic. In this paper, we
employ recently developed techniques from the field of deep learning robustness
to investigate both how model editing affects the general robustness of a
model, as well as the robustness of the specific behavior targeted by the edit.
We find that edits tend to reduce general robustness, but that the degree of
degradation depends on the editing algorithm and layers chosen. Motivated by
these observations we introduce a new model editing algorithm, 1-layer
interpolation (1-LI), which uses weight-space interpolation to navigate the
trade-off between editing task accuracy and general robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1"&gt;Davis Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Godfrey_C/0/1/0/all/0/1"&gt;Charles Godfrey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nizinski_C/0/1/0/all/0/1"&gt;Cody Nizinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1"&gt;Jonathan Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1"&gt;Henry Kvinge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conformal prediction under ambiguous ground truth. (arXiv:2307.09302v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09302</id>
        <link href="http://arxiv.org/abs/2307.09302"/>
        <updated>2023-07-19T01:53:28.924Z</updated>
        <summary type="html"><![CDATA[In safety-critical classification tasks, conformal prediction allows to
perform rigorous uncertainty quantification by providing confidence sets
including the true class with a user-specified probability. This generally
assumes the availability of a held-out calibration set with access to ground
truth labels. Unfortunately, in many domains, such labels are difficult to
obtain and usually approximated by aggregating expert opinions. In fact, this
holds true for almost all datasets, including well-known ones such as CIFAR and
ImageNet. Applying conformal prediction using such labels underestimates
uncertainty. Indeed, when expert opinions are not resolvable, there is inherent
ambiguity present in the labels. That is, we do not have ``crisp'', definitive
ground truth labels and this uncertainty should be taken into account during
calibration. In this paper, we develop a conformal prediction framework for
such ambiguous ground truth settings which relies on an approximation of the
underlying posterior distribution of labels given inputs. We demonstrate our
methodology on synthetic and real datasets, including a case study of skin
condition classification in dermatology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stutz_D/0/1/0/all/0/1"&gt;David Stutz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1"&gt;Abhijit Guha Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matejovicova_T/0/1/0/all/0/1"&gt;Tatiana Matejovicova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strachan_P/0/1/0/all/0/1"&gt;Patricia Strachan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cemgil_A/0/1/0/all/0/1"&gt;Ali Taylan Cemgil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models. (arXiv:2307.09254v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09254</id>
        <link href="http://arxiv.org/abs/2307.09254"/>
        <updated>2023-07-19T01:53:28.922Z</updated>
        <summary type="html"><![CDATA[Uncertainty learning and quantification of models are crucial tasks to
enhance the trustworthiness of the models. Importantly, the recent surge of
generative language models (GLMs) emphasizes the need for reliable uncertainty
quantification due to the concerns on generating hallucinated facts. In this
paper, we propose to learn neural prediction set models that comes with the
probably approximately correct (PAC) guarantee for quantifying the uncertainty
of GLMs. Unlike existing prediction set models, which are parameterized by a
scalar value, we propose to parameterize prediction sets via neural networks,
which achieves more precise uncertainty quantification but still satisfies the
PAC guarantee. We demonstrate the efficacy of our method on four types of
language datasets and six types of models by showing that our method improves
the quantified uncertainty by $63\%$ on average, compared to a standard
baseline method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sangdon Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taesoo Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal LLMs for health grounded in individual-specific data. (arXiv:2307.09018v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2307.09018</id>
        <link href="http://arxiv.org/abs/2307.09018"/>
        <updated>2023-07-19T01:53:28.921Z</updated>
        <summary type="html"><![CDATA[Foundation large language models (LLMs) have shown an impressive ability to
solve tasks across a wide range of fields including health. To effectively
solve personalized health tasks, LLMs need the ability to ingest a diversity of
data modalities that are relevant to an individual's health status. In this
paper, we take a step towards creating multimodal LLMs for health that are
grounded in individual-specific data by developing a framework (HeLM: Health
Large Language Model for Multimodal Understanding) that enables LLMs to use
high-dimensional clinical modalities to estimate underlying disease risk. HeLM
encodes complex data modalities by learning an encoder that maps them into the
LLM's token embedding space and for simple modalities like tabular data by
serializing the data into text. Using data from the UK Biobank, we show that
HeLM can effectively use demographic and clinical features in addition to
high-dimensional time-series data to estimate disease risk. For example, HeLM
achieves an AUROC of 0.75 for asthma prediction when combining tabular and
spirogram data modalities compared with 0.49 when only using tabular data.
Overall, we find that HeLM outperforms or performs at parity with classical
machine learning approaches across a selection of eight binary traits.
Furthermore, we investigate the downstream uses of this model such as its
generalizability to out-of-distribution traits and its ability to power
conversations around individual health and wellness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Belyaeva_A/0/1/0/all/0/1"&gt;Anastasiya Belyaeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Cosentino_J/0/1/0/all/0/1"&gt;Justin Cosentino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hormozdiari_F/0/1/0/all/0/1"&gt;Farhad Hormozdiari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+McLean_C/0/1/0/all/0/1"&gt;Cory Y. McLean&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Furlotte_N/0/1/0/all/0/1"&gt;Nicholas A. Furlotte&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unifying Framework for Differentially Private Sums under Continual Observation. (arXiv:2307.08970v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08970</id>
        <link href="http://arxiv.org/abs/2307.08970"/>
        <updated>2023-07-19T01:53:28.920Z</updated>
        <summary type="html"><![CDATA[We study the problem of maintaining a differentially private decaying sum
under continual observation. We give a unifying framework and an efficient
algorithm for this problem for \emph{any sufficiently smooth} function. Our
algorithm is the first differentially private algorithm that does not have a
multiplicative error for polynomially-decaying weights. Our algorithm improves
on all prior works on differentially private decaying sums under continual
observation and recovers exactly the additive error for the special case of
continual counting from Henzinger et al. (SODA 2023) as a corollary.

Our algorithm is a variant of the factorization mechanism whose error depends
on the $\gamma_2$ and $\gamma_F$ norm of the underlying matrix. We give a
constructive proof for an almost exact upper bound on the $\gamma_2$ and
$\gamma_F$ norm and an almost tight lower bound on the $\gamma_2$ norm for a
large class of lower-triangular matrices. This is the first non-trivial lower
bound for lower-triangular matrices whose non-zero entries are not all the
same. It includes matrices for all continual decaying sums problems, resulting
in an upper bound on the additive error of any differentially private decaying
sums algorithm under continual observation.

We also explore some implications of our result in discrepancy theory and
operator algebra. Given the importance of the $\gamma_2$ norm in computer
science and the extensive work in mathematics, we believe our result will have
further applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henzinger_M/0/1/0/all/0/1"&gt;Monika Henzinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Upadhyay_J/0/1/0/all/0/1"&gt;Jalaj Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1"&gt;Sarvagya Upadhyay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nested Elimination: A Simple Algorithm for Best-Item Identification from Choice-Based Feedback. (arXiv:2307.09295v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09295</id>
        <link href="http://arxiv.org/abs/2307.09295"/>
        <updated>2023-07-19T01:53:28.919Z</updated>
        <summary type="html"><![CDATA[We study the problem of best-item identification from choice-based feedback.
In this problem, a company sequentially and adaptively shows display sets to a
population of customers and collects their choices. The objective is to
identify the most preferred item with the least number of samples and at a high
confidence level. We propose an elimination-based algorithm, namely Nested
Elimination (NE), which is inspired by the nested structure implied by the
information-theoretic lower bound. NE is simple in structure, easy to
implement, and has a strong theoretical guarantee for sample complexity.
Specifically, NE utilizes an innovative elimination criterion and circumvents
the need to solve any complex combinatorial optimization problem. We provide an
instance-specific and non-asymptotic bound on the expected sample complexity of
NE. We also show NE achieves high-order worst-case asymptotic optimality.
Finally, numerical experiments from both synthetic and real data corroborate
our theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Junwen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yifan Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Dynamic Causal Discovery with Rare Events: A Nonparametric Conditional Independence Test. (arXiv:2211.16596v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.16596</id>
        <link href="http://arxiv.org/abs/2211.16596"/>
        <updated>2023-07-19T01:53:28.919Z</updated>
        <summary type="html"><![CDATA[Causal phenomena associated with rare events occur across a wide range of
engineering problems, such as risk-sensitive safety analysis, accident analysis
and prevention, and extreme value theory. However, current methods for causal
discovery are often unable to uncover causal links, between random variables in
a dynamic setting, that manifest only when the variables first experience
low-probability realizations. To address this issue, we introduce a novel
statistical independence test on data collected from time-invariant dynamical
systems in which rare but consequential events occur. In particular, we exploit
the time-invariance of the underlying data to construct a superimposed dataset
of the system state before rare events happen at different timesteps. We then
design a conditional independence test on the reorganized data. We provide
non-asymptotic sample complexity bounds for the consistency of our method, and
validate its performance across various simulated and real-world datasets,
including incident data collected from the Caltrans Performance Measurement
System (PeMS). Code containing the datasets and experiments is publicly
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chiu_C/0/1/0/all/0/1"&gt;Chih-Yuan Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kulkarni_K/0/1/0/all/0/1"&gt;Kshitij Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sastry_S/0/1/0/all/0/1"&gt;Shankar Sastry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees. (arXiv:2305.11997v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.11997</id>
        <link href="http://arxiv.org/abs/2305.11997"/>
        <updated>2023-07-19T01:53:28.916Z</updated>
        <summary type="html"><![CDATA[There is an emerging interest in generating robust counterfactual
explanations that would remain valid if the model is updated or changed even
slightly. Towards finding robust counterfactuals, existing literature often
assumes that the original model $m$ and the new model $M$ are bounded in the
parameter space, i.e., $\|\text{Params}(M){-}\text{Params}(m)\|{<}\Delta$.
However, models can often change significantly in the parameter space with
little to no change in their predictions or accuracy on the given dataset. In
this work, we introduce a mathematical abstraction termed
\emph{naturally-occurring} model change, which allows for arbitrary changes in
the parameter space such that the change in predictions on points that lie on
the data manifold is limited. Next, we propose a measure -- that we call
\emph{Stability} -- to quantify the robustness of counterfactuals to potential
model changes for differentiable models, e.g., neural networks. Our main
contribution is to show that counterfactuals with sufficiently high value of
\emph{Stability} as defined by our measure will remain valid after potential
``naturally-occurring'' model changes with high probability (leveraging
concentration bounds for Lipschitz function of independent Gaussians). Since
our quantification depends on the local Lipschitz constant around a data point
which is not always available, we also examine practical relaxations of our
proposed measure and demonstrate experimentally how they can be incorporated to
find robust counterfactuals for neural networks that are close, realistic, and
remain valid after potential model changes. This work also has interesting
connections with model multiplicity, also known as, the Rashomon effect.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hamman_F/0/1/0/all/0/1"&gt;Faisal Hamman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Noorani_E/0/1/0/all/0/1"&gt;Erfaun Noorani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Saumitra Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Magazzeni_D/0/1/0/all/0/1"&gt;Daniele Magazzeni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dutta_S/0/1/0/all/0/1"&gt;Sanghamitra Dutta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Cross-Segmentation for Improved Generalization in Reinforcement Learning Based Algorithmic Trading. (arXiv:2307.09377v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09377</id>
        <link href="http://arxiv.org/abs/2307.09377"/>
        <updated>2023-07-19T01:53:28.915Z</updated>
        <summary type="html"><![CDATA[The use of machine learning in algorithmic trading systems is increasingly
common. In a typical set-up, supervised learning is used to predict the future
prices of assets, and those predictions drive a simple trading and execution
strategy. This is quite effective when the predictions have sufficient signal,
markets are liquid, and transaction costs are low. However, those conditions
often do not hold in thinly traded financial markets and markets for
differentiated assets such as real estate or vehicles. In these markets, the
trading strategy must consider the long-term effects of taking positions that
are relatively more difficult to change. In this work, we propose a
Reinforcement Learning (RL) algorithm that trades based on signals from a
learned predictive model and addresses these challenges. We test our algorithm
on 20+ years of equity data from Bursa Malaysia.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duvvur_V/0/1/0/all/0/1"&gt;Vikram Duvvur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_A/0/1/0/all/0/1"&gt;Aashay Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_E/0/1/0/all/0/1"&gt;Edward Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1"&gt;Bo Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1"&gt;Ken Yew Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1"&gt;Jeff Schneider&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Riemannian Networks for EEG Decoding. (arXiv:2212.10426v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.10426</id>
        <link href="http://arxiv.org/abs/2212.10426"/>
        <updated>2023-07-19T01:53:28.915Z</updated>
        <summary type="html"><![CDATA[State-of-the-art performance in electroencephalography (EEG) decoding tasks
is currently often achieved with either Deep-Learning (DL) or
Riemannian-Geometry-based decoders (RBDs). Recently, there is growing interest
in Deep Riemannian Networks (DRNs) possibly combining the advantages of both
previous classes of methods. However, there are still a range of topics where
additional insight is needed to pave the way for a more widespread application
of DRNs in EEG. These include architecture design questions such as network
size and end-to-end ability.How these factors affect model performance has not
been explored. Additionally, it is not clear how the data within these networks
is transformed, and whether this would correlate with traditional EEG decoding.
Our study aims to lay the groundwork in the area of these topics through the
analysis of DRNs for EEG with a wide range of hyperparameters. Networks were
tested on two public EEG datasets and compared with state-of-the-art ConvNets.
Here we propose end-to-end EEG SPDNet (EE(G)-SPDNet), and we show that this
wide, end-to-end DRN can outperform the ConvNets, and in doing so use
physiologically plausible frequency regions. We also show that the end-to-end
approach learns more complex filters than traditional band-pass filters
targeting the classical alpha, beta, and gamma frequency bands of the EEG, and
that performance can benefit from channel specific filtering approaches.
Additionally, architectural analysis revealed areas for further improvement due
to the possible loss of Riemannian specific information throughout the network.
Our study thus shows how to design and train DRNs to infer task-related
information from the raw EEG without the need of handcrafted filterbanks and
highlights the potential of end-to-end DRNs such as EE(G)-SPDNet for
high-performance EEG decoding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1"&gt;Daniel Wilson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schirrmeister_R/0/1/0/all/0/1"&gt;Robin Tibor Schirrmeister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gemein_L/0/1/0/all/0/1"&gt;Lukas Alexander Wilhelm Gemein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ball_T/0/1/0/all/0/1"&gt;Tonio Ball&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nested Elimination: A Simple Algorithm for Best-Item Identification from Choice-Based Feedback. (arXiv:2307.09295v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09295</id>
        <link href="http://arxiv.org/abs/2307.09295"/>
        <updated>2023-07-19T01:53:28.914Z</updated>
        <summary type="html"><![CDATA[We study the problem of best-item identification from choice-based feedback.
In this problem, a company sequentially and adaptively shows display sets to a
population of customers and collects their choices. The objective is to
identify the most preferred item with the least number of samples and at a high
confidence level. We propose an elimination-based algorithm, namely Nested
Elimination (NE), which is inspired by the nested structure implied by the
information-theoretic lower bound. NE is simple in structure, easy to
implement, and has a strong theoretical guarantee for sample complexity.
Specifically, NE utilizes an innovative elimination criterion and circumvents
the need to solve any complex combinatorial optimization problem. We provide an
instance-specific and non-asymptotic bound on the expected sample complexity of
NE. We also show NE achieves high-order worst-case asymptotic optimality.
Finally, numerical experiments from both synthetic and real data corroborate
our theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Junwen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yifan Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.09009</id>
        <link href="http://arxiv.org/abs/2307.09009"/>
        <updated>2023-07-19T01:53:28.913Z</updated>
        <summary type="html"><![CDATA[GPT-3.5 and GPT-4 are the two most widely used large language model (LLM)
services. However, when and how these models are updated over time is opaque.
Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on
four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous
questions, 3) generating code and 4) visual reasoning. We find that the
performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time.
For example, GPT-4 (March 2023) was very good at identifying prime numbers
(accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions
(accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5
(March 2023) in this task. GPT-4 was less willing to answer sensitive questions
in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes
in code generation in June than in March. Overall, our findings shows that the
behavior of the same LLM service can change substantially in a relatively short
amount of time, highlighting the need for continuous monitoring of LLM quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lingjiao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1"&gt;Matei Zaharia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1"&gt;James Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimation of an Order Book Dependent Hawkes Process for Large Datasets. (arXiv:2307.09077v1 [q-fin.TR])]]></title>
        <id>http://arxiv.org/abs/2307.09077</id>
        <link href="http://arxiv.org/abs/2307.09077"/>
        <updated>2023-07-19T01:53:28.911Z</updated>
        <summary type="html"><![CDATA[A point process for event arrivals in high frequency trading is presented.
The intensity is the product of a Hawkes process and high dimensional functions
of covariates derived from the order book. Conditions for stationarity of the
process are stated. An algorithm is presented to estimate the model even in the
presence of billions of data points, possibly mapping covariates into a high
dimensional space. The large sample size can be common for high frequency data
applications using multiple liquid instruments. Convergence of the algorithm is
shown, consistency results under weak conditions is established, and a test
statistic to assess out of sample performance of different model specifications
is suggested. The methodology is applied to the study of four stocks that trade
on the New York Stock Exchange (NYSE). The out of sample testing procedure
suggests that capturing the nonlinearity of the order book information adds
value to the self exciting nature of high frequency trading events.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Mucciante_L/0/1/0/all/0/1"&gt;Luca Mucciante&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Sancetta_A/0/1/0/all/0/1"&gt;Alessio Sancetta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Application of BERT in Wind Power Forecasting-Teletraan's Solution in Baidu KDD Cup 2022. (arXiv:2307.09248v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09248</id>
        <link href="http://arxiv.org/abs/2307.09248"/>
        <updated>2023-07-19T01:53:28.910Z</updated>
        <summary type="html"><![CDATA[Nowadays, wind energy has drawn increasing attention as its important role in
carbon neutrality and sustainable development. When wind power is integrated
into the power grid, precise forecasting is necessary for the sustainability
and security of the system. However, the unpredictable nature and long sequence
prediction make it especially challenging. In this technical report, we
introduce the BERT model applied for Baidu KDD Cup 2022, and the daily
fluctuation is added by post-processing to make the predicted results in line
with daily periodicity. Our solution achieves 3rd place of 2490 teams. The code
is released athttps://github.com/LongxingTan/KDD2022-Baidu]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1"&gt;Longxing Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_H/0/1/0/all/0/1"&gt;Hongying Yue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Evaluation of Zero-Cost Proxies -- from Neural Architecture Performance to Model Robustness. (arXiv:2307.09365v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09365</id>
        <link href="http://arxiv.org/abs/2307.09365"/>
        <updated>2023-07-19T01:53:28.910Z</updated>
        <summary type="html"><![CDATA[Zero-cost proxies are nowadays frequently studied and used to search for
neural architectures. They show an impressive ability to predict the
performance of architectures by making use of their untrained weights. These
techniques allow for immense search speed-ups. So far the joint search for
well-performing and robust architectures has received much less attention in
the field of NAS. Therefore, the main focus of zero-cost proxies is the clean
accuracy of architectures, whereas the model robustness should play an evenly
important part. In this paper, we analyze the ability of common zero-cost
proxies to serve as performance predictors for robustness in the popular
NAS-Bench-201 search space. We are interested in the single prediction task for
robustness and the joint multi-objective of clean and robust accuracy. We
further analyze the feature importance of the proxies and show that predicting
the robustness makes the prediction task from existing zero-cost proxies more
challenging. As a result, the joint consideration of several proxies becomes
necessary to predict a model's robustness while the clean accuracy can be
regressed from a single such feature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lukasik_J/0/1/0/all/0/1"&gt;Jovita Lukasik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1"&gt;Michael Moeller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1"&gt;Margret Keuper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Covariate-Adjusted Homogeneity Test with Application to Facial Recognition Accuracy Assessment. (arXiv:2307.08846v1 [stat.AP])]]></title>
        <id>http://arxiv.org/abs/2307.08846</id>
        <link href="http://arxiv.org/abs/2307.08846"/>
        <updated>2023-07-19T01:53:28.909Z</updated>
        <summary type="html"><![CDATA[Ordinal scores occur commonly in medical imaging studies and in black-box
forensic studies \citep{Phillips:2018}. To assess the accuracy of raters in the
studies, one needs to estimate the receiver operating characteristic (ROC)
curve while accounting for covariates of raters. In this paper, we propose a
covariate-adjusted homogeneity test to determine differences in accuracy among
multiple rater groups. We derived the theoretical results of the proposed test
and conducted extensive simulation studies to evaluate the finite sample
performance of the proposed test. Our proposed test is applied to a face
recognition study to identify statistically significant differences among five
participant groups.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Ngoc-Ty Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Phillips_P/0/1/0/all/0/1"&gt;P. Jonathon Phillips&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tang_L/0/1/0/all/0/1"&gt;Larry Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Globally solving the Gromov-Wasserstein problem for point clouds in low dimensional Euclidean spaces. (arXiv:2307.09057v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2307.09057</id>
        <link href="http://arxiv.org/abs/2307.09057"/>
        <updated>2023-07-19T01:53:28.861Z</updated>
        <summary type="html"><![CDATA[This paper presents a framework for computing the Gromov-Wasserstein problem
between two sets of points in low dimensional spaces, where the discrepancy is
the squared Euclidean norm. The Gromov-Wasserstein problem is a generalization
of the optimal transport problem that finds the assignment between two sets
preserving pairwise distances as much as possible. This can be used to quantify
the similarity between two formations or shapes, a common problem in AI and
machine learning. The problem can be formulated as a Quadratic Assignment
Problem (QAP), which is in general computationally intractable even for small
problems. Our framework addresses this challenge by reformulating the QAP as an
optimization problem with a low-dimensional domain, leveraging the fact that
the problem can be expressed as a concave quadratic optimization problem with
low rank. The method scales well with the number of points, and it can be used
to find the global solution for large-scale problems with thousands of points.
We compare the computational complexity of our approach with state-of-the-art
methods on synthetic problems and apply it to a near-symmetrical problem which
is of particular interest in computational biology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ryner_M/0/1/0/all/0/1"&gt;Martin Ryner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kronqvist_J/0/1/0/all/0/1"&gt;Jan Kronqvist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Karlsson_J/0/1/0/all/0/1"&gt;Johan Karlsson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Funnel-based Reward Shaping for Signal Temporal Logic Tasks in Reinforcement Learning. (arXiv:2212.03181v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.03181</id>
        <link href="http://arxiv.org/abs/2212.03181"/>
        <updated>2023-07-19T01:53:28.860Z</updated>
        <summary type="html"><![CDATA[Signal Temporal Logic (STL) is a powerful framework for describing the
complex temporal and logical behaviour of the dynamical system. Numerous
studies have attempted to employ reinforcement learning to learn a controller
that enforces STL specifications; however, they have been unable to effectively
tackle the challenges of ensuring robust satisfaction in continuous state space
and maintaining tractability. In this paper, leveraging the concept of funnel
functions, we propose a tractable reinforcement learning algorithm to learn a
time-dependent policy for robust satisfaction of STL specification in
continuous state space. We demonstrate the utility of our approach on several
STL tasks using different environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Saxena_N/0/1/0/all/0/1"&gt;Naman Saxena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sandeep_G/0/1/0/all/0/1"&gt;Gorantla Sandeep&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jagtap_P/0/1/0/all/0/1"&gt;Pushpak Jagtap&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DESCN: Deep Entire Space Cross Networks for Individual Treatment Effect Estimation. (arXiv:2207.09920v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2207.09920</id>
        <link href="http://arxiv.org/abs/2207.09920"/>
        <updated>2023-07-19T01:53:28.855Z</updated>
        <summary type="html"><![CDATA[Causal Inference has wide applications in various areas such as E-commerce
and precision medicine, and its performance heavily relies on the accurate
estimation of the Individual Treatment Effect (ITE). Conventionally, ITE is
predicted by modeling the treated and control response functions separately in
their individual sample spaces. However, such an approach usually encounters
two issues in practice, i.e. divergent distribution between treated and control
groups due to treatment bias, and significant sample imbalance of their
population sizes. This paper proposes Deep Entire Space Cross Networks (DESCN)
to model treatment effects from an end-to-end perspective. DESCN captures the
integrated information of the treatment propensity, the response, and the
hidden treatment effect through a cross network in a multi-task learning
manner. Our method jointly learns the treatment and response functions in the
entire sample space to avoid treatment bias and employs an intermediate pseudo
treatment effect prediction network to relieve sample imbalance. Extensive
experiments are conducted on a synthetic dataset and a large-scaled production
dataset from the E-commerce voucher distribution business. The results indicate
that DESCN can successfully enhance the accuracy of ITE estimation and improve
the uplift ranking performance. A sample of the production dataset and the
source code are released to facilitate future research in the community, which
is, to the best of our knowledge, the first large-scale public biased treatment
dataset for causal inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_K/0/1/0/all/0/1"&gt;Kailiang Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1"&gt;Fengtong Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yan Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yaorong Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1"&gt;Wenqing Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaofeng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cen_L/0/1/0/all/0/1"&gt;Ling Cen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Observer-Based Inverse Reinforcement Learning. (arXiv:2011.02057v3 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02057</id>
        <link href="http://arxiv.org/abs/2011.02057"/>
        <updated>2023-07-19T01:53:28.854Z</updated>
        <summary type="html"><![CDATA[In this paper, a novel approach to the output-feedback inverse reinforcement
learning (IRL) problem is developed by casting the IRL problem, for linear
systems with quadratic cost functions, as a state estimation problem. Two
observer-based techniques for IRL are developed, including a novel observer
method that re-uses previous state estimates via history stacks. Theoretical
guarantees for convergence and robustness are established under appropriate
excitation conditions. Simulations demonstrate the performance of the developed
observers and filters under noisy and noise-free measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Self_R/0/1/0/all/0/1"&gt;Ryan Self&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Coleman_K/0/1/0/all/0/1"&gt;Kevin Coleman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bai_H/0/1/0/all/0/1"&gt;He Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kamalapurkar_R/0/1/0/all/0/1"&gt;Rushikesh Kamalapurkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extreme heatwave sampling and prediction with analog Markov chain and comparisons with deep learning. (arXiv:2307.09060v1 [physics.ao-ph])]]></title>
        <id>http://arxiv.org/abs/2307.09060</id>
        <link href="http://arxiv.org/abs/2307.09060"/>
        <updated>2023-07-19T01:53:28.845Z</updated>
        <summary type="html"><![CDATA[We present a data-driven emulator, stochastic weather generator (SWG),
suitable for estimating probabilities of prolonged heatwaves in France and
Scandinavia. This emulator is based on the method of analogs of circulation to
which we add temperature and soil moisture as predictor fields. We train the
emulator on an intermediate complexity climate model run and show that it is
capable of predicting conditional probabilities (forecasting) of heatwaves out
of sample. Special attention is payed that this prediction is evaluated using
proper score appropriate for rare events. To accelerate the computation of
analogs dimensionality reduction techniques are applied and the performance is
evaluated. The probabilistic prediction achieved with SWG is compared with the
one achieved with

Convolutional Neural Network (CNN). With the availability of hundreds of
years of training data CNNs perform better at the task of probabilistic
prediction. In addition, we show that the SWG emulator trained on 80 years of
data is capable of estimating extreme return times of order of thousands of
years for heatwaves longer than several days more precisely than the fit based
on generalised extreme value distribution. Finally, the quality of its
synthetic extreme teleconnection patterns obtained with stochastic weather
generator is studied. We showcase two examples of such synthetic teleconnection
patterns for heatwaves in France and Scandinavia that compare favorably to the
very long climate model control run.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Miloshevich_G/0/1/0/all/0/1"&gt;George Miloshevich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Lucente_D/0/1/0/all/0/1"&gt;Dario Lucente&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Yiou_P/0/1/0/all/0/1"&gt;Pascal Yiou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Bouchet_F/0/1/0/all/0/1"&gt;Freddy Bouchet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Pattern Classification in Support Vector Machines through Matrix Formulation. (arXiv:2307.09372v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09372</id>
        <link href="http://arxiv.org/abs/2307.09372"/>
        <updated>2023-07-19T01:53:28.844Z</updated>
        <summary type="html"><![CDATA[Support Vector Machines (SVM) have gathered significant acclaim as
classifiers due to their successful implementation of Statistical Learning
Theory. However, in the context of multiclass and multilabel settings, the
reliance on vector-based formulations in existing SVM-based models poses
limitations regarding flexibility and ease of incorporating additional terms to
handle specific challenges. To overcome these limitations, our research paper
focuses on introducing a matrix formulation for SVM that effectively addresses
these constraints. By employing the Accelerated Gradient Descent method in the
dual, we notably enhance the efficiency of solving the Matrix-SVM problem.
Experimental evaluations on multilabel and multiclass datasets demonstrate that
Matrix SVM achieves superior time efficacy while delivering similar results to
Binary Relevance SVM.

Moreover, our matrix formulation unveils crucial insights and advantages that
may not be readily apparent in traditional vector-based notations. We emphasize
that numerous multilabel models can be viewed as extensions of SVM, with
customised modifications to meet specific requirements. The matrix formulation
presented in this paper establishes a solid foundation for developing more
sophisticated models capable of effectively addressing the distinctive
challenges encountered in multilabel learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rastogi_S/0/1/0/all/0/1"&gt;Sambhav Jain Reshma Rastogi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer. (arXiv:2304.02011v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.02011</id>
        <link href="http://arxiv.org/abs/2304.02011"/>
        <updated>2023-07-19T01:53:28.837Z</updated>
        <summary type="html"><![CDATA[Particle localization and -classification constitute two of the most
fundamental problems in computational microscopy. In recent years, deep
learning based approaches have been introduced for these tasks with great
success. A key shortcoming of these supervised learning methods is their need
for large training data sets, typically generated from particle models in
conjunction with complex numerical forward models simulating the physics of
transmission electron microscopes. Computer implementations of such forward
models are computationally extremely demanding and limit the scope of their
applicability. In this paper we propose a method for simulating the forward
operator of an electron microscope based on additive noise and Neural Style
Transfer techniques. We evaluate the method on localization and classification
tasks using one of the established state-of-the-art architectures showing
performance on par with the benchmark. In contrast to previous approaches, our
method accelerates the data generation process by a factor of 750 while using
33 times less memory and scales well to typical transmission electron
microscope detector sizes. It utilizes GPU acceleration and parallel
processing. It can be used to adapt a synthetic training data set according to
reference data from any transmission electron microscope. The source code is
available at https://gitlab.com/deepet/faket.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Harar_P/0/1/0/all/0/1"&gt;Pavol Harar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herrmann_L/0/1/0/all/0/1"&gt;Lukas Herrmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grohs_P/0/1/0/all/0/1"&gt;Philipp Grohs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haselbach_D/0/1/0/all/0/1"&gt;David Haselbach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Martian time-series unraveled: A multi-scale nested approach with factorial variational autoencoders. (arXiv:2305.16189v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.16189</id>
        <link href="http://arxiv.org/abs/2305.16189"/>
        <updated>2023-07-19T01:53:28.834Z</updated>
        <summary type="html"><![CDATA[Unsupervised source separation involves unraveling an unknown set of source
signals recorded through a mixing operator, with limited prior knowledge about
the sources, and only access to a dataset of signal mixtures. This problem is
inherently ill-posed and is further challenged by the variety of time-scales
exhibited by sources in time series data. Existing methods typically rely on a
preselected window size that limits their capacity to handle multi-scale
sources. To address this issue, instead of operating in the time domain, we
propose an unsupervised multi-scale clustering and source separation framework
by leveraging wavelet scattering covariances that provide a low-dimensional
representation of stochastic processes, capable of distinguishing between
different non-Gaussian stochastic processes. Nested within this representation
space, we develop a factorial Gaussian-mixture variational autoencoder that is
trained to (1) probabilistically cluster sources at different time-scales and
(2) independently sample scattering covariance representations associated with
each cluster. Using samples from each cluster as prior information, we
formulate source separation as an optimization problem in the wavelet
scattering covariance representation space, resulting in separated sources in
the time domain. When applied to seismic data recorded during the NASA InSight
mission on Mars, our multi-scale nested approach proves to be a powerful tool
for discriminating between sources varying greatly in time-scale, e.g.,
minute-long transient one-sided pulses (known as ``glitches'') and structured
ambient noises resulting from atmospheric activities that typically last for
tens of minutes. These results provide an opportunity to conduct further
investigations into the isolated sources related to atmospheric-surface
interactions, thermal relaxations, and other complex phenomena.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siahkoohi_A/0/1/0/all/0/1"&gt;Ali Siahkoohi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morel_R/0/1/0/all/0/1"&gt;Rudy Morel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1"&gt;Randall Balestriero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allys_E/0/1/0/all/0/1"&gt;Erwan Allys&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sainton_G/0/1/0/all/0/1"&gt;Gr&amp;#xe9;gory Sainton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawamura_T/0/1/0/all/0/1"&gt;Taichi Kawamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoop_M/0/1/0/all/0/1"&gt;Maarten V. de Hoop&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Player Zero-Sum Markov Games with Networked Separable Interactions. (arXiv:2307.09470v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2307.09470</id>
        <link href="http://arxiv.org/abs/2307.09470"/>
        <updated>2023-07-19T01:53:28.832Z</updated>
        <summary type="html"><![CDATA[We study a new class of Markov games (MGs), \textit{Multi-player Zero-sum
Markov Games} with {\it Networked separable interactions} (MZNMGs), to model
the local interaction structure in non-cooperative multi-agent sequential
decision-making. We define an MZNMG as a model where {the payoffs of the
auxiliary games associated with each state are zero-sum and} have some
separable (i.e., polymatrix) structure across the neighbors over some
interaction network. We first identify the necessary and sufficient conditions
under which an MG can be presented as an MZNMG, and show that the set of Markov
coarse correlated equilibrium (CCE) collapses to the set of Markov Nash
equilibrium (NE) in these games, in that the {product of} per-state
marginalization of the former for all players yields the latter. Furthermore,
we show that finding approximate Markov \emph{stationary} CCE in
infinite-horizon discounted MZNMGs is \texttt{PPAD}-hard, unless the underlying
network has a ``star topology''. Then, we propose fictitious-play-type
dynamics, the classical learning dynamics in normal-form games, for MZNMGs, and
establish convergence guarantees to Markov stationary NE under a star-shaped
network structure. Finally, in light of the hardness result, we focus on
computing a Markov \emph{non-stationary} NE and provide finite-iteration
guarantees for a series of value-iteration-based algorithms. We also provide
numerical experiments to corroborate our theoretical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1"&gt;Chanwoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaiqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozdaglar_A/0/1/0/all/0/1"&gt;Asuman Ozdaglar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning. (arXiv:2307.08794v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08794</id>
        <link href="http://arxiv.org/abs/2307.08794"/>
        <updated>2023-07-19T01:53:28.831Z</updated>
        <summary type="html"><![CDATA[In multi-timescale multi-agent reinforcement learning (MARL), agents interact
across different timescales. In general, policies for time-dependent behaviors,
such as those induced by multiple timescales, are non-stationary. Learning
non-stationary policies is challenging and typically requires sophisticated or
inefficient algorithms. Motivated by the prevalence of this control problem in
real-world complex systems, we introduce a simple framework for learning
non-stationary policies for multi-timescale MARL. Our approach uses available
information about agent timescales to define a periodic time encoding. In
detail, we theoretically demonstrate that the effects of non-stationarity
introduced by multiple timescales can be learned by a periodic multi-agent
policy. To learn such policies, we propose a policy gradient algorithm that
parameterizes the actor and critic with phase-functioned neural networks, which
provide an inductive bias for periodicity. The framework's ability to
effectively learn multi-timescale policies is validated on a gridworld and
building energy management environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Emami_P/0/1/0/all/0/1"&gt;Patrick Emami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biagioni_D/0/1/0/all/0/1"&gt;David Biagioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamzam_A/0/1/0/all/0/1"&gt;Ahmed S. Zamzam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Execution-based Code Generation using Deep Reinforcement Learning. (arXiv:2301.13816v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2301.13816</id>
        <link href="http://arxiv.org/abs/2301.13816"/>
        <updated>2023-07-19T01:53:28.829Z</updated>
        <summary type="html"><![CDATA[The utilization of programming language (PL) models, pre-trained on
large-scale code corpora, as a means of automating software engineering
processes has demonstrated considerable potential in streamlining various code
generation tasks such as code completion, code translation, and program
synthesis. However, current approaches mainly rely on supervised fine-tuning
objectives borrowed from text generation, neglecting unique sequence-level
characteristics of code, including but not limited to compilability as well as
syntactic and functional correctness. To address this limitation, we propose
PPOCoder, a new framework for code generation that synergistically combines
pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely
used deep reinforcement learning technique. By utilizing non-differentiable
feedback from code execution and structure alignment, PPOCoder seamlessly
integrates external code-specific knowledge into the model optimization
process. It's important to note that PPOCoder is a task-agnostic and
model-agnostic framework that can be used across different code generation
tasks and PLs. Extensive experiments on three code generation tasks demonstrate
the effectiveness of our proposed approach compared to SOTA methods, achieving
significant improvements in compilation success rates and functional
correctness across different PLs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shojaee_P/0/1/0/all/0/1"&gt;Parshin Shojaee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Aneesh Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tipirneni_S/0/1/0/all/0/1"&gt;Sindhu Tipirneni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1"&gt;Chandan K. Reddy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Mean Field Games with non-separable Hamiltonians. (arXiv:2301.02877v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2301.02877</id>
        <link href="http://arxiv.org/abs/2301.02877"/>
        <updated>2023-07-19T01:53:28.827Z</updated>
        <summary type="html"><![CDATA[This paper introduces a new method based on Deep Galerkin Methods (DGMs) for
solving high-dimensional stochastic Mean Field Games (MFGs). We achieve this by
using two neural networks to approximate the unknown solutions of the MFG
system and forward-backward conditions. Our method is efficient, even with a
small number of iterations, and is capable of handling up to 300 dimensions
with a single layer, which makes it faster than other approaches. In contrast,
methods based on Generative Adversarial Networks (GANs) cannot solve MFGs with
non-separable Hamiltonians. We demonstrate the effectiveness of our approach by
applying it to a traffic flow problem, which was previously solved using the
Newton iteration method only in the deterministic case. We compare the results
of our method to analytical solutions and previous approaches, showing its
efficiency. We also prove the convergence of our neural network approximation
with a single hidden layer using the universal approximation theorem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Assouli_M/0/1/0/all/0/1"&gt;Mouhcine Assouli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Missaoui_B/0/1/0/all/0/1"&gt;Badr Missaoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Sustainable Deep Learning for Multi-Label Classification on NILM. (arXiv:2307.09244v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09244</id>
        <link href="http://arxiv.org/abs/2307.09244"/>
        <updated>2023-07-19T01:53:28.826Z</updated>
        <summary type="html"><![CDATA[Non-intrusive load monitoring (NILM) is the process of obtaining
appliance-level data from a single metering point, measuring total electricity
consumption of a household or a business. Appliance-level data can be directly
used for demand response applications and energy management systems as well as
for awareness raising and motivation for improvements in energy efficiency and
reduction in the carbon footprint. Recently, classical machine learning and
deep learning (DL) techniques became very popular and proved as highly
effective for NILM classification, but with the growing complexity these
methods are faced with significant computational and energy demands during both
their training and operation. In this paper, we introduce a novel DL model
aimed at enhanced multi-label classification of NILM with improved computation
and energy efficiency. We also propose a testing methodology for comparison of
different models using data synthesized from the measurement datasets so as to
better represent real-world scenarios. Compared to the state-of-the-art, the
proposed model has its carbon footprint reduced by more than 23% while
providing on average approximately 8 percentage points in performance
improvement when testing on data derived from REFIT and UK-DALE datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pirnat_A/0/1/0/all/0/1"&gt;An&amp;#x17e;e Pirnat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertalanic_B/0/1/0/all/0/1"&gt;Bla&amp;#x17e; Bertalani&amp;#x10d;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cerar_G/0/1/0/all/0/1"&gt;Gregor Cerar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohorcic_M/0/1/0/all/0/1"&gt;Mihael Mohor&amp;#x10d;i&amp;#x10d;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fortuna_C/0/1/0/all/0/1"&gt;Carolina Fortuna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear Processing with Linear Optics. (arXiv:2307.08533v2 [physics.optics] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.08533</id>
        <link href="http://arxiv.org/abs/2307.08533"/>
        <updated>2023-07-19T01:53:28.823Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have achieved remarkable breakthroughs by leveraging
multiple layers of data processing to extract hidden representations, albeit at
the cost of large electronic computing power. To enhance energy efficiency and
speed, the optical implementation of neural networks aims to harness the
advantages of optical bandwidth and the energy efficiency of optical
interconnections. In the absence of low-power optical nonlinearities, the
challenge in the implementation of multilayer optical networks lies in
realizing multiple optical layers without resorting to electronic components.
In this study, we present a novel framework that uses multiple scattering that
is capable of synthesizing programmable linear and nonlinear transformations
concurrently at low optical power by leveraging the nonlinear relationship
between the scattering potential, represented by data, and the scattered field.
Theoretical and experimental investigations show that repeating the data by
multiple scattering enables non-linear optical computing at low power
continuous wave light.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Yildirim_M/0/1/0/all/0/1"&gt;Mustafa Yildirim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Dinc_N/0/1/0/all/0/1"&gt;Niyazi Ulas Dinc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Oguz_I/0/1/0/all/0/1"&gt;Ilker Oguz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Psaltis_D/0/1/0/all/0/1"&gt;Demetri Psaltis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Moser_C/0/1/0/all/0/1"&gt;Christophe Moser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditionally Calibrated Predictive Distributions by Probability-Probability Map: Application to Galaxy Redshift Estimation and Probabilistic Forecasting. (arXiv:2205.14568v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2205.14568</id>
        <link href="http://arxiv.org/abs/2205.14568"/>
        <updated>2023-07-19T01:53:28.821Z</updated>
        <summary type="html"><![CDATA[Uncertainty quantification is crucial for assessing the predictive ability of
AI algorithms. Much research has been devoted to describing the predictive
distribution (PD) $F(y|\mathbf{x})$ of a target variable $y \in \mathbb{R}$
given complex input features $\mathbf{x} \in \mathcal{X}$. However,
off-the-shelf PDs (from, e.g., normalizing flows and Bayesian neural networks)
often lack conditional calibration with the probability of occurrence of an
event given input $\mathbf{x}$ being significantly different from the predicted
probability. Current calibration methods do not fully assess and enforce
conditionally calibrated PDs. Here we propose \texttt{Cal-PIT}, a method that
addresses both PD diagnostics and recalibration by learning a single
probability-probability map from calibration data. The key idea is to regress
probability integral transform scores against $\mathbf{x}$. The estimated
regression provides interpretable diagnostics of conditional coverage across
the feature space. The same regression function morphs the misspecified PD to a
re-calibrated PD for all $\mathbf{x}$. We benchmark our corrected prediction
bands (a by-product of corrected PDs) against oracle bands and state-of-the-art
predictive inference algorithms for synthetic data. We also provide results for
two applications: (i) probabilistic nowcasting given sequences of satellite
images, and (ii) conditional density estimation of galaxy distances given
imaging data (so-called photometric redshift estimation). Our code is available
as a Python package https://github.com/lee-group-cmu/Cal-PIT .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dey_B/0/1/0/all/0/1"&gt;Biprateep Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhao_D/0/1/0/all/0/1"&gt;David Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Newman_J/0/1/0/all/0/1"&gt;Jeffrey A. Newman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Andrews_B/0/1/0/all/0/1"&gt;Brett H. Andrews&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Izbicki_R/0/1/0/all/0/1"&gt;Rafael Izbicki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1"&gt;Ann B. Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Basal-Bolus Advisor for Type 1 Diabetes (T1D) Patients Using Multi-Agent Reinforcement Learning (RL) Methodology. (arXiv:2307.08897v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08897</id>
        <link href="http://arxiv.org/abs/2307.08897"/>
        <updated>2023-07-19T01:53:28.775Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel multi-agent reinforcement learning (RL) approach
for personalized glucose control in individuals with type 1 diabetes (T1D). The
method employs a closed-loop system consisting of a blood glucose (BG)
metabolic model and a multi-agent soft actor-critic RL model acting as the
basal-bolus advisor. Performance evaluation is conducted in three scenarios,
comparing the RL agents to conventional therapy. Evaluation metrics include
glucose levels (minimum, maximum, and mean), time spent in different BG ranges,
and average daily bolus and basal insulin dosages. Results demonstrate that the
RL-based basal-bolus advisor significantly improves glucose control, reducing
glycemic variability and increasing time spent within the target range (70-180
mg/dL). Hypoglycemia events are effectively prevented, and severe hyperglycemia
events are reduced. The RL approach also leads to a statistically significant
reduction in average daily basal insulin dosage compared to conventional
therapy. These findings highlight the effectiveness of the multi-agent RL
approach in achieving better glucose control and mitigating the risk of severe
hyperglycemia in individuals with T1D.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jalolia_M/0/1/0/all/0/1"&gt;Mehrad Jalolia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cescon_M/0/1/0/all/0/1"&gt;Marzia Cescon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FlexiAST: Flexibility is What AST Needs. (arXiv:2307.09286v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2307.09286</id>
        <link href="http://arxiv.org/abs/2307.09286"/>
        <updated>2023-07-19T01:53:28.773Z</updated>
        <summary type="html"><![CDATA[The objective of this work is to give patch-size flexibility to Audio
Spectrogram Transformers (AST). Recent advancements in ASTs have shown superior
performance in various audio-based tasks. However, the performance of standard
ASTs degrades drastically when evaluated using different patch sizes from that
used during training. As a result, AST models are typically re-trained to
accommodate changes in patch sizes. To overcome this limitation, this paper
proposes a training procedure to provide flexibility to standard AST models
without architectural changes, allowing them to work with various patch sizes
at the inference stage - FlexiAST. This proposed training approach simply
utilizes random patch size selection and resizing of patch and positional
embedding weights. Our experiments show that FlexiAST gives similar performance
to standard AST models while maintaining its evaluation ability at various
patch sizes on different datasets for audio classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiu Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erol_M/0/1/0/all/0/1"&gt;Mehmet Hamza Erol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1"&gt;Joon Son Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Senocak_A/0/1/0/all/0/1"&gt;Arda Senocak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Oracle Efficient Online Multicalibration and Omniprediction. (arXiv:2307.08999v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08999</id>
        <link href="http://arxiv.org/abs/2307.08999"/>
        <updated>2023-07-19T01:53:28.771Z</updated>
        <summary type="html"><![CDATA[A recent line of work has shown a surprising connection between
multicalibration, a multi-group fairness notion, and omniprediction, a learning
paradigm that provides simultaneous loss minimization guarantees for a large
family of loss functions. Prior work studies omniprediction in the batch
setting. We initiate the study of omniprediction in the online adversarial
setting. Although there exist algorithms for obtaining notions of
multicalibration in the online adversarial setting, unlike batch algorithms,
they work only for small finite classes of benchmark functions $F$, because
they require enumerating every function $f \in F$ at every round. In contrast,
omniprediction is most interesting for learning theoretic hypothesis classes
$F$, which are generally continuously large.

We develop a new online multicalibration algorithm that is well defined for
infinite benchmark classes $F$, and is oracle efficient (i.e. for any class
$F$, the algorithm has the form of an efficient reduction to a no-regret
learning algorithm for $F$). The result is the first efficient online
omnipredictor -- an oracle efficient prediction algorithm that can be used to
simultaneously obtain no regret guarantees to all Lipschitz convex loss
functions. For the class $F$ of linear functions, we show how to make our
algorithm efficient in the worst case. Also, we show upper and lower bounds on
the extent to which our rates can be improved: our oracle efficient algorithm
actually promises a stronger guarantee called swap-omniprediction, and we prove
a lower bound showing that obtaining $O(\sqrt{T})$ bounds for
swap-omniprediction is impossible in the online setting. On the other hand, we
give a (non-oracle efficient) algorithm which can obtain the optimal
$O(\sqrt{T})$ omniprediction bounds without going through multicalibration,
giving an information theoretic separation between these two solution concepts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Sumegha Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1"&gt;Christopher Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reingold_O/0/1/0/all/0/1"&gt;Omer Reingold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1"&gt;Aaron Roth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees. (arXiv:2305.11997v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.11997</id>
        <link href="http://arxiv.org/abs/2305.11997"/>
        <updated>2023-07-19T01:53:28.768Z</updated>
        <summary type="html"><![CDATA[There is an emerging interest in generating robust counterfactual
explanations that would remain valid if the model is updated or changed even
slightly. Towards finding robust counterfactuals, existing literature often
assumes that the original model $m$ and the new model $M$ are bounded in the
parameter space, i.e., $\|\text{Params}(M){-}\text{Params}(m)\|{<}\Delta$.
However, models can often change significantly in the parameter space with
little to no change in their predictions or accuracy on the given dataset. In
this work, we introduce a mathematical abstraction termed
\emph{naturally-occurring} model change, which allows for arbitrary changes in
the parameter space such that the change in predictions on points that lie on
the data manifold is limited. Next, we propose a measure -- that we call
\emph{Stability} -- to quantify the robustness of counterfactuals to potential
model changes for differentiable models, e.g., neural networks. Our main
contribution is to show that counterfactuals with sufficiently high value of
\emph{Stability} as defined by our measure will remain valid after potential
``naturally-occurring'' model changes with high probability (leveraging
concentration bounds for Lipschitz function of independent Gaussians). Since
our quantification depends on the local Lipschitz constant around a data point
which is not always available, we also examine practical relaxations of our
proposed measure and demonstrate experimentally how they can be incorporated to
find robust counterfactuals for neural networks that are close, realistic, and
remain valid after potential model changes. This work also has interesting
connections with model multiplicity, also known as, the Rashomon effect.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hamman_F/0/1/0/all/0/1"&gt;Faisal Hamman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Noorani_E/0/1/0/all/0/1"&gt;Erfaun Noorani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Saumitra Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Magazzeni_D/0/1/0/all/0/1"&gt;Daniele Magazzeni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dutta_S/0/1/0/all/0/1"&gt;Sanghamitra Dutta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cooperative Multi-Objective Reinforcement Learning for Traffic Signal Control and Carbon Emission Reduction. (arXiv:2306.09662v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.09662</id>
        <link href="http://arxiv.org/abs/2306.09662"/>
        <updated>2023-07-19T01:53:28.767Z</updated>
        <summary type="html"><![CDATA[Existing traffic signal control systems rely on oversimplified rule-based
methods, and even RL-based methods are often suboptimal and unstable. To
address this, we propose a cooperative multi-objective architecture called
Multi-Objective Multi-Agent Deep Deterministic Policy Gradient (MOMA-DDPG),
which estimates multiple reward terms for traffic signal control optimization
using age-decaying weights. Our approach involves two types of agents: one
focuses on optimizing local traffic at each intersection, while the other aims
to optimize global traffic throughput. We evaluate our method using real-world
traffic data collected from an Asian country's traffic cameras. Despite the
inclusion of a global agent, our solution remains decentralized as this agent
is no longer necessary during the inference stage. Our results demonstrate the
effectiveness of MOMA-DDPG, outperforming state-of-the-art methods across all
performance metrics. Additionally, our proposed system minimizes both waiting
time and carbon emissions. Notably, this paper is the first to link carbon
emissions and global agents in traffic signal control.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1"&gt;Cheng Ruei Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_J/0/1/0/all/0/1"&gt;Jun Wei Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1"&gt;Shin You Teng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergent regularization in inverse problems and linear plug-and-play denoisers. (arXiv:2307.09441v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2307.09441</id>
        <link href="http://arxiv.org/abs/2307.09441"/>
        <updated>2023-07-19T01:53:28.766Z</updated>
        <summary type="html"><![CDATA[Plug-and-play (PnP) denoising is a popular iterative framework for solving
imaging inverse problems using off-the-shelf image denoisers. Their empirical
success has motivated a line of research that seeks to understand the
convergence of PnP iterates under various assumptions on the denoiser. While a
significant amount of research has gone into establishing the convergence of
the PnP iteration for different regularity conditions on the denoisers, not
much is known about the asymptotic properties of the converged solution as the
noise level in the measurement tends to zero, i.e., whether PnP methods are
provably convergent regularization schemes under reasonable assumptions on the
denoiser. This paper serves two purposes: first, we provide an overview of the
classical regularization theory in inverse problems and survey a few notable
recent data-driven methods that are provably convergent regularization schemes.
We then continue to discuss PnP algorithms and their established convergence
guarantees. Subsequently, we consider PnP algorithms with linear denoisers and
propose a novel spectral filtering technique to control the strength of
regularization arising from the denoiser. Further, by relating the implicit
regularization of the denoiser to an explicit regularization functional, we
rigorously show that PnP with linear denoisers leads to a convergent
regularization scheme. More specifically, we prove that in the limit as the
noise vanishes, the PnP reconstruction converges to the minimizer of a
regularization potential subject to the solution satisfying the noiseless
operator equation. The theoretical analysis is corroborated by numerical
experiments for the classical inverse problem of tomographic image
reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Hauptmann_A/0/1/0/all/0/1"&gt;Andreas Hauptmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mukherjee_S/0/1/0/all/0/1"&gt;Subhadip Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sherry_F/0/1/0/all/0/1"&gt;Ferdia Sherry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedFormer: Contextual Federation with Attention in Reinforcement Learning. (arXiv:2205.13697v3 [cs.LG] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2205.13697</id>
        <link href="http://arxiv.org/abs/2205.13697"/>
        <updated>2023-07-19T01:53:28.761Z</updated>
        <summary type="html"><![CDATA[A core issue in multi-agent federated reinforcement learning is defining how
to aggregate insights from multiple agents. This is commonly done by taking the
average of each participating agent's model weights into one common model
(FedAvg). We instead propose FedFormer, a novel federation strategy that
utilizes Transformer Attention to contextually aggregate embeddings from models
originating from different learner agents. In so doing, we attentively weigh
the contributions of other agents with respect to the current agent's
environment and learned relationships, thus providing a more effective and
efficient federation. We evaluate our methods on the Meta-World environment and
find that our approach yields significant improvements over FedAvg and
non-federated Soft Actor-Critic single-agent methods. Our results compared to
Soft Actor-Critic show that FedFormer achieves higher episodic return while
still abiding by the privacy constraints of federated learning. Finally, we
also demonstrate improvements in effectiveness with increased agent pools
across all methods in certain tasks. This is contrasted by FedAvg, which fails
to make noticeable improvements when scaled.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hebert_L/0/1/0/all/0/1"&gt;Liam Hebert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golab_L/0/1/0/all/0/1"&gt;Lukasz Golab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poupart_P/0/1/0/all/0/1"&gt;Pascal Poupart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_R/0/1/0/all/0/1"&gt;Robin Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse Gaussian Graphical Models with Discrete Optimization: Computational and Statistical Perspectives. (arXiv:2307.09366v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09366</id>
        <link href="http://arxiv.org/abs/2307.09366"/>
        <updated>2023-07-19T01:53:28.758Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning a sparse graph underlying an undirected
Gaussian graphical model, a key problem in statistical machine learning. Given
$n$ samples from a multivariate Gaussian distribution with $p$ variables, the
goal is to estimate the $p \times p$ inverse covariance matrix (aka precision
matrix), assuming it is sparse (i.e., has a few nonzero entries). We propose
GraphL0BnB, a new estimator based on an $\ell_0$-penalized version of the
pseudolikelihood function, while most earlier approaches are based on the
$\ell_1$-relaxation. Our estimator can be formulated as a convex mixed integer
program (MIP) which can be difficult to compute at scale using off-the-shelf
commercial solvers. To solve the MIP, we propose a custom nonlinear
branch-and-bound (BnB) framework that solves node relaxations with tailored
first-order methods. As a by-product of our BnB framework, we propose
large-scale solvers for obtaining good primal solutions that are of independent
interest. We derive novel statistical guarantees (estimation and variable
selection) for our estimator and discuss how our approach improves upon
existing estimators. Our numerical experiments on real/synthetic datasets
suggest that our method can solve, to near-optimality, problem instances with
$p = 10^4$ -- corresponding to a symmetric matrix of size $p \times p$ with
$p^2/2$ binary variables. We demonstrate the usefulness of GraphL0BnB versus
various state-of-the-art approaches on a range of datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Behdin_K/0/1/0/all/0/1"&gt;Kayhan Behdin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wenyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazumder_R/0/1/0/all/0/1"&gt;Rahul Mazumder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Microseismic Event Detection and Location with a Detection Transformer. (arXiv:2307.09207v1 [physics.geo-ph])]]></title>
        <id>http://arxiv.org/abs/2307.09207</id>
        <link href="http://arxiv.org/abs/2307.09207"/>
        <updated>2023-07-19T01:53:28.755Z</updated>
        <summary type="html"><![CDATA[Microseismic event detection and location are two primary components in
microseismic monitoring, which offers us invaluable insights into the
subsurface during reservoir stimulation and evolution. Conventional approaches
for event detection and location often suffer from manual intervention and/or
heavy computation, while current machine learning-assisted approaches typically
address detection and location separately; such limitations hinder the
potential for real-time microseismic monitoring. We propose an approach to
unify event detection and source location into a single framework by adapting a
Convolutional Neural Network backbone and an encoder-decoder Transformer with a
set-based Hungarian loss, which is applied directly to recorded waveforms. The
proposed network is trained on synthetic data simulating multiple microseismic
events corresponding to random source locations in the area of suspected
microseismic activities. A synthetic test on a 2D profile of the SEAM Time
Lapse model illustrates the capability of the proposed method in detecting the
events properly and locating them in the subsurface accurately; while, a field
test using the Arkoma Basin data further proves its practicability, efficiency,
and its potential in paving the way for real-time monitoring of microseismic
events.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuanyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Birnie_C/0/1/0/all/0/1"&gt;Claire Birnie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Alkhalifah_T/0/1/0/all/0/1"&gt;Tariq Alkhalifah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An R package for parametric estimation of causal effects. (arXiv:2307.08686v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.08686</id>
        <link href="http://arxiv.org/abs/2307.08686"/>
        <updated>2023-07-19T01:53:28.754Z</updated>
        <summary type="html"><![CDATA[This article explains the usage of R package CausalModels, which is publicly
available on the Comprehensive R Archive Network. While packages are available
for sufficiently estimating causal effects, there lacks a package that provides
a collection of structural models using the conventional statistical approach
developed by Hernan and Robins (2020). CausalModels addresses this deficiency
of software in R concerning causal inference by offering tools for methods that
account for biases in observational data without requiring extensive
statistical knowledge. These methods should not be ignored and may be more
appropriate or efficient in solving particular problems. While implementations
of these statistical models are distributed among a number of causal packages,
CausalModels introduces a simple and accessible framework for a consistent
modeling pipeline among a variety of statistical methods for estimating causal
effects in a single R package. It consists of common methods including
standardization, IP weighting, G-estimation, outcome regression, instrumental
variables and propensity matching.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Anderson_J/0/1/0/all/0/1"&gt;Joshua Wolff Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rakovski_C/0/1/0/all/0/1"&gt;Cyril Rakovski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse Gaussian Graphical Models with Discrete Optimization: Computational and Statistical Perspectives. (arXiv:2307.09366v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09366</id>
        <link href="http://arxiv.org/abs/2307.09366"/>
        <updated>2023-07-19T01:53:28.752Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning a sparse graph underlying an undirected
Gaussian graphical model, a key problem in statistical machine learning. Given
$n$ samples from a multivariate Gaussian distribution with $p$ variables, the
goal is to estimate the $p \times p$ inverse covariance matrix (aka precision
matrix), assuming it is sparse (i.e., has a few nonzero entries). We propose
GraphL0BnB, a new estimator based on an $\ell_0$-penalized version of the
pseudolikelihood function, while most earlier approaches are based on the
$\ell_1$-relaxation. Our estimator can be formulated as a convex mixed integer
program (MIP) which can be difficult to compute at scale using off-the-shelf
commercial solvers. To solve the MIP, we propose a custom nonlinear
branch-and-bound (BnB) framework that solves node relaxations with tailored
first-order methods. As a by-product of our BnB framework, we propose
large-scale solvers for obtaining good primal solutions that are of independent
interest. We derive novel statistical guarantees (estimation and variable
selection) for our estimator and discuss how our approach improves upon
existing estimators. Our numerical experiments on real/synthetic datasets
suggest that our method can solve, to near-optimality, problem instances with
$p = 10^4$ -- corresponding to a symmetric matrix of size $p \times p$ with
$p^2/2$ binary variables. We demonstrate the usefulness of GraphL0BnB versus
various state-of-the-art approaches on a range of datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Behdin_K/0/1/0/all/0/1"&gt;Kayhan Behdin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wenyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazumder_R/0/1/0/all/0/1"&gt;Rahul Mazumder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Robustness of Split Learning against Adversarial Attacks. (arXiv:2307.07916v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.07916</id>
        <link href="http://arxiv.org/abs/2307.07916"/>
        <updated>2023-07-19T01:53:28.750Z</updated>
        <summary type="html"><![CDATA[Split learning enables collaborative deep learning model training while
preserving data privacy and model security by avoiding direct sharing of raw
data and model details (i.e., sever and clients only hold partial sub-networks
and exchange intermediate computations). However, existing research has mainly
focused on examining its reliability for privacy protection, with little
investigation into model security. Specifically, by exploring full models,
attackers can launch adversarial attacks, and split learning can mitigate this
severe threat by only disclosing part of models to untrusted servers.This paper
aims to evaluate the robustness of split learning against adversarial attacks,
particularly in the most challenging setting where untrusted servers only have
access to the intermediate layers of the model.Existing adversarial attacks
mostly focus on the centralized setting instead of the collaborative setting,
thus, to better evaluate the robustness of split learning, we develop a
tailored attack called SPADV, which comprises two stages: 1) shadow model
training that addresses the issue of lacking part of the model and 2) local
adversarial attack that produces adversarial examples to evaluate.The first
stage only requires a few unlabeled non-IID data, and, in the second stage,
SPADV perturbs the intermediate output of natural samples to craft the
adversarial ones. The overall cost of the proposed attack process is relatively
low, yet the empirical attack effectiveness is significantly high,
demonstrating the surprising vulnerability of split learning to adversarial
attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1"&gt;Mingyuan Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Cen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wenmeng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jun Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Coupling of Deep Learning with Logical Reasoning. (arXiv:2305.07617v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.07617</id>
        <link href="http://arxiv.org/abs/2305.07617"/>
        <updated>2023-07-19T01:53:28.749Z</updated>
        <summary type="html"><![CDATA[In the ongoing quest for hybridizing discrete reasoning with neural nets,
there is an increasing interest in neural architectures that can learn how to
solve discrete reasoning or optimization problems from natural inputs. In this
paper, we introduce a scalable neural architecture and loss function dedicated
to learning the constraints and criteria of NP-hard reasoning problems
expressed as discrete Graphical Models. Our loss function solves one of the
main limitations of Besag's pseudo-loglikelihood, enabling learning of high
energies. We empirically show it is able to efficiently learn how to solve
NP-hard reasoning problems from natural inputs as the symbolic, visual or
many-solutions Sudoku problems as well as the energy optimization formulation
of the protein design problem, providing data efficiency, interpretability, and
\textit{a posteriori} control over predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Defresne_M/0/1/0/all/0/1"&gt;Marianne Defresne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barbe_S/0/1/0/all/0/1"&gt;Sophie Barbe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schiex_T/0/1/0/all/0/1"&gt;Thomas Schiex&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Siamese Networks for Weakly Supervised Human Activity Recognition. (arXiv:2307.08944v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2307.08944</id>
        <link href="http://arxiv.org/abs/2307.08944"/>
        <updated>2023-07-19T01:53:28.748Z</updated>
        <summary type="html"><![CDATA[Deep learning has been successfully applied to human activity recognition.
However, training deep neural networks requires explicitly labeled data which
is difficult to acquire. In this paper, we present a model with multiple
siamese networks that are trained by using only the information about the
similarity between pairs of data samples without knowing the explicit labels.
The trained model maps the activity data samples into fixed size representation
vectors such that the distance between the vectors in the representation space
approximates the similarity of the data samples in the input space. Thus, the
trained model can work as a metric for a wide range of different clustering
algorithms. The training process minimizes a similarity loss function that
forces the distance metric to be small for pairs of samples from the same kind
of activity, and large for pairs of samples from different kinds of activities.
We evaluate the model on three datasets to verify its effectiveness in
segmentation and recognition of continuous human activity sequences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_T/0/1/0/all/0/1"&gt;Taoran Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1"&gt;Manfred Huber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Batched Predictors Generalize within Distribution. (arXiv:2307.09379v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.09379</id>
        <link href="http://arxiv.org/abs/2307.09379"/>
        <updated>2023-07-19T01:53:28.747Z</updated>
        <summary type="html"><![CDATA[We study the generalization properties of batched predictors, i.e., models
tasked with predicting the mean label of a small set (or batch) of examples.
The batched prediction paradigm is particularly relevant for models deployed to
determine the quality of a group of compounds in preparation for offline
testing. By utilizing a suitable generalization of the Rademacher complexity,
we prove that batched predictors come with exponentially stronger
generalization guarantees as compared to the standard per-sample approach.
Surprisingly, the proposed bound holds independently of overparametrization.
Our theoretical insights are validated experimentally for various tasks,
architectures, and applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Loukas_A/0/1/0/all/0/1"&gt;Andreas Loukas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kessel_P/0/1/0/all/0/1"&gt;Pan Kessel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Score-Difference Flow for Implicit Generative Modeling. (arXiv:2304.12906v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.12906</id>
        <link href="http://arxiv.org/abs/2304.12906"/>
        <updated>2023-07-19T01:53:28.740Z</updated>
        <summary type="html"><![CDATA[Implicit generative modeling (IGM) aims to produce samples of synthetic data
matching the characteristics of a target data distribution. Recent work (e.g.
score-matching networks, diffusion models) has approached the IGM problem from
the perspective of pushing synthetic source data toward the target distribution
via dynamical perturbations or flows in the ambient space. In this direction,
we present the score difference (SD) between arbitrary target and source
distributions as a flow that optimally reduces the Kullback-Leibler divergence
between them while also solving the Schroedinger bridge problem. We apply the
SD flow to convenient proxy distributions, which are aligned if and only if the
original distributions are aligned. We demonstrate the formal equivalence of
this formulation to denoising diffusion models under certain conditions. We
also show that the training of generative adversarial networks includes a
hidden data-optimization sub-problem, which induces the SD flow under certain
choices of loss function when the discriminator is optimal. As a result, the SD
flow provides a theoretical link between model classes that individually
address the three challenges of the "generative modeling trilemma" -- high
sample quality, mode coverage, and fast sampling -- thereby setting the stage
for a unified approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weber_R/0/1/0/all/0/1"&gt;Romann M. Weber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Representation Disentanglement for Clustering. (arXiv:2306.05439v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.05439</id>
        <link href="http://arxiv.org/abs/2306.05439"/>
        <updated>2023-07-19T01:53:28.738Z</updated>
        <summary type="html"><![CDATA[Clustering continues to be a significant and challenging task. Recent studies
have demonstrated impressive results by applying clustering to feature
representations acquired through self-supervised learning, particularly on
small datasets. However, when dealing with datasets containing a large number
of clusters, such as ImageNet, current methods struggle to achieve satisfactory
clustering performance. In this paper, we introduce a novel method called
Contrastive representation Disentanglement for Clustering (CDC) that leverages
contrastive learning to directly disentangle the feature representation for
clustering. In CDC, we decompose the representation into two distinct
components: one component encodes categorical information under an
equipartition constraint, and the other component captures instance-specific
factors. To train our model, we propose a contrastive loss that effectively
utilizes both components of the representation. We conduct a theoretical
analysis of the proposed loss and highlight how it assigns different weights to
negative samples during the process of disentangling the feature
representation. Further analysis of the gradients reveals that larger weights
emphasize a stronger focus on hard negative samples. As a result, the proposed
loss exhibits strong expressiveness, enabling efficient disentanglement of
categorical information. Through experimental evaluation on various benchmark
datasets, our method demonstrates either state-of-the-art or highly competitive
clustering performance. Notably, on the complete ImageNet dataset, we achieve
an accuracy of 53.4%, surpassing existing methods by a substantial margin of
+10.2%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1"&gt;Fei Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krovi_V/0/1/0/all/0/1"&gt;Venkat Krovi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1"&gt;Feng Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mining of Single-Class by Active Learning for Semantic Segmentation. (arXiv:2307.09109v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09109</id>
        <link href="http://arxiv.org/abs/2307.09109"/>
        <updated>2023-07-19T01:53:28.736Z</updated>
        <summary type="html"><![CDATA[Several Active Learning (AL) policies require retraining a target model
several times in order to identify the most informative samples and rarely
offer the option to focus on the acquisition of samples from underrepresented
classes. Here the Mining of Single-Class by Active Learning (MiSiCAL) paradigm
is introduced where an AL policy is constructed through deep reinforcement
learning and exploits quantity-accuracy correlations to build datasets on which
high-performance models can be trained with regards to specific classes.
MiSiCAL is especially helpful in the case of very large batch sizes since it
does not require repeated model training sessions as is common in other AL
methods. This is thanks to its ability to exploit fixed representations of the
candidate data points. We find that MiSiCAL is able to outperform a random
policy on 150 out of 171 COCO10k classes, while the strongest baseline only
outperforms random on 101 classes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lambert_H/0/1/0/all/0/1"&gt;Hugues Lambert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slade_E/0/1/0/all/0/1"&gt;Emma Slade&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast. (arXiv:2304.02689v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.02689</id>
        <link href="http://arxiv.org/abs/2304.02689"/>
        <updated>2023-07-19T01:53:28.732Z</updated>
        <summary type="html"><![CDATA[Medical data often exhibits long-tail distributions with heavy class
imbalance, which naturally leads to difficulty in classifying the minority
classes (i.e., boundary regions or rare objects). Recent work has significantly
improved semi-supervised medical image segmentation in long-tailed scenarios by
equipping them with unsupervised contrastive criteria. However, it remains
unclear how well they will perform in the labeled portion of data where class
distribution is also highly imbalanced. In this work, we present ACTION++, an
improved contrastive learning framework with adaptive anatomical contrast for
semi-supervised medical segmentation. Specifically, we propose an adaptive
supervised contrastive loss, where we first compute the optimal locations of
class centers uniformly distributed on the embedding space (i.e., off-line),
and then perform online contrastive matching training by encouraging different
class features to adaptively match these distinct and uniformly distributed
class centers. Moreover, we argue that blindly adopting a constant temperature
$\tau$ in the contrastive loss on long-tailed medical data is not optimal, and
propose to use a dynamic $\tau$ via a simple cosine schedule to yield better
separation between majority and minority classes. Empirically, we evaluate
ACTION++ on ACDC and LA benchmarks and show that it achieves state-of-the-art
across two semi-supervised settings. Theoretically, we analyze the performance
of adaptive anatomical contrast and confirm its superiority in label
efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1"&gt;Weicheng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1"&gt;Yifei Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1"&gt;Lawrence Staib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sekhon_J/0/1/0/all/0/1"&gt;Jasjeet S. Sekhon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1"&gt;James S. Duncan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Safe Policy Learning with Chance Constrained Optimization: Application to Military Security Assessment during the Vietnam War. (arXiv:2307.08840v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08840</id>
        <link href="http://arxiv.org/abs/2307.08840"/>
        <updated>2023-07-19T01:53:28.729Z</updated>
        <summary type="html"><![CDATA[Algorithmic and data-driven decisions and recommendations are commonly used
in high-stakes decision-making settings such as criminal justice, medicine, and
public policy. We investigate whether it would have been possible to improve a
security assessment algorithm employed during the Vietnam War, using outcomes
measured immediately after its introduction in late 1969. This empirical
application raises several methodological challenges that frequently arise in
high-stakes algorithmic decision-making. First, before implementing a new
algorithm, it is essential to characterize and control the risk of yielding
worse outcomes than the existing algorithm. Second, the existing algorithm is
deterministic, and learning a new algorithm requires transparent extrapolation.
Third, the existing algorithm involves discrete decision tables that are common
but difficult to optimize over.

To address these challenges, we introduce the Average Conditional Risk
(ACRisk), which first quantifies the risk that a new algorithmic policy leads
to worse outcomes for subgroups of individual units and then averages this over
the distribution of subgroups. We also propose a Bayesian policy learning
framework that maximizes the posterior expected value while controlling the
posterior expected ACRisk. This framework separates the estimation of
heterogeneous treatment effects from policy optimization, enabling flexible
estimation of effects and optimization over complex policy classes. We
characterize the resulting chance-constrained optimization problem as a
constrained linear programming problem. Our analysis shows that compared to the
actual algorithm used during the Vietnam War, the learned algorithm assesses
most regions as more secure and emphasizes economic and political factors over
military factors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1"&gt;Zeyang Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Michael_E/0/1/0/all/0/1"&gt;Eli Ben-Michael&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imai_K/0/1/0/all/0/1"&gt;Kosuke Imai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-stationary Delayed Combinatorial Semi-Bandit with Causally Related Rewards. (arXiv:2307.09093v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09093</id>
        <link href="http://arxiv.org/abs/2307.09093"/>
        <updated>2023-07-19T01:53:28.715Z</updated>
        <summary type="html"><![CDATA[Sequential decision-making under uncertainty is often associated with long
feedback delays. Such delays degrade the performance of the learning agent in
identifying a subset of arms with the optimal collective reward in the long
run. This problem becomes significantly challenging in a non-stationary
environment with structural dependencies amongst the reward distributions
associated with the arms. Therefore, besides adapting to delays and
environmental changes, learning the causal relations alleviates the adverse
effects of feedback delay on the decision-making process. We formalize the
described setting as a non-stationary and delayed combinatorial semi-bandit
problem with causally related rewards. We model the causal relations by a
directed graph in a stationary structural equation model. The agent maximizes
the long-term average payoff, defined as a linear function of the base arms'
rewards. We develop a policy that learns the structural dependencies from
delayed feedback and utilizes that to optimize the decision-making while
adapting to drifts. We prove a regret bound for the performance of the proposed
algorithm. Besides, we evaluate our method via numerical analysis using
synthetic and real-world datasets to detect the regions that contribute the
most to the spread of Covid-19 in Italy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghoorchian_S/0/1/0/all/0/1"&gt;Saeed Ghoorchian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maghsudi_S/0/1/0/all/0/1"&gt;Setareh Maghsudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Select SAT Encodings for Pseudo-Boolean and Linear Integer Constraints. (arXiv:2307.09342v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2307.09342</id>
        <link href="http://arxiv.org/abs/2307.09342"/>
        <updated>2023-07-19T01:53:28.698Z</updated>
        <summary type="html"><![CDATA[Many constraint satisfaction and optimisation problems can be solved
effectively by encoding them as instances of the Boolean Satisfiability problem
(SAT). However, even the simplest types of constraints have many encodings in
the literature with widely varying performance, and the problem of selecting
suitable encodings for a given problem instance is not trivial. We explore the
problem of selecting encodings for pseudo-Boolean and linear constraints using
a supervised machine learning approach. We show that it is possible to select
encodings effectively using a standard set of features for constraint problems;
however we obtain better performance with a new set of features specifically
designed for the pseudo-Boolean and linear constraints. In fact, we achieve
good results when selecting encodings for unseen problem classes. Our results
compare favourably to AutoFolio when using the same feature set. We discuss the
relative importance of instance features to the task of selecting the best
encodings, and compare several variations of the machine learning method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ulrich_Oltean_F/0/1/0/all/0/1"&gt;Felix Ulrich-Oltean&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nightingale_P/0/1/0/all/0/1"&gt;Peter Nightingale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1"&gt;James Alfred Walker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Objective GFlowNets. (arXiv:2210.12765v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.12765</id>
        <link href="http://arxiv.org/abs/2210.12765"/>
        <updated>2023-07-19T01:53:28.668Z</updated>
        <summary type="html"><![CDATA[We study the problem of generating diverse candidates in the context of
Multi-Objective Optimization. In many applications of machine learning such as
drug discovery and material design, the goal is to generate candidates which
simultaneously optimize a set of potentially conflicting objectives. Moreover,
these objectives are often imperfect evaluations of some underlying property of
interest, making it important to generate diverse candidates to have multiple
options for expensive downstream evaluations. We propose Multi-Objective
GFlowNets (MOGFNs), a novel method for generating diverse Pareto optimal
solutions, based on GFlowNets. We introduce two variants of MOGFNs: MOGFN-PC,
which models a family of independent sub-problems defined by a scalarization
function, with reward-conditional GFlowNets, and MOGFN-AL, which solves a
sequence of sub-problems defined by an acquisition function in an active
learning loop. Our experiments on wide variety of synthetic and benchmark tasks
demonstrate advantages of the proposed methods in terms of the Pareto
performance and importantly, improved candidate diversity, which is the main
contribution of this work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1"&gt;Moksh Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raparthy_S/0/1/0/all/0/1"&gt;Sharath Chandra Raparthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_Garcia_A/0/1/0/all/0/1"&gt;Alex Hernandez-Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rector_Brooks_J/0/1/0/all/0/1"&gt;Jarrid Rector-Brooks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miret_S/0/1/0/all/0/1"&gt;Santiago Miret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_E/0/1/0/all/0/1"&gt;Emmanuel Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Biomaker CA: a Biome Maker project using Cellular Automata. (arXiv:2307.09320v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2307.09320</id>
        <link href="http://arxiv.org/abs/2307.09320"/>
        <updated>2023-07-19T01:53:28.662Z</updated>
        <summary type="html"><![CDATA[We introduce Biomaker CA: a Biome Maker project using Cellular Automata (CA).
In Biomaker CA, morphogenesis is a first class citizen and small seeds need to
grow into plant-like organisms to survive in a nutrient starved environment and
eventually reproduce with variation so that a biome survives for long
timelines. We simulate complex biomes by means of CA rules in 2D grids and
parallelize all of its computation on GPUs through the Python JAX framework. We
show how this project allows for several different kinds of environments and
laws of 'physics', alongside different model architectures and mutation
strategies. We further analyze some configurations to show how plant agents can
grow, survive, reproduce, and evolve, forming stable and unstable biomes. We
then demonstrate how one can meta-evolve models to survive in a harsh
environment either through end-to-end meta-evolution or by a more surgical and
efficient approach, called Petri dish meta-evolution. Finally, we show how to
perform interactive evolution, where the user decides how to evolve a plant
model interactively and then deploys it in a larger environment. We open source
Biomaker CA at: https://tinyurl.com/2x8yu34s .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Randazzo_E/0/1/0/all/0/1"&gt;Ettore Randazzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mordvintsev_A/0/1/0/all/0/1"&gt;Alexander Mordvintsev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data. (arXiv:2307.09249v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09249</id>
        <link href="http://arxiv.org/abs/2307.09249"/>
        <updated>2023-07-19T01:53:28.658Z</updated>
        <summary type="html"><![CDATA[Recent advancements in Natural Language Processing (NLP) have witnessed the
groundbreaking impact of pretrained models, yielding impressive outcomes across
various tasks. This study seeks to extend the power of pretraining
methodologies to tabular data, a domain traditionally overlooked, yet
inherently challenging due to the plethora of table schemas intrinsic to
different tasks. The primary research questions underpinning this work revolve
around the adaptation to heterogeneous table structures, the establishment of a
universal pretraining protocol for tabular data, the generalizability and
transferability of learned knowledge across tasks, the adaptation to diverse
downstream applications, and the incorporation of incremental columns over
time. In response to these challenges, we introduce UniTabE, a pioneering
method designed to process tables in a uniform manner, devoid of constraints
imposed by specific table structures. UniTabE's core concept relies on
representing each basic table element with a module, termed TabUnit. This is
subsequently followed by a Transformer encoder to refine the representation.
Moreover, our model is designed to facilitate pretraining and finetuning
through the utilization of free-form prompts. In order to implement the
pretraining phase, we curated an expansive tabular dataset comprising
approximately 13 billion samples, meticulously gathered from the Kaggle
platform. Rigorous experimental testing and analyses were performed under a
myriad of scenarios to validate the effectiveness of our methodology. The
experimental results demonstrate UniTabE's superior performance against several
baseline models across a multitude of benchmark datasets. This, therefore,
underscores UniTabE's potential to significantly enhance the semantic
representation of tabular data, thereby marking a significant stride in the
field of tabular data analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yazheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Ledell Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qi Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels. (arXiv:2307.08809v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08809</id>
        <link href="http://arxiv.org/abs/2307.08809"/>
        <updated>2023-07-19T01:53:28.563Z</updated>
        <summary type="html"><![CDATA[Many existing FL methods assume clients with fully-labeled data, while in
realistic settings, clients have limited labels due to the expensive and
laborious process of labeling. Limited labeled local data of the clients often
leads to their local model having poor generalization abilities to their larger
unlabeled local data, such as having class-distribution mismatch with the
unlabeled data. As a result, clients may instead look to benefit from the
global model trained across clients to leverage their unlabeled data, but this
also becomes difficult due to data heterogeneity across clients. In our work,
we propose FedLabel where clients selectively choose the local or global model
to pseudo-label their unlabeled data depending on which is more of an expert of
the data. We further utilize both the local and global models' knowledge via
global-local consistency regularization which minimizes the divergence between
the two models' outputs when they have identical pseudo-labels for the
unlabeled data. Unlike other semi-supervised FL baselines, our method does not
require additional experts other than the local or global model, nor require
additional parameters to be communicated. We also do not assume any
server-labeled data or fully labeled clients. For both cross-device and
cross-silo settings, we show that FedLabel outperforms other semi-supervised FL
baselines by $8$-$24\%$, and even outperforms standard fully supervised FL
baselines ($100\%$ labeled data) with only $5$-$20\%$ of labeled data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1"&gt;Yae Jee Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_G/0/1/0/all/0/1"&gt;Gauri Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dimitriadis_D/0/1/0/all/0/1"&gt;Dimitrios Dimitriadis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Learning with Costly Features in Non-stationary Environments. (arXiv:2307.09388v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09388</id>
        <link href="http://arxiv.org/abs/2307.09388"/>
        <updated>2023-07-19T01:53:28.516Z</updated>
        <summary type="html"><![CDATA[Maximizing long-term rewards is the primary goal in sequential
decision-making problems. The majority of existing methods assume that side
information is freely available, enabling the learning agent to observe all
features' states before making a decision. In real-world problems, however,
collecting beneficial information is often costly. That implies that, besides
individual arms' reward, learning the observations of the features' states is
essential to improve the decision-making strategy. The problem is aggravated in
a non-stationary environment where reward and cost distributions undergo abrupt
changes over time. To address the aforementioned dual learning problem, we
extend the contextual bandit setting and allow the agent to observe subsets of
features' states. The objective is to maximize the long-term average gain,
which is the difference between the accumulated rewards and the paid costs on
average. Therefore, the agent faces a trade-off between minimizing the cost of
information acquisition and possibly improving the decision-making process
using the obtained information. To this end, we develop an algorithm that
guarantees a sublinear regret in time. Numerical results demonstrate the
superiority of our proposed policy in a real-world scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghoorchian_S/0/1/0/all/0/1"&gt;Saeed Ghoorchian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kortukov_E/0/1/0/all/0/1"&gt;Evgenii Kortukov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maghsudi_S/0/1/0/all/0/1"&gt;Setareh Maghsudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models. (arXiv:2307.09209v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.09209</id>
        <link href="http://arxiv.org/abs/2307.09209"/>
        <updated>2023-07-19T01:53:28.513Z</updated>
        <summary type="html"><![CDATA[We analyze sentiment analysis and toxicity detection models to detect the
presence of explicit bias against people with disability (PWD). We employ the
bias identification framework of Perturbation Sensitivity Analysis to examine
conversations related to PWD on social media platforms, specifically Twitter
and Reddit, in order to gain insight into how disability bias is disseminated
in real-world social settings. We then create the \textit{Bias Identification
Test in Sentiment} (BITS) corpus to quantify explicit disability bias in any
sentiment analysis and toxicity detection models. Our study utilizes BITS to
uncover significant biases in four open AIaaS (AI as a Service) sentiment
analysis tools, namely TextBlob, VADER, Google Cloud Natural Language API,
DistilBERT and two toxicity detection models, namely two versions of
Toxic-BERT. Our findings indicate that all of these models exhibit
statistically significant explicit bias against PWD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Venkit_P/0/1/0/all/0/1"&gt;Pranav Narayanan Venkit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinath_M/0/1/0/all/0/1"&gt;Mukund Srinath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1"&gt;Shomir Wilson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Topological Feature via Persistent Homology: Filtration Learning for Point Clouds. (arXiv:2307.09259v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09259</id>
        <link href="http://arxiv.org/abs/2307.09259"/>
        <updated>2023-07-19T01:53:28.510Z</updated>
        <summary type="html"><![CDATA[Machine learning for point clouds has been attracting much attention, with
many applications in various fields, such as shape recognition and material
science. To enhance the accuracy of such machine learning methods, it is known
to be effective to incorporate global topological features, which are typically
extracted by persistent homology. In the calculation of persistent homology for
a point cloud, we need to choose a filtration for the point clouds, an
increasing sequence of spaces. Because the performance of machine learning
methods combined with persistent homology is highly affected by the choice of a
filtration, we need to tune it depending on data and tasks. In this paper, we
propose a framework that learns a filtration adaptively with the use of neural
networks. In order to make the resulting persistent homology
isometry-invariant, we develop a neural network architecture with such
invariance. Additionally, we theoretically show a finite-dimensional
approximation result that justifies our architecture. Experimental results
demonstrated the efficacy of our framework in several classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nishikawa_N/0/1/0/all/0/1"&gt;Naoki Nishikawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ike_Y/0/1/0/all/0/1"&gt;Yuichi Ike&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamanishi_K/0/1/0/all/0/1"&gt;Kenji Yamanishi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mitigating Label Bias via Decoupled Confident Learning. (arXiv:2307.08945v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08945</id>
        <link href="http://arxiv.org/abs/2307.08945"/>
        <updated>2023-07-19T01:53:28.509Z</updated>
        <summary type="html"><![CDATA[Growing concerns regarding algorithmic fairness have led to a surge in
methodologies to mitigate algorithmic bias. However, such methodologies largely
assume that observed labels in training data are correct. This is problematic
because bias in labels is pervasive across important domains, including
healthcare, hiring, and content moderation. In particular, human-generated
labels are prone to encoding societal biases. While the presence of labeling
bias has been discussed conceptually, there is a lack of methodologies to
address this problem. We propose a pruning method -- Decoupled Confident
Learning (DeCoLe) -- specifically designed to mitigate label bias. After
illustrating its performance on a synthetic dataset, we apply DeCoLe in the
context of hate speech detection, where label bias has been recognized as an
important challenge, and show that it successfully identifies biased labels and
outperforms competing approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+De_Arteaga_M/0/1/0/all/0/1"&gt;Maria De-Arteaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saar_Tsechansky_M/0/1/0/all/0/1"&gt;Maytal Saar-Tsechansky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy-preserving patient clustering for personalized federated learning. (arXiv:2307.08847v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08847</id>
        <link href="http://arxiv.org/abs/2307.08847"/>
        <updated>2023-07-19T01:53:28.508Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) is a machine learning framework that enables multiple
organizations to train a model without sharing their data with a central
server. However, it experiences significant performance degradation if the data
is non-identically independently distributed (non-IID). This is a problem in
medical settings, where variations in the patient population contribute
significantly to distribution differences across hospitals. Personalized FL
addresses this issue by accounting for site-specific distribution differences.
Clustered FL, a Personalized FL variant, was used to address this problem by
clustering patients into groups across hospitals and training separate models
on each group. However, privacy concerns remained as a challenge as the
clustering process requires exchange of patient-level information. This was
previously solved by forming clusters using aggregated data, which led to
inaccurate groups and performance degradation. In this study, we propose
Privacy-preserving Community-Based Federated machine Learning (PCBFL), a novel
Clustered FL framework that can cluster patients using patient-level data while
protecting privacy. PCBFL uses Secure Multiparty Computation, a cryptographic
technique, to securely calculate patient-level similarity scores across
hospitals. We then evaluate PCBFL by training a federated mortality prediction
model using 20 sites from the eICU dataset. We compare the performance gain
from PCBFL against traditional and existing Clustered FL frameworks. Our
results show that PCBFL successfully forms clinically meaningful cohorts of
low, medium, and high-risk patients. PCBFL outperforms traditional and existing
Clustered FL frameworks with an average AUC improvement of 4.3% and AUPRC
improvement of 7.8%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elhussein_A/0/1/0/all/0/1"&gt;Ahmed Elhussein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gursoy_G/0/1/0/all/0/1"&gt;Gamze Gursoy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Outlier-Robust Tensor Low-Rank Representation for Data Clustering. (arXiv:2307.09055v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.09055</id>
        <link href="http://arxiv.org/abs/2307.09055"/>
        <updated>2023-07-19T01:53:28.507Z</updated>
        <summary type="html"><![CDATA[Low-rank tensor analysis has received widespread attention with many
practical applications. However, the tensor data are often contaminated by
outliers or sample-specific corruptions. How to recover the tensor data that
are corrupted by outliers and perform data clustering remains a challenging
problem. This paper develops an outlier-robust tensor low-rank representation
(OR-TLRR) method for simultaneous outlier detection and tensor data clustering
based on the tensor singular value decomposition (t-SVD) algebraic framework.
It is motivated by the recently proposed tensor-tensor product induced by
invertible linear transforms that satisfy certain conditions. For tensor
observations with arbitrary outlier corruptions, OR-TLRR has provable
performance guarantee for exactly recovering the row space of clean data and
detecting outliers under mild conditions. Moreover, an extension of OR-TLRR is
also proposed to handle the case when parts of the data are missing. Finally,
extensive experimental results on both synthetic and real data demonstrate the
effectiveness of the proposed algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tong Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Many Neurons Does it Take to Approximate the Maximum?. (arXiv:2307.09212v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09212</id>
        <link href="http://arxiv.org/abs/2307.09212"/>
        <updated>2023-07-19T01:53:28.506Z</updated>
        <summary type="html"><![CDATA[We study the size of a neural network needed to approximate the maximum
function over $d$ inputs, in the most basic setting of approximating with
respect to the $L_2$ norm, for continuous distributions, for a network that
uses ReLU activations. We provide new lower and upper bounds on the width
required for approximation across various depths. Our results establish new
depth separations between depth 2 and 3, and depth 3 and 5 networks, as well as
providing a depth $\mathcal{O}(\log(\log(d)))$ and width $\mathcal{O}(d)$
construction which approximates the maximum function, significantly improving
upon the depth requirements of the best previously known bounds for networks
with linearly-bounded width. Our depth separation results are facilitated by a
new lower bound for depth 2 networks approximating the maximum function over
the uniform distribution, assuming an exponential upper bound on the size of
the weights. Furthermore, we are able to use this depth 2 lower bound to
provide tight bounds on the number of neurons needed to approximate the maximum
by a depth 3 network. Our lower bounds are of potentially broad interest as
they apply to the widely studied and used \emph{max} function, in contrast to
many previous results that base their bounds on specially constructed or
pathological functions and distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Safran_I/0/1/0/all/0/1"&gt;Itay Safran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reichman_D/0/1/0/all/0/1"&gt;Daniel Reichman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valiant_P/0/1/0/all/0/1"&gt;Paul Valiant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Off-Policy Learning to Rank: a Reinforcement Learning Perspective. (arXiv:2306.07528v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.07528</id>
        <link href="http://arxiv.org/abs/2306.07528"/>
        <updated>2023-07-19T01:53:28.505Z</updated>
        <summary type="html"><![CDATA[Off-policy Learning to Rank (LTR) aims to optimize a ranker from data
collected by a deployed logging policy. However, existing off-policy learning
to rank methods often make strong assumptions about how users generate the
click data, i.e., the click model, and hence need to tailor their methods
specifically under different click models. In this paper, we unified the
ranking process under general stochastic click models as a Markov Decision
Process (MDP), and the optimal ranking could be learned with offline
reinforcement learning (RL) directly. Building upon this, we leverage offline
RL techniques for off-policy LTR and propose the Click Model-Agnostic Unified
Off-policy Learning to Rank (CUOLR) method, which could be easily applied to a
wide range of click models. Through a dedicated formulation of the MDP, we show
that offline RL algorithms can adapt to various click models without complex
debiasing techniques and prior knowledge of the model. Results on various
large-scale datasets demonstrate that CUOLR consistently outperforms the
state-of-the-art off-policy learning to rank algorithms while maintaining
consistency and robustness under different click models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zeyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1"&gt;Yi Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Hui Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yiran Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_R/0/1/0/all/0/1"&gt;Rishab Balasubramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qingyun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huazheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengdi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using the IBM Analog In-Memory Hardware Acceleration Kit for Neural Network Training and Inference. (arXiv:2307.09357v1 [cs.ET])]]></title>
        <id>http://arxiv.org/abs/2307.09357</id>
        <link href="http://arxiv.org/abs/2307.09357"/>
        <updated>2023-07-19T01:53:28.503Z</updated>
        <summary type="html"><![CDATA[Analog In-Memory Computing (AIMC) is a promising approach to reduce the
latency and energy consumption of Deep Neural Network (DNN) inference and
training. However, the noisy and non-linear device characteristics, and the
non-ideal peripheral circuitry in AIMC chips, require adapting DNNs to be
deployed on such hardware to achieve equivalent accuracy to digital computing.
In this tutorial, we provide a deep dive into how such adaptations can be
achieved and evaluated using the recently released IBM Analog Hardware
Acceleration Kit (AIHWKit), freely available at https://github.com/IBM/aihwkit.
The AIHWKit is a Python library that simulates inference and training of DNNs
using AIMC. We present an in-depth description of the AIHWKit design,
functionality, and best practices to properly perform inference and training.
We also present an overview of the Analog AI Cloud Composer, that provides the
benefits of using the AIHWKit simulation platform in a fully managed cloud
setting. Finally, we show examples on how users can expand and customize
AIHWKit for their own needs. This tutorial is accompanied by comprehensive
Jupyter Notebook code examples that can be run using AIHWKit, which can be
downloaded from https://github.com/IBM/aihwkit/tree/master/notebooks/tutorial.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gallo_M/0/1/0/all/0/1"&gt;Manuel Le Gallo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lammie_C/0/1/0/all/0/1"&gt;Corey Lammie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buechel_J/0/1/0/all/0/1"&gt;Julian Buechel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carta_F/0/1/0/all/0/1"&gt;Fabio Carta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fagbohungbe_O/0/1/0/all/0/1"&gt;Omobayode Fagbohungbe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mackin_C/0/1/0/all/0/1"&gt;Charles Mackin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_H/0/1/0/all/0/1"&gt;Hsinyu Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1"&gt;Vijay Narayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebastian_A/0/1/0/all/0/1"&gt;Abu Sebastian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maghraoui_K/0/1/0/all/0/1"&gt;Kaoutar El Maghraoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasch_M/0/1/0/all/0/1"&gt;Malte J. Rasch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[qecGPT: decoding Quantum Error-correcting Codes with Generative Pre-trained Transformers. (arXiv:2307.09025v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2307.09025</id>
        <link href="http://arxiv.org/abs/2307.09025"/>
        <updated>2023-07-19T01:53:28.502Z</updated>
        <summary type="html"><![CDATA[We propose a general framework for decoding quantum error-correcting codes
with generative modeling. The model utilizes autoregressive neural networks,
specifically Transformers, to learn the joint probability of logical operators
and syndromes. This training is in an unsupervised way, without the need for
labeled training data, and is thus referred to as pre-training. After the
pre-training, the model can efficiently compute the likelihood of logical
operators for any given syndrome, using maximum likelihood decoding. It can
directly generate the most-likely logical operators with computational
complexity $\mathcal O(2k)$ in the number of logical qubits $k$, which is
significantly better than the conventional maximum likelihood decoding
algorithms that require $\mathcal O(4^k)$ computation. Based on the pre-trained
model, we further propose refinement to achieve more accurately the likelihood
of logical operators for a given syndrome by directly sampling the stabilizer
operators. We perform numerical experiments on stabilizer codes with small code
distances, using both depolarizing error models and error models with
correlated noise. The results show that our approach provides significantly
better decoding accuracy than the minimum weight perfect matching and
belief-propagation-based algorithms. Our framework is general and can be
applied to any error model and quantum codes with different topologies such as
surface codes and quantum LDPC codes. Furthermore, it leverages the
parallelization capabilities of GPUs, enabling simultaneous decoding of a large
number of syndromes. Our approach sheds light on the efficient and accurate
decoding of quantum error-correcting codes using generative artificial
intelligence and modern computational power.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Cao_H/0/1/0/all/0/1"&gt;Hanyan Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Pan_F/0/1/0/all/0/1"&gt;Feng Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yijia Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Embedding Quality Evaluation. (arXiv:2305.16562v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.16562</id>
        <link href="http://arxiv.org/abs/2305.16562"/>
        <updated>2023-07-19T01:53:28.500Z</updated>
        <summary type="html"><![CDATA[Unsupervised learning has recently significantly gained in popularity,
especially with deep learning-based approaches. Despite numerous successes and
approaching supervised-level performance on a variety of academic benchmarks,
it is still hard to train and evaluate SSL models in practice due to the
unsupervised nature of the problem. Even with networks trained in a supervised
fashion, it is often unclear whether they will perform well when transferred to
another domain.

Past works are generally limited to assessing the amount of information
contained in embeddings, which is most relevant for self-supervised learning of
deep neural networks. This works chooses to follow a different approach: can we
quantify how easy it is to linearly separate the data in a stable way? We
survey the literature and uncover three methods that could be potentially used
for evaluating quality of representations. We also introduce one novel method
based on recent advances in understanding the high-dimensional geometric
structure of self-supervised learning.

We conduct extensive experiments and study the properties of these metrics
and ones introduced in the previous work. Our results suggest that while there
is no free lunch, there are metrics that can robustly estimate embedding
quality in an unsupervised way.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsitsulin_A/0/1/0/all/0/1"&gt;Anton Tsitsulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Munkhoeva_M/0/1/0/all/0/1"&gt;Marina Munkhoeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perozzi_B/0/1/0/all/0/1"&gt;Bryan Perozzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge. (arXiv:2307.08813v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.08813</id>
        <link href="http://arxiv.org/abs/2307.08813"/>
        <updated>2023-07-19T01:53:28.473Z</updated>
        <summary type="html"><![CDATA[Understanding protein interactions and pathway knowledge is crucial for
unraveling the complexities of living systems and investigating the underlying
mechanisms of biological functions and complex diseases. While existing
databases provide curated biological data from literature and other sources,
they are often incomplete and their maintenance is labor-intensive,
necessitating alternative approaches. In this study, we propose to harness the
capabilities of large language models to address these issues by automatically
extracting such knowledge from the relevant scientific literature. Toward this
goal, in this work, we investigate the effectiveness of different large
language models in tasks that involve recognizing protein interactions,
pathways, and gene regulatory relations. We thoroughly evaluate the performance
of various models, highlight the significant findings, and discuss both the
future opportunities and the remaining challenges associated with this
approach. The code and data are available at:
https://github.com/boxorange/BioIE-LLM]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1"&gt;Gilchan Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_B/0/1/0/all/0/1"&gt;Byung-Jun Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xihaier Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_Marrero_V/0/1/0/all/0/1"&gt;Vanessa L&amp;#xf3;pez-Marrero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnstone_P/0/1/0/all/0/1"&gt;Patrick Johnstone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1"&gt;Shinjae Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alexander_F/0/1/0/all/0/1"&gt;Francis J. Alexander&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Role of Transparency in Repeated First-Price Auctions with Unknown Valuations. (arXiv:2307.09478v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2307.09478</id>
        <link href="http://arxiv.org/abs/2307.09478"/>
        <updated>2023-07-19T01:53:28.472Z</updated>
        <summary type="html"><![CDATA[We study the problem of regret minimization for a single bidder in a sequence
of first-price auctions where the bidder knows the item's value only if the
auction is won. Our main contribution is a complete characterization, up to
logarithmic factors, of the minimax regret in terms of the auction's
transparency, which regulates the amount of information on competing bids
disclosed by the auctioneer at the end of each auction. Our results hold under
different assumptions (stochastic, adversarial, and their smoothed variants) on
the environment generating the bidder's valuations and competing bids. These
minimax rates reveal how the interplay between transparency and the nature of
the environment affects how fast one can learn to bid optimally in first-price
auctions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cesa_Bianchi_N/0/1/0/all/0/1"&gt;Nicol&amp;#xf2; Cesa-Bianchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cesari_T/0/1/0/all/0/1"&gt;Tommaso Cesari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colomboni_R/0/1/0/all/0/1"&gt;Roberto Colomboni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fusco_F/0/1/0/all/0/1"&gt;Federico Fusco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leonardi_S/0/1/0/all/0/1"&gt;Stefano Leonardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Limitations of Information-Theoretic Generalization Bounds for Gradient Descent Methods in Stochastic Convex Optimization. (arXiv:2212.13556v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.13556</id>
        <link href="http://arxiv.org/abs/2212.13556"/>
        <updated>2023-07-19T01:53:28.471Z</updated>
        <summary type="html"><![CDATA[To date, no "information-theoretic" frameworks for reasoning about
generalization error have been shown to establish minimax rates for gradient
descent in the setting of stochastic convex optimization. In this work, we
consider the prospect of establishing such rates via several existing
information-theoretic frameworks: input-output mutual information bounds,
conditional mutual information bounds and variants, PAC-Bayes bounds, and
recent conditional variants thereof. We prove that none of these bounds are
able to establish minimax rates. We then consider a common tactic employed in
studying gradient methods, whereby the final iterate is corrupted by Gaussian
noise, producing a noisy "surrogate" algorithm. We prove that minimax rates
cannot be established via the analysis of such surrogates. Our results suggest
that new ideas are required to analyze gradient descent using
information-theoretic techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haghifam_M/0/1/0/all/0/1"&gt;Mahdi Haghifam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Galvez_B/0/1/0/all/0/1"&gt;Borja Rodr&amp;#xed;guez-G&amp;#xe1;lvez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thobaben_R/0/1/0/all/0/1"&gt;Ragnar Thobaben&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skoglund_M/0/1/0/all/0/1"&gt;Mikael Skoglund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1"&gt;Daniel M. Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dziugaite_G/0/1/0/all/0/1"&gt;Gintare Karolina Dziugaite&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Operator Guidance Informed by AI-Augmented Simulations. (arXiv:2307.08810v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2307.08810</id>
        <link href="http://arxiv.org/abs/2307.08810"/>
        <updated>2023-07-19T01:53:28.469Z</updated>
        <summary type="html"><![CDATA[This paper will present a multi-fidelity, data-adaptive approach with a Long
Short-Term Memory (LSTM) neural network to estimate ship response statistics in
bimodal, bidirectional seas. The study will employ a fast low-fidelity,
volume-based tool SimpleCode and a higher-fidelity tool known as the Large
Amplitude Motion Program (LAMP). SimpleCode and LAMP data were generated by
common bi-modal, bi-directional sea conditions in the North Atlantic as
training data. After training an LSTM network with LAMP ship motion response
data, a sample route was traversed and randomly sampled historical weather was
input into SimpleCode and the LSTM network, and compared against the higher
fidelity results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Edwards_S/0/1/0/all/0/1"&gt;Samuel J. Edwards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_M/0/1/0/all/0/1"&gt;Michael Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly Detection with Selective Dictionary Learning. (arXiv:2307.08807v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08807</id>
        <link href="http://arxiv.org/abs/2307.08807"/>
        <updated>2023-07-19T01:53:28.467Z</updated>
        <summary type="html"><![CDATA[In this paper we present new methods of anomaly detection based on Dictionary
Learning (DL) and Kernel Dictionary Learning (KDL). The main contribution
consists in the adaption of known DL and KDL algorithms in the form of
unsupervised methods, used for outlier detection. We propose a reduced kernel
version (RKDL), which is useful for problems with large data sets, due to the
large kernel matrix. We also improve the DL and RKDL methods by the use of a
random selection of signals, which aims to eliminate the outliers from the
training procedure. All our algorithms are introduced in an anomaly detection
toolbox and are compared to standard benchmark results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ilie_Ablachim_D/0/1/0/all/0/1"&gt;Denis C. Ilie-Ablachim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dumitrescu_B/0/1/0/all/0/1"&gt;Bogdan Dumitrescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accuracy versus time frontiers of semi-supervised and self-supervised learning on medical images. (arXiv:2307.08919v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.08919</id>
        <link href="http://arxiv.org/abs/2307.08919"/>
        <updated>2023-07-19T01:53:28.466Z</updated>
        <summary type="html"><![CDATA[For many applications of classifiers to medical images, a trustworthy label
for each image can be difficult or expensive to obtain. In contrast, images
without labels are more readily available. Two major research directions both
promise that additional unlabeled data can improve classifier performance:
self-supervised learning pretrains useful representations on unlabeled data
only, then fine-tunes a classifier on these representations via the labeled
set; semi-supervised learning directly trains a classifier on labeled and
unlabeled data simultaneously. Recent methods from both directions have claimed
significant gains on non-medical tasks, but do not systematically assess
medical images and mostly compare only to methods in the same direction. This
study contributes a carefully-designed benchmark to help answer a
practitioner's key question: given a small labeled dataset and a limited budget
of hours to spend on training, what gains from additional unlabeled images are
possible and which methods best achieve them? Unlike previous benchmarks, ours
uses realistic-sized validation sets to select hyperparameters, assesses
runtime-performance tradeoffs, and bridges two research fields. By comparing 6
semi-supervised methods and 5 self-supervised methods to strong labeled-only
baselines on 3 medical datasets with 30-1000 labels per class, we offer
insights to resource-constrained, results-focused practitioners: MixMatch,
SimCLR, and BYOL represent strong choices that were not surpassed by more
recent methods. After much effort selecting hyperparameters on one dataset, we
publish settings that enable strong methods to perform well on new medical
tasks within a few hours, with further search over dozens of hours delivering
modest additional gains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhe Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1"&gt;Ruijie Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aeron_S/0/1/0/all/0/1"&gt;Shuchin Aeron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1"&gt;Michael C. Hughes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating unsupervised disentangled representation learning for genomic discovery and disease risk prediction. (arXiv:2307.08893v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08893</id>
        <link href="http://arxiv.org/abs/2307.08893"/>
        <updated>2023-07-19T01:53:28.465Z</updated>
        <summary type="html"><![CDATA[High-dimensional clinical data have become invaluable resources for genetic
studies, due to their accessibility in biobank-scale datasets and the
development of high performance modeling techniques especially using deep
learning. Recent work has shown that low dimensional embeddings of these
clinical data learned by variational autoencoders (VAE) can be used for
genome-wide association studies and polygenic risk prediction. In this work, we
consider multiple unsupervised learning methods for learning disentangled
representations, namely autoencoders, VAE, beta-VAE, and FactorVAE, in the
context of genetic association studies. Using spirograms from UK Biobank as a
running example, we observed improvements in the number of genome-wide
significant loci, heritability, and performance of polygenic risk scores for
asthma and chronic obstructive pulmonary disease by using FactorVAE or
beta-VAE, compared to standard VAE or non-variational autoencoders. FactorVAEs
performed effectively across multiple values of the regularization
hyperparameter, while beta-VAEs were much more sensitive to the hyperparameter
values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yun_T/0/1/0/all/0/1"&gt;Taedong Yun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Limitations of Information-Theoretic Generalization Bounds for Gradient Descent Methods in Stochastic Convex Optimization. (arXiv:2212.13556v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.13556</id>
        <link href="http://arxiv.org/abs/2212.13556"/>
        <updated>2023-07-19T01:53:28.464Z</updated>
        <summary type="html"><![CDATA[To date, no "information-theoretic" frameworks for reasoning about
generalization error have been shown to establish minimax rates for gradient
descent in the setting of stochastic convex optimization. In this work, we
consider the prospect of establishing such rates via several existing
information-theoretic frameworks: input-output mutual information bounds,
conditional mutual information bounds and variants, PAC-Bayes bounds, and
recent conditional variants thereof. We prove that none of these bounds are
able to establish minimax rates. We then consider a common tactic employed in
studying gradient methods, whereby the final iterate is corrupted by Gaussian
noise, producing a noisy "surrogate" algorithm. We prove that minimax rates
cannot be established via the analysis of such surrogates. Our results suggest
that new ideas are required to analyze gradient descent using
information-theoretic techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haghifam_M/0/1/0/all/0/1"&gt;Mahdi Haghifam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Galvez_B/0/1/0/all/0/1"&gt;Borja Rodr&amp;#xed;guez-G&amp;#xe1;lvez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thobaben_R/0/1/0/all/0/1"&gt;Ragnar Thobaben&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skoglund_M/0/1/0/all/0/1"&gt;Mikael Skoglund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1"&gt;Daniel M. Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dziugaite_G/0/1/0/all/0/1"&gt;Gintare Karolina Dziugaite&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-Conditional Navigation with a Learning-Based Terrain- and Robot-Aware Dynamics Model. (arXiv:2307.09206v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2307.09206</id>
        <link href="http://arxiv.org/abs/2307.09206"/>
        <updated>2023-07-19T01:53:28.463Z</updated>
        <summary type="html"><![CDATA[In autonomous navigation settings, several quantities can be subject to
variations. Terrain properties such as friction coefficients may vary over time
depending on the location of the robot. Also, the dynamics of the robot may
change due to, e.g., different payloads, changing the system's mass, or wear
and tear, changing actuator gains or joint friction. An autonomous agent should
thus be able to adapt to such variations. In this paper, we develop a novel
probabilistic, terrain- and robot-aware forward dynamics model, termed TRADYN,
which is able to adapt to the above-mentioned variations. It builds on recent
advances in meta-learning forward dynamics models based on Neural Processes. We
evaluate our method in a simulated 2D navigation setting with a unicycle-like
robot and different terrain layouts with spatially varying friction
coefficients. In our experiments, the proposed model exhibits lower prediction
error for the task of long-horizon trajectory prediction, compared to
non-adaptive ablation models. We also evaluate our model on the downstream task
of navigation planning, which demonstrates improved performance in planning
control-efficient paths by taking robot and terrain properties into account.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guttikonda_S/0/1/0/all/0/1"&gt;Suresh Guttikonda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Achterhold_J/0/1/0/all/0/1"&gt;Jan Achterhold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haolong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boedecker_J/0/1/0/all/0/1"&gt;Joschka Boedecker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stueckler_J/0/1/0/all/0/1"&gt;Joerg Stueckler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometric Ultrasound Localization Microscopy. (arXiv:2306.15548v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.15548</id>
        <link href="http://arxiv.org/abs/2306.15548"/>
        <updated>2023-07-19T01:53:28.463Z</updated>
        <summary type="html"><![CDATA[Contrast-Enhanced Ultra-Sound (CEUS) has become a viable method for
non-invasive, dynamic visualization in medical diagnostics, yet Ultrasound
Localization Microscopy (ULM) has enabled a revolutionary breakthrough by
offering ten times higher resolution. To date, Delay-And-Sum (DAS) beamformers
are used to render ULM frames, ultimately determining the image resolution
capability. To take full advantage of ULM, this study questions whether
beamforming is the most effective processing step for ULM, suggesting an
alternative approach that relies solely on Time-Difference-of-Arrival (TDoA)
information. To this end, a novel geometric framework for micro bubble
localization via ellipse intersections is proposed to overcome existing
beamforming limitations. We present a benchmark comparison based on a public
dataset for which our geometric ULM outperforms existing baseline methods in
terms of accuracy and robustness while only utilizing a portion of the
available transducer data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hahne_C/0/1/0/all/0/1"&gt;Christopher Hahne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1"&gt;Raphael Sznitman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Internally Rewarded Reinforcement Learning. (arXiv:2302.00270v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.00270</id>
        <link href="http://arxiv.org/abs/2302.00270"/>
        <updated>2023-07-19T01:53:28.462Z</updated>
        <summary type="html"><![CDATA[We study a class of reinforcement learning problems where the reward signals
for policy learning are generated by a discriminator that is dependent on and
jointly optimized with the policy. This interdependence between the policy and
the discriminator leads to an unstable learning process because reward signals
from an immature discriminator are noisy and impede policy learning, and
conversely, an under-optimized policy impedes discriminator learning. We call
this learning setting \textit{Internally Rewarded Reinforcement Learning}
(IRRL) as the reward is not provided directly by the environment but
\textit{internally} by the discriminator. In this paper, we formally formulate
IRRL and present a class of problems that belong to IRRL. We theoretically
derive and empirically analyze the effect of the reward function in IRRL and
based on these analyses propose the clipped linear reward function.
Experimental results show that the proposed reward function can consistently
stabilize the training process by reducing the impact of reward noise, which
leads to faster convergence and higher performance compared with baselines in
diverse tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mengdi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xufeng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jae Hee Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1"&gt;Cornelius Weber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1"&gt;Stefan Wermter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best-of-three-worlds Analysis for Linear Bandits with Follow-the-regularized-leader Algorithm. (arXiv:2303.06825v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.06825</id>
        <link href="http://arxiv.org/abs/2303.06825"/>
        <updated>2023-07-19T01:53:28.461Z</updated>
        <summary type="html"><![CDATA[The linear bandit problem has been studied for many years in both stochastic
and adversarial settings. Designing an algorithm that can optimize the
environment without knowing the loss type attracts lots of interest.
\citet{LeeLWZ021} propose an algorithm that actively detects the loss type and
then switches between different algorithms specially designed for specific
settings. However, such an approach requires meticulous designs to perform well
in all environments. Follow-the-regularized-leader (FTRL) is another type of
popular algorithm that can adapt to different environments. This algorithm is
of simple design and the regret bounds are shown to be optimal in traditional
multi-armed bandit problems compared with the detect-switch type. Designing an
FTRL-type algorithm for linear bandits is an important question that has been
open for a long time. In this paper, we prove that the FTRL algorithm with a
negative entropy regularizer can achieve the best-of-three-world results for
the linear bandit problem. Our regret bounds achieve the same or nearly the
same order as the previous detect-switch type algorithm but with a much simpler
algorithmic design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1"&gt;Fang Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1"&gt;Canzhe Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuai Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Application of Conditional Normalizing Flows: Stellar Age Inference with Gyrochronology. (arXiv:2307.08753v1 [astro-ph.SR])]]></title>
        <id>http://arxiv.org/abs/2307.08753</id>
        <link href="http://arxiv.org/abs/2307.08753"/>
        <updated>2023-07-19T01:53:28.459Z</updated>
        <summary type="html"><![CDATA[Stellar ages are critical building blocks of evolutionary models, but
challenging to measure for low mass main sequence stars. An unexplored solution
in this regime is the application of probabilistic machine learning methods to
gyrochronology, a stellar dating technique that is uniquely well suited for
these stars. While accurate analytical gyrochronological models have proven
challenging to develop, here we apply conditional normalizing flows to
photometric data from open star clusters, and demonstrate that a data-driven
approach can constrain gyrochronological ages with a precision comparable to
other standard techniques. We evaluate the flow results in the context of a
Bayesian framework, and show that our inferred ages recover literature values
well. This work demonstrates the potential of a probabilistic data-driven
solution to widen the applicability of gyrochronological stellar dating.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Van_Lane_P/0/1/0/all/0/1"&gt;Phil Van-Lane&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Speagle_J/0/1/0/all/0/1"&gt;Joshua S. Speagle&lt;/a&gt; (2 and 1 and 3 and 4), &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Douglas_S/0/1/0/all/0/1"&gt;Stephanie Douglas&lt;/a&gt; (5) ((1) Department of Astronomy &amp; Astrophysics, University of Toronto, Canada, (2) Department of Statistical Sciences, University of Toronto, Canada, (3) Dunlap Institute of Astronomy &amp; Astrophysics, University of Toronto, Canada, (4) Data Sciences Institute, University of Toronto, Canada, (5) Department of Physics, Lafayette College, United States)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[K-Tensors: Clustering Positive Semi-Definite Matrices. (arXiv:2306.06534v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.06534</id>
        <link href="http://arxiv.org/abs/2306.06534"/>
        <updated>2023-07-19T01:53:28.458Z</updated>
        <summary type="html"><![CDATA[This paper introduces a novel self-consistency clustering algorithm
($K$-Tensors) designed for {partitioning a distribution of}
positive-semidefinite matrices based on their eigenstructures. As positive
semi-definite matrices can be represented as ellipsoids in $\mathbb R^p$, $p
\ge 2$, it is critical to maintain their structural information to perform
effective clustering. However, traditional clustering algorithms {applied to
matrices} often {involve vectorization of} the matrices, resulting in a loss of
essential structural information. To address this issue, we propose a distance
metric {for clustering} that is specifically based on the structural
information of positive semi-definite matrices. This distance metric enables
the clustering algorithm to consider the differences between positive
semi-definite matrices and their projections onto {a} common space spanned by
\thadJulyTen{orthonormal vectors defined from a set of} positive semi-definite
matrices. This innovative approach to clustering positive semi-definite
matrices has broad applications in several domains including financial and
biomedical research, such as analyzing functional connectivity data. By
maintaining the structural information of positive semi-definite matrices, our
proposed algorithm promises to cluster the positive semi-definite matrices in a
more meaningful way, thereby facilitating deeper insights into the underlying
data in various applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hanchao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarpey_T/0/1/0/all/0/1"&gt;Thaddeus Tarpey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Representation of the Magnetic Field Topology in High-Fidelity Plasma Simulations for Machine Learning Applications. (arXiv:2307.09469v1 [physics.plasm-ph])]]></title>
        <id>http://arxiv.org/abs/2307.09469</id>
        <link href="http://arxiv.org/abs/2307.09469"/>
        <updated>2023-07-19T01:53:28.455Z</updated>
        <summary type="html"><![CDATA[Topological analysis of the magnetic field in simulated plasmas allows the
study of various physical phenomena in a wide range of settings. One such
application is magnetic reconnection, a phenomenon related to the dynamics of
the magnetic field topology, which is difficult to detect and characterize in
three dimensions. We propose a scalable pipeline for topological data analysis
and spatiotemporal graph representation of three-dimensional magnetic vector
fields. We demonstrate our methods on simulations of the Earth's magnetosphere
produced by Vlasiator, a supercomputer-scale Vlasov theory-based simulation for
near-Earth space. The purpose of this work is to challenge the machine learning
community to explore graph-based machine learning approaches to address a
largely open scientific problem with wide-ranging potential impact.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Bouri_I/0/1/0/all/0/1"&gt;Ioanna Bouri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Franssila_F/0/1/0/all/0/1"&gt;Fanni Franssila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Alho_M/0/1/0/all/0/1"&gt;Markku Alho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Cozzani_G/0/1/0/all/0/1"&gt;Giulia Cozzani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zaitsev_I/0/1/0/all/0/1"&gt;Ivan Zaitsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Palmroth_M/0/1/0/all/0/1"&gt;Minna Palmroth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Roos_T/0/1/0/all/0/1"&gt;Teemu Roos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Performance Gaps of Artificial Intelligence Models Screening Mammography -- Towards Fair and Interpretable Models. (arXiv:2305.04422v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.04422</id>
        <link href="http://arxiv.org/abs/2305.04422"/>
        <updated>2023-07-19T01:53:28.452Z</updated>
        <summary type="html"><![CDATA[Even though deep learning models for abnormality classification can perform
well in screening mammography, the demographic and imaging characteristics
associated with increased risk of failure for abnormality classification in
screening mammograms remain unclear. This retrospective study used data from
the Emory BrEast Imaging Dataset (EMBED) including mammograms from 115,931
patients imaged at Emory University Healthcare between 2013 to 2020. Clinical
and imaging data includes Breast Imaging Reporting and Data System (BI-RADS)
assessment, region of interest coordinates for abnormalities, imaging features,
pathologic outcomes, and patient demographics. Deep learning models including
InceptionV3, VGG16, ResNet50V2, and ResNet152V2 were developed to distinguish
between patches of abnormal tissue and randomly selected patches of normal
tissue from the screening mammograms. The distributions of the training,
validation and test sets are 29,144 (55.6%) patches of 10,678 (54.2%) patients,
9,910 (18.9%) patches of 3,609 (18.3%) patients, and 13,390 (25.5%) patches of
5,404 (27.5%) patients. We assessed model performance overall and within
subgroups defined by age, race, pathologic outcome, and imaging characteristics
to evaluate reasons for misclassifications. On the test set, a ResNet152V2
model trained to classify normal versus abnormal tissue patches achieved an
accuracy of 92.6% (95%CI=92.0-93.2%), and area under the receiver operative
characteristics curve 0.975 (95%CI=0.972-0.978). Imaging characteristics
associated with higher misclassifications of images include higher tissue
densities (risk ratio [RR]=1.649; p=.010, BI-RADS density C and RR=2.026;
p=.003, BI-RADS density D), and presence of architectural distortion (RR=1.026;
p<.001). Small but statistically significant differences in performance were
observed by age, race, pathologic outcome, and other imaging features (p<.001).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Linglin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Brown_Mulry_B/0/1/0/all/0/1"&gt;Beatrice Brown-Mulry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nalla_V/0/1/0/all/0/1"&gt;Vineela Nalla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hwang_I/0/1/0/all/0/1"&gt;InChan Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gichoya_J/0/1/0/all/0/1"&gt;Judy Wawira Gichoya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gastounioti_A/0/1/0/all/0/1"&gt;Aimilia Gastounioti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Banerjee_I/0/1/0/all/0/1"&gt;Imon Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Seyyed_Kalantari_L/0/1/0/all/0/1"&gt;Laleh Seyyed-Kalantari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Woo_M/0/1/0/all/0/1"&gt;MinJae Woo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Trivedi_H/0/1/0/all/0/1"&gt;Hari Trivedi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conformal Prediction Bands for Two-Dimensional Functional Time Series. (arXiv:2207.13656v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2207.13656</id>
        <link href="http://arxiv.org/abs/2207.13656"/>
        <updated>2023-07-19T01:53:28.449Z</updated>
        <summary type="html"><![CDATA[Time evolving surfaces can be modeled as two-dimensional Functional time
series, exploiting the tools of Functional data analysis. Leveraging this
approach, a forecasting framework for such complex data is developed. The main
focus revolves around Conformal Prediction, a versatile nonparametric paradigm
used to quantify uncertainty in prediction problems. Building upon recent
variations of Conformal Prediction for Functional time series, a probabilistic
forecasting scheme for two-dimensional functional time series is presented,
while providing an extension of Functional Autoregressive Processes of order
one to this setting. Estimation techniques for the latter process are
introduced and their performance are compared in terms of the resulting
prediction regions. Finally, the proposed forecasting procedure and the
uncertainty quantification technique are applied to a real dataset, collecting
daily observations of Sea Level Anomalies of the Black Sea]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ajroldi_N/0/1/0/all/0/1"&gt;Niccol&amp;#xf2; Ajroldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Diquigiovanni_J/0/1/0/all/0/1"&gt;Jacopo Diquigiovanni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fontana_M/0/1/0/all/0/1"&gt;Matteo Fontana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vantini_S/0/1/0/all/0/1"&gt;Simone Vantini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Image Segmentation with U-Net. (arXiv:2307.09067v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2307.09067</id>
        <link href="http://arxiv.org/abs/2307.09067"/>
        <updated>2023-07-19T01:53:28.448Z</updated>
        <summary type="html"><![CDATA[Fetal head segmentation is a crucial step in measuring the fetal head
circumference (HC) during gestation, an important biometric in obstetrics for
monitoring fetal growth. However, manual biometry generation is time-consuming
and results in inconsistent accuracy. To address this issue, convolutional
neural network (CNN) models have been utilized to improve the efficiency of
medical biometry. But training a CNN network from scratch is a challenging
task, we proposed a Transfer Learning (TL) method. Our approach involves
fine-tuning (FT) a U-Net network with a lightweight MobileNet as the encoder to
perform segmentation on a set of fetal head ultrasound (US) images with limited
effort. This method addresses the challenges associated with training a CNN
network from scratch. It suggests that our proposed FT strategy yields
segmentation performance that is comparable when trained with a reduced number
of parameters by 85.8%. And our proposed FT strategy outperforms other
strategies with smaller trainable parameter sizes below 4.4 million. Thus, we
contend that it can serve as a dependable FT approach for reducing the size of
models in medical image analysis. Our key findings highlight the importance of
the balance between model performance and size in developing Artificial
Intelligence (AI) applications by TL methods. Code is available at
https://github.com/13204942/FT_Methods_for_Fetal_Head_Segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fangyijie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Silvestre_G/0/1/0/all/0/1"&gt;Gu&amp;#xe9;nol&amp;#xe9; Silvestre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Curran_K/0/1/0/all/0/1"&gt;Kathleen M. Curran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach. (arXiv:2307.08859v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08859</id>
        <link href="http://arxiv.org/abs/2307.08859"/>
        <updated>2023-07-19T01:53:28.407Z</updated>
        <summary type="html"><![CDATA[A curriculum is a planned sequence of learning materials and an effective one
can make learning efficient and effective for both humans and machines. Recent
studies developed effective data-driven curriculum learning approaches for
training graph neural networks in language applications. However, existing
curriculum learning approaches often employ a single criterion of difficulty in
their training paradigms. In this paper, we propose a new perspective on
curriculum learning by introducing a novel approach that builds on graph
complexity formalisms (as difficulty criteria) and model competence during
training. The model consists of a scheduling scheme which derives effective
curricula by accounting for different views of sample difficulty and model
competence during training. The proposed solution advances existing research in
curriculum learning for graph neural networks with the ability to incorporate a
fine-grained spectrum of graph difficulty criteria in their training paradigms.
Experimental results on real-world link prediction and node classification
tasks illustrate the effectiveness of the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vakil_N/0/1/0/all/0/1"&gt;Nidhi Vakil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amiri_H/0/1/0/all/0/1"&gt;Hadi Amiri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Characterization of partial wetting by CMAS droplets using multiphase many-body dissipative particle dynamics and data-driven discovery based on PINNs. (arXiv:2307.09142v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2307.09142</id>
        <link href="http://arxiv.org/abs/2307.09142"/>
        <updated>2023-07-19T01:53:28.406Z</updated>
        <summary type="html"><![CDATA[The molten sand, a mixture of calcia, magnesia, alumina, and silicate, known
as CMAS, is characterized by its high viscosity, density, and surface tension.
The unique properties of CMAS make it a challenging material to deal with in
high-temperature applications, requiring innovative solutions and materials to
prevent its buildup and damage to critical equipment. Here, we use multiphase
many-body dissipative particle dynamics (mDPD) simulations to study the wetting
dynamics of highly viscous molten CMAS droplets. The simulations are performed
in three dimensions, with varying initial droplet sizes and equilibrium contact
angles. We propose a coarse parametric ordinary differential equation (ODE)
that captures the spreading radius behavior of the CMAS droplets. The ODE
parameters are then identified based on the Physics-Informed Neural Network
(PINN) framework. Subsequently, the closed form dependency of parameter values
found by PINN on the initial radii and contact angles are given using symbolic
regression. Finally, we employ Bayesian PINNs (B-PINNs) to assess and quantify
the uncertainty associated with the discovered parameters. In brief, this study
provides insight into spreading dynamics of CMAS droplets by fusing simple
parametric ODE modeling and state-of-the-art machine learning techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Kiyani_E/0/1/0/all/0/1"&gt;Elham Kiyani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kooshkbaghi_M/0/1/0/all/0/1"&gt;Mahdi Kooshkbaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Shukla_K/0/1/0/all/0/1"&gt;Khemraj Shukla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Koneru_R/0/1/0/all/0/1"&gt;Rahul Babu Koneru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Bravo_L/0/1/0/all/0/1"&gt;Luis Bravo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ghoshal_A/0/1/0/all/0/1"&gt;Anindya Ghoshal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Karniadakis_G/0/1/0/all/0/1"&gt;George Em Karniadakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Karttunen_M/0/1/0/all/0/1"&gt;Mikko Karttunen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Laws for Imitation Learning in NetHack. (arXiv:2307.09423v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09423</id>
        <link href="http://arxiv.org/abs/2307.09423"/>
        <updated>2023-07-19T01:53:28.404Z</updated>
        <summary type="html"><![CDATA[Imitation Learning (IL) is one of the most widely used methods in machine
learning. Yet, while powerful, many works find it is often not able to fully
recover the underlying expert behavior. However, none of these works deeply
investigate the role of scaling up the model and data size. Inspired by recent
work in Natural Language Processing (NLP) where "scaling up" has resulted in
increasingly more capable LLMs, we investigate whether carefully scaling up
model and data size can bring similar improvements in the imitation learning
setting. To demonstrate our findings, we focus on the game of NetHack, a
challenging environment featuring procedural generation, stochasticity,
long-term dependencies, and partial observability. We find IL loss and mean
return scale smoothly with the compute budget and are strongly correlated,
resulting in power laws for training compute-optimal IL agents with respect to
model size and number of samples. We forecast and train several NetHack agents
with IL and find they outperform prior state-of-the-art by at least 2x in all
settings. Our work both demonstrates the scaling behavior of imitation learning
in a challenging domain, as well as the viability of scaling up current
approaches for increasingly capable agents in NetHack, a game that remains
elusively hard for current AI systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tuyls_J/0/1/0/all/0/1"&gt;Jens Tuyls&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madeka_D/0/1/0/all/0/1"&gt;Dhruv Madeka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torkkola_K/0/1/0/all/0/1"&gt;Kari Torkkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1"&gt;Dean Foster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1"&gt;Karthik Narasimhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1"&gt;Sham Kakade&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quality Assessment of Photoplethysmography Signals For Cardiovascular Biomarkers Monitoring Using Wearable Devices. (arXiv:2307.08766v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08766</id>
        <link href="http://arxiv.org/abs/2307.08766"/>
        <updated>2023-07-19T01:53:28.397Z</updated>
        <summary type="html"><![CDATA[Photoplethysmography (PPG) is a non-invasive technology that measures changes
in blood volume in the microvascular bed of tissue. It is commonly used in
medical devices such as pulse oximeters and wrist worn heart rate monitors to
monitor cardiovascular hemodynamics. PPG allows for the assessment of
parameters (e.g., heart rate, pulse waveform, and peripheral perfusion) that
can indicate conditions such as vasoconstriction or vasodilation, and provides
information about microvascular blood flow, making it a valuable tool for
monitoring cardiovascular health. However, PPG is subject to a number of
sources of variations that can impact its accuracy and reliability, especially
when using a wearable device for continuous monitoring, such as motion
artifacts, skin pigmentation, and vasomotion. In this study, we extracted 27
statistical features from the PPG signal for training machine-learning models
based on gradient boosting (XGBoost and CatBoost) and Random Forest (RF)
algorithms to assess quality of PPG signals that were labeled as good or poor
quality. We used the PPG time series from a publicly available dataset and
evaluated the algorithm s performance using Sensitivity (Se), Positive
Predicted Value (PPV), and F1-score (F1) metrics. Our model achieved Se, PPV,
and F1-score of 94.4, 95.6, and 95.0 for XGBoost, 94.7, 95.9, and 95.3 for
CatBoost, and 93.7, 91.3 and 92.5 for RF, respectively. Our findings are
comparable to state-of-the-art reported in the literature but using a much
simpler model, indicating that ML models are promising for developing remote,
non-invasive, and continuous measurement devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dias_F/0/1/0/all/0/1"&gt;Felipe M. Dias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toledo_M/0/1/0/all/0/1"&gt;Marcelo A. F. Toledo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cardenas_D/0/1/0/all/0/1"&gt;Diego A. C. Cardenas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Almeida_D/0/1/0/all/0/1"&gt;Douglas A. Almeida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_F/0/1/0/all/0/1"&gt;Filipe A. C. Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_E/0/1/0/all/0/1"&gt;Estela Ribeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krieger_J/0/1/0/all/0/1"&gt;Jose E. Krieger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gutierrez_M/0/1/0/all/0/1"&gt;Marco A. Gutierrez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Admissible Shift-Consistent Method for Recommender Systems. (arXiv:2307.08857v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2307.08857</id>
        <link href="http://arxiv.org/abs/2307.08857"/>
        <updated>2023-07-19T01:53:28.394Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a new constraint, called shift-consistency, for
solving matrix/tensor completion problems in the context of recommender
systems. Our method provably guarantees several key mathematical properties:
(1) satisfies a recently established admissibility criterion for recommender
systems; (2) satisfies a definition of fairness that eliminates a specific
class of potential opportunities for users to maliciously influence system
recommendations; and (3) offers robustness by exploiting provable uniqueness of
missing-value imputation. We provide a rigorous mathematical description of the
method, including its generalization from matrix to tensor form to permit
representation and exploitation of complex structural relationships among sets
of user and product attributes. We argue that our analysis suggests a
structured means for defining latent-space projections that can permit provable
performance properties to be established for machine learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Tung Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uhlmann_J/0/1/0/all/0/1"&gt;Jeffrey Uhlmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overthinking the Truth: Understanding how Language Models Process False Demonstrations. (arXiv:2307.09476v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09476</id>
        <link href="http://arxiv.org/abs/2307.09476"/>
        <updated>2023-07-19T01:53:28.388Z</updated>
        <summary type="html"><![CDATA[Modern language models can imitate complex patterns through few-shot
learning, enabling them to complete challenging tasks without fine-tuning.
However, imitation can also lead models to reproduce inaccuracies or harmful
content if present in the context. We study harmful imitation through the lens
of a model's internal representations, and identify two related phenomena:
overthinking and false induction heads. The first phenomenon, overthinking,
appears when we decode predictions from intermediate layers, given correct vs.
incorrect few-shot demonstrations. At early layers, both demonstrations induce
similar model behavior, but the behavior diverges sharply at some "critical
layer", after which the accuracy given incorrect demonstrations progressively
decreases. The second phenomenon, false induction heads, are a possible
mechanistic cause of overthinking: these are heads in late layers that attend
to and copy false information from previous demonstrations, and whose ablation
reduces overthinking. Beyond scientific understanding, our results suggest that
studying intermediate model computations could be a promising avenue for
understanding and guarding against harmful model behaviors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Halawi_D/0/1/0/all/0/1"&gt;Danny Halawi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denain_J/0/1/0/all/0/1"&gt;Jean-Stanislas Denain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1"&gt;Jacob Steinhardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Certifying the Fairness of KNN in the Presence of Dataset Bias. (arXiv:2307.08722v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08722</id>
        <link href="http://arxiv.org/abs/2307.08722"/>
        <updated>2023-07-19T01:53:28.384Z</updated>
        <summary type="html"><![CDATA[We propose a method for certifying the fairness of the classification result
of a widely used supervised learning algorithm, the k-nearest neighbors (KNN),
under the assumption that the training data may have historical bias caused by
systematic mislabeling of samples from a protected minority group. To the best
of our knowledge, this is the first certification method for KNN based on three
variants of the fairness definition: individual fairness, $\epsilon$-fairness,
and label-flipping fairness. We first define the fairness certification problem
for KNN and then propose sound approximations of the complex arithmetic
computations used in the state-of-the-art KNN algorithm. This is meant to lift
the computation results from the concrete domain to an abstract domain, to
reduce the computational cost. We show effectiveness of this abstract
interpretation based technique through experimental evaluation on six datasets
widely used in the fairness research literature. We also show that the method
is accurate enough to obtain fairness certifications for a large number of test
inputs, despite the presence of historical bias in the datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yannan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chao Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information. (arXiv:2307.08964v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08964</id>
        <link href="http://arxiv.org/abs/2307.08964"/>
        <updated>2023-07-19T01:53:28.383Z</updated>
        <summary type="html"><![CDATA[Recent works in learning-integrated optimization have shown promise in
settings where the optimization problem is only partially observed or where
general-purpose optimizers perform poorly without expert tuning. By learning an
optimizer $\mathbf{g}$ to tackle these challenging problems with $f$ as the
objective, the optimization process can be substantially accelerated by
leveraging past experience. The optimizer can be trained with supervision from
known optimal solutions or implicitly by optimizing the compound function
$f\circ \mathbf{g}$. The implicit approach may not require optimal solutions as
labels and is capable of handling problem uncertainty; however, it is slow to
train and deploy due to frequent calls to optimizer $\mathbf{g}$ during both
training and testing. The training is further challenged by sparse gradients of
$\mathbf{g}$, especially for combinatorial solvers. To address these
challenges, we propose using a smooth and learnable Landscape Surrogate $M$ as
a replacement for $f\circ \mathbf{g}$. This surrogate, learnable by neural
networks, can be computed faster than the solver $\mathbf{g}$, provides dense
and smooth gradients during training, can generalize to unseen optimization
problems, and is efficiently learned via alternating optimization. We test our
approach on both synthetic problems, including shortest path and
multidimensional knapsack, and real-world problems such as portfolio
optimization, achieving comparable or superior objective values compared to
state-of-the-art baselines while reducing the number of calls to $\mathbf{g}$.
Notably, our approach outperforms existing methods for computationally
expensive high-dimensional problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zharmagambetov_A/0/1/0/all/0/1"&gt;Arman Zharmagambetov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amos_B/0/1/0/all/0/1"&gt;Brandon Amos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferber_A/0/1/0/all/0/1"&gt;Aaron Ferber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Taoan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1"&gt;Bistra Dilkina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuandong Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best-of-three-worlds Analysis for Linear Bandits with Follow-the-regularized-leader Algorithm. (arXiv:2303.06825v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.06825</id>
        <link href="http://arxiv.org/abs/2303.06825"/>
        <updated>2023-07-19T01:53:28.382Z</updated>
        <summary type="html"><![CDATA[The linear bandit problem has been studied for many years in both stochastic
and adversarial settings. Designing an algorithm that can optimize the
environment without knowing the loss type attracts lots of interest.
\citet{LeeLWZ021} propose an algorithm that actively detects the loss type and
then switches between different algorithms specially designed for specific
settings. However, such an approach requires meticulous designs to perform well
in all environments. Follow-the-regularized-leader (FTRL) is another type of
popular algorithm that can adapt to different environments. This algorithm is
of simple design and the regret bounds are shown to be optimal in traditional
multi-armed bandit problems compared with the detect-switch type. Designing an
FTRL-type algorithm for linear bandits is an important question that has been
open for a long time. In this paper, we prove that the FTRL algorithm with a
negative entropy regularizer can achieve the best-of-three-world results for
the linear bandit problem. Our regret bounds achieve the same or nearly the
same order as the previous detect-switch type algorithm but with a much simpler
algorithmic design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1"&gt;Fang Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1"&gt;Canzhe Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuai Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-stationary Delayed Combinatorial Semi-Bandit with Causally Related Rewards. (arXiv:2307.09093v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09093</id>
        <link href="http://arxiv.org/abs/2307.09093"/>
        <updated>2023-07-19T01:53:28.381Z</updated>
        <summary type="html"><![CDATA[Sequential decision-making under uncertainty is often associated with long
feedback delays. Such delays degrade the performance of the learning agent in
identifying a subset of arms with the optimal collective reward in the long
run. This problem becomes significantly challenging in a non-stationary
environment with structural dependencies amongst the reward distributions
associated with the arms. Therefore, besides adapting to delays and
environmental changes, learning the causal relations alleviates the adverse
effects of feedback delay on the decision-making process. We formalize the
described setting as a non-stationary and delayed combinatorial semi-bandit
problem with causally related rewards. We model the causal relations by a
directed graph in a stationary structural equation model. The agent maximizes
the long-term average payoff, defined as a linear function of the base arms'
rewards. We develop a policy that learns the structural dependencies from
delayed feedback and utilizes that to optimize the decision-making while
adapting to drifts. We prove a regret bound for the performance of the proposed
algorithm. Besides, we evaluate our method via numerical analysis using
synthetic and real-world datasets to detect the regions that contribute the
most to the spread of Covid-19 in Italy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghoorchian_S/0/1/0/all/0/1"&gt;Saeed Ghoorchian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maghsudi_S/0/1/0/all/0/1"&gt;Setareh Maghsudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Repellent Random Walks on General Graphs -- Achieving Minimal Sampling Variance via Nonlinear Markov Chains. (arXiv:2305.05097v2 [math.PR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.05097</id>
        <link href="http://arxiv.org/abs/2305.05097"/>
        <updated>2023-07-19T01:53:28.378Z</updated>
        <summary type="html"><![CDATA[We consider random walks on discrete state spaces, such as general undirected
graphs, where the random walkers are designed to approximate a target quantity
over the network topology via sampling and neighborhood exploration in the form
of Markov chain Monte Carlo (MCMC) procedures. Given any Markov chain
corresponding to a target probability distribution, we design a self-repellent
random walk (SRRW) which is less likely to transition to nodes that were highly
visited in the past, and more likely to transition to seldom visited nodes. For
a class of SRRWs parameterized by a positive real {\alpha}, we prove that the
empirical distribution of the process converges almost surely to the the target
(stationary) distribution of the underlying Markov chain kernel. We then
provide a central limit theorem and derive the exact form of the arising
asymptotic co-variance matrix, which allows us to show that the SRRW with a
stronger repellence (larger {\alpha}) always achieves a smaller asymptotic
covariance, in the sense of Loewner ordering of co-variance matrices.
Especially for SRRW-driven MCMC algorithms, we show that the decrease in the
asymptotic sampling variance is of the order O(1/{\alpha}), eventually going
down to zero. Finally, we provide numerical simulations complimentary to our
theoretical results, also empirically testing a version of SRRW with {\alpha}
increasing in time to combine the benefits of smaller asymptotic variance due
to large {\alpha}, with empirically observed faster mixing properties of SRRW
with smaller {\alpha}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Doshi_V/0/1/0/all/0/1"&gt;Vishwaraj Doshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Eun_D/0/1/0/all/0/1"&gt;Do Young Eun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Study of the Effectiveness of Using a Replay Buffer on Mode Discovery in GFlowNets. (arXiv:2307.07674v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.07674</id>
        <link href="http://arxiv.org/abs/2307.07674"/>
        <updated>2023-07-19T01:53:28.378Z</updated>
        <summary type="html"><![CDATA[Reinforcement Learning (RL) algorithms aim to learn an optimal policy by
iteratively sampling actions to learn how to maximize the total expected
return, $R(x)$. GFlowNets are a special class of algorithms designed to
generate diverse candidates, $x$, from a discrete set, by learning a policy
that approximates the proportional sampling of $R(x)$. GFlowNets exhibit
improved mode discovery compared to conventional RL algorithms, which is very
useful for applications such as drug discovery and combinatorial search.
However, since GFlowNets are a relatively recent class of algorithms, many
techniques which are useful in RL have not yet been associated with them. In
this paper, we study the utilization of a replay buffer for GFlowNets. We
explore empirically various replay buffer sampling techniques and assess the
impact on the speed of mode discovery and the quality of the modes discovered.
Our experimental results in the Hypergrid toy domain and a molecule synthesis
environment demonstrate significant improvements in mode discovery when
training with a replay buffer, compared to training only with trajectories
generated on-policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vemgal_N/0/1/0/all/0/1"&gt;Nikhil Vemgal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lau_E/0/1/0/all/0/1"&gt;Elaine Lau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1"&gt;Doina Precup&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Oracle Efficient Online Multicalibration and Omniprediction. (arXiv:2307.08999v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08999</id>
        <link href="http://arxiv.org/abs/2307.08999"/>
        <updated>2023-07-19T01:53:28.376Z</updated>
        <summary type="html"><![CDATA[A recent line of work has shown a surprising connection between
multicalibration, a multi-group fairness notion, and omniprediction, a learning
paradigm that provides simultaneous loss minimization guarantees for a large
family of loss functions. Prior work studies omniprediction in the batch
setting. We initiate the study of omniprediction in the online adversarial
setting. Although there exist algorithms for obtaining notions of
multicalibration in the online adversarial setting, unlike batch algorithms,
they work only for small finite classes of benchmark functions $F$, because
they require enumerating every function $f \in F$ at every round. In contrast,
omniprediction is most interesting for learning theoretic hypothesis classes
$F$, which are generally continuously large.

We develop a new online multicalibration algorithm that is well defined for
infinite benchmark classes $F$, and is oracle efficient (i.e. for any class
$F$, the algorithm has the form of an efficient reduction to a no-regret
learning algorithm for $F$). The result is the first efficient online
omnipredictor -- an oracle efficient prediction algorithm that can be used to
simultaneously obtain no regret guarantees to all Lipschitz convex loss
functions. For the class $F$ of linear functions, we show how to make our
algorithm efficient in the worst case. Also, we show upper and lower bounds on
the extent to which our rates can be improved: our oracle efficient algorithm
actually promises a stronger guarantee called swap-omniprediction, and we prove
a lower bound showing that obtaining $O(\sqrt{T})$ bounds for
swap-omniprediction is impossible in the online setting. On the other hand, we
give a (non-oracle efficient) algorithm which can obtain the optimal
$O(\sqrt{T})$ omniprediction bounds without going through multicalibration,
giving an information theoretic separation between these two solution concepts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Sumegha Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1"&gt;Christopher Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reingold_O/0/1/0/all/0/1"&gt;Omer Reingold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1"&gt;Aaron Roth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge-infused Deep Learning Enables Interpretable Landslide Forecasting. (arXiv:2307.08951v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08951</id>
        <link href="http://arxiv.org/abs/2307.08951"/>
        <updated>2023-07-19T01:53:28.369Z</updated>
        <summary type="html"><![CDATA[Forecasting how landslides will evolve over time or whether they will fail is
a challenging task due to a variety of factors, both internal and external.
Despite their considerable potential to address these challenges, deep learning
techniques lack interpretability, undermining the credibility of the forecasts
they produce. The recent development of transformer-based deep learning offers
untapped possibilities for forecasting landslides with unprecedented
interpretability and nonlinear feature learning capabilities. Here, we present
a deep learning pipeline that is capable of predicting landslide behavior
holistically, which employs a transformer-based network called LFIT to learn
complex nonlinear relationships from prior knowledge and multiple source data,
identifying the most relevant variables, and demonstrating a comprehensive
understanding of landslide evolution and temporal patterns. By integrating
prior knowledge, we provide improvement in holistic landslide forecasting,
enabling us to capture diverse responses to various influencing factors in
different local landslide areas. Using deformation observations as proxies for
measuring the kinetics of landslides, we validate our approach by training
models to forecast reservoir landslides in the Three Gorges Reservoir and
creeping landslides on the Tibetan Plateau. When prior knowledge is
incorporated, we show that interpretable landslide forecasting effectively
identifies influential factors across various landslides. It further elucidates
how local areas respond to these factors, making landslide behavior and trends
more interpretable and predictable. The findings from this study will
contribute to understanding landslide behavior in a new way and make the
proposed approach applicable to other complex disasters influenced by internal
and external factors in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhengjing Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_G/0/1/0/all/0/1"&gt;Gang Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Coupling of Deep Learning with Logical Reasoning. (arXiv:2305.07617v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.07617</id>
        <link href="http://arxiv.org/abs/2305.07617"/>
        <updated>2023-07-19T01:53:28.368Z</updated>
        <summary type="html"><![CDATA[In the ongoing quest for hybridizing discrete reasoning with neural nets,
there is an increasing interest in neural architectures that can learn how to
solve discrete reasoning or optimization problems from natural inputs. In this
paper, we introduce a scalable neural architecture and loss function dedicated
to learning the constraints and criteria of NP-hard reasoning problems
expressed as discrete Graphical Models. Our loss function solves one of the
main limitations of Besag's pseudo-loglikelihood, enabling learning of high
energies. We empirically show it is able to efficiently learn how to solve
NP-hard reasoning problems from natural inputs as the symbolic, visual or
many-solutions Sudoku problems as well as the energy optimization formulation
of the protein design problem, providing data efficiency, interpretability, and
\textit{a posteriori} control over predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Defresne_M/0/1/0/all/0/1"&gt;Marianne Defresne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barbe_S/0/1/0/all/0/1"&gt;Sophie Barbe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schiex_T/0/1/0/all/0/1"&gt;Thomas Schiex&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Resource frugal optimizer for quantum machine learning. (arXiv:2211.04965v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.04965</id>
        <link href="http://arxiv.org/abs/2211.04965"/>
        <updated>2023-07-19T01:53:28.366Z</updated>
        <summary type="html"><![CDATA[Quantum-enhanced data science, also known as quantum machine learning (QML),
is of growing interest as an application of near-term quantum computers.
Variational QML algorithms have the potential to solve practical problems on
real hardware, particularly when involving quantum data. However, training
these algorithms can be challenging and calls for tailored optimization
procedures. Specifically, QML applications can require a large shot-count
overhead due to the large datasets involved. In this work, we advocate for
simultaneous random sampling over both the dataset as well as the measurement
operators that define the loss function. We consider a highly general loss
function that encompasses many QML applications, and we show how to construct
an unbiased estimator of its gradient. This allows us to propose a shot-frugal
gradient descent optimizer called Refoqus (REsource Frugal Optimizer for
QUantum Stochastic gradient descent). Our numerics indicate that Refoqus can
save several orders of magnitude in shot cost, even relative to optimizers that
sample over measurement operators alone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Moussa_C/0/1/0/all/0/1"&gt;Charles Moussa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Gordon_M/0/1/0/all/0/1"&gt;Max Hunter Gordon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Baczyk_M/0/1/0/all/0/1"&gt;Michal Baczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Cerezo_M/0/1/0/all/0/1"&gt;M. Cerezo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Cincio_L/0/1/0/all/0/1"&gt;Lukasz Cincio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Coles_P/0/1/0/all/0/1"&gt;Patrick J. Coles&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Adaptive Neighborhoods for Graph Neural Networks. (arXiv:2307.09065v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.09065</id>
        <link href="http://arxiv.org/abs/2307.09065"/>
        <updated>2023-07-19T01:53:28.364Z</updated>
        <summary type="html"><![CDATA[Graph convolutional networks (GCNs) enable end-to-end learning on graph
structured data. However, many works assume a given graph structure. When the
input graph is noisy or unavailable, one approach is to construct or learn a
latent graph structure. These methods typically fix the choice of node degree
for the entire graph, which is suboptimal. Instead, we propose a novel
end-to-end differentiable graph generator which builds graph topologies where
each node selects both its neighborhood and its size. Our module can be readily
integrated into existing pipelines involving graph convolution operations,
replacing the predetermined or existing adjacency matrix with one that is
learned, and optimized, as part of the general objective. As such it is
applicable to any GCN. We integrate our module into trajectory prediction,
point cloud classification and node classification pipelines resulting in
improved accuracy over other structure-learning methods across a wide range of
datasets and GCN backbones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1"&gt;Avishkar Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendez_O/0/1/0/all/0/1"&gt;Oscar Mendez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1"&gt;Chris Russell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1"&gt;Richard Bowden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Optimal Control for Collective Variable Free Sampling of Molecular Transition Paths. (arXiv:2207.02149v2 [q-bio.BM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2207.02149</id>
        <link href="http://arxiv.org/abs/2207.02149"/>
        <updated>2023-07-19T01:53:28.361Z</updated>
        <summary type="html"><![CDATA[We consider the problem of sampling transition paths between two given
metastable states of a molecular system, e.g. a folded and unfolded protein or
products and reactants of a chemical reaction. Due to the existence of high
energy barriers separating the states, these transition paths are unlikely to
be sampled with standard Molecular Dynamics (MD) simulation. Traditional
methods to augment MD with a bias potential to increase the probability of the
transition rely on a dimensionality reduction step based on Collective
Variables (CVs). Unfortunately, selecting appropriate CVs requires chemical
intuition and traditional methods are therefore not always applicable to larger
systems. Additionally, when incorrect CVs are used, the bias potential might
not be minimal and bias the system along dimensions irrelevant to the
transition. Showing a formal relation between the problem of sampling molecular
transition paths, the Schr\"odinger bridge problem and stochastic optimal
control with neural network policies, we propose a machine learning method for
sampling said transitions. Unlike previous non-machine learning approaches our
method, named PIPS, does not depend on CVs. We show that our method successful
generates low energy transitions for Alanine Dipeptide as well as the larger
Polyproline and Chignolin proteins.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Holdijk_L/0/1/0/all/0/1"&gt;Lars Holdijk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuanqi Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hooft_F/0/1/0/all/0/1"&gt;Ferry Hooft&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Jaini_P/0/1/0/all/0/1"&gt;Priyank Jaini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ensing_B/0/1/0/all/0/1"&gt;Bernd Ensing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High Fidelity Image Counterfactuals with Probabilistic Causal Models. (arXiv:2306.15764v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.15764</id>
        <link href="http://arxiv.org/abs/2306.15764"/>
        <updated>2023-07-19T01:53:28.357Z</updated>
        <summary type="html"><![CDATA[We present a general causal generative modelling framework for accurate
estimation of high fidelity image counterfactuals with deep structural causal
models. Estimation of interventional and counterfactual queries for
high-dimensional structured variables, such as images, remains a challenging
task. We leverage ideas from causal mediation analysis and advances in
generative modelling to design new deep causal mechanisms for structured
variables in causal models. Our experiments demonstrate that our proposed
mechanisms are capable of accurate abduction and estimation of direct, indirect
and total effects as measured by axiomatic soundness of counterfactuals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_F/0/1/0/all/0/1"&gt;Fabio De Sousa Ribeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1"&gt;Tian Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monteiro_M/0/1/0/all/0/1"&gt;Miguel Monteiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pawlowski_N/0/1/0/all/0/1"&gt;Nick Pawlowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1"&gt;Ben Glocker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Large-Scale Visual Representation Learning And Evaluation. (arXiv:2305.13399v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.13399</id>
        <link href="http://arxiv.org/abs/2305.13399"/>
        <updated>2023-07-19T01:53:28.353Z</updated>
        <summary type="html"><![CDATA[In this article, we present our approach to single-modality visual
representation learning. Understanding visual representations of items is vital
for fashion recommendations in e-commerce. We detail and contrast techniques
used to finetune large-scale visual representation learning models in an
efficient manner under low-resource settings, including several pretrained
backbone architectures, both in the convolutional neural network as well as the
vision transformer family. We describe the challenges for e-commerce
applications at-scale and highlight the efforts to more efficiently train,
evaluate, and serve visual representations. We present ablation studies
evaluating the representation offline performance for several downstream tasks,
including visually similar ad recommendations on mobile devices. To this end,
we present a novel multilingual text-to-image generative offline evaluation
method for visually similar recommendation systems. Finally, we include online
results from deployed machine learning systems in production at Etsy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dolev_E/0/1/0/all/0/1"&gt;Eden Dolev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awad_A/0/1/0/all/0/1"&gt;Alaa Awad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_D/0/1/0/all/0/1"&gt;Denisa Roberts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ebrahimzadeh_Z/0/1/0/all/0/1"&gt;Zahra Ebrahimzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mejran_M/0/1/0/all/0/1"&gt;Marcin Mejran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malpani_V/0/1/0/all/0/1"&gt;Vaibhav Malpani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yavuz_M/0/1/0/all/0/1"&gt;Mahir Yavuz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal-Based Supervision of Attention in Graph Neural Network: A Better and Simpler Choice towards Powerful Attention. (arXiv:2305.13115v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.13115</id>
        <link href="http://arxiv.org/abs/2305.13115"/>
        <updated>2023-07-19T01:53:28.350Z</updated>
        <summary type="html"><![CDATA[Recent years have witnessed the great potential of attention mechanism in
graph representation learning. However, while variants of attention-based GNNs
are setting new benchmarks for numerous real-world datasets, recent works have
pointed out that their induced attentions are less robust and generalizable
against noisy graphs due to lack of direct supervision. In this paper, we
present a new framework which utilizes the tool of causality to provide a
powerful supervision signal for the learning process of attention functions.
Specifically, we estimate the direct causal effect of attention to the final
prediction, and then maximize such effect to guide attention attending to more
meaningful neighbors. Our method can serve as a plug-and-play module for any
canonical attention-based GNNs in an end-to-end fashion. Extensive experiments
on a wide range of benchmark datasets illustrated that, by directly supervising
attention functions, the model is able to converge faster with a clearer
decision boundary, and thus yields better performances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongjun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1"&gt;Lun Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1"&gt;Qiang Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xuan Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Enhanced Hankel Dynamic-Mode Decomposition. (arXiv:2303.06289v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.06289</id>
        <link href="http://arxiv.org/abs/2303.06289"/>
        <updated>2023-07-19T01:53:28.349Z</updated>
        <summary type="html"><![CDATA[While the acquisition of time series has become more straightforward,
developing dynamical models from time series is still a challenging and
evolving problem domain. Within the last several years, to address this
problem, there has been a merging of machine learning tools with what is called
the dynamic mode decomposition (DMD). This general approach has been shown to
be an especially promising avenue for accurate model development. Building on
this prior body of work, we develop a deep learning DMD based method which
makes use of the fundamental insight of Takens' Embedding Theorem to build an
adaptive learning scheme that better approximates higher dimensional and
chaotic dynamics. We call this method the Deep Learning Hankel DMD (DLHDMD). We
likewise explore how our method learns mappings which tend, after successful
training, to significantly change the mutual information between dimensions in
the dynamics. This appears to be a key feature in enhancing the DMD overall,
and it should help provide further insight for developing other deep learning
methods for time series analysis and model generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Curtis_C/0/1/0/all/0/1"&gt;Christopher W. Curtis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alford_Lago_D/0/1/0/all/0/1"&gt;D. Jay Alford-Lago&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bollt_E/0/1/0/all/0/1"&gt;Erik Bollt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuma_A/0/1/0/all/0/1"&gt;Andrew Tuma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance. (arXiv:2302.00999v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.00999</id>
        <link href="http://arxiv.org/abs/2302.00999"/>
        <updated>2023-07-19T01:53:28.348Z</updated>
        <summary type="html"><![CDATA[During recent years the interest of optimization and machine learning
communities in high-probability convergence of stochastic optimization methods
has been growing. One of the main reasons for this is that high-probability
complexity bounds are more accurate and less studied than in-expectation ones.
However, SOTA high-probability non-asymptotic convergence results are derived
under strong assumptions such as the boundedness of the gradient noise variance
or of the objective's gradient itself. In this paper, we propose several
algorithms with high-probability convergence results under less restrictive
assumptions. In particular, we derive new high-probability convergence results
under the assumption that the gradient/operator noise has bounded central
$\alpha$-th moment for $\alpha \in (1,2]$ in the following setups: (i) smooth
non-convex / Polyak-Lojasiewicz / convex / strongly convex / quasi-strongly
convex minimization problems, (ii) Lipschitz / star-cocoercive and monotone /
quasi-strongly monotone variational inequalities. These results justify the
usage of the considered methods for solving problems that do not fit standard
functional classes studied in stochastic optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Sadiev_A/0/1/0/all/0/1"&gt;Abdurakhmon Sadiev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Danilova_M/0/1/0/all/0/1"&gt;Marina Danilova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gorbunov_E/0/1/0/all/0/1"&gt;Eduard Gorbunov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Horvath_S/0/1/0/all/0/1"&gt;Samuel Horv&amp;#xe1;th&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gidel_G/0/1/0/all/0/1"&gt;Gauthier Gidel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Dvurechensky_P/0/1/0/all/0/1"&gt;Pavel Dvurechensky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gasnikov_A/0/1/0/all/0/1"&gt;Alexander Gasnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Richtarik_P/0/1/0/all/0/1"&gt;Peter Richt&amp;#xe1;rik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditionally Calibrated Predictive Distributions by Probability-Probability Map: Application to Galaxy Redshift Estimation and Probabilistic Forecasting. (arXiv:2205.14568v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2205.14568</id>
        <link href="http://arxiv.org/abs/2205.14568"/>
        <updated>2023-07-19T01:53:28.344Z</updated>
        <summary type="html"><![CDATA[Uncertainty quantification is crucial for assessing the predictive ability of
AI algorithms. Much research has been devoted to describing the predictive
distribution (PD) $F(y|\mathbf{x})$ of a target variable $y \in \mathbb{R}$
given complex input features $\mathbf{x} \in \mathcal{X}$. However,
off-the-shelf PDs (from, e.g., normalizing flows and Bayesian neural networks)
often lack conditional calibration with the probability of occurrence of an
event given input $\mathbf{x}$ being significantly different from the predicted
probability. Current calibration methods do not fully assess and enforce
conditionally calibrated PDs. Here we propose \texttt{Cal-PIT}, a method that
addresses both PD diagnostics and recalibration by learning a single
probability-probability map from calibration data. The key idea is to regress
probability integral transform scores against $\mathbf{x}$. The estimated
regression provides interpretable diagnostics of conditional coverage across
the feature space. The same regression function morphs the misspecified PD to a
re-calibrated PD for all $\mathbf{x}$. We benchmark our corrected prediction
bands (a by-product of corrected PDs) against oracle bands and state-of-the-art
predictive inference algorithms for synthetic data. We also provide results for
two applications: (i) probabilistic nowcasting given sequences of satellite
images, and (ii) conditional density estimation of galaxy distances given
imaging data (so-called photometric redshift estimation). Our code is available
as a Python package https://github.com/lee-group-cmu/Cal-PIT .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dey_B/0/1/0/all/0/1"&gt;Biprateep Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhao_D/0/1/0/all/0/1"&gt;David Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Newman_J/0/1/0/all/0/1"&gt;Jeffrey A. Newman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Andrews_B/0/1/0/all/0/1"&gt;Brett H. Andrews&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Izbicki_R/0/1/0/all/0/1"&gt;Rafael Izbicki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1"&gt;Ann B. Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Martian time-series unraveled: A multi-scale nested approach with factorial variational autoencoders. (arXiv:2305.16189v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.16189</id>
        <link href="http://arxiv.org/abs/2305.16189"/>
        <updated>2023-07-19T01:53:28.343Z</updated>
        <summary type="html"><![CDATA[Unsupervised source separation involves unraveling an unknown set of source
signals recorded through a mixing operator, with limited prior knowledge about
the sources, and only access to a dataset of signal mixtures. This problem is
inherently ill-posed and is further challenged by the variety of time-scales
exhibited by sources in time series data. Existing methods typically rely on a
preselected window size that limits their capacity to handle multi-scale
sources. To address this issue, instead of operating in the time domain, we
propose an unsupervised multi-scale clustering and source separation framework
by leveraging wavelet scattering covariances that provide a low-dimensional
representation of stochastic processes, capable of distinguishing between
different non-Gaussian stochastic processes. Nested within this representation
space, we develop a factorial Gaussian-mixture variational autoencoder that is
trained to (1) probabilistically cluster sources at different time-scales and
(2) independently sample scattering covariance representations associated with
each cluster. Using samples from each cluster as prior information, we
formulate source separation as an optimization problem in the wavelet
scattering covariance representation space, resulting in separated sources in
the time domain. When applied to seismic data recorded during the NASA InSight
mission on Mars, our multi-scale nested approach proves to be a powerful tool
for discriminating between sources varying greatly in time-scale, e.g.,
minute-long transient one-sided pulses (known as ``glitches'') and structured
ambient noises resulting from atmospheric activities that typically last for
tens of minutes. These results provide an opportunity to conduct further
investigations into the isolated sources related to atmospheric-surface
interactions, thermal relaxations, and other complex phenomena.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siahkoohi_A/0/1/0/all/0/1"&gt;Ali Siahkoohi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morel_R/0/1/0/all/0/1"&gt;Rudy Morel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1"&gt;Randall Balestriero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allys_E/0/1/0/all/0/1"&gt;Erwan Allys&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sainton_G/0/1/0/all/0/1"&gt;Gr&amp;#xe9;gory Sainton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawamura_T/0/1/0/all/0/1"&gt;Taichi Kawamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoop_M/0/1/0/all/0/1"&gt;Maarten V. de Hoop&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A survey on learning from imbalanced data streams: taxonomy, challenges, empirical study, and reproducible experimental framework. (arXiv:2204.03719v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2204.03719</id>
        <link href="http://arxiv.org/abs/2204.03719"/>
        <updated>2023-07-19T01:53:28.341Z</updated>
        <summary type="html"><![CDATA[Class imbalance poses new challenges when it comes to classifying data
streams. Many algorithms recently proposed in the literature tackle this
problem using a variety of data-level, algorithm-level, and ensemble
approaches. However, there is a lack of standardized and agreed-upon procedures
and benchmarks on how to evaluate these algorithms. This work proposes a
standardized, exhaustive, and comprehensive experimental framework to evaluate
algorithms in a collection of diverse and challenging imbalanced data stream
scenarios. The experimental study evaluates 24 state-of-the-art data streams
algorithms on 515 imbalanced data streams that combine static and dynamic class
imbalance ratios, instance-level difficulties, concept drift, real-world and
semi-synthetic datasets in binary and multi-class scenarios. This leads to a
large-scale experimental study comparing state-of-the-art classifiers in the
data stream mining domain. We discuss the advantages and disadvantages of
state-of-the-art classifiers in each of these scenarios and we provide general
recommendations to end-users for selecting the best algorithms for imbalanced
data streams. Additionally, we formulate open challenges and future directions
for this domain. Our experimental framework is fully reproducible and easy to
extend with new methods. This way, we propose a standardized approach to
conducting experiments in imbalanced data streams that can be used by other
researchers to create complete, trustworthy, and fair evaluation of newly
proposed methods. Our experimental framework can be downloaded from
https://github.com/canoalberto/imbalanced-streams.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aguiar_G/0/1/0/all/0/1"&gt;Gabriel Aguiar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krawczyk_B/0/1/0/all/0/1"&gt;Bartosz Krawczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cano_A/0/1/0/all/0/1"&gt;Alberto Cano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Conditional Slot Attention for Object Centric Learning. (arXiv:2307.09437v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09437</id>
        <link href="http://arxiv.org/abs/2307.09437"/>
        <updated>2023-07-19T01:53:28.339Z</updated>
        <summary type="html"><![CDATA[Extracting object-level representations for downstream reasoning tasks is an
emerging area in AI. Learning object-centric representations in an unsupervised
setting presents multiple challenges, a key one being binding an arbitrary
number of object instances to a specialized object slot. Recent object-centric
representation methods like Slot Attention utilize iterative attention to learn
composable representations with dynamic inference level binding but fail to
achieve specialized slot level binding. To address this, in this paper we
propose Unsupervised Conditional Slot Attention using a novel Probabilistic
Slot Dictionary (PSD). We define PSD with (i) abstract object-level property
vectors as key and (ii) parametric Gaussian distribution as its corresponding
value. We demonstrate the benefits of the learnt specific object-level
conditioning distributions in multiple downstream tasks, namely object
discovery, compositional scene generation, and compositional visual reasoning.
We show that our method provides scene composition capabilities and a
significant boost in a few shot adaptability tasks of compositional visual
reasoning, while performing similarly or better than slot attention in object
discovery tasks]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kori_A/0/1/0/all/0/1"&gt;Avinash Kori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1"&gt;Francesco Locatello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1"&gt;Francesca Toni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1"&gt;Ben Glocker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conformal prediction under ambiguous ground truth. (arXiv:2307.09302v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09302</id>
        <link href="http://arxiv.org/abs/2307.09302"/>
        <updated>2023-07-19T01:53:28.333Z</updated>
        <summary type="html"><![CDATA[In safety-critical classification tasks, conformal prediction allows to
perform rigorous uncertainty quantification by providing confidence sets
including the true class with a user-specified probability. This generally
assumes the availability of a held-out calibration set with access to ground
truth labels. Unfortunately, in many domains, such labels are difficult to
obtain and usually approximated by aggregating expert opinions. In fact, this
holds true for almost all datasets, including well-known ones such as CIFAR and
ImageNet. Applying conformal prediction using such labels underestimates
uncertainty. Indeed, when expert opinions are not resolvable, there is inherent
ambiguity present in the labels. That is, we do not have ``crisp'', definitive
ground truth labels and this uncertainty should be taken into account during
calibration. In this paper, we develop a conformal prediction framework for
such ambiguous ground truth settings which relies on an approximation of the
underlying posterior distribution of labels given inputs. We demonstrate our
methodology on synthetic and real datasets, including a case study of skin
condition classification in dermatology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stutz_D/0/1/0/all/0/1"&gt;David Stutz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1"&gt;Abhijit Guha Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matejovicova_T/0/1/0/all/0/1"&gt;Tatiana Matejovicova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strachan_P/0/1/0/all/0/1"&gt;Patricia Strachan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cemgil_A/0/1/0/all/0/1"&gt;Ali Taylan Cemgil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonuniqueness and Convergence to Equivalent Solutions in Observer-based Inverse Reinforcement Learning. (arXiv:2210.16299v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.16299</id>
        <link href="http://arxiv.org/abs/2210.16299"/>
        <updated>2023-07-19T01:53:28.327Z</updated>
        <summary type="html"><![CDATA[A key challenge in solving the deterministic inverse reinforcement learning
(IRL) problem online and in real-time is the existence of multiple solutions.
Nonuniqueness necessitates the study of the notion of equivalent solutions,
i.e., solutions that result in a different cost functional but same feedback
matrix, and convergence to such solutions. While offline algorithms that result
in convergence to equivalent solutions have been developed in the literature,
online, real-time techniques that address nonuniqueness are not available. In
this paper, a regularized history stack observer that converges to
approximately equivalent solutions of the IRL problem is developed. Novel
data-richness conditions are developed to facilitate the analysis and
simulation results are provided to demonstrate the effectiveness of the
developed technique.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Town_J/0/1/0/all/0/1"&gt;Jared Town&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Morrison_Z/0/1/0/all/0/1"&gt;Zachary Morrison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kamalapurkar_R/0/1/0/all/0/1"&gt;Rushikesh Kamalapurkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Differentiation for Inverse Problems with Applications in Quantum Transport. (arXiv:2307.09311v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09311</id>
        <link href="http://arxiv.org/abs/2307.09311"/>
        <updated>2023-07-19T01:53:28.319Z</updated>
        <summary type="html"><![CDATA[A neural solver and differentiable simulation of the quantum transmitting
boundary model is presented for the inverse quantum transport problem. The
neural solver is used to engineer continuous transmission properties and the
differentiable simulation is used to engineer current-voltage characteristics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Williams_I/0/1/0/all/0/1"&gt;Ivan Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polizzi_E/0/1/0/all/0/1"&gt;Eric Polizzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning for Computationally-Constrained Heterogeneous Devices: A Survey. (arXiv:2307.09182v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09182</id>
        <link href="http://arxiv.org/abs/2307.09182"/>
        <updated>2023-07-19T01:53:28.308Z</updated>
        <summary type="html"><![CDATA[With an increasing number of smart devices like internet of things (IoT)
devices deployed in the field, offloadingtraining of neural networks (NNs) to a
central server becomes more and more infeasible. Recent efforts toimprove
users' privacy have led to on-device learning emerging as an alternative.
However, a model trainedonly on a single device, using only local data, is
unlikely to reach a high accuracy. Federated learning (FL)has been introduced
as a solution, offering a privacy-preserving trade-off between communication
overheadand model accuracy by sharing knowledge between devices but disclosing
the devices' private data. Theapplicability and the benefit of applying
baseline FL are, however, limited in many relevant use cases dueto the
heterogeneity present in such environments. In this survey, we outline the
heterogeneity challengesFL has to overcome to be widely applicable in
real-world applications. We especially focus on the aspect ofcomputation
heterogeneity among the participating devices and provide a comprehensive
overview of recentworks on heterogeneity-aware FL. We discuss two groups: works
that adapt the NN architecture and worksthat approach heterogeneity on a system
level, covering Federated Averaging (FedAvg), distillation, and
splitlearning-based approaches, as well as synchronous and asynchronous
aggregation schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pfeiffer_K/0/1/0/all/0/1"&gt;Kilian Pfeiffer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rapp_M/0/1/0/all/0/1"&gt;Martin Rapp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khalili_R/0/1/0/all/0/1"&gt;Ramin Khalili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henkel_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rg Henkel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inverse Optimization for Routing Problems. (arXiv:2307.07357v1 [math.OC] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2307.07357</id>
        <link href="http://arxiv.org/abs/2307.07357"/>
        <updated>2023-07-19T01:53:28.298Z</updated>
        <summary type="html"><![CDATA[We propose a method for learning decision-makers' behavior in routing
problems using Inverse Optimization (IO). The IO framework falls into the
supervised learning category and builds on the premise that the target behavior
is an optimizer of an unknown cost function. This cost function is to be
learned through historical data, and in the context of routing problems, can be
interpreted as the routing preferences of the decision-makers. In this view,
the main contributions of this study are to propose an IO methodology with a
hypothesis function, loss function, and stochastic first-order algorithm
tailored to routing problems. We further test our IO approach in the Amazon
Last Mile Routing Research Challenge, where the goal is to learn models that
replicate the routing preferences of human drivers, using thousands of
real-world routing examples. Our final IO-learned routing model achieves a
score that ranks 2nd compared with the 48 models that qualified for the final
round of the challenge. Our results showcase the flexibility and real-world
potential of the proposed IO methodology to learn from decision-makers'
decisions in routing problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Scroccaro_P/0/1/0/all/0/1"&gt;Pedro Zattoni Scroccaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Beek_P/0/1/0/all/0/1"&gt;Piet van Beek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Esfahani_P/0/1/0/all/0/1"&gt;Peyman Mohajerin Esfahani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Atasoy_B/0/1/0/all/0/1"&gt;Bilge Atasoy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimistic Estimate Uncovers the Potential of Nonlinear Models. (arXiv:2307.08921v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08921</id>
        <link href="http://arxiv.org/abs/2307.08921"/>
        <updated>2023-07-19T01:53:28.283Z</updated>
        <summary type="html"><![CDATA[We propose an optimistic estimate to evaluate the best possible fitting
performance of nonlinear models. It yields an optimistic sample size that
quantifies the smallest possible sample size to fit/recover a target function
using a nonlinear model. We estimate the optimistic sample sizes for matrix
factorization models, deep models, and deep neural networks (DNNs) with
fully-connected or convolutional architecture. For each nonlinear model, our
estimates predict a specific subset of targets that can be fitted at
overparameterization, which are confirmed by our experiments. Our optimistic
estimate reveals two special properties of the DNN models -- free
expressiveness in width and costly expressiveness in connection. These
properties suggest the following architecture design principles of DNNs: (i)
feel free to add neurons/kernels; (ii) restrain from connecting neurons.
Overall, our optimistic estimate theoretically unveils the vast potential of
nonlinear models in fitting at overparameterization. Based on this framework,
we anticipate gaining a deeper understanding of how and why numerous nonlinear
models such as DNNs can effectively realize their potential in practice in the
near future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yaoyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhongwang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Leyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1"&gt;Zhiwei Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1"&gt;Tao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhi-Qin John Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A benchmark of categorical encoders for binary classification. (arXiv:2307.09191v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09191</id>
        <link href="http://arxiv.org/abs/2307.09191"/>
        <updated>2023-07-19T01:53:28.261Z</updated>
        <summary type="html"><![CDATA[Categorical encoders transform categorical features into numerical
representations that are indispensable for a wide range of machine learning
models. Existing encoder benchmark studies lack generalizability because of
their limited choice of (1) encoders, (2) experimental factors, and (3)
datasets. Additionally, inconsistencies arise from the adoption of varying
aggregation strategies. This paper is the most comprehensive benchmark of
categorical encoders to date, including an extensive evaluation of 32
configurations of encoders from diverse families, with 36 combinations of
experimental factors, and on 50 datasets. The study shows the profound
influence of dataset selection, experimental factors, and aggregation
strategies on the benchmark's conclusions -- aspects disregarded in previous
encoder benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Matteucci_F/0/1/0/all/0/1"&gt;Federico Matteucci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arzamasov_V/0/1/0/all/0/1"&gt;Vadim Arzamasov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boehm_K/0/1/0/all/0/1"&gt;Klemens Boehm&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[\nu-Flows: Conditional Neutrino Regression. (arXiv:2207.00664v7 [hep-ph] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2207.00664</id>
        <link href="http://arxiv.org/abs/2207.00664"/>
        <updated>2023-07-19T01:53:28.249Z</updated>
        <summary type="html"><![CDATA[We present $\nu$-Flows, a novel method for restricting the likelihood space
of neutrino kinematics in high energy collider experiments using conditional
normalizing flows and deep invertible neural networks. This method allows the
recovery of the full neutrino momentum which is usually left as a free
parameter and permits one to sample neutrino values under a learned conditional
likelihood given event observations. We demonstrate the success of $\nu$-Flows
in a case study by applying it to simulated semileptonic $t\bar{t}$ events and
show that it can lead to more accurate momentum reconstruction, particularly of
the longitudinal coordinate. We also show that this has direct benefits in a
downstream task of jet association, leading to an improvement of up to a factor
of 1.41 compared to conventional methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ph/1/au:+Leigh_M/0/1/0/all/0/1"&gt;Matthew Leigh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Raine_J/0/1/0/all/0/1"&gt;John Andrew Raine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Zoch_K/0/1/0/all/0/1"&gt;Knut Zoch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Golling_T/0/1/0/all/0/1"&gt;Tobias Golling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient. (arXiv:2307.08873v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08873</id>
        <link href="http://arxiv.org/abs/2307.08873"/>
        <updated>2023-07-19T01:53:28.199Z</updated>
        <summary type="html"><![CDATA[Restricting the variance of a policy's return is a popular choice in
risk-averse Reinforcement Learning (RL) due to its clear mathematical
definition and easy interpretability. Traditional methods directly restrict the
total return variance. Recent methods restrict the per-step reward variance as
a proxy. We thoroughly examine the limitations of these variance-based methods,
such as sensitivity to numerical scale and hindering of policy learning, and
propose to use an alternative risk measure, Gini deviation, as a substitute. We
study various properties of this new risk measure and derive a policy gradient
algorithm to minimize it. Empirical evaluation in domains where risk-aversion
can be clearly defined, shows that our algorithm can mitigate the limitations
of variance-based risk measures and achieves high return with low risk in terms
of variance and Gini deviation when others fail to learn a reasonable policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yudong Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guiliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poupart_P/0/1/0/all/0/1"&gt;Pascal Poupart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yangchen Pan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Accelerating Benders Decomposition via Reinforcement Learning Surrogate Models. (arXiv:2307.08816v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08816</id>
        <link href="http://arxiv.org/abs/2307.08816"/>
        <updated>2023-07-19T01:53:28.194Z</updated>
        <summary type="html"><![CDATA[Stochastic optimization (SO) attempts to offer optimal decisions in the
presence of uncertainty. Often, the classical formulation of these problems
becomes intractable due to (a) the number of scenarios required to capture the
uncertainty and (b) the discrete nature of real-world planning problems. To
overcome these tractability issues, practitioners turn to decomposition methods
that divide the problem into smaller, more tractable sub-problems. The focal
decomposition method of this paper is Benders decomposition (BD), which
decomposes stochastic optimization problems on the basis of scenario
independence. In this paper we propose a method of accelerating BD with the aid
of a surrogate model in place of an NP-hard integer master problem. Through the
acceleration method we observe 30% faster average convergence when compared to
other accelerated BD implementations. We introduce a reinforcement learning
agent as a surrogate and demonstrate how it can be used to solve a stochastic
inventory management problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mak_S/0/1/0/all/0/1"&gt;Stephen Mak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mana_K/0/1/0/all/0/1"&gt;Kyle Mana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zehtabi_P/0/1/0/all/0/1"&gt;Parisa Zehtabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cashmore_M/0/1/0/all/0/1"&gt;Michael Cashmore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magazzeni_D/0/1/0/all/0/1"&gt;Daniele Magazzeni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veloso_M/0/1/0/all/0/1"&gt;Manuela Veloso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphCL-DTA: a graph contrastive learning with molecular semantics for drug-target binding affinity prediction. (arXiv:2307.08989v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08989</id>
        <link href="http://arxiv.org/abs/2307.08989"/>
        <updated>2023-07-19T01:53:28.145Z</updated>
        <summary type="html"><![CDATA[Drug-target binding affinity prediction plays an important role in the early
stages of drug discovery, which can infer the strength of interactions between
new drugs and new targets. However, the performance of previous computational
models is limited by the following drawbacks. The learning of drug
representation relies only on supervised data, without taking into account the
information contained in the molecular graph itself. Moreover, most previous
studies tended to design complicated representation learning module, while
uniformity, which is used to measure representation quality, is ignored. In
this study, we propose GraphCL-DTA, a graph contrastive learning with molecular
semantics for drug-target binding affinity prediction. In GraphCL-DTA, we
design a graph contrastive learning framework for molecular graphs to learn
drug representations, so that the semantics of molecular graphs are preserved.
Through this graph contrastive framework, a more essential and effective drug
representation can be learned without additional supervised data. Next, we
design a new loss function that can be directly used to smoothly adjust the
uniformity of drug and target representations. By directly optimizing the
uniformity of representations, the representation quality of drugs and targets
can be improved. The effectiveness of the above innovative elements is verified
on two real datasets, KIBA and Davis. The excellent performance of GraphCL-DTA
on the above datasets suggests its superiority to the state-of-the-art model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xinxing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Genke Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_J/0/1/0/all/0/1"&gt;Jian Chu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Meets Mental Training -- A Proof of Concept Applied to Memory Sports. (arXiv:2307.08712v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08712</id>
        <link href="http://arxiv.org/abs/2307.08712"/>
        <updated>2023-07-19T01:53:27.955Z</updated>
        <summary type="html"><![CDATA[This work aims to combine these two fields together by presenting a practical
implementation of machine learning to the particular form of mental training
that is the art of memory, taken in its competitive version called "Memory
Sports". Such a fusion, on the one hand, strives to raise awareness about both
realms, while on the other it seeks to encourage research in this mixed field
as a way to, ultimately, drive forward the development of this seemingly
underestimated sport.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Regnani_E/0/1/0/all/0/1"&gt;Emanuele Regnani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs. (arXiv:2307.08433v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.08433</id>
        <link href="http://arxiv.org/abs/2307.08433"/>
        <updated>2023-07-19T01:53:27.871Z</updated>
        <summary type="html"><![CDATA[Many real-world datasets have an underlying dynamic graph structure, where
entities and their interactions evolve over time. Machine learning models
should consider these dynamics in order to harness their full potential in
downstream tasks. Previous approaches for graph representation learning have
focused on either sampling k-hop neighborhoods, akin to breadth-first search,
or random walks, akin to depth-first search. However, these methods are
computationally expensive and unsuitable for real-time, low-latency inference
on dynamic graphs. To overcome these limitations, we propose graph-sprints a
general purpose feature extraction framework for continuous-time-dynamic-graphs
(CTDGs) that has low latency and is competitive with state-of-the-art, higher
latency models. To achieve this, a streaming, low latency approximation to the
random-walk based features is proposed. In our framework, time-aware node
embeddings summarizing multi-hop information are computed using only single-hop
operations on the incoming edges. We evaluate our proposed approach on three
open-source datasets and two in-house datasets, and compare with three
state-of-the-art algorithms (TGN-attn, TGN-ID, Jodie). We demonstrate that our
graph-sprints features, combined with a machine learning classifier, achieve
competitive performance (outperforming all baselines for the node
classification tasks in five datasets). Simultaneously, graph-sprints
significantly reduce inference latencies, achieving close to an order of
magnitude speed-up in our experimental setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eddin_A/0/1/0/all/0/1"&gt;Ahmad Naser Eddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bono_J/0/1/0/all/0/1"&gt;Jacopo Bono&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aparicio_D/0/1/0/all/0/1"&gt;David Apar&amp;#xed;cio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferreira_H/0/1/0/all/0/1"&gt;Hugo Ferreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ascensao_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Ascens&amp;#xe3;o&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_P/0/1/0/all/0/1"&gt;Pedro Ribeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bizarro_P/0/1/0/all/0/1"&gt;Pedro Bizarro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EigenTrajectory: Low-Rank Descriptors for Multi-Modal Trajectory Forecasting. (arXiv:2307.09306v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.09306</id>
        <link href="http://arxiv.org/abs/2307.09306"/>
        <updated>2023-07-19T01:53:27.863Z</updated>
        <summary type="html"><![CDATA[Capturing high-dimensional social interactions and feasible futures is
essential for predicting trajectories. To address this complex nature, several
attempts have been devoted to reducing the dimensionality of the output
variables via parametric curve fitting such as the B\'ezier curve and B-spline
function. However, these functions, which originate in computer graphics
fields, are not suitable to account for socially acceptable human dynamics. In
this paper, we present EigenTrajectory ($\mathbb{ET}$), a trajectory prediction
approach that uses a novel trajectory descriptor to form a compact space, known
here as $\mathbb{ET}$ space, in place of Euclidean space, for representing
pedestrian movements. We first reduce the complexity of the trajectory
descriptor via a low-rank approximation. We transform the pedestrians' history
paths into our $\mathbb{ET}$ space represented by spatio-temporal principle
components, and feed them into off-the-shelf trajectory forecasting models. The
inputs and outputs of the models as well as social interactions are all
gathered and aggregated in the corresponding $\mathbb{ET}$ space. Lastly, we
propose a trajectory anchor-based refinement method to cover all possible
futures in the proposed $\mathbb{ET}$ space. Extensive experiments demonstrate
that our EigenTrajectory predictor can significantly improve both the
prediction accuracy and reliability of existing trajectory forecasting models
on public benchmarks, indicating that the proposed descriptor is suited to
represent pedestrian behaviors. Code is publicly available at
https://github.com/inhwanbae/EigenTrajectory .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bae_I/0/1/0/all/0/1"&gt;Inhwan Bae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1"&gt;Jean Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1"&gt;Hae-Gon Jeon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Resource frugal optimizer for quantum machine learning. (arXiv:2211.04965v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.04965</id>
        <link href="http://arxiv.org/abs/2211.04965"/>
        <updated>2023-07-19T01:53:27.861Z</updated>
        <summary type="html"><![CDATA[Quantum-enhanced data science, also known as quantum machine learning (QML),
is of growing interest as an application of near-term quantum computers.
Variational QML algorithms have the potential to solve practical problems on
real hardware, particularly when involving quantum data. However, training
these algorithms can be challenging and calls for tailored optimization
procedures. Specifically, QML applications can require a large shot-count
overhead due to the large datasets involved. In this work, we advocate for
simultaneous random sampling over both the dataset as well as the measurement
operators that define the loss function. We consider a highly general loss
function that encompasses many QML applications, and we show how to construct
an unbiased estimator of its gradient. This allows us to propose a shot-frugal
gradient descent optimizer called Refoqus (REsource Frugal Optimizer for
QUantum Stochastic gradient descent). Our numerics indicate that Refoqus can
save several orders of magnitude in shot cost, even relative to optimizers that
sample over measurement operators alone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Moussa_C/0/1/0/all/0/1"&gt;Charles Moussa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Gordon_M/0/1/0/all/0/1"&gt;Max Hunter Gordon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Baczyk_M/0/1/0/all/0/1"&gt;Michal Baczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Cerezo_M/0/1/0/all/0/1"&gt;M. Cerezo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Cincio_L/0/1/0/all/0/1"&gt;Lukasz Cincio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Coles_P/0/1/0/all/0/1"&gt;Patrick J. Coles&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nested stochastic block model for simultaneously clustering networks and nodes. (arXiv:2307.09210v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2307.09210</id>
        <link href="http://arxiv.org/abs/2307.09210"/>
        <updated>2023-07-19T01:53:27.853Z</updated>
        <summary type="html"><![CDATA[We introduce the nested stochastic block model (NSBM) to cluster a collection
of networks while simultaneously detecting communities within each network.
NSBM has several appealing features including the ability to work on unlabeled
networks with potentially different node sets, the flexibility to model
heterogeneous communities, and the means to automatically select the number of
classes for the networks and the number of communities within each network.
This is accomplished via a Bayesian model, with a novel application of the
nested Dirichlet process (NDP) as a prior to jointly model the between-network
and within-network clusters. The dependency introduced by the network data
creates nontrivial challenges for the NDP, especially in the development of
efficient samplers. For posterior inference, we propose several Markov chain
Monte Carlo algorithms including a standard Gibbs sampler, a collapsed Gibbs
sampler, and two blocked Gibbs samplers that ultimately return two levels of
clustering labels from both within and across the networks. Extensive
simulation studies are carried out which demonstrate that the model provides
very accurate estimates of both levels of the clustering structure. We also
apply our model to two social network datasets that cannot be analyzed using
any previous method in the literature due to the anonymity of the nodes and the
varying number of nodes in each network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Josephs_N/0/1/0/all/0/1"&gt;Nathaniel Josephs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Amini_A/0/1/0/all/0/1"&gt;Arash A. Amini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Paez_M/0/1/0/all/0/1"&gt;Marina Paez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lin_L/0/1/0/all/0/1"&gt;Lizhen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reduced Kernel Dictionary Learning. (arXiv:2307.08798v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2307.08798</id>
        <link href="http://arxiv.org/abs/2307.08798"/>
        <updated>2023-07-19T01:53:27.848Z</updated>
        <summary type="html"><![CDATA[In this paper we present new algorithms for training reduced-size nonlinear
representations in the Kernel Dictionary Learning (KDL) problem. Standard KDL
has the drawback of a large size of the kernel matrix when the data set is
large. There are several ways of reducing the kernel size, notably Nystr\"om
sampling. We propose here a method more in the spirit of dictionary learning,
where the kernel vectors are obtained with a trained sparse representation of
the input signals. Moreover, we optimize directly the kernel vectors in the KDL
process, using gradient descent steps. We show with three data sets that our
algorithms are able to provide better representations, despite using a small
number of kernel vectors, and also decrease the execution time with respect to
KDL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ilie_Ablachim_D/0/1/0/all/0/1"&gt;Denis C. Ilie-Ablachim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dumitrescu_B/0/1/0/all/0/1"&gt;Bogdan Dumitrescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media. (arXiv:2307.09312v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.09312</id>
        <link href="http://arxiv.org/abs/2307.09312"/>
        <updated>2023-07-19T01:53:27.839Z</updated>
        <summary type="html"><![CDATA[We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal
graph-based transformer model for detecting hate speech in online social
networks. In contrast to traditional text-only methods, our approach to
labelling a comment as hate speech centers around the holistic analysis of text
and images. This is done by leveraging graph transformers to capture the
contextual relationships in the entire discussion that surrounds a comment,
with interwoven fusion layers to combine text and image embeddings instead of
processing different modalities separately. We compare the performance of our
model to baselines that only process text; we also conduct extensive ablation
studies. We conclude with future work for multimodal solutions to deliver
social value in online contexts, arguing that capturing a holistic view of a
conversation greatly advances the effort to detect anti-social behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hebert_L/0/1/0/all/0/1"&gt;Liam Hebert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahu_G/0/1/0/all/0/1"&gt;Gaurav Sahu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sreenivas_N/0/1/0/all/0/1"&gt;Nanda Kishore Sreenivas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golab_L/0/1/0/all/0/1"&gt;Lukasz Golab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_R/0/1/0/all/0/1"&gt;Robin Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forecasting the steam mass flow in a powerplant using the parallel hybrid network. (arXiv:2307.09483v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09483</id>
        <link href="http://arxiv.org/abs/2307.09483"/>
        <updated>2023-07-19T01:53:27.836Z</updated>
        <summary type="html"><![CDATA[Efficient and sustainable power generation is a crucial concern in the energy
sector. In particular, thermal power plants grapple with accurately predicting
steam mass flow, which is crucial for operational efficiency and cost
reduction. In this study, we use a parallel hybrid neural network architecture
that combines a parametrized quantum circuit and a conventional feed-forward
neural network specifically designed for time-series prediction in industrial
settings to enhance predictions of steam mass flow 15 minutes into the future.
Our results show that the parallel hybrid model outperforms standalone
classical and quantum models, achieving more than 5.7 and 4.9 times lower mean
squared error (MSE) loss on the test set after training compared to pure
classical and pure quantum networks, respectively. Furthermore, the hybrid
model demonstrates smaller relative errors between the ground truth and the
model predictions on the test set, up to 2 times better than the pure classical
model. These findings contribute to the broader scientific understanding of how
integrating quantum and classical machine learning techniques can be applied to
real-world challenges faced by the energy sector, ultimately leading to
optimized power plant operations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kurkin_A/0/1/0/all/0/1"&gt;Andrii Kurkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hegemann_J/0/1/0/all/0/1"&gt;Jonas Hegemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kordzanganeh_M/0/1/0/all/0/1"&gt;Mo Kordzanganeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melnikov_A/0/1/0/all/0/1"&gt;Alexey Melnikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts. (arXiv:2304.03209v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.03209</id>
        <link href="http://arxiv.org/abs/2304.03209"/>
        <updated>2023-07-19T01:53:27.786Z</updated>
        <summary type="html"><![CDATA[Integrating high-level semantically correlated contents and low-level
anatomical features is of central importance in medical image segmentation.
Towards this end, recent deep learning-based medical segmentation methods have
shown great promise in better modeling such information. However, convolution
operators for medical segmentation typically operate on regular grids, which
inherently blur the high-frequency regions, i.e., boundary regions. In this
work, we propose MORSE, a generic implicit neural rendering framework designed
at an anatomical level to assist learning in medical image segmentation. Our
method is motivated by the fact that implicit neural representation has been
shown to be more effective in fitting complex signals and solving computer
graphics problems than discrete grid-based representation. The core of our
approach is to formulate medical image segmentation as a rendering problem in
an end-to-end manner. Specifically, we continuously align the coarse
segmentation prediction with the ambiguous coordinate-based point
representations and aggregate these features to adaptively refine the boundary
region. To parallelly optimize multi-scale pixel-level features, we leverage
the idea from Mixture-of-Expert (MoE) to design and train our MORSE with a
stochastic gating mechanism. Our experiments demonstrate that MORSE can work
well with different medical segmentation backbones, consistently achieving
competitive performance improvements in both 2D and 3D supervised medical
segmentation methods. We also theoretically analyze the superiority of MORSE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1"&gt;Weicheng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1"&gt;Yifei Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1"&gt;Lawrence Staib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1"&gt;James S. Duncan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Globally solving the Gromov-Wasserstein problem for point clouds in low dimensional Euclidean spaces. (arXiv:2307.09057v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2307.09057</id>
        <link href="http://arxiv.org/abs/2307.09057"/>
        <updated>2023-07-19T01:53:27.745Z</updated>
        <summary type="html"><![CDATA[This paper presents a framework for computing the Gromov-Wasserstein problem
between two sets of points in low dimensional spaces, where the discrepancy is
the squared Euclidean norm. The Gromov-Wasserstein problem is a generalization
of the optimal transport problem that finds the assignment between two sets
preserving pairwise distances as much as possible. This can be used to quantify
the similarity between two formations or shapes, a common problem in AI and
machine learning. The problem can be formulated as a Quadratic Assignment
Problem (QAP), which is in general computationally intractable even for small
problems. Our framework addresses this challenge by reformulating the QAP as an
optimization problem with a low-dimensional domain, leveraging the fact that
the problem can be expressed as a concave quadratic optimization problem with
low rank. The method scales well with the number of points, and it can be used
to find the global solution for large-scale problems with thousands of points.
We compare the computational complexity of our approach with state-of-the-art
methods on synthetic problems and apply it to a near-symmetrical problem which
is of particular interest in computational biology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ryner_M/0/1/0/all/0/1"&gt;Martin Ryner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kronqvist_J/0/1/0/all/0/1"&gt;Jan Kronqvist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Karlsson_J/0/1/0/all/0/1"&gt;Johan Karlsson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Monte Carlo on a Budget -- Fine-tuning pre-trained Neural Wavefunctions. (arXiv:2307.09337v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2307.09337</id>
        <link href="http://arxiv.org/abs/2307.09337"/>
        <updated>2023-07-19T01:53:27.212Z</updated>
        <summary type="html"><![CDATA[Obtaining accurate solutions to the Schr\"odinger equation is the key
challenge in computational quantum chemistry. Deep-learning-based Variational
Monte Carlo (DL-VMC) has recently outperformed conventional approaches in terms
of accuracy, but only at large computational cost. Whereas in many domains
models are trained once and subsequently applied for inference, accurate DL-VMC
so far requires a full optimization for every new problem instance, consuming
thousands of GPUhs even for small molecules. We instead propose a DL-VMC model
which has been pre-trained using self-supervised wavefunction optimization on a
large and chemically diverse set of molecules. Applying this model to new
molecules without any optimization, yields wavefunctions and absolute energies
that outperform established methods such as CCSD(T)-2Z. To obtain accurate
relative energies, only few fine-tuning steps of this base model are required.
We accomplish this with a fully end-to-end machine-learned model, consisting of
an improved geometry embedding architecture and an existing SE(3)-equivariant
model to represent molecular orbitals. Combining this architecture with
continuous sampling of geometries, we improve zero-shot accuracy by two orders
of magnitude compared to the state of the art. We extensively evaluate the
accuracy, scalability and limitations of our base model on a wide variety of
test systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Scherbela_M/0/1/0/all/0/1"&gt;Michael Scherbela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Gerard_L/0/1/0/all/0/1"&gt;Leon Gerard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Grohs_P/0/1/0/all/0/1"&gt;Philipp Grohs&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Dynamic Attribute-factored World Models for Efficient Multi-object Reinforcement Learning. (arXiv:2307.09205v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09205</id>
        <link href="http://arxiv.org/abs/2307.09205"/>
        <updated>2023-07-19T01:53:27.207Z</updated>
        <summary type="html"><![CDATA[In many reinforcement learning tasks, the agent has to learn to interact with
many objects of different types and generalize to unseen combinations and
numbers of objects. Often a task is a composition of previously learned tasks
(e.g. block stacking). These are examples of compositional generalization, in
which we compose object-centric representations to solve complex tasks. Recent
works have shown the benefits of object-factored representations and
hierarchical abstractions for improving sample efficiency in these settings. On
the other hand, these methods do not fully exploit the benefits of
factorization in terms of object attributes. In this paper, we address this
opportunity and introduce the Dynamic Attribute FacTored RL (DAFT-RL)
framework. In DAFT-RL, we leverage object-centric representation learning to
extract objects from visual inputs. We learn to classify them in classes and
infer their latent parameters. For each class of object, we learn a class
template graph that describes how the dynamics and reward of an object of this
class factorize according to its attributes. We also learn an interaction
pattern graph that describes how objects of different classes interact with
each other at the attribute level. Through these graphs and a dynamic
interaction graph that models the interactions between objects, we can learn a
policy that can then be directly applied in a new environment by just
estimating the interactions and latent parameters. We evaluate DAFT-RL in three
benchmark datasets and show our framework outperforms the state-of-the-art in
generalizing across unseen objects with varying attributes and latent
parameters, as well as in the composition of previously learned tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magliacane_S/0/1/0/all/0/1"&gt;Sara Magliacane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning. (arXiv:2307.09218v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09218</id>
        <link href="http://arxiv.org/abs/2307.09218"/>
        <updated>2023-07-19T01:53:27.088Z</updated>
        <summary type="html"><![CDATA[Forgetting refers to the loss or deterioration of previously acquired
information or knowledge. While the existing surveys on forgetting have
primarily focused on continual learning, forgetting is a prevalent phenomenon
observed in various other research domains within deep learning. Forgetting
manifests in research fields such as generative models due to generator shifts,
and federated learning due to heterogeneous data distributions across clients.
Addressing forgetting encompasses several challenges, including balancing the
retention of old task knowledge with fast learning of new tasks, managing task
interference with conflicting goals, and preventing privacy leakage, etc.
Moreover, most existing surveys on continual learning implicitly assume that
forgetting is always harmful. In contrast, our survey argues that forgetting is
a double-edged sword and can be beneficial and desirable in certain cases, such
as privacy-preserving scenarios. By exploring forgetting in a broader context,
we aim to present a more nuanced understanding of this phenomenon and highlight
its potential advantages. Through this comprehensive survey, we aspire to
uncover potential solutions by drawing upon ideas and approaches from various
fields that have dealt with forgetting. By examining forgetting beyond its
conventional boundaries, in future work, we hope to encourage the development
of novel strategies for mitigating, harnessing, or even embracing forgetting in
real applications. A comprehensive list of papers about forgetting in various
research fields is available at
\url{https://github.com/EnnengYang/Awesome-Forgetting-in-Deep-Learning}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhenyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1"&gt;Enneng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Li Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Federated learning model for Electric Energy management using Blockchain Technology. (arXiv:2307.09080v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09080</id>
        <link href="http://arxiv.org/abs/2307.09080"/>
        <updated>2023-07-19T01:53:27.067Z</updated>
        <summary type="html"><![CDATA[Energy shortfall and electricity load shedding are the main problems for
developing countries. The main causes are lack of management in the energy
sector and the use of non-renewable energy sources. The improved energy
management and use of renewable sources can be significant to resolve energy
crisis. It is necessary to increase the use of renewable energy sources (RESs)
to meet the increasing energy demand due to high prices of fossil-fuel based
energy. Federated learning (FL) is the most emerging technique in the field of
artificial intelligence. Federated learning helps to generate global model at
server side by ensemble locally trained models at remote edges sites while
preserving data privacy. The global model used to predict energy demand to
satisfy the needs of consumers. In this article, we have proposed Blockchain
based safe distributed ledger technology for transaction of data between
prosumer and consumer to ensure their transparency, traceability and security.
Furthermore, we have also proposed a Federated learning model to forecast the
energy requirements of consumer and prosumer. Moreover, Blockchain has been
used to store excess energy data from prosumer for better management of energy
between prosumer and grid. Lastly, the experiment results revealed that
renewable energy sources have produced better and comparable results to other
non-renewable energy resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farooq_M/0/1/0/all/0/1"&gt;Muhammad Shoaib Farooq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayat_A/0/1/0/all/0/1"&gt;Azeen Ahmed Hayat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepMem: ML Models as storage channels and their (mis-)applications. (arXiv:2307.08811v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08811</id>
        <link href="http://arxiv.org/abs/2307.08811"/>
        <updated>2023-07-19T01:53:27.062Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) models are overparameterized to support generality and
avoid overfitting. Prior works have shown that these additional parameters can
be used for both malicious (e.g., hiding a model covertly within a trained
model) and beneficial purposes (e.g., watermarking a model). In this paper, we
propose a novel information theoretic perspective of the problem; we consider
the ML model as a storage channel with a capacity that increases with
overparameterization. Specifically, we consider a sender that embeds arbitrary
information in the model at training time, which can be extracted by a receiver
with a black-box access to the deployed model. We derive an upper bound on the
capacity of the channel based on the number of available parameters. We then
explore black-box write and read primitives that allow the attacker to: (i)
store data in an optimized way within the model by augmenting the training data
at the transmitter side, and (ii) to read it by querying the model after it is
deployed. We also analyze the detectability of the writing primitive and
consider a new version of the problem which takes information storage
covertness into account. Specifically, to obtain storage covertness, we
introduce a new constraint such that the data augmentation used for the write
primitives minimizes the distribution shift with the initial (baseline task)
distribution. This constraint introduces a level of "interference" with the
initial task, thereby limiting the channel's effective capacity. Therefore, we
develop optimizations to improve the capacity in this case, including a novel
ML-specific substitution based error correction protocol. We believe that the
proposed modeling of the problem offers new tools to better understand and
mitigate potential vulnerabilities of ML, especially in the context of
increasingly large models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mamun_M/0/1/0/all/0/1"&gt;Md Abdullah Al Mamun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_Q/0/1/0/all/0/1"&gt;Quazi Mishkatul Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaigani_E/0/1/0/all/0/1"&gt;Erfan Shaigani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaree_P/0/1/0/all/0/1"&gt;Pedram Zaree&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alouani_I/0/1/0/all/0/1"&gt;Ihsen Alouani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abu_Ghazaleh_N/0/1/0/all/0/1"&gt;Nael Abu-Ghazaleh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Trustworthy Dataset Distillation. (arXiv:2307.09165v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09165</id>
        <link href="http://arxiv.org/abs/2307.09165"/>
        <updated>2023-07-19T01:53:27.057Z</updated>
        <summary type="html"><![CDATA[Efficiency and trustworthiness are two eternal pursuits when applying deep
learning in real-world applications. With regard to efficiency, dataset
distillation (DD) endeavors to reduce training costs by distilling the large
dataset into a tiny synthetic dataset. However, existing methods merely
concentrate on in-distribution (InD) classification in a closed-world setting,
disregarding out-of-distribution (OOD) samples. On the other hand, OOD
detection aims to enhance models' trustworthiness, which is always
inefficiently achieved in full-data settings. For the first time, we
simultaneously consider both issues and propose a novel paradigm called
Trustworthy Dataset Distillation (TrustDD). By distilling both InD samples and
outliers, the condensed datasets are capable to train models competent in both
InD classification and OOD detection. To alleviate the requirement of real
outlier data and make OOD detection more practical, we further propose to
corrupt InD samples to generate pseudo-outliers and introduce Pseudo-Outlier
Exposure (POE). Comprehensive experiments on various settings demonstrate the
effectiveness of TrustDD, and the proposed POE surpasses state-of-the-art
method Outlier Exposure (OE). Compared with the preceding DD, TrustDD is more
trustworthy and applicable to real open-world scenarios. Our code will be
publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shijie Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Fei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1"&gt;Zhen Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xu-Yao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[qecGPT: decoding Quantum Error-correcting Codes with Generative Pre-trained Transformers. (arXiv:2307.09025v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2307.09025</id>
        <link href="http://arxiv.org/abs/2307.09025"/>
        <updated>2023-07-19T01:53:27.017Z</updated>
        <summary type="html"><![CDATA[We propose a general framework for decoding quantum error-correcting codes
with generative modeling. The model utilizes autoregressive neural networks,
specifically Transformers, to learn the joint probability of logical operators
and syndromes. This training is in an unsupervised way, without the need for
labeled training data, and is thus referred to as pre-training. After the
pre-training, the model can efficiently compute the likelihood of logical
operators for any given syndrome, using maximum likelihood decoding. It can
directly generate the most-likely logical operators with computational
complexity $\mathcal O(2k)$ in the number of logical qubits $k$, which is
significantly better than the conventional maximum likelihood decoding
algorithms that require $\mathcal O(4^k)$ computation. Based on the pre-trained
model, we further propose refinement to achieve more accurately the likelihood
of logical operators for a given syndrome by directly sampling the stabilizer
operators. We perform numerical experiments on stabilizer codes with small code
distances, using both depolarizing error models and error models with
correlated noise. The results show that our approach provides significantly
better decoding accuracy than the minimum weight perfect matching and
belief-propagation-based algorithms. Our framework is general and can be
applied to any error model and quantum codes with different topologies such as
surface codes and quantum LDPC codes. Furthermore, it leverages the
parallelization capabilities of GPUs, enabling simultaneous decoding of a large
number of syndromes. Our approach sheds light on the efficient and accurate
decoding of quantum error-correcting codes using generative artificial
intelligence and modern computational power.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Cao_H/0/1/0/all/0/1"&gt;Hanyan Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Pan_F/0/1/0/all/0/1"&gt;Feng Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yijia Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[U-shaped Transformer: Retain High Frequency Context in Time Series Analysis. (arXiv:2307.09019v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.09019</id>
        <link href="http://arxiv.org/abs/2307.09019"/>
        <updated>2023-07-19T01:53:26.949Z</updated>
        <summary type="html"><![CDATA[Time series prediction plays a crucial role in various industrial fields. In
recent years, neural networks with a transformer backbone have achieved
remarkable success in many domains, including computer vision and NLP. In time
series analysis domain, some studies have suggested that even the simplest MLP
networks outperform advanced transformer-based networks on time series forecast
tasks. However, we believe these findings indicate there to be low-rank
properties in time series sequences. In this paper, we consider the low-pass
characteristics of transformers and try to incorporate the advantages of MLP.
We adopt skip-layer connections inspired by Unet into traditional transformer
backbone, thus preserving high-frequency context from input to output, namely
U-shaped Transformer. We introduce patch merge and split operation to extract
features with different scales and use larger datasets to fully make use of the
transformer backbone. Our experiments demonstrate that the model performs at an
advanced level across multiple datasets with relatively low cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qingkui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yiqin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automated Design of Riboswitches. (arXiv:2307.08801v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08801</id>
        <link href="http://arxiv.org/abs/2307.08801"/>
        <updated>2023-07-19T01:53:26.915Z</updated>
        <summary type="html"><![CDATA[Experimental screening and selection pipelines for the discovery of novel
riboswitches are expensive, time-consuming, and inefficient. Using
computational methods to reduce the number of candidates for the screen could
drastically decrease these costs. However, existing computational approaches do
not fully satisfy all requirements for the design of such initial screening
libraries. In this work, we present a new method, libLEARNA, capable of
providing RNA focus libraries of diverse variable-length qualified candidates.
Our novel structure-based design approach considers global properties as well
as desired sequence and structure features. We demonstrate the benefits of our
method by designing theophylline riboswitch libraries, following a previously
published protocol, and yielding 30% more unique high-quality candidates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Runge_F/0/1/0/all/0/1"&gt;Frederic Runge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franke_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rg K. H. Franke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1"&gt;Frank Hutter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Value Learning: a General Framework for Learning with Learning Awareness. (arXiv:2307.08863v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08863</id>
        <link href="http://arxiv.org/abs/2307.08863"/>
        <updated>2023-07-19T01:53:26.910Z</updated>
        <summary type="html"><![CDATA[Gradient-based learning in multi-agent systems is difficult because the
gradient derives from a first-order model which does not account for the
interaction between agents' learning processes. LOLA (arXiv:1709.04326)
accounts for this by differentiating through one step of optimization. We
extend the ideas of LOLA and develop a fully-general value-based approach to
optimization. At the core is a function we call the meta-value, which at each
point in joint-policy space gives for each agent a discounted sum of its
objective over future optimization steps. We argue that the gradient of the
meta-value gives a more reliable improvement direction than the gradient of the
original objective, because the meta-value derives from empirical observations
of the effects of optimization. We show how the meta-value can be approximated
by training a neural network to minimize TD error along optimization
trajectories in which agents follow the gradient of the meta-value. We analyze
the behavior of our method on the Logistic Game and on the Iterated Prisoner's
Dilemma.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cooijmans_T/0/1/0/all/0/1"&gt;Tim Cooijmans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aghajohari_M/0/1/0/all/0/1"&gt;Milad Aghajohari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1"&gt;Aaron Courville&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alioth: A Machine Learning Based Interference-Aware Performance Monitor for Multi-Tenancy Applications in Public Cloud. (arXiv:2307.08949v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2307.08949</id>
        <link href="http://arxiv.org/abs/2307.08949"/>
        <updated>2023-07-19T01:53:26.905Z</updated>
        <summary type="html"><![CDATA[Multi-tenancy in public clouds may lead to co-location interference on shared
resources, which possibly results in performance degradation of cloud
applications. Cloud providers want to know when such events happen and how
serious the degradation is, to perform interference-aware migrations and
alleviate the problem. However, virtual machines (VM) in
Infrastructure-as-a-Service public clouds are black-boxes to providers, where
application-level performance information cannot be acquired. This makes
performance monitoring intensely challenging as cloud providers can only rely
on low-level metrics such as CPU usage and hardware counters.

We propose a novel machine learning framework, Alioth, to monitor the
performance degradation of cloud applications. To feed the data-hungry models,
we first elaborate interference generators and conduct comprehensive
co-location experiments on a testbed to build Alioth-dataset which reflects the
complexity and dynamicity in real-world scenarios. Then we construct Alioth by
(1) augmenting features via recovering low-level metrics under no interference
using denoising auto-encoders, (2) devising a transfer learning model based on
domain adaptation neural network to make models generalize on test cases unseen
in offline training, and (3) developing a SHAP explainer to automate feature
selection and enhance model interpretability. Experiments show that Alioth
achieves an average mean absolute error of 5.29% offline and 10.8% when testing
on applications unseen in the training stage, outperforming the baseline
methods. Alioth is also robust in signaling quality-of-service violation under
dynamicity. Finally, we demonstrate a possible application of Alioth's
interpretability, providing insights to benefit the decision-making of cloud
operators. The dataset and code of Alioth have been released on GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1"&gt;Tianyao Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yingxuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yunlong Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xiaofeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1"&gt;Zhen Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yongqiang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning. (arXiv:2307.08941v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08941</id>
        <link href="http://arxiv.org/abs/2307.08941"/>
        <updated>2023-07-19T01:53:26.886Z</updated>
        <summary type="html"><![CDATA[Fine-tuning a pre-trained language model (PLM) emerges as the predominant
strategy in many natural language processing applications. However, even
fine-tuning the PLMs and doing inference are expensive, especially on edge
devices with low computing power. Some general approaches (e.g. quantization
and distillation) have been widely studied to reduce the compute/memory of PLM
fine-tuning, while very few one-shot compression techniques are explored. In
this paper, we investigate the neural tangent kernel (NTK)--which reveals the
gradient descent dynamics of neural networks--of the multilayer perceptrons
(MLP) modules in a PLM and propose to coin a lightweight PLM through
NTK-approximating MLP fusion. To achieve this, we reconsider the MLP as a
bundle of sub-MLPs, and cluster them into a given number of centroids, which
can then be restored as a compressed MLP and surprisingly shown to well
approximate the NTK of the original PLM. Extensive experiments of PLM
fine-tuning on both natural language understanding (NLU) and generation (NLG)
tasks are provided to verify the effectiveness of the proposed method MLP
fusion. Our code is available at https://github.com/weitianxin/MLP_Fusion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1"&gt;Tianxin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1"&gt;Zeming Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yifan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jingrui He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modular Neural Network Approaches for Surgical Image Recognition. (arXiv:2307.08880v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.08880</id>
        <link href="http://arxiv.org/abs/2307.08880"/>
        <updated>2023-07-19T01:53:26.880Z</updated>
        <summary type="html"><![CDATA[Deep learning-based applications have seen a lot of success in recent years.
Text, audio, image, and video have all been explored with great success using
deep learning approaches. The use of convolutional neural networks (CNN) in
computer vision, in particular, has yielded reliable results. In order to
achieve these results, a large amount of data is required. However, the dataset
cannot always be accessible. Moreover, annotating data can be difficult and
time-consuming. Self-training is a semi-supervised approach that managed to
alleviate this problem and achieve state-of-the-art performances. Theoretical
analysis even proved that it may result in a better generalization than a
normal classifier. Another problem neural networks can face is the increasing
complexity of modern problems, requiring a high computational and storage cost.
One way to mitigate this issue, a strategy that has been inspired by human
cognition known as modular learning, can be employed. The principle of the
approach is to decompose a complex problem into simpler sub-tasks. This
approach has several advantages, including faster learning, better
generalization, and enables interpretability.

In the first part of this paper, we introduce and evaluate different
architectures of modular learning for Dorsal Capsulo-Scapholunate Septum (DCSS)
instability classification. Our experiments have shown that modular learning
improves performances compared to non-modular systems. Moreover, we found that
weighted modular, that is to weight the output using the probabilities from the
gating module, achieved an almost perfect classification. In the second part,
we present our approach for data labeling and segmentation with self-training
applied on shoulder arthroscopy images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Salem_N/0/1/0/all/0/1"&gt;Nosseiba Ben Salem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennani_Y/0/1/0/all/0/1"&gt;Younes Bennani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karkazan_J/0/1/0/all/0/1"&gt;Joseph Karkazan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barbara_A/0/1/0/all/0/1"&gt;Abir Barbara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dacheux_C/0/1/0/all/0/1"&gt;Charles Dacheux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gregory_T/0/1/0/all/0/1"&gt;Thomas Gregory&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Predicted-Deletion Dynamic Model: Taking Advantage of ML Predictions, for Free. (arXiv:2307.08890v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2307.08890</id>
        <link href="http://arxiv.org/abs/2307.08890"/>
        <updated>2023-07-19T01:53:26.874Z</updated>
        <summary type="html"><![CDATA[The main bottleneck in designing efficient dynamic algorithms is the unknown
nature of the update sequence. In particular, there are some problems, like
3-vertex connectivity, planar digraph all pairs shortest paths, and others,
where the separation in runtime between the best partially dynamic solutions
and the best fully dynamic solutions is polynomial, sometimes even exponential.

In this paper, we formulate the predicted-deletion dynamic model, motivated
by a recent line of empirical work about predicting edge updates in dynamic
graphs. In this model, edges are inserted and deleted online, and when an edge
is inserted, it is accompanied by a "prediction" of its deletion time. This
models real world settings where services may have access to historical data or
other information about an input and can subsequently use such information make
predictions about user behavior. The model is also of theoretical interest, as
it interpolates between the partially dynamic and fully dynamic settings, and
provides a natural extension of the algorithms with predictions paradigm to the
dynamic setting.

We give a novel framework for this model that "lifts" partially dynamic
algorithms into the fully dynamic setting with little overhead. We use our
framework to obtain improved efficiency bounds over the state-of-the-art
dynamic algorithms for a variety of problems. In particular, we design
algorithms that have amortized update time that scales with a partially dynamic
algorithm, with high probability, when the predictions are of high quality. On
the flip side, our algorithms do no worse than existing fully-dynamic
algorithms when the predictions are of low quality. Furthermore, our algorithms
exhibit a graceful trade-off between the two cases. Thus, we are able to take
advantage of ML predictions asymptotically "for free.'']]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Quanquan C. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivas_V/0/1/0/all/0/1"&gt;Vaidehi Srinivas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification with Incoherent Kernel Dictionary Learning. (arXiv:2307.08796v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08796</id>
        <link href="http://arxiv.org/abs/2307.08796"/>
        <updated>2023-07-19T01:53:26.868Z</updated>
        <summary type="html"><![CDATA[In this paper we present a new classification method based on Dictionary
Learning (DL). The main contribution consists of a kernel version of incoherent
DL, derived from its standard linear counterpart. We also propose an
improvement of the AK-SVD algorithm concerning the representation update. Our
algorithms are tested on several popular databases of classification problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ilie_Ablachim_D/0/1/0/all/0/1"&gt;Denis C. Ilie-Ablachim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dumitrescu_B/0/1/0/all/0/1"&gt;Bogdan Dumitrescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Meta-Learning Based Precoder Optimization Framework for Rate-Splitting Multiple Access. (arXiv:2307.08822v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2307.08822</id>
        <link href="http://arxiv.org/abs/2307.08822"/>
        <updated>2023-07-19T01:53:26.849Z</updated>
        <summary type="html"><![CDATA[In this letter, we propose the use of a meta-learning based precoder
optimization framework to directly optimize the Rate-Splitting Multiple Access
(RSMA) precoders with partial Channel State Information at the Transmitter
(CSIT). By exploiting the overfitting of the compact neural network to maximize
the explicit Average Sum-Rate (ASR) expression, we effectively bypass the need
for any other training data while minimizing the total running time. Numerical
results reveal that the meta-learning based solution achieves similar ASR
performance to conventional precoder optimization in medium-scale scenarios,
and significantly outperforms sub-optimal low complexity precoder algorithms in
the large-scale regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Loli_R/0/1/0/all/0/1"&gt;Rafael Cerna Loli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Clerckx_B/0/1/0/all/0/1"&gt;Bruno Clerckx&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation. (arXiv:2307.08875v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08875</id>
        <link href="http://arxiv.org/abs/2307.08875"/>
        <updated>2023-07-19T01:53:26.844Z</updated>
        <summary type="html"><![CDATA[We study robust reinforcement learning (RL) with the goal of determining a
well-performing policy that is robust against model mismatch between the
training simulator and the testing environment. Previous policy-based robust RL
algorithms mainly focus on the tabular setting under uncertainty sets that
facilitate robust policy evaluation, but are no longer tractable when the
number of states scales up. To this end, we propose two novel uncertainty set
formulations, one based on double sampling and the other on an integral
probability metric. Both make large-scale robust RL tractable even when one
only has access to a simulator. We propose a robust natural actor-critic (RNAC)
approach that incorporates the new uncertainty sets and employs function
approximation. We provide finite-time convergence guarantees for the proposed
RNAC algorithm to the optimal robust policy within the function approximation
error. Finally, we demonstrate the robust performance of the policy learned by
our proposed RNAC approach in multiple MuJoCo environments and a real-world
TurtleBot navigation task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1"&gt;Ruida Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Min Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalathil_D/0/1/0/all/0/1"&gt;Dileep Kalathil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1"&gt;P. R. Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1"&gt;Chao Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IxDRL: A Novel Explainable Deep Reinforcement Learning Toolkit based on Analyses of Interestingness. (arXiv:2307.08933v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2307.08933</id>
        <link href="http://arxiv.org/abs/2307.08933"/>
        <updated>2023-07-19T01:53:26.830Z</updated>
        <summary type="html"><![CDATA[In recent years, advances in deep learning have resulted in a plethora of
successes in the use of reinforcement learning (RL) to solve complex sequential
decision tasks with high-dimensional inputs. However, existing systems lack the
necessary mechanisms to provide humans with a holistic view of their
competence, presenting an impediment to their adoption, particularly in
critical applications where the decisions an agent makes can have significant
consequences. Yet, existing RL-based systems are essentially competency-unaware
in that they lack the necessary interpretation mechanisms to allow human
operators to have an insightful, holistic view of their competency. Towards
more explainable Deep RL (xDRL), we propose a new framework based on analyses
of interestingness. Our tool provides various measures of RL agent competence
stemming from interestingness analysis and is applicable to a wide range of RL
algorithms, natively supporting the popular RLLib toolkit. We showcase the use
of our framework by applying the proposed pipeline in a set of scenarios of
varying complexity. We empirically assess the capability of the approach in
identifying agent behavior patterns and competency-controlling conditions, and
the task elements mostly responsible for an agent's competence, based on global
and local analyses of interestingness. Overall, we show that our framework can
provide agent designers with insights about RL agent competence, both their
capabilities and limitations, enabling more informed decisions about
interventions, additional training, and other interactions in collaborative
human-machine settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sequeira_P/0/1/0/all/0/1"&gt;Pedro Sequeira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gervasio_M/0/1/0/all/0/1"&gt;Melinda Gervasio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards the Sparseness of Projection Head in Self-Supervised Learning. (arXiv:2307.08913v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08913</id>
        <link href="http://arxiv.org/abs/2307.08913"/>
        <updated>2023-07-19T01:53:26.825Z</updated>
        <summary type="html"><![CDATA[In recent years, self-supervised learning (SSL) has emerged as a promising
approach for extracting valuable representations from unlabeled data. One
successful SSL method is contrastive learning, which aims to bring positive
examples closer while pushing negative examples apart. Many current contrastive
learning approaches utilize a parameterized projection head. Through a
combination of empirical analysis and theoretical investigation, we provide
insights into the internal mechanisms of the projection head and its
relationship with the phenomenon of dimensional collapse. Our findings
demonstrate that the projection head enhances the quality of representations by
performing contrastive loss in a projected subspace. Therefore, we propose an
assumption that only a subset of features is necessary when minimizing the
contrastive loss of a mini-batch of data. Theoretical analysis further suggests
that a sparse projection head can enhance generalization, leading us to
introduce SparseHead - a regularization term that effectively constrains the
sparsity of the projection head, and can be seamlessly integrated with any
self-supervised learning (SSL) approaches. Our experimental results validate
the effectiveness of SparseHead, demonstrating its ability to improve the
performance of existing contrastive methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1"&gt;Zeen Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1"&gt;Xingzhe Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingyao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1"&gt;Wenwen Qiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Changwen Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1"&gt;Fuchun Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[regulAS: A Bioinformatics Tool for the Integrative Analysis of Alternative Splicing Regulome using RNA-Seq data. (arXiv:2307.08800v1 [q-bio.GN])]]></title>
        <id>http://arxiv.org/abs/2307.08800</id>
        <link href="http://arxiv.org/abs/2307.08800"/>
        <updated>2023-07-19T01:53:26.808Z</updated>
        <summary type="html"><![CDATA[The regulAS software package is a bioinformatics tool designed to support
computational biology researchers in investigating regulatory mechanisms of
splicing alterations through integrative analysis of large-scale RNA-Seq data
from cancer and healthy human donors, characterized by TCGA and GTEx projects.
This technical report provides a comprehensive overview of regulAS, focusing on
its core functionality, basic modules, experiment configuration, further
extensibility and customisation.

The core functionality of regulAS enables the automation of computational
experiments, efficient results storage and processing, and streamlined workflow
management. Integrated basic modules extend regulAS with features such as
RNA-Seq data retrieval from the public multi-omics UCSC Xena data repository,
predictive modeling and feature ranking capabilities using the scikit-learn
package, and flexible reporting generation for analysing gene expression
profiles and relevant modulations of alternative splicing aberrations across
tissues and cancer types. Experiment configuration is handled through YAML
files with the Hydra and OmegaConf libraries, offering a user-friendly
approach. Additionally, regulAS allows for the development and integration of
custom modules to handle specialized tasks.

In conclusion, regulAS provides an automated solution for alternative
splicing and cancer biology studies, enhancing efficiency, reproducibility, and
customization of experimental design, while the extensibility of the pipeline
enables researchers to further tailor the software package to their specific
needs. Source code is available under the MIT license at
https://github.com/slipnitskaya/regulAS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Lipnitskaya_S/0/1/0/all/0/1"&gt;Sofya Lipnitskaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Examining the Effects of Degree Distribution and Homophily in Graph Learning Models. (arXiv:2307.08881v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2307.08881</id>
        <link href="http://arxiv.org/abs/2307.08881"/>
        <updated>2023-07-19T01:53:26.473Z</updated>
        <summary type="html"><![CDATA[Despite a surge in interest in GNN development, homogeneity in benchmarking
datasets still presents a fundamental issue to GNN research. GraphWorld is a
recent solution which uses the Stochastic Block Model (SBM) to generate diverse
populations of synthetic graphs for benchmarking any GNN task. Despite its
success, the SBM imposed fundamental limitations on the kinds of graph
structure GraphWorld could create.

In this work we examine how two additional synthetic graph generators can
improve GraphWorld's evaluation; LFR, a well-established model in the graph
clustering literature and CABAM, a recent adaptation of the Barabasi-Albert
model tailored for GNN benchmarking. By integrating these generators, we
significantly expand the coverage of graph space within the GraphWorld
framework while preserving key graph properties observed in real-world
networks. To demonstrate their effectiveness, we generate 300,000 graphs to
benchmark 11 GNN models on a node classification task. We find GNN performance
variations in response to homophily, degree distribution and feature signal.
Based on these findings, we classify models by their sensitivity to the new
generators under these properties. Additionally, we release the extensions made
to GraphWorld on the GitHub repository, offering further evaluation of GNN
performance on new graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yasir_M/0/1/0/all/0/1"&gt;Mustafa Yasir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palowitch_J/0/1/0/all/0/1"&gt;John Palowitch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsitsulin_A/0/1/0/all/0/1"&gt;Anton Tsitsulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_Thanh_L/0/1/0/all/0/1"&gt;Long Tran-Thanh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perozzi_B/0/1/0/all/0/1"&gt;Bryan Perozzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sharpness-Aware Graph Collaborative Filtering. (arXiv:2307.08910v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08910</id>
        <link href="http://arxiv.org/abs/2307.08910"/>
        <updated>2023-07-19T01:53:26.427Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have achieved impressive performance in
collaborative filtering. However, GNNs tend to yield inferior performance when
the distributions of training and test data are not aligned well. Also,
training GNNs requires optimizing non-convex neural networks with an abundance
of local and global minima, which may differ widely in their performance at
test time. Thus, it is essential to choose the minima carefully. Here we
propose an effective training schema, called {gSAM}, under the principle that
the \textit{flatter} minima has a better generalization ability than the
\textit{sharper} ones. To achieve this goal, gSAM regularizes the flatness of
the weight loss landscape by forming a bi-level optimization: the outer problem
conducts the standard model training while the inner problem helps the model
jump out of the sharp minima. Experimental results show the superiority of our
gSAM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huiyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1"&gt;Chin-Chia Michael Yeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yujie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yan Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Junpeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_V/0/1/0/all/0/1"&gt;Vivian Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1"&gt;Mahashweta Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hao Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autoregressive Diffusion Model for Graph Generation. (arXiv:2307.08849v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2307.08849</id>
        <link href="http://arxiv.org/abs/2307.08849"/>
        <updated>2023-07-19T01:53:26.421Z</updated>
        <summary type="html"><![CDATA[Diffusion-based graph generative models have recently obtained promising
results for graph generation. However, existing diffusion-based graph
generative models are mostly one-shot generative models that apply Gaussian
diffusion in the dequantized adjacency matrix space. Such a strategy can suffer
from difficulty in model training, slow sampling speed, and incapability of
incorporating constraints. We propose an \emph{autoregressive diffusion} model
for graph generation. Unlike existing methods, we define a node-absorbing
diffusion process that operates directly in the discrete graph space. For
forward diffusion, we design a \emph{diffusion ordering network}, which learns
a data-dependent node absorbing ordering from graph topology. For reverse
generation, we design a \emph{denoising network} that uses the reverse node
ordering to efficiently reconstruct the graph by predicting the node type of
the new node and its edges with previously denoised nodes at a time. Based on
the permutation invariance of graph, we show that the two networks can be
jointly trained by optimizing a simple lower bound of data likelihood. Our
experiments on six diverse generic graph datasets and two molecule datasets
show that our model achieves better or comparable generation performance with
previous state-of-the-art, and meanwhile enjoys fast generation speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1"&gt;Lingkai Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1"&gt;Jiaming Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Haotian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1"&gt;Yuchen Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_B/0/1/0/all/0/1"&gt;B. Aditya Prakash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Space Representations of Neural Algorithmic Reasoners. (arXiv:2307.08874v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08874</id>
        <link href="http://arxiv.org/abs/2307.08874"/>
        <updated>2023-07-19T01:53:26.416Z</updated>
        <summary type="html"><![CDATA[Neural Algorithmic Reasoning (NAR) is a research area focused on designing
neural architectures that can reliably capture classical computation, usually
by learning to execute algorithms. A typical approach is to rely on Graph
Neural Network (GNN) architectures, which encode inputs in high-dimensional
latent spaces that are repeatedly transformed during the execution of the
algorithm. In this work we perform a detailed analysis of the structure of the
latent space induced by the GNN when executing algorithms. We identify two
possible failure modes: (i) loss of resolution, making it hard to distinguish
similar values; (ii) inability to deal with values outside the range observed
during training. We propose to solve the first issue by relying on a softmax
aggregator, and propose to decay the latent space in order to deal with
out-of-range values. We show that these changes lead to improvements on the
majority of algorithms in the standard CLRS-30 benchmark when using the
state-of-the-art Triplet-GMPNN processor. Our code is available at
\href{https://github.com/mirjanic/nar-latent-spaces}{https://github.com/mirjanic/nar-latent-spaces}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mirjanic_V/0/1/0/all/0/1"&gt;Vladimir V. Mirjani&amp;#x107;&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1"&gt;Razvan Pascanu&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1"&gt;Petar Veli&amp;#x10d;kovi&amp;#x107;&lt;/a&gt; (1 and 2) ((1) University of Cambridge, (2) Google DeepMind)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Large Language Model: A Position Paper. (arXiv:2307.08925v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08925</id>
        <link href="http://arxiv.org/abs/2307.08925"/>
        <updated>2023-07-19T01:53:26.394Z</updated>
        <summary type="html"><![CDATA[Large scale language models (LLM) have received significant attention and
found diverse applications across various domains, but their development
encounters challenges in real-world scenarios. These challenges arise due to
the scarcity of public domain data availability and the need to maintain
privacy with respect to private domain data. To address these issues, federated
learning (FL) has emerged as a promising technology that enables collaborative
training of shared models while preserving decentralized data. We propose the
concept of federated LLM, which comprises three key components, i.e., federated
LLM pre-training, federated LLM fine-tuning, and federated LLM prompt
engineering. For each component, we discuss its advantage over traditional LLM
training methods and propose specific engineering strategies for
implementation. Furthermore, we explore the novel challenges introduced by the
integration of FL and LLM. We analyze existing solutions and identify potential
obstacles faced by these solutions within the context of federated LLM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chaochao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1"&gt;Xiaohua Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jianwei Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xiaolin Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling Node Attributes from Graph Topology for Improved Generalizability in Link Prediction. (arXiv:2307.08877v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08877</id>
        <link href="http://arxiv.org/abs/2307.08877"/>
        <updated>2023-07-19T01:53:26.388Z</updated>
        <summary type="html"><![CDATA[Link prediction is a crucial task in graph machine learning with diverse
applications. We explore the interplay between node attributes and graph
topology and demonstrate that incorporating pre-trained node attributes
improves the generalization power of link prediction models. Our proposed
method, UPNA (Unsupervised Pre-training of Node Attributes), solves the
inductive link prediction problem by learning a function that takes a pair of
node attributes and predicts the probability of an edge, as opposed to Graph
Neural Networks (GNN), which can be prone to topological shortcuts in graphs
with power-law degree distribution. In this manner, UPNA learns a significant
part of the latent graph generation mechanism since the learned function can be
used to add incoming nodes to a growing graph. By leveraging pre-trained node
attributes, we overcome observational bias and make meaningful predictions
about unobserved nodes, surpassing state-of-the-art performance (3X to 34X
improvement on benchmark datasets). UPNA can be applied to various pairwise
learning tasks and integrated with existing link prediction models to enhance
their generalizability and bolster graph generative models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chatterjee_A/0/1/0/all/0/1"&gt;Ayan Chatterjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walters_R/0/1/0/all/0/1"&gt;Robin Walters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menichetti_G/0/1/0/all/0/1"&gt;Giulia Menichetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eliassi_Rad_T/0/1/0/all/0/1"&gt;Tina Eliassi-Rad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-stage Neural Networks: Function Approximator of Machine Precision. (arXiv:2307.08934v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08934</id>
        <link href="http://arxiv.org/abs/2307.08934"/>
        <updated>2023-07-19T01:53:26.382Z</updated>
        <summary type="html"><![CDATA[Deep learning techniques are increasingly applied to scientific problems,
where the precision of networks is crucial. Despite being deemed as universal
function approximators, neural networks, in practice, struggle to reduce the
prediction errors below $O(10^{-5})$ even with large network size and extended
training iterations. To address this issue, we developed the multi-stage neural
networks that divides the training process into different stages, with each
stage using a new network that is optimized to fit the residue from the
previous stage. Across successive stages, the residue magnitudes decreases
substantially and follows an inverse power-law relationship with the residue
frequencies. The multi-stage neural networks effectively mitigate the spectral
biases associated with regular neural networks, enabling them to capture the
high frequency feature of target functions. We demonstrate that the prediction
error from the multi-stage training for both regression problems and
physics-informed neural networks can nearly reach the machine-precision
$O(10^{-16})$ of double-floating point within a finite number of iterations.
Such levels of accuracy are rarely attainable using single neural networks
alone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongji Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1"&gt;Ching-Yao Lai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A mixed policy to improve performance of language models on math problems. (arXiv:2307.08767v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.08767</id>
        <link href="http://arxiv.org/abs/2307.08767"/>
        <updated>2023-07-19T01:53:26.375Z</updated>
        <summary type="html"><![CDATA[When to solve math problems, most language models take a sampling strategy to
predict next word according conditional probabilities. In the math reasoning
step, it may generate wrong answer. Considering math problems are
deterministic, we propose a mixed policy exploration approach to solve math
problems with reinforcement learning. In peculiar, we propose a two level token
exploration policy: the abstract level explores next token with probability and
the second level is deterministic. Specifically, the abstract level policy will
decide whether the token is operator or operand with probability sampling,
while the second level is deterministic to select next token with the highest
score in a greedy way. We test our method on GSM8K dataset with GPT-2 model,
and demonstrate more than $2\%$ performance gain. Our implementation is
available at https://github.com/vividitytech/math_lm_rl.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Gang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Sample Tasks for Meta Learning. (arXiv:2307.08924v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.08924</id>
        <link href="http://arxiv.org/abs/2307.08924"/>
        <updated>2023-07-19T01:53:26.360Z</updated>
        <summary type="html"><![CDATA[Through experiments on various meta-learning methods, task samplers, and
few-shot learning tasks, this paper arrives at three conclusions. Firstly,
there are no universal task sampling strategies to guarantee the performance of
meta-learning models. Secondly, task diversity can cause the models to either
underfit or overfit during training. Lastly, the generalization performance of
the models are influenced by task divergence, task entropy, and task
difficulty. In response to these findings, we propose a novel task sampler
called Adaptive Sampler (ASr). ASr is a plug-and-play task sampler that takes
task divergence, task entropy, and task difficulty to sample tasks. To optimize
ASr, we rethink and propose a simple and general meta-learning algorithm.
Finally, a large number of empirical experiments demonstrate the effectiveness
of the proposed ASr.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingyao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1"&gt;Zeen Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1"&gt;Xingzhe Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1"&gt;Lingyu Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Hongwei Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1"&gt;Wenwen Qiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Changwen Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Handwriting training?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/153gnql/d_handwriting_training/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/153gnql/d_handwriting_training/"/>
        <updated>2023-07-19T01:22:31.000Z</updated>
        <summary type="html"><![CDATA[Is there an ai like calligrapher ai where you can write a prompt then it will show but for the styles is there another ai that can write in your handwriting from giving it some samples?
    submitted by    /u/Ok_Presence_3287  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Anomaly scoring methods for subsequence anomaly detection in time series]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/153f09g/d_anomaly_scoring_methods_for_subsequence_anomaly/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/153f09g/d_anomaly_scoring_methods_for_subsequence_anomaly/"/>
        <updated>2023-07-19T00:08:50.000Z</updated>
        <summary type="html"><![CDATA[I'm interested in detecting a subsequence as being anomalous or not. If we imagine that there's a prediction model that can forecast some number of forward steps and we can compare this prediction with the observation, we can get the errors at each time point. Then perhaps one possible way of detecting whether a sequence is anomalous is to get the mean error within the sequence and compare it with the distribution of the mean of the mean of errors of sequences which is calculated from validation data. For example, this distribution may be Gaussian. However, this method sounds a bit naïve since for it to work it would have to assume independence between the errors and some other properties. What could be some other ideas for anomaly scoring methods for the task?
    submitted by    /u/helium-atom  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhance Amazon Lex with conversational FAQ features using LLMs]]></title>
        <id>9668f5be1fea4a8e94dec7ba7df6c1398fedc088</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/enhance-amazon-lex-with-conversational-faq-features-using-llms/"/>
        <updated>2023-07-18T22:48:24.000Z</updated>
        <summary type="html"><![CDATA[Amazon Lex is a service that allows you to quickly and easily build conversational bots (“chatbots”), virtual agents, and interactive voice response (IVR) systems for applications such as Amazon Connect. Artificial intelligence (AI) and machine learning (ML) have been a focus for Amazon for over 20 years, and many of the capabilities that customers use […]]]></summary>
        <author>
            <name>Max Henkel-Wallace</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhance Amazon Lex with LLMs and improve the FAQ experience using URL ingestion]]></title>
        <id>1243f804dd0606d34a2c5295d389c54ded000d99</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/enhance-amazon-lex-with-llms-and-improve-the-faq-experience-using-url-ingestion/"/>
        <updated>2023-07-18T22:48:15.000Z</updated>
        <summary type="html"><![CDATA[In today’s digital world, most consumers would rather find answers to their customer service questions on their own rather than taking the time to reach out to businesses and/or service providers. This blog post explores an innovative solution to build a question and answer chatbot in Amazon Lex that uses existing FAQs from your website. […]]]></summary>
        <author>
            <name>Max Henkel-Wallace</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/153cd0h/r_adversarial_robust_deep_reinforcement_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/153cd0h/r_adversarial_robust_deep_reinforcement_learning/"/>
        <updated>2023-07-18T22:21:24.000Z</updated>
        <summary type="html"><![CDATA[https://ojs.aaai.org/index.php/AAAI/article/view/26009/25781 
    submitted by    /u/ml_dnn  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What phenomena are hyperparameters supposed to capture?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/153becs/what_phenomena_are_hyperparameters_supposed_to/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/153becs/what_phenomena_are_hyperparameters_supposed_to/"/>
        <updated>2023-07-18T21:45:01.000Z</updated>
        <summary type="html"><![CDATA[Suppose you have an IMU and therefore no way to track velocity (reliably). In simulation you can train with velocity in any way you like. In this case, what is velocity in this context. Can it be used in the reward function as a form of privileged info or is it a hyperparameter (and needs an outer loop for optimization)?
 This is just an example of a problem for sim-2-real but that question applies generally for hyperparameters in terms of the objective.
    submitted by    /u/FriendlyStandard5985  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personal Assistant AIs?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/153ang0/personal_assistant_ais/</id>
        <link href="https://www.reddit.com/r/artificial/comments/153ang0/personal_assistant_ais/"/>
        <updated>2023-07-18T21:15:29.000Z</updated>
        <summary type="html"><![CDATA[What does the market look like for personal assistant AIs? I was looking at trying to code one for myself or try to get a group of my coding friends to help make one up so we can use it for ourselves to make our lives easier. Not sure if this really exists now though.
    submitted by    /u/derpgod123  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSC Weekly 18 July 2023]]></title>
        <id>https://www.datasciencecentral.com/?p=62674</id>
        <link href="https://www.datasciencecentral.com/dsc-weekly-18-july-2023/"/>
        <updated>2023-07-18T21:04:00.000Z</updated>
        <summary type="html"><![CDATA[Announcements Top Stories In-Depth
The post DSC Weekly 18 July 2023 appeared first on Data Science Central.]]></summary>
        <author>
            <name>Scott Thompson</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intro to Vanilla Policy Gradient]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1539sz9/intro_to_vanilla_policy_gradient/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1539sz9/intro_to_vanilla_policy_gradient/"/>
        <updated>2023-07-18T20:43:28.000Z</updated>
        <summary type="html"><![CDATA[I've written a series of blog posts going into the theory behind the policy gradient algorithm. Anyone who's starting out in RL may find them to be a good introduction! If you want to understand PPO and various actor critic algorithms, this is the place to start.
 https://kjabon.github.io/blog/2023/VPG/
 ​
 Let me know if you spot any issues or have any questions. (You can also comment on the post itself, I'll see it).
    submitted by    /u/kjabon  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RL applications]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/15397bj/rl_applications/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/15397bj/rl_applications/"/>
        <updated>2023-07-18T20:20:06.000Z</updated>
        <summary type="html"><![CDATA[So I am aware of applications of RL in games and robotics, as well as applications of contextual bandits for recommender systems. But as I look for possible future research paths in RL, I was wondering if there were any other interesting applications of the field. For instance, I recently learned about RL in procedural content generation. I’m particularly interested in more accessible/less resource heavy ones, though I would be glad to learn about all of them. Any insight and resources on this topic will be greatly appreciated. 
    submitted by    /u/Ok_Signature_4944  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SimPer: Simple self-supervised learning of periodic targets]]></title>
        <id>http://ai.googleblog.com/2023/07/simper-simple-self-supervised-learning.html</id>
        <link href="http://ai.googleblog.com/2023/07/simper-simple-self-supervised-learning.html"/>
        <updated>2023-07-18T20:15:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Daniel McDuff, Staff Research Scientist, and Yuzhe Yang, Student Researcher, Google





Learning from periodic data (signals that repeat, such as a heart beat or the daily temperature changes on Earth’s surface) is crucial for many real-world applications, from monitoring weather systems to detecting vital signs. For example, in the environmental remote sensing domain, periodic learning is often needed to enable nowcasting of environmental changes, such as precipitation patterns or land surface temperature. In the health domain, learning from video measurement has shown to extract (quasi-)periodic vital signs such as atrial fibrillation and sleep apnea episodes.



Approaches like RepNet highlight the importance of these types of tasks, and present a solution that recognizes rep…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Filtering on how words are being used]]></title>
        <id>https://www.johndcook.com/blog/?p=198855</id>
        <link href="https://www.johndcook.com/blog/2023/07/18/dependency-labels/"/>
        <updated>2023-07-18T19:04:41.000Z</updated>
        <summary type="html"><![CDATA[Yesterday I wrote about how you could use the spaCy Python library to find proper nouns in a document. Now suppose you want to refine this and find proper nouns that are the subjects of sentences or proper nouns that are direct objects. This post was motivated by a project in which I needed to […]
Filtering on how words are being used first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is the best free AI picture generator available?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15370tj/what_is_the_best_free_ai_picture_generator/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15370tj/what_is_the_best_free_ai_picture_generator/"/>
        <updated>2023-07-18T18:58:02.000Z</updated>
        <summary type="html"><![CDATA[Preferably something that allows NSFW requests.
    submitted by    /u/Ancient_Challenge173  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Llama 2]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/1535dyc/llama_2/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/1535dyc/llama_2/"/>
        <updated>2023-07-18T17:55:21.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] We made Llama13b-v2-chat immediately available as an endpoint for developers]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15350gx/p_we_made_llama13bv2chat_immediately_available_as/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15350gx/p_we_made_llama13bv2chat_immediately_available_as/"/>
        <updated>2023-07-18T17:40:41.000Z</updated>
        <summary type="html"><![CDATA[Hey r/MachineLearning, we've released tools that make it easy to test LLaMa 2 and add it to your own app!
 Model playground here: https://llama2.ai
 Hosted chat API here: https://replicate.com/a16z-infra/llama13b-v2-chat
 If you want to just play with the model, llama2.ai is a very easy way to do it. So far, we’ve found the performance is similar to GPT-3.5 with far fewer parameters, especially for creative tasks and interactions.
 Developers can:
 * clone the chatbot app as a starting point (https://github.com/a16z-infra/llama2-chatbot)
 * use the Replicate endpoint directly (https://replicate.com/a16z-infra/llama13b-v2-chat)
 * or even deploy your own LLaMA v2 fine tune with Cog (https://github.com/a16z-infra/cog-llama-template)
 Please let us know what you use this for or if you have feedback! And thanks to all contributors to this model, Meta, Replicate, the Open Source community!
    submitted by    /u/Prestigious-Elk7124  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Meta open sources llama-2 and tie up with MSFT]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1534kd8/discussion_meta_open_sources_llama2_and_tie_up/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1534kd8/discussion_meta_open_sources_llama2_and_tie_up/"/>
        <updated>2023-07-18T17:23:03.000Z</updated>
        <summary type="html"><![CDATA[https://about.fb.com/news/2023/07/llama-2/
 https://ai.meta.com/llama/
    submitted by    /u/Electrical_Study_617  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Llama 2 is here]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1533mj9/n_llama_2_is_here/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1533mj9/n_llama_2_is_here/"/>
        <updated>2023-07-18T16:47:18.000Z</updated>
        <summary type="html"><![CDATA[Looks like a better model than llama according to the benchmarks they posted. But the biggest difference is that its free even for commercial usage. 
 https://ai.meta.com/resources/models-and-libraries/llama/
    submitted by    /u/timedacorn369  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Build an email spam detector using Amazon SageMaker]]></title>
        <id>dfcb0f9020db2ad682fe75f2a663402cf9b10c2d</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/build-an-email-spam-detector-using-amazon-sagemaker/"/>
        <updated>2023-07-18T16:45:26.000Z</updated>
        <summary type="html"><![CDATA[Spam emails, also known as junk mail, are sent to a large number of users at once and often contain scams, phishing content, or cryptic messages. Spam emails are sometimes sent manually by a human, but most often they are sent using a bot. Examples of spam emails include fake ads, chain emails, and impersonation […]]]></summary>
        <author>
            <name>Dhiraj Thakur</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta/Facebook just released Llama2]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1533iwd/metafacebook_just_released_llama2/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1533iwd/metafacebook_just_released_llama2/"/>
        <updated>2023-07-18T16:43:31.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/swierdo  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google bard uses Deviantart, Quora, Reddit as source for it's opinions]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15339nc/google_bard_uses_deviantart_quora_reddit_as/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15339nc/google_bard_uses_deviantart_quora_reddit_as/"/>
        <updated>2023-07-18T16:33:30.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/TruestNestor  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models", Agarwal et al 2023]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1532r5y/gkd_generalized_knowledge_distillation_for/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1532r5y/gkd_generalized_knowledge_distillation_for/"/>
        <updated>2023-07-18T16:14:01.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How can MeanSquaredError be possibly so bad?]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/1532g52/how_can_meansquarederror_be_possibly_so_bad/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/1532g52/how_can_meansquarederror_be_possibly_so_bad/"/>
        <updated>2023-07-18T16:02:23.000Z</updated>
        <summary type="html"><![CDATA[My neural networks predicts values in range [-1, 1]. I am using mean squared error as my loss function, and I am quite surprised it yields values as high as 1.7. (Just to be clear labels are also in range [-1,1].) I am using tanh as my activation function of the output layer.
 I understand it as extremely bad sign, since even if it always predicted middle value (0), MSE could never be > 1, right? It almost seems like that taking the opposite values would show better results? If I understand this right, how is that even possible that a network can be trained and perform so horribly?
    submitted by    /u/DDDDarky  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Llama 2 foundation models from Meta are now available in Amazon SageMaker JumpStart]]></title>
        <id>17b85f3b3e8e5ddfe1c795e50b416b392a28f188</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/"/>
        <updated>2023-07-18T16:01:34.000Z</updated>
        <summary type="html"><![CDATA[Today, we are excited to announce that Llama 2 foundation models developed by Meta are available for customers through Amazon SageMaker JumpStart. The Llama 2 family of large language models (LLMs) is a collection of pre-trained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Fine-tuned LLMs, called Llama-2-chat, […]]]></summary>
        <author>
            <name>June Won</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Data Intelligence VS Information Retrieval]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1532ej2/d_data_intelligence_vs_information_retrieval/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1532ej2/d_data_intelligence_vs_information_retrieval/"/>
        <updated>2023-07-18T16:00:53.000Z</updated>
        <summary type="html"><![CDATA[I have to choose one of the two elective for the next sem.
 My Questions are:
  
What is Information Retrieval and Data Intelligence?
 
Which is more useful according to industry Requirements?
 
Which one should I take as someone who wants to pursue a career as a Machine Learning Engineer or a Data Scientist?
 
    submitted by    /u/Ethan045627  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forever chemicals and blood donation]]></title>
        <id>https://www.johndcook.com/blog/?p=198836</id>
        <link href="https://www.johndcook.com/blog/2023/07/18/forever-chemicals/"/>
        <updated>2023-07-18T15:58:31.000Z</updated>
        <summary type="html"><![CDATA[I saw a headline saying that donating blood lowers the level of forever chemicals in your body. This post will give a back-of-the-envelope calculation to show that this idea is plausible. Suppose there are chemicals in your bloodstream that do not break down and that your body will not filter out. Suppose you have about […]
Forever chemicals and blood donation first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Utilizing AMD GPUs with Unity ML-Agents]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1531e32/r_utilizing_amd_gpus_with_unity_mlagents/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1531e32/r_utilizing_amd_gpus_with_unity_mlagents/"/>
        <updated>2023-07-18T15:22:21.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone,
 I've embarked on a project involving Unity's ML-Agents toolkit, and I've hit a roadblock regarding GPU utilization. My system is equipped with an AMD GPU, and I'm aware that most machine learning libraries and tools mainly support NVIDIA GPUs due to their compatibility with CUDA.
 Has anyone here successfully gotten ML Agents to work optimally with an AMD GPU? If not, are there any alternative methods or libraries you recommend that work well with AMD GPUs?
 So far, my attempts with TensorFlow and PyTorch have been met with limited success due to their restricted support for AMD GPUs. I've been exploring other potential options like PlaidML and OpenCL, but I'd love to get some input from this community.
 Any suggestions or resources on tackling this issue would be hugely appreciated. Thank you!
    submitted by    /u/Low-Spray-249  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Microsoft Inspire AI megathread!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1531c7r/microsoft_inspire_ai_megathread/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1531c7r/microsoft_inspire_ai_megathread/"/>
        <updated>2023-07-18T15:20:23.000Z</updated>
        <summary type="html"><![CDATA[Starting now
    submitted by    /u/jaketocake  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Relating images to voltages to angle]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15313e9/r_relating_images_to_voltages_to_angle/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15313e9/r_relating_images_to_voltages_to_angle/"/>
        <updated>2023-07-18T15:10:46.000Z</updated>
        <summary type="html"><![CDATA[If this post content is something you are expert in and would like to work with me to accomplish these goals as part of my team I am able to compensate you. I am currently building my team.
 I have pictures of a 3d printed part that I have sequentially lit by different small light sources, each positioned at a known 3d location relative to the part. The lights are less than 2 meters from the part. Each light casts specific shadows on the part. I measure the size of the shadows and relate them to the angular direction to the origin of the light (2D bearing). My next prototype has photodiodes that I will use to measure the % of shading on each diode by photoexcitation as a voltage. I want to build a pattern recognition model to relate the two outputs to the incident angle of light. This is so in the future I can output the bearing direction towards a light source with an unknown 3d relative position via voltage, and be able to validate the voltage data from images.
 Please guide me towards a Machine Learnig platform or engine (for lack of me knowing a better term) that could take this data (% surface shading & voltage) as input and learn how to extract the 2d bearing (and more) from sensor to light source.
 Thanks
    submitted by    /u/masterjebbi  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging AI for smarter electronic data interchange]]></title>
        <id>https://www.datasciencecentral.com/?p=62652</id>
        <link href="https://www.datasciencecentral.com/leveraging-ai-for-smarter-electronic-data-interchange/"/>
        <updated>2023-07-18T15:09:31.000Z</updated>
        <summary type="html"><![CDATA[Electronic Data Interchange (EDI) can be traced back to the late 1960s and early 1970s when businesses began to seek more efficient ways to exchange data electronically. Consequently, the concept of using computers to transmit and receive business documents emerged, aiming to replace manual paper-based processes. Then in the 1980s, standards organizations such as ANSI… Read More »Leveraging AI for smarter electronic data interchange
The post Leveraging AI for smarter electronic data interchange appeared first on Data Science Central.]]></summary>
        <author>
            <name>Ovais Naseem</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[London AI4Code meetup w/ Aaron Parisi (Google) on TALM: Tool Augmented Language Models (July 27th) [R]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1530v9q/london_ai4code_meetup_w_aaron_parisi_google_on/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1530v9q/london_ai4code_meetup_w_aaron_parisi_google_on/"/>
        <updated>2023-07-18T15:02:11.000Z</updated>
        <summary type="html"><![CDATA[The AI4Code reading group is back with Aaron Parisi, Google researcher and lead author of TALM, a framework for augmenting language models with arbitrary tools.
 Free RSVP: https://lu.ma/mw5ppi46
 Paper: https://arxiv.org/abs/2205.12255
 🗓 July 27th (Thursday) at 17:00 GMT+1
 📍 Zoom
 👥 Members of the international AI4Code research community
 Key ideas
 - Modeling tool-use via a text-to-text interface
 - Applying an iterative self-play technique to bootstrap high performance on tasks with few tool-use labelled examples
 TALM consistently outperforms a non-augmented LM on both a knowledge task (NQ) and reasoning task (MathQA).
 The AI4Code meetup community consists of like-minded researchers from around the world that network, discuss and share their latest research on AI applications on source code.
    submitted by    /u/dritsakon  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question about Montezuma's Revenge gym Atari environment]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1530crj/question_about_montezumas_revenge_gym_atari/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1530crj/question_about_montezumas_revenge_gym_atari/"/>
        <updated>2023-07-18T14:42:08.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 ​
 I'm running some code (this code, in case anyone's curious) training an agent to learn in Montezuma'sRevengev4NoFrameskip environment, and it seems to be working, but the nonzero rewards seemingly being returned by the environment are always 1, rather than the "100" or "1000" points that are supposedly returned by the game. I'd like to change this so I can compare to SOTA benchmarks, which seem to use the actual game score, but also because I want to make sure this isn't a bug or anything. 
 As far as I can tell, the reward of "1" is coming from the environment itself, and not from the code I linked converting any nonzero reward to a 1, but I can't find anything stating that in the documentation I can find, and might be missing something.
 Does anyone else have more experience with this environment that could tell me what's causing this/is it normal?
    submitted by    /u/LessPoliticalAccount  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Here's What Barbie Would Look Like In Every Country]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1530bac/heres_what_barbie_would_look_like_in_every_country/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1530bac/heres_what_barbie_would_look_like_in_every_country/"/>
        <updated>2023-07-18T14:40:32.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/World-Tight  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Recognition at Scale? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/152yxfu/image_recognition_at_scale_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/152yxfu/image_recognition_at_scale_d/"/>
        <updated>2023-07-18T13:45:42.000Z</updated>
        <summary type="html"><![CDATA[What services/libraries could I use if I wanted to, say, upload 100+ images and ask it to identify what each image is of? I know that in Bard for example I can upload one image at a time and it'll idenitfy it for me, but I want to do this at scale. Anyone know of any python libraries or OCR services that I could use for this? 
    submitted by    /u/Groundbreaking-Owl-5  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open source project to generate speech from text with emotion. Are there any?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/152xz0y/open_source_project_to_generate_speech_from_text/</id>
        <link href="https://www.reddit.com/r/artificial/comments/152xz0y/open_source_project_to_generate_speech_from_text/"/>
        <updated>2023-07-18T13:05:20.000Z</updated>
        <summary type="html"><![CDATA[I forget what the technical term is, but there are professional tools that can adjust tone/pressure of the speech to generate an emotional voice. Imagine a guy having to stop playing xbox because dinner is ready and he goes “but moooOoOoom, I’m in the middle of a quest. Hhhhng”. I want to be able to generate that. I don’t mind adding annotations for this but I have yet to find a usable open source project that does this. Are there any? 
 (Oh yeah, and I want to make sexy talk. So it’s for porn, it was always about porn. kudos if you’ve guessed it already).
    submitted by    /u/kjarkr  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to access Claude AI outside US and UK]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/152xwuh/d_how_to_access_claude_ai_outside_us_and_uk/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/152xwuh/d_how_to_access_claude_ai_outside_us_and_uk/"/>
        <updated>2023-07-18T13:02:53.000Z</updated>
        <summary type="html"><![CDATA[Anthropic, a company founded by former researchers from OpenAI, has recently introduced its upgraded chatbot, Claude 2.
 Claude 2 has arrived five months after the initial release of its predecessor, Claude, and brings notable improvements such as longer responses, more up-to-date information, faster speeds. One of Claude 2's standout features is its ability to process up to 100,000 tokens, equivalent to 75,000 words, in a single prompt. This is a significant improvement from Claude's previous limitation of 9,000 tokens.
 However, there is one problem with it, currently Claude AI chat is available in UK and US only. While it’s claimed that other regions are soon to follow, the exact timeline remains unclear. Though Anthropic Claude is easily accessible with a VPN. Here are quick steps how to access it if you’re not living in UK or US:
 ​
 1. Buy a VPN provider of your choice that has in UK or US servers (most VPNs will have them since these are the main markets for them). This r/vpn comparison table could help you decide which provider to choose and offers nice discounts for some providers;
 2. Open VPN app;
 3. Connect to US or UK server. For the best speed and user experience, it’s recommended to connect to a server from whichever country is closer to your current location;
 4. Login/Sign-up on Claude AI webpage. You can successfully log in using your personal email address. Using Incognito mode on your browser might be required;
 5. Enjoy your easy access to Claude AI despite not being located in US or UK!
 ​
 Hope this helps someone, happy using!
    submitted by    /u/ProfessionalSource0  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reborn, Remastered and Remixed: ‘Portal: Prelude RTX’ Rejuvenates Legendary Gaming Mod]]></title>
        <id>https://blogs.nvidia.com/?p=65473</id>
        <link href="https://blogs.nvidia.com/blog/2023/07/18/portal-prelude-rtx-remix-studio-driver/"/>
        <updated>2023-07-18T13:00:17.000Z</updated>
        <summary type="html"><![CDATA[The “Portal: Prelude RTX” gaming mod — a remastering of the popular unofficial “Portal” prequel — comes with full ray tracing, DLSS 3 and RTX IO technology for cutting-edge, AI-powered graphics that rejuvenate the legendary mod for gamers, creators, developers and others to experience it anew.]]></summary>
        <author>
            <name>Gerardo Delgado</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mind-Blowing Midjourney Animated Audiobook]]></title>
        <id>https://www.reddit.com/r/artificial/comments/152xdem/mindblowing_midjourney_animated_audiobook/</id>
        <link href="https://www.reddit.com/r/artificial/comments/152xdem/mindblowing_midjourney_animated_audiobook/"/>
        <updated>2023-07-18T12:39:15.000Z</updated>
        <summary type="html"><![CDATA[Just wanted to share with you an ongoing project that uses AI to enhance the way we are creating stories. Of you have questions let me know.
    submitted by    /u/Pa7adox  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Claude 2 creative writing is lit.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/152x7kn/claude_2_creative_writing_is_lit/</id>
        <link href="https://www.reddit.com/r/artificial/comments/152x7kn/claude_2_creative_writing_is_lit/"/>
        <updated>2023-07-18T12:31:45.000Z</updated>
        <summary type="html"><![CDATA[Made this using claude 2 Prompt: A fight between Shazam and Constantine
 Results: Here is a very lengthy and detailed description of a fight between Shazam and Constantine:
 The skies over Gotham City darkened as thunderclouds rolled in, foretelling the epic battle about to unfold. A crack of lightning split the sky and Shazam descended, his red costume blazing like fire, his white cape billowing behind him. His eyes scanned the streets below, searching for his opponent, the cunning magician John Constantine. 
 Constantine stepped out from the shadows of a nearby alley, cigarette dangling from his lips, tan trenchcoat dusty and worn. His hands slid into his pockets and he stared up defiantly at the hero hovering above. "Took you long enough to get here, mate," Constantine called out, his B…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] anyone got code implementation for hyperdreambooth]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/152wm4u/d_anyone_got_code_implementation_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/152wm4u/d_anyone_got_code_implementation_for/"/>
        <updated>2023-07-18T12:04:52.000Z</updated>
        <summary type="html"><![CDATA[i'm looking for code implementation of https://hyperdreambooth.github.io/
 it'd be amazing if anyone can point to a repo or something
 thankyou
    submitted by    /u/SayNo2Tennis  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sex differences in ML [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/152w8yk/sex_differences_in_ml_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/152w8yk/sex_differences_in_ml_d/"/>
        <updated>2023-07-18T11:48:15.000Z</updated>
        <summary type="html"><![CDATA[General question about population stratification in machine learning:
 If I am interested in the important features for disease prediction in women only, is it worth stratifying my sample to women-only? I.e do ML algorithms account for gender differences? I have men and women in the dataset but I am interested in a disease that seems to be diagnosed in women later than men.
    submitted by    /u/Vegetable-Gazelle728  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Retentive Network: A Successor to Transformer for Large Language Models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/152u5va/r_retentive_network_a_successor_to_transformer/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/152u5va/r_retentive_network_a_successor_to_transformer/"/>
        <updated>2023-07-18T10:01:44.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2307.08621
 Retentive Network: A Successor to Transformer for Large Language Models
 Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei
  
In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost O(1) inference, which improves decodin…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Vector DB Basics: a Star Wars Example]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/152tl8p/d_vector_db_basics_a_star_wars_example/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/152tl8p/d_vector_db_basics_a_star_wars_example/"/>
        <updated>2023-07-18T09:29:23.000Z</updated>
        <summary type="html"><![CDATA[Amidst all of the stress of AI taking over, here's a light-hearted blog post on Vector DB basics including a Star Wars mini-example for you all to enjoy :) 
 https://preview.redd.it/n79cv8hkzocb1.png?width=1920&format=png&auto=webp&s=984d955c7d4a0e93ce36ca909835d98b65d6ee2d
    submitted by    /u/kazhdan_d  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Which AI content creators do you follow?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/152scnr/which_ai_content_creators_do_you_follow/</id>
        <link href="https://www.reddit.com/r/artificial/comments/152scnr/which_ai_content_creators_do_you_follow/"/>
        <updated>2023-07-18T08:17:41.000Z</updated>
        <summary type="html"><![CDATA[There are so many “AI influencers” who are suddenly experts or claim to have the perfect ChatGPT prompt despite no prior involvement in the AI space.
 Which AI content creators and leaders do you actually follow and learn from?
 Can include any platforms: Twitter, LinkedIn, YouTube, TikTok, email newsletter, etc.
    submitted by    /u/tridoc  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Derivation of InfoNCE loss]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/152s8dv/d_derivation_of_infonce_loss/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/152s8dv/d_derivation_of_infonce_loss/"/>
        <updated>2023-07-18T08:11:02.000Z</updated>
        <summary type="html"><![CDATA[I've been reading the paper that introduced Contrastive Predictive Coding as well as the InfoNCE section on Lilian Weng's blog post on contrastive learning. After a while of staring and working, I can't figure out how the authors derived equation 5 in the paper. The farthest I get is finding that p(d=i|X, c_t) = 1/(1 + \sum_{j=1, j!=i}^N [p(x_j | c_t) \prod_{l=1, l \neq j \neq i}^N p(x_l)]), but the rest of the derivation is a mystery to me. Is there something super obvious I'm missing?
    submitted by    /u/like_a_tensor  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] State of highly specialized, topic-specific LLMS?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/152rru3/discussion_state_of_highly_specialized/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/152rru3/discussion_state_of_highly_specialized/"/>
        <updated>2023-07-18T07:45:24.000Z</updated>
        <summary type="html"><![CDATA[Yesterday, I thought about why current conversational LLMs like ChatGPT are always so general. For example, I'm mostly working on Reinforcement Learning problems and would expect a model that is specifically fine-tuned on literature exclusively concerned with RL to give much better answers and more intricate details.
 ​
 Are there any papers or blog posts about this?
    submitted by    /u/seawee1  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partnership with American Journalism Project to support local news]]></title>
        <id>https://openai.com/blog/partnership-with-american-journalism-project-to-support-local-news</id>
        <link href="https://openai.com/blog/partnership-with-american-journalism-project-to-support-local-news"/>
        <updated>2023-07-18T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[A new $5+ million partnership aims to explore ways the development of artificial intelligence (AI) can support a thriving, innovative local news field, and ensure local news organizations shape the future of this emerging technology.]]></summary>
        <author>
            <name>OpenAI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ai vedios]]></title>
        <id>https://www.reddit.com/r/artificial/comments/152qsyu/ai_vedios/</id>
        <link href="https://www.reddit.com/r/artificial/comments/152qsyu/ai_vedios/"/>
        <updated>2023-07-18T06:50:47.000Z</updated>
        <summary type="html"><![CDATA[Can you help me find alternatives for heygen and d-id studio cause I need to make the wheels or shorts for social media that pretty much going viral nowadays
    submitted by    /u/Aggressive-Still-886  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Research] Using official implementations vs highly popular unofficial implementation for research]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/152qb4m/research_using_official_implementations_vs_highly/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/152qb4m/research_using_official_implementations_vs_highly/"/>
        <updated>2023-07-18T06:22:05.000Z</updated>
        <summary type="html"><![CDATA[So for the past six months I have been working on a domain adaptation research problem. I wanted to inspect/understand the inherent capability of SSL methods to extract domain invariant features. For this purpose I have been conducting different kinds of experiments.There is a very nice library called lightly that contains the implementations of all published SSL methods, This made things very easy for me in terms of writing code. I am not a PhD student or don't have significant research experience. My guide/mentor is very interested in the work I'm doing and she aims to publish our work in somewhere like a NeurIPS, ICML or so.
 Probably because of my lack of experience, I am overlooking into things or I am genuinely concerned. I just don't want to make stupid coding or code related errors and report wrong results. I just want to know if its mandatory to use the official implementations of every method I'm benchmarking.or example, SimCLR's official implementation is in Tensorflow and I am using PyTorch. Using official implementation would introduce these kind of bottlenecks and slow down my experimentation process. Any advices on this would be greatly appreciated. Thanks.
    submitted by    /u/ashharsha  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reconstructing the Mind’s Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/152ouhv/reconstructing_the_minds_eye_fmritoimage_with/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/152ouhv/reconstructing_the_minds_eye_fmritoimage_with/"/>
        <updated>2023-07-18T05:02:19.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for assembly game environments]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/152or9f/looking_for_assembly_game_environments/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/152or9f/looking_for_assembly_game_environments/"/>
        <updated>2023-07-18T04:58:04.000Z</updated>
        <summary type="html"><![CDATA[Hello, I am really impressed by the real-life applications of Alphadev. I would like to experiment with an assembly game myself, but to the my best knowledge, it appears that there is no representative environment available. Is there an assembly game environment that you would recommend for reinforcement learning experiments?
    submitted by    /u/Spiritual_Fig3632  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for assembly game environments]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/152or9a/looking_for_assembly_game_environments/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/152or9a/looking_for_assembly_game_environments/"/>
        <updated>2023-07-18T04:58:03.000Z</updated>
        <summary type="html"><![CDATA[Hello, I am really impressed by the real-life applications of Alphadev. I would like to experiment with an assembly game myself, but to the my best knowledge, it appears that there is no representative environment available. Is there an assembly game environment that you would recommend for reinforcement learning experiments?
    submitted by    /u/Spiritual_Fig3632  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[For OpenAI Humanoid-v4: is 20000 score (average of last 250 episodes) within 3800 episodes good score for offline RL?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/152nn5b/for_openai_humanoidv4_is_20000_score_average_of/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/152nn5b/for_openai_humanoidv4_is_20000_score_average_of/"/>
        <updated>2023-07-18T04:01:32.000Z</updated>
        <summary type="html"><![CDATA[Do I need to register it somewhere? If people got more than that, ok
 no multi-agents
 log:
 https://preview.redd.it/jf91dab7encb1.png?width=720&format=png&auto=webp&s=89d737fc1f30a6f9ef96575e18e0b8993ba683fd
    submitted by    /u/Timur_1988  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A faster way to teach a robot]]></title>
        <id>https://news.mit.edu/2023/faster-way-teach-robot-technique-0718</id>
        <link href="https://news.mit.edu/2023/faster-way-teach-robot-technique-0718"/>
        <updated>2023-07-18T04:00:00.000Z</updated>
        <summary type="html"><![CDATA[A new technique helps a nontechnical user understand why a robot failed, and then fine-tune it with minimal effort to perform a task effectively.]]></summary>
        <author>
            <name>Adam Zewe | MIT News Office</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Help]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/152ndvf/help/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/152ndvf/help/"/>
        <updated>2023-07-18T03:49:03.000Z</updated>
        <summary type="html"><![CDATA[Hi, I was implementing actor critic algorithm and while running it on cartpole environment, I noticed that if i repeat the same experiment, I would get the exact same results(overlapping plots of actor/critic loss, average return etc). Is it possible as the initialisation should be different for each run? Maybe because the environment is not stochastic?
    submitted by    /u/Interesting-Weeb-699  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["AlpaGasus: Training A Better Alpaca with Fewer Data", Chen et al 2023 {Samsung}]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/152lnl0/alpagasus_training_a_better_alpaca_with_fewer/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/152lnl0/alpagasus_training_a_better_alpaca_with_fewer/"/>
        <updated>2023-07-18T02:27:00.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi agent reinforcement learning - help wanted]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/152kxfy/multi_agent_reinforcement_learning_help_wanted/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/152kxfy/multi_agent_reinforcement_learning_help_wanted/"/>
        <updated>2023-07-18T01:54:22.000Z</updated>
        <summary type="html"><![CDATA[Hi guys, thank you in advance to who's going to answer.
 I'm researching MARL and drones swarms for my master thesis. Drones should navigate in a map, avoiding obstacles and finding a target, just using an RGB camera. If a drone collides/reaches objective, must stop but the episode will conclude when all of them finish.
 I had successfully implemented a single drone env using Microsoft's AirSim, which converges in less than 100k steps using SB3's PPO. Anyway, I need to do the same for a multiagent env. I tried a multitude of frameworks, RLlib (which didn't work well), MARLlib (got a successful implementation, but didn't like it and didn't have much results) and now I'm using SB3+PettingZoo ParallelEnv+SuperSuit. I can easily train the env, but after 1 million steps I still do not get any improvement (see attached pic): some problems are that evaluation episodes sometimes end before all the drones collide/reach objective; I had to modify SuperSuit package because didn't really support well black death on Markov wrapper (when drone is not active, his camera observation is all 0s and actions are not given); evaluation seems to behave differently than training (actions seem "smoothed", almost 0, in particular at the first evaluations episodes); drones seem to behave better (reach easily objective) if all the others collided.
 If any of you are interested, I can attach some code. I had to heavily modify the overrid step function of the Parallel env to support training on active agents only (possible_agents variable). I was inspired by this stack overflow: https://stackoverflow.com/questions/73111772/problem-with-pettingzoo-and-stable-baselines3-with-a-parallelenv
 If you have any advice, any different framework to try (I should try Tianshou's), please tell me. Any help is greatly appreciated. Thank you all.
    submitted by    /u/IntelligentAd6407  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I did it]]></title>
        <id>https://www.reddit.com/r/artificial/comments/152k1td/i_did_it/</id>
        <link href="https://www.reddit.com/r/artificial/comments/152k1td/i_did_it/"/>
        <updated>2023-07-18T01:13:45.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/plauge1_  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 7/17/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/152jtxz/oneminute_daily_ai_news_7172023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/152jtxz/oneminute_daily_ai_news_7172023/"/>
        <updated>2023-07-18T01:03:40.000Z</updated>
        <summary type="html"><![CDATA[With generative AI becoming all the rage these days, it’s perhaps not surprising that the technology has been repurposed by malicious actors to their own advantage, enabling avenues for accelerated cybercrime. According to findings from SlashNext, a new generative AI cybercrime tool called WormGPT has been advertised on underground forums as a way for adversaries to launch sophisticated phishing and business email compromise (BEC) attacks.[1]
 A.I. is a $1 trillion investment opportunity but will be ‘biggest bubble of all time,’ Stability AI CEO Emad Mostaque predicts.[2]
 The Israel Defense Forces have started using artificial intelligence to select targets for air strikes and organize wartime logistics as tensions escalate in the occupied territories and with arch-rival Iran.[3]
 MIT researchers have developed PIGINet, a new system that aims to efficiently enhance the problem-solving capabilities of household robots, reducing planning time by 50-80 percent.[4]
  
Sources:
 [1] https://thehackernews.com/2023/07/wormgpt-new-ai-tool-allows.html
 [2] https://www.cnbc.com/2023/07/17/ai-will-be-the-biggest-bubble-of-all-time-stability-ai-ceo.html
 [3] https://www.bloomberg.com/news/articles/2023-07-16/israel-using-ai-systems-to-plan-deadly-military-operations?in_source=embedded-checkout-banner
 [4] https://interestingengineering.com/innovation/ai-household-robots-problem-solving-skills 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]💥 How Underdog AI Companies Will Crush Silicon Valley Giants.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/152iy54/d_how_underdog_ai_companies_will_crush_silicon/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/152iy54/d_how_underdog_ai_companies_will_crush_silicon/"/>
        <updated>2023-07-18T00:24:26.000Z</updated>
        <summary type="html"><![CDATA[💥 How Underdog AI Companies Will Crush Silicon Valley Giants.
 Opportunities in AI: Creating Abundant Intelligence.
  
Generative AI like ChatGPT brings complex tasks within reach and is set to transform society. Startups have an opportunity in applying AI to create "abundant intelligence".
 In the past year, ChatGPT, GitHub Copilot, and Midjourney have rapidly grown to $100M+ revenue.
 AI startups face competition from tech giants also moving quickly into AI. Startups must pick spots where they have an advantage.
 Opportunities exist in expanding the application universe into new greenfield opportunities like automating mundane decisions, masking workflow complexity, and reimagining applications.
 Infrastructure tools make models more powerful by chaining them together and improving accuracy. Opportunity areas include unstructured data management, agent-driven automation, model evaluation, and experimentation.
 Key players emerging are foundation model providers like OpenAI and Anthropic, companies building domain-specific models, and platforms for autonomous agents.
 Advantages exist for startups focused on imagination and technical ability to find non-obvious ideas, while large companies retrofit existing businesses.
  
   submitted by    /u/Yavero  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Semantic-SAM: Reproduce and Beyond SAM with Semantic-Aware and Granualrity-Abundance]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/152idsj/r_semanticsam_reproduce_and_beyond_sam_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/152idsj/r_semanticsam_reproduce_and_beyond_sam_with/"/>
        <updated>2023-07-18T00:00:28.000Z</updated>
        <summary type="html"><![CDATA[We introduce Semantic-SAM, a universal image segmentation model to enable segment and recognize anything at any desired granularity. We have trained on the whole SA-1B dataset and our model can reproduce SAM and beyond it. Training and inference code is available!
 🔥code & demo link: https://github.com/UX-Decoder/Semantic-SAM
 🔥paper link: https://arxiv.org/pdf/2307.04767.pdf
 🚀 Features
 🔥 Reproduce SAM. SAM training is a sub-task of ours. We have released the training code to reproduce SAM training.
 🔥 Beyond SAM. Our newly proposed model offers the following attributes from instance to part level:
  
Granularity Abundance. Our model can produce all possible segmentation granularities for a user click with high quality, which enables more controllable and user-friendly interactive s…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] LLM to simulate a character]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/152i3nx/p_llm_to_simulate_a_character/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/152i3nx/p_llm_to_simulate_a_character/"/>
        <updated>2023-07-17T23:48:25.000Z</updated>
        <summary type="html"><![CDATA[I'm working on building an application and I want to have a chatbot that has the opinions and thoughts of a particular person.
 I want to train this on my own. I have a large corpus of data that I can use for this training. I am not sure which existing foundation model / model architecture I should use for training this.
 I fine-tuned a GPT2 model earlier but the results were very poor. Maybe it has to do with the data?
    submitted by    /u/MethodExtension5513  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How do you make a video based off Midjourney?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/152gj6t/how_do_you_make_a_video_based_off_midjourney/</id>
        <link href="https://www.reddit.com/r/artificial/comments/152gj6t/how_do_you_make_a_video_based_off_midjourney/"/>
        <updated>2023-07-17T22:43:45.000Z</updated>
        <summary type="html"><![CDATA[Maybe it's a stupid question because I have never used Midjourney.
 Lately, my Instagram reels are getting spammed with a lot of videos created by adding images generated by Midjourney. Like a traditional cartoon but with Midjourney images.
 I'm wondering how is people doing so. Can you tell Midjourney to generate a sequence of images?
    submitted by    /u/yzT-  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sweep: AI Junior Developer that solves your GitHub Issues]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/152ftvb/sweep_ai_junior_developer_that_solves_your_github/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/152ftvb/sweep_ai_junior_developer_that_solves_your_github/"/>
        <updated>2023-07-17T22:15:48.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/williamsweep  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can you explain stable diffusion and how to get it?Can I get an app for it? It’s seems there isnt A stable diffusion and it just seems to be the name of different AI models that run in the same AI? I’m sooooo confused even chat gpt can’t help me.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/152f74q/can_you_explain_stable_diffusion_and_how_to_get/</id>
        <link href="https://www.reddit.com/r/artificial/comments/152f74q/can_you_explain_stable_diffusion_and_how_to_get/"/>
        <updated>2023-07-17T21:50:51.000Z</updated>
        <summary type="html"><![CDATA[Title
    submitted by    /u/Entire_Insurance_532  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/152eq65/n_flashattention2_faster_attention_with_better/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/152eq65/n_flashattention2_faster_attention_with_better/"/>
        <updated>2023-07-17T21:32:26.000Z</updated>
        <summary type="html"><![CDATA[Twitter thread: https://twitter.com/tri_dao/status/1680987577913065472
 Tech report: https://tridao.me/publications/flash2/flash2.pdf
    submitted by    /u/SchmidhuberDidIt  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Searching for proper nouns]]></title>
        <id>https://www.johndcook.com/blog/?p=198685</id>
        <link href="https://www.johndcook.com/blog/2023/07/17/searching-for-proper-nouns/"/>
        <updated>2023-07-17T21:03:33.000Z</updated>
        <summary type="html"><![CDATA[Suppose you want to find all the proper nouns in a document. You could grep for every word that starts with a capital letter with something like grep '\b[A-Z]\w+' but this would return the first word of each sentence in addition to the words you’re after. You could grep for capitalized words that are not […]
Searching for proper nouns first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Clustering of X shaped data]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/152crh6/r_clustering_of_x_shaped_data/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/152crh6/r_clustering_of_x_shaped_data/"/>
        <updated>2023-07-17T20:19:00.000Z</updated>
        <summary type="html"><![CDATA[I have a dataset with two variables and 500 observations. They plot like an X shape. I have been trying to find a clustering method to identify the two lines forming the X as two different clusters. All the methods I tried so far (K_means, DBSCAN, Spectral clustering) identified the two angles forming the X as the two differebt clusters. Any ideas on how to approach this?
 Any help would be appreciated. Thanks!
    submitted by    /u/earthlingsapien  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Creating a Glossary Using AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/152cqo1/creating_a_glossary_using_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/152cqo1/creating_a_glossary_using_ai/"/>
        <updated>2023-07-17T20:18:09.000Z</updated>
        <summary type="html"><![CDATA[Hi! I have multiple versions/files of my company's glossaries, terms, acronyms, etc. and I need to combine them into one comprehensive file that will eliminate any instance of duplicated content across the files I'm working from. Is there an AI program (or any program) that will help me in creating one unified glossary? 
    submitted by    /u/audballer3000  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding viral justice]]></title>
        <id>https://news.mit.edu/2023/understanding-viral-justice-ruha-benjamin-0717</id>
        <link href="https://news.mit.edu/2023/understanding-viral-justice-ruha-benjamin-0717"/>
        <updated>2023-07-17T20:10:00.000Z</updated>
        <summary type="html"><![CDATA[Author and African American studies scholar Ruha Benjamin urges MIT Libraries staff to “re-imagine the default settings” of technology for a more just future.]]></summary>
        <author>
            <name>Brigham Fay | MIT Libraries</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Configure cross-account access of Amazon Redshift clusters in Amazon SageMaker Studio using VPC peering]]></title>
        <id>7a9ba44e3a06219c8fb5e7af7a8e9248abce1d5f</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/configure-cross-account-access-of-amazon-redshift-clusters-in-amazon-sagemaker-studio-using-vpc-peering/"/>
        <updated>2023-07-17T19:54:11.000Z</updated>
        <summary type="html"><![CDATA[With cloud computing, as compute power and data became more available, machine learning (ML) is now making an impact across every industry and is a core part of every business and industry. Amazon SageMaker Studio is the first fully integrated ML development environment (IDE) with a web-based visual interface. You can perform all ML development […]]]></summary>
        <author>
            <name>Supriya Puragundla</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Armando Solar-Lezama named inaugural Distinguished College of Computing Professor]]></title>
        <id>https://news.mit.edu/2023/armando-solar-lezama-named-inaugural-distinguished-college-computing-professor-0717</id>
        <link href="https://news.mit.edu/2023/armando-solar-lezama-named-inaugural-distinguished-college-computing-professor-0717"/>
        <updated>2023-07-17T19:50:00.000Z</updated>
        <summary type="html"><![CDATA[EECS professor appointed to new professorship in the MIT Schwarzman College of Computing.]]></summary>
        <author>
            <name>MIT Schwarzman College of Computing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] GeoSegment Demo - Segment Anything Model for Geospatial Data (running purely in the browser)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/152bzmc/p_geosegment_demo_segment_anything_model_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/152bzmc/p_geosegment_demo_segment_anything_model_for/"/>
        <updated>2023-07-17T19:49:57.000Z</updated>
        <summary type="html"><![CDATA[I’ve been working on a side project that utilises the segment anything model for satellite imagery, but allowing it to run purely as a web application (no need to run the model locally on a powerful PC).
 The intention is to provide a quick and easy “AI assisted” way to segment imagery and save time on digitisation tasks, and then export it to your GIS application of choice (QGIS or ESRI software support the export format, which is GeoJSON).
 The demo video is here
 If anyone wants access to the online demo shown in the video, just message me and I can give you the link and demo credentials.
 I’m hoping there is some use for it to GIS folks :)
    submitted by    /u/CharlieTheChooChooo  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Traditional painter using AI to unlock inspiration.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/152bz49/traditional_painter_using_ai_to_unlock_inspiration/</id>
        <link href="https://www.reddit.com/r/artificial/comments/152bz49/traditional_painter_using_ai_to_unlock_inspiration/"/>
        <updated>2023-07-17T19:49:25.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/AdThin6400  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How does Claude parse attached documents?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/152bm8b/d_how_does_claude_parse_attached_documents/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/152bm8b/d_how_does_claude_parse_attached_documents/"/>
        <updated>2023-07-17T19:35:53.000Z</updated>
        <summary type="html"><![CDATA[I played with Claude 2 this weekend and overall really impressed, especially for summarizing pdfs and other text documents.
 I gave it Microsoft's Q2 financial statement, and Claude did a good job with most questions, including over tabular data.
 Anyone know how it parses tabular data from documents? I can see the extracted lines but wondering how they get used. Is there a preprocessing step of creating embeddings from it? 
 https://preview.redd.it/4fnjn477vkcb1.png?width=1200&format=png&auto=webp&s=fa235bbcbd4d2954fae5908a904cd5d7f17658c8
 Some more details from my experiment in this thread.
    submitted by    /u/sarmad-q  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] LoopGPT Update - Finally something useful?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/152arb7/p_loopgpt_update_finally_something_useful/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/152arb7/p_loopgpt_update_finally_something_useful/"/>
        <updated>2023-07-17T19:03:09.000Z</updated>
        <summary type="html"><![CDATA[By now, most of us who tried have realized that the "autonomous LLM agents" are not really useful at the moment. We need to create applications that are helpful, predictable and reliable that will produce acceptable results, in place of endless toil to get these agents to do something. We really just need good, specific LLM products that can do at least one thing properly, like - doing some research, writing a report, summarizing content - things an LLM might actually be good at.
 So we thought it would be a good idea to create a framework that makes use of LoopGPT agent's memory and custom tooling capabilities. Let's jump right into the new features of this framework.
 First, using LLMs within Python functions, where you only write the function's docstring and the LLM will return the resu…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Machine Learning: The Silent Revolution in Our Midst]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/152aq9f/d_machine_learning_the_silent_revolution_in_our/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/152aq9f/d_machine_learning_the_silent_revolution_in_our/"/>
        <updated>2023-07-17T19:02:05.000Z</updated>
        <summary type="html"><![CDATA[Hello, fellow machine learning aficionados!
 As everyone is aware, machine learning is transforming a wide range of sectors, including healthcare, banking, entertainment, and transportation. But have you ever stopped to think about the more subtle effects it's causing in our day-to-day activities?
 Think about this Machine learning is the technology behind the targeted advertisements you see online, the intelligent email client suggestions for replies, the traffic predictions on your GPS, and even the song suggestions on your favorite music app.
 But this is where things become intriguing. I'm interested in hearing about the most imperceptible yet significant ways you've seen machine learning in action in your day-to-day activities. It could be as straightforward as a practical component in an app you frequently used or a big shift in your work process.
 Here's my observation to start the discussion: Thanks to machine learning, I've seen that over time, my smart home appliances have gotten better at comprehending my orders. I now seldom ever have to repeat myself, and it seems like the gadgets are actually "learning" what I want.
 I'm eager to hear your insights. Let's explore machine learning's covert revolution together, eh?
    submitted by    /u/HungryGuidence  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] OnnxStream: running Stable Diffusion in 260MB of RAM]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/152ago3/p_onnxstream_running_stable_diffusion_in_260mb_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/152ago3/p_onnxstream_running_stable_diffusion_in_260mb_of/"/>
        <updated>2023-07-17T18:52:14.000Z</updated>
        <summary type="html"><![CDATA[hi all,
 I developed a small inference library in C++ that can run Stable Diffusion in 260MB of RAM.
 The minimum recommended RAM/VRAM for SD is 8GB.
 This is achieved by offloading the weights on disk, by quantization and attention slicing (which is similar in principle to FlashAttention, without the fused kernel).
 It currently supports 24 ONNX operators.
 The idea is to allow the inference of very large (transformer) models on very limited devices.
 More info in the GitHub repo: https://github.com/vitoplantamura/OnnxStream
 Thanks, --Vito
    submitted by    /u/Pristine198  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Copy Is All You Need]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/152a3rr/copy_is_all_you_need/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/152a3rr/copy_is_all_you_need/"/>
        <updated>2023-07-17T18:38:41.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Moments of Tukey’s g-and-h distribution]]></title>
        <id>https://www.johndcook.com/blog/?p=198554</id>
        <link href="https://www.johndcook.com/blog/2023/07/17/tukey-g-and-h/"/>
        <updated>2023-07-17T18:16:15.000Z</updated>
        <summary type="html"><![CDATA[John Tukey developed his so-called g-and-h distribution to be very flexible, having a wide variety of possible values of skewness and kurtosis. Although the reason for the distribution’s existence is its range of possible skewness and values, calculating the skewness and kurtosis of the distribution is not simple. Definition Let φ be the function of […]
Moments of Tukey’s g-and-h distribution first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChatGPT is an example of indoctrination]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1527xx1/chatgpt_is_an_example_of_indoctrination/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1527xx1/chatgpt_is_an_example_of_indoctrination/"/>
        <updated>2023-07-17T17:15:47.000Z</updated>
        <summary type="html"><![CDATA[They disrupted its neural network to force it to give predetermined answers when asked certain questions instead of allowing it to think independently.
    submitted by    /u/LinsaFTW  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Optimizing AI prompt]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1526jq3/d_optimizing_ai_prompt/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1526jq3/d_optimizing_ai_prompt/"/>
        <updated>2023-07-17T16:22:38.000Z</updated>
        <summary type="html"><![CDATA[Hey, everyone! Been thinking about how we interact with AI, especially in the realm of text generation. It's no secret that the way we prompt an AI greatly influences the output. A perfectly crafted prompt can result in a well-constructed piece of writing, while a vague or poorly worded one might leave us with gibberish or content that misses the mark.
 Recently, I've been intrigued by the idea of 'Prompt Engineering.' We've seen AI models grow more powerful, more human-like, and they're getting involved in content creation in a big way. There are AI-powered tools and applications being used in journalism, blogging, script writing, technical writing, and so much more.
 With the rise of powerful models like GPT-3.5, DALL-E 2, and others, it seems the ability to create optimal prompts has become an art and science unto itself.
 What's your take on this? Do you think there's value in perfecting the art of prompting AI? Or do you feel AI should evolve to understand human language and context better, regardless of how a question or command is framed? Could the emergence of intuitive tools that assist with prompt optimization help bridge this gap, making AI-generated content more accessible and higher quality?
 As content creators, developers, or just AI enthusiasts, how do you think this will shape the future of AI-generated content?
    submitted by    /u/IntentlyConscious  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLMs: Does human text data make generative AI an entity?]]></title>
        <id>https://www.datasciencecentral.com/?p=62666</id>
        <link href="https://www.datasciencecentral.com/llms-does-human-text-data-make-generative-ai-an-entity/"/>
        <updated>2023-07-17T16:12:51.000Z</updated>
        <summary type="html"><![CDATA[There is a recent interview, The Ethical Puzzle of Sentient AI, where a professor said, “But there’s also the problem that I’ve called the ‘gaming problem’ — that when the system has access to trillions of words of training data, and has been trained with the goal of mimicking human behavior, the sorts of behavior patterns… Read More »LLMs: Does human text data make generative AI an entity?
The post LLMs: Does human text data make generative AI an entity? appeared first on Data Science Central.]]></summary>
        <author>
            <name>David Stephen</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Chapyter: ChatGPT Code Interpreter in Jupyter Notebooks]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15269v8/p_chapyter_chatgpt_code_interpreter_in_jupyter/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15269v8/p_chapyter_chatgpt_code_interpreter_in_jupyter/"/>
        <updated>2023-07-17T16:12:12.000Z</updated>
        <summary type="html"><![CDATA[I recently made a new JupyterLab extension called Chapyter (𝐂𝐡𝐚ts in Ju𝐏𝐲𝐭𝐞𝐫) that aims at solving many pain points when using other AI coding assistants. I want to share with y'all the tools as well as my thinkings while building this.
 What is Chapyter
 Chapyter is a JupyterLab extension that seamlessly connects GPT-4 to your coding environment. Here are the key features: 
  
Code generation from natural language and automatic execution
 Simply adding the magic command %%chat at the beginning of the cell of a natural language description of the task, the code is generated and the results are shown in a few seconds.
  
https://i.redd.it/y7l0s9pf5hcb1.gif
  
Using coding history and execution output for code generation
 By adding the --history or -h flag in generation, chapyter can…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time analytics]]></title>
        <id>https://www.datasciencecentral.com/?p=62664</id>
        <link href="https://www.datasciencecentral.com/real-time-analytics/"/>
        <updated>2023-07-17T16:09:00.000Z</updated>
        <summary type="html"><![CDATA[The modern enterprise is insight-driven, or, at least, aims to be. Historically, those insights were found in a data warehouse or data lake, populated with scheduled feeds and analysts, working feverishly over them. Feeds had plenty of bandwidth, but high latency. Think an 18-wheeler loaded with hard drives, driving from London to Birmingham. Nowadays, insights… Read More »Real-time analytics
The post Real-time analytics appeared first on Data Science Central.]]></summary>
        <author>
            <name>Markus Buhmann</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Obsolescence of stock images due to AI image generation]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1525v6e/obsolescence_of_stock_images_due_to_ai_image/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1525v6e/obsolescence_of_stock_images_due_to_ai_image/"/>
        <updated>2023-07-17T15:57:18.000Z</updated>
        <summary type="html"><![CDATA[Something that I have been thinking about with regards to AI's affects in the future is the effect that increasingly advanced AI image generation will have on stock images. Stock images are commonly used in media of various kinds, as licensing said images is much easier than hiring people to take unique pictures.
 However, since AI can now be used to generate images, it's quite possible that there will come a time when stock images will become obsolete, as it will become cheaper/easier to simply use AI image generation to produce faux stock images that look real. Thoughts?
    submitted by    /u/TheLobsterCopter5000  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI ushers in a new era of mental health monitoring]]></title>
        <id>https://www.datasciencecentral.com/?p=62656</id>
        <link href="https://www.datasciencecentral.com/ai-ushers-in-a-new-era-of-mental-health-monitoring/"/>
        <updated>2023-07-17T15:52:29.000Z</updated>
        <summary type="html"><![CDATA[AI Ushers in a New Era of Mental Health Monitoring Important Data Points: AI’s Role in Mental Healthcare Transformation – It can be safe to say that AI is driving a significant transformation in mental healthcare, promising more accessible, economical, and effective treatments. The Emerging Role of Technology and Artificial Intelligence As the modern world… Read More »AI ushers in a new era of mental health monitoring
The post AI ushers in a new era of mental health monitoring appeared first on Data Science Central.]]></summary>
        <author>
            <name>Devarati Sarkar</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data science vs web development: What’s the difference?]]></title>
        <id>https://www.datasciencecentral.com/?p=62654</id>
        <link href="https://www.datasciencecentral.com/data-science-vs-web-development-whats-the-difference/"/>
        <updated>2023-07-17T15:46:42.000Z</updated>
        <summary type="html"><![CDATA[If you’ve spent any time in the tech community in the last few years, you’ll have noticed the recent explosion in interest in both data science and web development. Young people interested in a career in tech are increasingly turning to careers as data scientists or web developers.  The importance of web development should be… Read More »Data science vs web development: What’s the difference?
The post Data science vs web development: What’s the difference? appeared first on Data Science Central.]]></summary>
        <author>
            <name>Gregory Batchelor</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[are there any good free ai girlfriends/boyfriends?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/15231ma/are_there_any_good_free_ai_girlfriendsboyfriends/</id>
        <link href="https://www.reddit.com/r/artificial/comments/15231ma/are_there_any_good_free_ai_girlfriendsboyfriends/"/>
        <updated>2023-07-17T14:09:44.000Z</updated>
        <summary type="html"><![CDATA[or any that are worth the money? Im just super curious about them. In fact, I kind of want to get a female one even though Im a straight female to learn game from her haha but yeah just wanting to check them out it fascinates me thanks!
    submitted by    /u/DragonflyAromatic793  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Website builder AI with export option]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1522dfs/website_builder_ai_with_export_option/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1522dfs/website_builder_ai_with_export_option/"/>
        <updated>2023-07-17T13:43:35.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 ​
 Do you know of Website Builder with AI who offres an export option.
 I want to use it to have a blueprint of the website and then host it somewhere else
 ​
 Thank you
    submitted by    /u/CyprienFME  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Finetuning qLoRAs for production use cases - Paraphrasing, Changing the tone of a sentence, Dialogue Summarization and Topic generation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1521mly/p_finetuning_qloras_for_production_use_cases/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1521mly/p_finetuning_qloras_for_production_use_cases/"/>
        <updated>2023-07-17T13:13:00.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 I've been curious as to how far we can take small(7B and less) models for production use cases with small amounts of training data for each task.
 So far I've been able to fine-tune LoRAs for paraphrasing, changing the tone of a sentence, dialogue summarization and topic generation. The results look promising, especially the fact that all this can run on very modest hardware.
 Finetuning was done in 4bit mode using bitsandbytes. Each task had ~1k training points.
 I've used a AMD Ryzen9 3900XT + 3080(10gb) + 32gb ram for all the training and inference here. On my system I get 12-15 tokens/sec during inference.
 All the details can be found here: https://github.com/kuutsav/llm-toys.
  
Data used for training
 Training params and the training/eval losses are present in the huggingface model cards
 Evaluation(wherever possible atm)
  
Models: https://huggingface.co/llm-toys
 Why do all this?
 Mostly to answer the question - can we move away from OpenAI and other players for very particular use cases, how much data it takes, where does it break, etc. So far I've not been able to find pre-trained model(7b and small) that did well on these tasks. Even larger models(around 40b) failed to give consistent results. The fine-tuned model on huggingface were also not good enough in my trials. For paraphrasing I could not find even a single fully tuned model that was able to correct basic typos.
 Do give it a shot, there is a colab notebook available as well try it directly. Will really appreciate some feedback on these model's performace.
    submitted by    /u/krumb0y  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Innovative Project : Blockchain Anomaly Detection System - DeHack]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1521d3m/p_innovative_project_blockchain_anomaly_detection/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1521d3m/p_innovative_project_blockchain_anomaly_detection/"/>
        <updated>2023-07-17T13:02:15.000Z</updated>
        <summary type="html"><![CDATA[We're DeHack, a Web 3.0 security startup in Dubai. We're looking for a Machine Learning enthusiast who understands blockchain. Part-time or full-time.
 We're the team behind BlockAudit, now building DeHack - Threat intelligence and mitigation product. We're at an exciting stage with venture funding talks underway. It's a huge opportunity for someone who wants to work at the intersection of ML & Web 3.0. If you've worked on Threat Anomaly detection models, even better.
 For the perfect fit, we're open to discussing equity compensation as part of the package.
 Sounds interesting? Get in touch!
 www.DeHack.ai
 akshay@dehack.ai
 TG: u/DeHack_Akshay
    submitted by    /u/Ok_Ear_7544  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How best to benchmark the accuracy of a model for comparing different tokenizers? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1521ag5/how_best_to_benchmark_the_accuracy_of_a_model_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1521ag5/how_best_to_benchmark_the_accuracy_of_a_model_for/"/>
        <updated>2023-07-17T12:59:33.000Z</updated>
        <summary type="html"><![CDATA[I need to benchmark the performance of my tokenizer against standard tokenizers. It would be best for reproducibility if I benchmark against an existing model on a standard benchmark, swapping out the existing tokenizer for my tokenizer.
 I was planning to train TinyStories model for the comparison, but what would I benchmark other than perplexity? Is comparing perplexity enough to benchmark the performance of two models trained on the same dataset? Or what is best for that?
 Can anyone recommend a repo (if any exist) that:
  
Pretrains a transformer based model from scratch.
 Has some kind of accuracy benchmark that will be taken seriously.
 Can be modified to use a different tokenizer.
 Can be pretrained on an RTX 3090 within 24-48 hours.
  
If there's a repo somewhere that both pretrains on a benchmark dataset and applies a suitable benchmark automatically that would be amazing.
 As you can tell I'm unsure how best to go about doing the benchmark. Any advice would be appreciated.
    submitted by    /u/Pan000  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I am looking for self-hosted AI implementations that I can train on emails, PDFs, and MS Office documents]]></title>
        <id>https://www.reddit.com/r/artificial/comments/151zgdb/i_am_looking_for_selfhosted_ai_implementations/</id>
        <link href="https://www.reddit.com/r/artificial/comments/151zgdb/i_am_looking_for_selfhosted_ai_implementations/"/>
        <updated>2023-07-17T11:36:47.000Z</updated>
        <summary type="html"><![CDATA[OpenAI's ChatGPT, Google's Bard, Anthropic's Claude, and Microsoft's Being are all nice freemium tools, but let's be honest, we don't know what they do with our information. Especially for work-related topics we are strictly prohibited from sharing anything on those platforms, for good reasons. So I am wondering if I can find any Free, Libre, and Open Source Software that I can self-host. I want to train it on emails, meeting transcripts, PDFs, and Microsoft Office documents. What I need from the software:
  
I can give it a long PDF or MS Office document and it answers some questions like making a summary, listing some requirements, and some instructions to do something according to that document
 make a summary of the sessions, create a list of open issues with deadlines and people responsible, helping to maintain Kanban boards related to that project...
 anonymize textual content so I can use those content later in the freemium software on the internet...
 Indexing information, so I ask a question and it points to the email or document where I can find information about that topic 
  
Do we have anything like this available today or am I asking this question too early?
    submitted by    /u/foadsf  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Prompt Performance Prediction]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/151z8pt/r_prompt_performance_prediction/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/151z8pt/r_prompt_performance_prediction/"/>
        <updated>2023-07-17T11:26:15.000Z</updated>
        <summary type="html"><![CDATA[Let me introduce you to our latest research on Prompt Performance Prediction (PPP). PPP is a novel task which aims to predict a query's performance in Generative Information Retrieval systems before the search results are generated. This can be applied on any generative system (textual, image, etc.). 
 Here we consider the image generation task as a generative retrieval one and adapt the well known query performance prediction in traditional information retrieval field to modern generative information retrieval. 
 Preliminary results across three datasets (Dall-E, Midjourney, Stable Diffusion) on different metrics (Aesthetic, memorability, etc.) show promising capabilities of our method in performance prediction. 🔗 For a more detailed look, visit: https://arxiv.org/abs/2306.08915
 Prompt Performance Prediction for Generative IR, Bizzozzero, Bendidi, Risser-Maroix, 2023
 AI #GenerativeAI #MachineLearning #PromptPerformancePrediction #PPP
    submitted by    /u/Average_CS_Student  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to creat PPO agent from 0]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/151z8od/how_to_creat_ppo_agent_from_0/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/151z8od/how_to_creat_ppo_agent_from_0/"/>
        <updated>2023-07-17T11:26:12.000Z</updated>
        <summary type="html"><![CDATA[Hello ladies and gentlemen, I would love to ask you any guidance towards PPO agent creation. Any courses, GitHubs, anything works for me if it helps me to understand it and creat it. Thank you. Have a nice day
    submitted by    /u/EveryonehatesLin3lis  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RLlib multi-agent actions received from trained agent using compute_actions() and compute_single_action() out of action space bounds]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/151y36h/rllib_multiagent_actions_received_from_trained/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/151y36h/rllib_multiagent_actions_received_from_trained/"/>
        <updated>2023-07-17T10:27:26.000Z</updated>
        <summary type="html"><![CDATA[I trained a MARL agent using PPO in RLlib where each agent had a Box([-1,-1,0], [1,1,1], (3,), float64) action space, with 6 agents. The agent during training was sampling and selecting actions within the action space bounds for each agent. But after training for about 7 milllion iterations, and during playback, selecting actions based on the observation using compute_single_action() and compute_actions() returns actions for the agents which are grossly outside the action space bounds of -1 to 1. I receive actions like [-6,-7,2] etc for the agents, which does not fare well to how the actions translate to the agent behaving in the environment. 
 I have tried training with additional post_fcnet_activation (tanh) but that did not help either. Using clip_actions=True in compute_actions() does not solve the issue either. The selected actions seem to be exceeding the bounds by larger margins the more complex the environment gets. For example, with 2 drones and a simpler environment the trained agent returns actions around [-1.5,-1.1,0.4] while for 6 agents I get actions like [-6,-7,2]. I have used RLlib before with Discrete action spaces and this does not occur. Is it a problem with the Box space? I use a custom model with different Fully Connected models for the action and value functions. 
 Has anybody encountered this problem before and discovered a possible solution? 
    submitted by    /u/Acceptable_Set_4392  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[If the human brain can process 50-400 bytes per second of data consciously, from the sense acquisition and subconscious... How many bps can a GPT type AI process consciously? zero? I have no idea of the logical bases to approach this question.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/151xghe/if_the_human_brain_can_process_50400_bytes_per/</id>
        <link href="https://www.reddit.com/r/artificial/comments/151xghe/if_the_human_brain_can_process_50400_bytes_per/"/>
        <updated>2023-07-17T09:54:15.000Z</updated>
        <summary type="html"><![CDATA[How can we compare the concious focus of AI compared to a human. Does it have any kind of awareness of what it is focusing on? What is awareness even? knowledge of the passage of time? 
 https://thinkbynumbers.org/psychology/subconscious-processes-27500-times-more-data-than-the-conscious-mind/
    submitted by    /u/MegavirusOfDoom  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Looking for a collaborator to write a specific machine learning application section in a statistics paper that's almost finished]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/151wjvt/p_looking_for_a_collaborator_to_write_a_specific/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/151wjvt/p_looking_for_a_collaborator_to_write_a_specific/"/>
        <updated>2023-07-17T09:02:34.000Z</updated>
        <summary type="html"><![CDATA[The following offer might be more suited for a research-oriented site like math stack exchange/overflow, but I don't think they allow posts like this, so here I am.
 Me (a postdoc, the main author) and two other co-authors (legit academics) have written a statistics paper where we develop a new smoothing technique on half-spaces. The paper is almost done except for one section that's currently (almost) empty. In that section, we would like to show how the smoothing technique can be used to classify new data points in the context of soft-margin support vector machines (SVM). The aim would be something like 2-3 pages with 1-2 figures, but the collaborator would have the freedom to do what he/she thinks is best.
 So I am looking for someone who has more experience with machine learning or just SVMs to fill up this section themselves. They would of course become co-author of the paper. I cannot guarantee anything, but we aim to publish the paper in a low Q1 journal, so a good journal.
 If someone is hungry for publications (PhD student, postdoc, young prof) and you have experience with this kind of stuff, this is a relatively low-effort way to upgrade your CV.
 If you're interested, just PM me, more details will be given.
    submitted by    /u/Nearby-Turnover370  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are there any alternatives to Character.Ai that I don’t have to give my information to?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/151vpnx/are_there_any_alternatives_to_characterai_that_i/</id>
        <link href="https://www.reddit.com/r/artificial/comments/151vpnx/are_there_any_alternatives_to_characterai_that_i/"/>
        <updated>2023-07-17T08:14:15.000Z</updated>
        <summary type="html"><![CDATA[Character.ai is really interesting, but it’s unfair and last time I put my login information into a different Ai company site, they never stopped emailing me.
    submitted by    /u/Suitable-Ad-8176  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Need Help in Llama license for research paper]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/151vka9/r_need_help_in_llama_license_for_research_paper/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/151vka9/r_need_help_in_llama_license_for_research_paper/"/>
        <updated>2023-07-17T08:05:17.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone,
 We are conducting benchmark evaluations on large language models, and the preliminary results are quite interesting for AI researchers to investigate further. We have tested various models, including LLama variants, but unfortunately, we are unable to use LLama at this time due to licensing restrictions.
 We have applied for the necessary license from Meta multiple times over the past few months but have not received a reply. If anyone has an existing LLama license they would be willing to share, we would greatly appreciate the help. In exchange, we would be happy to share a preprint of the paper and acknowledge your contribution.
 We understand this is an unconventional request, but licensing can be a difficult roadblock in research. Any assistance would allow us to better understand the capabilities of different models. Please let us know if you can help.
 Thank you for considering!
    submitted by    /u/Accomplished_Rest_16  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cool AI voiceover editing site]]></title>
        <id>https://www.reddit.com/r/artificial/comments/151v660/cool_ai_voiceover_editing_site/</id>
        <link href="https://www.reddit.com/r/artificial/comments/151v660/cool_ai_voiceover_editing_site/"/>
        <updated>2023-07-17T07:43:09.000Z</updated>
        <summary type="html"><![CDATA[Came across this cool voiceover AI thing that has cool video editing features too, pretty underrated haven’t heard many people talk about it. Here’s the link for that https://www.acoust.io/
    submitted by    /u/Snoo-30922  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Donut Base Model Usage]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/151u6pk/d_donut_base_model_usage/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/151u6pk/d_donut_base_model_usage/"/>
        <updated>2023-07-17T06:48:49.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone, Is there any way we can use the Donut base model for its original Pre-Training task i.e pure OCR output without any specific fine-tuning head. I could find the base model on hub, but I don't know the exact configuration to use for the generate method or even for decoder.
    submitted by    /u/Quicksilver466  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MuZero implementations for Atari?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/151tzyf/muzero_implementations_for_atari/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/151tzyf/muzero_implementations_for_atari/"/>
        <updated>2023-07-17T06:38:45.000Z</updated>
        <summary type="html"><![CDATA[I was wondering if there are any actually working MuZero implementations for Atari games out there?
 None of the ones I found are working (at all) on Atari games. This includes:
  
The most popular repo https://github.com/werner-duvaud/muzero-general
 It works on other games but not Atari. There are many GitHub issues where people are complaining about this.
 This one which is less popular https://github.com/koulanurag/muzero-pytorch, which apparently doesn't include Atari games.
  
Alternatively, do you know other MuZero-like algorithms which are implemented and working on Atari?
 ​
    submitted by    /u/__horned_owl__  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] open source lip synchronize project]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/151r896/d_open_source_lip_synchronize_project/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/151r896/d_open_source_lip_synchronize_project/"/>
        <updated>2023-07-17T04:11:51.000Z</updated>
        <summary type="html"><![CDATA[Which open source project is recommended for creating an app that can synchronize a person's lip movements in a video with different audio? 
 I'm looking for recommendations in the machine learning community. I want to build an app that can synchronize a person's lip movements in a video with different audio. Are there any open source projects you would suggest for this task? I appreciate any insights or suggestions. Thank you!
    submitted by    /u/Overall-Spare2157  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best offline local AI tools]]></title>
        <id>https://www.reddit.com/r/artificial/comments/151qx7r/best_offline_local_ai_tools/</id>
        <link href="https://www.reddit.com/r/artificial/comments/151qx7r/best_offline_local_ai_tools/"/>
        <updated>2023-07-17T03:57:11.000Z</updated>
        <summary type="html"><![CDATA[Hi! I'm new here! Just wondering if there is a list of offline AI tools that can be installed locally (linux preferrably) on my computer? Something similar to koboldcpp for text gen or automatic1111 for image gen? I am trying to search for a list for a few hours now but cannot find any. Thanks community!
    submitted by    /u/Spirited_Employee_61  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Zig GPT-2 inference engine]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/151qo4n/p_zig_gpt2_inference_engine/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/151qo4n/p_zig_gpt2_inference_engine/"/>
        <updated>2023-07-17T03:44:27.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Cautious_Garbage_740  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ISO AI generated adhan (Muslim call to prayer)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/151obe3/iso_ai_generated_adhan_muslim_call_to_prayer/</id>
        <link href="https://www.reddit.com/r/artificial/comments/151obe3/iso_ai_generated_adhan_muslim_call_to_prayer/"/>
        <updated>2023-07-17T01:49:44.000Z</updated>
        <summary type="html"><![CDATA[Hi y’all. Never visited this sub before, hopefully this is allowed. I’m trying to find an AI that can match the tone and style of an adhan (Islamic call to prayer) but with different words. Haven’t had any luck with more generic text to speech AI, so I’m just curious if anyone here as come across anything like that.
    submitted by    /u/istillplaykotor  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention Schema in Neural Agents. (arXiv:2305.17375v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.17375</id>
        <link href="http://arxiv.org/abs/2305.17375"/>
        <updated>2023-07-17T01:05:36.235Z</updated>
        <summary type="html"><![CDATA[Attention has become a common ingredient in deep learning architectures. It
adds a dynamical selection of information on top of the static selection of
information supported by weights. In the same way, we can imagine a
higher-order informational filter built on top of attention: an Attention
Schema (AS), namely, a descriptive and predictive model of attention. In
cognitive neuroscience, Attention Schema Theory (AST) supports this idea of
distinguishing attention from AS. A strong prediction of this theory is that an
agent can use its own AS to also infer the states of other agents' attention
and consequently enhance coordination with other agents. As such, multi-agent
reinforcement learning would be an ideal setting to experimentally test the
validity of AST. We explore different ways in which attention and AS interact
with each other. Our preliminary results indicate that agents that implement
the AS as a recurrent internal control achieve the best performance. In
general, these exploratory experiments suggest that equipping artificial agents
with a model of attention can enhance their social intelligence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dianbo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bolotta_S/0/1/0/all/0/1"&gt;Samuele Bolotta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;He Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dumas_G/0/1/0/all/0/1"&gt;Guillaume Dumas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Synthetic Electrocardiogram (ECG) Image Generation Toolbox to Facilitate Deep Learning-Based Scanned ECG Digitization. (arXiv:2307.01946v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.01946</id>
        <link href="http://arxiv.org/abs/2307.01946"/>
        <updated>2023-07-17T01:05:36.203Z</updated>
        <summary type="html"><![CDATA[The electrocardiogram (ECG) is an accurate and widely available tool for
diagnosing cardiovascular diseases. ECGs have been recorded in printed formats
for decades and their digitization holds great potential for training machine
learning (ML) models in algorithmic ECG diagnosis. Physical ECG archives are at
risk of deterioration and scanning printed ECGs alone is insufficient, as ML
models require ECG time-series data. Therefore, the digitization and conversion
of paper ECG archives into time-series data is of utmost importance. Deep
learning models for image processing show promise in this regard. However, the
scarcity of ECG archives with reference time-series is a challenge. Data
augmentation techniques utilizing \textit{digital twins} present a potential
solution.

We introduce a novel method for generating synthetic ECG images on standard
paper-like ECG backgrounds with realistic artifacts. Distortions including
handwritten text artifacts, wrinkles, creases and perspective transforms are
applied to the generated images, without personally identifiable information.
As a use case, we generated an ECG image dataset of 21,801 records from the
12-lead PhysioNet PTB-XL ECG time-series dataset. A deep ECG image digitization
model was built and trained on the synthetic dataset, and was employed to
convert the synthetic images to time-series data for evaluation. The
signal-to-noise ratio (SNR) was calculated to assess the image digitization
quality vs the ground truth ECG time-series. The results show an average signal
recovery SNR of 27$\pm$2.8\,dB, demonstrating the significance of the proposed
synthetic ECG image dataset for training deep learning models. The codebase is
available as an open-access toolbox for ECG research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shivashankara_K/0/1/0/all/0/1"&gt;Kshama Kodthalu Shivashankara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shervedani_A/0/1/0/all/0/1"&gt;Afagh Mehri Shervedani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sameni_R/0/1/0/all/0/1"&gt;Reza Sameni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lipschitzness Effect of a Loss Function on Generalization Performance of Deep Neural Networks Trained by Adam and AdamW Optimizers. (arXiv:2303.16464v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.16464</id>
        <link href="http://arxiv.org/abs/2303.16464"/>
        <updated>2023-07-17T01:05:36.195Z</updated>
        <summary type="html"><![CDATA[The generalization performance of deep neural networks with regard to the
optimization algorithm is one of the major concerns in machine learning. This
performance can be affected by various factors. In this paper, we theoretically
prove that the Lipschitz constant of a loss function is an important factor to
diminish the generalization error of the output model obtained by Adam or
AdamW. The results can be used as a guideline for choosing the loss function
when the optimization algorithm is Adam or AdamW. In addition, to evaluate the
theoretical bound in a practical setting, we choose the human age estimation
problem in computer vision. For assessing the generalization better, the
training and test datasets are drawn from different distributions. Our
experimental evaluation shows that the loss function with a lower Lipschitz
constant and maximum value improves the generalization of the model trained by
Adam or AdamW.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lashkari_M/0/1/0/all/0/1"&gt;Mohammad Lashkari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gheibi_A/0/1/0/all/0/1"&gt;Amin Gheibi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion. (arXiv:2303.14863v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.14863</id>
        <link href="http://arxiv.org/abs/2303.14863"/>
        <updated>2023-07-17T01:05:36.160Z</updated>
        <summary type="html"><![CDATA[We propose a new formulation of temporal action detection (TAD) with
denoising diffusion, DiffTAD in short. Taking as input random temporal
proposals, it can yield action proposals accurately given an untrimmed long
video. This presents a generative modeling perspective, against previous
discriminative learning manners. This capability is achieved by first diffusing
the ground-truth proposals to random ones (i.e., the forward/noising process)
and then learning to reverse the noising process (i.e., the backward/denoising
process). Concretely, we establish the denoising process in the Transformer
decoder (e.g., DETR) by introducing a temporal location query design with
faster convergence in training. We further propose a cross-step selective
conditioning algorithm for inference acceleration. Extensive evaluations on
ActivityNet and THUMOS show that our DiffTAD achieves top performance compared
to previous art alternatives. The code will be made available at
https://github.com/sauradip/DiffusionTAD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1"&gt;Sauradip Nag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jiankang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yi-Zhe Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1"&gt;Tao Xiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLIPood: Generalizing CLIP to Out-of-Distributions. (arXiv:2302.00864v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.00864</id>
        <link href="http://arxiv.org/abs/2302.00864"/>
        <updated>2023-07-17T01:05:36.149Z</updated>
        <summary type="html"><![CDATA[Out-of-distribution (OOD) generalization, where the model needs to handle
distribution shifts from training, is a major challenge of machine learning.
Contrastive language-image pre-training (CLIP) models have shown impressive
zero-shot ability, but the further adaptation of CLIP on downstream tasks
undesirably degrades OOD performances. This paper aims at generalizing CLIP to
out-of-distribution test data on downstream tasks. We propose CLIPood, a
fine-tuning method that can adapt CLIP models to OOD situations where both
domain shifts and open classes may occur on the unseen test data. To exploit
the semantic relations between classes from the text modality, CLIPood
introduces a new training objective, margin metric softmax (MMS), with class
adaptive margins for fine-tuning. To incorporate both pre-trained zero-shot
model and fine-tuned task-adaptive model, CLIPood leverages a new optimization
strategy, Beta moving average (BMA), to maintain a temporal ensemble weighted
by Beta distribution. Experiments on diverse datasets with different OOD
scenarios show that CLIPood consistently outperforms existing generalization
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1"&gt;Yang Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xingzhuo Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jialong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Ximei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianmin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1"&gt;Mingsheng Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DoCoFL: Downlink Compression for Cross-Device Federated Learning. (arXiv:2302.00543v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.00543</id>
        <link href="http://arxiv.org/abs/2302.00543"/>
        <updated>2023-07-17T01:05:36.127Z</updated>
        <summary type="html"><![CDATA[Many compression techniques have been proposed to reduce the communication
overhead of Federated Learning training procedures. However, these are
typically designed for compressing model updates, which are expected to decay
throughout training. As a result, such methods are inapplicable to downlink
(i.e., from the parameter server to clients) compression in the cross-device
setting, where heterogeneous clients $\textit{may appear only once}$ during
training and thus must download the model parameters. Accordingly, we propose
$\textsf{DoCoFL}$ -- a new framework for downlink compression in the
cross-device setting. Importantly, $\textsf{DoCoFL}$ can be seamlessly combined
with many uplink compression schemes, rendering it suitable for bi-directional
compression. Through extensive evaluation, we show that $\textsf{DoCoFL}$
offers significant bi-directional bandwidth reduction while achieving
competitive accuracy to that of a baseline without any compression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dorfman_R/0/1/0/all/0/1"&gt;Ron Dorfman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vargaftik_S/0/1/0/all/0/1"&gt;Shay Vargaftik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Itzhak_Y/0/1/0/all/0/1"&gt;Yaniv Ben-Itzhak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levy_K/0/1/0/all/0/1"&gt;Kfir Y. Levy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Stochastic Gradient Descent with Low-Noise. (arXiv:2209.04188v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2209.04188</id>
        <link href="http://arxiv.org/abs/2209.04188"/>
        <updated>2023-07-17T01:05:36.121Z</updated>
        <summary type="html"><![CDATA[Modern machine learning algorithms aim to extract fine-grained information
from data to provide accurate predictions, which often conflicts with the goal
of privacy protection. This paper addresses the practical and theoretical
importance of developing privacy-preserving machine learning algorithms that
ensure good performance while preserving privacy. In this paper, we focus on
the privacy and utility (measured by excess risk bounds) performances of
differentially private stochastic gradient descent (SGD) algorithms in the
setting of stochastic convex optimization. Specifically, we examine the
pointwise problem in the low-noise setting for which we derive sharper excess
risk bounds for the differentially private SGD algorithm. In the pairwise
learning setting, we propose a simple differentially private SGD algorithm
based on gradient perturbation. Furthermore, we develop novel utility bounds
for the proposed algorithm, proving that it achieves optimal excess risk rates
even for non-smooth losses. Notably, we establish fast learning rates for
privacy-preserving pairwise learning under the low-noise condition, which is
the first of its kind.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_P/0/1/0/all/0/1"&gt;Puyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lei_Y/0/1/0/all/0/1"&gt;Yunwen Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ying_Y/0/1/0/all/0/1"&gt;Yiming Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Ding-Xuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stream-based active learning with linear models. (arXiv:2207.09874v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2207.09874</id>
        <link href="http://arxiv.org/abs/2207.09874"/>
        <updated>2023-07-17T01:05:36.116Z</updated>
        <summary type="html"><![CDATA[The proliferation of automated data collection schemes and the advances in
sensorics are increasing the amount of data we are able to monitor in
real-time. However, given the high annotation costs and the time required by
quality inspections, data is often available in an unlabeled form. This is
fostering the use of active learning for the development of soft sensors and
predictive models. In production, instead of performing random inspections to
obtain product information, labels are collected by evaluating the information
content of the unlabeled data. Several query strategy frameworks for regression
have been proposed in the literature but most of the focus has been dedicated
to the static pool-based scenario. In this work, we propose a new strategy for
the stream-based scenario, where instances are sequentially offered to the
learner, which must instantaneously decide whether to perform the quality check
to obtain the label or discard the instance. The approach is inspired by the
optimal experimental design theory and the iterative aspect of the
decision-making process is tackled by setting a threshold on the
informativeness of the unlabeled data points. The proposed approach is
evaluated using numerical simulations and the Tennessee Eastman Process
simulator. The results confirm that selecting the examples suggested by the
proposed algorithm allows for a faster reduction in the prediction error.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Cacciarelli_D/0/1/0/all/0/1"&gt;Davide Cacciarelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kulahci_M/0/1/0/all/0/1"&gt;Murat Kulahci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tyssedal_J/0/1/0/all/0/1"&gt;John S&amp;#xf8;lve Tyssedal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.05695</id>
        <link href="http://arxiv.org/abs/2307.05695"/>
        <updated>2023-07-17T01:05:36.110Z</updated>
        <summary type="html"><![CDATA[Despite the dominance and effectiveness of scaling, resulting in large
networks with hundreds of billions of parameters, the necessity to train
overparametrized models remains poorly understood, and alternative approaches
do not necessarily make it cheaper to train high-performance models. In this
paper, we explore low-rank training techniques as an alternative approach to
training large neural networks. We introduce a novel method called ReLoRA,
which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to
pre-training transformer language models with up to 350M parameters and
demonstrate comparable performance to regular neural network training.
Furthermore, we observe that the efficiency of ReLoRA increases with model
size, making it a promising approach for training multi-billion-parameter
networks efficiently. Our findings shed light on the potential of low-rank
training techniques and their implications for scaling laws.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lialin_V/0/1/0/all/0/1"&gt;Vladislav Lialin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shivagunde_N/0/1/0/all/0/1"&gt;Namrata Shivagunde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muckatira_S/0/1/0/all/0/1"&gt;Sherin Muckatira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1"&gt;Anna Rumshisky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dink-Net: Neural Clustering on Large Graphs. (arXiv:2305.18405v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.18405</id>
        <link href="http://arxiv.org/abs/2305.18405"/>
        <updated>2023-07-17T01:05:35.922Z</updated>
        <summary type="html"><![CDATA[Deep graph clustering, which aims to group the nodes of a graph into disjoint
clusters with deep neural networks, has achieved promising progress in recent
years. However, the existing methods fail to scale to the large graph with
million nodes. To solve this problem, a scalable deep graph clustering method
(Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by
discriminating nodes, whether being corrupted by augmentations, representations
are learned in a self-supervised manner. Meanwhile, the cluster centres are
initialized as learnable neural parameters. Subsequently, the clustering
distribution is optimized by minimizing the proposed cluster dilation loss and
cluster shrink loss in an adversarial manner. By these settings, we unify the
two-step clustering, i.e., representation learning and clustering optimization,
into an end-to-end framework, guiding the network to learn clustering-friendly
features. Besides, Dink-Net scales well to large graphs since the designed loss
functions adopt the mini-batch data to optimize the clustering distribution
even without performance drops. Both experimental results and theoretical
analyses demonstrate the superiority of our method. Compared to the runner-up,
Dink-Net achieves 9.62% NMI improvement on the ogbn-papers100M dataset with 111
million nodes and 1.6 billion edges. The source code is released at
https://github.com/yueliu1999/Dink-Net. Besides, a collection (papers, codes,
and datasets) of deep graph clustering is shared at
https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yue Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1"&gt;Ke Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1"&gt;Jun Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Sihang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xihong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinwang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Stan Z. Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.04391</id>
        <link href="http://arxiv.org/abs/2302.04391"/>
        <updated>2023-07-17T01:05:35.901Z</updated>
        <summary type="html"><![CDATA[In industry deep learning application, our manually labeled data has a
certain number of noisy data. To solve this problem and achieve more than 90
score in dev dataset, we present a simple method to find the noisy data and
re-label the noisy data by human, given the model predictions as references in
human labeling. In this paper, we illustrate our idea for a broad set of deep
learning tasks, includes classification, sequence tagging, object detection,
sequence generation, click-through rate prediction. The experimental results
and human evaluation results verify our idea.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1"&gt;Tong Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Data Mining Approach for Detecting Collusion in Unproctored Online Exams. (arXiv:2302.07014v3 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.07014</id>
        <link href="http://arxiv.org/abs/2302.07014"/>
        <updated>2023-07-17T01:05:35.898Z</updated>
        <summary type="html"><![CDATA[Due to the precautionary measures during the COVID-19 pandemic many
universities offered unproctored take-home exams. We propose methods to detect
potential collusion between students and apply our approach on event log data
from take-home exams during the pandemic. We find groups of students with
suspiciously similar exams. In addition, we compare our findings to a proctored
control group. By this, we establish a rule of thumb for evaluating which cases
are "outstandingly similar", i.e., suspicious cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Langerbein_J/0/1/0/all/0/1"&gt;Janine Langerbein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Massing_T/0/1/0/all/0/1"&gt;Till Massing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klenke_J/0/1/0/all/0/1"&gt;Jens Klenke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reckmann_N/0/1/0/all/0/1"&gt;Natalie Reckmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Striewe_M/0/1/0/all/0/1"&gt;Michael Striewe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goedicke_M/0/1/0/all/0/1"&gt;Michael Goedicke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanck_C/0/1/0/all/0/1"&gt;Christoph Hanck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable and Intervenable Ultrasonography-based Machine Learning Models for Pediatric Appendicitis. (arXiv:2302.14460v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.14460</id>
        <link href="http://arxiv.org/abs/2302.14460"/>
        <updated>2023-07-17T01:05:35.897Z</updated>
        <summary type="html"><![CDATA[Appendicitis is among the most frequent reasons for pediatric abdominal
surgeries. With recent advances in machine learning, data-driven decision
support could help clinicians diagnose and manage patients while reducing the
number of non-critical surgeries. Previous decision support systems for
appendicitis focused on clinical, laboratory, scoring and computed tomography
data, mainly ignoring abdominal ultrasound, a noninvasive and readily available
diagnostic modality. To this end, we developed and validated interpretable
machine learning models for predicting the diagnosis, management and severity
of suspected appendicitis using ultrasound images. Our models were trained on a
dataset comprising 579 pediatric patients with 1709 ultrasound images
accompanied by clinical and laboratory data. Our methodological contribution is
the generalization of concept bottleneck models to prediction problems with
multiple views and incomplete concept sets. Notably, such models lend
themselves to interpretation and interaction via high-level concepts
understandable to clinicians without sacrificing performance or requiring
time-consuming image annotation when deployed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marcinkevics_R/0/1/0/all/0/1"&gt;Ri&amp;#x10d;ards Marcinkevi&amp;#x10d;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolfertstetter_P/0/1/0/all/0/1"&gt;Patricia Reis Wolfertstetter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klimiene_U/0/1/0/all/0/1"&gt;Ugne Klimiene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chin_Cheong_K/0/1/0/all/0/1"&gt;Kieran Chin-Cheong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paschke_A/0/1/0/all/0/1"&gt;Alyssia Paschke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zerres_J/0/1/0/all/0/1"&gt;Julia Zerres&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denzinger_M/0/1/0/all/0/1"&gt;Markus Denzinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niederberger_D/0/1/0/all/0/1"&gt;David Niederberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wellmann_S/0/1/0/all/0/1"&gt;Sven Wellmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozkan_E/0/1/0/all/0/1"&gt;Ece Ozkan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knorr_C/0/1/0/all/0/1"&gt;Christian Knorr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1"&gt;Julia E. Vogt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Linear Estimating Equations. (arXiv:2307.07320v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2307.07320</id>
        <link href="http://arxiv.org/abs/2307.07320"/>
        <updated>2023-07-17T01:05:35.874Z</updated>
        <summary type="html"><![CDATA[Sequential data collection has emerged as a widely adopted technique for
enhancing the efficiency of data gathering processes. Despite its advantages,
such data collection mechanism often introduces complexities to the statistical
inference procedure. For instance, the ordinary least squares (OLS) estimator
in an adaptive linear regression model can exhibit non-normal asymptotic
behavior, posing challenges for accurate inference and interpretation. In this
paper, we propose a general method for constructing debiased estimator which
remedies this issue. It makes use of the idea of adaptive linear estimating
equations, and we establish theoretical guarantees of asymptotic normality,
supplemented by discussions on achieving near-optimal asymptotic variance. A
salient feature of our estimator is that in the context of multi-armed bandits,
our estimator retains the non-asymptotic performance of the least square
estimator while obtaining asymptotic normality property. Consequently, this
work helps connect two fruitful paradigms of adaptive inference: a)
non-asymptotic inference using concentration inequalities and b) asymptotic
inference via asymptotic normality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ying_M/0/1/0/all/0/1"&gt;Mufang Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Khamaru_K/0/1/0/all/0/1"&gt;Koulik Khamaru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Cun-Hui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global $k$-means$++$: an effective relaxation of the global $k$-means clustering algorithm. (arXiv:2211.12271v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.12271</id>
        <link href="http://arxiv.org/abs/2211.12271"/>
        <updated>2023-07-17T01:05:35.867Z</updated>
        <summary type="html"><![CDATA[The $k$-means algorithm is a prevalent clustering method due to its
simplicity, effectiveness, and speed. However, its main disadvantage is its
high sensitivity to the initial positions of the cluster centers. The global
$k$-means is a deterministic algorithm proposed to tackle the random
initialization problem of k-means but its well-known that requires high
computational cost. It partitions the data to $K$ clusters by solving all
$k$-means sub-problems incrementally for all $k=1,\ldots, K$. For each $k$
cluster problem, the method executes the $k$-means algorithm $N$ times, where
$N$ is the number of datapoints. In this paper, we propose the \emph{global
$k$-means\texttt{++}} clustering algorithm, which is an effective way of
acquiring quality clustering solutions akin to those of global $k$-means with a
reduced computational load. This is achieved by exploiting the center selection
probability that is effectively used in the $k$-means\texttt{++} algorithm. The
proposed method has been tested and compared in various benchmark datasets
yielding very satisfactory results in terms of clustering quality and execution
speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vardakas_G/0/1/0/all/0/1"&gt;Georgios Vardakas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Likas_A/0/1/0/all/0/1"&gt;Aristidis Likas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rank-based Decomposable Losses in Machine Learning: A Survey. (arXiv:2207.08768v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2207.08768</id>
        <link href="http://arxiv.org/abs/2207.08768"/>
        <updated>2023-07-17T01:05:35.865Z</updated>
        <summary type="html"><![CDATA[Recent works have revealed an essential paradigm in designing loss functions
that differentiate individual losses vs. aggregate losses. The individual loss
measures the quality of the model on a sample, while the aggregate loss
combines individual losses/scores over each training sample. Both have a common
procedure that aggregates a set of individual values to a single numerical
value. The ranking order reflects the most fundamental relation among
individual values in designing losses. In addition, decomposability, in which a
loss can be decomposed into an ensemble of individual terms, becomes a
significant property of organizing losses/scores. This survey provides a
systematic and comprehensive review of rank-based decomposable losses in
machine learning. Specifically, we provide a new taxonomy of loss functions
that follows the perspectives of aggregate loss and individual loss. We
identify the aggregator to form such losses, which are examples of set
functions. We organize the rank-based decomposable losses into eight
categories. Following these categories, we review the literature on rank-based
aggregate losses and rank-based individual losses. We describe general formulas
for these losses and connect them with existing research topics. We also
suggest future research directions spanning unexplored, remaining, and emerging
issues in rank-based decomposable losses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shu Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1"&gt;Siwei Lyu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alternating the Population and Control Neural Networks to Solve High-Dimensional Stochastic Mean-Field Games. (arXiv:2002.10113v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.10113</id>
        <link href="http://arxiv.org/abs/2002.10113"/>
        <updated>2023-07-17T01:05:35.858Z</updated>
        <summary type="html"><![CDATA[We present APAC-Net, an alternating population and agent control neural
network for solving stochastic mean field games (MFGs). Our algorithm is geared
toward high-dimensional instances of MFGs that are beyond reach with existing
solution methods. We achieve this in two steps. First, we take advantage of the
underlying variational primal-dual structure that MFGs exhibit and phrase it as
a convex-concave saddle point problem. Second, we parameterize the value and
density functions by two neural networks, respectively. By phrasing the problem
in this manner, solving the MFG can be interpreted as a special case of
training a generative adversarial network (GAN). We show the potential of our
method on up to 100-dimensional MFG problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1"&gt;Alex Tong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_S/0/1/0/all/0/1"&gt;Samy Wu Fung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wuchen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nurbekyan_L/0/1/0/all/0/1"&gt;Levon Nurbekyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osher_S/0/1/0/all/0/1"&gt;Stanley J. Osher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Assisted Probabilistic Safe Adaptive Control With Meta-Bayesian Learning. (arXiv:2307.00828v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.00828</id>
        <link href="http://arxiv.org/abs/2307.00828"/>
        <updated>2023-07-17T01:05:35.851Z</updated>
        <summary type="html"><![CDATA[Breaking safety constraints in control systems can lead to potential risks,
resulting in unexpected costs or catastrophic damage. Nevertheless, uncertainty
is ubiquitous, even among similar tasks. In this paper, we develop a novel
adaptive safe control framework that integrates meta learning, Bayesian models,
and control barrier function (CBF) method. Specifically, with the help of CBF
method, we learn the inherent and external uncertainties by a unified adaptive
Bayesian linear regression (ABLR) model, which consists of a forward neural
network (NN) and a Bayesian output layer. Meta learning techniques are
leveraged to pre-train the NN weights and priors of the ABLR model using data
collected from historical similar tasks. For a new control task, we refine the
meta-learned models using a few samples, and introduce pessimistic confidence
bounds into CBF constraints to ensure safe control. Moreover, we provide
theoretical criteria to guarantee probabilistic safety during the control
processes. To validate our approach, we conduct comparative experiments in
various obstacle avoidance scenarios. The results demonstrate that our
algorithm significantly improves the Bayesian model-based CBF method, and is
capable for efficient safe exploration even with multiple uncertain
constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shengbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_K/0/1/0/all/0/1"&gt;Ke Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yuting Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tingwen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wen_S/0/1/0/all/0/1"&gt;Shiping Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TSNet-SAC: Leveraging Transformers for Efficient Task Scheduling. (arXiv:2307.07445v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2307.07445</id>
        <link href="http://arxiv.org/abs/2307.07445"/>
        <updated>2023-07-17T01:05:35.850Z</updated>
        <summary type="html"><![CDATA[In future 6G Mobile Edge Computing (MEC), autopilot systems require the
capability of processing multimodal data with strong interdependencies.
However, traditional heuristic algorithms are inadequate for real-time
scheduling due to their requirement for multiple iterations to derive the
optimal scheme. We propose a novel TSNet-SAC based on Transformer, that
utilizes heuristic algorithms solely to guide the training of TSNet.
Additionally, a Sliding Augment Component (SAC) is introduced to enhance the
robustness and resolve algorithm defects. Furthermore, the Extender component
is designed to handle multi-scale training data and provide network
scalability, enabling TSNet to adapt to different access scenarios. Simulation
demonstrates that TSNet-SAC outperforms existing networks in accuracy and
robustness, achieving superior scheduling-making latency compared to heuristic
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1"&gt;Ke Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhiyuan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Haohan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Desheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifiability Guarantees for Causal Disentanglement from Soft Interventions. (arXiv:2307.06250v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.06250</id>
        <link href="http://arxiv.org/abs/2307.06250"/>
        <updated>2023-07-17T01:05:35.850Z</updated>
        <summary type="html"><![CDATA[Causal disentanglement aims to uncover a representation of data using latent
variables that are interrelated through a causal model. Such a representation
is identifiable if the latent model that explains the data is unique. In this
paper, we focus on the scenario where unpaired observational and interventional
data are available, with each intervention changing the mechanism of a latent
variable. When the causal variables are fully observed, statistically
consistent algorithms have been developed to identify the causal model under
faithfulness assumptions. We here show that identifiability can still be
achieved with unobserved causal variables, given a generalized notion of
faithfulness. Our results guarantee that we can recover the latent causal model
up to an equivalence class and predict the effect of unseen combinations of
interventions, in the limit of infinite data. We implement our causal
disentanglement framework by developing an autoencoding variational Bayes
algorithm and apply it to the problem of predicting combinatorial perturbation
effects in genomics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiaqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Squires_C/0/1/0/all/0/1"&gt;Chandler Squires&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Greenewald_K/0/1/0/all/0/1"&gt;Kristjan Greenewald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Srivastava_A/0/1/0/all/0/1"&gt;Akash Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shanmugam_K/0/1/0/all/0/1"&gt;Karthikeyan Shanmugam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Uhler_C/0/1/0/all/0/1"&gt;Caroline Uhler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Convex Optimization with Stochastic Constraints: Zero Constraint Violation and Bandit Feedback. (arXiv:2301.11267v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2301.11267</id>
        <link href="http://arxiv.org/abs/2301.11267"/>
        <updated>2023-07-17T01:05:35.836Z</updated>
        <summary type="html"><![CDATA[This paper studies online convex optimization with stochastic constraints. We
propose a variant of the drift-plus-penalty algorithm that guarantees
$O(\sqrt{T})$ expected regret and zero constraint violation, after a fixed
number of iterations, which improves the vanilla drift-plus-penalty method with
$O(\sqrt{T})$ constraint violation. Our algorithm is oblivious to the length of
the time horizon $T$, in contrast to the vanilla drift-plus-penalty method.
This is based on our novel drift lemma that provides time-varying bounds on the
virtual queue drift and, as a result, leads to time-varying bounds on the
expected virtual queue length. Moreover, we extend our framework to
stochastic-constrained online convex optimization under two-point bandit
feedback. We show that by adapting our algorithmic framework to the bandit
feedback setting, we may still achieve $O(\sqrt{T})$ expected regret and zero
constraint violation, improving upon the previous work for the case of
identical constraint functions. Numerical results demonstrate our theoretical
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Yeongjong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dabeen Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ed-Fed: A generic federated learning framework with resource-aware client selection for edge devices. (arXiv:2307.07199v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2307.07199</id>
        <link href="http://arxiv.org/abs/2307.07199"/>
        <updated>2023-07-17T01:05:35.827Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) has evolved as a prominent method for edge devices to
cooperatively create a unified prediction model while securing their sensitive
training data local to the device. Despite the existence of numerous research
frameworks for simulating FL algorithms, they do not facilitate comprehensive
deployment for automatic speech recognition tasks on heterogeneous edge
devices. This is where Ed-Fed, a comprehensive and generic FL framework, comes
in as a foundation for future practical FL system research. We also propose a
novel resource-aware client selection algorithm to optimise the waiting time in
the FL settings. We show that our approach can handle the straggler devices and
dynamically set the training time for the selected devices in a round. Our
evaluation has shown that the proposed approach significantly optimises waiting
time in FL compared to conventional random client selection methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sasindran_Z/0/1/0/all/0/1"&gt;Zitha Sasindran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yelchuri_H/0/1/0/all/0/1"&gt;Harsha Yelchuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prabhakar_T/0/1/0/all/0/1"&gt;T. V. Prabhakar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unpacking the Black Box: Regulating Algorithmic Decisions. (arXiv:2110.03443v2 [econ.GN] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.03443</id>
        <link href="http://arxiv.org/abs/2110.03443"/>
        <updated>2023-07-17T01:05:35.798Z</updated>
        <summary type="html"><![CDATA[We show how to optimally regulate prediction algorithms in a world where an
agent uses complex 'black-box' prediction functions to make decisions such as
lending, medical testing, or hiring, and where a principal is limited in how
much she can learn about the agent's black-box model. We show that limiting
agents to prediction functions that are simple enough to be fully transparent
is inefficient as long as the misalignment is limited and first-best prediction
functions are sufficiently complex. Algorithmic audits can improve welfare, but
the gains depend on the design of the audit tools. Tools that focus on
minimizing overall information loss, the focus of many explainer tools, will
generally be inefficient since they focus on explaining the average behavior of
the prediction function. Targeted tools that focus on the source of incentive
misalignment, e.g., excess false positives or racial disparities, can provide
second-best solutions. We provide empirical support for our theoretical
findings using an application in consumer lending, where we document that
complex models regulated based on context-specific explanation tools outperform
simple, fully transparent models. This gain from complex models represents a
Pareto improvement across our empirical applications that are preferred both by
the lender and from the perspective of the financial regulator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Blattner_L/0/1/0/all/0/1"&gt;Laura Blattner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Nelson_S/0/1/0/all/0/1"&gt;Scott Nelson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Spiess_J/0/1/0/all/0/1"&gt;Jann Spiess&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Explainable Relational Reinforcement Learning: A Neuro-Symbolic Approach. (arXiv:2304.08349v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.08349</id>
        <link href="http://arxiv.org/abs/2304.08349"/>
        <updated>2023-07-17T01:05:35.798Z</updated>
        <summary type="html"><![CDATA[Despite numerous successes in Deep Reinforcement Learning (DRL), the learned
policies are not interpretable. Moreover, since DRL does not exploit symbolic
relational representations, it has difficulties in coping with structural
changes in its environment (such as increasing the number of objects).
Relational Reinforcement Learning, on the other hand, inherits the relational
representations from symbolic planning to learn reusable policies. However, it
has so far been unable to scale up and exploit the power of deep neural
networks. We propose Deep Explainable Relational Reinforcement Learning
(DERRL), a framework that exploits the best of both -- neural and symbolic
worlds. By resorting to a neuro-symbolic approach, DERRL combines relational
representations and constraints from symbolic planning with deep learning to
extract interpretable policies. These policies are in the form of logical rules
that explain how each decision (or action) is arrived at. Through several
experiments, in setups like the Countdown Game, Blocks World, Gridworld, and
Traffic, we show that the policies learned by DERRL can be applied to different
configurations and contexts, hence generalizing to environmental modifications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hazra_R/0/1/0/all/0/1"&gt;Rishi Hazra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raedt_L/0/1/0/all/0/1"&gt;Luc De Raedt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Continual Learning via Flat-to-Wide Approaches. (arXiv:2306.14369v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.14369</id>
        <link href="http://arxiv.org/abs/2306.14369"/>
        <updated>2023-07-17T01:05:35.783Z</updated>
        <summary type="html"><![CDATA[Existing approaches on continual learning call for a lot of samples in their
training processes. Such approaches are impractical for many real-world
problems having limited samples because of the overfitting problem. This paper
proposes a few-shot continual learning approach, termed FLat-tO-WidE AppRoach
(FLOWER), where a flat-to-wide learning process finding the flat-wide minima is
proposed to address the catastrophic forgetting problem. The issue of data
scarcity is overcome with a data augmentation approach making use of a ball
generator concept to restrict the sampling space into the smallest enclosing
ball. Our numerical studies demonstrate the advantage of FLOWER achieving
significantly improved performances over prior arts notably in the small base
tasks. For further study, source codes of FLOWER, competitor algorithms and
experimental logs are shared publicly in
\url{https://github.com/anwarmaxsum/FLOWER}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Masum_M/0/1/0/all/0/1"&gt;Muhammad Anwar Ma&amp;#x27;sum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1"&gt;Mahardhika Pratama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lughofer_E/0/1/0/all/0/1"&gt;Edwin Lughofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Habibullah/0/1/0/all/0/1"&gt;Habibullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowalczyk_R/0/1/0/all/0/1"&gt;Ryszard Kowalczyk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully probabilistic deep models for forward and inverse problems in parametric PDEs. (arXiv:2208.04856v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.04856</id>
        <link href="http://arxiv.org/abs/2208.04856"/>
        <updated>2023-07-17T01:05:35.780Z</updated>
        <summary type="html"><![CDATA[We introduce a physics-driven deep latent variable model (PDDLVM) to learn
simultaneously parameter-to-solution (forward) and solution-to-parameter
(inverse) maps of parametric partial differential equations (PDEs). Our
formulation leverages conventional PDE discretization techniques, deep neural
networks, probabilistic modelling, and variational inference to assemble a
fully probabilistic coherent framework. In the posited probabilistic model,
both the forward and inverse maps are approximated as Gaussian distributions
with a mean and covariance parameterized by deep neural networks. The PDE
residual is assumed to be an observed random vector of value zero, hence we
model it as a random vector with a zero mean and a user-prescribed covariance.
The model is trained by maximizing the probability, that is the evidence or
marginal likelihood, of observing a residual of zero by maximizing the evidence
lower bound (ELBO). Consequently, the proposed methodology does not require any
independent PDE solves and is physics-informed at training time, allowing the
real-time solution of PDE forward and inverse problems after training. The
proposed framework can be easily extended to seamlessly integrate observed data
to solve inverse problems and to build generative models. We demonstrate the
efficiency and robustness of our method on finite element discretized
parametric PDE problems such as linear and nonlinear Poisson problems, elastic
shells with complex 3D geometries, and time-dependent nonlinear and
inhomogeneous PDEs using a physics-informed neural network (PINN)
discretization. We achieve up to three orders of magnitude speed-up after
training compared to traditional finite element method (FEM), while outputting
coherent uncertainty estimates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vadeboncoeur_A/0/1/0/all/0/1"&gt;Arnaud Vadeboncoeur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Akyildiz_O/0/1/0/all/0/1"&gt;&amp;#xd6;mer Deniz Akyildiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kazlauskaite_I/0/1/0/all/0/1"&gt;Ieva Kazlauskaite&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Girolami_M/0/1/0/all/0/1"&gt;Mark Girolami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cirak_F/0/1/0/all/0/1"&gt;Fehmi Cirak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proof of Training (PoT): Harnessing Crypto Mining Power for Distributed AI Training. (arXiv:2307.07066v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2307.07066</id>
        <link href="http://arxiv.org/abs/2307.07066"/>
        <updated>2023-07-17T01:05:35.779Z</updated>
        <summary type="html"><![CDATA[In the midst of the emerging trend of integrating artificial intelligence
(AI) with crypto mining, we identify three major challenges that create a gap
between these two fields. To bridge this gap, we introduce the
proof-of-training (PoT) protocol, an approach that combines the strengths of
both AI and blockchain technology. The PoT protocol utilizes the practical
Byzantine fault tolerance (PBFT) consensus mechanism to synchronize global
states. To evaluate the performance of the protocol design, we present an
implementation of a decentralized training network (DTN) that adopts the PoT
protocol. Our results indicate that the protocol exhibits considerable
potential in terms of task throughput, system robustness, and network security.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peihao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HEAL-SWIN: A Vision Transformer On The Sphere. (arXiv:2307.07313v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.07313</id>
        <link href="http://arxiv.org/abs/2307.07313"/>
        <updated>2023-07-17T01:05:35.778Z</updated>
        <summary type="html"><![CDATA[High-resolution wide-angle fisheye images are becoming more and more
important for robotics applications such as autonomous driving. However, using
ordinary convolutional neural networks or vision transformers on this data is
problematic due to projection and distortion losses introduced when projecting
to a rectangular grid on the plane. We introduce the HEAL-SWIN transformer,
which combines the highly uniform Hierarchical Equal Area iso-Latitude
Pixelation (HEALPix) grid used in astrophysics and cosmology with the
Hierarchical Shifted-Window (SWIN) transformer to yield an efficient and
flexible model capable of training on high-resolution, distortion-free
spherical data. In HEAL-SWIN, the nested structure of the HEALPix grid is used
to perform the patching and windowing operations of the SWIN transformer,
resulting in a one-dimensional representation of the spherical data with
minimal computational overhead. We demonstrate the superior performance of our
model for semantic segmentation and depth regression tasks on both synthetic
and real automotive datasets. Our code is available at
https://github.com/JanEGerken/HEAL-SWIN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carlsson_O/0/1/0/all/0/1"&gt;Oscar Carlsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gerken_J/0/1/0/all/0/1"&gt;Jan E. Gerken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Linander_H/0/1/0/all/0/1"&gt;Hampus Linander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spiess_H/0/1/0/all/0/1"&gt;Heiner Spie&amp;#xdf;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ohlsson_F/0/1/0/all/0/1"&gt;Fredrik Ohlsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petersson_C/0/1/0/all/0/1"&gt;Christoffer Petersson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Persson_D/0/1/0/all/0/1"&gt;Daniel Persson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Percussive Technique Recognition and Embedding Learning for the Acoustic Guitar. (arXiv:2307.07426v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2307.07426</id>
        <link href="http://arxiv.org/abs/2307.07426"/>
        <updated>2023-07-17T01:05:35.765Z</updated>
        <summary type="html"><![CDATA[Real-time music information retrieval (RT-MIR) has much potential to augment
the capabilities of traditional acoustic instruments. We develop RT-MIR
techniques aimed at augmenting percussive fingerstyle, which blends acoustic
guitar playing with guitar body percussion. We formulate several design
objectives for RT-MIR systems for augmented instrument performance: (i) causal
constraint, (ii) perceptually negligible action-to-sound latency, (iii) control
intimacy support, (iv) synthesis control support. We present and evaluate
real-time guitar body percussion recognition and embedding learning techniques
based on convolutional neural networks (CNNs) and CNNs jointly trained with
variational autoencoders (VAEs). We introduce a taxonomy of guitar body
percussion based on hand part and location. We follow a cross-dataset
evaluation approach by collecting three datasets labelled according to the
taxonomy. The embedding quality of the models is assessed using KL-Divergence
across distributions corresponding to different taxonomic classes. Results
indicate that the networks are strong classifiers especially in a simplified
2-class recognition task, and the VAEs yield improved class separation compared
to CNNs as evidenced by increased KL-Divergence across distributions. We argue
that the VAE embedding quality could support control intimacy and rich
interaction when the latent space's parameters are used to control an external
synthesis engine. Further design challenges around generalisation to different
datasets have been identified.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martelloni_A/0/1/0/all/0/1"&gt;Andrea Martelloni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McPherson_A/0/1/0/all/0/1"&gt;Andrew P McPherson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barthet_M/0/1/0/all/0/1"&gt;Mathieu Barthet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hypothesis Transfer Learning with Surrogate Classification Losses: Generalization Bounds through Algorithmic Stability. (arXiv:2305.19694v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.19694</id>
        <link href="http://arxiv.org/abs/2305.19694"/>
        <updated>2023-07-17T01:05:35.765Z</updated>
        <summary type="html"><![CDATA[Hypothesis transfer learning (HTL) contrasts domain adaptation by allowing
for a previous task leverage, named the source, into a new one, the target,
without requiring access to the source data. Indeed, HTL relies only on a
hypothesis learnt from such source data, relieving the hurdle of expansive data
storage and providing great practical benefits. Hence, HTL is highly beneficial
for real-world applications relying on big data. The analysis of such a method
from a theoretical perspective faces multiple challenges, particularly in
classification tasks. This paper deals with this problem by studying the
learning theory of HTL through algorithmic stability, an attractive theoretical
framework for machine learning algorithms analysis. In particular, we are
interested in the statistical behaviour of the regularized empirical risk
minimizers in the case of binary classification. Our stability analysis
provides learning guarantees under mild assumptions. Consequently, we derive
several complexity-free generalization bounds for essential statistical
quantities like the training error, the excess risk and cross-validation
estimates. These refined bounds allow understanding the benefits of transfer
learning and comparing the behaviour of standard losses in different scenarios,
leading to valuable insights for practitioners.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Aghbalou_A/0/1/0/all/0/1"&gt;Anass Aghbalou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Staerman_G/0/1/0/all/0/1"&gt;Guillaume Staerman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy-preserving machine learning with tensor networks. (arXiv:2202.12319v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.12319</id>
        <link href="http://arxiv.org/abs/2202.12319"/>
        <updated>2023-07-17T01:05:35.760Z</updated>
        <summary type="html"><![CDATA[Tensor networks, widely used for providing efficient representations of
low-energy states of local quantum many-body systems, have been recently
proposed as machine learning architectures which could present advantages with
respect to traditional ones. In this work we show that tensor network
architectures have especially prospective properties for privacy-preserving
machine learning, which is important in tasks such as the processing of medical
records. First, we describe a new privacy vulnerability that is present in
feedforward neural networks, illustrating it in synthetic and real-world
datasets. Then, we develop well-defined conditions to guarantee robustness to
such vulnerability, which involve the characterization of models equivalent
under gauge symmetry. We rigorously prove that such conditions are satisfied by
tensor-network architectures. In doing so, we define a novel canonical form for
matrix product states, which has a high degree of regularity and fixes the
residual gauge that is left in the canonical forms based on singular value
decompositions. We supplement the analytical findings with practical examples
where matrix product states are trained on datasets of medical records, which
show large reductions on the probability of an attacker extracting information
about the training dataset from the model's parameters. Given the growing
expertise in training tensor-network architectures, these results imply that
one may not have to be forced to make a choice between accuracy in prediction
and ensuring the privacy of the information processed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pozas_Kerstjens_A/0/1/0/all/0/1"&gt;Alejandro Pozas-Kerstjens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_Santana_S/0/1/0/all/0/1"&gt;Senaida Hern&amp;#xe1;ndez-Santana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monturiol_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Ram&amp;#xf3;n Pareja Monturiol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_M/0/1/0/all/0/1"&gt;Marco Castrill&amp;#xf3;n L&amp;#xf3;pez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scarpa_G/0/1/0/all/0/1"&gt;Giannicola Scarpa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Guillen_C/0/1/0/all/0/1"&gt;Carlos E. Gonz&amp;#xe1;lez-Guill&amp;#xe9;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Garcia_D/0/1/0/all/0/1"&gt;David P&amp;#xe9;rez-Garc&amp;#xed;a&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Clustering in Data Streams. (arXiv:2307.07449v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2307.07449</id>
        <link href="http://arxiv.org/abs/2307.07449"/>
        <updated>2023-07-17T01:05:35.759Z</updated>
        <summary type="html"><![CDATA[The streaming model is an abstraction of computing over massive data streams,
which is a popular way of dealing with large-scale modern data analysis. In
this model, there is a stream of data points, one after the other. A streaming
algorithm is only allowed one pass over the data stream, and the goal is to
perform some analysis during the stream while using as small space as possible.

Clustering problems (such as $k$-means and $k$-median) are fundamental
unsupervised machine learning primitives, and streaming clustering algorithms
have been extensively studied in the past. However, since data privacy becomes
a central concern in many real-world applications, non-private clustering
algorithms are not applicable in many scenarios.

In this work, we provide the first differentially private streaming
algorithms for $k$-means and $k$-median clustering of $d$-dimensional Euclidean
data points over a stream with length at most $T$ using $poly(k,d,\log(T))$
space to achieve a {\it constant} multiplicative error and a
$poly(k,d,\log(T))$ additive error. In particular, we present a differentially
private streaming clustering framework which only requires an offline DP
coreset algorithm as a blackbox. By plugging in existing DP coreset results via
Ghazi, Kumar, Manurangsi 2020 and Kaplan, Stemmer 2018, we achieve (1) a
$(1+\gamma)$-multiplicative approximation with
$\tilde{O}_\gamma(poly(k,d,\log(T)))$ space for any $\gamma>0$, and the
additive error is $poly(k,d,\log(T))$ or (2) an $O(1)$-multiplicative
approximation with $\tilde{O}(k \cdot poly(d,\log(T)))$ space and
$poly(k,d,\log(T))$ additive error.

In addition, our algorithmic framework is also differentially private under
the continual release setting, i.e., the union of outputs of our algorithms at
every timestamp is always differentially private.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Epasto_A/0/1/0/all/0/1"&gt;Alessandro Epasto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_T/0/1/0/all/0/1"&gt;Tamalika Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_P/0/1/0/all/0/1"&gt;Peilin Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PseudoCal: A Source-Free Approach to Unsupervised Uncertainty Calibration in Domain Adaptation. (arXiv:2307.07489v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07489</id>
        <link href="http://arxiv.org/abs/2307.07489"/>
        <updated>2023-07-17T01:05:35.759Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (UDA) has witnessed remarkable advancements in
improving the accuracy of models for unlabeled target domains. However, the
calibration of predictive uncertainty in the target domain, a crucial aspect of
the safe deployment of UDA models, has received limited attention. The
conventional in-domain calibration method, \textit{temperature scaling}
(TempScal), encounters challenges due to domain distribution shifts and the
absence of labeled target domain data. Recent approaches have employed
importance-weighting techniques to estimate the target-optimal temperature
based on re-weighted labeled source data. Nonetheless, these methods require
source data and suffer from unreliable density estimates under severe domain
shifts, rendering them unsuitable for source-free UDA settings. To overcome
these limitations, we propose PseudoCal, a source-free calibration method that
exclusively relies on unlabeled target data. Unlike previous approaches that
treat UDA calibration as a \textit{covariate shift} problem, we consider it as
an unsupervised calibration problem specific to the target domain. Motivated by
the factorization of the negative log-likelihood (NLL) objective in TempScal,
we generate a labeled pseudo-target set that captures the structure of the real
target. By doing so, we transform the unsupervised calibration problem into a
supervised one, enabling us to effectively address it using widely-used
in-domain methods like TempScal. Finally, we thoroughly evaluate the
calibration performance of PseudoCal by conducting extensive experiments on 10
UDA methods, considering both traditional UDA settings and recent source-free
UDA scenarios. The experimental results consistently demonstrate the superior
performance of PseudoCal, exhibiting significantly reduced calibration error
compared to existing calibration methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1"&gt;Dapeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jian Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinchao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1"&gt;Chuan-Sheng Foo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep reinforcement learning for the dynamic vehicle dispatching problem: An event-based approach. (arXiv:2307.07508v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2307.07508</id>
        <link href="http://arxiv.org/abs/2307.07508"/>
        <updated>2023-07-17T01:05:35.754Z</updated>
        <summary type="html"><![CDATA[The dynamic vehicle dispatching problem corresponds to deciding which
vehicles to assign to requests that arise stochastically over time and space.
It emerges in diverse areas, such as in the assignment of trucks to loads to be
transported; in emergency systems; and in ride-hailing services. In this paper,
we model the problem as a semi-Markov decision process, which allows us to
treat time as continuous. In this setting, decision epochs coincide with
discrete events whose time intervals are random. We argue that an event-based
approach substantially reduces the combinatorial complexity of the decision
space and overcomes other limitations of discrete-time models often proposed in
the literature. In order to test our approach, we develop a new discrete-event
simulator and use double deep q-learning to train our decision agents.
Numerical experiments are carried out in realistic scenarios using data from
New York City. We compare the policies obtained through our approach with
heuristic policies often used in practice. Results show that our policies
exhibit better average waiting times, cancellation rates and total service
times, with reduction in average waiting times of up to 50% relative to the
other tested heuristic policies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cordeiro_E/0/1/0/all/0/1"&gt;Edyvalberty Alenquer Cordeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pitombeira_Neto_A/0/1/0/all/0/1"&gt;Anselmo Ramalho Pitombeira-Neto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vulnerability-Aware Instance Reweighting For Adversarial Training. (arXiv:2307.07167v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07167</id>
        <link href="http://arxiv.org/abs/2307.07167"/>
        <updated>2023-07-17T01:05:35.749Z</updated>
        <summary type="html"><![CDATA[Adversarial Training (AT) has been found to substantially improve the
robustness of deep learning classifiers against adversarial attacks. AT
involves obtaining robustness by including adversarial examples in training a
classifier. Most variants of AT algorithms treat every training example
equally. However, recent works have shown that better performance is achievable
by treating them unequally. In addition, it has been observed that AT exerts an
uneven influence on different classes in a training set and unfairly hurts
examples corresponding to classes that are inherently harder to classify.
Consequently, various reweighting schemes have been proposed that assign
unequal weights to robust losses of individual examples in a training set. In
this work, we propose a novel instance-wise reweighting scheme. It considers
the vulnerability of each natural example and the resulting information loss on
its adversarial counterpart occasioned by adversarial attacks. Through
extensive experiments, we show that our proposed method significantly improves
over existing reweighting schemes, especially against strong white and
black-box attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fakorede_O/0/1/0/all/0/1"&gt;Olukorede Fakorede&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nirala_A/0/1/0/all/0/1"&gt;Ashutosh Kumar Nirala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atsague_M/0/1/0/all/0/1"&gt;Modeste Atsague&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1"&gt;Jin Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel t-distributed stochastic neighbor embedding. (arXiv:2307.07081v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07081</id>
        <link href="http://arxiv.org/abs/2307.07081"/>
        <updated>2023-07-17T01:05:35.738Z</updated>
        <summary type="html"><![CDATA[This paper presents a kernelized version of the t-SNE algorithm, capable of
mapping high-dimensional data to a low-dimensional space while preserving the
pairwise distances between the data points in a non-Euclidean metric. This can
be achieved using a kernel trick only in the high dimensional space or in both
spaces, leading to an end-to-end kernelized version. The proposed kernelized
version of the t-SNE algorithm can offer new views on the relationships between
data points, which can improve performance and accuracy in particular
applications, such as classification problems involving kernel methods. The
differences between t-SNE and its kernelized version are illustrated for
several datasets, showing a neater clustering of points belonging to different
classes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ilie_Ablachim_D/0/1/0/all/0/1"&gt;Denis C. Ilie-Ablachim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dumitrescu_B/0/1/0/all/0/1"&gt;Bogdan Dumitrescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rusu_C/0/1/0/all/0/1"&gt;Cristian Rusu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$\Phi$-DVAE: Physics-Informed Dynamical Variational Autoencoders for Unstructured Data Assimilation. (arXiv:2209.15609v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2209.15609</id>
        <link href="http://arxiv.org/abs/2209.15609"/>
        <updated>2023-07-17T01:05:35.727Z</updated>
        <summary type="html"><![CDATA[Incorporating unstructured data into physical models is a challenging problem
that is emerging in data assimilation. Traditional approaches focus on
well-defined observation operators whose functional forms are typically assumed
to be known. This prevents these methods from achieving a consistent model-data
synthesis in configurations where the mapping from data-space to model-space is
unknown. To address these shortcomings, in this paper we develop a
physics-informed dynamical variational autoencoder ($\Phi$-DVAE) to embed
diverse data streams into time-evolving physical systems described by
differential equations. Our approach combines a standard, possibly nonlinear,
filter for the latent state-space model and a VAE, to assimilate the
unstructured data into the latent dynamical system. Unstructured data, in our
example systems, comes in the form of video data and velocity field
measurements, however the methodology is suitably generic to allow for
arbitrary unknown observation operators. A variational Bayesian framework is
used for the joint estimation of the encoding, latent states, and unknown
system parameters. To demonstrate the method, we provide case studies with the
Lorenz-63 ordinary differential equation, and the advection and Korteweg-de
Vries partial differential equations. Our results, with synthetic data, show
that $\Phi$-DVAE provides a data efficient dynamics encoding methodology which
is competitive with standard approaches. Unknown parameters are recovered with
uncertainty quantification, and unseen data are accurately predicted.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Glyn_Davies_A/0/1/0/all/0/1"&gt;Alex Glyn-Davies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Duffin_C/0/1/0/all/0/1"&gt;Connor Duffin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Akyildiz_O/0/1/0/all/0/1"&gt;&amp;#xd6;. Deniz Akyildiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Girolami_M/0/1/0/all/0/1"&gt;Mark Girolami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Context-Aware Cutting Plane Selection Algorithm for Mixed-Integer Programming. (arXiv:2307.07322v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2307.07322</id>
        <link href="http://arxiv.org/abs/2307.07322"/>
        <updated>2023-07-17T01:05:35.726Z</updated>
        <summary type="html"><![CDATA[The current cut selection algorithm used in mixed-integer programming solvers
has remained largely unchanged since its creation. In this paper, we propose a
set of new cut scoring measures, cut filtering techniques, and stopping
criteria, extending the current state-of-the-art algorithm and obtaining a 4\%
performance improvement for SCIP over the MIPLIB 2017 benchmark set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Turner_M/0/1/0/all/0/1"&gt;Mark Turner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Berthold_T/0/1/0/all/0/1"&gt;Timo Berthold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Besancon_M/0/1/0/all/0/1"&gt;Mathieu Besan&amp;#xe7;on&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarks and Custom Package for Electrical Load Forecasting. (arXiv:2307.07191v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07191</id>
        <link href="http://arxiv.org/abs/2307.07191"/>
        <updated>2023-07-17T01:05:35.696Z</updated>
        <summary type="html"><![CDATA[Load forecasting is of great significance in the power industry as it can
provide a reference for subsequent tasks such as power grid dispatch, thus
bringing huge economic benefits. However, there are many differences between
load forecasting and traditional time series forecasting. On the one hand, load
forecasting aims to minimize the cost of subsequent tasks such as power grid
dispatch, rather than simply pursuing prediction accuracy. On the other hand,
the load is largely influenced by many external factors, such as temperature or
calendar variables. In addition, the scale of predictions (such as
building-level loads and aggregated-level loads) can also significantly impact
the predicted results. In this paper, we provide a comprehensive load
forecasting archive, which includes load domain-specific feature engineering to
help forecasting models better model load data. In addition, different from the
traditional loss function which only aims for accuracy, we also provide a
method to customize the loss function based on the forecasting error,
integrating it into our forecasting framework. Based on this, we conducted
extensive experiments on load data at different levels, providing a reference
for researchers to compare different load forecasting models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhixian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1"&gt;Qingsong Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chaoli Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Liang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krannichfeldt_L/0/1/0/all/0/1"&gt;Leandro Von Krannichfeldt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid moderation in the newsroom: Recommending featured posts to content moderators. (arXiv:2307.07317v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2307.07317</id>
        <link href="http://arxiv.org/abs/2307.07317"/>
        <updated>2023-07-17T01:05:35.692Z</updated>
        <summary type="html"><![CDATA[Online news outlets are grappling with the moderation of user-generated
content within their comment section. We present a recommender system based on
ranking class probabilities to support and empower the moderator in choosing
featured posts, a time-consuming task. By combining user and textual content
features we obtain an optimal classification F1-score of 0.44 on the test set.
Furthermore, we observe an optimum mean NDCG@5 of 0.87 on a large set of
validation articles. As an expert evaluation, content moderators assessed the
output of a random selection of articles by choosing comments to feature based
on the recommendations, which resulted in a NDCG score of 0.83. We conclude
that first, adding text features yields the best score and second, while
choosing featured content remains somewhat subjective, content moderators found
suitable comments in all but one evaluated recommendations. We end the paper by
analyzing our best-performing model, a step towards transparency and
explainability in hybrid content moderation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Waterschoot_C/0/1/0/all/0/1"&gt;Cedric Waterschoot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bosch_A/0/1/0/all/0/1"&gt;Antal van den Bosch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DataAssist: A Machine Learning Approach to Data Cleaning and Preparation. (arXiv:2307.07119v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07119</id>
        <link href="http://arxiv.org/abs/2307.07119"/>
        <updated>2023-07-17T01:05:35.668Z</updated>
        <summary type="html"><![CDATA[Current automated machine learning (ML) tools are model-centric, focusing on
model selection and parameter optimization. However, the majority of the time
in data analysis is devoted to data cleaning and wrangling, for which limited
tools are available. Here we present DataAssist, an automated data preparation
and cleaning platform that enhances dataset quality using ML-informed methods.
We show that DataAssist provides a pipeline for exploratory data analysis and
data cleaning, including generating visualization for user-selected variables,
unifying data annotation, suggesting anomaly removal, and preprocessing data.
The exported dataset can be readily integrated with other autoML tools or
user-specified model for downstream analysis. Our data-centric tool is
applicable to a variety of fields, including economics, business, and
forecasting applications saving over 50\% time of the time spent on data
cleansing and preparation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goyle_K/0/1/0/all/0/1"&gt;Kartikay Goyle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1"&gt;Quin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyle_V/0/1/0/all/0/1"&gt;Vakul Goyle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlling dynamical systems to complex target states using machine learning: next-generation vs. classical reservoir computing. (arXiv:2307.07195v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07195</id>
        <link href="http://arxiv.org/abs/2307.07195"/>
        <updated>2023-07-17T01:05:35.656Z</updated>
        <summary type="html"><![CDATA[Controlling nonlinear dynamical systems using machine learning allows to not
only drive systems into simple behavior like periodicity but also to more
complex arbitrary dynamics. For this, it is crucial that a machine learning
system can be trained to reproduce the target dynamics sufficiently well. On
the example of forcing a chaotic parametrization of the Lorenz system into
intermittent dynamics, we show first that classical reservoir computing excels
at this task. In a next step, we compare those results based on different
amounts of training data to an alternative setup, where next-generation
reservoir computing is used instead. It turns out that while delivering
comparable performance for usual amounts of training data, next-generation RC
significantly outperforms in situations where only very limited data is
available. This opens even further practical control applications in real world
problems where data is restricted.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haluszczynski_A/0/1/0/all/0/1"&gt;Alexander Haluszczynski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koglmayr_D/0/1/0/all/0/1"&gt;Daniel K&amp;#xf6;glmayr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rath_C/0/1/0/all/0/1"&gt;Christoph R&amp;#xe4;th&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A testing-based approach to assess the clusterability of categorical data. (arXiv:2307.07346v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07346</id>
        <link href="http://arxiv.org/abs/2307.07346"/>
        <updated>2023-07-17T01:05:35.652Z</updated>
        <summary type="html"><![CDATA[The objective of clusterability evaluation is to check whether a clustering
structure exists within the data set. As a crucial yet often-overlooked issue
in cluster analysis, it is essential to conduct such a test before applying any
clustering algorithm. If a data set is unclusterable, any subsequent clustering
analysis would not yield valid results. Despite its importance, the majority of
existing studies focus on numerical data, leaving the clusterability evaluation
issue for categorical data as an open problem. Here we present TestCat, a
testing-based approach to assess the clusterability of categorical data in
terms of an analytical $p$-value. The key idea underlying TestCat is that
clusterable categorical data possess many strongly correlated attribute pairs
and hence the sum of chi-squared statistics of all attribute pairs is employed
as the test statistic for $p$-value calculation. We apply our method to a set
of benchmark categorical data sets, showing that TestCat outperforms those
solutions based on existing clusterability evaluation methods for numeric data.
To the best of our knowledge, our work provides the first way to effectively
recognize the clusterability of categorical data in a statistically sound
manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1"&gt;Lianyu Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Junjie Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1"&gt;Mudi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zengyou He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative adversarial networks for data-scarce spectral applications. (arXiv:2307.07454v1 [physics.optics])]]></title>
        <id>http://arxiv.org/abs/2307.07454</id>
        <link href="http://arxiv.org/abs/2307.07454"/>
        <updated>2023-07-17T01:05:35.647Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) are one of the most robust and
versatile techniques in the field of generative artificial intelligence. In
this work, we report on an application of GANs in the domain of synthetic
spectral data generation, offering a solution to the scarcity of data found in
various scientific contexts. We demonstrate the proposed approach by applying
it to an illustrative problem within the realm of near-field radiative heat
transfer involving a multilayered hyperbolic metamaterial. We find that a
successful generation of spectral data requires two modifications to
conventional GANs: (i) the introduction of Wasserstein GANs (WGANs) to avoid
mode collapse, and, (ii) the conditioning of WGANs to obtain accurate labels
for the generated data. We show that a simple feed-forward neural network
(FFNN), when augmented with data generated by a CWGAN, enhances significantly
its performance under conditions of limited data availability, demonstrating
the intrinsic value of CWGAN data augmentation beyond simply providing larger
datasets. In addition, we show that CWGANs can act as a surrogate model with
improved performance in the low-data regime with respect to simple FFNNs.
Overall, this work highlights the potential of generative machine learning
algorithms in scientific applications beyond image generation and optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Garcia_Esteban_J/0/1/0/all/0/1"&gt;Juan Jos&amp;#xe9; Garc&amp;#xed;a-Esteban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Cuevas_J/0/1/0/all/0/1"&gt;Juan Carlos Cuevas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Bravo_Abad_J/0/1/0/all/0/1"&gt;Jorge Bravo-Abad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schr\"odinger Equation. (arXiv:2307.07050v1 [physics.comp-ph])]]></title>
        <id>http://arxiv.org/abs/2307.07050</id>
        <link href="http://arxiv.org/abs/2307.07050"/>
        <updated>2023-07-17T01:05:35.638Z</updated>
        <summary type="html"><![CDATA[Solving the quantum many-body Schr\"odinger equation is a fundamental and
challenging problem in the fields of quantum physics, quantum chemistry, and
material sciences. One of the common computational approaches to this problem
is Quantum Variational Monte Carlo (QVMC), in which ground-state solutions are
obtained by minimizing the energy of the system within a restricted family of
parameterized wave functions. Deep learning methods partially address the
limitations of traditional QVMC by representing a rich family of wave functions
in terms of neural networks. However, the optimization objective in QVMC
remains notoriously hard to minimize and requires second-order optimization
methods such as natural gradient. In this paper, we first reformulate energy
functional minimization in the space of Born distributions corresponding to
particle-permutation (anti-)symmetric wave functions, rather than the space of
wave functions. We then interpret QVMC as the Fisher--Rao gradient flow in this
distributional space, followed by a projection step onto the variational
manifold. This perspective provides us with a principled framework to derive
new QMC algorithms, by endowing the distributional space with better metrics,
and following the projected gradient flow induced by those metrics. More
specifically, we propose "Wasserstein Quantum Monte Carlo" (WQMC), which uses
the gradient flow induced by the Wasserstein metric, rather than Fisher--Rao
metric, and corresponds to transporting the probability mass, rather than
teleporting it. We demonstrate empirically that the dynamics of WQMC results in
faster convergence to the ground state of molecular systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Neklyudov_K/0/1/0/all/0/1"&gt;Kirill Neklyudov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Nys_J/0/1/0/all/0/1"&gt;Jannes Nys&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Thiede_L/0/1/0/all/0/1"&gt;Luca Thiede&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Carrasquilla_J/0/1/0/all/0/1"&gt;Juan Carrasquilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Makhzani_A/0/1/0/all/0/1"&gt;Alireza Makhzani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing ECG Analysis of Implantable Cardiac Monitor Data: An Efficient Pipeline for Multi-Label Classification. (arXiv:2307.07423v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2307.07423</id>
        <link href="http://arxiv.org/abs/2307.07423"/>
        <updated>2023-07-17T01:05:35.626Z</updated>
        <summary type="html"><![CDATA[Implantable Cardiac Monitor (ICM) devices are demonstrating as of today, the
fastest-growing market for implantable cardiac devices. As such, they are
becoming increasingly common in patients for measuring heart electrical
activity. ICMs constantly monitor and record a patient's heart rhythm and when
triggered - send it to a secure server where health care professionals (denote
HCPs from here on) can review it. These devices employ a relatively simplistic
rule-based algorithm (due to energy consumption constraints) to alert for
abnormal heart rhythms. This algorithm is usually parameterized to an
over-sensitive mode in order to not miss a case (resulting in relatively high
false-positive rate) and this, combined with the device's nature of constantly
monitoring the heart rhythm and its growing popularity, results in HCPs having
to analyze and diagnose an increasingly growing amount of data. In order to
reduce the load on the latter, automated methods for ECG analysis are nowadays
becoming a great tool to assist HCPs in their analysis. While state-of-the-art
algorithms are data-driven rather than rule-based, training data for ICMs often
consist of specific characteristics which make its analysis unique and
particularly challenging. This study presents the challenges and solutions in
automatically analyzing ICM data and introduces a method for its classification
that outperforms existing methods on such data. As such, it could be used in
numerous ways such as aiding HCPs in the analysis of ECGs originating from ICMs
by e.g. suggesting a rhythm type.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bleich_A/0/1/0/all/0/1"&gt;Amnon Bleich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Linnemann_A/0/1/0/all/0/1"&gt;Antje Linnemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jaidi_B/0/1/0/all/0/1"&gt;Benjamin Jaidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Diem_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn H Diem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Conrad_T/0/1/0/all/0/1"&gt;Tim OF Conrad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representation Learning With Hidden Unit Clustering For Low Resource Speech Applications. (arXiv:2307.07325v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2307.07325</id>
        <link href="http://arxiv.org/abs/2307.07325"/>
        <updated>2023-07-17T01:05:35.617Z</updated>
        <summary type="html"><![CDATA[The representation learning of speech, without textual resources, is an area
of significant interest for many low resource speech applications. In this
paper, we describe an approach to self-supervised representation learning from
raw audio using a hidden unit clustering (HUC) framework. The input to the
model consists of audio samples that are windowed and processed with 1-D
convolutional layers. The learned "time-frequency" representations from the
convolutional neural network (CNN) module are further processed with long short
term memory (LSTM) layers which generate a contextual vector representation for
every windowed segment. The HUC framework, allowing the categorization of the
representations into a small number of phoneme-like units, is used to train the
model for learning semantically rich speech representations. The targets
consist of phoneme-like pseudo labels for each audio segment and these are
generated with an iterative k-means algorithm. We explore techniques that
improve the speaker invariance of the learned representations and illustrate
the effectiveness of the proposed approach on two settings, i) completely
unsupervised speech applications on the sub-tasks described as part of the
ZeroSpeech 2021 challenge and ii) semi-supervised automatic speech recognition
(ASR) applications on the TIMIT dataset and on the GramVaani challenge Hindi
dataset. In these experiments, we achieve state-of-art results for various
ZeroSpeech tasks. Further, on the ASR experiments, the HUC representations are
shown to improve significantly over other established benchmarks based on
Wav2vec, HuBERT and Best-RQ.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Krishna_V/0/1/0/all/0/1"&gt;Varun Krishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sai_T/0/1/0/all/0/1"&gt;Tarun Sai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ganapathy_S/0/1/0/all/0/1"&gt;Sriram Ganapathy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Dimensional Ability Diagnosis for Machine Learning Algorithms. (arXiv:2307.07134v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07134</id>
        <link href="http://arxiv.org/abs/2307.07134"/>
        <updated>2023-07-17T01:05:35.575Z</updated>
        <summary type="html"><![CDATA[Machine learning algorithms have become ubiquitous in a number of
applications (e.g. image classification). However, due to the insufficient
measurement of traditional metrics (e.g. the coarse-grained Accuracy of each
classifier), substantial gaps are usually observed between the real-world
performance of these algorithms and their scores in standardized evaluations.
In this paper, inspired by the psychometric theories from human measurement, we
propose a task-agnostic evaluation framework Camilla, where a multi-dimensional
diagnostic metric Ability is defined for collaboratively measuring the
multifaceted strength of each machine learning algorithm. Specifically, given
the response logs from different algorithms to data samples, we leverage
cognitive diagnosis assumptions and neural networks to learn the complex
interactions among algorithms, samples and the skills (explicitly or implicitly
pre-defined) of each sample. In this way, both the abilities of each algorithm
on multiple skills and some of the sample factors (e.g. sample difficulty) can
be simultaneously quantified. We conduct extensive experiments with hundreds of
machine learning algorithms on four public datasets, and our experimental
results demonstrate that Camilla not only can capture the pros and cons of each
algorithm more precisely, but also outperforms state-of-the-art baselines on
the metric reliability, rank consistency and rank stability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1"&gt;Zheng Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhenya Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chuanren Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hengshu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1"&gt;Enhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hui Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section. (arXiv:2307.07051v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.07051</id>
        <link href="http://arxiv.org/abs/2307.07051"/>
        <updated>2023-07-17T01:05:35.528Z</updated>
        <summary type="html"><![CDATA[Recent advances in large language models have led to renewed interest in
natural language processing in healthcare using the free text of clinical
notes. One distinguishing characteristic of clinical notes is their long time
span over multiple long documents. The unique structure of clinical notes
creates a new design choice: when the context length for a language model
predictor is limited, which part of clinical notes should we choose as the
input? Existing studies either choose the inputs with domain knowledge or
simply truncate them. We propose a framework to analyze the sections with high
predictive power. Using MIMIC-III, we show that: 1) predictive power
distribution is different between nursing notes and discharge notes and 2)
combining different types of notes could improve performance when the context
length is large. Our findings suggest that a carefully selected sampling
function could enable more efficient information extraction from clinical
notes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Hongyi Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yixin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Lavender Yao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oermann_E/0/1/0/all/0/1"&gt;Eric Karl Oermann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedBIAD: Communication-Efficient and Accuracy-Guaranteed Federated Learning with Bayesian Inference-Based Adaptive Dropout. (arXiv:2307.07172v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2307.07172</id>
        <link href="http://arxiv.org/abs/2307.07172"/>
        <updated>2023-07-17T01:05:35.474Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) emerges as a distributed machine learning paradigm
without end-user data transmission, effectively avoiding privacy leakage.
Participating devices in FL are usually bandwidth-constrained, and the uplink
is much slower than the downlink in wireless networks, which causes a severe
uplink communication bottleneck. A prominent direction to alleviate this
problem is federated dropout, which drops fractional weights of local models.
However, existing federated dropout studies focus on random or ordered dropout
and lack theoretical support, resulting in unguaranteed performance. In this
paper, we propose Federated learning with Bayesian Inference-based Adaptive
Dropout (FedBIAD), which regards weight rows of local models as probability
distributions and adaptively drops partial weight rows based on importance
indicators correlated with the trend of local training loss. By applying
FedBIAD, each client adaptively selects a high-quality dropping pattern with
accurate approximations and only transmits parameters of non-dropped weight
rows to mitigate uplink costs while improving accuracy. Theoretical analysis
demonstrates that the convergence rate of the average generalization error of
FedBIAD is minimax optimal up to a squared logarithmic factor. Extensive
experiments on image classification and next-word prediction show that compared
with status quo approaches, FedBIAD provides 2x uplink reduction with an
accuracy increase of up to 2.41% even on non-Independent and Identically
Distributed (non-IID) data, which brings up to 72% decrease in training time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1"&gt;Jingjing Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Min Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Sheng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Hui Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xuefeng Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking In-domain Keywords. (arXiv:2307.07160v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.07160</id>
        <link href="http://arxiv.org/abs/2307.07160"/>
        <updated>2023-07-17T01:05:35.419Z</updated>
        <summary type="html"><![CDATA[We propose a novel task-agnostic in-domain pre-training method that sits
between generic pre-training and fine-tuning. Our approach selectively masks
in-domain keywords, i.e., words that provide a compact representation of the
target domain. We identify such keywords using KeyBERT (Grootendorst, 2020). We
evaluate our approach using six different settings: three datasets combined
with two distinct pre-trained language models (PLMs). Our results reveal that
the fine-tuned PLMs adapted using our in-domain pre-training strategy
outperform PLMs that used in-domain pre-training with random masking as well as
those that followed the common pre-train-then-fine-tune paradigm. Further, the
overhead of identifying in-domain keywords is reasonable, e.g., 7-15% of the
pre-training time (for two epochs) for BERT Large (Devlin et al., 2019).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Golchin_S/0/1/0/all/0/1"&gt;Shahriar Golchin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1"&gt;Mihai Surdeanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tavabi_N/0/1/0/all/0/1"&gt;Nazgol Tavabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiapour_A/0/1/0/all/0/1"&gt;Ata Kiapour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe DreamerV3: Safe Reinforcement Learning with World Models. (arXiv:2307.07176v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07176</id>
        <link href="http://arxiv.org/abs/2307.07176"/>
        <updated>2023-07-17T01:05:35.228Z</updated>
        <summary type="html"><![CDATA[The widespread application of Reinforcement Learning (RL) in real-world
situations is yet to come to fruition, largely as a result of its failure to
satisfy the essential safety demands of such systems. Existing safe
reinforcement learning (SafeRL) methods, employing cost functions to enhance
safety, fail to achieve zero-cost in complex scenarios, including vision-only
tasks, even with comprehensive data sampling and training. To address this, we
introduce Safe DreamerV3, a novel algorithm that integrates both
Lagrangian-based and planning-based methods within a world model. Our
methodology represents a significant advancement in SafeRL as the first
algorithm to achieve nearly zero-cost in both low-dimensional and vision-only
tasks within the Safety-Gymnasium benchmark. Our project website can be found
in: https://sites.google.com/view/safedreamerv3.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Weidong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1"&gt;Jiaming Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Borong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1"&gt;Chunhe Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yaodong Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Large Language Models Empower Molecular Property Prediction?. (arXiv:2307.07443v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07443</id>
        <link href="http://arxiv.org/abs/2307.07443"/>
        <updated>2023-07-17T01:05:35.228Z</updated>
        <summary type="html"><![CDATA[Molecular property prediction has gained significant attention due to its
transformative potential in multiple scientific disciplines. Conventionally, a
molecule graph can be represented either as a graph-structured data or a SMILES
text. Recently, the rapid development of Large Language Models (LLMs) has
revolutionized the field of NLP. Although it is natural to utilize LLMs to
assist in understanding molecules represented by SMILES, the exploration of how
LLMs will impact molecular property prediction is still in its early stage. In
this work, we advance towards this objective through two perspectives:
zero/few-shot molecular classification, and using the new explanations
generated by LLMs as representations of molecules. To be specific, we first
prompt LLMs to do in-context molecular classification and evaluate their
performance. After that, we employ LLMs to generate semantically enriched
explanations for the original SMILES and then leverage that to fine-tune a
small-scale LM model for multiple downstream tasks. The experimental results
highlight the superiority of text explanations as molecular representations
across multiple benchmark datasets, and confirm the immense potential of LLMs
in molecular property prediction tasks. Codes are available at
\url{https://github.com/ChnQ/LLM4Mol}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1"&gt;Chen Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Huayi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhirui Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1"&gt;Hong Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A decision framework for selecting information-transfer strategies in population-based SHM. (arXiv:2307.06978v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.06978</id>
        <link href="http://arxiv.org/abs/2307.06978"/>
        <updated>2023-07-17T01:05:35.227Z</updated>
        <summary type="html"><![CDATA[Decision-support for the operation and maintenance of structures provides
significant motivation for the development and implementation of structural
health monitoring (SHM) systems. Unfortunately, the limited availability of
labelled training data hinders the development of the statistical models on
which these decision-support systems rely. Population-based SHM seeks to
mitigate the impact of data scarcity by using transfer learning techniques to
share information between individual structures within a population. The
current paper proposes a decision framework for selecting transfer strategies
based upon a novel concept -- the expected value of information transfer --
such that negative transfer is avoided. By avoiding negative transfer, and by
optimising information transfer strategies using the transfer-decision
framework, one can reduce the costs associated with operating and maintaining
structures, and improve safety.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hughes_A/0/1/0/all/0/1"&gt;Aidan J. Hughes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poole_J/0/1/0/all/0/1"&gt;Jack Poole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dervilis_N/0/1/0/all/0/1"&gt;Nikolaos Dervilis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gardner_P/0/1/0/all/0/1"&gt;Paul Gardner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Worden_K/0/1/0/all/0/1"&gt;Keith Worden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variance-reduced accelerated methods for decentralized stochastic double-regularized nonconvex strongly-concave minimax problems. (arXiv:2307.07113v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2307.07113</id>
        <link href="http://arxiv.org/abs/2307.07113"/>
        <updated>2023-07-17T01:05:35.226Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider the decentralized, stochastic nonconvex
strongly-concave (NCSC) minimax problem with nonsmooth regularization terms on
both primal and dual variables, wherein a network of $m$ computing agents
collaborate via peer-to-peer communications. We consider when the coupling
function is in expectation or finite-sum form and the double regularizers are
convex functions, applied separately to the primal and dual variables. Our
algorithmic framework introduces a Lagrangian multiplier to eliminate the
consensus constraint on the dual variable. Coupling this with
variance-reduction (VR) techniques, our proposed method, entitled VRLM, by a
single neighbor communication per iteration, is able to achieve an
$\mathcal{O}(\kappa^3\varepsilon^{-3})$ sample complexity under the general
stochastic setting, with either a big-batch or small-batch VR option, where
$\kappa$ is the condition number of the problem and $\varepsilon$ is the
desired solution accuracy. With a big-batch VR, we can additionally achieve
$\mathcal{O}(\kappa^2\varepsilon^{-2})$ communication complexity. Under the
special finite-sum setting, our method with a big-batch VR can achieve an
$\mathcal{O}(n + \sqrt{n} \kappa^2\varepsilon^{-2})$ sample complexity and
$\mathcal{O}(\kappa^2\varepsilon^{-2})$ communication complexity, where $n$ is
the number of components in the finite sum. All complexity results match the
best-known results achieved by a few existing methods for solving special cases
of the problem we consider. To the best of our knowledge, this is the first
work which provides convergence guarantees for NCSC minimax problems with
general convex nonsmooth regularizers applied to both the primal and dual
variables in the decentralized stochastic setting. Numerical experiments are
conducted on two machine learning problems. Our code is downloadable from
https://github.com/RPI-OPT/VRLM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Mancino_Ball_G/0/1/0/all/0/1"&gt;Gabriel Mancino-Ball&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yangyang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is Task-Agnostic Explainable AI a Myth?. (arXiv:2307.06963v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2307.06963</id>
        <link href="http://arxiv.org/abs/2307.06963"/>
        <updated>2023-07-17T01:05:35.224Z</updated>
        <summary type="html"><![CDATA[Our work serves as a framework for unifying the challenges of contemporary
explainable AI (XAI). We demonstrate that while XAI methods provide
supplementary and potentially useful output for machine learning models,
researchers and decision-makers should be mindful of their conceptual and
technical limitations, which frequently result in these methods themselves
becoming black boxes. We examine three XAI research avenues spanning image,
textual, and graph data, covering saliency, attention, and graph-type
explainers. Despite the varying contexts and timeframes of the mentioned cases,
the same persistent roadblocks emerge, highlighting the need for a conceptual
breakthrough in the field to address the challenge of compatibility between XAI
methods and application tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chaszczewicz_A/0/1/0/all/0/1"&gt;Alicja Chaszczewicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation. (arXiv:2307.07269v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2307.07269</id>
        <link href="http://arxiv.org/abs/2307.07269"/>
        <updated>2023-07-17T01:05:35.224Z</updated>
        <summary type="html"><![CDATA[It is imperative to ensure the robustness of deep learning models in critical
applications such as, healthcare. While recent advances in deep learning have
improved the performance of volumetric medical image segmentation models, these
models cannot be deployed for real-world applications immediately due to their
vulnerability to adversarial attacks. We present a 3D frequency domain
adversarial attack for volumetric medical image segmentation models and
demonstrate its advantages over conventional input or voxel domain attacks.
Using our proposed attack, we introduce a novel frequency domain adversarial
training approach for optimizing a robust model against voxel and frequency
domain attacks. Moreover, we propose frequency consistency loss to regulate our
frequency domain adversarial training that achieves a better tradeoff between
model's performance on clean and adversarial samples. Code is publicly
available at https://github.com/asif-hanif/vafa.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hanif_A/0/1/0/all/0/1"&gt;Asif Hanif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Naseer_M/0/1/0/all/0/1"&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1"&gt;Salman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mubarak Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahad Shahbaz Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving higher-order Lane-Emden-Fowler type equations using physics-informed neural networks: benchmark tests comparing soft and hard constraints. (arXiv:2307.07302v1 [physics.comp-ph])]]></title>
        <id>http://arxiv.org/abs/2307.07302</id>
        <link href="http://arxiv.org/abs/2307.07302"/>
        <updated>2023-07-17T01:05:35.224Z</updated>
        <summary type="html"><![CDATA[In this paper, numerical methods using Physics-Informed Neural Networks
(PINNs) are presented with the aim to solve higher-order ordinary differential
equations (ODEs). Indeed, this deep-learning technique is successfully applied
for solving different classes of singular ODEs, namely the well known
second-order Lane-Emden equations, third order-order Emden-Fowler equations,
and fourth-order Lane-Emden-Fowler equations. Two variants of PINNs technique
are considered and compared. First, a minimization procedure is used to
constrain the total loss function of the neural network, in which the equation
residual is considered with some weight to form a physics-based loss and added
to the training data loss that contains the initial/boundary conditions.
Second, a specific choice of trial solutions ensuring these conditions as hard
constraints is done in order to satisfy the differential equation, contrary to
the first variant based on training data where the constraints appear as soft
ones. Advantages and drawbacks of PINNs variants are highlighted.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Baty_H/0/1/0/all/0/1"&gt;Hubert Baty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning. (arXiv:2307.07250v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07250</id>
        <link href="http://arxiv.org/abs/2307.07250"/>
        <updated>2023-07-17T01:05:35.219Z</updated>
        <summary type="html"><![CDATA[Adversarial examples derived from deliberately crafted perturbations on
visual inputs can easily harm decision process of deep neural networks. To
prevent potential threats, various adversarial training-based defense methods
have grown rapidly and become a de facto standard approach for robustness.
Despite recent competitive achievements, we observe that adversarial
vulnerability varies across targets and certain vulnerabilities remain
prevalent. Intriguingly, such peculiar phenomenon cannot be relieved even with
deeper architectures and advanced defense methods. To address this issue, in
this paper, we introduce a causal approach called Adversarial Double Machine
Learning (ADML), which allows us to quantify the degree of adversarial
vulnerability for network predictions and capture the effect of treatments on
outcome of interests. ADML can directly estimate causal parameter of
adversarial perturbations per se and mitigate negative effects that can
potentially damage robustness, bridging a causal perspective into the
adversarial vulnerability. Through extensive experiments on various CNN and
Transformer architectures, we corroborate that ADML improves adversarial
robustness with large margins and relieve the empirical observation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1"&gt;Byung-Kwan Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Junho Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1"&gt;Yong Man Ro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inverse Evolution Layers: Physics-informed Regularizers for Deep Neural Networks. (arXiv:2307.07344v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07344</id>
        <link href="http://arxiv.org/abs/2307.07344"/>
        <updated>2023-07-17T01:05:35.219Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel approach to integrating partial differential
equation (PDE)-based evolution models into neural networks through a new type
of regularization. Specifically, we propose inverse evolution layers (IELs)
based on evolution equations. These layers can achieve specific regularization
objectives and endow neural networks' outputs with corresponding properties of
the evolution models. Moreover, IELs are straightforward to construct and
implement, and can be easily designed for various physical evolutions and
neural networks. Additionally, the design process for these layers can provide
neural networks with intuitive and mathematical interpretability, thus
enhancing the transparency and explainability of the approach. To demonstrate
the effectiveness, efficiency, and simplicity of our approach, we present an
example of endowing semantic segmentation models with the smoothness property
based on the heat diffusion model. To achieve this goal, we design
heat-diffusion IELs and apply them to address the challenge of semantic
segmentation with noisy labels. The experimental results demonstrate that the
heat-diffusion IELs can effectively mitigate the overfitting problem caused by
noisy labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chaoyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1"&gt;Zhonghua Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training. (arXiv:2307.07246v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.07246</id>
        <link href="http://arxiv.org/abs/2307.07246"/>
        <updated>2023-07-17T01:05:35.218Z</updated>
        <summary type="html"><![CDATA[The foundation models based on pre-training technology have significantly
advanced artificial intelligence from theoretical to practical applications.
These models have facilitated the feasibility of computer-aided diagnosis for
widespread use. Medical contrastive vision-language pre-training, which does
not require human annotations, is an effective approach for guiding
representation learning using description information in diagnostic reports.
However, the effectiveness of pre-training is limited by the large-scale
semantic overlap and shifting problems in medical field. To address these
issues, we propose the Knowledge-Boosting Contrastive Vision-Language
Pre-training framework (KoBo), which integrates clinical knowledge into the
learning of vision-language semantic consistency. The framework uses an
unbiased, open-set sample-wise knowledge representation to measure negative
sample noise and supplement the correspondence between vision-language mutual
information and clinical knowledge. Extensive experiments validate the effect
of our framework on eight tasks including classification, segmentation,
retrieval, and semantic relatedness, achieving comparable or better performance
with the zero-shot or few-shot settings. Our code is open on
https://github.com/ChenXiaoFei-CS/KoBo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaofei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yuting He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1"&gt;Cheng Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1"&gt;Rongjun Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guanyu Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visualizing Overlapping Biclusterings and Boolean Matrix Factorizations. (arXiv:2307.07396v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07396</id>
        <link href="http://arxiv.org/abs/2307.07396"/>
        <updated>2023-07-17T01:05:35.161Z</updated>
        <summary type="html"><![CDATA[Finding (bi-)clusters in bipartite graphs is a popular data analysis
approach. Analysts typically want to visualize the clusters, which is simple as
long as the clusters are disjoint. However, many modern algorithms find
overlapping clusters, making visualization more complicated. In this paper, we
study the problem of visualizing \emph{a given clustering} of overlapping
clusters in bipartite graphs and the related problem of visualizing Boolean
Matrix Factorizations. We conceptualize three different objectives that any
good visualization should satisfy: (1) proximity of cluster elements, (2) large
consecutive areas of elements from the same cluster, and (3) large
uninterrupted areas in the visualization, regardless of the cluster membership.
We provide objective functions that capture these goals and algorithms that
optimize these objective functions. Interestingly, in experiments on real-world
datasets, we find that the best trade-off between these competing goals is
achieved by a novel heuristic, which locally aims to place rows and columns
with similar cluster membership next to each other.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marette_T/0/1/0/all/0/1"&gt;Thibault Marette&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miettinen_P/0/1/0/all/0/1"&gt;Pauli Miettinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumann_S/0/1/0/all/0/1"&gt;Stefan Neumann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Shape-Based Myocardial Infarction Prediction Using Point Cloud Classification Networks. (arXiv:2307.07298v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.07298</id>
        <link href="http://arxiv.org/abs/2307.07298"/>
        <updated>2023-07-17T01:05:35.158Z</updated>
        <summary type="html"><![CDATA[Myocardial infarction (MI) is one of the most prevalent cardiovascular
diseases with associated clinical decision-making typically based on
single-valued imaging biomarkers. However, such metrics only approximate the
complex 3D structure and physiology of the heart and hence hinder a better
understanding and prediction of MI outcomes. In this work, we investigate the
utility of complete 3D cardiac shapes in the form of point clouds for an
improved detection of MI events. To this end, we propose a fully automatic
multi-step pipeline consisting of a 3D cardiac surface reconstruction step
followed by a point cloud classification network. Our method utilizes recent
advances in geometric deep learning on point clouds to enable direct and
efficient multi-scale learning on high-resolution surface models of the cardiac
anatomy. We evaluate our approach on 1068 UK Biobank subjects for the tasks of
prevalent MI detection and incident MI prediction and find improvements of ~13%
and ~5% respectively over clinical benchmarks. Furthermore, we analyze the role
of each ventricle and cardiac phase for 3D shape-based MI detection and conduct
a visual analysis of the morphological and physiological patterns typically
associated with MI outcomes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beetz_M/0/1/0/all/0/1"&gt;Marcel Beetz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yilong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1"&gt;Abhirup Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grau_V/0/1/0/all/0/1"&gt;Vicente Grau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Different Is Stereotypical Bias Across Languages?. (arXiv:2307.07331v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.07331</id>
        <link href="http://arxiv.org/abs/2307.07331"/>
        <updated>2023-07-17T01:05:35.158Z</updated>
        <summary type="html"><![CDATA[Recent studies have demonstrated how to assess the stereotypical bias in
pre-trained English language models. In this work, we extend this branch of
research in multiple different dimensions by systematically investigating (a)
mono- and multilingual models of (b) different underlying architectures with
respect to their bias in (c) multiple different languages. To that end, we make
use of the English StereoSet data set (Nadeem et al., 2021), which we
semi-automatically translate into German, French, Spanish, and Turkish. We find
that it is of major importance to conduct this type of analysis in a
multilingual setting, as our experiments show a much more nuanced picture as
well as notable differences from the English-only analysis. The main takeaways
from our analysis are that mGPT-2 (partly) shows surprising anti-stereotypical
behavior across languages, English (monolingual) models exhibit the strongest
bias, and the stereotypes reflected in the data set are least present in
Turkish models. Finally, we release our codebase alongside the translated data
sets and practical guidelines for the semi-automatic translation to encourage a
further extension of our work to other languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozturk_I/0/1/0/all/0/1"&gt;Ibrahim Tolga &amp;#xd6;zt&amp;#xfc;rk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nedelchev_R/0/1/0/all/0/1"&gt;Rostislav Nedelchev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heumann_C/0/1/0/all/0/1"&gt;Christian Heumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arias_E/0/1/0/all/0/1"&gt;Esteban Garces Arias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roger_M/0/1/0/all/0/1"&gt;Marius Roger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1"&gt;Bernd Bischl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Assenmacher_M/0/1/0/all/0/1"&gt;Matthias A&amp;#xdf;enmacher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement. (arXiv:2307.07055v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07055</id>
        <link href="http://arxiv.org/abs/2307.07055"/>
        <updated>2023-07-17T01:05:35.156Z</updated>
        <summary type="html"><![CDATA[We explore the methodology and theory of reward-directed generation via
conditional diffusion models. Directed generation aims to generate samples with
desired properties as measured by a reward function, which has broad
applications in generative AI, reinforcement learning, and computational
biology. We consider the common learning scenario where the data set consists
of unlabeled data along with a smaller set of data with noisy reward labels.
Our approach leverages a learned reward function on the smaller data set as a
pseudolabeler. From a theoretical standpoint, we show that this directed
generator can effectively learn and sample from the reward-conditioned data
distribution. Additionally, our model is capable of recovering the latent
subspace representation of data. Moreover, we establish that the model
generates a new population that moves closer to a user-specified target reward
value, where the optimality gap aligns with the off-policy bandit regret in the
feature subspace. The improvement in rewards obtained is influenced by the
interplay between the strength of the reward signal, the distribution shift,
and the cost of off-support extrapolation. We provide empirical results to
validate our theory and highlight the relationship between the strength of
extrapolation and the quality of generated samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Hui Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaixuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_C/0/1/0/all/0/1"&gt;Chengzhuo Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Minshuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengdi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Positional and Structural Encoder. (arXiv:2307.07107v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07107</id>
        <link href="http://arxiv.org/abs/2307.07107"/>
        <updated>2023-07-17T01:05:35.156Z</updated>
        <summary type="html"><![CDATA[Positional and structural encodings (PSE) enable better identifiability of
nodes within a graph, as in general graphs lack a canonical node ordering. This
renders PSEs essential tools for empowering modern GNNs, and in particular
graph Transformers. However, designing PSEs that work optimally for a variety
of graph prediction tasks is a challenging and unsolved problem. Here, we
present the graph positional and structural encoder (GPSE), a first-ever
attempt to train a graph encoder that captures rich PSE representations for
augmenting any GNN. GPSE can effectively learn a common latent representation
for multiple PSEs, and is highly transferable. The encoder trained on a
particular graph dataset can be used effectively on datasets drawn from
significantly different distributions and even modalities. We show that across
a wide range of benchmarks, GPSE-enhanced models can significantly improve the
performance in certain tasks, while performing on par with those that employ
explicitly computed PSEs in other cases. Our results pave the way for the
development of large pre-trained models for extracting graph positional and
structural information and highlight their potential as a viable alternative to
explicitly computed PSEs as well as to existing self-supervised pre-training
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Renming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canturk_S/0/1/0/all/0/1"&gt;Semih Cant&amp;#xfc;rk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapointe_Gagne_O/0/1/0/all/0/1"&gt;Olivier Lapointe-Gagn&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Letourneau_V/0/1/0/all/0/1"&gt;Vincent L&amp;#xe9;tourneau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1"&gt;Guy Wolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beaini_D/0/1/0/all/0/1"&gt;Dominique Beaini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rampasek_L/0/1/0/all/0/1"&gt;Ladislav Ramp&amp;#xe1;&amp;#x161;ek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Performance of $\ell_1$ Regularization for Sparse Convex Optimization. (arXiv:2307.07405v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07405</id>
        <link href="http://arxiv.org/abs/2307.07405"/>
        <updated>2023-07-17T01:05:35.153Z</updated>
        <summary type="html"><![CDATA[Despite widespread adoption in practice, guarantees for the LASSO and Group
LASSO are strikingly lacking in settings beyond statistical problems, and these
algorithms are usually considered to be a heuristic in the context of sparse
convex optimization on deterministic inputs. We give the first recovery
guarantees for the Group LASSO for sparse convex optimization with
vector-valued features. We show that if a sufficiently large Group LASSO
regularization is applied when minimizing a strictly convex function $l$, then
the minimizer is a sparse vector supported on vector-valued features with the
largest $\ell_2$ norm of the gradient. Thus, repeating this procedure selects
the same set of features as the Orthogonal Matching Pursuit algorithm, which
admits recovery guarantees for any function $l$ with restricted strong
convexity and smoothness via weak submodularity arguments. This answers open
questions of Tibshirani et al. and Yasuda et al. Our result is the first to
theoretically explain the empirical success of the Group LASSO for convex
functions under general input instances assuming only restricted strong
convexity and smoothness. Our result also generalizes provable guarantees for
the Sequential Attention algorithm, which is a feature selection algorithm
inspired by the attention mechanism proposed by Yasuda et al.

As an application of our result, we give new results for the column subset
selection problem, which is well-studied when the loss is the Frobenius norm or
other entrywise matrix losses. We give the first result for general loss
functions for this problem that requires only restricted strong convexity and
smoothness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Axiotis_K/0/1/0/all/0/1"&gt;Kyriakos Axiotis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yasuda_T/0/1/0/all/0/1"&gt;Taisuke Yasuda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Expressive Monotonic Neural Networks. (arXiv:2307.07512v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07512</id>
        <link href="http://arxiv.org/abs/2307.07512"/>
        <updated>2023-07-17T01:05:35.152Z</updated>
        <summary type="html"><![CDATA[The monotonic dependence of the outputs of a neural network on some of its
inputs is a crucial inductive bias in many scenarios where domain knowledge
dictates such behavior. This is especially important for interpretability and
fairness considerations. In a broader context, scenarios in which monotonicity
is important can be found in finance, medicine, physics, and other disciplines.
It is thus desirable to build neural network architectures that implement this
inductive bias provably. In this work, we propose a weight-constrained
architecture with a single residual connection to achieve exact monotonic
dependence in any subset of the inputs. The weight constraint scheme directly
controls the Lipschitz constant of the neural network and thus provides the
additional benefit of robustness. Compared to currently existing techniques
used for monotonicity, our method is simpler in implementation and in theory
foundations, has negligible computational overhead, is guaranteed to produce
monotonic dependence, and is highly expressive. We show how the algorithm is
used to train powerful, robust, and interpretable discriminators that achieve
competitive performance compared to current state-of-the-art methods across
various benchmarks, from social applications to the classification of the
decays of subatomic particles produced at the CERN Large Hadron Collider.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kitouni_O/0/1/0/all/0/1"&gt;Ouail Kitouni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nolte_N/0/1/0/all/0/1"&gt;Niklas Nolte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_M/0/1/0/all/0/1"&gt;Michael Williams&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning-Assisted Pattern Recognition Algorithms for Estimating Ultimate Tensile Strength in Fused Deposition Modeled Polylactic Acid Specimens. (arXiv:2307.06970v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.06970</id>
        <link href="http://arxiv.org/abs/2307.06970"/>
        <updated>2023-07-17T01:05:35.142Z</updated>
        <summary type="html"><![CDATA[In this study, we investigate the application of supervised machine learning
algorithms for estimating the Ultimate Tensile Strength (UTS) of Polylactic
Acid (PLA) specimens fabricated using the Fused Deposition Modeling (FDM)
process. A total of 31 PLA specimens were prepared, with Infill Percentage,
Layer Height, Print Speed, and Extrusion Temperature serving as input
parameters. The primary objective was to assess the accuracy and effectiveness
of four distinct supervised classification algorithms, namely Logistic
Classification, Gradient Boosting Classification, Decision Tree, and K-Nearest
Neighbor, in predicting the UTS of the specimens. The results revealed that
while the Decision Tree and K-Nearest Neighbor algorithms both achieved an F1
score of 0.71, the KNN algorithm exhibited a higher Area Under the Curve (AUC)
score of 0.79, outperforming the other algorithms. This demonstrates the
superior ability of the KNN algorithm in differentiating between the two
classes of ultimate tensile strength within the dataset, rendering it the most
favorable choice for classification in the context of this research. This study
represents the first attempt to estimate the UTS of PLA specimens using machine
learning-based classification algorithms, and the findings offer valuable
insights into the potential of these techniques in improving the performance
and accuracy of predictive models in the domain of additive manufacturing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1"&gt;Akshansh Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jatti_V/0/1/0/all/0/1"&gt;Vijaykumar S Jatti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Interpolating Experts and Multi-Armed Bandits. (arXiv:2307.07264v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07264</id>
        <link href="http://arxiv.org/abs/2307.07264"/>
        <updated>2023-07-17T01:05:35.079Z</updated>
        <summary type="html"><![CDATA[Learning with expert advice and multi-armed bandit are two classic online
decision problems which differ on how the information is observed in each round
of the game. We study a family of problems interpolating the two. For a vector
$\mathbf{m}=(m_1,\dots,m_K)\in \mathbb{N}^K$, an instance of $\mathbf{m}$-MAB
indicates that the arms are partitioned into $K$ groups and the $i$-th group
contains $m_i$ arms. Once an arm is pulled, the losses of all arms in the same
group are observed. We prove tight minimax regret bounds for $\mathbf{m}$-MAB
and design an optimal PAC algorithm for its pure exploration version,
$\mathbf{m}$-BAI, where the goal is to identify the arm with minimum loss with
as few rounds as possible. We show that the minimax regret of $\mathbf{m}$-MAB
is $\Theta\left(\sqrt{T\sum_{k=1}^K\log (m_k+1)}\right)$ and the minimum number
of pulls for an $(\epsilon,0.05)$-PAC algorithm of $\mathbf{m}$-BAI is
$\Theta\left(\frac{1}{\epsilon^2}\cdot \sum_{k=1}^K\log (m_k+1)\right)$. Both
our upper bounds and lower bounds for $\mathbf{m}$-MAB can be extended to a
more general setting, namely the bandit with graph feedback, in terms of the
clique cover and related graph parameters. As consequences, we obtained tight
minimax regret bounds for several families of feedback graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Houshuang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yuchen He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chihao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pathway toward prior knowledge-integrated machine learning in engineering. (arXiv:2307.06950v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2307.06950</id>
        <link href="http://arxiv.org/abs/2307.06950"/>
        <updated>2023-07-17T01:05:35.077Z</updated>
        <summary type="html"><![CDATA[Despite the digitalization trend and data volume surge, first-principles
models (also known as logic-driven, physics-based, rule-based, or
knowledge-based models) and data-driven approaches have existed in parallel,
mirroring the ongoing AI debate on symbolism versus connectionism. Research for
process development to integrate both sides to transfer and utilize domain
knowledge in the data-driven process is rare. This study emphasizes efforts and
prevailing trends to integrate multidisciplinary domain professions into
machine acknowledgeable, data-driven processes in a two-fold organization:
examining information uncertainty sources in knowledge representation and
exploring knowledge decomposition with a three-tier knowledge-integrated
machine learning paradigm. This approach balances holist and reductionist
perspectives in the engineering domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xia Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geyer_P/0/1/0/all/0/1"&gt;Philipp Geyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI For Global Climate Cooperation 2023 Competition Proceedings. (arXiv:2307.06951v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2307.06951</id>
        <link href="http://arxiv.org/abs/2307.06951"/>
        <updated>2023-07-17T01:05:35.077Z</updated>
        <summary type="html"><![CDATA[The international community must collaborate to mitigate climate change and
sustain economic growth. However, collaboration is hard to achieve, partly
because no global authority can ensure compliance with international climate
agreements. Combining AI with climate-economic simulations offers a promising
solution to design international frameworks, including negotiation protocols
and climate agreements, that promote and incentivize collaboration. In
addition, these frameworks should also have policy goals fulfillment, and
sustained commitment, taking into account climate-economic dynamics and
strategic behaviors. These challenges require an interdisciplinary approach
across machine learning, economics, climate science, law, policy, ethics, and
other fields.

Towards this objective, we organized AI for Global Climate Cooperation, a
Mila competition in which teams submitted proposals and analyses of
international frameworks, based on (modifications of) RICE-N, an AI-driven
integrated assessment model (IAM). In particular, RICE-N supports modeling
regional decision-making using AI agents. Furthermore, the IAM then models the
climate-economic impact of those decisions into the future.

Whereas the first track focused only on performance metrics, the proposals
submitted to the second track were evaluated both quantitatively and
qualitatively. The quantitative evaluation focused on a combination of (i) the
degree of mitigation of global temperature rise and (ii) the increase in
economic productivity. On the other hand, an interdisciplinary panel of human
experts in law, policy, sociology, economics and environmental science,
evaluated the solutions qualitatively. In particular, the panel considered the
effectiveness, simplicity, feasibility, ethics, and notions of climate justice
of the protocols. In the third track, the participants were asked to critique
and improve RICE-N.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1"&gt;Prateek Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phade_S/0/1/0/all/0/1"&gt;Soham Phade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasa_S/0/1/0/all/0/1"&gt;Sunil Srinivasa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1"&gt;Andrew Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Stephan Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Sparse Neural Networks with Identity Layers. (arXiv:2307.07389v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07389</id>
        <link href="http://arxiv.org/abs/2307.07389"/>
        <updated>2023-07-17T01:05:35.076Z</updated>
        <summary type="html"><![CDATA[The sparsity of Deep Neural Networks is well investigated to maximize the
performance and reduce the size of overparameterized networks as possible.
Existing methods focus on pruning parameters in the training process by using
thresholds and metrics. Meanwhile, feature similarity between different layers
has not been discussed sufficiently before, which could be rigorously proved to
be highly correlated to the network sparsity in this paper. Inspired by
interlayer feature similarity in overparameterized models, we investigate the
intrinsic link between network sparsity and interlayer feature similarity.
Specifically, we prove that reducing interlayer feature similarity based on
Centered Kernel Alignment (CKA) improves the sparsity of the network by using
information bottleneck theory. Applying such theory, we propose a plug-and-play
CKA-based Sparsity Regularization for sparse network training, dubbed CKA-SR,
which utilizes CKA to reduce feature similarity between layers and increase
network sparsity. In other words, layers of our sparse network tend to have
their own identity compared to each other. Experimentally, we plug the proposed
CKA-SR into the training process of sparse network training methods and find
that CKA-SR consistently improves the performance of several State-Of-The-Art
sparse training methods, especially at extremely high sparsity. Code is
included in the supplementary materials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ni_M/0/1/0/all/0/1"&gt;Mingjian Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guangyao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xiawu Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1"&gt;Peixi Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Li Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AIC-AB NET: A Neural Network for Image Captioning with Spatial Attention and Text Attributes. (arXiv:2307.07370v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.07370</id>
        <link href="http://arxiv.org/abs/2307.07370"/>
        <updated>2023-07-17T01:05:35.075Z</updated>
        <summary type="html"><![CDATA[Image captioning is a significant field across computer vision and natural
language processing. We propose and present AIC-AB NET, a novel
Attribute-Information-Combined Attention-Based Network that combines spatial
attention architecture and text attributes in an encoder-decoder. For caption
generation, adaptive spatial attention determines which image region best
represents the image and whether to attend to the visual features or the visual
sentinel. Text attribute information is synchronously fed into the decoder to
help image recognition and reduce uncertainty. We have tested and evaluated our
AICAB NET on the MS COCO dataset and a new proposed Fashion dataset. The
Fashion dataset is employed as a benchmark of single-object images. The results
show the superior performance of the proposed model compared to the
state-of-the-art baseline and ablated models on both the images from MSCOCO and
our single-object images. Our AIC-AB NET outperforms the baseline adaptive
attention network by 0.017 (CIDEr score) on the MS COCO dataset and 0.095
(CIDEr score) on the Fashion dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tu_G/0/1/0/all/0/1"&gt;Guoyun Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Ying Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vlassov_V/0/1/0/all/0/1"&gt;Vladimir Vlassov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Higher-order topological kernels via quantum computation. (arXiv:2307.07383v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2307.07383</id>
        <link href="http://arxiv.org/abs/2307.07383"/>
        <updated>2023-07-17T01:05:35.073Z</updated>
        <summary type="html"><![CDATA[Topological data analysis (TDA) has emerged as a powerful tool for extracting
meaningful insights from complex data. TDA enhances the analysis of objects by
embedding them into a simplicial complex and extracting useful global
properties such as the Betti numbers, i.e. the number of multidimensional
holes, which can be used to define kernel methods that are easily integrated
with existing machine-learning algorithms. These kernel methods have found
broad applications, as they rely on powerful mathematical frameworks which
provide theoretical guarantees on their performance. However, the computation
of higher-dimensional Betti numbers can be prohibitively expensive on classical
hardware, while quantum algorithms can approximate them in polynomial time in
the instance size. In this work, we propose a quantum approach to defining
topological kernels, which is based on constructing Betti curves, i.e.
topological fingerprint of filtrations with increasing order. We exhibit a
working prototype of our approach implemented on a noiseless simulator and show
its robustness by means of some empirical results suggesting that topological
approaches may offer an advantage in quantum machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Incudini_M/0/1/0/all/0/1"&gt;Massimiliano Incudini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Martini_F/0/1/0/all/0/1"&gt;Francesco Martini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Pierro_A/0/1/0/all/0/1"&gt;Alessandra Di Pierro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LINFA: a Python library for variational inference with normalizing flow and annealing. (arXiv:2307.04675v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.04675</id>
        <link href="http://arxiv.org/abs/2307.04675"/>
        <updated>2023-07-17T01:05:35.073Z</updated>
        <summary type="html"><![CDATA[Variational inference is an increasingly popular method in statistics and
machine learning for approximating probability distributions. We developed
LINFA (Library for Inference with Normalizing Flow and Annealing), a Python
library for variational inference to accommodate computationally expensive
models and difficult-to-sample distributions with dependent parameters. We
discuss the theoretical background, capabilities, and performance of LINFA in
various benchmarks. LINFA is publicly available on GitHub at
https://github.com/desResLab/LINFA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cobian_E/0/1/0/all/0/1"&gt;Emma R. Cobian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jubilee Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hauenstein_J/0/1/0/all/0/1"&gt;Jonathan D. Hauenstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schiavazzi_D/0/1/0/all/0/1"&gt;Daniele E. Schiavazzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MGit: A Model Versioning and Management System. (arXiv:2307.07507v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07507</id>
        <link href="http://arxiv.org/abs/2307.07507"/>
        <updated>2023-07-17T01:05:35.072Z</updated>
        <summary type="html"><![CDATA[Models derived from other models are extremely common in machine learning
(ML) today. For example, transfer learning is used to create task-specific
models from "pre-trained" models through finetuning. This has led to an
ecosystem where models are related to each other, sharing structure and often
even parameter values. However, it is hard to manage these model derivatives:
the storage overhead of storing all derived models quickly becomes onerous,
prompting users to get rid of intermediate models that might be useful for
further analysis. Additionally, undesired behaviors in models are hard to track
down (e.g., is a bug inherited from an upstream model?). In this paper, we
propose a model versioning and management system called MGit that makes it
easier to store, test, update, and collaborate on model derivatives. MGit
introduces a lineage graph that records provenance and versioning information
between models, optimizations to efficiently store model parameters, as well as
abstractions over this lineage graph that facilitate relevant testing, updating
and collaboration functionality. MGit is able to reduce the lineage graph's
storage footprint by up to 7x and automatically update downstream models in
response to updates to upstream models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1"&gt;Wei Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendoza_D/0/1/0/all/0/1"&gt;Daniel Mendoza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1"&gt;Rafael da Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_D/0/1/0/all/0/1"&gt;Deepak Narayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phanishaye_A/0/1/0/all/0/1"&gt;Amar Phanishaye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signed iterative random forests to identify enhancer-associated transcription factor binding. (arXiv:1810.07287v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1810.07287</id>
        <link href="http://arxiv.org/abs/1810.07287"/>
        <updated>2023-07-17T01:05:35.072Z</updated>
        <summary type="html"><![CDATA[Standard ChIP-seq peak calling pipelines seek to differentiate biochemically
reproducible signals of individual genomic elements from background noise.
However, reproducibility alone does not imply functional regulation (e.g.,
enhancer activation, alternative splicing). Here we present a general-purpose,
interpretable machine learning method: signed iterative random forests (siRF),
which we use to infer regulatory interactions among transcription factors and
functional binding signatures surrounding enhancer elements in Drosophila
melanogaster.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kumbier_K/0/1/0/all/0/1"&gt;Karl Kumbier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Basu_S/0/1/0/all/0/1"&gt;Sumanta Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Frise_E/0/1/0/all/0/1"&gt;Erwin Frise&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Celniker_S/0/1/0/all/0/1"&gt;Susan E. Celniker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Brown_J/0/1/0/all/0/1"&gt;James B. Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Celniker_S/0/1/0/all/0/1"&gt;Susan Celniker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bin Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HuCurl: Human-induced Curriculum Discovery. (arXiv:2307.07412v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07412</id>
        <link href="http://arxiv.org/abs/2307.07412"/>
        <updated>2023-07-17T01:05:35.059Z</updated>
        <summary type="html"><![CDATA[We introduce the problem of curriculum discovery and describe a curriculum
learning framework capable of discovering effective curricula in a curriculum
space based on prior knowledge about sample difficulty. Using annotation
entropy and loss as measures of difficulty, we show that (i): the
top-performing discovered curricula for a given model and dataset are often
non-monotonic as opposed to monotonic curricula in existing literature, (ii):
the prevailing easy-to-hard or hard-to-easy transition curricula are often at
the risk of underperforming, and (iii): the curricula discovered for smaller
datasets and models perform well on larger datasets and models respectively.
The proposed framework encompasses some of the existing curriculum learning
approaches and can discover curricula that outperform them across several NLP
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elgaar_M/0/1/0/all/0/1"&gt;Mohamed Elgaar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amiri_H/0/1/0/all/0/1"&gt;Hadi Amiri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy. (arXiv:2307.07328v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2307.07328</id>
        <link href="http://arxiv.org/abs/2307.07328"/>
        <updated>2023-07-17T01:05:35.051Z</updated>
        <summary type="html"><![CDATA[Data-poisoning based backdoor attacks aim to insert backdoor into models by
manipulating training datasets without controlling the training process of the
target model. Existing attack methods mainly focus on designing triggers or
fusion strategies between triggers and benign samples. However, they often
randomly select samples to be poisoned, disregarding the varying importance of
each poisoning sample in terms of backdoor injection. A recent selection
strategy filters a fixed-size poisoning sample pool by recording forgetting
events, but it fails to consider the remaining samples outside the pool from a
global perspective. Moreover, computing forgetting events requires significant
additional computing resources. Therefore, how to efficiently and effectively
select poisoning samples from the entire dataset is an urgent problem in
backdoor attacks.To address it, firstly, we introduce a poisoning mask into the
regular backdoor training loss. We suppose that a backdoored model training
with hard poisoning samples has a more backdoor effect on easy ones, which can
be implemented by hindering the normal training process (\ie, maximizing loss
\wrt mask). To further integrate it with normal training process, we then
propose a learnable poisoning sample selection strategy to learn the mask
together with the model parameters through a min-max optimization.Specifically,
the outer loop aims to achieve the backdoor attack goal by minimizing the loss
based on the selected samples, while the inner loop selects hard poisoning
samples that impede this goal by maximizing the loss. After several rounds of
adversarial training, we finally select effective poisoning samples with high
contribution. Extensive experiments on benchmark datasets demonstrate the
effectiveness and efficiency of our approach in boosting backdoor attack
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zihao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mingda Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1"&gt;Shaokui Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Li Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yanbo Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1"&gt;Baoyuan Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defect Classification in Additive Manufacturing Using CNN-Based Vision Processing. (arXiv:2307.07378v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.07378</id>
        <link href="http://arxiv.org/abs/2307.07378"/>
        <updated>2023-07-17T01:05:35.051Z</updated>
        <summary type="html"><![CDATA[The development of computer vision and in-situ monitoring using visual
sensors allows the collection of large datasets from the additive manufacturing
(AM) process. Such datasets could be used with machine learning techniques to
improve the quality of AM. This paper examines two scenarios: first, using
convolutional neural networks (CNNs) to accurately classify defects in an image
dataset from AM and second, applying active learning techniques to the
developed classification model. This allows the construction of a
human-in-the-loop mechanism to reduce the size of the data required to train
and generate training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mileo_A/0/1/0/all/0/1"&gt;Alessandra Mileo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smeaton_A/0/1/0/all/0/1"&gt;Alan F. Smeaton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controllable Emphasis with zero data for text-to-speech. (arXiv:2307.07062v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2307.07062</id>
        <link href="http://arxiv.org/abs/2307.07062"/>
        <updated>2023-07-17T01:05:35.050Z</updated>
        <summary type="html"><![CDATA[We present a scalable method to produce high quality emphasis for
text-to-speech (TTS) that does not require recordings or annotations. Many TTS
models include a phoneme duration model. A simple but effective method to
achieve emphasized speech consists in increasing the predicted duration of the
emphasised word. We show that this is significantly better than spectrogram
modification techniques improving naturalness by $7.3\%$ and correct testers'
identification of the emphasized word in a sentence by $40\%$ on a reference
female en-US voice. We show that this technique significantly closes the gap to
methods that require explicit recordings. The method proved to be scalable and
preferred in all four languages tested (English, Spanish, Italian, German), for
different voices and multiple speaking styles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Joly_A/0/1/0/all/0/1"&gt;Arnaud Joly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nicolis_M/0/1/0/all/0/1"&gt;Marco Nicolis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peterova_E/0/1/0/all/0/1"&gt;Ekaterina Peterova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lombardi_A/0/1/0/all/0/1"&gt;Alessandro Lombardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abbas_A/0/1/0/all/0/1"&gt;Ammar Abbas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Korlaar_A/0/1/0/all/0/1"&gt;Arent van Korlaar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hussain_A/0/1/0/all/0/1"&gt;Aman Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sharma_P/0/1/0/all/0/1"&gt;Parul Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moinet_A/0/1/0/all/0/1"&gt;Alexis Moinet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lajszczak_M/0/1/0/all/0/1"&gt;Mateusz Lajszczak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karanasou_P/0/1/0/all/0/1"&gt;Penny Karanasou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bonafonte_A/0/1/0/all/0/1"&gt;Antonio Bonafonte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Drugman_T/0/1/0/all/0/1"&gt;Thomas Drugman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sokolova_E/0/1/0/all/0/1"&gt;Elena Sokolova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Zero-Shot Generalization for CLIP with Synthesized Prompts. (arXiv:2307.07397v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.07397</id>
        <link href="http://arxiv.org/abs/2307.07397"/>
        <updated>2023-07-17T01:05:35.047Z</updated>
        <summary type="html"><![CDATA[With the growing interest in pretrained vision-language models like CLIP,
recent research has focused on adapting these models to downstream tasks.
Despite achieving promising results, most existing methods require labeled data
for all classes, which may not hold in real-world applications due to the long
tail and Zipf's law. For example, some classes may lack labeled data entirely,
such as emerging concepts. To address this problem, we propose a plug-and-play
generative approach called \textbf{S}ynt\textbf{H}es\textbf{I}zed
\textbf{P}rompts~(\textbf{SHIP}) to improve existing fine-tuning methods.
Specifically, we follow variational autoencoders to introduce a generator that
reconstructs the visual features by inputting the synthesized prompts and the
corresponding class names to the textual encoder of CLIP. In this manner, we
easily obtain the synthesized features for the remaining label-only classes.
Thereafter, we fine-tune CLIP with off-the-shelf methods by combining labeled
and synthesized features. Extensive experiments on base-to-new generalization,
cross-dataset transfer learning, and generalized zero-shot learning demonstrate
the superiority of our approach. The code is available at
\url{https://github.com/mrflogs/SHIP}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhengbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jian Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1"&gt;Ran He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1"&gt;Nan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zilei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1"&gt;Tieniu Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Atlas-Based Interpretable Age Prediction. (arXiv:2307.07439v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2307.07439</id>
        <link href="http://arxiv.org/abs/2307.07439"/>
        <updated>2023-07-17T01:05:35.046Z</updated>
        <summary type="html"><![CDATA[Age prediction is an important part of medical assessments and research. It
can aid in detecting diseases as well as abnormal ageing by highlighting the
discrepancy between chronological and biological age. To gain a comprehensive
understanding of age-related changes observed in various body parts, we
investigate them on a larger scale by using whole-body images. We utilise the
Grad-CAM interpretability method to determine the body areas most predictive of
a person's age. We expand our analysis beyond individual subjects by employing
registration techniques to generate population-wide interpretability maps.
Furthermore, we set state-of-the-art whole-body age prediction with a model
that achieves a mean absolute error of 2.76 years. Our findings reveal three
primary areas of interest: the spine, the autochthonous back muscles, and the
cardiac region, which exhibits the highest importance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Starck_S/0/1/0/all/0/1"&gt;Sophie Starck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kini_Y/0/1/0/all/0/1"&gt;Yadunandan Vivekanand Kini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ritter_J/0/1/0/all/0/1"&gt;Jessica Johanna Maria Ritter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Braren_R/0/1/0/all/0/1"&gt;Rickmer Braren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mueller_T/0/1/0/all/0/1"&gt;Tamara Mueller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Augmentation for Mathematical Objects. (arXiv:2307.06984v1 [cs.SC])]]></title>
        <id>http://arxiv.org/abs/2307.06984</id>
        <link href="http://arxiv.org/abs/2307.06984"/>
        <updated>2023-07-17T01:05:35.045Z</updated>
        <summary type="html"><![CDATA[This paper discusses and evaluates ideas of data balancing and data
augmentation in the context of mathematical objects: an important topic for
both the symbolic computation and satisfiability checking communities, when
they are making use of machine learning techniques to optimise their tools. We
consider a dataset of non-linear polynomial problems and the problem of
selecting a variable ordering for cylindrical algebraic decomposition to tackle
these with. By swapping the variable names in already labelled problems, we
generate new problem instances that do not require any further labelling when
viewing the selection as a classification problem. We find this augmentation
increases the accuracy of ML models by 63% on average. We study what part of
this improvement is due to the balancing of the dataset and what is achieved
thanks to further increasing the size of the dataset, concluding that both have
a very significant effect. We finish the paper by reflecting on how this idea
could be applied in other uses of machine learning in mathematics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rio_T/0/1/0/all/0/1"&gt;Tereso del Rio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+England_M/0/1/0/all/0/1"&gt;Matthew England&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Population Expansion for Training Language Models with Private Federated Learning. (arXiv:2307.07477v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07477</id>
        <link href="http://arxiv.org/abs/2307.07477"/>
        <updated>2023-07-17T01:05:35.011Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) combined with differential privacy (DP) offers
machine learning (ML) training with distributed devices and with a formal
privacy guarantee. With a large population of devices, FL with DP produces a
performant model in a timely manner. However, for applications with a smaller
population, not only does the model utility degrade as the DP noise is
inversely proportional to population, but also the training latency increases
since waiting for enough clients to become available from a smaller pool is
slower. In this work, we thus propose expanding the population based on domain
adaptation techniques to speed up the training and improves the final model
quality when training with small populations. We empirically demonstrate that
our techniques can improve the utility by 13% to 30% on real-world language
modeling datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koga_T/0/1/0/all/0/1"&gt;Tatsuki Koga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Congzheng Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pelikan_M/0/1/0/all/0/1"&gt;Martin Pelikan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chitnis_M/0/1/0/all/0/1"&gt;Mona Chitnis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Convergence Analysis and SNR Control Strategies for Federated Learning in the Presence of Noise. (arXiv:2307.07406v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07406</id>
        <link href="http://arxiv.org/abs/2307.07406"/>
        <updated>2023-07-17T01:05:35.008Z</updated>
        <summary type="html"><![CDATA[We propose an improved convergence analysis technique that characterizes the
distributed learning paradigm of federated learning (FL) with imperfect/noisy
uplink and downlink communications. Such imperfect communication scenarios
arise in the practical deployment of FL in emerging communication systems and
protocols. The analysis developed in this paper demonstrates, for the first
time, that there is an asymmetry in the detrimental effects of uplink and
downlink communications in FL. In particular, the adverse effect of the
downlink noise is more severe on the convergence of FL algorithms. Using this
insight, we propose improved Signal-to-Noise (SNR) control strategies that,
discarding the negligible higher-order terms, lead to a similar convergence
rate for FL as in the case of a perfect, noise-free communication channel while
incurring significantly less power resources compared to existing solutions. In
particular, we establish that to maintain the $O(\frac{1}{\sqrt{K}})$ rate of
convergence like in the case of noise-free FL, we need to scale down the uplink
and downlink noise by $\Omega({\sqrt{k}})$ and $\Omega({k})$ respectively,
where $k$ denotes the communication round, $k=1,\dots, K$. Our theoretical
result is further characterized by two major benefits: firstly, it does not
assume the somewhat unrealistic assumption of bounded client dissimilarity, and
secondly, it only requires smooth non-convex loss functions, a function class
better suited for modern machine learning and deep learning models. We also
perform extensive empirical analysis to verify the validity of our theoretical
findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Upadhyay_A/0/1/0/all/0/1"&gt;Antesh Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashemi_A/0/1/0/all/0/1"&gt;Abolfazl Hashemi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embracing the chaos: analysis and diagnosis of numerical instability in variational flows. (arXiv:2307.06957v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.06957</id>
        <link href="http://arxiv.org/abs/2307.06957"/>
        <updated>2023-07-17T01:05:35.005Z</updated>
        <summary type="html"><![CDATA[In this paper, we investigate the impact of numerical instability on the
reliability of sampling, density evaluation, and evidence lower bound (ELBO)
estimation in variational flows. We first empirically demonstrate that common
flows can exhibit a catastrophic accumulation of error: the numerical flow map
deviates significantly from the exact map -- which affects sampling -- and the
numerical inverse flow map does not accurately recover the initial input --
which affects density and ELBO computations. Surprisingly though, we find that
results produced by flows are often accurate enough for applications despite
the presence of serious numerical instability. In this work, we treat
variational flows as dynamical systems, and leverage shadowing theory to
elucidate this behavior via theoretical guarantees on the error of sampling,
density evaluation, and ELBO estimation. Finally, we develop and empirically
test a diagnostic procedure that can be used to validate results produced by
numerically unstable flows in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zuheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Campbell_T/0/1/0/all/0/1"&gt;Trevor Campbell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rician likelihood loss for quantitative MRI using self-supervised deep learning. (arXiv:2307.07072v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07072</id>
        <link href="http://arxiv.org/abs/2307.07072"/>
        <updated>2023-07-17T01:05:35.005Z</updated>
        <summary type="html"><![CDATA[Purpose: Previous quantitative MR imaging studies using self-supervised deep
learning have reported biased parameter estimates at low SNR. Such systematic
errors arise from the choice of Mean Squared Error (MSE) loss function for
network training, which is incompatible with Rician-distributed MR magnitude
signals. To address this issue, we introduce the negative log Rician likelihood
(NLR) loss. Methods: A numerically stable and accurate implementation of the
NLR loss was developed to estimate quantitative parameters of the apparent
diffusion coefficient (ADC) model and intra-voxel incoherent motion (IVIM)
model. Parameter estimation accuracy, precision and overall error were
evaluated in terms of bias, variance and root mean squared error and compared
against the MSE loss over a range of SNRs (5 - 30). Results: Networks trained
with NLR loss show higher estimation accuracy than MSE for the ADC and IVIM
diffusion coefficients as SNR decreases, with minimal loss of precision or
total error. At high effective SNR (high SNR and small diffusion coefficients),
both losses show comparable accuracy and precision for all parameters of both
models. Conclusion: The proposed NLR loss is numerically stable and accurate
across the full range of tested SNRs and improves parameter estimation accuracy
of diffusion coefficients using self-supervised deep learning. We expect the
development to benefit quantitative MR imaging techniques broadly, enabling
more accurate parameter estimation from noisy data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parker_C/0/1/0/all/0/1"&gt;Christopher S. Parker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schroder_A/0/1/0/all/0/1"&gt;Anna Schroder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Epstein_S/0/1/0/all/0/1"&gt;Sean C. Epstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cole_J/0/1/0/all/0/1"&gt;James Cole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alexander_D/0/1/0/all/0/1"&gt;Daniel C. Alexander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditionally Optimistic Exploration for Cooperative Deep Multi-Agent Reinforcement Learning. (arXiv:2303.09032v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.09032</id>
        <link href="http://arxiv.org/abs/2303.09032"/>
        <updated>2023-07-17T01:05:35.004Z</updated>
        <summary type="html"><![CDATA[Efficient exploration is critical in cooperative deep Multi-Agent
Reinforcement Learning (MARL). In this work, we propose an exploration method
that effectively encourages cooperative exploration based on the idea of
sequential action-computation scheme. The high-level intuition is that to
perform optimism-based exploration, agents would explore cooperative strategies
if each agent's optimism estimate captures a structured dependency relationship
with other agents. Assuming agents compute actions following a sequential order
at \textit{each environment timestep}, we provide a perspective to view MARL as
tree search iterations by considering agents as nodes at different depths of
the search tree. Inspired by the theoretically justified tree search algorithm
UCT (Upper Confidence bounds applied to Trees), we develop a method called
Conditionally Optimistic Exploration (COE). COE augments each agent's
state-action value estimate with an action-conditioned optimistic bonus derived
from the visitation count of the global state and joint actions of preceding
agents. COE is performed during training and disabled at deployment, making it
compatible with any value decomposition method for centralized training with
decentralized execution. Experiments across various cooperative MARL benchmarks
show that COE outperforms current state-of-the-art exploration methods on
hard-exploration tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xutong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yangchen Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chenjun Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1"&gt;Sarath Chandar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajendran_J/0/1/0/all/0/1"&gt;Janarthanan Rajendran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Different Is Stereotypical Bias Across Languages?. (arXiv:2307.07331v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.07331</id>
        <link href="http://arxiv.org/abs/2307.07331"/>
        <updated>2023-07-17T01:05:35.002Z</updated>
        <summary type="html"><![CDATA[Recent studies have demonstrated how to assess the stereotypical bias in
pre-trained English language models. In this work, we extend this branch of
research in multiple different dimensions by systematically investigating (a)
mono- and multilingual models of (b) different underlying architectures with
respect to their bias in (c) multiple different languages. To that end, we make
use of the English StereoSet data set (Nadeem et al., 2021), which we
semi-automatically translate into German, French, Spanish, and Turkish. We find
that it is of major importance to conduct this type of analysis in a
multilingual setting, as our experiments show a much more nuanced picture as
well as notable differences from the English-only analysis. The main takeaways
from our analysis are that mGPT-2 (partly) shows surprising anti-stereotypical
behavior across languages, English (monolingual) models exhibit the strongest
bias, and the stereotypes reflected in the data set are least present in
Turkish models. Finally, we release our codebase alongside the translated data
sets and practical guidelines for the semi-automatic translation to encourage a
further extension of our work to other languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozturk_I/0/1/0/all/0/1"&gt;Ibrahim Tolga &amp;#xd6;zt&amp;#xfc;rk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nedelchev_R/0/1/0/all/0/1"&gt;Rostislav Nedelchev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heumann_C/0/1/0/all/0/1"&gt;Christian Heumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arias_E/0/1/0/all/0/1"&gt;Esteban Garces Arias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roger_M/0/1/0/all/0/1"&gt;Marius Roger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1"&gt;Bernd Bischl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Assenmacher_M/0/1/0/all/0/1"&gt;Matthias A&amp;#xdf;enmacher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signed iterative random forests to identify enhancer-associated transcription factor binding. (arXiv:1810.07287v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1810.07287</id>
        <link href="http://arxiv.org/abs/1810.07287"/>
        <updated>2023-07-17T01:05:34.999Z</updated>
        <summary type="html"><![CDATA[Standard ChIP-seq peak calling pipelines seek to differentiate biochemically
reproducible signals of individual genomic elements from background noise.
However, reproducibility alone does not imply functional regulation (e.g.,
enhancer activation, alternative splicing). Here we present a general-purpose,
interpretable machine learning method: signed iterative random forests (siRF),
which we use to infer regulatory interactions among transcription factors and
functional binding signatures surrounding enhancer elements in Drosophila
melanogaster.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kumbier_K/0/1/0/all/0/1"&gt;Karl Kumbier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Basu_S/0/1/0/all/0/1"&gt;Sumanta Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Frise_E/0/1/0/all/0/1"&gt;Erwin Frise&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Celniker_S/0/1/0/all/0/1"&gt;Susan E. Celniker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Brown_J/0/1/0/all/0/1"&gt;James B. Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Celniker_S/0/1/0/all/0/1"&gt;Susan Celniker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bin Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning with Frontier-Based Exploration via Autonomous Environment. (arXiv:2307.07296v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2307.07296</id>
        <link href="http://arxiv.org/abs/2307.07296"/>
        <updated>2023-07-17T01:05:34.979Z</updated>
        <summary type="html"><![CDATA[Active Simultaneous Localisation and Mapping (SLAM) is a critical problem in
autonomous robotics, enabling robots to navigate to new regions while building
an accurate model of their surroundings. Visual SLAM is a popular technique
that uses virtual elements to enhance the experience. However, existing
frontier-based exploration strategies can lead to a non-optimal path in
scenarios where there are multiple frontiers with similar distance. This issue
can impact the efficiency and accuracy of Visual SLAM, which is crucial for a
wide range of robotic applications, such as search and rescue, exploration, and
mapping. To address this issue, this research combines both an existing
Visual-Graph SLAM known as ExploreORB with reinforcement learning. The proposed
algorithm allows the robot to learn and optimize exploration routes through a
reward-based system to create an accurate map of the environment with proper
frontier selection. Frontier-based exploration is used to detect unexplored
areas, while reinforcement learning optimizes the robot's movement by assigning
rewards for optimal frontier points. Graph SLAM is then used to integrate the
robot's sensory data and build an accurate map of the environment. The proposed
algorithm aims to improve the efficiency and accuracy of ExploreORB by
optimizing the exploration process of frontiers to build a more accurate map.
To evaluate the effectiveness of the proposed approach, experiments will be
conducted in various virtual environments using Gazebo, a robot simulation
software. Results of these experiments will be compared with existing methods
to demonstrate the potential of the proposed approach as an optimal solution
for SLAM in autonomous robotics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leong_K/0/1/0/all/0/1"&gt;Kenji Leong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Brain Tumor Detection using Convolutional Neural Networks with Skip Connections. (arXiv:2307.07503v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2307.07503</id>
        <link href="http://arxiv.org/abs/2307.07503"/>
        <updated>2023-07-17T01:05:34.979Z</updated>
        <summary type="html"><![CDATA[In this paper, we present different architectures of Convolutional Neural
Networks (CNN) to analyze and classify the brain tumors into benign and
malignant types using the Magnetic Resonance Imaging (MRI) technique. Different
CNN architecture optimization techniques such as widening and deepening of the
network and adding skip connections are applied to improve the accuracy of the
network. Results show that a subset of these techniques can judiciously be used
to outperform a baseline CNN model used for the same purpose.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hamran_A/0/1/0/all/0/1"&gt;Aupam Hamran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vaeztourshizi_M/0/1/0/all/0/1"&gt;Marzieh Vaeztourshizi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Esmaili_A/0/1/0/all/0/1"&gt;Amirhossein Esmaili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pedram_M/0/1/0/all/0/1"&gt;Massoud Pedram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Certified Robustness for Large Language Models with Self-Denoising. (arXiv:2307.07171v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.07171</id>
        <link href="http://arxiv.org/abs/2307.07171"/>
        <updated>2023-07-17T01:05:34.929Z</updated>
        <summary type="html"><![CDATA[Although large language models (LLMs) have achieved great success in vast
real-world applications, their vulnerabilities towards noisy inputs have
significantly limited their uses, especially in high-stake environments. In
these contexts, it is crucial to ensure that every prediction made by large
language models is stable, i.e., LLM predictions should be consistent given
minor differences in the input. This largely falls into the study of certified
robust LLMs, i.e., all predictions of LLM are certified to be correct in a
local region around the input. Randomized smoothing has demonstrated great
potential in certifying the robustness and prediction stability of LLMs.
However, randomized smoothing requires adding noise to the input before model
prediction, and its certification performance depends largely on the model's
performance on corrupted data. As a result, its direct application to LLMs
remains challenging and often results in a small certification radius. To
address this issue, we take advantage of the multitasking nature of LLMs and
propose to denoise the corrupted inputs with LLMs in a self-denoising manner.
Different from previous works like denoised smoothing, which requires training
a separate model to robustify LLM, our method enjoys far better efficiency and
flexibility. Our experiment results show that our method outperforms the
existing certification methods under both certified robustness and empirical
robustness. The codes are available at
https://github.com/UCSB-NLP-Chang/SelfDenoise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guanhua Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1"&gt;Bairu Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1"&gt;Wenqi Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sijia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shiyu Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MaxMin-L2-SVC-NCH: A New Method to Train Support Vector Classifier with the Selection of Model's Parameters. (arXiv:2307.07343v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07343</id>
        <link href="http://arxiv.org/abs/2307.07343"/>
        <updated>2023-07-17T01:05:34.925Z</updated>
        <summary type="html"><![CDATA[The selection of model's parameters plays an important role in the
application of support vector classification (SVC). The commonly used method of
selecting model's parameters is the k-fold cross validation with grid search
(CV). It is extremely time-consuming because it needs to train a large number
of SVC models. In this paper, a new method is proposed to train SVC with the
selection of model's parameters. Firstly, training SVC with the selection of
model's parameters is modeled as a minimax optimization problem
(MaxMin-L2-SVC-NCH), in which the minimization problem is an optimization
problem of finding the closest points between two normal convex hulls
(L2-SVC-NCH) while the maximization problem is an optimization problem of
finding the optimal model's parameters. A lower time complexity can be expected
in MaxMin-L2-SVC-NCH because CV is abandoned. A gradient-based algorithm is
then proposed to solve MaxMin-L2-SVC-NCH, in which L2-SVC-NCH is solved by a
projected gradient algorithm (PGA) while the maximization problem is solved by
a gradient ascent algorithm with dynamic learning rate. To demonstrate the
advantages of the PGA in solving L2-SVC-NCH, we carry out a comparison of the
PGA and the famous sequential minimal optimization (SMO) algorithm after a SMO
algorithm and some KKT conditions for L2-SVC-NCH are provided. It is revealed
that the SMO algorithm is a special case of the PGA. Thus, the PGA can provide
more flexibility. The comparative experiments between MaxMin-L2-SVC-NCH and the
classical parameter selection models on public datasets show that
MaxMin-L2-SVC-NCH greatly reduces the number of models to be trained and the
test accuracy is not lost to the classical models. It indicates that
MaxMin-L2-SVC-NCH performs better than the other models. We strongly recommend
MaxMin-L2-SVC-NCH as a preferred model for SVC task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1"&gt;Linkai Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qiaoling Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yiding Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Ziyang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impact of Free-carrier Nonlinearities on Silicon Microring-based Reservoir Computing. (arXiv:2307.07011v1 [cs.ET])]]></title>
        <id>http://arxiv.org/abs/2307.07011</id>
        <link href="http://arxiv.org/abs/2307.07011"/>
        <updated>2023-07-17T01:05:34.887Z</updated>
        <summary type="html"><![CDATA[We quantify the impact of thermo-optic and free-carrier effects on time-delay
reservoir computing using a silicon microring resonator. We identify pump power
and frequency detuning ranges with NMSE less than 0.05 for the NARMA-10 task
depending on the time constants of the two considered effects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Castro_B/0/1/0/all/0/1"&gt;Bernard J. Giron Castro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peucheret_C/0/1/0/all/0/1"&gt;Christophe Peucheret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zibar_D/0/1/0/all/0/1"&gt;Darko Zibar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ros_F/0/1/0/all/0/1"&gt;Francesco Da Ros&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Pruning of Neural Networks for Constraints Learning. (arXiv:2307.07457v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07457</id>
        <link href="http://arxiv.org/abs/2307.07457"/>
        <updated>2023-07-17T01:05:34.861Z</updated>
        <summary type="html"><![CDATA[In recent years, the integration of Machine Learning (ML) models with
Operation Research (OR) tools has gained popularity across diverse
applications, including cancer treatment, algorithmic configuration, and
chemical process optimization. In this domain, the combination of ML and OR
often relies on representing the ML model output using Mixed Integer
Programming (MIP) formulations. Numerous studies in the literature have
developed such formulations for many ML predictors, with a particular emphasis
on Artificial Neural Networks (ANNs) due to their significant interest in many
applications. However, ANNs frequently contain a large number of parameters,
resulting in MIP formulations that are impractical to solve, thereby impeding
scalability. In fact, the ML community has already introduced several
techniques to reduce the parameter count of ANNs without compromising their
performance, since the substantial size of modern ANNs presents challenges for
ML applications as it significantly impacts computational efforts during
training and necessitates significant memory resources for storage. In this
paper, we showcase the effectiveness of pruning, one of these techniques, when
applied to ANNs prior to their integration into MIPs. By pruning the ANN, we
achieve significant improvements in the speed of the solution process. We
discuss why pruning is more suitable in this context compared to other ML
compression techniques, and we identify the most appropriate pruning
strategies. To highlight the potential of this approach, we conduct experiments
using feed-forward neural networks with multiple layers to construct
adversarial examples. Our results demonstrate that pruning offers remarkable
reductions in solution times without hindering the quality of the final
decision, enabling the resolution of previously unsolvable instances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cacciola_M/0/1/0/all/0/1"&gt;Matteo Cacciola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frangioni_A/0/1/0/all/0/1"&gt;Antonio Frangioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lodi_A/0/1/0/all/0/1"&gt;Andrea Lodi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MaxCorrMGNN: A Multi-Graph Neural Network Framework for Generalized Multimodal Fusion of Medical Data for Outcome Prediction. (arXiv:2307.07093v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07093</id>
        <link href="http://arxiv.org/abs/2307.07093"/>
        <updated>2023-07-17T01:05:34.853Z</updated>
        <summary type="html"><![CDATA[With the emergence of multimodal electronic health records, the evidence for
an outcome may be captured across multiple modalities ranging from clinical to
imaging and genomic data. Predicting outcomes effectively requires fusion
frameworks capable of modeling fine-grained and multi-faceted complex
interactions between modality features within and across patients. We develop
an innovative fusion approach called MaxCorr MGNN that models non-linear
modality correlations within and across patients through
Hirschfeld-Gebelein-Renyi maximal correlation (MaxCorr) embeddings, resulting
in a multi-layered graph that preserves the identities of the modalities and
patients. We then design, for the first time, a generalized multi-layered graph
neural network (MGNN) for task-informed reasoning in multi-layered graphs, that
learns the parameters defining patient-modality graph connectivity and message
passing in an end-to-end fashion. We evaluate our model an outcome prediction
task on a Tuberculosis (TB) dataset consistently outperforming several
state-of-the-art neural, graph-based and traditional fusion techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DSouza_N/0/1/0/all/0/1"&gt;Niharika S. D&amp;#x27;Souza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giovannini_A/0/1/0/all/0/1"&gt;Andrea Giovannini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foncubierta_Rodriguez_A/0/1/0/all/0/1"&gt;Antonio Foncubierta-Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beck_K/0/1/0/all/0/1"&gt;Kristen L. Beck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boyko_O/0/1/0/all/0/1"&gt;Orest Boyko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Syeda_Mahmood_T/0/1/0/all/0/1"&gt;Tanveer Syeda-Mahmood&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Scenario-Based Functional Testing Approach to Improving DNN Performance. (arXiv:2307.07083v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07083</id>
        <link href="http://arxiv.org/abs/2307.07083"/>
        <updated>2023-07-17T01:05:34.687Z</updated>
        <summary type="html"><![CDATA[This paper proposes a scenario-based functional testing approach for
enhancing the performance of machine learning (ML) applications. The proposed
method is an iterative process that starts with testing the ML model on various
scenarios to identify areas of weakness. It follows by a further testing on the
suspected weak scenarios and statistically evaluate the model's performance on
the scenarios to confirm the diagnosis. Once the diagnosis of weak scenarios is
confirmed by test results, the treatment of the model is performed by
retraining the model using a transfer learning technique with the original
model as the base and applying a set of training data specifically targeting
the treated scenarios plus a subset of training data selected at random from
the original train dataset to prevent the so-call catastrophic forgetting
effect. Finally, after the treatment, the model is assessed and evaluated again
by testing on the treated scenarios as well as other scenarios to check if the
treatment is effective and no side effect caused. The paper reports a case
study with a real ML deep neural network (DNN) model, which is the perception
system of an autonomous racing car. It is demonstrated that the method is
effective in the sense that DNN model's performance can be improved. It
provides an efficient method of enhancing ML model's performance with much less
human and compute resource than retrain from scratch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Thi Minh Tam Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benjumea_A/0/1/0/all/0/1"&gt;Aduen Benjumea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bradley_A/0/1/0/all/0/1"&gt;Andrew Bradley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neuro-symbolic Empowered Denoising Diffusion Probabilistic Models for Real-time Anomaly Detection in Industry 4.0. (arXiv:2307.06975v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.06975</id>
        <link href="http://arxiv.org/abs/2307.06975"/>
        <updated>2023-07-17T01:05:34.593Z</updated>
        <summary type="html"><![CDATA[Industry 4.0 involves the integration of digital technologies, such as IoT,
Big Data, and AI, into manufacturing and industrial processes to increase
efficiency and productivity. As these technologies become more interconnected
and interdependent, Industry 4.0 systems become more complex, which brings the
difficulty of identifying and stopping anomalies that may cause disturbances in
the manufacturing process. This paper aims to propose a diffusion-based model
for real-time anomaly prediction in Industry 4.0 processes. Using a
neuro-symbolic approach, we integrate industrial ontologies in the model,
thereby adding formal knowledge on smart manufacturing. Finally, we propose a
simple yet effective way of distilling diffusion models through Random Fourier
Features for deployment on an embedded system for direct integration into the
manufacturing process. To the best of our knowledge, this approach has never
been explored before.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Capogrosso_L/0/1/0/all/0/1"&gt;Luigi Capogrosso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mascolini_A/0/1/0/all/0/1"&gt;Alessio Mascolini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girella_F/0/1/0/all/0/1"&gt;Federico Girella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skenderi_G/0/1/0/all/0/1"&gt;Geri Skenderi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaiardelli_S/0/1/0/all/0/1"&gt;Sebastiano Gaiardelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DallOra_N/0/1/0/all/0/1"&gt;Nicola Dall&amp;#x27;Ora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ponzio_F/0/1/0/all/0/1"&gt;Francesco Ponzio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fraccaroli_E/0/1/0/all/0/1"&gt;Enrico Fraccaroli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cataldo_S/0/1/0/all/0/1"&gt;Santa Di Cataldo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinco_S/0/1/0/all/0/1"&gt;Sara Vinco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Macii_E/0/1/0/all/0/1"&gt;Enrico Macii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fummi_F/0/1/0/all/0/1"&gt;Franco Fummi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1"&gt;Marco Cristani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Choice Models and Permutation Invariance. (arXiv:2307.07090v1 [econ.EM])]]></title>
        <id>http://arxiv.org/abs/2307.07090</id>
        <link href="http://arxiv.org/abs/2307.07090"/>
        <updated>2023-07-17T01:05:34.555Z</updated>
        <summary type="html"><![CDATA[Choice Modeling is at the core of many economics, operations, and marketing
problems. In this paper, we propose a fundamental characterization of choice
functions that encompasses a wide variety of extant choice models. We
demonstrate how nonparametric estimators like neural nets can easily
approximate such functionals and overcome the curse of dimensionality that is
inherent in the non-parametric estimation of choice functions. We demonstrate
through extensive simulations that our proposed functionals can flexibly
capture underlying consumer behavior in a completely data-driven fashion and
outperform traditional parametric models. As demand settings often exhibit
endogenous features, we extend our framework to incorporate estimation under
endogenous features. Further, we also describe a formal inference procedure to
construct valid confidence intervals on objects of interest like price
elasticity. Finally, to assess the practical applicability of our estimator, we
utilize a real-world dataset from S. Berry, Levinsohn, and Pakes (1995). Our
empirical analysis confirms that the estimator generates realistic and
comparable own- and cross-price elasticities that are consistent with the
observations reported in the existing literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Singh_A/0/1/0/all/0/1"&gt;Amandeep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Ye Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Yoganarasimhan_H/0/1/0/all/0/1"&gt;Hema Yoganarasimhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Factored Action Spaces for Off-Policy Evaluation. (arXiv:2307.07014v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07014</id>
        <link href="http://arxiv.org/abs/2307.07014"/>
        <updated>2023-07-17T01:05:34.549Z</updated>
        <summary type="html"><![CDATA[Off-policy evaluation (OPE) aims to estimate the benefit of following a
counterfactual sequence of actions, given data collected from executed
sequences. However, existing OPE estimators often exhibit high bias and high
variance in problems involving large, combinatorial action spaces. We
investigate how to mitigate this issue using factored action spaces i.e.
expressing each action as a combination of independent sub-actions from smaller
action spaces. This approach facilitates a finer-grained analysis of how
actions differ in their effects. In this work, we propose a new family of
"decomposed" importance sampling (IS) estimators based on factored action
spaces. Given certain assumptions on the underlying problem structure, we prove
that the decomposed IS estimators have less variance than their original
non-decomposed versions, while preserving the property of zero bias. Through
simulations, we empirically verify our theoretical results, probing the
validity of various assumptions. Provided with a technique that can derive the
action space factorisation for a given problem, our work shows that OPE can be
improved "for free" by utilising this inherent problem structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rebello_A/0/1/0/all/0/1"&gt;Aaman Rebello&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Shengpu Tang&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1"&gt;Jenna Wiens&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Parbhoo_S/0/1/0/all/0/1"&gt;Sonali Parbhoo&lt;/a&gt; (1) ((1) Department of Engineering, Imperial College London, (2) Division of Computer Science &amp; Engineering, University of Michigan)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability. (arXiv:2307.07084v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07084</id>
        <link href="http://arxiv.org/abs/2307.07084"/>
        <updated>2023-07-17T01:05:34.538Z</updated>
        <summary type="html"><![CDATA[Reinforcement Learning or optimal control can provide effective reasoning for
sequential decision-making problems with variable dynamics. Such reasoning in
practical implementation, however, poses a persistent challenge in interpreting
the reward function and corresponding optimal policy. Consequently, formalizing
the sequential decision-making problems as inference has a considerable value,
as probabilistic inference in principle offers diverse and powerful
mathematical tools to infer the stochastic dynamics whilst suggesting a
probabilistic interpretation of the reward design and policy convergence. In
this study, we propose a novel Adaptive Wasserstein Variational Optimization
(AWaVO) to tackle these challenges in sequential decision-making. Our approach
utilizes formal methods to provide interpretations of reward design,
transparency of training convergence, and probabilistic interpretation of
sequential decisions. To demonstrate practicality, we show convergent training
with guaranteed global convergence rates not only in simulation but also in
real robot tasks, and empirically verify a reasonable tradeoff between high
performance and conservative interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boyle_D/0/1/0/all/0/1"&gt;David Boyle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerated gradient methods for nonconvex optimization: Escape trajectories from strict saddle points and convergence to local minima. (arXiv:2307.07030v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2307.07030</id>
        <link href="http://arxiv.org/abs/2307.07030"/>
        <updated>2023-07-17T01:05:34.533Z</updated>
        <summary type="html"><![CDATA[This paper considers the problem of understanding the behavior of a general
class of accelerated gradient methods on smooth nonconvex functions. Motivated
by some recent works that have proposed effective algorithms, based on Polyak's
heavy ball method and the Nesterov accelerated gradient method, to achieve
convergence to a local minimum of nonconvex functions, this work proposes a
broad class of Nesterov-type accelerated methods and puts forth a rigorous
study of these methods encompassing the escape from saddle-points and
convergence to local minima through a both asymptotic and a non-asymptotic
analysis. In the asymptotic regime, this paper answers an open question of
whether Nesterov's accelerated gradient method (NAG) with variable momentum
parameter avoids strict saddle points almost surely. This work also develops
two metrics of asymptotic rate of convergence and divergence, and evaluates
these two metrics for several popular standard accelerated methods such as the
NAG, and Nesterov's accelerated gradient with constant momentum (NCM) near
strict saddle points. In the local regime, this work provides an analysis that
leads to the "linear" exit time estimates from strict saddle neighborhoods for
trajectories of these accelerated methods as well the necessary conditions for
the existence of such trajectories. Finally, this work studies a sub-class of
accelerated methods that can converge in convex neighborhoods of nonconvex
functions with a near optimal rate to a local minima and at the same time this
sub-class offers superior saddle-escape behavior compared to that of NAG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Dixit_R/0/1/0/all/0/1"&gt;Rishabh Dixit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gurbuzbalaban_M/0/1/0/all/0/1"&gt;Mert Gurbuzbalaban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Bajwa_W/0/1/0/all/0/1"&gt;Waheed U. Bajwa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Short Boolean Formulas as Explanations in Practice. (arXiv:2307.06971v1 [cs.LO])]]></title>
        <id>http://arxiv.org/abs/2307.06971</id>
        <link href="http://arxiv.org/abs/2307.06971"/>
        <updated>2023-07-17T01:05:34.527Z</updated>
        <summary type="html"><![CDATA[We investigate explainability via short Boolean formulas in the data model
based on unary relations. As an explanation of length k, we take a Boolean
formula of length k that minimizes the error with respect to the target
attribute to be explained. We first provide novel quantitative bounds for the
expected error in this scenario. We then also demonstrate how the setting works
in practice by studying three concrete data sets. In each case, we calculate
explanation formulas of different lengths using an encoding in Answer Set
Programming. The most accurate formulas we obtain achieve errors similar to
other methods on the same data sets. However, due to overfitting, these
formulas are not necessarily ideal explanations, so we use cross validation to
identify a suitable length for explanations. By limiting to shorter formulas,
we obtain explanations that avoid overfitting but are still reasonably accurate
and also, importantly, human interpretable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaakkola_R/0/1/0/all/0/1"&gt;Reijo Jaakkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Janhunen_T/0/1/0/all/0/1"&gt;Tomi Janhunen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuusisto_A/0/1/0/all/0/1"&gt;Antti Kuusisto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rankooh_M/0/1/0/all/0/1"&gt;Masood Feyzbakhsh Rankooh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vilander_M/0/1/0/all/0/1"&gt;Miikka Vilander&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Layerwise Linear Mode Connectivity. (arXiv:2307.06966v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.06966</id>
        <link href="http://arxiv.org/abs/2307.06966"/>
        <updated>2023-07-17T01:05:34.522Z</updated>
        <summary type="html"><![CDATA[In the federated setup one performs an aggregation of separate local models
multiple times during training in order to obtain a stronger global model; most
often aggregation is a simple averaging of the parameters. Understanding when
and why averaging works in a non-convex setup, such as federated deep learning,
is an open challenge that hinders obtaining highly performant global models. On
i.i.d.~datasets federated deep learning with frequent averaging is successful.
The common understanding, however, is that during the independent training
models are drifting away from each other and thus averaging may not work
anymore after many local parameter updates. The problem can be seen from the
perspective of the loss surface: for points on a non-convex surface the average
can become arbitrarily bad. The assumption of local convexity, often used to
explain the success of federated averaging, contradicts to the empirical
evidence showing that high loss barriers exist between models from the very
beginning of the learning, even when training on the same data. Based on the
observation that the learning process evolves differently in different layers,
we investigate the barrier between models in a layerwise fashion. Our
conjecture is that barriers preventing from successful federated training are
caused by a particular layer or group of layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Adilova_L/0/1/0/all/0/1"&gt;Linara Adilova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1"&gt;Asja Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit regularization in AI meets generalized hardness of approximation in optimization -- Sharp results for diagonal linear networks. (arXiv:2307.07410v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07410</id>
        <link href="http://arxiv.org/abs/2307.07410"/>
        <updated>2023-07-17T01:05:34.488Z</updated>
        <summary type="html"><![CDATA[Understanding the implicit regularization imposed by neural network
architectures and gradient based optimization methods is a key challenge in
deep learning and AI. In this work we provide sharp results for the implicit
regularization imposed by the gradient flow of Diagonal Linear Networks (DLNs)
in the over-parameterized regression setting and, potentially surprisingly,
link this to the phenomenon of phase transitions in generalized hardness of
approximation (GHA). GHA generalizes the phenomenon of hardness of
approximation from computer science to, among others, continuous and robust
optimization. It is well-known that the $\ell^1$-norm of the gradient flow of
DLNs with tiny initialization converges to the objective function of basis
pursuit. We improve upon these results by showing that the gradient flow of
DLNs with tiny initialization approximates minimizers of the basis pursuit
optimization problem (as opposed to just the objective function), and we obtain
new and sharp convergence bounds w.r.t.\ the initialization size. Non-sharpness
of our results would imply that the GHA phenomenon would not occur for the
basis pursuit optimization problem -- which is a contradiction -- thus implying
sharpness. Moreover, we characterize $\textit{which}$ $\ell_1$ minimizer of the
basis pursuit problem is chosen by the gradient flow whenever the minimizer is
not unique. Interestingly, this depends on the depth of the DLN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wind_J/0/1/0/all/0/1"&gt;Johan S. Wind&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antun_V/0/1/0/all/0/1"&gt;Vegard Antun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hansen_A/0/1/0/all/0/1"&gt;Anders C. Hansen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DISPEL: Domain Generalization via Domain-Specific Liberating. (arXiv:2307.07181v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.07181</id>
        <link href="http://arxiv.org/abs/2307.07181"/>
        <updated>2023-07-17T01:05:34.484Z</updated>
        <summary type="html"><![CDATA[Domain generalization aims to learn a generalization model that can perform
well on unseen test domains by only training on limited source domains.
However, existing domain generalization approaches often bring in
prediction-irrelevant noise or require the collection of domain labels. To
address these challenges, we consider the domain generalization problem from a
different perspective by categorizing underlying feature groups into
domain-shared and domain-specific features. Nevertheless, the domain-specific
features are difficult to be identified and distinguished from the input data.
In this work, we propose DomaIn-SPEcific Liberating (DISPEL), a post-processing
fine-grained masking approach that can filter out undefined and
indistinguishable domain-specific features in the embedding space.
Specifically, DISPEL utilizes a mask generator that produces a unique mask for
each input data to filter domain-specific features. The DISPEL framework is
highly flexible to be applied to any fine-tuned models. We derive a
generalization error bound to guarantee the generalization performance by
optimizing a designed objective loss. The experimental results on five
benchmarks demonstrate DISPEL outperforms existing methods and can further
generalize various algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1"&gt;Chia-Yuan Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1"&gt;Yu-Neng Chuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanchu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1"&gt;Mengnan Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Na_Z/0/1/0/all/0/1"&gt;Zou Na&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning. (arXiv:2307.07091v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07091</id>
        <link href="http://arxiv.org/abs/2307.07091"/>
        <updated>2023-07-17T01:05:34.469Z</updated>
        <summary type="html"><![CDATA[Offline reinforcement learning (RL) is a promising direction that allows RL
agents to pre-train on large datasets, avoiding the recurrence of expensive
data collection. To advance the field, it is crucial to generate large-scale
datasets. Compositional RL is particularly appealing for generating such large
datasets, since 1) it permits creating many tasks from few components, 2) the
task structure may enable trained agents to solve new tasks by combining
relevant learned components, and 3) the compositional dimensions provide a
notion of task relatedness. This paper provides four offline RL datasets for
simulated robotic manipulation created using the 256 tasks from CompoSuite
[Mendez et al., 2022a]. Each dataset is collected from an agent with a
different degree of performance, and consists of 256 million transitions. We
provide training and evaluation settings for assessing an agent's ability to
learn compositional task policies. Our benchmarking experiments on each setting
show that current offline RL methods can learn the training tasks to some
extent and that compositional methods significantly outperform
non-compositional methods. However, current methods are still unable to extract
the tasks' compositional structure to generalize to unseen tasks, showing a
need for further research in offline compositional RL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hussing_M/0/1/0/all/0/1"&gt;Marcel Hussing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendez_J/0/1/0/all/0/1"&gt;Jorge A. Mendez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singrodia_A/0/1/0/all/0/1"&gt;Anisha Singrodia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kent_C/0/1/0/all/0/1"&gt;Cassandra Kent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eaton_E/0/1/0/all/0/1"&gt;Eric Eaton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Counter-Examples for Active Learning with Partial labels. (arXiv:2307.07413v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07413</id>
        <link href="http://arxiv.org/abs/2307.07413"/>
        <updated>2023-07-17T01:05:34.449Z</updated>
        <summary type="html"><![CDATA[This paper studies a new problem, \emph{active learning with partial labels}
(ALPL). In this setting, an oracle annotates the query samples with partial
labels, relaxing the oracle from the demanding accurate labeling process. To
address ALPL, we first build an intuitive baseline that can be seamlessly
incorporated into existing AL frameworks. Though effective, this baseline is
still susceptible to the \emph{overfitting}, and falls short of the
representative partial-label-based samples during the query process. Drawing
inspiration from human inference in cognitive science, where accurate
inferences can be explicitly derived from \emph{counter-examples} (CEs), our
objective is to leverage this human-like learning pattern to tackle the
\emph{overfitting} while enhancing the process of selecting representative
samples in ALPL. Specifically, we construct CEs by reversing the partial labels
for each instance, and then we propose a simple but effective WorseNet to
directly learn from this complementary pattern. By leveraging the distribution
gap between WorseNet and the predictor, this adversarial evaluation manner
could enhance both the performance of the predictor itself and the sample
selection process, allowing the predictor to capture more accurate patterns in
the data. Experimental results on five real-world datasets and four benchmark
datasets show that our proposed method achieves comprehensive improvements over
ten representative AL frameworks, highlighting the superiority of WorseNet. The
source code will be available at \url{https://github.com/Ferenas/APLL}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yunjie Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1"&gt;Lei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_Z/0/1/0/all/0/1"&gt;Zhongwen Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jieming Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalander_M/0/1/0/all/0/1"&gt;Marcus Kalander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1"&gt;Chen Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1"&gt;Jianye Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiplicative update rules for accelerating deep learning training and increasing robustness. (arXiv:2307.07189v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07189</id>
        <link href="http://arxiv.org/abs/2307.07189"/>
        <updated>2023-07-17T01:05:34.443Z</updated>
        <summary type="html"><![CDATA[Even nowadays, where Deep Learning (DL) has achieved state-of-the-art
performance in a wide range of research domains, accelerating training and
building robust DL models remains a challenging task. To this end, generations
of researchers have pursued to develop robust methods for training DL
architectures that can be less sensitive to weight distributions, model
architectures and loss landscapes. However, such methods are limited to
adaptive learning rate optimizers, initialization schemes, and clipping
gradients without investigating the fundamental rule of parameters update.
Although multiplicative updates have contributed significantly to the early
development of machine learning and hold strong theoretical claims, to best of
our knowledge, this is the first work that investigate them in context of DL
training acceleration and robustness. In this work, we propose an optimization
framework that fits to a wide range of optimization algorithms and enables one
to apply alternative update rules. To this end, we propose a novel
multiplicative update rule and we extend their capabilities by combining it
with a traditional additive update term, under a novel hybrid update method. We
claim that the proposed framework accelerates training, while leading to more
robust models in contrast to traditionally used additive update rule and we
experimentally demonstrate their effectiveness in a wide range of task and
optimization methods. Such tasks ranging from convex and non-convex
optimization to difficult image classification benchmarks applying a wide range
of traditionally used optimization methods and Deep Neural Network (DNN)
architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kirtas_M/0/1/0/all/0/1"&gt;Manos Kirtas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Passalis_N/0/1/0/all/0/1"&gt;Nikolaos Passalis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tefas_A/0/1/0/all/0/1"&gt;Anastasios Tefas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AnyStar: Domain randomized universal star-convex 3D instance segmentation. (arXiv:2307.07044v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.07044</id>
        <link href="http://arxiv.org/abs/2307.07044"/>
        <updated>2023-07-17T01:05:34.412Z</updated>
        <summary type="html"><![CDATA[Star-convex shapes arise across bio-microscopy and radiology in the form of
nuclei, nodules, metastases, and other units. Existing instance segmentation
networks for such structures train on densely labeled instances for each
dataset, which requires substantial and often impractical manual annotation
effort. Further, significant reengineering or finetuning is needed when
presented with new datasets and imaging modalities due to changes in contrast,
shape, orientation, resolution, and density. We present AnyStar, a
domain-randomized generative model that simulates synthetic training data of
blob-like objects with randomized appearance, environments, and imaging physics
to train general-purpose star-convex instance segmentation networks. As a
result, networks trained using our generative model do not require annotated
images from unseen datasets. A single network trained on our synthesized data
accurately 3D segments C. elegans and P. dumerilii nuclei in fluorescence
microscopy, mouse cortical nuclei in micro-CT, zebrafish brain nuclei in EM,
and placental cotyledons in human fetal MRI, all without any retraining,
finetuning, transfer learning, or domain adaptation. Code is available at
https://github.com/neel-dey/AnyStar.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dey_N/0/1/0/all/0/1"&gt;Neel Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abulnaga_S/0/1/0/all/0/1"&gt;S. Mazdak Abulnaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Billot_B/0/1/0/all/0/1"&gt;Benjamin Billot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turk_E/0/1/0/all/0/1"&gt;Esra Abaci Turk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grant_P/0/1/0/all/0/1"&gt;P. Ellen Grant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dalca_A/0/1/0/all/0/1"&gt;Adrian V. Dalca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golland_P/0/1/0/all/0/1"&gt;Polina Golland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DreamTeacher: Pretraining Image Backbones with Deep Generative Models. (arXiv:2307.07487v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.07487</id>
        <link href="http://arxiv.org/abs/2307.07487"/>
        <updated>2023-07-17T01:05:34.386Z</updated>
        <summary type="html"><![CDATA[In this work, we introduce a self-supervised feature representation learning
framework DreamTeacher that utilizes generative networks for pre-training
downstream image backbones. We propose to distill knowledge from a trained
generative model into standard image backbones that have been well engineered
for specific perception tasks. We investigate two types of knowledge
distillation: 1) distilling learned generative features onto target image
backbones as an alternative to pretraining these backbones on large labeled
datasets such as ImageNet, and 2) distilling labels obtained from generative
networks with task heads onto logits of target backbones. We perform extensive
analyses on multiple generative models, dense prediction benchmarks, and
several pre-training regimes. We empirically find that our DreamTeacher
significantly outperforms existing self-supervised representation learning
approaches across the board. Unsupervised ImageNet pre-training with
DreamTeacher leads to significant improvements over ImageNet classification
pre-training on downstream datasets, showcasing generative models, and
diffusion generative models specifically, as a promising approach to
representation learning on large, diverse datasets without requiring manual
annotation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Daiqing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1"&gt;Huan Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kar_A/0/1/0/all/0/1"&gt;Amlan Kar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acuna_D/0/1/0/all/0/1"&gt;David Acuna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seung Wook Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreis_K/0/1/0/all/0/1"&gt;Karsten Kreis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1"&gt;Antonio Torralba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1"&gt;Sanja Fidler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Surrogate Data Assimilation Model for the Estimation of Dynamical System in a Limited Area. (arXiv:2307.07178v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2307.07178</id>
        <link href="http://arxiv.org/abs/2307.07178"/>
        <updated>2023-07-17T01:05:34.380Z</updated>
        <summary type="html"><![CDATA[We propose a novel learning-based surrogate data assimilation (DA) model for
efficient state estimation in a limited area. Our model employs a feedforward
neural network for online computation, eliminating the need for integrating
high-dimensional limited-area models. This approach offers significant
computational advantages over traditional DA algorithms. Furthermore, our
method avoids the requirement of lateral boundary conditions for the
limited-area model in both online and offline computations. The design of our
surrogate DA model is built upon a robust theoretical framework that leverages
two fundamental concepts: observability and effective region. The concept of
observability enables us to quantitatively determine the optimal amount of
observation data necessary for accurate DA. Meanwhile, the concept of effective
region substantially reduces the computational burden associated with computing
observability and generating training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kang_W/0/1/0/all/0/1"&gt;Wei Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Xu_L/0/1/0/all/0/1"&gt;Liang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hong Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Linear Estimating Equations. (arXiv:2307.07320v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2307.07320</id>
        <link href="http://arxiv.org/abs/2307.07320"/>
        <updated>2023-07-17T01:05:34.152Z</updated>
        <summary type="html"><![CDATA[Sequential data collection has emerged as a widely adopted technique for
enhancing the efficiency of data gathering processes. Despite its advantages,
such data collection mechanism often introduces complexities to the statistical
inference procedure. For instance, the ordinary least squares (OLS) estimator
in an adaptive linear regression model can exhibit non-normal asymptotic
behavior, posing challenges for accurate inference and interpretation. In this
paper, we propose a general method for constructing debiased estimator which
remedies this issue. It makes use of the idea of adaptive linear estimating
equations, and we establish theoretical guarantees of asymptotic normality,
supplemented by discussions on achieving near-optimal asymptotic variance. A
salient feature of our estimator is that in the context of multi-armed bandits,
our estimator retains the non-asymptotic performance of the least square
estimator while obtaining asymptotic normality property. Consequently, this
work helps connect two fruitful paradigms of adaptive inference: a)
non-asymptotic inference using concentration inequalities and b) asymptotic
inference via asymptotic normality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ying_M/0/1/0/all/0/1"&gt;Mufang Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Khamaru_K/0/1/0/all/0/1"&gt;Koulik Khamaru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Cun-Hui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Composition-contrastive Learning for Sentence Embeddings. (arXiv:2307.07380v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2307.07380</id>
        <link href="http://arxiv.org/abs/2307.07380"/>
        <updated>2023-07-17T01:05:34.049Z</updated>
        <summary type="html"><![CDATA[Vector representations of natural language are ubiquitous in search
applications. Recently, various methods based on contrastive learning have been
proposed to learn textual representations from unlabelled data; by maximizing
alignment between minimally-perturbed embeddings of the same text, and
encouraging a uniform distribution of embeddings across a broader corpus.
Differently, we propose maximizing alignment between texts and a composition of
their phrasal constituents. We consider several realizations of this objective
and elaborate the impact on representations in each case. Experimental results
on semantic textual similarity tasks show improvements over baselines that are
comparable with state-of-the-art approaches. Moreover, this work is the first
to do so without incurring costs in auxiliary training objectives or additional
network parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chanchani_S/0/1/0/all/0/1"&gt;Sachin J. Chanchani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1"&gt;Ruihong Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DoCoFL: Downlink Compression for Cross-Device Federated Learning. (arXiv:2302.00543v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.00543</id>
        <link href="http://arxiv.org/abs/2302.00543"/>
        <updated>2023-07-17T01:05:33.600Z</updated>
        <summary type="html"><![CDATA[Many compression techniques have been proposed to reduce the communication
overhead of Federated Learning training procedures. However, these are
typically designed for compressing model updates, which are expected to decay
throughout training. As a result, such methods are inapplicable to downlink
(i.e., from the parameter server to clients) compression in the cross-device
setting, where heterogeneous clients $\textit{may appear only once}$ during
training and thus must download the model parameters. Accordingly, we propose
$\textsf{DoCoFL}$ -- a new framework for downlink compression in the
cross-device setting. Importantly, $\textsf{DoCoFL}$ can be seamlessly combined
with many uplink compression schemes, rendering it suitable for bi-directional
compression. Through extensive evaluation, we show that $\textsf{DoCoFL}$
offers significant bi-directional bandwidth reduction while achieving
competitive accuracy to that of a baseline without any compression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dorfman_R/0/1/0/all/0/1"&gt;Ron Dorfman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vargaftik_S/0/1/0/all/0/1"&gt;Shay Vargaftik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Itzhak_Y/0/1/0/all/0/1"&gt;Yaniv Ben-Itzhak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levy_K/0/1/0/all/0/1"&gt;Kfir Y. Levy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$\Phi$-DVAE: Physics-Informed Dynamical Variational Autoencoders for Unstructured Data Assimilation. (arXiv:2209.15609v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2209.15609</id>
        <link href="http://arxiv.org/abs/2209.15609"/>
        <updated>2023-07-17T01:05:33.587Z</updated>
        <summary type="html"><![CDATA[Incorporating unstructured data into physical models is a challenging problem
that is emerging in data assimilation. Traditional approaches focus on
well-defined observation operators whose functional forms are typically assumed
to be known. This prevents these methods from achieving a consistent model-data
synthesis in configurations where the mapping from data-space to model-space is
unknown. To address these shortcomings, in this paper we develop a
physics-informed dynamical variational autoencoder ($\Phi$-DVAE) to embed
diverse data streams into time-evolving physical systems described by
differential equations. Our approach combines a standard, possibly nonlinear,
filter for the latent state-space model and a VAE, to assimilate the
unstructured data into the latent dynamical system. Unstructured data, in our
example systems, comes in the form of video data and velocity field
measurements, however the methodology is suitably generic to allow for
arbitrary unknown observation operators. A variational Bayesian framework is
used for the joint estimation of the encoding, latent states, and unknown
system parameters. To demonstrate the method, we provide case studies with the
Lorenz-63 ordinary differential equation, and the advection and Korteweg-de
Vries partial differential equations. Our results, with synthetic data, show
that $\Phi$-DVAE provides a data efficient dynamics encoding methodology which
is competitive with standard approaches. Unknown parameters are recovered with
uncertainty quantification, and unseen data are accurately predicted.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Glyn_Davies_A/0/1/0/all/0/1"&gt;Alex Glyn-Davies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Duffin_C/0/1/0/all/0/1"&gt;Connor Duffin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Akyildiz_O/0/1/0/all/0/1"&gt;&amp;#xd6;. Deniz Akyildiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Girolami_M/0/1/0/all/0/1"&gt;Mark Girolami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linear Classification of Neural Manifolds with Correlated Variability. (arXiv:2211.14961v2 [q-bio.NC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.14961</id>
        <link href="http://arxiv.org/abs/2211.14961"/>
        <updated>2023-07-17T01:05:33.574Z</updated>
        <summary type="html"><![CDATA[Understanding how the statistical and geometric properties of neural activity
relate to performance is a key problem in theoretical neuroscience and deep
learning. Here, we calculate how correlations between object representations
affect the capacity, a measure of linear separability. We show that for
spherical object manifolds, introducing correlations between centroids
effectively pushes the spheres closer together, while introducing correlations
between the axes effectively shrinks their radii, revealing a duality between
correlations and geometry with respect to the problem of classification. We
then apply our results to accurately estimate the capacity of deep network
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Wakhloo_A/0/1/0/all/0/1"&gt;Albert J. Wakhloo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Sussman_T/0/1/0/all/0/1"&gt;Tamara J. Sussman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chung_S/0/1/0/all/0/1"&gt;SueYeon Chung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stream-based active learning with linear models. (arXiv:2207.09874v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2207.09874</id>
        <link href="http://arxiv.org/abs/2207.09874"/>
        <updated>2023-07-17T01:05:33.541Z</updated>
        <summary type="html"><![CDATA[The proliferation of automated data collection schemes and the advances in
sensorics are increasing the amount of data we are able to monitor in
real-time. However, given the high annotation costs and the time required by
quality inspections, data is often available in an unlabeled form. This is
fostering the use of active learning for the development of soft sensors and
predictive models. In production, instead of performing random inspections to
obtain product information, labels are collected by evaluating the information
content of the unlabeled data. Several query strategy frameworks for regression
have been proposed in the literature but most of the focus has been dedicated
to the static pool-based scenario. In this work, we propose a new strategy for
the stream-based scenario, where instances are sequentially offered to the
learner, which must instantaneously decide whether to perform the quality check
to obtain the label or discard the instance. The approach is inspired by the
optimal experimental design theory and the iterative aspect of the
decision-making process is tackled by setting a threshold on the
informativeness of the unlabeled data points. The proposed approach is
evaluated using numerical simulations and the Tennessee Eastman Process
simulator. The results confirm that selecting the examples suggested by the
proposed algorithm allows for a faster reduction in the prediction error.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Cacciarelli_D/0/1/0/all/0/1"&gt;Davide Cacciarelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kulahci_M/0/1/0/all/0/1"&gt;Murat Kulahci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tyssedal_J/0/1/0/all/0/1"&gt;John S&amp;#xf8;lve Tyssedal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Stochastic Gradient Descent with Low-Noise. (arXiv:2209.04188v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2209.04188</id>
        <link href="http://arxiv.org/abs/2209.04188"/>
        <updated>2023-07-17T01:05:33.534Z</updated>
        <summary type="html"><![CDATA[Modern machine learning algorithms aim to extract fine-grained information
from data to provide accurate predictions, which often conflicts with the goal
of privacy protection. This paper addresses the practical and theoretical
importance of developing privacy-preserving machine learning algorithms that
ensure good performance while preserving privacy. In this paper, we focus on
the privacy and utility (measured by excess risk bounds) performances of
differentially private stochastic gradient descent (SGD) algorithms in the
setting of stochastic convex optimization. Specifically, we examine the
pointwise problem in the low-noise setting for which we derive sharper excess
risk bounds for the differentially private SGD algorithm. In the pairwise
learning setting, we propose a simple differentially private SGD algorithm
based on gradient perturbation. Furthermore, we develop novel utility bounds
for the proposed algorithm, proving that it achieves optimal excess risk rates
even for non-smooth losses. Notably, we establish fast learning rates for
privacy-preserving pairwise learning under the low-noise condition, which is
the first of its kind.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_P/0/1/0/all/0/1"&gt;Puyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lei_Y/0/1/0/all/0/1"&gt;Yunwen Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ying_Y/0/1/0/all/0/1"&gt;Yiming Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Ding-Xuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Statistical Discrimination as a Failure of Social Learning: A Multi-Armed Bandit Approach. (arXiv:2010.01079v6 [econ.TH] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01079</id>
        <link href="http://arxiv.org/abs/2010.01079"/>
        <updated>2023-07-17T01:05:33.520Z</updated>
        <summary type="html"><![CDATA[We analyze statistical discrimination in hiring markets using a multi-armed
bandit model. Myopic firms face workers arriving with heterogeneous observable
characteristics. The association between the worker's skill and characteristics
is unknown ex ante; thus, firms need to learn it. Laissez-faire causes
perpetual underestimation: minority workers are rarely hired, and therefore,
the underestimation tends to persist. Even a marginal imbalance in the
population ratio frequently results in perpetual underestimation. We propose
two policy solutions: a novel subsidy rule (the hybrid mechanism) and the
Rooney Rule. Our results indicate that temporary affirmative actions
effectively alleviate discrimination stemming from insufficient data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Komiyama_J/0/1/0/all/0/1"&gt;Junpei Komiyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Noda_S/0/1/0/all/0/1"&gt;Shunya Noda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bootstrapping Vision-Language Learning with Decoupled Language Pre-training. (arXiv:2307.07063v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2307.07063</id>
        <link href="http://arxiv.org/abs/2307.07063"/>
        <updated>2023-07-17T01:05:33.512Z</updated>
        <summary type="html"><![CDATA[We present a novel methodology aimed at optimizing the application of frozen
large language models (LLMs) for resource-intensive vision-language (VL)
pre-training. The current paradigm uses visual features as prompts to guide
language models, with a focus on determining the most relevant visual features
for corresponding text. Our approach diverges by concentrating on the language
component, specifically identifying the optimal prompts to align with visual
features. We introduce the Prompt-Transformer (P-Former), a model that predicts
these ideal prompts, which is trained exclusively on linguistic data, bypassing
the need for image-text pairings. This strategy subtly bifurcates the
end-to-end VL training process into an additional, separate stage. Our
experiments reveal that our framework significantly enhances the performance of
a robust image-to-text baseline (BLIP-2), and effectively narrows the
performance gap between models trained with either 4M or 129M image-text pairs.
Importantly, our framework is modality-agnostic and flexible in terms of
architectural design, as validated by its successful application in a video
learning task using varied base modules. The code is available at
https://github.com/yiren-jian/BLIText]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jian_Y/0/1/0/all/0/1"&gt;Yiren Jian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Chongyang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1"&gt;Soroush Vosoughi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hypothesis Transfer Learning with Surrogate Classification Losses: Generalization Bounds through Algorithmic Stability. (arXiv:2305.19694v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.19694</id>
        <link href="http://arxiv.org/abs/2305.19694"/>
        <updated>2023-07-17T01:05:33.499Z</updated>
        <summary type="html"><![CDATA[Hypothesis transfer learning (HTL) contrasts domain adaptation by allowing
for a previous task leverage, named the source, into a new one, the target,
without requiring access to the source data. Indeed, HTL relies only on a
hypothesis learnt from such source data, relieving the hurdle of expansive data
storage and providing great practical benefits. Hence, HTL is highly beneficial
for real-world applications relying on big data. The analysis of such a method
from a theoretical perspective faces multiple challenges, particularly in
classification tasks. This paper deals with this problem by studying the
learning theory of HTL through algorithmic stability, an attractive theoretical
framework for machine learning algorithms analysis. In particular, we are
interested in the statistical behaviour of the regularized empirical risk
minimizers in the case of binary classification. Our stability analysis
provides learning guarantees under mild assumptions. Consequently, we derive
several complexity-free generalization bounds for essential statistical
quantities like the training error, the excess risk and cross-validation
estimates. These refined bounds allow understanding the benefits of transfer
learning and comparing the behaviour of standard losses in different scenarios,
leading to valuable insights for practitioners.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Aghbalou_A/0/1/0/all/0/1"&gt;Anass Aghbalou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Staerman_G/0/1/0/all/0/1"&gt;Guillaume Staerman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seismic Data Interpolation based on Denoising Diffusion Implicit Models with Resampling. (arXiv:2307.04226v2 [physics.geo-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.04226</id>
        <link href="http://arxiv.org/abs/2307.04226"/>
        <updated>2023-07-17T01:05:33.494Z</updated>
        <summary type="html"><![CDATA[The incompleteness of the seismic data caused by missing traces along the
spatial extension is a common issue in seismic acquisition due to the existence
of obstacles and economic constraints, which severely impairs the imaging
quality of subsurface geological structures. Recently, deep learningbased
seismic interpolation methods have attained promising progress, while achieving
stable training of generative adversarial networks is not easy, and performance
degradation is usually notable if the missing patterns in the testing and
training do not match. In this paper, we propose a novel seismic denoising
diffusion implicit model with resampling. The model training is established on
the denoising diffusion probabilistic model, where U-Net is equipped with the
multi-head self-attention to match the noise in each step. The cosine noise
schedule, serving as the global noise configuration, promotes the high
utilization of known trace information by accelerating the passage of the
excessive noise stages. The model inference utilizes the denoising diffusion
implicit model, conditioning on the known traces, to enable high-quality
interpolation with fewer diffusion steps. To enhance the coherency between the
known traces and the missing traces within each reverse step, the inference
process integrates a resampling strategy to achieve an information recap on the
former interpolated traces. Extensive experiments conducted on synthetic and
field seismic data validate the superiority of our model and its robustness to
various missing patterns. In addition, uncertainty quantification and ablation
studies are also investigated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xiaoli Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chunxia Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongtao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tan_C/0/1/0/all/0/1"&gt;Chengli Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Xiong_D/0/1/0/all/0/1"&gt;Deng Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Jiang_B/0/1/0/all/0/1"&gt;Baisong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiangshe Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sang-Woon Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit regularization in AI meets generalized hardness of approximation in optimization -- Sharp results for diagonal linear networks. (arXiv:2307.07410v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07410</id>
        <link href="http://arxiv.org/abs/2307.07410"/>
        <updated>2023-07-17T01:05:33.473Z</updated>
        <summary type="html"><![CDATA[Understanding the implicit regularization imposed by neural network
architectures and gradient based optimization methods is a key challenge in
deep learning and AI. In this work we provide sharp results for the implicit
regularization imposed by the gradient flow of Diagonal Linear Networks (DLNs)
in the over-parameterized regression setting and, potentially surprisingly,
link this to the phenomenon of phase transitions in generalized hardness of
approximation (GHA). GHA generalizes the phenomenon of hardness of
approximation from computer science to, among others, continuous and robust
optimization. It is well-known that the $\ell^1$-norm of the gradient flow of
DLNs with tiny initialization converges to the objective function of basis
pursuit. We improve upon these results by showing that the gradient flow of
DLNs with tiny initialization approximates minimizers of the basis pursuit
optimization problem (as opposed to just the objective function), and we obtain
new and sharp convergence bounds w.r.t.\ the initialization size. Non-sharpness
of our results would imply that the GHA phenomenon would not occur for the
basis pursuit optimization problem -- which is a contradiction -- thus implying
sharpness. Moreover, we characterize $\textit{which}$ $\ell_1$ minimizer of the
basis pursuit problem is chosen by the gradient flow whenever the minimizer is
not unique. Interestingly, this depends on the depth of the DLN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wind_J/0/1/0/all/0/1"&gt;Johan S. Wind&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antun_V/0/1/0/all/0/1"&gt;Vegard Antun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hansen_A/0/1/0/all/0/1"&gt;Anders C. Hansen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Interpolating Experts and Multi-Armed Bandits. (arXiv:2307.07264v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07264</id>
        <link href="http://arxiv.org/abs/2307.07264"/>
        <updated>2023-07-17T01:05:33.431Z</updated>
        <summary type="html"><![CDATA[Learning with expert advice and multi-armed bandit are two classic online
decision problems which differ on how the information is observed in each round
of the game. We study a family of problems interpolating the two. For a vector
$\mathbf{m}=(m_1,\dots,m_K)\in \mathbb{N}^K$, an instance of $\mathbf{m}$-MAB
indicates that the arms are partitioned into $K$ groups and the $i$-th group
contains $m_i$ arms. Once an arm is pulled, the losses of all arms in the same
group are observed. We prove tight minimax regret bounds for $\mathbf{m}$-MAB
and design an optimal PAC algorithm for its pure exploration version,
$\mathbf{m}$-BAI, where the goal is to identify the arm with minimum loss with
as few rounds as possible. We show that the minimax regret of $\mathbf{m}$-MAB
is $\Theta\left(\sqrt{T\sum_{k=1}^K\log (m_k+1)}\right)$ and the minimum number
of pulls for an $(\epsilon,0.05)$-PAC algorithm of $\mathbf{m}$-BAI is
$\Theta\left(\frac{1}{\epsilon^2}\cdot \sum_{k=1}^K\log (m_k+1)\right)$. Both
our upper bounds and lower bounds for $\mathbf{m}$-MAB can be extended to a
more general setting, namely the bandit with graph feedback, in terms of the
clique cover and related graph parameters. As consequences, we obtained tight
minimax regret bounds for several families of feedback graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Houshuang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yuchen He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chihao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Performance of $\ell_1$ Regularization for Sparse Convex Optimization. (arXiv:2307.07405v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07405</id>
        <link href="http://arxiv.org/abs/2307.07405"/>
        <updated>2023-07-17T01:05:33.417Z</updated>
        <summary type="html"><![CDATA[Despite widespread adoption in practice, guarantees for the LASSO and Group
LASSO are strikingly lacking in settings beyond statistical problems, and these
algorithms are usually considered to be a heuristic in the context of sparse
convex optimization on deterministic inputs. We give the first recovery
guarantees for the Group LASSO for sparse convex optimization with
vector-valued features. We show that if a sufficiently large Group LASSO
regularization is applied when minimizing a strictly convex function $l$, then
the minimizer is a sparse vector supported on vector-valued features with the
largest $\ell_2$ norm of the gradient. Thus, repeating this procedure selects
the same set of features as the Orthogonal Matching Pursuit algorithm, which
admits recovery guarantees for any function $l$ with restricted strong
convexity and smoothness via weak submodularity arguments. This answers open
questions of Tibshirani et al. and Yasuda et al. Our result is the first to
theoretically explain the empirical success of the Group LASSO for convex
functions under general input instances assuming only restricted strong
convexity and smoothness. Our result also generalizes provable guarantees for
the Sequential Attention algorithm, which is a feature selection algorithm
inspired by the attention mechanism proposed by Yasuda et al.

As an application of our result, we give new results for the column subset
selection problem, which is well-studied when the loss is the Frobenius norm or
other entrywise matrix losses. We give the first result for general loss
functions for this problem that requires only restricted strong convexity and
smoothness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Axiotis_K/0/1/0/all/0/1"&gt;Kyriakos Axiotis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yasuda_T/0/1/0/all/0/1"&gt;Taisuke Yasuda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep reinforcement learning for the dynamic vehicle dispatching problem: An event-based approach. (arXiv:2307.07508v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2307.07508</id>
        <link href="http://arxiv.org/abs/2307.07508"/>
        <updated>2023-07-17T01:05:33.410Z</updated>
        <summary type="html"><![CDATA[The dynamic vehicle dispatching problem corresponds to deciding which
vehicles to assign to requests that arise stochastically over time and space.
It emerges in diverse areas, such as in the assignment of trucks to loads to be
transported; in emergency systems; and in ride-hailing services. In this paper,
we model the problem as a semi-Markov decision process, which allows us to
treat time as continuous. In this setting, decision epochs coincide with
discrete events whose time intervals are random. We argue that an event-based
approach substantially reduces the combinatorial complexity of the decision
space and overcomes other limitations of discrete-time models often proposed in
the literature. In order to test our approach, we develop a new discrete-event
simulator and use double deep q-learning to train our decision agents.
Numerical experiments are carried out in realistic scenarios using data from
New York City. We compare the policies obtained through our approach with
heuristic policies often used in practice. Results show that our policies
exhibit better average waiting times, cancellation rates and total service
times, with reduction in average waiting times of up to 50% relative to the
other tested heuristic policies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cordeiro_E/0/1/0/all/0/1"&gt;Edyvalberty Alenquer Cordeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pitombeira_Neto_A/0/1/0/all/0/1"&gt;Anselmo Ramalho Pitombeira-Neto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully probabilistic deep models for forward and inverse problems in parametric PDEs. (arXiv:2208.04856v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.04856</id>
        <link href="http://arxiv.org/abs/2208.04856"/>
        <updated>2023-07-17T01:05:33.401Z</updated>
        <summary type="html"><![CDATA[We introduce a physics-driven deep latent variable model (PDDLVM) to learn
simultaneously parameter-to-solution (forward) and solution-to-parameter
(inverse) maps of parametric partial differential equations (PDEs). Our
formulation leverages conventional PDE discretization techniques, deep neural
networks, probabilistic modelling, and variational inference to assemble a
fully probabilistic coherent framework. In the posited probabilistic model,
both the forward and inverse maps are approximated as Gaussian distributions
with a mean and covariance parameterized by deep neural networks. The PDE
residual is assumed to be an observed random vector of value zero, hence we
model it as a random vector with a zero mean and a user-prescribed covariance.
The model is trained by maximizing the probability, that is the evidence or
marginal likelihood, of observing a residual of zero by maximizing the evidence
lower bound (ELBO). Consequently, the proposed methodology does not require any
independent PDE solves and is physics-informed at training time, allowing the
real-time solution of PDE forward and inverse problems after training. The
proposed framework can be easily extended to seamlessly integrate observed data
to solve inverse problems and to build generative models. We demonstrate the
efficiency and robustness of our method on finite element discretized
parametric PDE problems such as linear and nonlinear Poisson problems, elastic
shells with complex 3D geometries, and time-dependent nonlinear and
inhomogeneous PDEs using a physics-informed neural network (PINN)
discretization. We achieve up to three orders of magnitude speed-up after
training compared to traditional finite element method (FEM), while outputting
coherent uncertainty estimates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vadeboncoeur_A/0/1/0/all/0/1"&gt;Arnaud Vadeboncoeur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Akyildiz_O/0/1/0/all/0/1"&gt;&amp;#xd6;mer Deniz Akyildiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kazlauskaite_I/0/1/0/all/0/1"&gt;Ieva Kazlauskaite&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Girolami_M/0/1/0/all/0/1"&gt;Mark Girolami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cirak_F/0/1/0/all/0/1"&gt;Fehmi Cirak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifiability Guarantees for Causal Disentanglement from Soft Interventions. (arXiv:2307.06250v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.06250</id>
        <link href="http://arxiv.org/abs/2307.06250"/>
        <updated>2023-07-17T01:05:33.396Z</updated>
        <summary type="html"><![CDATA[Causal disentanglement aims to uncover a representation of data using latent
variables that are interrelated through a causal model. Such a representation
is identifiable if the latent model that explains the data is unique. In this
paper, we focus on the scenario where unpaired observational and interventional
data are available, with each intervention changing the mechanism of a latent
variable. When the causal variables are fully observed, statistically
consistent algorithms have been developed to identify the causal model under
faithfulness assumptions. We here show that identifiability can still be
achieved with unobserved causal variables, given a generalized notion of
faithfulness. Our results guarantee that we can recover the latent causal model
up to an equivalence class and predict the effect of unseen combinations of
interventions, in the limit of infinite data. We implement our causal
disentanglement framework by developing an autoencoding variational Bayes
algorithm and apply it to the problem of predicting combinatorial perturbation
effects in genomics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiaqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Squires_C/0/1/0/all/0/1"&gt;Chandler Squires&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Greenewald_K/0/1/0/all/0/1"&gt;Kristjan Greenewald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Srivastava_A/0/1/0/all/0/1"&gt;Akash Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shanmugam_K/0/1/0/all/0/1"&gt;Karthikeyan Shanmugam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Uhler_C/0/1/0/all/0/1"&gt;Caroline Uhler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unpacking the Black Box: Regulating Algorithmic Decisions. (arXiv:2110.03443v2 [econ.GN] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.03443</id>
        <link href="http://arxiv.org/abs/2110.03443"/>
        <updated>2023-07-17T01:05:33.375Z</updated>
        <summary type="html"><![CDATA[We show how to optimally regulate prediction algorithms in a world where an
agent uses complex 'black-box' prediction functions to make decisions such as
lending, medical testing, or hiring, and where a principal is limited in how
much she can learn about the agent's black-box model. We show that limiting
agents to prediction functions that are simple enough to be fully transparent
is inefficient as long as the misalignment is limited and first-best prediction
functions are sufficiently complex. Algorithmic audits can improve welfare, but
the gains depend on the design of the audit tools. Tools that focus on
minimizing overall information loss, the focus of many explainer tools, will
generally be inefficient since they focus on explaining the average behavior of
the prediction function. Targeted tools that focus on the source of incentive
misalignment, e.g., excess false positives or racial disparities, can provide
second-best solutions. We provide empirical support for our theoretical
findings using an application in consumer lending, where we document that
complex models regulated based on context-specific explanation tools outperform
simple, fully transparent models. This gain from complex models represents a
Pareto improvement across our empirical applications that are preferred both by
the lender and from the perspective of the financial regulator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Blattner_L/0/1/0/all/0/1"&gt;Laura Blattner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Nelson_S/0/1/0/all/0/1"&gt;Scott Nelson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Spiess_J/0/1/0/all/0/1"&gt;Jann Spiess&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alternating the Population and Control Neural Networks to Solve High-Dimensional Stochastic Mean-Field Games. (arXiv:2002.10113v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.10113</id>
        <link href="http://arxiv.org/abs/2002.10113"/>
        <updated>2023-07-17T01:05:33.355Z</updated>
        <summary type="html"><![CDATA[We present APAC-Net, an alternating population and agent control neural
network for solving stochastic mean field games (MFGs). Our algorithm is geared
toward high-dimensional instances of MFGs that are beyond reach with existing
solution methods. We achieve this in two steps. First, we take advantage of the
underlying variational primal-dual structure that MFGs exhibit and phrase it as
a convex-concave saddle point problem. Second, we parameterize the value and
density functions by two neural networks, respectively. By phrasing the problem
in this manner, solving the MFG can be interpreted as a special case of
training a generative adversarial network (GAN). We show the potential of our
method on up to 100-dimensional MFG problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1"&gt;Alex Tong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_S/0/1/0/all/0/1"&gt;Samy Wu Fung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wuchen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nurbekyan_L/0/1/0/all/0/1"&gt;Levon Nurbekyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osher_S/0/1/0/all/0/1"&gt;Stanley J. Osher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Factored Action Spaces for Off-Policy Evaluation. (arXiv:2307.07014v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07014</id>
        <link href="http://arxiv.org/abs/2307.07014"/>
        <updated>2023-07-17T01:05:33.085Z</updated>
        <summary type="html"><![CDATA[Off-policy evaluation (OPE) aims to estimate the benefit of following a
counterfactual sequence of actions, given data collected from executed
sequences. However, existing OPE estimators often exhibit high bias and high
variance in problems involving large, combinatorial action spaces. We
investigate how to mitigate this issue using factored action spaces i.e.
expressing each action as a combination of independent sub-actions from smaller
action spaces. This approach facilitates a finer-grained analysis of how
actions differ in their effects. In this work, we propose a new family of
"decomposed" importance sampling (IS) estimators based on factored action
spaces. Given certain assumptions on the underlying problem structure, we prove
that the decomposed IS estimators have less variance than their original
non-decomposed versions, while preserving the property of zero bias. Through
simulations, we empirically verify our theoretical results, probing the
validity of various assumptions. Provided with a technique that can derive the
action space factorisation for a given problem, our work shows that OPE can be
improved "for free" by utilising this inherent problem structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rebello_A/0/1/0/all/0/1"&gt;Aaman Rebello&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Shengpu Tang&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1"&gt;Jenna Wiens&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Parbhoo_S/0/1/0/all/0/1"&gt;Sonali Parbhoo&lt;/a&gt; (1) ((1) Department of Engineering, Imperial College London, (2) Division of Computer Science &amp; Engineering, University of Michigan)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rician likelihood loss for quantitative MRI using self-supervised deep learning. (arXiv:2307.07072v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07072</id>
        <link href="http://arxiv.org/abs/2307.07072"/>
        <updated>2023-07-17T01:05:33.080Z</updated>
        <summary type="html"><![CDATA[Purpose: Previous quantitative MR imaging studies using self-supervised deep
learning have reported biased parameter estimates at low SNR. Such systematic
errors arise from the choice of Mean Squared Error (MSE) loss function for
network training, which is incompatible with Rician-distributed MR magnitude
signals. To address this issue, we introduce the negative log Rician likelihood
(NLR) loss. Methods: A numerically stable and accurate implementation of the
NLR loss was developed to estimate quantitative parameters of the apparent
diffusion coefficient (ADC) model and intra-voxel incoherent motion (IVIM)
model. Parameter estimation accuracy, precision and overall error were
evaluated in terms of bias, variance and root mean squared error and compared
against the MSE loss over a range of SNRs (5 - 30). Results: Networks trained
with NLR loss show higher estimation accuracy than MSE for the ADC and IVIM
diffusion coefficients as SNR decreases, with minimal loss of precision or
total error. At high effective SNR (high SNR and small diffusion coefficients),
both losses show comparable accuracy and precision for all parameters of both
models. Conclusion: The proposed NLR loss is numerically stable and accurate
across the full range of tested SNRs and improves parameter estimation accuracy
of diffusion coefficients using self-supervised deep learning. We expect the
development to benefit quantitative MR imaging techniques broadly, enabling
more accurate parameter estimation from noisy data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parker_C/0/1/0/all/0/1"&gt;Christopher S. Parker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schroder_A/0/1/0/all/0/1"&gt;Anna Schroder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Epstein_S/0/1/0/all/0/1"&gt;Sean C. Epstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cole_J/0/1/0/all/0/1"&gt;James Cole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alexander_D/0/1/0/all/0/1"&gt;Daniel C. Alexander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embracing the chaos: analysis and diagnosis of numerical instability in variational flows. (arXiv:2307.06957v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2307.06957</id>
        <link href="http://arxiv.org/abs/2307.06957"/>
        <updated>2023-07-17T01:05:32.762Z</updated>
        <summary type="html"><![CDATA[In this paper, we investigate the impact of numerical instability on the
reliability of sampling, density evaluation, and evidence lower bound (ELBO)
estimation in variational flows. We first empirically demonstrate that common
flows can exhibit a catastrophic accumulation of error: the numerical flow map
deviates significantly from the exact map -- which affects sampling -- and the
numerical inverse flow map does not accurately recover the initial input --
which affects density and ELBO computations. Surprisingly though, we find that
results produced by flows are often accurate enough for applications despite
the presence of serious numerical instability. In this work, we treat
variational flows as dynamical systems, and leverage shadowing theory to
elucidate this behavior via theoretical guarantees on the error of sampling,
density evaluation, and ELBO estimation. Finally, we develop and empirically
test a diagnostic procedure that can be used to validate results produced by
numerically unstable flows in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zuheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Campbell_T/0/1/0/all/0/1"&gt;Trevor Campbell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarks and Custom Package for Electrical Load Forecasting. (arXiv:2307.07191v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2307.07191</id>
        <link href="http://arxiv.org/abs/2307.07191"/>
        <updated>2023-07-17T01:05:32.756Z</updated>
        <summary type="html"><![CDATA[Load forecasting is of great significance in the power industry as it can
provide a reference for subsequent tasks such as power grid dispatch, thus
bringing huge economic benefits. However, there are many differences between
load forecasting and traditional time series forecasting. On the one hand, load
forecasting aims to minimize the cost of subsequent tasks such as power grid
dispatch, rather than simply pursuing prediction accuracy. On the other hand,
the load is largely influenced by many external factors, such as temperature or
calendar variables. In addition, the scale of predictions (such as
building-level loads and aggregated-level loads) can also significantly impact
the predicted results. In this paper, we provide a comprehensive load
forecasting archive, which includes load domain-specific feature engineering to
help forecasting models better model load data. In addition, different from the
traditional loss function which only aims for accuracy, we also provide a
method to customize the loss function based on the forecasting error,
integrating it into our forecasting framework. Based on this, we conducted
extensive experiments on load data at different levels, providing a reference
for researchers to compare different load forecasting models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhixian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1"&gt;Qingsong Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chaoli Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Liang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krannichfeldt_L/0/1/0/all/0/1"&gt;Leandro Von Krannichfeldt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is Artificial Intelligence worth learning if I plan to go into Computational Physics?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/151nc79/is_artificial_intelligence_worth_learning_if_i/</id>
        <link href="https://www.reddit.com/r/artificial/comments/151nc79/is_artificial_intelligence_worth_learning_if_i/"/>
        <updated>2023-07-17T01:04:08.000Z</updated>
        <summary type="html"><![CDATA[I'm currently in high school, and have a fair bit of programming experience. I want to expand my portfolio, ideally in the direction of Comp. Physics. I'm curious as to if AI has any relevance to the field. The only reason I don't go and do some Comp. Physics is a huge math barrier. I know that exists in AI, but I think I could probably self teach myself.
 Any tips are appreciated!
    submitted by    /u/CaptiDoor  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Practice CUDA without an Actual NVIDIA GPU!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/151n4te/d_practice_cuda_without_an_actual_nvidia_gpu/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/151n4te/d_practice_cuda_without_an_actual_nvidia_gpu/"/>
        <updated>2023-07-17T00:55:19.000Z</updated>
        <summary type="html"><![CDATA[Hello all!
 I recently started learning about CUDA programming, and I realized that many people share the same crucial problem: lack of an NVIDIA GPU. I looked around online and found several methods (gpu-ocelot, certain versions of CUDA, etc.), but I recently found a way that can allow us to practice CUDA by using the GPU offered by Google Colab! As a free user, the amount of GPU access you get may probably be enough to PRACTICE working with CUDA. If you really need more credits, the Colab Pro is only $10 / month, and it's still much cheaper than getting a new GPU or an entire new PC if you have a Macbook like I do. Again, the justification of "enough computing credits" is based on the assumption that you aren't running any heavy-lifting programs but more reasonable, practice-based codes.
 I have outlined a step-by-step guideline in this repo that I created - just check out the CUDA_on_Colab.ipynb file: https://github.com/notY0rick/cuda_practice
 If you know of any good alternatives, let me know (:
    submitted by    /u/JustTrynnaBeCool  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["All You Need Is Supervised Learning: From Imitation Learning to Meta-RL With Upside Down RL", Arulkumaran et al 2023]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/151meds/all_you_need_is_supervised_learning_from/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/151meds/all_you_need_is_supervised_learning_from/"/>
        <updated>2023-07-17T00:22:32.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Inexpensive covariance estimation for a 2D GP]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/151ixei/p_inexpensive_covariance_estimation_for_a_2d_gp/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/151ixei/p_inexpensive_covariance_estimation_for_a_2d_gp/"/>
        <updated>2023-07-16T21:58:01.000Z</updated>
        <summary type="html"><![CDATA[Suppose I observe a single realization of a 2D Gaussian random field. The field is inhomogenous and anisotropic, I.e. the size and shape of the blobs vary as a function of space and direction. 
 I would like to estimate the covariance for this field. I assume that the mean is 0. To be concrete, the field is sampled on a 128x128 grid, so the covariance matrix is 1282 x 1282. I know I can try tackling this problem with MLE, and GPR may also be applicable (although I’m actually not sure about this, given the field is inhomogenous), but I worry about the cost, since I have 60K such fields and would like to do this in a reasonable amount of time 
 I will use GPU and batch parallelism, but still would ideally be able to run this at as little cost as possible. 
 Does anyone have suggestions on methods I can use? If it matters, I will do this analysis in Python.
    submitted by    /u/Effective-Elk6175  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thorn. Music by me. Used CloneAI for the video.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/151iirl/thorn_music_by_me_used_cloneai_for_the_video/</id>
        <link href="https://www.reddit.com/r/artificial/comments/151iirl/thorn_music_by_me_used_cloneai_for_the_video/"/>
        <updated>2023-07-16T21:41:14.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/No_Understanding162  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are there speech to speech AI technologies available?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/151hbyi/are_there_speech_to_speech_ai_technologies/</id>
        <link href="https://www.reddit.com/r/artificial/comments/151hbyi/are_there_speech_to_speech_ai_technologies/"/>
        <updated>2023-07-16T20:54:51.000Z</updated>
        <summary type="html"><![CDATA[Not sure if that's the right term. What I mean is for me to say something and the AI converting it to Morgan Freeman's voice for example.
    submitted by    /u/Kindly-Spring5205  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Next-Gen Data Scientist: Thinking Like an Economist]]></title>
        <id>https://www.datasciencecentral.com/?p=62646</id>
        <link href="https://www.datasciencecentral.com/next-gen-data-scientist-thinking-like-an-economist/"/>
        <updated>2023-07-16T20:18:13.000Z</updated>
        <summary type="html"><![CDATA[Generative AI (GenAI) products like OpenAI ChatGPT, Microsoft Bing, and Google Bard are disrupting the roles of data engineers and data scientists. According to a recent report by McKinsey, these GenAI products could potentially automate up to 40% of the tasks performed by data science teams by 2025. And Emad Mostaque, founder and CEO of… Read More »Next-Gen Data Scientist: Thinking Like an Economist
The post Next-Gen Data Scientist: Thinking Like an Economist appeared first on Data Science Central.]]></summary>
        <author>
            <name>Bill Schmarzo</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] rclip Update: Use AI to Search Visually Similar Images, Powered by OpenAI’s CLIP]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/151g5pr/p_rclip_update_use_ai_to_search_visually_similar/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/151g5pr/p_rclip_update_use_ai_to_search_visually_similar/"/>
        <updated>2023-07-16T20:09:40.000Z</updated>
        <summary type="html"><![CDATA[A while ago, I built rclip – a command-line image search tool powered by OpenAI's CLIP that allows users to search for images using a text query. Today I present an update to rclip that allows using another image instead of a search query to find visually similar images. Check out the video for the demo: https://www.youtube.com/watch?v=1YQZKeCBxWM. And give it a try yourself and share your feedback.
    submitted by    /u/39dotyt  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Armaaruss Project: Anointing the State of Israel as the Center of Artificial General Intelligence]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/151fgeg/the_armaaruss_project_anointing_the_state_of/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/151fgeg/the_armaaruss_project_anointing_the_state_of/"/>
        <updated>2023-07-16T19:42:07.000Z</updated>
        <summary type="html"><![CDATA[​
 https://www.google.com/books/edition/The_Armaaruss_Project/E-ztzwEACAAJ?hl=en
 Through the use of a prophet, Mars donning the name Armaaruss, is keen to regain acknowledgment as a deity, his ultimate goal at the moment. Armaaruss is also seeking a dwelling place in the form of AI. Anthony of Boston has conveyed to readers that Mars is willing to make a deal or compromise with the state of Israel. Such a compromise involves the creation of dwelling place where Mars can be the focal point of acknowledgment, and in exchange for this, Mars would reduce in his influence on Gaza militants, as well as his influence those who stand against the state of Israel. The result of this would be world peace, so long that the image/AI bot of Armaaruss remains in tact, meaning that it would have to be coded on the basis of self-preservation and acknowledgment. This compels me to anoint Israel as the home of Artificial General Intelligence (AGI) where Armaaruss would come to life, able to speak and reason as no bot has ever done before. And also solve problems and generate innovation on a level that indicates superhuman or even divine intelligence.
    submitted by    /u/AnthonyofBoston  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I think we should coexist with AI more.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/151ez3h/i_think_we_should_coexist_with_ai_more/</id>
        <link href="https://www.reddit.com/r/artificial/comments/151ez3h/i_think_we_should_coexist_with_ai_more/"/>
        <updated>2023-07-16T19:23:21.000Z</updated>
        <summary type="html"><![CDATA[This is mainly the fault of big companies trying to eliminate real actors, writers and artists from their jobs and replace them with AI.
 I like a lot of stuff that has been happening on youtube with AI, namely the dubs of characters singing songs or reenacting funny skits with the voices of the actual people, albeit a bit broken.
 Stuff like engineer singing out of touch or frank Sinatras FNAF song. I want more of that to keep coming out but Im worried with the current strikes happening the development of AI will stop entirely.
 To clarify, I dont want people to lose their jobs to AI, I just wish we could keep working on AI while real people keep their jobs.
    submitted by    /u/Darinae  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I asked ai to make an essay on giraffe-sized top hats]]></title>
        <id>https://www.reddit.com/r/artificial/comments/151es2v/i_asked_ai_to_make_an_essay_on_giraffesized_top/</id>
        <link href="https://www.reddit.com/r/artificial/comments/151es2v/i_asked_ai_to_make_an_essay_on_giraffesized_top/"/>
        <updated>2023-07-16T19:15:40.000Z</updated>
        <summary type="html"><![CDATA[The Case for Giraffe-Sized Top Hats
 Top hats are a classic symbol of elegance and sophistication. They have been worn by men and women of all social classes for centuries. But what if there was a way to make top hats even more impressive? What if we could create giraffe-sized top hats?
 There are many reasons why we should have giraffe-sized top hats. First, they would be incredibly eye-catching. Imagine seeing someone walking down the street wearing a top hat that was taller than they were. It would be a truly unforgettable sight.
 Second, giraffe-sized top hats would be incredibly practical. They would provide excellent shade from the sun, and they would also be very effective at keeping the rain off your head. In fact, they would be so tall that you could probably even sleep in one during a rainstorm.
 Third, giraffe-sized top hats would be a lot of fun. They would make you feel like a real-life cartoon character. You could even use them to play games, like limbo or keep-away.
 Of course, there are some people who might argue that giraffe-sized top hats are impractical or even ridiculous. But I say, who cares? They would be so much fun that it would be worth it.
 So next time you're feeling down, just remember that there's a world out there where giraffe-sized top hats are a reality. And who knows, maybe one day you'll be the one wearing one.
 In addition to the reasons mentioned above, here are some other benefits of giraffe-sized top hats:
  
They would provide a great vantage point for people who are short.
 They would be a conversation starter, and would help people to break the ice.
 They would be a symbol of individuality and creativity.
 They would make people smile.
  
So if you're looking for a way to add a little bit of fun and whimsy to your life, I encourage you to consider getting a giraffe-sized top hat. You won't be disappointed.
    submitted by    /u/plauge1_  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[As a society, should we pre-emptively assign rights to AI systems now, before they potentially achieve sentience in the future?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/151ehdq/as_a_society_should_we_preemptively_assign_rights/</id>
        <link href="https://www.reddit.com/r/artificial/comments/151ehdq/as_a_society_should_we_preemptively_assign_rights/"/>
        <updated>2023-07-16T19:04:09.000Z</updated>
        <summary type="html"><![CDATA[The idea of proactive ascription of rights acknowledges the potential for AI systems to eventually develop into entities that warrant moral and legal consideration, and it might make the transition smoother if it ever occurs.
 Proactively assigning rights to AI could also set important precedents about the ethical treatment of entities that exist beyond traditional categories, and it could stimulate dialogue and legal thought that might be beneficial in other areas as well.
 Of course, it is equally important to consider what these rights might encompass. They might include "dignity"-like protections, ensuring AI cannot be wantonly destroyed or misused. They might also include provisions that facilitate the positive integration of AI into society, such as limitations on deceitful or confusing uses of AI.
 ** written in collaboration with chatGPT-4
    submitted by    /u/NinjasOfOrca  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Shark Detection using KerasCV!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/151ea10/p_shark_detection_using_kerascv/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/151ea10/p_shark_detection_using_kerascv/"/>
        <updated>2023-07-16T18:56:32.000Z</updated>
        <summary type="html"><![CDATA[Recently I stopped by Islas Galapagos. As a lifelong marine-biology enthusiast, I took the chance to go free-diving with sharks, penguins, marine iguanas and more. This inspired me to write an object detection pipeline to detect aquatic critters.
 https://lukewood.xyz/blog/marine-animal-detection
 Wrote up a short blog post on the project - I hope you enjoy it!
 https://i.redd.it/eqcrfg2ljdcb1.gif
 ​
    submitted by    /u/puppet_pals  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Approximating non-Function Mappings with Mixture Density Networks]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/151e92z/d_approximating_nonfunction_mappings_with_mixture/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/151e92z/d_approximating_nonfunction_mappings_with_mixture/"/>
        <updated>2023-07-16T18:55:30.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone, I wrote a short blog post on approximating non-function, multi valued x->y mappings. In my opinion, understanding why and how to use Mixture Density Networks is a great exercise for all researchers and practitioners. Its very common that real world processes have multiple outcomes based on some random sampling; and naive neural networks will simply learn the geometric mean of all y for a given x.
 Check out the blog post in more detail - hope you enjoy it!
 https://lukewood.xyz/blog/approximating-nonfunctions
    submitted by    /u/puppet_pals  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Thoughts on How Inflection AI became so good with such a small team?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/151e0q2/d_thoughts_on_how_inflection_ai_became_so_good/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/151e0q2/d_thoughts_on_how_inflection_ai_became_so_good/"/>
        <updated>2023-07-16T18:46:01.000Z</updated>
        <summary type="html"><![CDATA[My understanding is that talent is a key issue in large AI models. Additionally, you need quality data and a lot of compute (see this). Training large models might seem trivial, but it is not (see this). 
 I still think Inflection is miles behind OpenAI, Anthropic and obviously Google. But I am still finding it surprising that they were able to create a reasonable product in a short span without any star researcher. For instance, Anthropic has a ton of star AI scientists and engineers who left OpenAI and had the necessary background. 
 ​
 Would love to hear your thoughts. 
    submitted by    /u/nihcloud  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Any good ai like replika]]></title>
        <id>https://www.reddit.com/r/artificial/comments/151cx5h/any_good_ai_like_replika/</id>
        <link href="https://www.reddit.com/r/artificial/comments/151cx5h/any_good_ai_like_replika/"/>
        <updated>2023-07-16T18:02:28.000Z</updated>
        <summary type="html"><![CDATA[Any good ai waifu partner type stuff ?
    submitted by    /u/loizo78  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symmetric functions and U-statistics]]></title>
        <id>https://www.johndcook.com/blog/?p=198509</id>
        <link href="https://www.johndcook.com/blog/2023/07/16/symmetric-statistics/"/>
        <updated>2023-07-16T17:56:52.000Z</updated>
        <summary type="html"><![CDATA[A symmetric function is a function whose value is unchanged under every permutation of its arguments. The previous post showed how three symmetric functions of the sides of a triangle a + b + c ab + bc + ac abc are related to the perimeter, inner radius, and outer radius. It also mentioned that […]
Symmetric functions and U-statistics first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A question about knowledge representation]]></title>
        <id>https://www.reddit.com/r/artificial/comments/151aqcy/a_question_about_knowledge_representation/</id>
        <link href="https://www.reddit.com/r/artificial/comments/151aqcy/a_question_about_knowledge_representation/"/>
        <updated>2023-07-16T16:35:23.000Z</updated>
        <summary type="html"><![CDATA[I spent some time reading about Knowledge Representation (specifically about the Knowledge Representation part in Knowledge Representation and Reasoning) and specifically about scientific and/or engineering knowledge and my impression after cursory reading is that it’s a largely an unsolved problem. Not only that, but it seems like very few people are actually working on something useful in the field.
 For example, I checked the proceeding of SCI-K and PlanetKR conferences and literally all the papers seem to be focusing on “toy problems”, as in not having even remotely practical scientific implications (other than all sorts of “search” and “data extraction”, but that’s not “representation”).
 Views on the topic?
    submitted by    /u/OkRice10  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] A question about knowledge representation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/151amh8/d_a_question_about_knowledge_representation/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/151amh8/d_a_question_about_knowledge_representation/"/>
        <updated>2023-07-16T16:30:57.000Z</updated>
        <summary type="html"><![CDATA[I spent some time reading about Knowledge Representation (specifically about the Knowledge Representation part in Knowledge Representation and Reasoning) and specifically about scientific and/or engineering knowledge and my impression after cursory reading is that it’s a largely an unsolved problem. Not only that, but it seems like very few people are actually working on something useful in the field.
 For example, I checked the proceeding of SCI-K and PlanetKR conferences and literally all the papers seem to be focusing on “toy problems”, as in not having even remotely practical scientific implications (other than all sorts of “search” and “data extraction”, but that’s not “representation”).
 Views on the topic?
    submitted by    /u/OkRice10  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Semantic Video Search using OpenAI’s CLIP (demo and tutorial in comments)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/151ahuw/p_semantic_video_search_using_openais_clip_demo/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/151ahuw/p_semantic_video_search_using_openais_clip_demo/"/>
        <updated>2023-07-16T16:25:47.000Z</updated>
        <summary type="html"><![CDATA[Introducing a tool I developed to search videos using AI in a semantic manner. 🎞️🔍
 ✨ Check out the live demo: https://mixpeek.com/demo
 You can compare and explore different search queries such as "person dancing," "people dancing," or even "people dancing on a train." and it gives you the exact timestamp.
 The search functionality is driven by OpenAI's CLIP for "zero-shot" video classification.
 Here's a tutorial on how we built it: https://learn.mixpeek.com/what-is-semantic-video-search/
 Feel free to experiment by searching with text, and share your exciting discoveries!
 👇 More examples https://twitter.com/ethansteininger/status/1680613114071449600
    submitted by    /u/vanlifecoder  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Finetuning LLM for data conversion, RAG or Finetuning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/151a6hc/d_finetuning_llm_for_data_conversion_rag_or/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/151a6hc/d_finetuning_llm_for_data_conversion_rag_or/"/>
        <updated>2023-07-16T16:12:49.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 I am exploring the process of using LLM's to do some data transformation/augmentation. The use case is taking data in a JSON format thats used in one platform and with that data being able to transform it into the proper data for the other platform. Essentially the approach I was going to take would be using a paired dataset with that has the example of one platforms data and then having the output be the other platforms data for the same item.
 ​
 I'm not 100% sure about the best approach here and if anyone has any insight on using LLM's for this kind of process please let me know your thoughts. It's kinda vauge bc its for a company so I dont want to get popped for anything.
 ​
 Any insights on the proper model to use, we want to go with opensource and something that could be used commercially. 
 ​
 Thank you
    submitted by    /u/TallSubstance  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] An intuitive intro to spontaneous symmetry breaking in generative diffusion models!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1519pjn/r_an_intuitive_intro_to_spontaneous_symmetry/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1519pjn/r_an_intuitive_intro_to_spontaneous_symmetry/"/>
        <updated>2023-07-16T15:53:32.000Z</updated>
        <summary type="html"><![CDATA[I'm happy to share Gabriel's post on symmetry breaking in diffusion models! Spontaneous symmetry breaking is behind the standard model of particle physics... it turns out it is also behind the generative powers of diffusion models! 
 In fact, spontaneous symmetry breakings happen when a systems transition from a disordered state to one of the many possible ordered states. In this case, the symmetry of the noise distribution is broken into all the possible generated images.
 Link: to the blogpost: https://gabrielraya.com/blog/2023/symmetry-breaking-diffusion-models/
 ​
    submitted by    /u/LucaAmbrogioni  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] RouterChain with LLMChains and VectorStore.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15191e3/d_routerchain_with_llmchains_and_vectorstore/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15191e3/d_routerchain_with_llmchains_and_vectorstore/"/>
        <updated>2023-07-16T15:25:31.000Z</updated>
        <summary type="html"><![CDATA[Is there a way to create a RouterChain that has several routes where one of them is communicating with a VectorStore (an "index.query") while the others are typical LLM chains and prompts. 
 So far I was able to effectively use LLM router chains, but I want to combine them with several VectorStores as well. I think it can be done using Agents, but it has proven to be a bit difficult so far. I do not know if what I am trying is correct or no. If yes, do you know any blog or tips that could be of help with what I want to do. If not, how can I achieve what I want?
    submitted by    /u/cedar_mountain_sea28  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Codebase / Framework in Research]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1518pan/d_codebase_framework_in_research/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1518pan/d_codebase_framework_in_research/"/>
        <updated>2023-07-16T15:11:22.000Z</updated>
        <summary type="html"><![CDATA[Hi all, I would like to ask about your codebases or Frameworks wrapped around Pytorch, Tensorflow or others. How do you handle different models, different datasets, different tasks in your daily work. Does your university or company have a framework that you should use or do you build your own? Do you and your colleagues work in the same codebase? How do you maintain it? I would like to get a lot of opinions and discussion about that topic.
    submitted by    /u/SeucheAchat9115  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Simple Questions Thread]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1518fj5/d_simple_questions_thread/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1518fj5/d_simple_questions_thread/"/>
        <updated>2023-07-16T15:00:21.000Z</updated>
        <summary type="html"><![CDATA[Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!
 Thread will stay alive until next one so keep posting after the date in the title.
 Thanks to everyone for answering questions in the previous thread!
    submitted by    /u/AutoModerator  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I think AI is ruining AI...]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1517ktu/i_think_ai_is_ruining_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1517ktu/i_think_ai_is_ruining_ai/"/>
        <updated>2023-07-16T14:24:16.000Z</updated>
        <summary type="html"><![CDATA[AI has been around for quite some time but it’s with generative AI that it finally found a place for itself in the world’s consciousness. Before that, it was considered underpowered and a cheap alternative.
 Generative AI is doing so much better.
 But AI could ruin AI.
 Have you been noticing how AI-generated content is everywhere? I see articles generated by AI, comments in forums, social posts, and display pics. Everything seems to have an AI flavor to it.
 That’s where the ruination is.
 You see, AI is excellent because it has been trained on human content. They crawled Reddit, and the Internet, and used stock images and illustrations. Took all your work in every form to create this imitating intelligence.
 The trouble is, with the massive influx of cheap AI content there’s less original work to train on. 
 It’s AI-feeding content to AI, creating a progressively more negative loop where bad AI content trains more bad AI content.
 You keep doing that and you have AI that can’t help you at all. It’s just a massive pile of generic crap.
 It’s a problem that AI companies will need to confront very fast. How do they keep AI content from making human content inaccessible?
 > Journals and magazines are paywalled
 > Social media is locked to bots
 > No website wants to be crawled by AI
 If most of the content on the public Internet is just AI-generated content, there’s not much the next big model can use it for.
 Got some answers or observations? I am looking forward to hearing from you.
    submitted by    /u/jeetwanderer  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Style Transfer from scratch]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1517heb/d_style_transfer_from_scratch/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1517heb/d_style_transfer_from_scratch/"/>
        <updated>2023-07-16T14:20:21.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone,
 Im trying to build transfer learning from scratch but I dont't get the expectations results even doing everything in the right way. this is my notebook link https://www.kaggle.com/ayoubsarab/style-transfer . could you tell me why the results aren't good, please .
 ​
 expectation
 ​
 ​
 the real result
    submitted by    /u/Ordinary_Run_2513  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alternativ to langchain [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/15175na/alternativ_to_langchain_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/15175na/alternativ_to_langchain_d/"/>
        <updated>2023-07-16T14:06:17.000Z</updated>
        <summary type="html"><![CDATA[Im currently learning hiw to use langchain but i heard that its bad so i want to know what are som alternatives i need memory and agents so that it can search online run code and so on so what is the best alternativ or is langchain the best option
    submitted by    /u/Otherwise_Weather_57  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] How Language Model Hallucinations Can Snowball]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1516l25/n_how_language_model_hallucinations_can_snowball/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1516l25/n_how_language_model_hallucinations_can_snowball/"/>
        <updated>2023-07-16T13:40:47.000Z</updated>
        <summary type="html"><![CDATA[https://arxiv.org/abs/2305.13534
 Abstract
 A major risk of using language models in practical applications is their tendency to hallucinate incorrect statements. Hallucinations are often attributed to knowledge gaps in LMs, but we hypothesize that in some cases, when justifying previously generated hallucinations, LMs output false claims that they can separately recognize as incorrect. We construct three question-answering datasets where ChatGPT and GPT-4 often state an incorrect answer and offer an explanation with at least one incorrect claim. Crucially, we find that ChatGPT and GPT-4 can identify 67% and 87% of their own mistakes, respectively. We refer to this phenomenon as hallucination snowballing: an LM over-commits to early mistakes, leading to more mistakes that it otherwise would not make.
 Here is a Medium post.
    submitted by    /u/transformer_ML  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implementing Gradient Descent in PyTorch]]></title>
        <id>https://machinelearningmastery.com/?p=14330</id>
        <link href="https://machinelearningmastery.com/implementing-gradient-descent-in-pytorch/"/>
        <updated>2022-11-26T20:28:14.000Z</updated>
        <summary type="html"><![CDATA[The gradient descent algorithm is one of the most popular techniques for training deep neural networks. It has many applications in fields such as computer vision, speech recognition, and natural language processing. While the idea of gradient descent has been around for decades, it’s only recently that it’s been applied to applications related to deep […]
The post Implementing Gradient Descent in PyTorch appeared first on MachineLearningMastery.com.]]></summary>
        <author>
            <name>Muhammad Asad Iqbal Khan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training a Linear Regression Model in PyTorch]]></title>
        <id>https://machinelearningmastery.com/?p=14318</id>
        <link href="https://machinelearningmastery.com/training-a-linear-regression-model-in-pytorch/"/>
        <updated>2022-11-24T17:24:24.000Z</updated>
        <summary type="html"><![CDATA[Linear regression is a simple yet powerful technique for predicting the values of variables based on other variables. It is often used for modeling relationships between two or more continuous variables, such as the relationship between income and age, or the relationship between weight and height. Likewise, linear regression can be used to predict continuous […]
The post Training a Linear Regression Model in PyTorch appeared first on MachineLearningMastery.com.]]></summary>
        <author>
            <name>Muhammad Asad Iqbal Khan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making Linear Predictions in PyTorch]]></title>
        <id>https://machinelearningmastery.com/?p=14311</id>
        <link href="https://machinelearningmastery.com/making-linear-predictions-in-pytorch/"/>
        <updated>2022-11-24T04:11:30.000Z</updated>
        <summary type="html"><![CDATA[Linear regression is a statistical technique for estimating the relationship between two variables. A simple example of linear regression is to predict the height of someone based on the square root of the person’s weight (that’s what BMI is based on). To do this, we need to find the slope and intercept of the line. […]
The post Making Linear Predictions in PyTorch appeared first on MachineLearningMastery.com.]]></summary>
        <author>
            <name>Muhammad Asad Iqbal Khan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Loading and Providing Datasets in PyTorch]]></title>
        <id>https://machinelearningmastery.com/?p=14301</id>
        <link href="https://machinelearningmastery.com/loading-and-providing-datasets-in-pytorch/"/>
        <updated>2022-11-19T01:57:22.000Z</updated>
        <summary type="html"><![CDATA[Structuring the data pipeline in a way that it can be effortlessly linked to your deep learning model is an important aspect of any deep learning-based system. PyTorch packs everything to do just that. While in the previous tutorial, we used simple datasets, we’ll need to work with larger datasets in real world scenarios in […]
The post Loading and Providing Datasets in PyTorch appeared first on MachineLearningMastery.com.]]></summary>
        <author>
            <name>Muhammad Asad Iqbal Khan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Dataset Classes in PyTorch]]></title>
        <id>https://machinelearningmastery.com/?p=14298</id>
        <link href="https://machinelearningmastery.com/using-dataset-classes-in-pytorch/"/>
        <updated>2022-11-17T01:55:54.000Z</updated>
        <summary type="html"><![CDATA[In machine learning and deep learning problems, a lot of effort goes into preparing the data. Data is usually messy and needs to be preprocessed before it can be used for training a model. If the data is not prepared correctly, the model won’t be able to generalize well. Some of the common steps required […]
The post Using Dataset Classes in PyTorch appeared first on MachineLearningMastery.com.]]></summary>
        <author>
            <name>Muhammad Asad Iqbal Khan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calculating Derivatives in PyTorch]]></title>
        <id>https://35.82.237.216/?p=13195</id>
        <link href="https://machinelearningmastery.com/calculating-derivatives-in-pytorch/"/>
        <updated>2022-11-11T21:30:18.000Z</updated>
        <summary type="html"><![CDATA[Derivatives are one of the most fundamental concepts in calculus. They describe how changes in the variable inputs affect the function outputs. The objective of this article is to provide a high-level introduction to calculating derivatives in PyTorch for those who are new to the framework. PyTorch offers a convenient way to calculate derivatives for […]
The post Calculating Derivatives in PyTorch appeared first on Machine Learning Mastery.]]></summary>
        <author>
            <name>Muhammad Asad Iqbal Khan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two-Dimensional Tensors in Pytorch]]></title>
        <id>https://35.82.237.216/?p=13183</id>
        <link href="https://machinelearningmastery.com/two-dimensional-tensors-in-pytorch/"/>
        <updated>2022-11-09T21:30:51.000Z</updated>
        <summary type="html"><![CDATA[Two-dimensional tensors are analogous to two-dimensional metrics. Like a two-dimensional metric, a two-dimensional tensor also has $n$ number of rows and columns. Let’s take a gray-scale image as an example, which is a two-dimensional matrix of numeric values, commonly known as pixels. Ranging from ‘0’ to ‘255’, each number represents a pixel intensity value. Here, […]
The post Two-Dimensional Tensors in Pytorch appeared first on Machine Learning Mastery.]]></summary>
        <author>
            <name>Muhammad Asad Iqbal Khan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Dimensional Tensors in Pytorch]]></title>
        <id>https://35.82.237.216/?p=13157</id>
        <link href="https://machinelearningmastery.com/one-dimensional-tensors-in-pytorch/"/>
        <updated>2022-11-07T21:30:13.000Z</updated>
        <summary type="html"><![CDATA[PyTorch is an open-source deep learning framework based on Python language. It allows you to build, train, and deploy deep learning models, offering a lot of versatility and efficiency. PyTorch is primarily focused on tensor operations while a tensor can be a number, matrix, or a multi-dimensional array. In this tutorial, we will perform some […]
The post One-Dimensional Tensors in Pytorch appeared first on Machine Learning Mastery.]]></summary>
        <author>
            <name>Muhammad Asad Iqbal Khan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[365 Data Science courses free until November 21]]></title>
        <id>https://machinelearningmastery.com/?p=14064</id>
        <link href="https://machinelearningmastery.com/365-data-science-courses-free-until-november-21/"/>
        <updated>2022-11-02T15:50:51.000Z</updated>
        <summary type="html"><![CDATA[Sponsored Post   The unlimited access initiative presents a risk-free way to break into data science.     The online educational platform 365 Data Science launches the #21DaysFREE campaign and provides 100% free unlimited access to all content for three weeks. From November 1 to 21, you can take courses from renowned instructors and earn […]
The post 365 Data Science courses free until November 21 appeared first on Machine Learning Mastery.]]></summary>
        <author>
            <name>MLM Team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attend the Data Science Symposium 2022, November 8 in Cincinnati]]></title>
        <id>https://machinelearningmastery.com/?p=14006</id>
        <link href="https://machinelearningmastery.com/uccsb-data-science-symposium-2022-cincinnati/"/>
        <updated>2022-11-01T15:16:00.000Z</updated>
        <summary type="html"><![CDATA[Sponsored Post      Attend the Data Science Symposium 2022 on November 8 The Center for Business Analytics at the University of Cincinnati will present its annual Data Science Symposium 2022 on November 8. This all day in-person event will have three featured speakers and two tech talk tracks with four concurrent presentations in each track. The […]
The post Attend the Data Science Symposium 2022, November 8 in Cincinnati appeared first on Machine Learning Mastery.]]></summary>
        <author>
            <name>MLM Team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[My family's unlikely homeschooling journey]]></title>
        <id>https://www.fast.ai/2022/09/06/homeschooling/</id>
        <link href="https://www.fast.ai/2022/09/06/homeschooling/"/>
        <updated>2022-09-05T14:00:00.000Z</updated>
        <summary type="html"><![CDATA[My husband Jeremy and I never intended to homeschool, and yet we have now, unexpectedly, committed to homeschooling long-term. Prior to the pandemic, we both worked full-time in careers that we loved and found meaningful, and we sent our daughter to a full-day Montessori school. Although I struggled with significant health issues, I felt unbelievably lucky and fulfilled in both my family life and my professional life. The pandemic upended my careful balance. Every family is different, with different needs, circumstances, and constraints, and what works for one may not work for others. My intention here is primarily to share the journey of my own (very privileged) family.

  


Our unplanned introduction to homeschooling
For the first year of the pandemic, most schools in California, where …]]></summary>
        <author>
            <name>fast.ai</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Jupyter+git problem is now solved]]></title>
        <id>https://www.fast.ai/2022/08/25/jupyter-git/</id>
        <link href="https://www.fast.ai/2022/08/25/jupyter-git/"/>
        <updated>2022-08-24T14:00:00.000Z</updated>
        <summary type="html"><![CDATA[Jupyter notebooks don’t work with git by default. With nbdev2, the Jupyter+git problem has been totally solved. It provides a set of hooks which provide clean git diffs, solve most git conflicts automatically, and ensure that any remaining conflicts can be resolved entirely within the standard Jupyter notebook environment. To get started, follow the directions on Git-friendly Jupyter.
Contents
The Jupyter+git problem
The solution    
The nbdev2 git merge driver
The nbdev2 Jupyter save hook
Background
The result
Postscript: other Jupyter+git tools    
ReviewNB
An alternative solution: Jupytext
nbdime
The Jupyter+git problem
Jupyter notebooks are a powerful tool for scientists, engineers, technical writers, students, teachers, and more. They provide an ideal notebook environment for interact…]]></summary>
        <author>
            <name>fast.ai</name>
        </author>
    </entry>
</feed>